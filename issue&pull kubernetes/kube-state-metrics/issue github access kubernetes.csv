name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
kube-state-metrics,ghodss,https://github.com/kubernetes/kube-state-metrics/pull/1,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/1,kube-state-metrics initial code,,closed,True,2016-05-06 21:39:36,2016-05-10 00:40:37
kube-state-metrics,ghodss,https://github.com/kubernetes/kube-state-metrics/pull/2,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/2,Update README,"Clarify the difference between Heapster and kube-state-metrics.
",closed,True,2016-05-26 19:53:22,2016-05-26 20:42:46
kube-state-metrics,ghodss,https://github.com/kubernetes/kube-state-metrics/pull/3,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/3,Add Travis,,closed,True,2016-05-26 20:14:39,2016-05-26 20:30:25
kube-state-metrics,rvrignaud,https://github.com/kubernetes/kube-state-metrics/issues/4,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/4,Get metrics of pods status per namespace,"Hello,

It would be really useful to get metric of status pods per namespaces like: Running, CrashLoopBackoff, Pending, etc ...

Thanks !
",closed,False,2016-06-03 07:34:57,2016-09-16 08:00:00
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/issues/5,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/5,Make metric collection synchronous,"Currently metrics get updated every 10 seconds in the background. This means scrapers record metrics that are off by an unknown duration of 0 to 10 seconds. It also means that the maximum sampling interval is fixed to 10 seconds, which does not generally fit every use case.

When writing Prometheus exporters, the best practice approach is to gather the data synchronously as `/metrics` is accessed to guarantee accurate sample data. Sampling frequency should be defined by the clients.

I'd like this exporter to follow that approach. I understand that the current behavior is an easy way to prevent overloading (esp. with multiple clients). If development on this repo is continued   quota and auth have to be addressed anyway to defend against mis-behaving clients. But we probably shouldn't limit possibilities at the core of the application.

@kubernetes/sig-instrumentation 
",closed,False,2016-08-31 07:51:56,2016-09-07 14:05:58
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/pull/6,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/6,Rework deployment metrics,"This change extracts the deployment metrics into their own
deployment collector. The metrics are collected synchronously with
each scrape rather than every 10 seconds as suggested in #5.

The `deployment` label replaces the `name` label to avoid name
collisions further up in the monitoring chain.

This includes a vendoring update to have access to the `Gather()` functionality. There's quite a bit of helper code for the comparison. I expect that we'll have similar testing facilities available upstream soon enough and can remove it.
This also switches towards using `ConstMetrics` as direct instrumentation is not intended for sample data retrieved from external sources.

From here we could easily break out other collectors and quickly add further metrics.

@kubernetes/sig-instrumentation @ghodss 
",closed,True,2016-08-31 14:26:04,2016-09-02 05:48:24
kube-state-metrics,ghodss,https://github.com/kubernetes/kube-state-metrics/issues/7,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/7,Upgrade to kubernetes/client-go,"Currently this project vendors in the core kube project, but it should be upgraded to use the new independent kubernetes Go client at https://github.com/kubernetes/client-go.
",closed,False,2016-09-01 22:59:28,2016-09-14 15:38:48
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/pull/8,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/8,Node metrics,"@ghodss @grobie 

This reworks node metrics in the same manner as deployment metrics.
The node states are broken out into one time series for each possible
conditional state. One constant metric holds textual meta information
about cluster nodes.

The 3 metrics/node state are a bit unusual. Discussed this with @brian-brazil. There are different solutions with their pros and cons. The one chosen here appears to be the most robust against future changes to the possible conditions and confusing results when trying to aggregate.

There are actually more fields in the API's node information block. But I think obscure ""system"", ""boot"", and ""machine"" IDs should be only added after a valid use case has been discovered.
",closed,True,2016-09-02 14:49:52,2016-09-07 08:52:22
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/9,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/9,Container restart metric,"This reworks the container restart metric in the same manner as
deployment metrics. The format remains unchanged to the previous format
as in one metric per container via labels.

@fabxc @ghodss 
",closed,True,2016-09-05 13:52:48,2016-09-07 08:20:31
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/10,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/10,update and extend metrics documentation,"We have done some renaming and additions to the metrics, so updating the docs and extending it with metrics type.

@fabxc @ghodss
",closed,True,2016-09-07 09:15:23,2016-09-07 13:51:23
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/pull/11,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/11,Add more deployment metrics,"@brancz @kubernetes/sig-instrumentation 

This adds deployment status and spec metrics. Existing status metric names were changed to contain `_status` to resolve ambiguity and be generally easier to understand.
",closed,True,2016-09-08 13:52:59,2016-09-09 08:58:09
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/pull/12,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/12,Add more node metrics,"@brancz 
",closed,True,2016-09-09 08:30:04,2016-09-09 09:00:27
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/13,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/13,add more pod metrics,"@fabxc
",closed,True,2016-09-12 14:35:00,2016-09-12 16:03:04
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/14,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/14,prefix metrics with `kube` and fix deployment docs,"@fabxc this should be the last thing missing before we can do a release 🎉 
",closed,True,2016-09-12 16:49:31,2016-09-12 16:52:12
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/15,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/15,the `--dry-run` parameter was recently removed,"@fabxc 
",closed,True,2016-09-13 07:43:13,2016-09-13 08:54:15
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/16,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/16,cleanup unused code,"these were artifacts from the previous approach where the metrics were collected periodically

@fabxc
",closed,True,2016-09-13 09:20:37,2016-09-13 09:58:24
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/17,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/17,use k8s.io/client-go,"Closes #7 @ghodss @fabxc 
",closed,True,2016-09-14 08:26:38,2016-09-14 15:38:48
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/18,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/18,add CHANGELOG.md,"@fabxc
",closed,True,2016-09-14 15:58:17,2016-09-14 16:01:13
kube-state-metrics,rvrignaud,https://github.com/kubernetes/kube-state-metrics/issues/19,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/19,Difference between node_status_capacity_memory and node_status_allocateable_memory,"Hello,

With 0.2.0 release, it seems that kube_node_status_allocateable_memory_bytes and kube_node_status_capacity_memory_bytes report same values for nodes (with pod scheduled with required memory).

I'm looking to get available or reserved memory per node. Am'I missing something ?
",closed,False,2016-09-15 08:29:06,2017-03-14 06:06:04
kube-state-metrics,ahakanbaba,https://github.com/kubernetes/kube-state-metrics/issues/20,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/20,kube-state-metrics process generates uninteresting metrics about itself with go_* prefix,"If you look at the output of the curl (mentioned in the README.md ) the start of the output has these: 

go_gc_duration_seconds{quantile=""0""} 6.1791e-05
go_gc_duration_seconds{quantile=""0.25""} 6.3312e-05
go_gc_duration_seconds{quantile=""0.5""} 0.000103029
go_gc_duration_seconds{quantile=""0.75""} 0.000126511
go_gc_duration_seconds{quantile=""1""} 0.000670194
go_gc_duration_seconds_sum 0.001212694
go_gc_duration_seconds_count 7
//  HELP go_goroutines Number of goroutines that currently exist.
// TYPE go_goroutines gauge
go_goroutines 23

As far as I understand these are go framework metrics about the kube-state-metrics process, which are quite uninteresting. 

The reason of that should be the prometheus.UninstrumentedHandler() function call. 
If I am not mistaken that handler by default adds metrics about the process. 

The definition says this: 
// UninstrumentedHandler returns an HTTP handler for the DefaultGatherer.
//
// Deprecated: Use promhttp.Handler instead. See there for further documentation.
func UninstrumentedHandler() http.Handler {

This issue could be resolved by using promHttp handler and separating the metrics that start with go_ from the rest.  (I know this is a vague explanation. After some more investigation,  I can add more details about the proposed impl changes to this issue. ) 

We would like to improve the kube-state-metrics in that regard. 
The primary purpose of this issue is to communicate this to other developers and avoid replicated work. 
",closed,False,2016-09-16 17:14:56,2016-10-05 03:22:44
kube-state-metrics,ahakanbaba,https://github.com/kubernetes/kube-state-metrics/pull/21,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/21,Remove non kube metrics,"In this change we are removing the uninteresting metrics from the output related to the running kube-state-metrics process. They were prefixed with go_\* 

We use a separate registry distinct from the defaultRegistry in the prometheus go client. 
Also use the recent promhttp implementation. 

Since the kube-state-metrics now depends on the prometheus/promhttp package, we also  committed a compatible version of the prometheus/promhttp into the vendor directory. 

Needless to say, all tests in this package still pass with this change. 

@brancz @fabxc 
",closed,True,2016-09-19 03:36:58,2016-10-05 09:16:56
kube-state-metrics,wombat,https://github.com/kubernetes/kube-state-metrics/pull/22,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/22,Added docker image and kubernetes files,"I added a Dockerfile, Kubernetes deployment/service and build instructions
",closed,True,2016-09-19 15:36:33,2016-09-20 09:11:56
kube-state-metrics,gdvalle,https://github.com/kubernetes/kube-state-metrics/pull/23,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/23,Add kube_pod_status_scheduled metric,,closed,True,2016-09-19 16:29:22,2016-09-19 16:40:32
kube-state-metrics,gdvalle,https://github.com/kubernetes/kube-state-metrics/pull/24,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/24,Add test for kube_pod_status_scheduled,"Forgot to include the test in #23.
",closed,True,2016-09-19 17:07:39,2016-09-20 09:10:47
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/pull/25,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/25,Add Makefile,"@brancz 
",closed,True,2016-09-20 09:31:48,2016-09-20 10:28:33
kube-state-metrics,wombat,https://github.com/kubernetes/kube-state-metrics/pull/26,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/26,Update container build instructions,"I removed the docker run command to build the binary and docker image and replaced it with the appropriate make command
",closed,True,2016-09-20 10:43:26,2016-09-20 12:01:30
kube-state-metrics,somejfn,https://github.com/kubernetes/kube-state-metrics/issues/27,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/27,Feature request:  Add more labels to kube_pod_container_status_running,"Thanks to this project I can create Prom alerts on the hosted deployment state.  Big +1 in filling this gap.

Now Im attempting to create an alert based on a deployment's ""dispersion"" ratio.... that is,  should the Kube scheduler had little choice in picking nodes with resources (i.e. node outage),  that a given deployment's replicas are running on the same or too few Kubernetes node.   

I think kube_pod_container_status_running would be the right place... but I'd need to have the deployment name and the node the pod is running on as labels. Then a Prom query would compute a ratio on POD running / unique nodes.  Does that make sense ?  
",closed,False,2016-09-20 12:05:24,2018-02-15 19:42:05
kube-state-metrics,grobie,https://github.com/kubernetes/kube-state-metrics/issues/28,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/28,Panic when showing flags,"I find it unexpected that a simple `kube-state-metrics -h` will cause a stack trace to be printed.
",closed,False,2016-09-22 08:54:06,2016-10-17 13:32:30
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/issues/29,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/29,upgrade to new version of client-go,"client-go has released a new client version, under 1.5

https://github.com/kubernetes/client-go
",closed,False,2016-09-26 08:28:44,2016-10-18 11:49:53
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/30,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/30,use kubernetes informer framework,"Fixes #29 .

@fabxc @ghodss 
",closed,True,2016-09-27 12:21:43,2016-10-18 11:49:53
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/pull/31,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/31,Add official gcr.io image URL,"@brancz @w0mbat
",closed,True,2016-10-04 10:13:12,2016-10-04 12:02:02
kube-state-metrics,lymichaels,https://github.com/kubernetes/kube-state-metrics/issues/32,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/32,Missing deployment metric metadata.generation ,"I'm trying to use the metrics provided by this project but it's missing the metadata.generation metric to check for status of a deployment. It would be great if that could be added in!

Below is from the kubernetes docs about deployments. I'm using that as a guideline to check if deployments fail or not and then send out an alert if it does fail.

> After creating or updating a Deployment, you would want to confirm whether it succeeded or not. The simplest way to do this is through kubectl rollout status.
> 
> This verifies the Deployment’s **.status.observedGeneration >= .metadata.generation**, and its up-to-date replicas (**.status.updatedReplicas**) matches the desired replicas (**.spec.replicas**) to determine if the rollout succeeded. If the rollout is still in progress, it watches for Deployment status changes and prints related messages.
> Note that it’s impossible to know whether a Deployment will ever succeed, so if the above command doesn’t return success, you’ll need to timeout and give up at some point.
> Additionally, if you set **.spec.minReadySeconds**, you would also want to check if the available replicas (**.status.availableReplicas**) matches the desired replicas too.
",closed,False,2016-10-04 15:09:24,2016-10-12 10:37:30
kube-state-metrics,ahakanbaba,https://github.com/kubernetes/kube-state-metrics/issues/33,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/33,Feature Request: Add a new metric for currently used resources in nodes. ,"Hi All,
We would like to propose a new metric addition that describes the used resources by pods in every node. 
Its name could be 
kube_node_current_pod_cpu_requests,
kube_node_requested_cpu_resources,
kube_node_allocated_cpu_resources. 

The names extend naturally for _memory_ 

Or something else suggested by you we are open to suggestions for names. 

As the desired semantics, we propose this metric to contain the sum of requested resources of all the currently running pods in that node.

The same computation is done every time one executes `kubectl describe node`. 
The resources table is computed every time from scratch. 
For example

```
 Namespace          Name                        CPU Requests    CPU Limits  Memory Requests Memory Limits
  ---------         ----                        ------------    ----------  --------------- -------------
  default           k8s-master-127.0.0.1                0 (0%)      0 (0%)      0 (0%)      0 (0%)
  default           k8s-proxy-127.0.0.1             0 (0%)      0 (0%)      0 (0%)      0 (0%)
  hello-world-hbaba-build   hello-world-hbaba-989929512-6jiaf       800m (20%)  20 (500%)   100Mi (2%)  100Mi (2%)
  hello-world-hbaba-build   hello-world-hbaba-989929512-8hq9v       800m (20%)  20 (500%)   100Mi (2%)  100Mi (2%)
  hello-world-hbaba-build   hello-world-hbaba-989929512-jjd5n       800m (20%)  20 (500%)   100Mi (2%)  100Mi (2%)
  hello-world-hbaba-build   hello-world-hbaba-989929512-n00t2       800m (20%)  20 (500%)   100Mi (2%)  100Mi (2%)
  kube-system           kube-dns-v10-z4ics              310m (7%)   310m (7%)   170Mi (4%)  170Mi (4%)
Allocated resources:
  (Total limits may be over 100%, i.e., overcommitted. More info: http://releases.k8s.io/HEAD/docs/user-guide/compute-resources.md)
  CPU Requests  CPU Limits  Memory Requests Memory Limits
  ------------  ----------  --------------- -------------
  3510m (87%)   80310m (2007%)  570Mi (14%) 570Mi (14%)
```

The code for that table is in `kubernetes/pkg/kubectl/describe.go` file in `describeNodeResource` function. It iterates over a collection that is accessed like this : 
`nodeNonTerminatedPodsList, err := d.Core().Pods(namespace).List(api.ListOptions{FieldSelector: fieldSelector})`

For the above example the printed metrics would be 

```
kube_node_current_pod_cpu_requests{node=""compute-node01""} 3510
kube_node_current_pod_memory_requests{node=""compute-node01""} 80310
```

Our use case for this metric is the following: 
We would like to understand the ""utilization"" of the nodes and we would like to understand how much resources have been allocated to running pods and how much is left. 
We are interested in this ""currentPodRequests"" information at the node level as well as at the global cluster level. 

We have created this issue to get any reactions to this suggestion. Has there been any similar discussions before? Would a PR adding this metric welcomed ? 

Another alternative would be to extend the NodeStatus type and add a ""currentPodRequests"" field in the NodeStatus type in the kubernetes API. 

We have also created an [issue](https://github.com/kubernetes/kubernetes/issues/34073) about that approach in the kubernetes repo. Since that is an API change we cannot anticipate the reaction it is going to get .

We propose that it is valuable to add such a currentPodRequests metric for nodes into kube-state-metrics. 

Thanks
",closed,False,2016-10-06 17:07:41,2019-03-29 09:40:55
kube-state-metrics,ahakanbaba,https://github.com/kubernetes/kube-state-metrics/pull/34,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/34,Addition of per node requested cpu and memory resources.,"This change relates to issue #33.
We added a new Collector that uses both the nodeList and podList.
For each node, the collector finds the relevant pods and sums their
requested resources.
This is very similar to the `kubectl describe node` implementation.

In this change automated tests are missing. I will follow up with
another commit for the tests.

I wanted to create this commit early to get responses and feedback as
soon as possible.
",closed,True,2016-10-07 01:05:33,2016-10-08 05:50:34
kube-state-metrics,ahakanbaba,https://github.com/kubernetes/kube-state-metrics/pull/35,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/35,Added metrics describing the cpu and mem requests per container .,"These metrics are retrieved using the v1 api directly.
No additional acucmulation or computation is done.

The state is maintained per container in a pod in kubernetes.
This change exposes the state in kubernetes via  kube-state-metrics

The metrics are also tagged by the node,where  the containers are residing.
Our use case for these metrics are as follows:

We would like to accumulate these per node and understand how much
remaining un-requested resourced are available in each node.

This would give an indication of the ""utilization"" of the cluster to us.
",closed,True,2016-10-08 05:47:26,2016-10-13 18:21:04
kube-state-metrics,shamil,https://github.com/kubernetes/kube-state-metrics/pull/36,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/36,Add deployment metadata.generation metric,"This PR adds deployment `metadata.generation` metric
Fixes #32 
",closed,True,2016-10-11 21:55:41,2016-10-12 10:39:08
kube-state-metrics,rul,https://github.com/kubernetes/kube-state-metrics/pull/37,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/37,Add node unschedulable metric,,closed,True,2016-10-12 03:56:34,2016-10-12 10:13:45
kube-state-metrics,ahakanbaba,https://github.com/kubernetes/kube-state-metrics/issues/38,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/38,"Typo in the metric name ""allocateable""","As far as I know this should be spelled as allocatable. Without an ""e"" between the ""t"" and ""a"".

I apologize if there are alternate spellings of this word. I am by no means an expert in spelling. 

Nevertheless, I see value to be consistent with the [kubernetes repo](https://github.com/kubernetes/kubernetes/blob/master/pkg/api/types.go#L2161) in terms of spelling. 

It also seems like google suggests the allocatable spelling 
https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=allocateable
",closed,False,2016-10-14 18:00:31,2016-10-18 11:49:09
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/39,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/39,fix allocateable -> allocatable,"fixes #38 

@fabxc @ahakanbaba 
",closed,True,2016-10-17 11:31:09,2016-10-18 11:49:10
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/40,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/40,normalize cpu cores unit,"As @fabxc pointed out in [this comment](https://github.com/kubernetes/kube-state-metrics/pull/35#discussion_r83591789) it is best practice to use base units.

@ahakanbaba
",closed,True,2016-10-17 12:36:35,2016-10-17 13:02:19
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/pull/41,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/41,Fix resource fractions,"This fixes the generation of resource metrics to respect milli
precision.

@brancz I realized I introduced this mistake when reviewing your last PR.
",closed,True,2016-10-17 13:06:15,2016-10-17 13:43:05
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/42,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/42,don't panic when requesting help,"fixes #28

@fabxc @grobie
",closed,True,2016-10-17 13:24:14,2016-10-17 13:32:30
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/43,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/43,Cut v0.3.0,"@fabxc @ghodss 

Mostly brought to you by: @gdvalle @ahakanbaba @shamil @rul
",closed,True,2016-10-18 12:04:43,2016-10-18 12:19:22
kube-state-metrics,lymichaels,https://github.com/kubernetes/kube-state-metrics/issues/44,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/44,Release 0.3.0 is not pushed up into the container registry,"It is currently not there.

> docker pull gcr.io/google_containers/kube-state-metrics:v0.3.0
> Pulling repository gcr.io/google_containers/kube-state-metrics
> Tag v0.3.0 not found in repository gcr.io/google_containers/kube-state-metrics
",closed,False,2016-10-18 15:27:30,2016-10-19 08:18:34
kube-state-metrics,sebv,https://github.com/kubernetes/kube-state-metrics/pull/45,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/45,healthcheck metrics,"This does the following:
- adds event watcher (with pod events filtering)
- adds 2 healthcheck related metrics:
  - total number of healthcheck failures
  - seconds since last healthcheck failure

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/45)
<!-- Reviewable:end -->
",closed,True,2016-10-21 09:11:19,2018-02-17 03:13:01
kube-state-metrics,matthughes,https://github.com/kubernetes/kube-state-metrics/issues/46,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/46,Support non-ssl api server,"I am not currently running k8s with TLS on either the api servers or the kubelet.  I was hoping to run the `kube-state-metrics` deployment, but it seems to want to authenticate strictly with service tokens over HTTPS.

I have tried passing in api server directly.

```
docker run -i -t gcr.io/google_containers/kube-state-metrics:v0.3.0 --in-cluster=false --apiserver=http://10.101.0.1:8080
F1025 18:57:59.409874       1 main.go:86] Failed to create client: invalid configuration: no configuration has been provided
goroutine 1 [running]:
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.stacks(0x1cc5700, 0xc400000000, 0x7d, 0xac)
    src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:766 +0xa5
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.(*loggingT).output(0x1ca5220, 0xc400000003, 0xc4200e6c00, 0x1c402df, 0x7, 0x56, 0x0)
    src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:717 +0x337
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.(*loggingT).printf(0x1ca5220, 0x3, 0x1475343, 0x1b, 0xc420665ec8, 0x1, 0x1)
    src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:655 +0x14c
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.Fatalf(0x1475343, 0x1b, 0xc420665ec8, 0x1, 0x1)
    src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.main()
    src/k8s.io/kube-state-metrics/main.go:86 +0x20b
```

It's not very clear to me _what exactly_ is expected in the `--config` flag, but I also provided a valid `./kube/config` file for that API server but the container still complains about invalid configuration.

BTW, the currently `--help` indicates that flags should be in the form of `--flag value`, but IME, you have to specify `--flag=value`.

Is this a worthwhile thing to support or am I fighting upstream by not adding TLS to my api/workers?
",closed,False,2016-10-25 19:02:23,2018-02-18 15:49:00
kube-state-metrics,aabed,https://github.com/kubernetes/kube-state-metrics/issues/47,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/47,Bad certificate ,"I get this error when I start the pod
do I need to change my CA?
or there is a workaround 
` Failed to create client: ERROR communicating with apiserver: Get https://10.254.0.1:443/version: x509: certificate is valid for 10.0.1.5, 10.0.0.1, not 10.254.0.1
goroutine 1 [running]:
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.stacks(0x1cc5700, 0xc400000000, 0xcd, 0x1e8)
        src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:766 +0xa5
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.(*loggingT).output(0x1ca5220, 0xc400000003, 0xc42016cc00, 0x1c402df, 0x7, 0x56, 0x0)
        src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:717 +0x337
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.(*loggingT).printf(0x1ca5220, 0x3, 0x1475343, 0x1b, 0xc420645ec8, 0x1, 0x1)
        src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:655 +0x14c
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.Fatalf(0x1475343, 0x1b, 0xc420645ec8, 0x1, 0x1)
        src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.main()
        src/k8s.io/kube-state-metrics/main.go:86 +0x20b ` ",closed,False,2016-11-07 17:11:07,2016-11-09 17:46:36
kube-state-metrics,dominikschulz,https://github.com/kubernetes/kube-state-metrics/issues/48,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/48,Report CPU and Memory requests and limits,"We are trying to keep an eye on the overall CPU and Memory requests and limits across the cluster.

`kubectl describe nodes` reports this information per node, but this exporter does not seem to expose it.

Example output:

```
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted. More info: http://releases.k8s.io/HEAD/docs/user-guide/compute-resources.md)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
 1095m (54%)	5480m (274%)	786Mi (10%)	3418Mi (45%)
```

As kubectl is reporting this information it is certainly available to this exporter as well.

Is there any way to get these numbers from the already exported metrics or could someone look into exporting them?

IMHO these ""Cluster Usage"" metrics are the most important metrics K8s has, and right now it's a little hard (or at least unclear) how to get them.",closed,False,2016-11-17 06:26:56,2016-11-22 13:05:05
kube-state-metrics,dominikschulz,https://github.com/kubernetes/kube-state-metrics/pull/49,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/49,Export per container limits,Fixes #48,closed,True,2016-11-17 08:46:54,2016-11-22 13:05:05
kube-state-metrics,natalia-k,https://github.com/kubernetes/kube-state-metrics/issues/50,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/50,Feature request: Add ReplicationController and Daemonsets metrics ,"Hello,
Is it possible to add ReplicationController and Daemonsets metrics  as well?
I am using the following metrics to alert  if number of running pods is not equal to replicas define in rc, ds : 
    rc_status_readyReplicas, rc_spec_replicas 
   ds_status_currentNumberScheduled, ds_status_desiredNumberScheduled

I find also, that Kubernetes API don't provide the rc_status_readyReplicas metric at all when it's 0 - how this can be fixed ?
Thanks!",closed,False,2016-11-24 06:51:14,2017-06-28 10:51:13
kube-state-metrics,slaws,https://github.com/kubernetes/kube-state-metrics/pull/51,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/51,Add resource quotas metrics,"Hello,

This is a proposal to add ResourceQuota metrics.
I needed these metrics to monitor quota usage.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/51)
<!-- Reviewable:end -->
",closed,True,2016-11-26 12:00:28,2016-12-15 21:13:28
kube-state-metrics,Nalum,https://github.com/kubernetes/kube-state-metrics/pull/52,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/52,Pod Metric Labels,"I want to add the Node Name to the Pod metric labels as I think it make more sense to have that in there rather than or as well as the Node IP.

I also think given that the Node IP is used elsewhere it would be worth adding it to the NodeInfo metric. I'm willing to do this but haven't started on it yet as I'd like input as to which IP to use. I'll open an issue for discussion around that.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/52)
<!-- Reviewable:end -->
",closed,True,2016-11-26 14:21:21,2016-12-12 21:00:14
kube-state-metrics,Nalum,https://github.com/kubernetes/kube-state-metrics/issues/53,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/53,NodeInfo Metric,"I think it is worth adding the Node IP address to the NodeInfo metric but I'd like feedback on if the public, private or both (or all if there is more than the 2) should be added as labels?

This is the structure I see when I get a node and out put as yaml:
```
status:
  addresses:
  - address: X.X.X.X
    type: InternalIP
  - address: X.X.X.X
    type: ExternalIP
```

I'm happy to create a PR for this if it is something you think would be good to have and if I can get that bit of direction.",closed,False,2016-11-26 14:53:35,2018-02-17 01:11:02
kube-state-metrics,dominikschulz,https://github.com/kubernetes/kube-state-metrics/pull/54,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/54,Add DaemonSet metrics,"This PR adds DaemonSet state metrics.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/54)
<!-- Reviewable:end -->
",closed,True,2016-12-04 11:47:24,2016-12-12 20:55:45
kube-state-metrics,dominikschulz,https://github.com/kubernetes/kube-state-metrics/pull/55,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/55,Add ReplicaSet metrics.,"This PR adds ReplicaSet metrics.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/55)
<!-- Reviewable:end -->
",closed,True,2016-12-04 14:40:17,2017-02-07 18:11:06
kube-state-metrics,dominikschulz,https://github.com/kubernetes/kube-state-metrics/pull/56,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/56,Add ReplicationController metrics,"This PR adds ReplicationController metrics.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/56)
<!-- Reviewable:end -->
",closed,True,2016-12-04 14:40:39,2017-03-07 11:09:50
kube-state-metrics,jackzampolin,https://github.com/kubernetes/kube-state-metrics/issues/57,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/57,[feature-request] Add metrics for Services,Would be great to get network activity for the various services or other service level metrics.,closed,False,2016-12-05 19:27:04,2018-03-01 23:15:50
kube-state-metrics,Crapworks,https://github.com/kubernetes/kube-state-metrics/issues/58,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/58,[feature] Add metrics for v1.ComponentStatusList,"Hi all!

I just stumbled across this project and was doing what you are doing in a much more sloppy way via python. I would like to switch over to kube-state-metric but there is one thing missing: Getting the component statuses. They can be retrieved via [API](http://kubernetes.io/docs/api-reference/v1/definitions/#_v1_componentstatuslist). I was trying to do it myself and create a PR but I have never worked with GO before, so I am afraid I am not much of a help here.

This endpoint basicaly shows the internal monitoring information about the kube-controller-manager, the sheduler and the etcd servers attached. I am mainly using it to create alerts via [Kapacitor](https://github.com/influxdata/kapacitor). Would that be something you could implement?

Cheers,
Christian",closed,False,2016-12-09 11:16:44,2016-12-09 19:23:19
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/59,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/59,Consistently name pod container resource metrics,"As discussed in https://github.com/kubernetes/kube-state-metrics/pull/49#discussion_r88411936 this PR aims to consistently name container resource metrics.

In terms of `resource` vs. `resources` I followed the example of where we have multiple `Containers` in a `Pod` and chose `resource`. Let me know if you think this is ok.

@matthiasr @dominikschulz @fabxc @alexsomesan

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/59)
<!-- Reviewable:end -->
",closed,True,2016-12-12 23:35:46,2016-12-15 21:39:56
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/60,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/60,rename resource_quota to resourcequota,"The convention of using an object kind in the metric name is to use the
same that would be used when using `kubectl`. So in this case `kubectl
get resourcequota`.

As discussed in the metric convention document.

@fabxc @matthiasr

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/60)
<!-- Reviewable:end -->
",closed,True,2016-12-20 14:11:54,2016-12-20 23:20:38
kube-state-metrics,frodenas,https://github.com/kubernetes/kube-state-metrics/issues/61,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/61,kubeconfig flag is ignored when not in cluster,"When using the exporter setting the `--in-cluster` to `false`, the `--kubeconfig` flag is ignored. If your `kubeconfig` file is not at one of the standard locations, then the exporter is unable to contact the apiserver.",closed,False,2016-12-20 21:36:07,2016-12-20 23:13:20
kube-state-metrics,frodenas,https://github.com/kubernetes/kube-state-metrics/pull/62,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/62,Fix loading kubeconfig file,"Fixes #61

* Don't set a default --kubeconfig flag value so NewDefaultClientConfigLoadingRules
will pick up the default locations

* Set an ExplicitPath using --kubeconfig flag value

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/62)
<!-- Reviewable:end -->
",closed,True,2016-12-20 21:37:14,2016-12-20 23:13:20
kube-state-metrics,nvartolomei,https://github.com/kubernetes/kube-state-metrics/issues/63,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/63,StafeulSet metrics?,Is anyone working on these?,closed,False,2016-12-22 11:13:49,2017-08-25 09:05:56
kube-state-metrics,amolsh,https://github.com/kubernetes/kube-state-metrics/issues/64,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/64,"Not able to get ""kube_node_status_phase"" metric","I am running Kubernets on coreos. For monitoring, I am using Prometheus, Grafana and kube-state metrics.
I wanted to check status of nodes in cluster , whether  they are running or not. I thought, I could get info from ""kube_node_status_phase"" metric. But , it is not showing in prometheus. Except this, I am able to see all metrics.
Can anyone help me to sort this out?",closed,False,2016-12-27 11:21:05,2018-01-06 16:39:48
kube-state-metrics,olivierboucher,https://github.com/kubernetes/kube-state-metrics/pull/65,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/65,Added node name to container status queries and adapted tests,"Hi, this is simply a useful addition to the container status gauge metrics. 

Background: I'm using Google Cloud's Stackdriver Monitoring and the entries require a bit more context.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/65)
<!-- Reviewable:end -->
",closed,True,2016-12-30 20:39:15,2017-03-08 10:29:45
kube-state-metrics,dhawal55,https://github.com/kubernetes/kube-state-metrics/pull/66,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/66,Add flag to override resync period,"The hard-coded cache period of 5 mins is too long for us as we want to setup alerts using kube-state-metrics. Adding flag to override the cache/resync period.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/66)
<!-- Reviewable:end -->
",closed,True,2017-01-06 19:28:17,2017-01-10 18:22:50
kube-state-metrics,stvnwrgs,https://github.com/kubernetes/kube-state-metrics/issues/67,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/67,Not able to get kube_pod_container_limits_cpu_cores metric,"I'm not able to see the `kube_pod_container_limits_cpu_cores` metric in prometheus. But i can see other metrics like: `kube_pod_container_requested_cpu_cores`

Setup:
GKE with k8s 1.5.1

Maybe related? #64 ",closed,False,2017-01-09 16:53:24,2017-02-15 08:46:33
kube-state-metrics,koudaiii,https://github.com/kubernetes/kube-state-metrics/pull/68,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/68,"If --in-cluster is false, kubeconfig needs to be valid.","## WHY


```bash
$ docker run -p :8080 gcr.io/google_containers/kube-state-metrics:v0.3.0 --in-cluster=false
F0111 08:01:30.648367       1 main.go:78] --apiserver not set and --in-cluster is false; apiserver must be set to a valid URL
goroutine 1 [running]:
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.stacks(0x1cc5700, 0xc400000000, 0x7e, 0xd3)
	src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:766 +0xa5
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.(*loggingT).output(0x1ca5220, 0xc400000003, 0xc42006acc0, 0x1c402df, 0x7, 0x4e, 0x0)
	src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:717 +0x337
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.(*loggingT).printf(0x1ca5220, 0xc400000003, 0x14a177f, 0x53, 0x0, 0x0, 0x0)
	src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:655 +0x14c
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.Fatalf(0x14a177f, 0x53, 0x0, 0x0, 0x0)
	src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.main()
```

- Add `--apiserver` flag

```bash
$ docker run -p :8080 gcr.io/google_containers/kube-state-metrics:v0.3.0 --in-cluster=false --apiserver=https://xxx.xxx.xxx.xxx
F0111 07:28:31.482691       1 main.go:86] Failed to create client: invalid configuration: no configuration has been provided
goroutine 1 [running]:
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.stacks(0x1cc5700, 0xc400000000, 0x7d, 0xbe)
	src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:766 +0xa5
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.(*loggingT).output(0x1ca5220, 0xc400000003, 0xc42006acc0, 0x1c402df, 0x7, 0x56, 0x0)
	src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:717 +0x337
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.(*loggingT).printf(0x1ca5220, 0x3, 0x1475343, 0x1b, 0xc420689ed8, 0x1, 0x1)
	src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:655 +0x14c
k8s.io/kube-state-metrics/vendor/github.com/golang/glog.Fatalf(0x1475343, 0x1b, 0xc420689ed8, 0x1, 0x1)
	src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.main()
	src/k8s.io/kube-state-metrics/main.go:86 +0x20b
```

ref. https://github.com/kubernetes/kube-state-metrics/issues/46#issuecomment-271647623

> I can confirm that --apiserver=http://... works, without setting --in-cluster=false.


- It can be done by setting arbitrary `--apiserver` and kubeconfig correctly

```bash
$ docker run -v /.kube/config:/.kube/config -p :8080 gcr.io/google_containers/kube-state-metrics:v0.3.0 --in-cluster=false --apiserver=https://xxx.xxx.xxx.xxx
```

or

```bash
$ docker run -v /.kube/config:/.kube/config -p :8080 gcr.io/google_containers/kube-state-metrics:v0.3.0 --in-cluster=false --apiserver=hoo
```

I think If `--in-cluster` is` false`,  `--apiserver` flag not be used. Instead kubeconfig needs to be valid.

## WHAT

if `in-cluster` is `false`,
  - check kubeconfig is valid.
  - delete `--apiserver` check

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/68)
<!-- Reviewable:end -->
",closed,True,2017-01-11 08:47:22,2017-01-11 09:27:16
kube-state-metrics,fate-grand-order,https://github.com/kubernetes/kube-state-metrics/pull/69,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/69,"fix misspell ""with"" in deployment_test.go","

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/69)
<!-- Reviewable:end -->
",closed,True,2017-01-17 07:23:05,2017-01-17 07:25:48
kube-state-metrics,ghost,https://github.com/kubernetes/kube-state-metrics/issues/70,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/70,Should allow aggregation of pod/container metrics by deployment,"Pods and containers export important metrics like health or container restart count. These metrics are most useful when viewed at a deployment aggregation level (i.e., summed over all pods belonging to the same deployment) or on the replica set level. Individual pods are less useful, because a pod might go away for benign reasons.

To do the aggregation, I need labels that reference the deployment. For example, for the standard pod from a deployment named ""foo-12345-fpgj"", I'd need a label ""foo"" that doesn't include the replica set identifier (""12345"") or the pod identifier (""fpgj"").

This bug is for tracking. We're already in touch with the Stackdriver and Kubernetes folks in Google who're hopefully making this happen.",closed,False,2017-01-18 13:08:03,2018-04-08 16:03:51
kube-state-metrics,bluecamel,https://github.com/kubernetes/kube-state-metrics/pull/71,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/71,Add HorizontaPodAutoscaler metrics.,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/71)
<!-- Reviewable:end -->
",closed,True,2017-01-22 07:09:09,2017-12-03 02:05:54
kube-state-metrics,mikekap,https://github.com/kubernetes/kube-state-metrics/issues/72,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/72,[feature-request] Add metrics from CronJobs,"Most important for monitoring: last successful run time. Last run time, duration, overrun indication, etc would be awesome too :).

CronJobs are still in alpha though, so it might be good to wait for them to at least make it to beta.",closed,False,2017-01-22 23:23:49,2017-06-23 07:37:29
kube-state-metrics,therc,https://github.com/kubernetes/kube-state-metrics/issues/73,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/73,Release 0.4,"There are new changes such as Daemonset metrics and per-container limits, but the last release is 0.3, from November. Also, I have code for the Datadog agent to scrape k-s-m for the new data, but quite understandably the maintainers want to see a new official release before accepting my change.

What does it take or who needs to be convinced to make a new release?",closed,False,2017-01-23 03:22:10,2017-02-10 02:19:25
kube-state-metrics,vdavidoff,https://github.com/kubernetes/kube-state-metrics/issues/74,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/74,[feature-request] Deployment Conditions,"That'd be great if kube-state-metrics exposed Deployment Conditions. My use case is to easily know if a Deployment is in progress or not, which I plan to use along with Prometheus and an inhibit rule in AlertManager.

I see Deployment Conditions were added to the go k8s client around git commit ae6775eeec5cc9e96cc5cec848f589158acf7d92, and I have tried to add this support in myself by modifying kube-state-metrics, but I'm running into issues building against the newer go client. This is at least partly because I'm hacking at it right now - I don't really know golang or the process of golang development.

I'm happy to keep trying on this, but it's possible one of you will figure it out before I do.",closed,False,2017-01-26 00:01:12,2017-01-27 15:14:00
kube-state-metrics,vdavidoff,https://github.com/kubernetes/kube-state-metrics/pull/75,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/75,client-go update and new Deployment metric,"* Update to the newest tagged client-go release, v2.0.0-alpha.1
* Expose Deployment RollingUpdate Strategy MaxUnavailable

@brancz showed interest in pulling in a new client-go version as mentioned in [issue 74](https://github.com/kubernetes/kube-state-metrics/issues/74). I chose the newest tag there, even though it's alpha, because the newest non-alpha tag (v1.5.0) is effectively a NOOP between the client-go ref committed to master, and is earlier than the major changes that restructure the directory hierarchy in client-go (the 1.4 and 1.5 subdirectories went away, and 1.5 effectively moved up a level). In other words, it made no sense to update to v1.5.0, and I preferred to pull in something that was at least tagged instead of master.

The new metric exposed is also discussed in [issue 74](https://github.com/kubernetes/kube-state-metrics/issues/74).

I think I got this right but if there are any questions, comments, or criticisms, I'm all ears.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/75)
<!-- Reviewable:end -->
",closed,True,2017-01-27 05:05:18,2017-01-27 08:27:21
kube-state-metrics,vdavidoff,https://github.com/kubernetes/kube-state-metrics/pull/76,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/76,Add kube_deployment_spec_strategy_rollingupdate_max_unavailable,"This documentation update got missed in [PR 75](https://github.com/kubernetes/kube-state-metrics/pull/75).

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/76)
<!-- Reviewable:end -->
",closed,True,2017-01-27 16:04:28,2017-01-27 17:38:10
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/77,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/77,allow choosing which collectors to run,"Extract collector registration into each collector file and improve flag handling and configure glog.

This is backward compatible, but will allow us to potentially disable new collectors by default.

@alexsomesan @fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/77)
<!-- Reviewable:end -->
",closed,True,2017-02-02 10:39:45,2017-02-02 13:29:42
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/78,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/78,add replicaset metrics,"Continuation of #55 rebased and adapted to new registration mechanism.

Thanks @dominikschulz for the head start!

@alexsomesan @fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/78)
<!-- Reviewable:end -->
",closed,True,2017-02-02 13:34:31,2017-02-03 09:39:46
kube-state-metrics,sstarcher,https://github.com/kubernetes/kube-state-metrics/issues/79,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/79,Collect componentstatuses info,It would be nice to collect the health for the componentstatuses objects,closed,False,2017-02-06 14:37:43,2017-12-24 14:35:23
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/80,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/80,CHANGELOG: add changelog entry for v0.4.0 release,"@alexsomesan @fabxc 

/cc @therc 

Closes #73

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/80)
<!-- Reviewable:end -->
",closed,True,2017-02-07 18:52:07,2017-02-10 02:19:25
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/81,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/81,fix make test units with cgo,"`go test -race` can not be run with cgo disabled. This will result `make test-unit` failed.

FYI:
* https://blog.golang.org/race-detector
* https://github.com/golang/go/issues/14481

/cc @brancz @vdavidoff

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/81)
<!-- Reviewable:end -->
",closed,True,2017-02-08 13:07:27,2017-02-15 03:11:33
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/82,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/82,update travis ci conf,"This PR depends on #81 which should be merged first.

* Update travis ci configuration using Makefile
* adding test for golang 1.7 :)

/cc @brancz @vdavidoff

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/82)
<!-- Reviewable:end -->
",closed,True,2017-02-08 13:12:01,2017-02-15 04:19:34
kube-state-metrics,andrewhowdencom,https://github.com/kubernetes/kube-state-metrics/issues/83,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/83,Link to the gcr.io container on readme,"Suggest linking to gcr.io/google_containers/kube-state-metrics:v0.3.0 on README. That's what CoreOS are doing. 

https://github.com/coreos/kube-prometheus/blob/master/manifests/exporters/kube-state-metrics-depl.yaml#L14",closed,False,2017-02-08 17:13:01,2017-02-15 20:27:34
kube-state-metrics,rvrignaud,https://github.com/kubernetes/kube-state-metrics/issues/84,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/84,Panic runtime error for 0.4.0 in GKE 1.5.2,"Hello,

I just tried to upgrade kube-state-metric pod in 0.4.0. I'm running a GKE cluster in 1.5.2.
Kube-state-metrics is running in Ubuntu 16.04 container and built using:
```
apt-get install -y golang git
cd /root
mkdir -p ""$GOPATH""
go get github.com/kubernetes/kube-state-metrics
mv ""${GOPATH}/bin/kube-state-metrics"" /usr/local/bin/
```


Here is the stacktrace I got when I try to get /metric

```
kube-state-metrics
I0210 07:30:30.326926     175 main.go:139] Using default collectors
I0210 07:30:30.327196     175 main.go:186] service account token present: true
I0210 07:30:30.327224     175 main.go:187] service host: https://10.255.240.1:443
I0210 07:30:30.327902     175 main.go:213] Testing communication with server
I0210 07:30:30.359162     175 main.go:218] Communication with server successful
I0210 07:30:30.359497     175 main.go:263] Active collectors: resourcequotas,replicasets,daemonsets,deployments,pods,nodes
I0210 07:30:30.359533     175 main.go:227] Starting metrics server: :80
panic: runtime error: invalid memory address or nil pointer dereference
[signal 0xb code=0x1 addr=0x0 pc=0x402578]
goroutine 81 [running]:
panic(0x18c3a00, 0xc82000a0a0)
	/usr/lib/go-1.6/src/runtime/panic.go:481 +0x3e6
main.(*deploymentCollector).collectDeployment(0xc820456490, 0xc82006e900, 0x0, 0x0, 0x0, 0x0, 0xc820495bc4, 0x6, 0x0, 0x0, ...)
	/root/go/src/github.com/kubernetes/kube-state-metrics/deployment.go:153 +0x238
main.(*deploymentCollector).Collect(0xc820456490, 0xc82006e900)
	/root/go/src/github.com/kubernetes/kube-state-metrics/deployment.go:135 +0x232
github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus.(*Registry).Gather.func2(0xc8204842c0, 0xc82006e900, 0x7f4865a756a8, 0xc820456490)
	/root/go/src/github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus/registry.go:433 +0x58
created by github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus.(*Registry).Gather
	/root/go/src/github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus/registry.go:434 +0x360
```",closed,False,2017-02-10 07:34:58,2017-02-10 23:36:42
kube-state-metrics,rvrignaud,https://github.com/kubernetes/kube-state-metrics/issues/85,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/85,Provide binary for releases ,"Hello,

It would be great if community could provide Linux/amd64 binaries for releases.

Thoughts ?",closed,False,2017-02-10 07:37:08,2018-06-13 10:21:25
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/86,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/86,deployment: skip max unavailable if rolling update is unset,"This previously caused panics when objects did not have the
RollingUpdate struct set.

I also validated, that the tests would now also panic, without the fix.

Closes #84. I'll create a patch release with this once the fix is merged.

@alexsomesan @fabxc 

/cc @rvrignaud @stvnwrgs

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/86)
<!-- Reviewable:end -->
",closed,True,2017-02-10 20:36:51,2017-02-10 21:18:59
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/87,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/87,cut v0.4.1,"@fabxc @alexsomesan @mattriasr 

/cc @rvrignaud @stvnwrgs

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/87)
<!-- Reviewable:end -->
",closed,True,2017-02-10 23:25:58,2017-02-10 23:34:14
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/88,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/88,bump deployment kube-state-metrics version to v0.4.1,"Bump deployment.yml kube-state-metrics version to v0.4.1.

/cc @brancz @fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/88)
<!-- Reviewable:end -->
",closed,True,2017-02-12 08:42:25,2017-02-16 00:03:58
kube-state-metrics,andrewhowdencom,https://github.com/kubernetes/kube-state-metrics/issues/89,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/89,Makefile tip,"You can get the latest git tag in the makefile as follows:

```
# http://stackoverflow.com/questions/1404796/how-to-get-the-latest-tag-name-in-current-branch-in-git
TAG := $(shell git describe --abbrev=0)
```",closed,False,2017-02-13 08:58:33,2017-02-15 05:03:57
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/90,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/90,Makefile: retrieve tag through git,"Before this we had to remember to bump the version in the Makefile (which we forgot in the last two releases), now it will automatically be retrieved through git.

Closes #89 

@fabxc 

/cc @andyxning @andrewhowdencom

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/90)
<!-- Reviewable:end -->
",closed,True,2017-02-15 05:02:43,2017-02-15 05:12:03
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/91,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/91,README: Add container image,"Closes #83 

@fabxc 

/cc @andrewhowdencom

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/91)
<!-- Reviewable:end -->
",closed,True,2017-02-15 17:49:20,2017-02-15 20:27:40
kube-state-metrics,qrpike,https://github.com/kubernetes/kube-state-metrics/pull/92,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/92,health check and resource limits?,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/92)
<!-- Reviewable:end -->
",closed,True,2017-02-22 09:27:18,2017-08-23 14:30:00
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/93,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/93,Add example RBAC ClusterRole,"@fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/93)
<!-- Reviewable:end -->
",closed,True,2017-02-27 09:12:25,2017-03-24 11:58:36
kube-state-metrics,padolan,https://github.com/kubernetes/kube-state-metrics/pull/94,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/94,fix service label to match deployment pod label,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/94)
<!-- Reviewable:end -->
",closed,True,2017-02-28 06:17:48,2017-03-07 11:12:14
kube-state-metrics,rcoh,https://github.com/kubernetes/kube-state-metrics/pull/95,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/95,Fix default port in README,"The default port is actually 8080.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/95)
<!-- Reviewable:end -->
",closed,True,2017-03-02 00:27:38,2017-03-02 07:24:22
kube-state-metrics,dblackdblack,https://github.com/kubernetes/kube-state-metrics/pull/96,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/96,Fix the deployment definition for v0.4.1,"The deployment .yaml for v0.4.1 called out the docker image with tag 0.3.0. You'll probably have to bump version to 0.4.2 in order to fix.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/96)
<!-- Reviewable:end -->
",closed,True,2017-03-03 19:10:17,2017-03-23 12:47:18
kube-state-metrics,sophaskins,https://github.com/kubernetes/kube-state-metrics/issues/97,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/97,Docs for kube_replicaset metrics don't match implementation,"The project [README.md](https://github.com/kubernetes/kube-state-metrics/blob/b7458223c078844dc2f8a2c9b63ed5f1829b4d48/README.md) lists the ReplicaSet metrics as:

- kube_replicaset_status_replicas
- kube_replicaset_status_replicas_available
- kube_replicaset_status_replicas_unavailable
- kube_replicaset_status_replicas_updated
- kube_replicaset_status_replicas_observed_generation
- kube_replicaset_spec_replicas
- kube_replicaset_spec_paused
- kube_replicaset_metadata_generation

but the [implementation](https://github.com/kubernetes/kube-state-metrics/blob/b7458223c078844dc2f8a2c9b63ed5f1829b4d48/replicaset.go) defines:

- kube_replicaset_status_replicas
- kube_replicaset_status_fully_labeled_replicas
- kube_replicaset_status_ready_replicas
- kube_replicaset_status_observed_generation
- kube_replicaset_spec_replicas
- kube_replicaset_metadata_generation

It looks like the documentation is just a duplicate of the metrics for Deployments w/ 'replicaset' replacing 'deployment' - should the docs be updated to match? Is the implementation as desired?",closed,False,2017-03-06 21:45:27,2017-03-07 16:52:47
kube-state-metrics,gigawhat,https://github.com/kubernetes/kube-state-metrics/pull/98,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/98,fix deployment template pod label to match service selector label,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/98)
<!-- Reviewable:end -->
",closed,True,2017-03-06 23:35:07,2017-03-07 08:28:54
kube-state-metrics,gyliu513,https://github.com/kubernetes/kube-state-metrics/issues/99,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/99,The explanation for `kube-state-metrics-vs-heapster` is not accurate,"In https://github.com/kubernetes/kube-state-metrics#kube-state-metrics-vs-heapster , it  claims that `some monitoring systems such as Prometheus do not use Heapster for metric collection at all and instead implement their own`.

I think that this may be not accurate, as actually, I can always use Heapster as a prometheus scrape targets, right? ",closed,False,2017-03-07 05:18:11,2017-03-08 10:28:00
kube-state-metrics,gyliu513,https://github.com/kubernetes/kube-state-metrics/pull/100,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/100,Clarified that prometheus can use heapster as scrape target.,"Fix #99

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/100)
<!-- Reviewable:end -->
",closed,True,2017-03-07 13:04:35,2017-03-08 10:28:41
kube-state-metrics,tmegow,https://github.com/kubernetes/kube-state-metrics/issues/101,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/101,Errors with ReplicationController metrics,"Love this app! It's always worked great with zero issues, thank you.

Today, I excitedly tried out commit b68036d, however I'm getting errors like these in the logs and no rc metrics in /metrics.

```
E0307 14:41:26.060729       1 reflector.go:199] github.com/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.ReplicationController: the server could not find the requested resource
E0307 14:41:27.064053       1 reflector.go:199] github.com/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.ReplicationController: the server could not find the requested resource
E0307 14:41:28.066296       1 reflector.go:199] github.com/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.ReplicationController: the server could not find the requested resource
```",closed,False,2017-03-07 15:00:20,2017-03-09 16:55:11
kube-state-metrics,sophaskins,https://github.com/kubernetes/kube-state-metrics/pull/102,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/102,correct README.md to match metrics defined in replicaset.go,"Fixes https://github.com/kubernetes/kube-state-metrics/issues/97 by bringing README.md in sync with the metrics defined in `replicaset.go`

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/102)
<!-- Reviewable:end -->
",closed,True,2017-03-07 15:34:36,2017-03-07 16:52:47
kube-state-metrics,tmegow,https://github.com/kubernetes/kube-state-metrics/issues/103,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/103,derp,,closed,False,2017-03-08 00:04:14,2017-03-08 00:17:36
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/104,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/104,replicationcontrollers are part of core not extentions,"Fixes #101 .

@fabxc 

/cc @tmegow

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/104)
<!-- Reviewable:end -->
",closed,True,2017-03-09 13:48:09,2017-03-16 08:42:17
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/105,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/105,kube_pod_info add controller info ,"It's necessary for kube_pod_info to keep controller info
/cc @fabxc 
<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/105)
<!-- Reviewable:end -->
",closed,True,2017-03-14 06:12:07,2017-03-27 07:41:12
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/106,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/106,fix rc api namespace,"`replicationcontrollers` resources is under `api/v1` namespace instead of `apis/extensions/v1beta1`. Thus we need to use `CoreV1()`. :)


/ping @dominikschulz @brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/106)
<!-- Reviewable:end -->
",closed,True,2017-03-16 06:26:55,2017-03-16 08:41:48
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/107,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/107,add limit range,"add limit range metrics
@brancz 
cc/ @fabxc 
<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/107)
<!-- Reviewable:end -->
",closed,True,2017-03-20 15:27:59,2017-03-28 01:06:00
kube-state-metrics,SleepyBrett,https://github.com/kubernetes/kube-state-metrics/issues/108,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/108,kube_node_info should add node labels,"We run several node pools and use labels to differentiate them (as well as taints). One node pool in particular is built to be a canary for future versions of our underlying OS. I'd like to be able to build an alert the fires if that node goes away (assumably because some change in the lower env causes it's kubelet to fail). 

Being able to to say sum(kube_node_info{mynodelabel=""canary""}) == 0 would be awesome.",closed,False,2017-03-21 17:12:16,2017-04-28 14:57:24
kube-state-metrics,piosz,https://github.com/kubernetes/kube-state-metrics/issues/109,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/109,Provide more structure in source code,Currently almost everything is in top level directory. It would be great to have some structure here to make the code more readable.,closed,False,2017-03-23 09:10:18,2017-05-08 12:41:21
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/110,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/110,kubernetes: add complete set of manifests,"With kubernetes 1.6 RBAC is enabled by default, so we should make sure that the manifests provided here follow the best practice. Therefore I added a full set of manifests from ServiceAccount, over Roles and Deployment so a user can really just do: `kubectl apply -f kubernetes/` to deploy on 1.6.

This uses the v1alpha1 objects of the RBAC API, which is still present in 1.6 but it likely subject for removal in following releases. But for now they still work, so once there is a stable release of 1.6 we can change these to the v1beta1 objects. For the time being these objects are compatible with 1.5 clusters as well.

@piosz @fabxc @mxinden

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/110)
<!-- Reviewable:end -->
",closed,True,2017-03-24 10:06:32,2017-04-05 09:05:18
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/issues/111,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/111,Add healthz info for kubelet and kube-proxy,"How about we add health check metrics about `apiserver`, `scheduler`, `controller-manager`, `kubelet` and `kube-proxy`. 

/ping @brancz @piosz ",closed,False,2017-03-24 12:47:20,2017-05-09 07:31:32
kube-state-metrics,Resisty,https://github.com/kubernetes/kube-state-metrics/issues/112,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/112,No logs available,"I have kube-state-metrics running as a deployment via ansible on my clusters:
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kube-state-metrics
spec:
  replicas: 4
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      containers:
      - name: kube-state-metrics
        image: gcr.io/google_containers/kube-state-metrics:v0.3.0
        ports:
        - name: metrics
          containerPort: 8080
        resources:
          requests:
            memory: {{ kube_state_mem_req }}
            cpu: 100m
          limits:
            memory: {{ kube_state_mem_lim }}
            cpu: 200m
```
I've had to bump kube_state_mem_(req|lim) to 800Mi in order to get the pods to stay up; the pods have started OOMKilling/CrashLoopBackoff'ing.

I'd like to know why, but the containers are basically inscrutable. There's no way to shell in and `docker logs` is empty.

It'd be great if there was more information on what's going on, please and thanks!
",closed,False,2017-03-24 17:51:21,2017-09-12 21:07:55
kube-state-metrics,maiconbaumx,https://github.com/kubernetes/kube-state-metrics/issues/113,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/113,Error to execute: make container,"My fault, sorry.",closed,False,2017-03-26 01:00:17,2017-03-26 01:40:11
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/114,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/114,add limit range,"add limit range metrics and readme,thanks
@brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/114)
<!-- Reviewable:end -->
",closed,True,2017-03-28 05:02:58,2017-04-05 08:38:40
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/115,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/115,delete back-slant  ,"my mistake!by the way ,do we have a plan to add others metrics(ingress、statefulset、pvc、pv、namespace.etc)?

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/115)
<!-- Reviewable:end -->
",closed,True,2017-03-28 14:43:13,2017-03-29 12:21:00
kube-state-metrics,bjartek,https://github.com/kubernetes/kube-state-metrics/issues/116,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/116,Support for API-groups and other kinds like the openshift objects,"What are the plans regarding supporting other non core
API groups?

We are using OpenShift and are seriously looking into this, but it would be nice to support kinds like DeploymentConfig, BuildConfig and ImageStreams. 

Currently they exist in the oapi API server but it is changing with the introduction of API groups. https://github.com/openshift/origin/pull/12986

I am at kubecon in Berlin so if any of you are here I would love to chat about this. Ping me as @bjartek on twitter or just reply here. ",closed,False,2017-03-29 08:17:21,2017-04-05 15:20:33
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/117,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/117,fix start_time in kube_pod_info,"@brancz hi,I found start_time label of kube_pod_info in readme that is not be implemented.
<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/117)
<!-- Reviewable:end -->
",closed,True,2017-03-29 16:27:13,2017-05-12 06:57:23
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/118,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/118,correct_typos,"Correct some typos about limit ranges. 

@brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/118)
<!-- Reviewable:end -->
",closed,True,2017-04-05 08:57:02,2017-04-05 09:31:35
kube-state-metrics,hkaj,https://github.com/kubernetes/kube-state-metrics/pull/119,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/119,[doc] fix metric names,"Resource and limit metric names are wrong in the README, I ran a sanity check against the code and found a few others.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/119)
<!-- Reviewable:end -->
",closed,True,2017-04-05 18:39:33,2017-04-06 07:58:57
kube-state-metrics,xwinie,https://github.com/kubernetes/kube-state-metrics/issues/120,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/120,the server has asked for the client to provide credentials,"kube-state-metrics ：2017-04-12T01:56:13.362556000Z F0412 01:56:13.361945       1 main.go:73] Failed to create client: ERROR communicating with apiserver: the server has asked for the client to provide credentials
2017-04-12T01:56:13.362761000Z goroutine 1 [running]:
2017-04-12T01:56:13.362917000Z k8s.io/kube-state-metrics/vendor/github.com/golang/glog.stacks(0x234fb00, 0x0, 0x0, 0x0)
2017-04-12T01:56:13.363052000Z 	/usr/local/google/home/pszczesniak/go/src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:766 +0xb8
2017-04-12T01:56:13.363185000Z k8s.io/kube-state-metrics/vendor/github.com/golang/glog.(*loggingT).output(0x232f800, 0xc800000003, 0xc82028aa80, 0x2303db0, 0x7, 0x49, 0x0)
2017-04-12T01:56:13.363307000Z 	/usr/local/google/home/pszczesniak/go/src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:717 +0x259
2017-04-12T01:56:13.363422000Z k8s.io/kube-state-metrics/vendor/github.com/golang/glog.(*loggingT).printf(0x232f800, 0xc800000003, 0x1a0e560, 0x1b, 0xc820555ec0, 0x1, 0x1)
2017-04-12T01:56:13.363540000Z 	/usr/local/google/home/pszczesniak/go/src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:655 +0x1d4
2017-04-12T01:56:13.363669000Z k8s.io/kube-state-metrics/vendor/github.com/golang/glog.Fatalf(0x1a0e560, 0x1b, 0xc820555ec0, 0x1, 0x1)
2017-04-12T01:56:13.363784000Z 	/usr/local/google/home/pszczesniak/go/src/k8s.io/kube-state-metrics/vendor/github.com/golang/glog/glog.go:1145 +0x5d
2017-04-12T01:56:13.363903000Z main.main()
2017-04-12T01:56:13.364022000Z 	/usr/local/google/home/pszczesniak/go/src/k8s.io/kube-state-metrics/main.go:73 +0x271

yaml:
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kube-state-metrics-deployment
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kube-state-metrics
        version: ""v0.3.0""
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8080'
    spec:
      containers:
      - name: kube-state-metrics
        image: registry.cn-shenzhen.aliyuncs.com/kim-docker/kube-state-metrics:v0.3.0
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: ""kubeconfig""
          mountPath: ""/etc/kubernetes/""
          readOnly: true
        args:
          # - --in-cluster=true
          # - --apiserver=https://kubernetes.default.svc
          - --kubeconfig=/etc/kubernetes/kubeconfig
      volumes:
      - name: ""kubeconfig""
        hostPath:
          path: ""/etc/kubernetes/""

kubeconfig:

apiVersion: v1
kind: Config
clusters:
- name: local
  cluster:
    certificate-authority: /etc/kubernetes/ssl/ca.crt
    server: https://10.254.0.1
users:
- name: kubelet
  user:
    client-certificate: /etc/kubernetes/ssl/node.crt
    client-key: /etc/kubernetes/ssl/node.key
contexts:
- context:
    cluster: local
    user: kubelet
  name: kubelet-context
current-context: kubelet-context

",closed,False,2017-04-12 02:02:00,2018-02-21 04:49:00
kube-state-metrics,auhlig,https://github.com/kubernetes/kube-state-metrics/pull/121,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/121,"add NodeMemoryPressure, NodeDiskPressure, NodeNetworkUnavailable","Adding node condition KernelDeadlock as introduced by the [Node Problem Detector](https://github.com/kubernetes/node-problem-detector/blob/master/config/kernel-monitor.json)  and fix [TODO](https://github.com/kubernetes/kube-state-metrics/blob/master/node.go#L174) by adding NodeMemoryPressure,  NodeDiskPressure, NodeNetworkUnavailable.
Comes with tests. Also updated readme. 

**Tests are expected to fail** for now since https://github.com/kubernetes/kubernetes/pull/44099 is still pending. Tested successfully by modifying the respective types.go in the vendor folder.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/121)
<!-- Reviewable:end -->
",closed,True,2017-04-12 12:49:42,2017-04-26 12:34:20
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/issues/122,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/122,[ingress]feature request,"gentle ping  @brancz 
I will work on adding some metrics,here is the relevant information . would you have some suggeustions？
#### service
| Metric name| Metric type | Labels/tags |
| ---------- | ----------- | ----------- |
| kube_service_spec_servicetype | Gauge | `clusterIP `=&lt;cluster ip&gt; <br> `namespace`=&lt;service-namespace&gt; <br>`service`=&lt;service-name&gt;<br>`type`=&lt;ClusterIP\|NodePort\|None\&gt;
#### pv
| Metric name| Metric type | Labels/tags |
| ---------- | ----------- | ----------- |
| kube_persistentvolume_status | Gauge | `persistentvolume`=&lt;pv-name&gt; <br> `namespace`=&lt;pvc-namespace&gt; <br>`phase`=&lt;Bound\|Failed\|Pending\|Available\|Released&gt;<br>`volume`=&lt;pvc-namespace&gt;

 #### namespace
| Metric name| Metric type | Labels/tags |
| ---------- | ----------- | ----------- |
| kube_namespace_status_phase | Gauge | `name`=&lt;namespace-name&gt; <br> `namespace`=&lt;pvc-namespace&gt; <br>`phase`=&lt;Terminating\|Active&gt;<br>`create_time`=&lt;date-time the server time when this object was created&gt;  


#### ingress
| Metric name| Metric type | Labels/tags |
| ---------- | ----------- | ----------- |
| kube_ingress_info | Gauge | `name`=&lt;ingress-name&gt; <br> `namespace`=&lt;ingress-namespace&gt; <br>`create_time`=&lt;date-time the server time when this object was created&gt;
| kube_ingress_metadata-generation | Gauge | `name`=&lt;ingress-name&gt; <br> `namespace`=&lt;ingress-namespace&gt;
|kube_ingress_loadbalancer|Gauge|`name`=&lt;ingress-name&gt; <br> `namespace`=&lt;ingress-namespace&gt;<br>`IP`=&lt;IP set for loadbalancer&gt; <br>`hostname`=&lt;loadbalancer hostname based on dns&gt;

",closed,False,2017-04-12 14:47:46,2018-01-15 22:25:01
kube-state-metrics,cemo,https://github.com/kubernetes/kube-state-metrics/issues/123,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/123,Grafana dashboard,I think that it might be really cool to have a dashboard using this project.,closed,False,2017-04-16 08:15:20,2018-06-11 15:39:23
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/issues/124,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/124,1.0 stabilization,"As discussed in the last SIG instrumentation meeting, we plan to do a first stable release of kube-state-metrics.  
As we have been mostly adding functionality for a while, rather than changing existing one, there's nothing fundamental to change here.

* [x] double-check all existing metrics for compliance with our [guidelines](https://github.com/kubernetes/community/blob/master/contributors/devel/instrumentation.md)
* [x] double-check current functionality does not conflict with future plans of fetching partial metrics, i.e. only for pod metrics for certrain deployments
* [x] load test kube-state-metrics to ensure it scales with large clusters and derive resource requirements (@piosz, can you help with that?)
* [x] provide deployment manifest that scales with cluster size using [pod nanny](https://github.com/kubernetes/contrib/tree/master/addon-resizer)",closed,False,2017-04-18 05:53:02,2017-09-13 02:52:26
kube-state-metrics,yannrouillard,https://github.com/kubernetes/kube-state-metrics/issues/125,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/125,Add support for ready metrics for daemonset,"Currently, the metrics exported for daemonset doesn't allow to detect that a damonset pod is not ready.
There is not equivalent of kube_replicaset_status_ready_replicas.

Would it be possible to add support for a similar metric for daemonset ?

Thanks in advance.

Yann",closed,False,2017-04-21 02:29:13,2017-04-24 09:39:39
kube-state-metrics,nvartolomei,https://github.com/kubernetes/kube-state-metrics/pull/126,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/126,Expose daemonset status number ready gauge,"Closes #125 

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/126)
<!-- Reviewable:end -->
",closed,True,2017-04-22 16:42:01,2017-04-24 10:05:49
kube-state-metrics,lesterwang,https://github.com/kubernetes/kube-state-metrics/issues/127,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/127,Failed to create the client,"When I try to start the kube-state-metrics in Kubernetes(1.3.7), I get the following error:
```
I0425 01:13:00.524520       1 main.go:139] Using default collectors
I0425 01:13:00.525263       1 main.go:186] service account token present: true
I0425 01:13:00.525332       1 main.go:187] service host: https://10.253.0.1:443
I0425 01:13:00.526352       1 main.go:213] Testing communication with server
F0425 01:13:00.550739       1 main.go:156] Failed to create client: ERROR communicating with apiserver: the server has asked for the client to provide credentials
```
Then I delete the rc and svc to create again, the error becomes:
```
I0425 01:30:18.545484       1 main.go:139] Using default collectors
I0425 01:30:18.545707       1 main.go:186] service account token present: true
I0425 01:30:18.545721       1 main.go:187] service host: https://10.253.0.1:443
I0425 01:30:18.546196       1 main.go:213] Testing communication with server
F0425 01:30:18.547422       1 main.go:156] Failed to create client: ERROR communicating with apiserver: Get https://10.253.0.1:443/version: x509: certificate signed by unknown authority
```

The rc file is:
~~~~
apiVersion: v1
kind: ReplicationController
metadata:
  name: kube-state-metrics
  namespace: monitor
  labels:
    name: kube-state-metrics
spec:
  replicas: 1
  selector:
    name: kube-state-metrics
  template:
    metadata:
      labels:
        name: kube-state-metrics
      annotations:
        prometheus.io/scrape: ""true""
    spec:
      containers:
      - name: kube-state-metrics
        image: gcr.io/google_containers/kube-state-metrics:v0.4.1
        ports:
        - containerPort: 8080
      nodeSelector:
        kubernetes.io/hostname: 10.22.96.10
~~~~
The svc file is:
~~~~
apiVersion: v1
kind: Service
metadata:
  name: kube-state-metrics
  namespace: monitor
  labels:
    name: kube-state-metrics
spec:
  type: NodePort
  ports:
  - name: main
    port: 8080
    nodePort: 30880
  selector:
    name: kube-state-metrics
~~~~
Does anyone know this? Thank you in advance",closed,False,2017-04-25 01:41:10,2017-05-08 05:51:52
kube-state-metrics,devth,https://github.com/kubernetes/kube-state-metrics/issues/128,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/128,StatefulSet metrics,Would be useful to have these.,closed,False,2017-04-26 01:59:20,2017-04-26 07:16:41
kube-state-metrics,cemo,https://github.com/kubernetes/kube-state-metrics/issues/129,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/129,Documentation of the metrics,"I can be considered pretty much new to monitoring land. It would be really cool to document metrics with some example usages. I know that some metrics are pretty much obvious from its name but its usage with some other metrics would be beneficial for all. I can also try my best to provide necessary information with my limited knowledge.

Another important thing is that compatibility of metrics with k8s version. Maybe something like `since` might be a good idea.

",closed,False,2017-04-26 19:51:44,2017-07-31 08:33:50
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/130,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/130,Add Pod/Node Labels metric,"Closes #108

@fabxc @mxinden

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/130)
<!-- Reviewable:end -->
",closed,True,2017-04-28 14:00:40,2017-05-09 08:14:55
kube-state-metrics,vdavidoff,https://github.com/kubernetes/kube-state-metrics/pull/131,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/131,Expose Job and CronJob metrics,"This is meant to address https://github.com/kubernetes/kube-state-metrics/issues/72

These changes aren't as baked as I'd normally want them to be before submitting a PR, but I was encouraged to go ahead and submit by @brancz, so here you go. I can confirm that I am able to build and run this code, apparently without any issues.

I am not a golang programmer and admittedly sort of reverse engineered what I have done here. I based both of these new modules (perhaps the wrong term) on deployment.go. Aside from the missing test files, the things I know might need to change are:
* For metics that are statuses like ""failed"", or ""succeeded"", I am just exposing one metric with the value of 0 or 1 (~boolean) rather than exposing each condition twice with a ""condition"" field of ""true"" or ""false"" and a value that indicates if the condition is true. I saw the use of a ""condition"" field in some other metrics, but it just seems unnecessary. I realize there may be a good reason why I should have used a condition field instead.
* I am not exporting ""info"" metrics (e.g. kube_pod_info) because I don't think they're necessary, but I'm not opposed to adding those if I should.
* I used metric type Counter for values that are seconds since the epoch, because that seemed to make more sense to me than gauge, since the value will only ever increase.
* When cycling through Job conditions, the code assumes that it will only ever see a Failed or Complete condition, and if it does, it can safely make assumptions (i.e. if the first condition is Failed, it can assume the succeeded metric should be 0 and failed should be 1). I think this is safe, but I may not understand why it's not.
* I'm using the extractCreatedBy function that's defined in pod.go in my job.go. This feels kind of weird to me since one file's code is reaching into another's without any explicit pointer to the other file, but maybe I am just not used to golang.
* Following existing examples, the name of the Job would be exposed in the ""job"" field, but Prometheus exports its own ""job"" field related to the job that scraped the metrics, which would result in our Job field being named ""exported_job"". To avoid this I named our field ""job_name"".

Please let me know what you'd like to see change here, and I'll happily make those changes.

Thanks.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/131)
<!-- Reviewable:end -->
",closed,True,2017-05-02 12:58:48,2017-06-12 14:08:06
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/132,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/132,*: cut v0.5.0,"It's been almost 3 months since the last release and we have a bunch of new features that were added since the last release, so we should do another release.

@fabxc @mxinden @andyxning 

/cc @piosz (to push the new image once tagged)

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/132)
<!-- Reviewable:end -->
",closed,True,2017-05-03 12:43:25,2017-05-03 15:35:58
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/133,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/133,fix service and limitrange rbac roles and fix order and naming,"I noticed an inconsistency of the name of the LimitRange collector. In contrast to all the other collectors the LimitRange collector was singular. In addition I ordered the collectors in lexicographical order for ease to sync the list of available and default collectors.

In addition to that the default provided RBAC roles did not contain the recently added Services as well as LimitRange resources.

@fabxc @mxinden @andyxning 

This should be merged before #132 so we don't introduce a collector breaking change as the LimitRange collector has not been released yet.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/133)
<!-- Reviewable:end -->
",closed,True,2017-05-03 12:56:39,2017-05-03 14:31:54
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/134,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/134,update image tags,"We'll primarily distribute the kube-state-metrics container image via quay from now on as none of the kube-state-metrics maintainers have the ability to update the image on gcr (but we do for quay) and therefore it creates an unnecessary hassle when releasing.

@fabxc @mxinden @andyxning

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/134)
<!-- Reviewable:end -->
",closed,True,2017-05-03 15:55:12,2017-05-05 15:51:14
kube-state-metrics,jeremyd,https://github.com/kubernetes/kube-state-metrics/issues/135,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/135,docker image not available for v0.5.0,Trying to run the latest deployment.yml in kube and getting image pull errors for gcr.io/google_containers/kube-state-metrics:v0.5.0  not found.,closed,False,2017-05-04 21:26:40,2017-06-23 14:58:07
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/136,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/136,Restructure collectors and documentation,"Closes #109

@fabxc @andyxning @mxinden

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/136)
<!-- Reviewable:end -->
",closed,True,2017-05-05 14:11:01,2017-05-08 13:52:04
kube-state-metrics,andrewklau,https://github.com/kubernetes/kube-state-metrics/issues/137,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/137,Accessing node labels,"Is there plans to support node labels? 

ie. I want to get the cpu requests of all pods and then filter it by nodes that are in the `region=example`

Kubernetes metrics like `container_memory_usage_bytes` provide these labels but not the ones provided by kube-state-metrics",closed,False,2017-05-06 00:09:28,2018-03-20 08:54:47
kube-state-metrics,sheerun,https://github.com/kubernetes/kube-state-metrics/issues/138,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/138,Wrong namespace reported,"Example metric:


kube_deployment_spec_replicas{deployment=""segmenter"",endpoint=""http-metrics"",exported_namespace=""segmenter"",instance=""10.244.0.87:8080"",job=""kube-state-metrics"",namespace=""default"",pod=""kube-state-metrics-412796371-tpbv1"",pod_app=""kube-state-metrics"",service=""kube-state-metrics"",svc_app=""kube-state-metrics"",svc_k8s_app=""kube-state-metrics""}


The namespace should be ""segmenter"" instead. For some reason it reports ""default"" what breaks grafana dashboard.",closed,False,2017-05-06 19:08:32,2017-05-08 07:47:34
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/issues/139,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/139,kube_pod_info using CreatedByAnnotation or OwnerReference,"is it important to update `kube_pod_info` from `CreatedByAnnotation` to  `OwnerReference`?
@brancz ",closed,False,2017-05-08 09:07:02,2017-05-16 07:36:43
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/140,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/140,enhance doc,"Enhance doc.
* add pod label and node label metrics
* add service metrics

/cc @brancz @fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/140)
<!-- Reviewable:end -->
",closed,True,2017-05-09 09:16:10,2017-05-09 10:25:24
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/141,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/141,correct client version,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/141)
<!-- Reviewable:end -->
",closed,True,2017-05-12 07:03:08,2017-05-12 11:11:26
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/142,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/142,add OwnerReference in kube_pod_info,"fix [#139](https://github.com/kubernetes/kube-state-metrics/issues/139)

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/142)
<!-- Reviewable:end -->
",closed,True,2017-05-13 02:20:00,2017-05-16 07:33:03
kube-state-metrics,robsonpeixoto,https://github.com/kubernetes/kube-state-metrics/issues/143,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/143,Fail to install on GKE,"```
$ git clone git@github.com:kubernetes/kube-state-metrics.git
$ cd kube-state-metrics
$ git checkout v0.5.0
$ kubectl apply -f kubernetes/
deployment ""kube-state-metrics"" created
serviceaccount ""kube-state-metrics"" created
service ""kube-state-metrics"" created
error validating ""kubernetes/kube-state-metrics-cluster-role-binding.yaml"": error validating data: couldn't find type: v1alpha1.ClusterRoleBinding; if you choose to ignore these errors, turn validation off with --validate=false
error validating ""kubernetes/kube-state-metrics-cluster-role.yaml"": error validating data: couldn't find type: v1alpha1.ClusterRole; if you choose to ignore these errors, turn validation off with --validate=false
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T20:33:11Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T20:22:08Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
```",closed,False,2017-05-16 01:28:03,2017-06-12 12:56:32
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/issues/144,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/144,example in each metric doc,"is it suitable to add a column for  example？@andyxning @brancz 
# Pod Metrics

| Metric name| Metric type | Labels/tags | Examples   |
| ---------- | ----------- | ----------- |----------- |
| kube_pod_info | Gauge | `pod`=&lt;pod-name&gt; <br> `namespace`=&lt;pod-namespace&gt; <br> `host_ip`=&lt;host-ip&gt; <br> `pod_ip`=&lt;pod-ip&gt; <br> `start_time`=&lt;date-time since kubelet acknowledged pod&gt; <br> `node`=&lt;node-name&gt;<br> `created_by`=&lt;created_by&gt;  |kube_pod_info{host_ip=""10.142.21.158"",namespace=""kube-system"",pod=""nginx-ingress-controller-49379976-bsw1p"",pod_ip=""10.142.21.158""} 1|",closed,False,2017-05-16 05:59:49,2017-05-17 01:09:58
kube-state-metrics,robsonpeixoto,https://github.com/kubernetes/kube-state-metrics/pull/145,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/145,Fix rbac version to work on GKE,"Fix #143

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/145)
<!-- Reviewable:end -->
",closed,True,2017-05-16 10:41:20,2017-06-13 03:51:30
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/146,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/146,add node summary grafana,"This PR fix #123 .

Currently, i suggest we contains cluster and single dashboards. 
* cluster: Aggregated dashboards for a cluster.
* single: single dashboards for a single resource(e.g. `node` or `pod`).

@brancz @andrewhowdencom @cemo

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/146)
<!-- Reviewable:end -->
",closed,True,2017-05-16 14:37:22,2017-09-14 04:27:03
kube-state-metrics,WIZARD-CXY,https://github.com/kubernetes/kube-state-metrics/issues/147,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/147,Two same metrics exist??,"Hi, I deployed the kube-state-metrics according to the doc and find that there exists two different records for one same metric like kube_pod_container_status_restarts , only instance label is different?

{container=""kube-controller-manager"",instance=""172.20.250.58:8080"",job=""kubernetes-service-endpoints"",k8s_app=""kube-state-metrics"",kubernetes_name=""kube-state-metrics"",kubernetes_namespace=""kube-system"",namespace=""kube-system"",pod=""kube-controller-manager-xs342""}
{container=""kube-controller-manager"",instance=""172.20.27.196:8080"",job=""kubernetes-service-endpoints"",k8s_app=""kube-state-metrics"",kubernetes_name=""kube-state-metrics"",kubernetes_namespace=""kube-system"",namespace=""kube-system"",pod=""kube-controller-manager-xs342""}

Is it for HA purpose or some other purposes? Because I want to set up alert and don't want to get things duplicated?Thanks",closed,False,2017-05-31 08:23:28,2017-06-01 02:53:31
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/148,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/148,update README.md,"update README.md (add service metrics)
@brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/148)
<!-- Reviewable:end -->
",closed,True,2017-06-13 06:05:31,2017-06-13 06:33:57
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/149,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/149,add namespace metric,"add namespace metric

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/149)
<!-- Reviewable:end -->
",closed,True,2017-06-13 14:30:47,2017-07-17 09:49:59
kube-state-metrics,stijndehaes,https://github.com/kubernetes/kube-state-metrics/pull/150,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/150,Added stateful set metric,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/150)
<!-- Reviewable:end -->
",closed,True,2017-06-18 18:27:41,2017-06-18 18:43:56
kube-state-metrics,stijndehaes,https://github.com/kubernetes/kube-state-metrics/pull/151,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/151,Added StatefulSet metrics,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/151)
<!-- Reviewable:end -->
",closed,True,2017-06-18 18:48:13,2017-06-23 13:08:23
kube-state-metrics,zenman94,https://github.com/kubernetes/kube-state-metrics/issues/152,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/152,Get metrics only for one namespace,"Dear all,

I try to deploy monitoring on a CaaS. I'm limited to one namespace on a Kubernetes cluster and I try to monitor deployments, replicas, respawn in this namespace.
When I try to deploy kube-state-metrics I have a lot of error because it list all cluster objects : 
_User *** cannot list all pods in the cluster_

Is it possible to limit listing activity only for one or some namespace depit of all cluster?

Regards",closed,False,2017-06-22 07:21:36,2017-08-21 15:16:08
kube-state-metrics,nlamirault,https://github.com/kubernetes/kube-state-metrics/pull/153,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/153,Multi arch binaries,"Signed-off-by: Nicolas Lamirault <nicolas.lamirault@gmail.com>

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/153)
<!-- Reviewable:end -->
",closed,True,2017-06-23 13:48:36,2017-09-09 00:40:31
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/154,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/154,collectors/pod: flatten created by label and extract owners,"The created by label previously was in the format ""<kind>/<name>"", this
is now normalized in to a respective label for each.

Additionally it is possible that a Pod has multiple OwnerReferences.
Previously multiple `kube_pod_info` metrics were generated for each
OwnerReference, unnecessarily duplicating all other labels of the
`kube_pod_info` metric.

The owner part has not been released yet, so it's non breaking,
however, the `created_by` label is breaking. This will be noted in
the changelog.

@fabxc @andyxning @mxinden @gouthamve

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/154)
<!-- Reviewable:end -->
",closed,True,2017-06-27 14:43:04,2017-07-28 13:43:54
kube-state-metrics,julia-stripe,https://github.com/kubernetes/kube-state-metrics/pull/155,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/155,Add metric for cronjob scheduling delay,"We're scheduling cronjobs! When they fail to get scheduled onto a node (for instance because the cluster is overloaded), we want to know how far behind they are.

This adds a delay metric for every job that

* isn't suspended
* has a `lastScheduleTime`

& uses the same cronjob library that kube-cron uses to calculate cron job schedule times.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/155)
<!-- Reviewable:end -->
",closed,True,2017-06-29 17:12:28,2017-07-11 12:14:58
kube-state-metrics,ethernetdan,https://github.com/kubernetes/kube-state-metrics/pull/156,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/156,Add provider ID to Node collector,"This PR adds the provider ID of a Node to the labels which the Node collector makes available for scraping. This is useful for joining kube-state-metrics data with cloud provider data.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/156)
<!-- Reviewable:end -->
",closed,True,2017-06-29 19:44:14,2017-06-30 09:26:44
kube-state-metrics,beorn7,https://github.com/kubernetes/kube-state-metrics/pull/157,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/157,Fix a convoluted sentence and a broken link in README.md,"@brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/157)
<!-- Reviewable:end -->
",closed,True,2017-07-03 11:22:17,2017-07-03 11:23:44
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/158,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/158,split deploy configuration,"mkdir deployWithRBAC and deployWithoutRBAC

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/158)
<!-- Reviewable:end -->
",closed,True,2017-07-03 15:28:59,2017-07-31 12:40:55
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/159,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/159,add namespace metric,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/159)
<!-- Reviewable:end -->
",closed,True,2017-07-04 03:02:18,2017-07-05 03:31:06
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/160,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/160,fix namespace test failure,"@brancz Sorry for having replied to you so late, but things have really been hectic around here.
<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/160)
<!-- Reviewable:end -->
",closed,True,2017-07-04 03:08:04,2017-09-12 05:39:42
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/issues/161,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/161,Phase metrics are not following enumeration semantics.,"For pod conditions we introduced enumerations (true, false, unknown). For each possible state, an object exposes a time series. The series for the condition the pod is currently is set to 1, the others to 0.

Now we have new metrics like the pod phase, which broke with that pattern and just expose a single series by flipping the label value to whatever phase we are currently in.
This breaks various handy computations and is inconsistent in general.

In general, we should just add it back. What's making me hesitant still is the series blow up for a relatively small piece of information. Possible phases are: Pending, Running, Succeeded, Failed, Unknown.

@brian-brazil ",closed,False,2017-07-04 09:52:49,2018-05-17 12:42:10
kube-state-metrics,loburm,https://github.com/kubernetes/kube-state-metrics/issues/162,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/162,Decrease number of labels for some metrics,"I'm working on integration of kube-state-metrics with prometheus-to-sd. Recently have met one really big issue related to the metric kube_pod_labels. As I have understood this metric in theory can have unlimited number of labels attached to it:

https://github.com/kubernetes/kube-state-metrics/blob/3bf74507e082607d9ce6d5c935efe6459d67099d/collectors/pod.go#L220

Unfortunately we can't create metrics in the Stackdriver that have more than 10 labels (and I assume that other systems also can't handle big number of labels). Is it a known issue and is there any plans of fixing such behavior?

/c @piosz ",closed,False,2017-07-05 10:04:23,2017-07-05 13:54:13
kube-state-metrics,errordeveloper,https://github.com/kubernetes/kube-state-metrics/issues/163,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/163,"kube_pod_info{start_time=...} documented, but unimplemented",See https://stackoverflow.com/a/44862179/717998.,closed,False,2017-07-05 15:04:06,2017-08-01 08:19:16
kube-state-metrics,loburm,https://github.com/kubernetes/kube-state-metrics/issues/164,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/164,Support of Summary metric type,"Currently prometheus-to-sd supports only Gauge, Counter and Histogram. We still missing support of Summary. Considering that summaries are not aggregatable I think it should map to Gauge, Distribution type in stackdriver.",closed,False,2017-07-06 09:40:39,2017-07-06 09:48:40
kube-state-metrics,loburm,https://github.com/kubernetes/kube-state-metrics/issues/165,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/165,Support metric types with floating point,"If we have Gauge of Counter type then we automatically assign to it int64 value type. Unfortunately there is no value type in the prometheus format, so we need to try to determine it base on the actual metric numbers.",closed,False,2017-07-06 09:47:44,2017-07-06 09:48:58
kube-state-metrics,camilocot,https://github.com/kubernetes/kube-state-metrics/issues/166,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/166,k8s.io/kube-state-metrics/collectors not found,"I'm tryng to download project dependencies but I get the following error for `k8s.io/kube-state-metrics/collectors` package:

```
go get k8s.io/kube-state-metrics/collectors
package k8s.io/kube-state-metrics/collectors: unrecognized import path ""k8s.io/kube-state-metrics/collectors"" (parse https://k8s.io/kube-state-metrics/collectors?go-get=1: no go-import meta tags)
```
",closed,False,2017-07-06 13:42:29,2018-05-30 17:40:39
kube-state-metrics,SleepyBrett,https://github.com/kubernetes/kube-state-metrics/issues/167,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/167,"kube_pod(_container)?_status_*, Add label for reason","Right  now we're running into some issues that are hard to find with the current kube_pod_status_* and kube_container_status_* metrics.

On the pod_status side there are only four possible states (that I have seen) Running, Pending, Succeeded, Failed. One problem is that it's really hard to detect what kubernetes calls CrashLoopBackOff.

It might be nice to expose metrics more in line with what kubectl shows for pods and containers in the Status column of it's display which means digging into the  status.containerStatuses.state.*.reason field. Perhaps exposing this in a reason label.

Thoughts?",closed,False,2017-07-06 16:27:13,2018-03-09 12:12:34
kube-state-metrics,alamaison,https://github.com/kubernetes/kube-state-metrics/issues/168,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/168,0.6 release?,Is it a good time for a 0.6 release?  We use the public docker images and would like to take advantage of the recent StatefulSet support.,closed,False,2017-07-07 09:47:41,2017-08-03 07:44:54
kube-state-metrics,alamaison,https://github.com/kubernetes/kube-state-metrics/issues/169,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/169,Record deployment labels,"How can we filter the deployment metrics such as `kube_deployment_status_replicas_available` by labels we define in the deployment

We label each deployment/service/etc with the project it belongs to.  Each project has its own prometheus, and alerts are defined there.  We would like to be able to alert on replica unavailability, but without the project label being included in the metric, each project would end up alerting on all other projects too.  Is there a way to do this?",closed,False,2017-07-07 10:53:36,2017-07-27 12:13:24
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/pull/170,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/170,*: cut 0.6.0,"@alamaison

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/170)
<!-- Reviewable:end -->
",closed,True,2017-07-11 12:19:25,2017-08-02 09:55:27
kube-state-metrics,julia-stripe,https://github.com/kubernetes/kube-state-metrics/pull/171,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/171,Add kube_cronjob_next_schedule_time metric to documentation,"The new `kube_cronjob_next_schedule_time` metric was missing from the documentation, this adds it.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/171)
<!-- Reviewable:end -->
",closed,True,2017-07-11 14:45:37,2017-07-17 13:44:02
kube-state-metrics,julia-stripe,https://github.com/kubernetes/kube-state-metrics/pull/172,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/172,Change kube_cronjob_status_last_schedule_time from a counter to a gauge,"If I understand correctly `kube_cronjob_status_last_schedule_time` should actually be a gauge (we shouldn't be adding unix timestamps together)

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/172)
<!-- Reviewable:end -->
",closed,True,2017-07-11 14:56:14,2017-07-11 15:08:20
kube-state-metrics,syedashrafulla,https://github.com/kubernetes/kube-state-metrics/issues/173,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/173,"For some sets of metrics, export each set as a single metric.","The advantage of such a compaction of metrics is
- users can aggregate (say, in Stackdriver) rather than relying on a separate ""total"" metric,
- all the interesting metrics can be returned in one fetch rather than multiple fetches,
- the metrics can end up in the same shard for sharded monitoring solutions such as Stackdriver, and
- the metrics are more clearly defined by measurement instead of measurement and descriptors, which makes graphs in Stackdriver clearer.

Here are a few suggestions from the [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics/tree/master/Documentation) documentation:

Metrics | New metric name | New Label | Label Values
-|-|-|-
[DaemonSet](https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/daemonset-metrics.md) | `kube_daemonset_count` | `status` | `{Scheduled, Pending, Misscheduled, Ready}`
[Deployment](https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/deployment-metrics.md) | `kube_deployment_status_replicas` | `status` | `{Available, Unavailable, Updated}`
[Job](https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/job-metrics.md) | `kube_job_count` | `status` | `{Active, Succeeded, Failed}`
[Pod](https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/pod-metrics.md) | `kube_pod_status_phase` | `phase` | `{Pending, Ready, Scheduled, Running, Succeeded, Failed, Unknown}`
[Pod](https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/pod-metrics.md) | `kube_pod_container_info` | `status` | `{Waiting, Running, Ready, Terminated}`
[ReplicaSet](https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/replicaset-metrics.md) | `kube_replicaset_count` | `status` | `{Ready, Fully Labelled}`
[ReplicationController](https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/replicationcontroller-metrics.md) | `kube_replicationcontroller_replica_count` | `status` | `{Ready, Fully Labelled, Available}`

If I've mistakenly assumed two label values are mutually exclusive, using multiple labels still seems better than encoding label values in the metric name for the advantages described above.",closed,False,2017-07-12 15:56:53,2018-03-02 00:16:00
kube-state-metrics,shiranp,https://github.com/kubernetes/kube-state-metrics/issues/174,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/174,Error when building from master,"I don't have much experience with go so maybe i'm missing something.
I needed the StatefulSet collector so I am building from master in a docker image using the following Dockerfile:

    FROM golang:1.7-alpine
    
    ENV GOOS=linux GOARCH=amd64 CGO_ENABLED=0
    
    # Get Software
    RUN apk update \
     && apk add ca-certificates wget git\
     && update-ca-certificates
    RUN wget https://github.com/kubernetes/kube-state-metrics/archive/master.zip
    RUN unzip master.zip
    
    WORKDIR ./kube-state-metrics-master/
    
    # Build binary
    RUN cp -r ./vendor/* /go/src/
    RUN mkdir -p /go/src/k8s.io/kube-state-metrics/collectors/
    RUN cp -r ./collectors/* /go/src/k8s.io/kube-state-metrics/collectors/
    RUN go build -o /kube-state-metrics
    
    # Cleanup
    WORKDIR /
    RUN apk del --purge -f wget git
    
    ENTRYPOINT [""/kube-state-metrics"", ""--port=8080""]
    
* NOTE: I had some trouble with the dependencies so i just copied the contents of vendor directory to $GOPATH/src.

When i run the container and have my prometheus server scrape it (through nodePort service) I get the following error:

    panic: runtime error: invalid memory address or nil pointer dereference
    [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x547000]
    goroutine 187 [running]:
    panic(0x13341e0, 0xc420016060)
            /usr/local/go/src/runtime/panic.go:500 +0x1a1
    k8s.io/kube-state-metrics/collectors.(*statefulSetCollector).collectStatefulSet(0xc4203b6830, 0xc420340780, 0x0, 0x0, 0x0, 0x0, 0xc420275e94, 0x5, 0x0, 0x0, ...)
            /go/src/k8s.io/kube-state-metrics/collectors/statefulset.go:95 +0xa0
    k8s.io/kube-state-metrics/collectors.(*statefulSetCollector).Collect(0xc4203b6830, 0xc420340780)
            /go/src/k8s.io/kube-state-metrics/collectors/statefulset.go:85 +0x10c
    github.com/prometheus/client_golang/prometheus.(*Registry).Gather.func2(0xc4202f5470, 0xc420340780, 0x1be0000, 0xc4203b6830)
            /go/src/github.com/prometheus/client_golang/prometheus/registry.go:433 +0x63
    created by github.com/prometheus/client_golang/prometheus.(*Registry).Gather
            /go/src/github.com/prometheus/client_golang/prometheus/registry.go:434 +0x2b6

Any Suggestions?
Thanks.",closed,False,2017-07-20 12:51:46,2017-08-06 09:01:58
kube-state-metrics,alamaison,https://github.com/kubernetes/kube-state-metrics/pull/175,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/175,Collect labels for deployments and statefulsets.,"It builds but is untested because `go test` just says `?   	k8s.io/kube-state-metrics	[no test files]`.

I'm not a go programmer, so if this is not the correct way to run tests please let me know. 

I'm not sure I understand the significance of the default labels.  Copying how the service collector defines them, I've added `namespace` and the type of object being labelled.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/175)
<!-- Reviewable:end -->
",closed,True,2017-07-24 14:03:17,2017-07-25 09:27:35
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/176,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/176,fix statefulset observedgeneration is nil,"Fix #174

I am not so familiar with statefulset. However, according to the [comment](https://github.com/kubernetes/kube-state-metrics/blob/master/vendor/k8s.io/client-go/pkg/apis/apps/v1beta1/types.go#L87-L95) about `ObservedGeneration` and  [the function in Kubernetes](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/statefulset/stateful_set_utils.go#L351-L362). `ObservedGeneration` is optional and may be `nil`.  Deference a `nil` will panic the app.

We should filter out `ObservedGeneration` when it is `nil`.

/cc @shiranp @brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/176)
<!-- Reviewable:end -->
",closed,True,2017-07-26 15:38:08,2017-07-27 00:32:08
kube-state-metrics,samuelmanzer,https://github.com/kubernetes/kube-state-metrics/pull/177,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/177,add setup instructions to README.md,"I hit [this issue](https://github.com/kubernetes/kube-state-metrics/issues/166) recently as well, and thought that adding this to the README might clarify things.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/177)
<!-- Reviewable:end -->
",closed,True,2017-07-26 21:44:34,2017-07-27 07:27:18
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/178,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/178,add readme for supported kubernetes version,"Fix #129 partially.

Add readme about the supported Kubernetes version for kube-state-metrics.   This makes the supported Kubernetes version info more clear. :)

/cc @brancz @cemo

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/178)
<!-- Reviewable:end -->
",closed,True,2017-07-27 13:34:58,2017-07-31 09:01:21
kube-state-metrics,cofyc,https://github.com/kubernetes/kube-state-metrics/pull/179,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/179,Add PersistentVolumeClaim metrics,"What this PR does / why we need it:

As an admin operator of multi-tenant clusters, I need to monitor PVC bound status. If a PVC failed (for example, in case RBD Provisioner is not working or Ceph cluster is full), I can know it as soon as possible.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/179)
<!-- Reviewable:end -->
",closed,True,2017-07-28 11:33:26,2017-08-01 02:23:30
kube-state-metrics,fabxc,https://github.com/kubernetes/kube-state-metrics/issues/180,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/180,Handle pod status phase correctly,"The [status phase metric](https://github.com/kubernetes/kube-state-metrics/blob/e8e5e144678064d3f4d3ba5bd8e1bbaeea9daf96/collectors/pod.go#L273) for pod's is currently set to 1 for the phase it is in. No series are exposed for for the phases it is not in. That's not the correct approach for enumerations. 

We've to export a series for each possible phase. Phases the pod is not in must be set to 0.
A correct example of this is the [phase metric for nodes](https://github.com/kubernetes/kube-state-metrics/blob/master/collectors/node.go#L232-L236).",closed,False,2017-07-28 13:40:51,2017-07-31 08:38:52
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/181,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/181,collectors: generate a metric per possible phase per pod,"Fixes #180 

@fabxc @mxinden @gouthamve

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/181)
<!-- Reviewable:end -->
",closed,True,2017-07-28 14:25:25,2017-07-31 08:38:54
kube-state-metrics,julia-stripe,https://github.com/kubernetes/kube-state-metrics/pull/182,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/182,Add creation time metrics for all collectors,"We want to track when pods are created so that we can monitor various pod-age-related metrics, like ""are there pods that are > 30 minutes old and stuck in the 'Pending' state"".

While adding a 'pod created time' metric I figured it made sense to add a created time metric for all the other collectors as well since they all have a `CreationTimestamp` that's easy to expose.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/182)
<!-- Reviewable:end -->
",closed,True,2017-07-28 18:31:21,2017-08-20 01:32:27
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/issues/183,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/183,Update client-go to support Kubernetes 1.7,"I think we should update client-go to `4.0.beta.0` to support Kubernetes 1.7 since it has been released.

This should be done before 1.0 GA.

/cc @brancz @fabxc  ",closed,False,2017-07-29 02:19:43,2017-07-31 13:00:11
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/184,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/184,add kube_pod_start_time metric,"Fix #163

This PR add `kube_pod_start_time` metric with the value of pod start time in unix timestamp.

/cc @errordeveloper @fabxc @brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/184)
<!-- Reviewable:end -->
",closed,True,2017-07-29 15:17:02,2017-08-01 09:15:03
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/185,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/185,bump client go to v4.0.0-beta.0 to support kubernetes 1.7,"Fix #183

This PR will bump client-go to `v4.0.0-beta.0` to support Kubernetes 1.7.

/cc @brancz @fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/185)
<!-- Reviewable:end -->
",closed,True,2017-07-30 06:43:50,2017-07-31 13:09:39
kube-state-metrics,WIZARD-CXY,https://github.com/kubernetes/kube-state-metrics/issues/186,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/186,Want to add a new node condition metric(KernelDeadlock),"Hi~
     I am using node-problem-detector (https://github.com/kubernetes/node-problem-detector) for detecting any kernel and docker problems on my node. I notice there already has some node metrics(https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/node-metrics.md). I want to add more conditions like KernelDeadlock node condition into kube-state-metrics. After  a quick code searching, I found it is easy to add it to the existing system. I can mimic node ready condition https://github.com/kubernetes/kube-state-metrics/blob/master/collectors/node.go#L60 to finish the job.For the future, I think it is nice to have more node condition type. Because it is not a default node condition in k8s, I wonder is it ok for me to add this to the kube-state-metrics?",closed,False,2017-07-31 03:32:15,2017-07-31 06:58:42
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/187,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/187,Fix job nil deference,"Fix #174 about job collector.

This PR will fix [the panic](https://github.com/kubernetes/kube-state-metrics/issues/174#issuecomment-318907880) when collecting job metrics.

/cc @brancz @fabxc @shiranp

Btw, i will re-check all the de-reference about pointers when collecting metrics for all collector later.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/187)
<!-- Reviewable:end -->
",closed,True,2017-07-31 05:28:17,2017-07-31 09:04:54
kube-state-metrics,linhdsv,https://github.com/kubernetes/kube-state-metrics/issues/188,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/188,Unable to show metrics on my own services,"![image](https://user-images.githubusercontent.com/19921743/28768056-7407bf38-7600-11e7-831a-efe7ad071f7c.png)
It shown my endpoints in target page but its state always be DOWN. What am I missing here?
BTW I'm using Prometheus-operator.",closed,False,2017-07-31 07:58:15,2017-08-02 08:53:16
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/189,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/189,update metrics document,"update metrics document

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/189)
<!-- Reviewable:end -->
",closed,True,2017-07-31 07:59:59,2017-07-31 09:19:43
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/190,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/190,add check for nil before deference,"This PR will check for pointer values before dereference to avoid panic dereferencing a nil pointer.

/cc @brancz @fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/190)
<!-- Reviewable:end -->
",closed,True,2017-07-31 14:19:53,2017-08-24 09:49:48
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/191,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/191,update client-go version in readme,"Fix #185 

It's my fault not updating client-go version to `v4.0.0-beta.0` in readme in #185. This PR updates the version info about client-go to `v4.0.0-beta.0` in Readme. 

/cc @brancz @fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/191)
<!-- Reviewable:end -->
",closed,True,2017-07-31 14:28:27,2017-08-02 10:36:58
kube-state-metrics,cofyc,https://github.com/kubernetes/kube-state-metrics/issues/192,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/192,kube_service_labels metric has inconsistent label dimensions,"Hi, guys

In recent commit 1f8761d4b0b1047e0d1651bff7be19a2bbd34a4a, prometheus [client_go](https://github.com/prometheus/client_golang) library is updated, it checks inconsistent label dimensions now, `/metrics` will report following errors (test against kubernetes `./hack/local-up-cluster.sh`):

```
# curl -s http://localhost:8081/metrics
An error has occurred during metrics gathering:

collected metric kube_service_labels label:<name:""label_addonmanager_kubernetes_io_mode"" value:""Reconcile"" > label:<name:""label_k8s_app"" value:""kube-dns"" > label:<name:""label_kubernetes_io_cluster_service"" value:""true"" > label:<name:""label_kubernetes_io_name"" value:""KubeDNS"" > label:<name:""namespace"" value:""kube-system"" > label:<name:""service"" value:""kube-dns"" > gauge:<value:1 >  has label dimensions inconsistent with previously collected metrics in the same metric family
```

I checked the previous version of prometheus `client_go` library, found that `checkMetricConsistency` function (which is used to check inconsistent label) is disabled (b12f83db99b1199eaf2e2506343694489d95a6bc). 

Should we change `kube_service_labels` metric to have consistent label dimension or update current prometheus `client_go` library to disable `checkMetricConsistency` functionality?",closed,False,2017-08-01 03:18:37,2017-08-01 09:57:26
kube-state-metrics,cofyc,https://github.com/kubernetes/kube-state-metrics/pull/193,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/193,Add a flexible node condition metric `kube_node_status_condition`,"**What this PR does / why we need it**:

This metric use `type` label to enumerate all conditions contained in
node Status.Conditions array.

Condition is enumerable, and finite. Adding it in label, makes it
extensible. Especially, Kuberentes community already have a project
[node-problem-detector](https://github.com/kubernetes/node-problem-detector) which reports customized condition `KernelDeadlock` for cluster node.

**Special notes for your reviewer**:

If `kube-state-metrics` can expose all types of conditions, it will be very convenient to monitor all possible node conditions, whether they are reported by Kubernetes core or by third-party addons, like`node-problem-detector`.

And if new node conditions are added in [kubernetes](https://github.com/kubernetes/kubernetes) in future, they will be exposed automatically.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/193)
<!-- Reviewable:end -->
",closed,True,2017-08-01 06:19:36,2018-03-12 12:08:48
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/194,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/194,prometheus client: disable metric consistency,"This reverts the client_golang library back to the state that we need and was previously released with `v0.5.0`. #185 updated the deps and I missed to note that we were overriding this manual patch.

Fixes #192

@fabxc @andyxning @matthiasr 

@cofyc as you reported this, could you quickly try this out and report whether it fixes the behavior you were seeing?

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/194)
<!-- Reviewable:end -->
",closed,True,2017-08-01 09:20:23,2017-08-28 18:56:46
kube-state-metrics,rndstr,https://github.com/kubernetes/kube-state-metrics/issues/195,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/195,Allow aggregation of pod metrics by service,"Similar to #70 I'm looking for a way to gather all the pods, but belonging to a service. Since one pod can be selected by multiple services it could be something like `kube_pod_services{pod=…, service=…}`.

Would there be interest for this in `kube-state-metrics`?",closed,False,2017-08-01 13:55:37,2018-03-09 11:11:33
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/196,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/196,README: add resource recommendation,"Based on @matthiasr's [comment](https://github.com/kubernetes/kube-state-metrics/issues/124#issuecomment-318727127).

@fabxc @andyxning

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/196)
<!-- Reviewable:end -->
",closed,True,2017-08-01 15:18:07,2017-08-01 16:34:53
kube-state-metrics,cofyc,https://github.com/kubernetes/kube-state-metrics/pull/197,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/197,Fix label names of `kube_node_status_condition` in docs.,"hi, @brancz 

Forgot to update docs in commit https://github.com/kubernetes/kube-state-metrics/commit/dda7a4f1e25df4d6e3a6a6f018a51b0808fdea91 of https://github.com/kubernetes/kube-state-metrics/pull/193.

It's my bad >_<.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/197)
<!-- Reviewable:end -->
",closed,True,2017-08-01 15:33:15,2018-03-12 12:08:42
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/198,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/198,Cut 1.0 rc1,"I'm inviting everyone to review the changelog to make sure I've not missed anything and thoroughly test this to make sure we have a solid 1.0 release.

@andyxning @fabxc @piosz @matthiasr @loburm 

/cc @ethernetdan @julia-stripe

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/198)
<!-- Reviewable:end -->
",closed,True,2017-08-02 12:32:20,2017-08-02 13:55:35
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/199,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/199,remove incorrect changelog line,"As mentioned in this https://github.com/kubernetes/kube-state-metrics/pull/198#issuecomment-319677172, this line is incorrect.

@andyxning thanks for catching!

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/199)
<!-- Reviewable:end -->
",closed,True,2017-08-02 13:54:56,2017-08-02 14:29:55
kube-state-metrics,WIZARD-CXY,https://github.com/kubernetes/kube-state-metrics/pull/200,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/200,add autoscaler for kube-state-metrics,"For https://github.com/kubernetes/kube-state-metrics/issues/124
(provide deployment manifest that scales with cluster size using pod nanny)
This setup is tested on my gke cluster, now kube-state-metrics pod has two container, kube-state-metrics and pod-nanny
```
➜  kube-state-metrics git:(addautoscaler) ✗ kubectl get po -n kube-system
NAME                                                  READY     STATUS    RESTARTS   AGE
event-exporter-v0.1.4-4272745813-m1sqp                2/2       Running   0          6d
fluentd-gcp-v2.0-290kq                                2/2       Running   0          6d
fluentd-gcp-v2.0-7wmrd                                2/2       Running   0          6d
fluentd-gcp-v2.0-qtr9r                                2/2       Running   0          6d
heapster-v1.4.0-2764992688-s3q6v                      2/2       Running   0          6d
kube-dns-1413379277-387r0                             3/3       Running   0          6d
kube-dns-1413379277-z057j                             3/3       Running   0          6d
kube-dns-autoscaler-3880103346-73r5g                  1/1       Running   0          6d
kube-proxy-gke-cluster-1-default-pool-d10ff02d-cft7   1/1       Running   0          6d
kube-proxy-gke-cluster-1-default-pool-d10ff02d-mmfk   1/1       Running   0          6d
kube-proxy-gke-cluster-1-default-pool-d10ff02d-x4pg   1/1       Running   0          6d
kube-state-metrics-1585740284-w6lnr                   2/2       Running   0          1m
kubernetes-dashboard-1962351010-km5wg                 1/1       Running   0          6d
l7-default-backend-2954409777-rrwkq                   1/1       Running   0          6d
```
The cpu and memory parameters can be adjusted too.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/200)
<!-- Reviewable:end -->
",closed,True,2017-08-03 06:12:54,2017-09-13 02:51:02
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/201,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/201,add namespace cmd arg,"Fix #152

This PR add `namespaces` command line argument to kube-state-metrics. It's value is in the same format as `collectors`. If specified with delimited namespaces, kube-state-metrics will only collect resources in those namespaces.

/cc @brancz @matthiasr

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/201)
<!-- Reviewable:end -->
",closed,True,2017-08-03 12:15:06,2017-08-21 21:48:13
kube-state-metrics,asifdxtreme,https://github.com/kubernetes/kube-state-metrics/pull/202,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/202,Fix Typo in the Readme,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/202)
<!-- Reviewable:end -->
",closed,True,2017-08-03 15:04:40,2017-08-03 15:10:03
kube-state-metrics,asifdxtreme,https://github.com/kubernetes/kube-state-metrics/pull/203,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/203,Split the UT and Build in different Stages,"Split the UT and Build Stages in different jobs of travis CI so that it's easy for user to trace what went wrong in CI

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/203)
<!-- Reviewable:end -->
",closed,True,2017-08-03 15:43:24,2017-08-04 07:41:10
kube-state-metrics,asifdxtreme,https://github.com/kubernetes/kube-state-metrics/pull/204,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/204,Add build status badge in Readme,"Added Travis CI build status badge in Readme

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/204)
<!-- Reviewable:end -->
",closed,True,2017-08-03 15:54:33,2017-08-04 07:38:18
kube-state-metrics,asifdxtreme,https://github.com/kubernetes/kube-state-metrics/pull/205,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/205,Add GoReportCard badge,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/205)
<!-- Reviewable:end -->
",closed,True,2017-08-04 08:34:29,2017-08-04 08:41:33
kube-state-metrics,asifdxtreme,https://github.com/kubernetes/kube-state-metrics/pull/206,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/206,gofmt,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/206)
<!-- Reviewable:end -->
",closed,True,2017-08-04 08:40:55,2017-08-07 08:37:22
kube-state-metrics,samuelmanzer,https://github.com/kubernetes/kube-state-metrics/pull/207,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/207,"Update README.md with ""go get"" instructions","Now that [this PR](https://github.com/kubernetes/k8s.io/pull/71) went through in `k8s.io` repo, `go get` works

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/207)
<!-- Reviewable:end -->
",closed,True,2017-08-06 14:40:30,2017-08-07 08:55:15
kube-state-metrics,simt2,https://github.com/kubernetes/kube-state-metrics/issues/208,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/208,gcr.io/google_containers/kube-state-metrics:v1.0.0-rc.1 is missing PersistentVolumeClaim metrics,"I just deployed `gcr.io/google_containers/kube-state-metrics:v1.0.0-rc.1` in my cluster to use the new PersistentVolumeClaim metrics, but it seems the image does not contain the right kube-state-metrics version or the binary is somehow missing some features: 

```
> kubectl get pod kube-state-metrics-302237901-rdfl4 -o=jsonpath='{.spec.containers[0].image}'
gcr.io/google_containers/kube-state-metrics:v1.0.0-rc.1
```

```
kubectl port-forward kube-state-metrics-302237901-rdfl4 8080:8080
```

```
> curl -s localhost:8080/metrics | grep kube_persistentvolumeclaim_status_phase
[no output]
```
",closed,False,2017-08-07 07:16:37,2017-08-07 07:27:34
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/209,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/209,update comment code,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/209)
<!-- Reviewable:end -->
",closed,True,2017-08-07 07:52:54,2017-08-07 08:56:01
kube-state-metrics,asifdxtreme,https://github.com/kubernetes/kube-state-metrics/pull/210,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/210,Add gofmt check in travis CI,"Add a new stage ""go fmt"" check in travis CI  as per suggestions in https://github.com/kubernetes/kube-state-metrics/pull/206

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/210)
<!-- Reviewable:end -->
",closed,True,2017-08-07 08:30:59,2017-08-07 08:39:06
kube-state-metrics,asifdxtreme,https://github.com/kubernetes/kube-state-metrics/pull/211,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/211,Add godoc reference,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/211)
<!-- Reviewable:end -->
",closed,True,2017-08-07 09:32:53,2017-08-08 15:29:04
kube-state-metrics,unguiculus,https://github.com/kubernetes/kube-state-metrics/issues/212,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/212,Alpha features should not be used by default,"v1.0.0-rc.1 uses alpha features which may not be available for every one. At least, they are not in our case on GKE. It doesn't seem to be do harm, but the log is flooded with this:

```
E0808 12:43:47.637347       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E0808 12:43:48.739642       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E0808 12:43:49.742757       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E0808 12:43:50.745885       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E0808 12:43:51.749028       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E0808 12:43:52.752262       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E0808 12:43:53.755323       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E0808 12:43:54.839768       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E0808 12:43:55.939709       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E0808 12:43:56.943320       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
```

Please make the use of alpha feature configurable. Ideally they should be turned off by default.
",closed,False,2017-08-08 12:49:10,2018-02-24 02:43:09
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/213,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/213,*: cut 1.0,"As we haven't had any bug reports or code changes over the past week, here goes the final 1.0 release.

As I'm a bit short on time today, but wanted to get this out there I already marked the date to be tomorrow as I'm not planing on finishing the release today, but giving everyone the chance to review once more over night/day around the globe.

:tada: It's happening :tada: 

@fabxc @matthiasr @piosz @loburm @andyxning 

I'll synchronize with @loburm tomorrow to make sure we have the container images online at the point where we merge this.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/213)
<!-- Reviewable:end -->
",closed,True,2017-08-08 15:55:40,2017-08-09 16:21:58
kube-state-metrics,unguiculus,https://github.com/kubernetes/kube-state-metrics/pull/214,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/214,Update usage docs with cli args,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/214)
<!-- Reviewable:end -->
",closed,True,2017-08-09 07:02:35,2018-03-09 20:20:33
kube-state-metrics,loburm,https://github.com/kubernetes/kube-state-metrics/pull/215,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/215,Fix push command.,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/215)
<!-- Reviewable:end -->
",closed,True,2017-08-09 14:12:20,2017-08-09 16:18:18
kube-state-metrics,WIZARD-CXY,https://github.com/kubernetes/kube-state-metrics/issues/216,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/216,[feature] add pod gpu resource limit and request monitoring,"Hi~
   Our team use gpu for ml task and we think it is nice to have gpu resource monitoring in kube-state-metrics, Now I find the doc, only to find it doesn't have yet。I'm happy to submit a pr to add this feature",closed,False,2017-08-10 01:41:35,2017-08-21 14:37:33
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/217,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/217,add deploy file with no rbac config,"merge files and add deploy file with no rbac config

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/217)
<!-- Reviewable:end -->
",closed,True,2017-08-10 02:33:08,2017-08-11 00:37:32
kube-state-metrics,WIZARD-CXY,https://github.com/kubernetes/kube-state-metrics/pull/218,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/218,add gpu resource monitoring,"fix #216

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/218)
<!-- Reviewable:end -->
",closed,True,2017-08-10 10:47:45,2018-05-24 14:01:10
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/219,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/219,add simple componentstatus metrics,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/219)
<!-- Reviewable:end -->
",closed,True,2017-08-14 04:55:33,2018-05-10 10:00:18
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/220,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/220,checkout kubernetes cluster supported feature,"https://github.com/kubernetes/kube-state-metrics/issues/212
<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/220)
<!-- Reviewable:end -->
",closed,True,2017-08-14 09:50:09,2018-01-30 07:04:25
kube-state-metrics,crimsonfaith91,https://github.com/kubernetes/kube-state-metrics/issues/221,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/221,kube-state-metrics should not use CreatedByAnnotation,"Based on this [announcement](https://groups.google.com/forum/#!msg/kubernetes-dev/juMOsdaCml0/FwVNJA9uBAAJ), `CreatedByAnnotation` will be deprecated in 1.8 in favor of `ControllerRef`. However, the annotation is still used in this repo [here](https://github.com/kubernetes/kube-state-metrics/blob/master/collectors/pod.go#L195).

As `CreatedByAnnotation` will be deprecated, we may have to remove this [gauge](https://github.com/kubernetes/kube-state-metrics/blob/master/collectors/pod.go#L272) or set it to `<none>`. I wish to ask for feedback regarding removal of the gauge.",closed,False,2017-08-15 00:48:17,2018-01-03 14:32:30
kube-state-metrics,ajrcastro,https://github.com/kubernetes/kube-state-metrics/issues/222,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/222,~,,closed,False,2017-08-15 20:20:15,2017-08-15 20:20:18
kube-state-metrics,crimsonfaith91,https://github.com/kubernetes/kube-state-metrics/pull/223,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/223,remove kube_pod_info gauge,"Addressing https://github.com/kubernetes/kube-state-metrics/issues/221
`kube_pod_info` gauge is not needed anymore as `CreatedByAnnotation` will be deprecated and removed.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/223)
<!-- Reviewable:end -->
",closed,True,2017-08-16 22:13:53,2017-08-17 02:22:43
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/224,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/224,add service type,"https://github.com/kubernetes/kube-state-metrics/issues/122
`kube_service_spec_type{namespace=""default"",service=""test-service"",type=""ClusterIP""}` and `kube_service_info{namespace=""default"",service=""test-service"",clusterIP=""192.168.0.1""}`

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/224)
<!-- Reviewable:end -->
",closed,True,2017-08-17 02:46:10,2017-12-03 15:07:42
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/225,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/225,add ingress metric,"https://github.com/kubernetes/kube-state-metrics/issues/122
<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/225)
<!-- Reviewable:end -->
",closed,True,2017-08-17 04:59:30,2018-08-31 17:26:58
kube-state-metrics,cofyc,https://github.com/kubernetes/kube-state-metrics/pull/226,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/226,Add PVC requested storage metric and storageclass label,"**What this PR does / why we need it**:

With `kube_persistentvolumeclaim_resource_requests_storage` metric, we can monitor users requested capacity. This is like `kube_pod_container_resource_requests_(cpu_cores|memory_bytes)` metrics.

With new label `storageclass` in pvc metrics, we can group metrics by storage class. In practice, a cluster may have multiple storage class for different dynamic volume types. For example, in our multi-tenant cluster, we deployed rbd/nfs storage classes and corresponding provisioners. For monitor/alerting, it's better if we can distinguish metrics by storage class name.

**Special notes for your reviewer**:

Example output:

```
# curl -s http://localhost:80/metrics | grep '^kube_persistent'
kube_persistentvolumeclaim_resource_requests_storage{namespace=""default"",persistentvolumeclaim=""mongo-persistent-storage-mongo-0"",storageclass=""rbd""} 1.073741824e+09
kube_persistentvolumeclaim_resource_requests_storage{namespace=""default"",persistentvolumeclaim=""registry-storage"",storageclass=""rbd""} 2.147483648e+09
kube_persistentvolumeclaim_resource_requests_storage{namespace=""kube-system"",persistentvolumeclaim=""alertmanager-kirk-monitor-db-alertmanager-kirk-monitor-0"",storageclass=""rbd""} 1.073741824e+09
kube_persistentvolumeclaim_resource_requests_storage{namespace=""kube-system"",persistentvolumeclaim=""kirk-monitor-grafana"",storageclass=""rbd""} 1.073741824e+09
kube_persistentvolumeclaim_resource_requests_storage{namespace=""kube-system"",persistentvolumeclaim=""prometheus-kirk-monitor-db-prometheus-kirk-monitor-0"",storageclass=""rbd""} 1.073741824e+09
```

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/226)
<!-- Reviewable:end -->",closed,True,2017-08-17 11:57:55,2018-03-12 12:08:55
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/227,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/227,add pesistent volume metric,"https://github.com/kubernetes/kube-state-metrics/issues/122
add pesistent volume metric

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/227)
<!-- Reviewable:end -->
",closed,True,2017-08-17 14:26:34,2017-08-23 07:12:03
kube-state-metrics,chriwill,https://github.com/kubernetes/kube-state-metrics/pull/228,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/228,Set target operating system and compilation architecture dynamically,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/228)
<!-- Reviewable:end -->
",closed,True,2017-08-18 20:16:46,2017-08-19 17:13:05
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/229,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/229,collectors: run go fmt on all collectors,"After merging #182 CI started failing as we recently introduced a check for that.

@andyxning let's make sure we rebase / ask to rebase all PRs opened after that change.

/cc @fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/229)
<!-- Reviewable:end -->
",closed,True,2017-08-19 17:10:51,2017-08-21 09:25:16
kube-state-metrics,joemiller,https://github.com/kubernetes/kube-state-metrics/pull/230,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/230,implement horizontal pod autoscaler (HPA) metrics,"This picks up the effort in PR #71 with the following changes:

- adds `.Spec.TargetCPU` and `.Status.CurrentCPU` metrics
- adds `kube_hpa_labels` metrics similar to other collectors
- refactored to fit the new code arrangement in ./collectors


<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/230)
<!-- Reviewable:end -->
",closed,True,2017-08-20 15:09:29,2017-10-23 14:41:14
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/231,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/231,add owner info in each workload app,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/231)
<!-- Reviewable:end -->
",closed,True,2017-08-21 06:14:17,2017-12-07 07:58:56
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/232,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/232,[WIP]go fmt correct,"fix `go fmt` errors.https://github.com/kubernetes/kube-state-metrics/issues/233

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/232)
<!-- Reviewable:end -->
",closed,True,2017-08-21 06:24:46,2017-08-21 09:26:15
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/issues/233,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/233,[WIP]go fmt error,"After [that commit](https://github.com/kubernetes/kube-state-metrics/pull/210) merged, now PRs could not passed `Go fmt` TEST!

https://github.com/kubernetes/kube-state-metrics/pull/232<img width=""423"" alt=""2017-08-21 2 26 47"" src=""https://user-images.githubusercontent.com/5785605/29506151-cc3b640c-867c-11e7-930b-bce1cee2c610.png"">

<img width=""588"" alt=""2017-08-21 2 28 32"" src=""https://user-images.githubusercontent.com/5785605/29506340-c22bc8e8-867d-11e7-81da-d0a02f848ff8.png"">


",closed,False,2017-08-21 06:26:32,2017-08-23 09:10:38
kube-state-metrics,chlunde,https://github.com/kubernetes/kube-state-metrics/pull/234,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/234,Add metrics for namespace label and created time,"This allows prometheus reporting based on labels per namespace.

For example, if each namespace has a team label, you can report on the
memory use per team like this:

    sum(sum(container_memory_usage_bytes) by(namespace) * on(namespace)
    group_right() kube_namespace_labels) by (label_team)

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/234)
<!-- Reviewable:end -->
",closed,True,2017-08-22 07:23:04,2017-08-23 09:26:36
kube-state-metrics,chlunde,https://github.com/kubernetes/kube-state-metrics/issues/235,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/235,Missing metrics in documentation,"A few metrics are missing or typoed in the docs.

```bash
grep -hoE '(kube_[^ ]+)' Documentation/* | sort -u > documented_metrics
grep -hoE '(kube_[^{]+){' collectors/*_test.go | sort -u | tr -d '{' > tested_metrics # looking for TYPE is probably better
diff -u0 tested_metrics documented_metrics
```

Gives the following (and two false positives):
```diff
--- tested_metrics      2017-08-22 21:09:53.715925313 +0200
+++ documented_metrics  2017-08-22 21:09:34.960378911 +0200
@@ -1 +0,0 @@
-kube_cronjob_created
@@ -8 +6,0 @@
-kube_daemonset_created
@@ -14,2 +11,0 @@
-kube_deployment_created
-kube_deployment_labels
@@ -26 +21,0 @@
-kube_job_created
@@ -29 +24 @@
-kube_job_spec_active_deadline_seconds
+kube_job_spec_active_dealine_seconds
@@ -38,2 +32,0 @@
-kube_limitrange_created
-kube_node_created
@@ -64 +56,0 @@
-kube_pod_created
@@ -72 +63,0 @@
-kube_replicaset_created
@@ -79 +69,0 @@
-kube_replicationcontroller_created
@@ -88,2 +77,0 @@
-kube_resourcequota_created
-kube_service_created
@@ -92,2 +79,0 @@
-kube_statefulset_created
-kube_statefulset_labels
```",closed,False,2017-08-22 19:15:09,2017-08-25 09:06:59
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/236,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/236,persistent volume metrics,"my mistake! persistent volume
https://github.com/kubernetes/kube-state-metrics/pull/227
 @brancz @andyxning PTAL
<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/236)
<!-- Reviewable:end -->
",closed,True,2017-08-23 07:19:56,2017-12-03 15:58:49
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/237,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/237,add cronjob-metric doc,"add cronjob-metric doc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/237)
<!-- Reviewable:end -->
",closed,True,2017-08-23 08:50:51,2017-08-23 09:23:09
kube-state-metrics,matthiasr,https://github.com/kubernetes/kube-state-metrics/pull/238,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/238,Match deployment resources to recommendation,"Adjust the deployment to match the recommended resources (#196). The baseline resources are set to the basic recommendation for a 100 node cluster.

Pod nanny (#200) does not support ""the baseline includes the first 100 nodes"", so instead set the threshold until it needs to adjust wide. This over-estimates resource needs for intermediate cluster sizes, but I'd rather be on the safe side. @WIZARD-CXY is this sensible?

Fixes #112.

/cc @brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/238)
<!-- Reviewable:end -->
",closed,True,2017-08-23 09:27:50,2017-08-24 07:18:12
kube-state-metrics,smarterclayton,https://github.com/kubernetes/kube-state-metrics/issues/239,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/239,Panic when pods have an owner without controllers,"```
	owners := p.GetOwnerReferences()
	if len(owners) == 0 {
		addGauge(descPodOwner, 1, ""<none>"", ""<none>"", ""<none>"")
	} else {
		for _, owner := range owners {
			addGauge(descPodOwner, 1, owner.Kind, owner.Name, strconv.FormatBool(*owner.Controller))
		}
	}
```

panics when you have an `Owner` without `Controller`.  `Controller` is an optional field - anyone doing garbage collection (which is what owner is for) will be broken with this check.

```
I0823 14:33:36.780982       1 main.go:243] Starting metrics server: :8080
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x122eb23]

goroutine 245 [running]:
k8s.io/kube-state-metrics/collectors.(*podCollector).collectPod(0xc42040dbe0, 0xc429e3acc0, 0x0, 0x0, 0x0, 0x0, 0xc428c28d40, 0xf, 0x0, 0x0, ...)
	/usr/local/google/home/loburm/go/src/k8s.io/kube-state-metrics/collectors/pod.go:279 +0x313
k8s.io/kube-state-metrics/collectors.(*podCollector).Collect(0xc42040dbe0, 0xc429e3acc0)
	/usr/local/google/home/loburm/go/src/k8s.io/kube-state-metrics/collectors/pod.go:214 +0x117
k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus.(*Registry).Gather.func2(0xc427fae8d0, 0xc429e3acc0, 0x1d6dd60, 0xc42040dbe0)
	/usr/local/google/home/loburm/go/src/k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus/registry.go:433 +0x61
created by k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus.(*Registry).Gather
	/usr/local/google/home/loburm/go/src/k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus/registry.go:434 +0x27c
```

The Collector dereference should be guarded.",closed,False,2017-08-23 14:40:11,2017-08-24 09:52:02
kube-state-metrics,smarterclayton,https://github.com/kubernetes/kube-state-metrics/pull/240,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/240,Guard owner.controller dereference (it can be nil),"Fixes #239

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/240)
<!-- Reviewable:end -->
",closed,True,2017-08-23 14:56:02,2017-08-24 07:25:58
kube-state-metrics,chlunde,https://github.com/kubernetes/kube-state-metrics/pull/241,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/241,Add check for missing documentation,"Fixes #235

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/241)
<!-- Reviewable:end -->
",closed,True,2017-08-23 20:18:15,2017-08-25 09:06:59
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/242,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/242,Cut 1.0.1,"Backporting this as it's going to take a bit of time until we release `v1.1.0`, as there are few more things to figure out for that. But to ensure people have an official release and don't get nil panics I wanted to just backport this.

The fix that is being backported is #240, which was originally reported in #239.

@fabxc @andyxning @matthiasr 

I'll coordinate publishing the images with @loburm / @piosz .

/cc @smarterclayton

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/242)
<!-- Reviewable:end -->
",closed,True,2017-08-24 08:52:37,2017-08-25 08:37:05
kube-state-metrics,smarterclayton,https://github.com/kubernetes/kube-state-metrics/issues/243,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/243,kube_pod_status_ready includes completed pods,"The metrics for `kube_pod_status_ready` are probably less useful than they can be because they include completed pods, which are dead.  So all completed pods are always ""unready"", but to an end user looking at them to determine health of the cluster, they're meaningless.

Completed pods are never going to be ready, but when I'm trying to figure out whether a cluster upgrade caused outage on running pods, I want to measure the number of unready pods at the current moment.  I think on the whole, conditions don't apply to completed pods (for the most part).

Conditions on a completed pod:

```
  - lastProbeTime: null
    lastTransitionTime: 2017-08-23T22:10:16Z
    reason: PodCompleted
    status: ""True""
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: 2017-08-23T22:11:29Z
    reason: PodCompleted
    status: ""False""
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: 2017-08-23T22:10:16Z
    status: ""True""
    type: PodScheduled
```

Scheduled might be a counterexample (there are other ones too).

Is the point of a ksm metric to report what the API reports, or to provide operational value?  If operational value, I'd argue that we should filter ready for completed pods.",closed,False,2017-08-24 16:08:03,2018-03-10 18:42:33
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/244,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/244,merge v1.0.1 release back into master,"I tagged and published the release and pushed the image to `quay.io/coreos/kube-state-metrics:v1.0.1`, and asked @loburm to publish the gcr.io image.

@andyxning @fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/244)
<!-- Reviewable:end -->
",closed,True,2017-08-25 09:02:30,2017-09-13 08:02:50
kube-state-metrics,kawych,https://github.com/kubernetes/kube-state-metrics/pull/245,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/245,Fix gcloud push command,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/245)
<!-- Reviewable:end -->
",closed,True,2017-08-28 13:11:42,2017-08-28 13:19:44
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/246,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/246,move default image registry to quay,"This PR will move kube-state-metrics image registry from `gcr.io` to `quay.io`.

This is mainly because there are some times([#224 comment](https://github.com/kubernetes/kube-state-metrics/pull/244#issuecomment-325298896), [#88 comment](https://github.com/kubernetes/kube-state-metrics/pull/88#issuecomment-279228915)) the latest image can not be pushed to gcr.io timely. Users may be delayed to use the latest kube-state-metrics for this reason.  A better way was to use `quay.io` as the docker registry as @brancz should be able to push the latest image when a new kube-state-metrics is released.

/cc @brancz @loburm @fabxc @piosz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/246)
<!-- Reviewable:end -->
",closed,True,2017-08-28 13:36:46,2017-09-07 11:52:27
kube-state-metrics,smarterclayton,https://github.com/kubernetes/kube-state-metrics/issues/247,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/247,"Job start time can be nil, causes panic","https://github.com/openshift/kube-state-metrics/blob/master/collectors/job.go#L181 start time can be nil

```
I0901 18:50:38.616013       1 main.go:254] Starting metrics server: :8080
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x1229fe0]

goroutine 309 [running]:
k8s.io/kube-state-metrics/collectors.(*jobCollector).collectJob(0xc4203844d0, 0xc43b992f00, 0x0, 0x0, 0x0, 0x0, 0xc4240c7960, 0x12, 0x0, 0x0, ...)
	/go/src/k8s.io/kube-state-metrics/collectors/job.go:181 +0x210
k8s.io/kube-state-metrics/collectors.(*jobCollector).Collect(0xc4203844d0, 0xc43b992f00)
	/go/src/k8s.io/kube-state-metrics/collectors/job.go:146 +0x133
k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus.(*Registry).Gather.func2(0xc43be4fa20, 0xc43b992f00, 0x1d6dca0, 0xc4203844d0)
	/go/src/k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus/registry.go:433 +0x61
created by k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus.(*Registry).Gather
	/go/src/k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus/registry.go:434 +0x27c
```",closed,False,2017-09-01 18:57:20,2017-09-02 03:17:46
kube-state-metrics,smarterclayton,https://github.com/kubernetes/kube-state-metrics/pull/248,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/248,Job start time can be nil,"Fixes #247

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/248)
<!-- Reviewable:end -->
",closed,True,2017-09-02 03:03:04,2017-09-02 03:17:46
kube-state-metrics,lgarithm,https://github.com/kubernetes/kube-state-metrics/pull/249,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/249,add nvidia gpu monitoring,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/249)
<!-- Reviewable:end -->
",closed,True,2017-09-04 09:50:20,2017-09-05 08:19:18
kube-state-metrics,andrewhowdencom,https://github.com/kubernetes/kube-state-metrics/issues/250,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/250,Feature request: Create a new `kube_${entity}_annotations` set of metrics,"The use case for this is to append annotations to metrics, such that new slices of the data can be created for review. For example, adding a `sitewards.com/project: ""SITEWARDS""` to a pod would allow querying of the sum of CPU used by the Sitewards project.

This is possible now, but not so elegant. Currently, there are two metrics that expose metadata about a pod:

kube_pod_info
kube_pod_labels

This is useful in combination with the `group` feature of Prometheus to append additional metadata to the metrics. This is demonstrated more fully in the following post:

https://www.weave.works/blog/aggregating-pod-resource-cpu-memory-usage-arbitrary-labels-prometheus/

Kubernetes has two mechanisms that are capable of storing arbitrary key/value data:

- labels
- annotations

Labels are load bearing, in the sense that the core API uses them to make decisions around which traffic should be routed to what application, or how many of a given application should exist. It is perhaps thus not a good idea to overload these labels simply to add arbitrary key / value data, only useful when viewing data exported from the cluster.

Thus, I propose a new metric is created for each monitored entity that exports the annotations associated with a pod.

Annotations are problematic in a couple of ways:

1. They can contain large blogs of data. `kubernetes.io/created-by=${JSON}` includes a large json blob. I would thus suggest they are truncated at some point.
2. The data they store can be unique, and thus create lots of time-series. Programs may store backup information there, or last modified, or a number of other properties. To resolve this, I would suggest adding a CLI argument that limited the annotations filtered by CSV. This may further help with 1.
3. They are arbitrary, and have no character limitations. I would suggest simply running a replace with the invalid characters to `_`.

Such a patch is likely behind my skills as a golang developer.. But this is a problem that I'm facing, and I'm thus documenting it here in case someone is inspired, and would look to implement this.",closed,False,2017-09-05 12:48:33,2018-03-11 00:48:33
kube-state-metrics,atombender,https://github.com/kubernetes/kube-state-metrics/issues/251,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/251,CronJob errors when running on 1.6.x without alpha features enabled,"We're running on 1.6.7 on GKE, without alpha features enabled. This causes kube-state-metrics to fail with this error:

```
kube-state-metrics E0905 18:02:56.748146       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
```
",closed,False,2017-09-05 18:04:42,2017-09-06 10:04:39
kube-state-metrics,Pensu,https://github.com/kubernetes/kube-state-metrics/pull/252,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/252,Making targets for multi-arch,"This PR ensures multi arch builds for kube-state-metrics.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/252)
<!-- Reviewable:end -->
",closed,True,2017-09-06 09:01:06,2017-09-14 06:14:51
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/253,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/253,add pprof,"Add `pprof` support to kube-state-metrics. This will make debug more easily.

/cc @brancz @fabxc

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/253)
<!-- Reviewable:end -->
",closed,True,2017-09-09 07:09:36,2017-09-14 08:43:44
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/254,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/254,add collected resource objects length log,"This PR will add the log about collected resource object number. This may be useful for debug as described in [#122 comment](https://github.com/kubernetes/kube-state-metrics/issues/112#issuecomment-328302552).

/cc @brancz @fabxc 

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/254)
<!-- Reviewable:end -->
",closed,True,2017-09-12 21:04:13,2017-09-13 09:53:54
kube-state-metrics,caarlos0,https://github.com/kubernetes/kube-state-metrics/issues/255,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/255,Report waiting reason as well,"It would be neat to report as well why a container is in `Waiting` state, for example, `ContainerCreating`, etc.

I'll try to pull this off but I thought it would be neat to have the issue as well in case I don't :D ",closed,False,2017-09-14 13:22:59,2017-09-26 14:20:03
kube-state-metrics,caarlos0,https://github.com/kubernetes/kube-state-metrics/pull/256,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/256,Created kube_pod_container_status_waiting_reason,"closes #255

1. not sure if this is the better approach to this
2. not sure if those are the only possible waiting reasons. Couldn't find docs on this, poke into the kubernetes source and added the ones I found :(

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/256)
<!-- Reviewable:end -->
",closed,True,2017-09-14 14:03:29,2017-09-27 11:15:59
kube-state-metrics,jac-stripe,https://github.com/kubernetes/kube-state-metrics/issues/257,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/257,kube-state-metrics consuming too much memory,"kube-state-metrics is using >400mb of RAM. It is also very slow when I query `/metrics`. The kubernetes cluster has 2700 job objects. It seems surprising that this would consume 400mb of RAM for metrics aggregation. Below is a pprof top trace. This is running the latest git revision (d316c013fae8965bfb75bafda9453ca2ef54c48f)
```
(pprof) top
Showing nodes accounting for 526.72MB, 86.90% of 606.14MB total
Dropped 148 nodes (cum <= 3.03MB)
Showing top 10 nodes out of 110
      flat  flat%   sum%        cum   cum%
  195.01MB 32.17% 32.17%   202.01MB 33.33%  github.com/prometheus/client_golang/prometheus.makeLabelPairs
  101.26MB 16.71% 48.88%   148.26MB 24.46%  github.com/prometheus/client_golang/prometheus.(*Registry).Gather
   74.28MB 12.26% 61.13%    74.81MB 12.34%  k8s.io/kube-state-metrics/collectors.RegisterJobCollector.func1                                                                 47MB  7.75% 68.89%       47MB  7.75%  github.com/prometheus/client_golang/prometheus.populateMetric
   27.60MB  4.55% 73.44%    30.60MB  5.05%  k8s.io/client-go/pkg/api/v1.codecSelfer1234.decSliceVolume
   23.01MB  3.80% 77.24%    23.01MB  3.80%  runtime.rawstringtmp
   18.97MB  3.13% 80.37%    19.55MB  3.22%  github.com/golang/protobuf/proto.(*Buffer).EncodeStringBytes
   15.50MB  2.56% 82.92%   217.51MB 35.88%  github.com/prometheus/client_golang/prometheus.NewConstMetric
   13.50MB  2.23% 85.15%    14.02MB  2.31%  runtime.mapassign
   10.58MB  1.74% 86.90%    12.71MB  2.10%  compress/flate.NewWriter
```",open,False,2017-09-15 01:16:09,2019-03-21 18:43:32
kube-state-metrics,txg1550759,https://github.com/kubernetes/kube-state-metrics/issues/258,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/258,cannot find pod's network monitoring,"cannot find pod's network monitoring metrics,What happened? how fix bug?",closed,False,2017-09-15 15:20:41,2018-03-24 11:58:40
kube-state-metrics,drinktee,https://github.com/kubernetes/kube-state-metrics/pull/259,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/259,add go flag to parse glog flags,"It is need to parse glog flags. So we can use  glog flags such as --v=4, --logtostderr=true

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/259)
<!-- Reviewable:end -->
",closed,True,2017-09-15 15:33:31,2017-09-16 20:21:54
kube-state-metrics,jurgenweber,https://github.com/kubernetes/kube-state-metrics/issues/260,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/260,panic after fixing jobs rbac,"Hi

```
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5"", GitCommit:""17d7182a7ccbb167074be7a87f0a68bd00d58d97"", GitTreeState:""clean"", BuildDate:""2017-08-31T09:14:02Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.4"", GitCommit:""793658f2d7ca7f064d2bdf606519f9fe1229c381"", GitTreeState:""clean"", BuildDate:""2017-08-17T08:30:51Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

after upgrading to v1.0.1 from 0.4.1 I found I needed some further rbac perimissions, no problems.

Once I added jobs as below: 

```
- apiGroups:
  - batch
  resources:
  - cronjobs
  - jobs
  verbs:
  - list
  - watch
```

I started receiving:

```
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] panic: runtime error: invalid memory address or nil pointer dereference
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] [signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x11e0172]
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4]
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] goroutine 136 [running]:
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] time.time.Time.Unix(...)
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] /usr/local/google/home/kawych/go-tools/src/k8s.io/kube-state-metrics/collectors/job.go:173
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] k8s.io/kube-state-metrics/collectors.(*jobCollector).collectJob(0xc420394bd0, 0xc4205cdaa0, 0x0, 0x0, 0x0, 0x0, 0xc4215c0c00, 0x1e, 0x0, 0x0, ...)
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] /usr/local/google/home/kawych/go-tools/src/k8s.io/kube-state-metrics/collectors/job.go:173 +0x1f2
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] k8s.io/kube-state-metrics/collectors.(*jobCollector).Collect(0xc420394bd0, 0xc4205cdaa0)
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] /usr/local/google/home/kawych/go-tools/src/k8s.io/kube-state-metrics/collectors/job.go:141 +0x12f
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] /usr/local/google/home/kawych/go-tools/src/k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus/registry.go:433 +0x61
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] created by k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus.(*Registry).Gather
[prometheus-prometheus-kube-state-metrics-1085824839-jvcz4] /usr/local/google/home/kawych/go-tools/src/k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus/registry.go:431 +0x271
```
If I disable the rbac permissions and just leave this
```
[prometheus-prometheus-kube-state-metrics-1085824839-npk9s] E0918 01:29:10.670107       1 reflector.go:201] k8s.io/kube-state-metrics/collectors/job.go:106: Failed to list *v1.Job: User ""system:serviceaccount:devops:prometheus-prometheus-kube-state-metrics"" cannot list jobs.batch at the cluster scope. (get jobs.batch)
```
in the logs, at least kube-state-metrics stops crashing.

Thanks",closed,False,2017-09-18 01:29:40,2017-10-13 08:21:54
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/261,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/261,add Compatibility matrix,"Due to breaking changes between Kubernetes API versions, some features might not work in kube-state-metric ,using compatibility matrix  to mark  supported/unsupported version range

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/261)
<!-- Reviewable:end -->
",closed,True,2017-09-18 06:46:10,2017-10-09 13:25:43
kube-state-metrics,superbrothers,https://github.com/kubernetes/kube-state-metrics/pull/262,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/262,Improve a usage message for collectors flag,"This PR Improves a usage message for collectors flag. It makes a sentence clear and shows a default value.

```
      --collectors string                Comma-separated list of collectors to be enabled. Defaults to ""services,persistentvolumeclaims,nodes,replicationcontrollers,statefulsets,replicasets,resourcequotas,jobs,namespaces,daemonsets,deployments,limitranges,pods,cronjobs""
```

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/262)
<!-- Reviewable:end -->
",closed,True,2017-09-20 07:14:18,2017-09-21 13:27:26
kube-state-metrics,smarterclayton,https://github.com/kubernetes/kube-state-metrics/issues/263,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/263,Difference between a restart=Never and restart=Always is very important for pod states,"Currently pod metrics don't distinguish well between restart Never and Always pods.  Since the two workloads are very, very different, it doesn't seem unreasonable to distinguish them.

`kube_pod_created` really lacks the equivalent in `kube_pod_terminated` - once a pod is terminated, the majority of its function on the cluster is complete.  None of the existing pod metrics represent that in any way.

Example queries that are difficult / impossible:

1. number of pods that would fall under the terminating quota at this time
2. sum of pod resource limits in the terminating quota
3. average runtime of pods per namespace
4. average runtime of pods by resource request",closed,False,2017-09-20 13:51:00,2018-10-01 08:03:06
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/264,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/264,use protobuf instead of json,"client-go defaults to using json as the content type, as suggested in #257 it would be a good idea to use protobuf instead. As currently all objects used in kube-state-metrics support protobuf, this should be safe to do.

@smarterclayton @andyxning

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/264)
<!-- Reviewable:end -->
",closed,True,2017-09-21 14:55:04,2018-06-25 07:17:02
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/issues/265,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/265,multi-version resource metrics supported,"Is it suitable to change the arch of collectors like [typed](https://github.com/kubernetes/client-go/tree/release-5.0/kubernetes/typed),according to the comments of the [PR](https://github.com/kubernetes/kube-state-metrics/pull/220/)
![2017-09-25 9 53 41](https://user-images.githubusercontent.com/5785605/30789475-619afe38-a1d8-11e7-8744-9acb70317597.png)
@andyxning @brancz ",closed,False,2017-09-25 02:00:44,2018-01-23 02:25:22
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/266,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/266,[kubernetes 1.8] update client-go(release-5.0) and add k8s.io/api(rel…,"- [x] update client-go(release-5.0) for kubernetes 1.8

- [x] add k8s.io/api for kubernetes 1.8

- [ ] migrate DaemonSet, Deployment, ReplicaSet, and StatefulSet from v1beta1 to v1beta2

- [ ] migrate cronjob from v1alpha1 to v1beta1


@andyxning  @brancz PTAL!
<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/266)
<!-- Reviewable:end -->
",closed,True,2017-09-27 06:41:16,2017-10-11 14:05:12
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/267,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/267,log the group version for resources,"This PR will log the currently used group version for each resource. This should be user friendly in debugging as discussed in [#256 comment](https://github.com/kubernetes/kube-state-metrics/issues/265#issuecomment-332525436).

/cc @brancz @matthiasr

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/267)
<!-- Reviewable:end -->
",closed,True,2017-09-28 05:11:51,2017-09-30 06:26:20
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/issues/268,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/268,v1.1.0 Release tracking issue,"- [x] Update `client-go` to v5.0.0 release. `client-go` v5.0.0 will support Kubernetes 1.8. With updating to v5.0.0 `client-go`, kube-state-metrics will support Kubernetes 1.8. But `client-go` 
 v5.0.0 has not been released. As discussed with @caesarxuchao offline, v5.0.0 `client-go` should be released in this week. If this is reliable, kube-state-metrics should wait for updating `client-go` but this should be confirmed by @caesarxuchao . #269 @andyxning
- [x] Update `kubernetes` to tag 1.8. #269 @andyxning 
- [ ] ~~add `kube_persistentvolume_status`.~~ #236  @zouyee
- [ ] ~~add ingress metric~~ #225 @zouyee  
- [ ] ~~add service type~~ #224  @zouyee 
- [ ] ~~add simple componentstatus metrics~~ #219 @zouyee 
- [x] add a compatibility description about different group version for resources. As discussed in #265. #271 @andyxning

As for `use protobuf instead of json`(#264) and `checkout kubernetes cluster supported feature`(#220) can not be fulfilled in this week, imo, i suggest to postpone those PRs to next release.

This release schedule heavily depends on @zouyee , could you please take some time to help us to make this.

@brancz @matthiasr ",closed,False,2017-10-09 02:03:55,2017-10-12 10:41:00
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/269,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/269,bump kubernetes to 1.8,"This PR update kubernetes dependencies to 1.8.

/cc @brancz @zouyee

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/269)
<!-- Reviewable:end -->
",closed,True,2017-10-10 02:54:09,2017-10-11 13:04:18
kube-state-metrics,chasemaier,https://github.com/kubernetes/kube-state-metrics/issues/270,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/270,Cannot Create Role / Cluster Role,"I am attempting to apply the contents of the [`kubernetes`](https://github.com/kubernetes/kube-state-metrics/tree/master/kubernetes) directory to a cluster (formed via `kube-aws` on AWS).

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.6"", GitCommit:""7fa1c1756d8bc963f1a389f4a6937dc71f08ada2"", GitTreeState:""clean"", BuildDate:""2017-06-16T18:34:20Z"", GoVersion:""go1.7.6"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.6+coreos.1"", GitCommit:""42a5c8b99c994a51d9ceaed5d0254f177e97d419"", GitTreeState:""clean"", BuildDate:""2017-06-21T01:10:07Z"", GoVersion:""go1.7.6"", Compiler:""gc"", Platform:""linux/amd64""}
```

```
$ kubectl apply -f roles/kube-state-metrics/

clusterrolebinding ""kube-state-metrics"" configured
deployment ""kube-state-metrics"" configured
rolebinding ""kube-state-metrics"" configured
serviceaccount ""kube-state-metrics"" configured
service ""kube-state-metrics"" configured
Error from server (Forbidden): error when creating ""roles/kube-state-metrics/kube-state-metrics-cluster-role.yaml"": clusterroles.rbac.authorization.k8s.io ""kube-state-metrics"" is forbidden: attempt to grant extra privileges: [{[list] [] [nodes] [] []} {[watch] [] [nodes] [] []} {[list] [] [pods] [] []} {[watch] [] [pods] [] []} {[list] [] [services] [] []} {[watch] [] [services] [] []} {[list] [] [resourcequotas] [] []} {[watch] [] [resourcequotas] [] []} {[list] [] [replicationcontrollers] [] []} {[watch] [] [replicationcontrollers] [] []} {[list] [] [limitranges] [] []} {[watch] [] [limitranges] [] []} {[list] [] [persistentvolumeclaims] [] []} {[watch] [] [persistentvolumeclaims] [] []} {[list] [extensions] [daemonsets] [] []} {[watch] [extensions] [daemonsets] [] []} {[list] [extensions] [deployments] [] []} {[watch] [extensions] [deployments] [] []} {[list] [extensions] [replicasets] [] []} {[watch] [extensions] [replicasets] [] []} {[list] [apps] [statefulsets] [] []} {[watch] [apps] [statefulsets] [] []} {[list] [batch] [cronjobs] [] []} {[watch] [batch] [cronjobs] [] []} {[list] [batch] [jobs] [] []} {[watch] [batch] [jobs] [] []}] user=&{kube-admin  [kube-aws system:authenticated] map[]} ownerrules=[] ruleResolutionErrors=[]
Error from server (Forbidden): error when creating ""roles/kube-state-metrics/kube-state-metrics-role.yaml"": roles.rbac.authorization.k8s.io ""kube-state-metrics-resizer"" is forbidden: attempt to grant extra privileges: [{[get] [] [pods] [] []} {[get] [extensions] [deployments] [kube-state-metrics] []} {[update] [extensions] [deployments] [kube-state-metrics] []}] user=&{kube-admin  [kube-aws system:authenticated] map[]} ownerrules=[] ruleResolutionErrors=[]
```

The pods from the defined deployment seem to run despite the roles failing to create, but the roles are not created successfully.

```
$ kubectl logs -f kube-state-metrics-2331359496-ltkvj --namespace=kube-system -c kube-state-metrics
I1010 20:27:09.850349       1 main.go:154] Using default collectors
I1010 20:27:09.850677       1 main.go:202] service account token present: true
I1010 20:27:09.850692       1 main.go:203] service host: https://10.3.0.1:443
I1010 20:27:09.851385       1 main.go:229] Testing communication with server
I1010 20:27:10.052076       1 main.go:234] Communication with server successful
I1010 20:27:10.052328       1 main.go:279] Active collectors: deployments,limitranges,replicationcontrollers,statefulsets,services,cronjobs,persistentvolumeclaims,pods,replicasets,jobs,daemonsets,nodes,resourcequotas
I1010 20:27:10.052343       1 main.go:243] Starting metrics server: :8080
```

Is there another way the roles should be applied to ensure the pods have access to all the available metrics?",closed,False,2017-10-10 20:39:16,2017-12-23 09:47:52
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/271,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/271,update resource group version compatibility,"This PR will update readme in
* update description about latest `client-go` version to `release-5.0`.
* add the deprecation policy for records in compatibility matrix.
* add description for resource group version compatibility.

/cc @brancz @zouyee

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/271)
<!-- Reviewable:end -->
",closed,True,2017-10-11 14:04:27,2017-10-11 14:22:47
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/272,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/272,CHANGELOG.md: add changelog entry for v1.1.0-rc.0,"We've collected quite a few features, bug fixes and enhancements and in order to ensure full compatibility with Kubernetes 1.8 client-go was updated as well.

@fabxc @andyxning @zouyee Please have a look.

/cc @piosz @loburm please get ready in order to push the published image to gcr.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/272)
<!-- Reviewable:end -->
",closed,True,2017-10-12 09:26:33,2017-10-12 12:04:03
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/273,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/273,kubernetes: update sample manifests,"With rc.0 released, we should promote the usage. The only new resource being used are namespace objects, other than that, no other RBAC changes.

@andyxning

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/273)
<!-- Reviewable:end -->
",closed,True,2017-10-12 10:29:33,2017-10-12 10:38:37
kube-state-metrics,svend,https://github.com/kubernetes/kube-state-metrics/pull/274,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/274,Add volumename label to persistentvolumeclaim_info metric ,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/274)
<!-- Reviewable:end -->
",closed,True,2017-10-12 18:37:11,2017-10-20 02:14:17
kube-state-metrics,sgmiller,https://github.com/kubernetes/kube-state-metrics/issues/275,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/275,"v1.0.1 not gathering data, repeated CronJob error","Kubernetes v1.7.4, only this in the logs for kube-state-metrics:

E1013 15:02:43.602589       7 reflector.go:201] k8s.io/kube-state-metrics/collectors/cronjob.go:86: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
",closed,False,2017-10-13 15:09:45,2017-10-16 13:05:31
kube-state-metrics,caarlos0,https://github.com/kubernetes/kube-state-metrics/pull/276,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/276,added termination reason metric,"very much like #256, but for containers that were terminated.

It should be useful to detect apps that are being constantly OOMKilled, for example.

I got the possible reasons from kubernetes codebase:

https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/dockershim/docker_container.go#L328-L361

+cc @brancz 

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/276)
<!-- Reviewable:end -->
",closed,True,2017-10-17 16:46:08,2017-10-19 13:30:26
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/277,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/277,add code_of_conduct.md,"add code_of_conduct.md

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/277)
<!-- Reviewable:end -->
",closed,True,2017-10-19 03:02:30,2017-10-19 09:15:01
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/278,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/278,*: cut v1.1.0,"@fabxc @andyxning

/cc @piosz @loburm 

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/278)
<!-- Reviewable:end -->
",closed,True,2017-10-19 09:18:32,2017-10-19 10:06:36
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/issues/279,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/279,add kube-state-metrics to cluster-metrics ,releated PR [add-on#54235](https://github.com/kubernetes/kubernetes/pull/54235),closed,False,2017-10-19 14:07:26,2017-12-05 12:09:26
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/280,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/280,update deploy config apiversion,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/280)
<!-- Reviewable:end -->
",closed,True,2017-10-22 02:03:51,2017-10-22 15:17:37
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/281,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/281,update README,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/281)
<!-- Reviewable:end -->
",closed,True,2017-10-22 02:07:42,2017-10-22 06:00:47
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/282,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/282,Add simple e2e test,"This PR add simple e2e test for kube-state-metrics.

This PR leverage minikube to setup an available Kubernetes cluster. This PR reference https://kinvolk.io/blog/2017/10/running-kubernetes-on-travis-ci-with-minikube/ doc.

e2e test logic
* set up an kubernetes v1.8.0 cluster with minikube
* apply kube-state-metrics manifests. Exit with errors
* build and run kube-state-metrics in standalone
* wait for kube-state-metrics to run by accessing `healthz` endpoint
* access `metrics` endpoint
* access `healthz` endpoint to check for that kube-state-metrics is still running after accessing `metrics` endpoint.

**Note**: 
After adding promtool as the metric validate tool, `kube_pod_container_status_restarts` as a Counter is not a valid metric name. This will result in `kube_pod_container_status_restarts: counter metrics should have ""_total"" suffix` error.

So, in order to make promtool work properly, A name from `kube_pod_container_status_restarts` to `kube_pod_container_status_restarts_total` will be made. This is a breaking change. Which should  be noted additionally.


<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/282)
<!-- Reviewable:end -->
",closed,True,2017-10-22 05:49:15,2018-03-13 02:27:11
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/283,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/283,fix hpa stale imports,"Quick fix hpa stale imports.

> collectors/hpa.go:25:2: cannot find package ""k8s.io/client-go/pkg/api"" in any of:
	/home/travis/gopath/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/pkg/api (vendor tree)
	/home/travis/.gimme/versions/go1.8.linux.amd64/src/k8s.io/client-go/pkg/api (from $GOROOT)
	/home/travis/gopath/src/k8s.io/client-go/pkg/api (from $GOPATH)
collectors/hpa.go:26:2: cannot find package ""k8s.io/client-go/pkg/apis/autoscaling/v1"" in any of:
	/home/travis/gopath/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/pkg/apis/autoscaling/v1 (vendor tree)
	/home/travis/.gimme/versions/go1.8.linux.amd64/src/k8s.io/client-go/pkg/apis/autoscaling/v1 (from $GOROOT)
	/home/travis/gopath/src/k8s.io/client-go/pkg/apis/autoscaling/v1 (from $GOPATH)

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/283)
<!-- Reviewable:end -->
",closed,True,2017-10-24 02:38:23,2017-10-24 06:17:34
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/284,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/284,fix travis ci build,"As for now `make` does not equal to `make build`. We should make it explicitly to test with `make build`.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/284)
<!-- Reviewable:end -->
",closed,True,2017-10-24 02:49:01,2017-10-24 06:18:37
kube-state-metrics,svend,https://github.com/kubernetes/kube-state-metrics/pull/285,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/285,Omit static compute resource requests and limits from deployment,"The addon-resizer will dynamically set the pod compute resource values. If the
values are also set statically in the deployment configuration, reapplying the
configuration will result in the pods getting replaced. Without the static
resource, the deployment configuration can be reapplied, and the pods will not
be replaced.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/285)
<!-- Reviewable:end -->
",closed,True,2017-10-24 18:52:53,2017-10-26 07:32:35
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/286,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/286,update development content,"`--kubeconfig`  need to be given

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/286)
<!-- Reviewable:end -->
",closed,True,2017-10-25 07:37:03,2017-10-26 02:45:32
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/287,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/287,fix hpa nil error,"fixes https://github.com/kubernetes/kube-state-metrics/issues/288
<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/287)
<!-- Reviewable:end -->
",closed,True,2017-10-25 07:54:58,2017-10-25 11:25:41
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/issues/288,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/288,nil pointer:collector running found error,"I1025 15:50:58.112998   11627 main.go:166] Using default collectors
I1025 15:50:58.113125   11627 main.go:173] Using all namespace
I1025 15:50:58.113422   11627 main.go:220] service account token present: true
I1025 15:50:58.113433   11627 main.go:221] service host: https://x.x.x.x:6443
I1025 15:50:58.114174   11627 main.go:247] Testing communication with server
I1025 15:50:58.173924   11627 main.go:252] Communication with server successful
I1025 15:50:58.173953   11627 pod.go:182] collect pod with v1
I1025 15:50:58.174025   11627 replicaset.go:75] collect replicaset with extensions/v1beta1
I1025 15:50:58.174069   11627 namespace.go:68] collect namespace with v1
I1025 15:50:58.174099   11627 hpa.go:75] collect hpa with autoscaling/v2beta1
I1025 15:50:58.174126   11627 daemonset.go:70] collect daemonset with extensions/v1beta1
I1025 15:50:58.174160   11627 deployment.go:107] collect deployment with extensions/v1beta1
I1025 15:50:58.174215   11627 limitrange.go:57] collect limitrange with v1
I1025 15:50:58.174244   11627 node.go:130] collect node with v1
I1025 15:50:58.174947   11627 resourcequota.go:55] collect resourcequota with v1
I1025 15:50:58.175013   11627 service.go:61] collect service with v1
I1025 15:50:58.175226   11627 cronjob.go:81] collect cronjob with batch/v2alpha1
I1025 15:50:58.175349   11627 persistentvolumeclaim.go:67] collect persistentvolumeclaim with v1
I1025 15:50:58.175398   11627 replicationcontroller.go:81] collect replicationcontroller with v1
I1025 15:50:58.175442   11627 job.go:100] collect job with batch/v1
E1025 15:50:58.175404   11627 runtime.go:66] Observed a panic: ""invalid memory address or nil pointer dereference"" (runtime error: invalid memory address or nil pointer dereference)
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:72
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:65
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:51
/usr/local/Cellar/go/1.9/libexec/src/runtime/asm_amd64.s:509
/usr/local/Cellar/go/1.9/libexec/src/runtime/panic.go:491
/usr/local/Cellar/go/1.9/libexec/src/runtime/panic.go:63
/usr/local/Cellar/go/1.9/libexec/src/runtime/signal_unix.go:367
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/listwatch.go:67
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/listwatch.go:101
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/reflector.go:249
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/reflector.go:204
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/reflector.go:203
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/controller.go:122
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:54
/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:71
/usr/local/Cellar/go/1.9/libexec/src/runtime/asm_amd64.s:2337
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x48 pc=0x1ad3329]

goroutine 69 [running]:
k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:58 +0x111
panic(0x1bf8200, 0x24acde0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/panic.go:491 +0x283
k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache.NewListWatchFromClient.func1(0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x1d1f647, ...)
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/listwatch.go:67 +0xe9
k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache.(*ListWatch).List(0xc4203ec420, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/listwatch.go:101 +0x6b
k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache.(*Reflector).ListAndWatch(0xc4205b4000, 0x0, 0x0, 0x0)
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/reflector.go:249 +0x246
k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache.(*Reflector).Run.func1()
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/reflector.go:204 +0x33
k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil.func1(0xc4204c0718)
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133 +0x5e
k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc4205e7f18, 0x3b9aca00, 0x0, 0xc4202ce001, 0x0)
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134 +0xbd
k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait.Until(0xc4204c0718, 0x3b9aca00, 0x0)
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88 +0x4d
k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache.(*Reflector).Run(0xc4205b4000, 0x0)
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/reflector.go:203 +0x164
k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache.(*Reflector).Run-fm(0x0)
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/client-go/tools/cache/controller.go:122 +0x34
k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait.(*Group).StartWithChannel.func1()
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:54 +0x31
k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1(0xc420294020, 0xc42024c060)
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:71 +0x4f
created by k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait.(*Group).Start
	/Users/zoues/Code/golang/src/k8s.io/kube-state-metrics/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:69 +0x62
exit status 2
",closed,False,2017-10-25 08:00:42,2017-10-25 11:25:41
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/289,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/289,update kubernetes client-go tag to v5.0.0 which release with k8s 1.8.0,"update kubernetes client-go tag to v5.0.0 which release with kubernetes 1.8.0

/cc @brancz  @andyxning 
<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/289)
<!-- Reviewable:end -->
",closed,True,2017-10-26 07:59:52,2017-10-26 08:37:35
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/issues/290,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/290,Add more resource instance to e2e test ,"Currently, we only check for ClusterRoleBinding, ClusterRole, ServiceAccount, Deployment, RoleBinding, Role and Service.

We should add more resources when access kube-state-metrics `metrics` endpoint. This will make the e2e test more reliable and more help for us to discover problems.",closed,False,2017-10-27 10:43:05,2018-05-02 11:14:23
kube-state-metrics,f0,https://github.com/kubernetes/kube-state-metrics/pull/291,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/291,Update CronJobs to v1beta1,"with kubernetes 1.8 cronjobs are at version v1beta1, and enabled per default

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/291)
<!-- Reviewable:end -->
",closed,True,2017-10-30 08:15:22,2017-12-07 01:03:06
kube-state-metrics,mikebryant,https://github.com/kubernetes/kube-state-metrics/pull/292,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/292,Add StatefulSet replica detail metrics,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/292)
<!-- Reviewable:end -->
",closed,True,2017-11-01 18:13:00,2017-11-01 18:39:02
kube-state-metrics,felixPG,https://github.com/kubernetes/kube-state-metrics/issues/293,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/293,Expose more labels on services,"Hi:

Currently **kube_service_info** metrics only expose information about a service name, instance, etc. but what about the name of pods that this service could access?
",closed,False,2017-11-02 09:40:05,2018-01-25 10:39:38
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/294,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/294,new statement for alpha collectors and metrics in README,"/cc @brancz  @andyxning 
<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/294)
<!-- Reviewable:end -->
",closed,True,2017-11-03 03:27:27,2017-11-06 09:26:14
kube-state-metrics,jicki,https://github.com/kubernetes/kube-state-metrics/issues/295,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/295,Failed to list *v2alpha1.CronJob: the server could not find the requested resource,"```
kubectl get nodes
NAME          STATUS    ROLES     AGE       VERSION
172.16.1.64   Ready     <none>    20d       v1.8.1
172.16.1.65   Ready     <none>    20d       v1.8.1
172.16.1.66   Ready     <none>    20d       v1.8.1
```

```
ld not find the requested resource
E1106 07:09:40.573135       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:93: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E1106 07:09:41.574269       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:93: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E1106 07:09:42.575712       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:93: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E1106 07:09:43.576884       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:93: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E1106 07:09:44.578064       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:93: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E1106 07:09:45.581133       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:93: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E1106 07:09:46.582299       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:93: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E1106 07:09:47.583628       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:93: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E1106 07:09:48.584764       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:93: Failed to list *v2alpha1.CronJob: the server could not find the requested resource
E1106 07:09:49.586064       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:93: Failed to list *v2alpha1.CronJob: the server could not find the requested resource

```


pls  help me `",closed,False,2017-11-06 07:13:28,2018-04-08 06:16:48
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/issues/296,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/296,Expose metrics about kube-state-metrics itself,"kube-state-metrics currently only exposes metrics about the Kubernetes objects, but kube-state-metrics itself should also be ""monitorable"". For this purpose I would suggest an additional metrics endpoint (maybe `/metrics/kube-state-metrics`, there we can expose metrics like the number of objects used to generate metrics and go runtime as well as process statistics.

@andyxning ",closed,False,2017-11-06 09:51:19,2018-01-03 15:45:28
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/issues/297,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/297,Allow using RBAC to protect metrics endpoint(s),"In order to prevent any Pod in a Kubernetes cluster to request the metrics endpoint of kube-state-metrics, it should allow enabling RBAC validation with a running apiserver. Since kube-state-metrics heavily relies on a running apiserver anyways, this is no additional dependency.

@andyxning ",closed,False,2017-11-06 10:02:38,2018-01-03 13:02:37
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/298,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/298,daemonset status info,"<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/298)
<!-- Reviewable:end -->
kube_daemonset_status_number_available(unavailable)  which different with kube_daemonset_status_number_ready
1. kube_daemonset_status_number_available 
> (available info about ds)The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and **available**
2. kube_daemonset_status_number_unavailable
> (unavailable info about ds)The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available
3. kube_daemonset_updated_number_scheduled
> nodes that are running **updated daemon pod**
/cc @brancz ",closed,True,2017-11-07 02:09:42,2017-12-02 11:02:23
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/299,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/299,deployment ready replica info,"kube_deployment_status_replicas_ready
> Total number of ready pods targeted by this deployment

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/299)
<!-- Reviewable:end -->
",closed,True,2017-11-07 04:34:26,2017-11-08 07:44:44
kube-state-metrics,ddii,https://github.com/kubernetes/kube-state-metrics/issues/300,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/300,Add new node,"Hi, 
Please understand that my English is not good.

I have already k8s cluster. 

root@master:~/kube-state-metrics# kubectl get nodes
NAME      STATUS    ROLES     AGE       VERSION
master    Ready     master    12d       v1.8.2
ml1       Ready     <none>    8d        v1.8.2

And i deployed kube-state-metrics.

root@master:~/kube-state-metrics# kubectl get pods -n kube-system
NAME                                       READY     STATUS    RESTARTS   AGE
calico-etcd-9cfj2                          1/1       Running   0          12d
calico-kube-controllers-6ff88bf6d4-m7s8d   1/1       Running   0          12d
calico-node-9pgk5                          2/2       Running   0          8d
calico-node-z7r2v                          2/2       Running   0          12d
etcd-master                                1/1       Running   0          12d
kube-apiserver-master                      1/1       Running   0          12d
kube-controller-manager-master             1/1       Running   0          12d
kube-dns-545bc4bfd4-j7jxv                  3/3       Running   0          12d
kube-proxy-f9fc9                           1/1       Running   0          8d
kube-proxy-ltv9n                           1/1       Running   0          12d
kube-scheduler-master                      1/1       Running   0          12d
kube-state-metrics-767c674dfd-pqhf2        2/2       Running   0          16h

After, I added ml2 nodes to the cluster. but i can't see new kube-state-metric pods for new node.

root@master:~/kube-state-metrics# kubectl get nodes
NAME      STATUS    ROLES     AGE       VERSION
master    Ready     master    12d       v1.8.2
ml1       Ready     <none>    8d        v1.8.2
ml2       Ready     <none>    16h       v1.8.2


root@master:~/kube-state-metrics# kubectl get pods -n kube-system
NAME                                       READY     STATUS    RESTARTS   AGE
calico-etcd-9cfj2                          1/1       Running   0          12d
calico-kube-controllers-6ff88bf6d4-m7s8d   1/1       Running   0          12d
calico-node-9pgk5                          2/2       Running   0          8d
calico-node-gttj4                          2/2       Running   1          16h
calico-node-z7r2v                          2/2       Running   0          12d
etcd-master                                1/1       Running   0          12d
kube-apiserver-master                      1/1       Running   0          12d
kube-controller-manager-master             1/1       Running   0          12d
kube-dns-545bc4bfd4-j7jxv                  3/3       Running   0          12d
kube-proxy-76gdf                           1/1       Running   0          16h
kube-proxy-f9fc9                           1/1       Running   0          8d
kube-proxy-ltv9n                           1/1       Running   0          12d
kube-scheduler-master                      1/1       Running   0          12d
kube-state-metrics-767c674dfd-pqhf2        2/2       Running   0          16h

When adding a new node, Can be deployed new kube-state-metric pods automatically for new node?

",closed,False,2017-11-08 01:30:18,2017-11-08 05:41:42
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/301,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/301,add ksm version command line argument,"add ksm buildinfo api and scrapy tagert info which log kubernetes cluster version
/cc @andyxning  @brancz 
<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/301)
<!-- Reviewable:end -->
",closed,True,2017-11-08 07:45:42,2018-01-08 08:26:23
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/302,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/302,Allow using RBAC to protect metrics endpoint,"Closes #297

@andyxning @smarterclayton

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/302)
<!-- Reviewable:end -->
",closed,True,2017-11-08 16:19:51,2018-01-03 13:03:20
kube-state-metrics,JosephSalisbury,https://github.com/kubernetes/kube-state-metrics/issues/303,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/303,Add metrics for tpr/crd objects,"I would find it useful to be able to have metrics for the number of tpr / crd objects currently in the cluster.

Happy to provide PRs if there's consensus on usefulness.",closed,False,2017-11-13 17:11:36,2017-11-14 12:23:37
kube-state-metrics,yeplaa,https://github.com/kubernetes/kube-state-metrics/issues/304,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/304,Space used in persistvolumeclaim,"Hello,

My environment is kube-state-metrics 1.1, Prometheus 1.7.1, K8S 1.7.5.
With kube-state-metrics, I would like to retrieve the stats about used persistentvolumeclaim space.
Actually, it seems only the metrics ""kube_persistentvolumeclaim_resource_requests_storage_bytes"" is available with kube-state-metrics.  Are there any other metrics available for my need on k8s 1.6/1.7? 
On K8s 1.8, kubelet exposes the metrics kubelet_volume_stats_available_bytes, kubelet_volume_stats_used_bytes that could meet the need.  
Is it expected that kube-state-metrics can recover it?

Thank's
Loïc",closed,False,2017-11-17 09:14:57,2017-12-14 17:21:21
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/305,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/305,Update README.md,"/cc @brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/305)
<!-- Reviewable:end -->
",closed,True,2017-11-20 15:11:34,2017-11-20 15:57:51
kube-state-metrics,tmegow,https://github.com/kubernetes/kube-state-metrics/issues/306,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/306,Unable to enable HPA collector/no hpa metrics,"I'm running the latest v1.1.0 release and I am unable to see hpa metrics
```
# curl -s 10.77.77.177:8003/metrics | grep kube_hpa
#
```

Looking at the help output I see this informative message: `--collectors string                Comma-separated list of collectors to be enabled. Defaults to ""cronjobs,daemonsets,deployments,jobs,limitranges,namespaces,nodes,persistentvolumeclaims,pods,replicasets,replicationcontrollers,resourcequotas,services,statefulsets""`

If I try specifically enabling that collector with `--collectors=""cronjobs,daemonsets,deployments,jobs,limitranges,namespaces,nodes,persistentvolumeclaims,pods,replicasets,replicationcontrollers,resourcequotas,services,statefulsets,horizontalpodautoscalers""` I get:
`F1121 17:50:38.057655       1 main.go:98] Collector ""horizontalpodautoscalers"" does not exist`

How can I see the hpa metrics? Where am I going wrong?",closed,False,2017-11-21 17:59:19,2017-11-22 11:23:30
kube-state-metrics,xufaning,https://github.com/kubernetes/kube-state-metrics/issues/307,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/307,"kube_deployment_labels can't read new ""dc""","I deploy a kube-state-metrics on our openshift 3.6. I can get metrics use ""kube_deployment_..."" like this
kube_deployment_created{deployment=""grafana"",namespace=""maintenance""} 
kube_deployment_created{deployment=""kube-state-metrics"",namespace=""maintenance""} 
kube_deployment_created{deployment=""prometheus"",namespace=""maintenance""} 
so I can get three dc object metrics .
I have a dc/tiger-demo in namespace ""jksdemo"" ,I can;t get it's metrics about kube_deployment_...
but I can get pod metrics about ""dc/tiger-demo""
please help me ,why?  I can't get ""dc/tiger-demo""  metrics .",closed,False,2017-11-27 04:15:22,2017-11-28 08:39:33
kube-state-metrics,beatlejuse,https://github.com/kubernetes/kube-state-metrics/issues/308,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/308,deploy error,"i have a problem with deploy ClusterRole, ClusterRoleBinding, Role and RoleBinding.
here are my .yaml:
https://gist.github.com/beatlejuse/8ed9ac9db38c5b58c07cdf7e1d8d3fb3
and here are my logs:
Error from server (Forbidden): error when creating ""./k8s/monitoring/exporters/kubernetes/kube-state-metrics-role.yml"": clusterroles.rbac.authorization.k8s.io ""kube-state-metrics"" is forbidden: attempt to grant extra privileges: [PolicyRule{Resources:[""nodes""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""nodes""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""pods""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""pods""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""services""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""services""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""resourcequotas""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""resourcequotas""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""replicationcontrollers""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""replicationcontrollers""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""limitranges""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""limitranges""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""persistentvolumeclaims""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""persistentvolumeclaims""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""namespaces""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""namespaces""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""daemonsets""], APIGroups:[""extensions""], Verbs:[""list""]} PolicyRule{Resources:[""daemonsets""], APIGroups:[""extensions""], Verbs:[""watch""]} PolicyRule{Resources:[""deployments""], APIGroups:[""extensions""], Verbs:[""list""]} PolicyRule{Resources:[""deployments""], APIGroups:[""extensions""], Verbs:[""watch""]} PolicyRule{Resources:[""replicasets""], APIGroups:[""extensions""], Verbs:[""list""]} PolicyRule{Resources:[""replicasets""], APIGroups:[""extensions""], Verbs:[""watch""]} PolicyRule{Resources:[""statefulsets""], APIGroups:[""apps""], Verbs:[""list""]} PolicyRule{Resources:[""statefulsets""], APIGroups:[""apps""], Verbs:[""watch""]}] user=&{kube-admin  [system:authenticated] map[]} ownerrules=[] ruleResolutionErrors=[]
Error from server (Forbidden): error when creating ""./k8s/monitoring/exporters/kubernetes/kube-state-metrics-role.yml"": roles.rbac.authorization.k8s.io ""kube-state-metrics-resizer"" is forbidden: attempt to grant extra privileges: [PolicyRule{Resources:[""pods""], APIGroups:[""""], Verbs:[""get""]}] user=&{kube-admin  [system:authenticated] map[]} ownerrules=[] ruleResolutionErrors=[]
error validating ""./k8s/monitoring/exporters/kubernetes/kube-state-metrics-role.yml"": error validating data: found invalid field namespace for v1.ServiceAccount; if you choose to ignore these errors, turn validation off with --validate=false

I use k8s v1.8.1 and have full admin rights in k8s.",closed,False,2017-11-27 07:08:55,2017-12-01 07:18:34
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/309,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/309,metrics about persist volume,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/309)
<!-- Reviewable:end -->
",closed,True,2017-12-03 02:03:00,2017-12-03 02:05:17
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/310,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/310,code refactor,"This PR will refactor #301 . Just simple code refactor and no functional change.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/310)
<!-- Reviewable:end -->
",closed,True,2017-12-03 02:48:46,2017-12-03 03:27:11
kube-state-metrics,cpointner,https://github.com/kubernetes/kube-state-metrics/pull/311,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/311,fix API Endpoint for CronJobs,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/311)
<!-- Reviewable:end -->
",closed,True,2017-12-06 14:55:47,2017-12-11 10:03:54
kube-state-metrics,cmur2,https://github.com/kubernetes/kube-state-metrics/issues/312,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/312,Why service-endpoint scraping instead of pod scraping with Prometheus?,"In the [Kubernetes Deployment section in your README](https://github.com/kubernetes/kube-state-metrics/blob/master/README.md#kubernetes-deployment) you state that kube-state-metrics should be setup using a Kubernetes deployment and service as example. Prometheus will scrape it automatically if the service-endpoints scraping is properly configures.

Now I wonder why kube-state-metrics needs a Kubernetes service besides the deployment and why not just scrape the kube-state-metrics pod(s) directly using the following YAML snippet in the deployment:

```yaml
spec:
  template:
    metadata:
      annotations:
        prometheus.io/scrape: ""true""
        prometheus.io/port: ""8080""
```

I found a [thread on the Prometheus Users mailing list](https://groups.google.com/forum/#!topic/prometheus-users/jDEzVHUUFzw) where the consensus seems to be:

> An important distinction is for Pods that don't have any corresponding Services - they won't be scraped by the endpoints job, but will be by the pods job.  If you ever find yourself adding a Service just to collect metrics, you might consider scraping pods instead.

What is your stance on that case?",closed,False,2017-12-07 08:30:29,2017-12-18 13:25:53
kube-state-metrics,podnov,https://github.com/kubernetes/kube-state-metrics/issues/313,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/313,Add kube_job_labels and kube_cronjob_labels,,closed,False,2017-12-08 17:52:00,2017-12-10 15:30:10
kube-state-metrics,podnov,https://github.com/kubernetes/kube-state-metrics/pull/314,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/314,Add support for kube_job_labels and kube_cronjob_labels,"This fixes #313

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/314)
<!-- Reviewable:end -->
",closed,True,2017-12-08 17:53:39,2017-12-10 16:49:11
kube-state-metrics,jordigilh,https://github.com/kubernetes/kube-state-metrics/pull/315,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/315,Add Endpoints collector,"#57 
This PR adds a new metrics collector for Endpoint objects.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/315)
<!-- Reviewable:end -->
",closed,True,2017-12-14 16:07:17,2017-12-14 18:56:20
kube-state-metrics,jordigilh,https://github.com/kubernetes/kube-state-metrics/pull/316,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/316,Add Endpoint collector,"#57
This PR adds a new metrics collector for Endpoint objects.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/316)
<!-- Reviewable:end -->
",closed,True,2017-12-14 18:58:22,2017-12-22 16:50:14
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/317,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/317,"Revert ""add componentstatus metrics""","This reverts commit 5452f063935185d32c2fffcdbcf1f3decf1db3da.

As for now `componentstatuses` does not support `watch` method, we can not watch on it.  This should fix [219 comment](https://github.com/kubernetes/kube-state-metrics/pull/219#issuecomment-351824814).

ping @zouyee 
/cc @brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/317)
<!-- Reviewable:end -->
",closed,True,2017-12-15 06:27:30,2018-01-05 13:35:23
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/318,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/318,Allow setting host/ip to bind HTTP server to,"If one wants to use a proxy to be the only path to the metrics endpoint that kube-state-metrics exposes, we need to be able to allow binding to a specific interface rather than all (which is what we are currently doing). When the `--host` flag is not set, the previous behavior is continued, meaning this is not a breaking change.

@andyxning

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/318)
<!-- Reviewable:end -->
",closed,True,2017-12-17 15:52:02,2017-12-20 15:08:18
kube-state-metrics,Colstuwjx,https://github.com/kubernetes/kube-state-metrics/issues/319,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/319,best practise for alert rules,"As there is exposing metrics via kube-state-metrics, it would be better if we offer a sample alert rule for these metrics.",closed,False,2017-12-18 06:23:22,2018-05-02 11:12:41
kube-state-metrics,AndreaGiardini,https://github.com/kubernetes/kube-state-metrics/issues/320,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/320,Release docker image for latest,"Hello everyone

I noticed that the `latest` tag for the docker image corresponds to the latest release that has been published. Could we change this behavior in order to have `latest` = latest commit on master?

",open,False,2017-12-18 13:50:25,2019-01-10 16:51:25
kube-state-metrics,utsav2307,https://github.com/kubernetes/kube-state-metrics/issues/321,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/321,Not able to get HPA metrics,"I have deployed kube-state-metrics in my cluster, now when i do a curl on /metrics for the container endpoint, I am able to get Replication Controller and Deployments metrics, but there are no metrics for Horizontal Pod Autoscaler. Any idea what can be the issue? ",closed,False,2017-12-20 04:56:09,2018-01-15 08:41:36
kube-state-metrics,spiffxp,https://github.com/kubernetes/kube-state-metrics/pull/322,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/322,Add code-of-conduct.md,"Refer to kubernetes/community as authoritative source for code of conduct

ref: kubernetes/community#1527

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/322)
<!-- Reviewable:end -->
",closed,True,2017-12-20 18:33:12,2017-12-22 13:23:59
kube-state-metrics,jordigilh,https://github.com/kubernetes/kube-state-metrics/pull/323,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/323,Pluralized 'Endpoint' in collector output name,"Convention name for collectors referenced in configuration seems to be that they should be pluralized:
`I1221 11:02:43.323740       1 main.go:325] Active collectors: jobs,cronjobs,namespaces,endpoint,daemonsets,services,replicasets,replicationcontrollers,statefulsets,persistentvolumeclaims,componentstatuses,nodes,horizontalpodautoscalers,pods,resourcequotas,persistentvolumes,deployments,limitranges`

I didn't realize that I added 'endpoint' in singular, so this PR addresses that in the 2 areas where it is referenced, changing the word to 'endpoints' to match convention.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/323)
<!-- Reviewable:end -->
",closed,True,2017-12-22 17:09:33,2017-12-24 13:55:58
kube-state-metrics,thockin,https://github.com/kubernetes/kube-state-metrics/pull/324,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/324,Convert registry to k8s.gcr.io,"This PR was auto-generated.  Please apply human expertise to review for correctness.

Followup to https://github.com/kubernetes/kubernetes/pull/54174

xref https://github.com/kubernetes/release/issues/281

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/324)
<!-- Reviewable:end -->
",closed,True,2017-12-22 18:01:29,2017-12-24 14:09:24
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/325,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/325,Rm code_of_conduct.md,"Remove `code_of_conduct.md` for the duplication of `code-of-conduct.md`.
/cc @brancz 
<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/325)
<!-- Reviewable:end -->
",closed,True,2017-12-22 21:56:15,2017-12-27 20:33:21
kube-state-metrics,KieranP,https://github.com/kubernetes/kube-state-metrics/pull/326,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/326,Update README to include note for GKE users,"Include a note in the README for GKE users based on a reference provided by @brancz 

Reference: https://github.com/kubernetes/kube-state-metrics/issues/270#issuecomment-335759009

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/326)
<!-- Reviewable:end -->
",closed,True,2017-12-23 05:24:49,2017-12-23 09:46:42
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/327,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/327,add rbac config for endpoint,"Add rbac config for endpoints.

@jordigilh 
/cc @brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/327)
<!-- Reviewable:end -->
",closed,True,2017-12-24 14:01:45,2018-01-03 13:09:06
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/328,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/328,bump client-go to v6.0.0,"This PR will bump client-go dep to v6.0.0 to support Kubernetes 1.9.0.

Note: 
This PR removes `CreatedByAnnotation` annotation by using `GetControllerOf` from [`metav1`](https://github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/controller_ref.go#L32-L40) as `CreatedByAnnotation` has been deleted in Kubernetes 1.9.

/cc @brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/328)
<!-- Reviewable:end -->
",closed,True,2017-12-24 14:48:25,2018-01-03 13:37:34
kube-state-metrics,ahrtr,https://github.com/kubernetes/kube-state-metrics/issues/329,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/329,Question about the default Kubernetes Pod Metrics dashboard ,"I downloaded the default kubernetes Pod Metrics dashboard as below,
https://grafana.com/dashboards/747 

But I found that some metrics as below used in the dashboard do not exist in prometheus. What did I miss?
container_memory_working_set_bytes
container_cpu_usage_seconds_total
container_network_receive_bytes_total

Or can someone please provide some guide how to update the dashboard?

Some related information is as below,
prometheus version: v2.0.0
grafana version: v4.6.2
kube-state-metrics: quay.io/coreos/kube-state-metrics:v1.1.0
addon-resizer: gcr.io/google_containers/addon-resizer:1.0
OS: Oracle Linux 7.3",closed,False,2017-12-27 09:49:20,2017-12-29 06:32:42
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/330,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/330,add kube-state-metrics own metrics,"close #296 

This PR:
* add `--ksm-metrics-port` command line argument to expose ksm own metrics
* add go and process metrics
* expose `ksm_resources_per_scrape ` and `ksm_scrape_error_total`
  * `scrape_error_total`: Number of resources returned per scrape
  * `ksm_scrape_error_total`: Total scrape errors encountered when scraping a resource

/cc @brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/330)
<!-- Reviewable:end -->
",closed,True,2017-12-27 17:22:07,2018-01-04 02:34:11
kube-state-metrics,ahrtr,https://github.com/kubernetes/kube-state-metrics/issues/331,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/331,The value of kube_node_info is still 1 even when the node has already been shutted down,"I was planning to use the value of metrics ""kube_node_info"" to get the status of each node in k8s cluster. If the value is 1, then it means UP; and if it's 0, it means DOWN. But it seems that the value is always 1, even a node has already been shutted down. 

If the metrics ""kube_node_info"" can't be used to get each node's status, then which one should I use? Thanks. ",closed,False,2017-12-29 06:36:43,2018-01-03 11:00:24
kube-state-metrics,rajatjindal,https://github.com/kubernetes/kube-state-metrics/issues/332,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/332,Adding more labels to kube_pod_status_phase,"we had a situation where # of failed pod counts increased dramatically, and we were wondering what happened. 

on debugging we found 2 nodes were having docker issues and most of the failed nodes were being scheduled on those problematic nodes.

i think it will be useful to add more labels to kube_pod_status_phase, so that we can run query like all failed pods count group by node.

![screen shot 2018-01-04 at 10 23 43 am](https://user-images.githubusercontent.com/612092/34578331-6771b03a-f139-11e7-8516-daa6f4394fc5.png)
",closed,False,2018-01-04 18:24:32,2018-01-10 06:47:42
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/333,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/333,kubernetes: Add correct componentstatuses to ClusterRole,"I believe this is all that needs to be fixed in regards to the errors previously printed.

@andyxning @zouyee

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/333)
<!-- Reviewable:end -->
",closed,True,2018-01-05 12:48:24,2018-01-05 13:06:15
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/334,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/334,kubernetes: Add missing RBAC roles,"While doing some testing for the next release, I found out that we're missing some RBAC roles in the example.

@andyxning

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/334)
<!-- Reviewable:end -->
",closed,True,2018-01-05 13:11:23,2018-01-05 13:27:00
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/335,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/335,Cut 1.2.0-rc.0,"Happy new year :tada: ! We have gathered *a lot* of features and due to the recent Kubernetes 1.9 release, we should also release to ensure compatibility. Assuming everything goes well, we will be aiming for Monday the 15th of January for the stable 1.2.0 release.

@andyxning @zouyee @fabxc 

@piosz @loburm when merged I'd be great if one of you can push the release image to k8s.gcr.io.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/335)
<!-- Reviewable:end -->
",closed,True,2018-01-05 13:58:42,2018-01-07 08:23:43
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/336,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/336,kubernetes: Update version in manifest,"@andyxning

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/336)
<!-- Reviewable:end -->
",closed,True,2018-01-05 14:33:26,2018-01-05 14:45:20
kube-state-metrics,dannyk81,https://github.com/kubernetes/kube-state-metrics/issues/337,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/337,ksm v1.2.0-rc.0 on K8s 1.5.3 - Failed to list *v1beta1.CronJob,"Hey!

Was eager to try the new RC, however I'm getting the following errors when it tries to collect CronJobs:
```
E0105 16:13:29.620093       1 reflector.go:205] k8s.io/kube-state-metrics/collectors/cronjob.go:103: Failed to list *v1beta1.CronJob: the server could not find the requested resource
```

We are running Kubernetes 1.5.3, the following Api Versions are supported:
```
$ kubectl api-versions
apps/v1beta1
authentication.k8s.io/v1beta1
authorization.k8s.io/v1beta1
autoscaling/v1
batch/v1
batch/v2alpha1
certificates.k8s.io/v1alpha1
extensions/v1beta1
policy/v1beta1
rbac.authorization.k8s.io/v1alpha1
storage.k8s.io/v1beta1
v1
```

I believe the issue is that Batch API version in K8s 1.5.3 is still `batch/v2alpha1` and server doesn't support `batch/v1beta1`.

`batch/v1beta1` is available with Kubernetes 1.8 onwards (according to [this](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/))

WDYT?

",closed,False,2018-01-05 16:22:36,2018-06-10 06:06:26
kube-state-metrics,dannyk81,https://github.com/kubernetes/kube-state-metrics/issues/338,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/338,job label in kube_job_* metrics conflicts with the job label defined by scrape_config in Prometheus,"All the `kube_job_*` metrics have a `job` label with the name of the Job, however this label is conflicting with the `job` label assigned by Prometheus per what is defined in the `job_name` of the `scrape_config`

Prometheus keeps the original value of the label `job` in a new label called `exported_job`, but this is a bit confusing.

I think it would be better to rename the `job` label in the `kube_job_*` metrics to `job_name`",closed,False,2018-01-05 18:24:58,2018-07-07 01:00:18
kube-state-metrics,saady,https://github.com/kubernetes/kube-state-metrics/issues/339,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/339,Some metrics are not reporting all namespaces,"kube_pod_container_resource_requests_cpu_cores and kube_pod_container_resource_requests_memory_bytes  only reporting few namespaces pods containers cpu and memory usage.

  Deployment:

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: monitoring
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kube-state-metrics
        version: ""v1.1.0""
    spec:
      containers:
      - name: kube-state-metrics
        image: gcr.io/google_containers/kube-state-metrics:v1.1.0
        command: [""/kube-state-metrics"", ""--port=8080"", ""--collectors=deployments,daemonsets,jobs,limitranges,nodes,pods,replicasets,replicationcontrollers,resourcequotas,services,statefulsets,persistentvolumeclaims""]
        ports:
        - containerPort: 8080
```

Service


```
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: 'true'
  name: kube-state-metrics
  namespace: monitoring
  labels:
    app: kube-state-metrics
spec:
  ports:
  - name: kube-state-metrics
    port: 8080
    protocol: TCP
  selector:
    app: kube-state-metrics
```",closed,False,2018-01-05 22:38:47,2018-04-09 11:25:32
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/340,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/340,bump image version and apiVersion,"
<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/340)
<!-- Reviewable:end -->

  /cc @brancz ",closed,True,2018-01-07 04:12:05,2018-01-08 14:31:12
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/341,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/341,add release prodceduer doc,"Add release procedure doc in case we may forget to do some steps for releasing. :)

@brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/341)
<!-- Reviewable:end -->
",closed,True,2018-01-08 14:20:55,2018-01-09 13:04:38
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/342,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/342,add OWNER file for somebody create pr and issue to ask for help,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/342)
<!-- Reviewable:end -->
",closed,True,2018-01-10 07:53:03,2018-01-10 08:54:56
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/343,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/343,[WIP] add support for resource group version,"Close #357.

Add check for supported resource group version. This can help us:
* check whether the specified resource group version supports list and watch methods inside kube-state-metrics instead of relying on e2e tests. With e2e,  we can only test kube-state-metrics against one Kubernetes version. By enabling this function inside kube-state-metrics, we can fail fast when running with any Kubernetes with various versions and configurations. This can also help us from using resources which do not support both list and watch methods. Btw, this can help us avoid what we do for componentstatus in #219 . However, for now we have no 
* check whether the specified resource is supported by the server and fail fast. I prefer fail fast than log the resource on found on the server error. This makes kube-state-metrics's log more clean and make users to be informed earlier about the abnormal.
* Add supports for multi group version of a resource
  * add `--collectors-config` to specify the wanted group version of a collector in the format of `collector1=groupVersion,collector2=groupVersion`.
  * add `--list-registered-collectors` to list all supported collectors and the corresponding group versions
  * The logic for this functionality to work is like below:
    * first get all enabled collectors with the default group version
    * merge the enabled collectors  with the specified collectors config
    * check the finally enabled collectors against the server 
    * run the specified collectors with a group version 

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/343)
<!-- Reviewable:end -->

/cc @brancz ",closed,True,2018-01-12 08:59:06,2019-01-27 09:16:46
kube-state-metrics,utsav2307,https://github.com/kubernetes/kube-state-metrics/issues/344,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/344,Duration of kube_pod_container_status_terminated_reason metrics,"I understand that this metrics is keeping track of pods which were terminated and the reason for it, but the metrics store data of how many days? Like the data will be of last two days or last 12 hours?",closed,False,2018-01-15 04:19:30,2018-09-05 03:20:08
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/345,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/345,Cut 1.2,"After a testing period of 10 days, there were no additional bugs found or features introduced. So we're cutting the final 1.2.0 release.

@andyxning @zouyee 

@piosz @loburm stay tuned to push the container images to k8s.gcr.io.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/345)
<!-- Reviewable:end -->
",closed,True,2018-01-15 09:59:16,2018-01-15 12:57:32
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/346,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/346,Update README.md about cronjob version,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/346)
<!-- Reviewable:end -->
",closed,True,2018-01-15 13:57:34,2018-04-19 08:13:16
kube-state-metrics,grosser,https://github.com/kubernetes/kube-state-metrics/issues/347,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/347,use dumb-init as entry-point ?,"I can make a PR if you want it, we are trying to use it for all images we deploy.

https://github.com/Yelp/dumb-init",closed,False,2018-01-16 18:10:42,2018-01-18 15:24:28
kube-state-metrics,cullepl,https://github.com/kubernetes/kube-state-metrics/issues/348,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/348,Is there or will there ever be metric for pods in terminating state?,"I see there are metrics for if a pod is terminated `kube_pod_container_status_terminated` however, we sometimes observe pods in terminating state. 

I was wondering if there is a way via kube-state-metrics to find this state?  I couldnt see anything and I'm running v1.2.0

All I see in our prometheus are these:

![image](https://user-images.githubusercontent.com/13000925/35053646-9d18807e-fba2-11e7-85b9-ed0c76f9e276.png)


If pods are stuck in terminating state then we usually have to take actions against the hosts they are running on.
We'd ideally like a solution via prometheus and alertmanager using kube-state-metrics metrics to alert us rather than having to create something home grown.
",closed,False,2018-01-17 16:23:50,2019-02-21 12:57:27
kube-state-metrics,thockin,https://github.com/kubernetes/kube-state-metrics/pull/349,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/349,Pushes go to staging-k8s.gcr.io,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/349)
<!-- Reviewable:end -->
",closed,True,2018-01-17 22:22:08,2018-01-19 07:39:56
kube-state-metrics,r0fls,https://github.com/kubernetes/kube-state-metrics/issues/350,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/350,Empty reply from server,"When I hit the metrics endpoint for the kube-state-metrics service I get an empty reply from the server:

```
$ curl kube-state-metrics:8080/metrics
curl: (52) Empty reply from server
```

It returns the correct response for `/`:

```
$ curl kube-state-metrics:8080/
<html>
             <head><title>Kube Metrics Server</title></head>
             <body>
             <h1>Kube Metrics</h1>
                         <ul>
             <li><a href='/metrics'>metrics</a></li>
             <li><a href='/healthz'>healthz</a></li>
                         </ul>
             </body>
             </html>
```

The kube-state-metrics logs don't show any errors or debug information. I'm raising this issue in hopes of suggestions to debug this, but also because I'm going to create a corresponding PR that adds logging to the prom handler.",closed,False,2018-01-18 23:25:34,2018-01-20 00:31:02
kube-state-metrics,r0fls,https://github.com/kubernetes/kube-state-metrics/pull/351,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/351,enable log in the prom http handler,"This is related to #350.

I'm happy to make this more configurable, if desired. However I think passing some log handler to prometheus is better than ignoring its logs entirely.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/351)
<!-- Reviewable:end -->
",closed,True,2018-01-18 23:28:06,2018-01-20 00:34:11
kube-state-metrics,r0fls,https://github.com/kubernetes/kube-state-metrics/issues/352,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/352,make container should target linux,"When I run `make container` on my mac, I would expect the resulting docker image to be usable:

```
~/go/src/k8s.io/kube-state-metrics [master] 🚀 make container
cp -r * /var/folders/6c/1n3kjfy533530c2xlj_8r90c0000gq/T/tmp.GBNQSp83
GOOS=darwin GOARCH=amd64 CGO_ENABLED=0 go build -o /var/folders/6c/1n3kjfy533530c2xlj_8r90c0000gq/T/tmp.GBNQSp83/kube-state-metrics
docker build -t quay.io/coreos/kube-state-metrics-amd64:v1.2.0 /var/folders/6c/1n3kjfy533530c2xlj_8r90c0000gq/T/tmp.GBNQSp83
Sending build context to Docker daemon  88.37MB
Step 1/5 : FROM scratch
 ---> 
Step 2/5 : COPY kube-state-metrics /
 ---> Using cache
 ---> c9421874ce4e
Step 3/5 : VOLUME /tmp
 ---> Using cache
 ---> e887ca62544b
Step 4/5 : ENTRYPOINT [""/kube-state-metrics"", ""--port=8080""]
 ---> Using cache
 ---> 3a267e34b867
Step 5/5 : EXPOSE 8080
 ---> Using cache
 ---> 3459b9ca91d1
Successfully built 3459b9ca91d1
Successfully tagged quay.io/coreos/kube-state-metrics-amd64:v1.2.0
# Adding check for amd64
docker tag quay.io/coreos/kube-state-metrics-amd64:v1.2.0 quay.io/coreos/kube-state-metrics:v1.2.0
```

However, presumably because it's setting `GOOS=darwin` in the `go build` command, I get:

```
~/go/src/k8s.io/kube-state-metrics [master] 🚀 docker run -t quay.io/coreos/kube-state-metrics-amd64:v1.2.0 quay.io/coreos/kube-state-metrics:v1.2.0
standard_init_linux.go:195: exec user process caused ""exec format error""
```",closed,False,2018-01-19 23:48:02,2018-01-20 08:24:42
kube-state-metrics,r0fls,https://github.com/kubernetes/kube-state-metrics/pull/353,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/353,use linux in go build for container,"fixes: #352

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/353)
<!-- Reviewable:end -->
",closed,True,2018-01-19 23:52:21,2018-01-20 04:49:30
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/354,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/354,increase log verbosity,"Increase log verbosity to `V(4)` to decrease useless log in kube-state-metrics common use. This has been discussed in slack sig-instrumentation.

/cc @brancz @DirectXMan12

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/354)
<!-- Reviewable:end -->
",closed,True,2018-01-20 03:24:30,2018-01-20 09:04:01
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/355,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/355,fix container binary GOOS,"Fix #352 

This PR will make the `GOOS` for container and push target to `linux`.

/cc @brancz  @r0fls

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/355)
<!-- Reviewable:end -->
",closed,True,2018-01-20 03:44:25,2018-01-20 09:04:34
kube-state-metrics,utsav2307,https://github.com/kubernetes/kube-state-metrics/issues/356,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/356,additional status in kube_pod_container_status_waiting_reason,"currently only two types of status are available in kube_pod_container_status_waiting_reason metrics (ContainerCreating,ErrImagePull). 
Can this be modified to have ""Pending""  as well, when the pods dont have enough resources to run?",closed,False,2018-01-22 08:34:35,2018-01-27 02:41:26
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/issues/357,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/357,Proposal for supporting different group versions for a single resource.,"This Proposal is used to demonstrating the problem how to support different group version for a single resource.

docs: https://docs.google.com/document/d/1f7Q_FVODNNc5IytBYbLbLLIWSlefuk4d9YYbIsj99Ec/edit#


/cc @brancz 
/ping @zouyee  as you have proposed a similar PR for attempting to support this functionality.

We need to make a conclusion about this in this week and implement it later as this has been delayed for a long time and we definitely need this feature.",closed,False,2018-01-22 15:00:32,2018-09-24 19:59:57
kube-state-metrics,marsty,https://github.com/kubernetes/kube-state-metrics/issues/358,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/358,prometheus targets kubernetes-service-endpoints state is Down,"

Error server returned HTTP status 404 Not Found  why?",closed,False,2018-01-23 09:01:17,2018-05-02 11:11:49
kube-state-metrics,jtslear,https://github.com/kubernetes/kube-state-metrics/pull/359,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/359,Adds additional container waiting reasons,"* Adds:
  * `CrashLoopBackOff`
  * `ImagePullBackOff`
* Also adds testing for the existing `ErrImagePull`
* addresses #356

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/359)
<!-- Reviewable:end -->
",closed,True,2018-01-26 02:02:09,2018-01-27 02:40:30
kube-state-metrics,maloochpich,https://github.com/kubernetes/kube-state-metrics/issues/360,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/360,kube_state_matrics is not showing valid result,"When I try to use any kube graphs for prometheus is showing invalid result.

Example:
when I run kube_pod_container_status_waiting_reason:


`kube_pod_container_status_waiting_reason{app=""kube-state-metrics"",container=""blackbox-exporter"",job=""kubernetes-endpoints"",kubernetes_name=""kube-state-metrics"",kubernetes_namespace=""monitoring"",namespace=""monitoring"",pod=""blackbox-exporter-85cdb97d8f-6fjn2"",reason=""ErrImagePull""}`

$kubectl get pods -n monitoring
  blackbox-exporter-85cdb97d8f-6fjn2    1/1       Running   0          2h
",closed,False,2018-01-26 20:15:39,2018-08-04 05:40:28
kube-state-metrics,crunchie84,https://github.com/kubernetes/kube-state-metrics/pull/361,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/361,Corrected misnamed type in log,"hpa logging for collector misnamed 'jobs'.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/361)
<!-- Reviewable:end -->
",closed,True,2018-01-29 13:12:40,2018-01-29 14:43:07
kube-state-metrics,r0fls,https://github.com/kubernetes/kube-state-metrics/issues/362,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/362,invalid memory address or nil pointer dereference,"I'm planning to look into this more later, but I'm seeing this error and the pod is in the CrashLoopBackOff state:

```
kubectl  logs -n prd354 shared-ksm-4065890672-zj9vd  -f
I0131 01:29:19.794874   11483 main.go:154] Using default collectors
I0131 01:29:19.795261   11483 main.go:202] service account token present: true
I0131 01:29:19.795281   11483 main.go:203] service host: https://10.3.0.1:443
I0131 01:29:19.796336   11483 main.go:229] Testing communication with server
I0131 01:29:19.807566   11483 main.go:234] Communication with server successful
I0131 01:29:19.808578   11483 main.go:279] Active collectors: daemonsets,jobs,replicationcontrollers,services,statefulsets,persistentvolumeclaims,deployments,limitranges,cronjobs,resourcequotas,nodes,pods,replicasets
I0131 01:29:19.808611   11483 main.go:243] Starting metrics server: :8080
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x11e0172]

goroutine 179 [running]:
time.time.Time.Unix(...)
        /usr/local/google/home/kawych/go-tools/src/k8s.io/kube-state-metrics/collectors/job.go:173
k8s.io/kube-state-metrics/collectors.(*jobCollector).collectJob(0xc42038c7a0, 0xc420058de0, 0x0, 0x0, 0x0, 0x0, 0xc4202f6760, 0xf, 0x0, 0x0, ...)
        /usr/local/google/home/kawych/go-tools/src/k8s.io/kube-state-metrics/collectors/job.go:173 +0x1f2
k8s.io/kube-state-metrics/collectors.(*jobCollector).Collect(0xc42038c7a0, 0xc420058de0)
        /usr/local/google/home/kawych/go-tools/src/k8s.io/kube-state-metrics/collectors/job.go:141 +0x12f
k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus.(*Registry).Gather.func2(0xc420870830, 0xc420058de0, 0x1da0ea0, 0xc42038c7a0)
        /usr/local/google/home/kawych/go-tools/src/k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus/registry.go:433 +0x61
created by k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus.(*Registry).Gather
        /usr/local/google/home/kawych/go-tools/src/k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus/registry.go:431 +0x271
```",closed,False,2018-01-31 01:35:03,2018-01-31 16:38:26
kube-state-metrics,vmranh,https://github.com/kubernetes/kube-state-metrics/issues/363,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/363,Clarify k8s version compatibility,"I found the [compatibility ](https://github.com/kubernetes/kube-state-metrics#compatibility-matrix)matrix very useful but a bit confusing, I read it as 1.2/master is compatible with k8s 1.7.

When I applied the configurations (from master) I got the following error: 
`""no kind ""ClusterRoleBinding"" is registered for version ""rbac.authorization.k8s.io/v1""`

I realize that the container itself is backward compatible, but the rest of the k8s deployment objects (ClusterRoleBinding, ClusterRole, Deployment, RoleBinding) were not compatible with 1.7, maybe its worth adding a note about it.

Thanks !",closed,False,2018-02-01 22:10:37,2018-02-06 13:49:01
kube-state-metrics,hangyan,https://github.com/kubernetes/kube-state-metrics/pull/364,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/364,Add some note for old kubernetes cluster to deploy ksm,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/364)
<!-- Reviewable:end -->
",closed,True,2018-02-06 12:22:23,2018-02-06 14:44:34
kube-state-metrics,hangyan,https://github.com/kubernetes/kube-state-metrics/pull/365,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/365,Support deployment MaxSurge,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/365)
<!-- Reviewable:end -->
",closed,True,2018-02-12 09:17:53,2018-03-08 10:31:56
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/366,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/366,bump addon-resize image version,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/366)
<!-- Reviewable:end -->
",closed,True,2018-02-12 12:34:47,2018-02-14 06:07:14
kube-state-metrics,noah-mercado,https://github.com/kubernetes/kube-state-metrics/issues/367,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/367,kube_job_status_failed metric not fluctuating,"The documentation for kube_job_status_failed claims that this metric is a gauge and therefore should fluctuate. When using this metric and summing by the job name to get the number of times each job has failed, the numbers are consistent and there is no fluctuation for weeks on end which is hard to believe.

Here's the query I'm using in grafana to display the each job that has failed:

sum(kube_job_status_failed) by (exported_job) > 0

Is this an issue with the documentation? Or am I calculating the metric wrong?  Thanks!",closed,False,2018-02-14 18:49:34,2018-08-04 05:40:28
kube-state-metrics,dohnto,https://github.com/kubernetes/kube-state-metrics/issues/368,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/368,Add optional GET arguments to /metrics,"# Current situation

Our company have on-premise kubernetes cluster for several teams. One team use several namespaces. We want to run single kube-state-metrics as a deployment inside kubernetes and we want to allow teams to run their own prometheus and scrape kube-state-metrics deployment.

However, every team has to have their special relabel configuration and drop other namespaces otherwise dataseries count would rise significantly. Also most of the time series are transferred through networked and they are just dropped by prometheus.

# Proposed solution

Would it be possible to add optional GET arguments to metricsPath? I could imagine something like:
```
/metrics?namespace=team1-staging&namespace=team1-production
/metrics?collectors=deployments&namespace=team1-staging
```

Although I am not sure whether this is compatible with gRPC. I am willing to do the implementation but first I would like to discuss it with maintainers.

Thank you for any ideas or tips how to solve our issue.",closed,False,2018-02-15 10:31:27,2018-08-08 00:09:30
kube-state-metrics,matthewrj,https://github.com/kubernetes/kube-state-metrics/issues/369,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/369,Flag for adjusting log level,I'd like to run kube-state-metrics with a warning log level as I don't need to see all the info logs printed every minute. Can we please add a flag for adjusting log level/verbosity?,closed,False,2018-02-16 19:27:09,2018-02-27 07:21:08
kube-state-metrics,younid,https://github.com/kubernetes/kube-state-metrics/issues/370,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/370,Is there any metrics about configmaps and secrets ?,"Hello,

I'm looking for the way to monitoring configmaps and secrets changes. Is there any thing like this in kube-state-metrics ? If not do you know if there is a way to have it ?

The goal of this task is to be able to detect any changes made on some critical configmap or secret used by my pods.

Regards",closed,False,2018-02-16 23:26:06,2018-03-01 09:33:08
kube-state-metrics,metalmatze,https://github.com/kubernetes/kube-state-metrics/pull/371,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/371,Use Kubernete's own func that creates config by apiserver or kubeconfig,"Instead of implementing this logic ourselves we can simply leverage Kubernet's built-in function, which does exactly what we need.

We want to run kube-state-metrics outside of a cluster and simply give it an apiserver url. Right now this also needs a kubeconfig.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/371)
<!-- Reviewable:end -->
",closed,True,2018-02-17 15:46:56,2018-03-08 02:06:56
kube-state-metrics,pvanderlinden,https://github.com/kubernetes/kube-state-metrics/issues/372,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/372,kube-state-metrics crashes a lot,"I keep seeing this for the kube-state-metrics pod:
```
  Warning  Unhealthy  3m (x338 over 2h)     kubelet, <poolname>-9eab464a-krc0  Readiness probe failed: Get http://10.4.5.49:8080/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
```
but the logs show no errors.

This is on kubernetes 1.9 with kube-state-metrics 1.2.0 on GKE.",closed,False,2018-02-19 13:39:44,2018-02-19 16:28:28
kube-state-metrics,juliantaylor,https://github.com/kubernetes/kube-state-metrics/issues/373,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/373,"port confusion, document default port documentation in docker image","hi,
the default ports for the metrics in the binary is 80 and 81 while the readme says the default for the main metrics is 8080 in the first paragraph and the telemetry port default is not mentioned at all.
To make it worse the docker image sets one of them by entrypoint to 8080 but leaves the telemetry port at the privileged 81.

This is pretty confusing, the readme should not state that the default is 8080 when this it really is 80 in the binary, the documentation should probably mention that the docker image changes the default and the docker image should probably also change the default of the telemetry port so that it does not default to a privileged port (or not change the default at all).
",closed,False,2018-02-23 09:11:46,2018-02-27 08:52:41
kube-state-metrics,carlosedp,https://github.com/kubernetes/kube-state-metrics/issues/374,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/374,Compiling v1.2.0 on ARM64 gives error,"Trying to compile v1.2.0, I get the error:

```

rock64@kubemaster1:~/go/src/github.com/kubernetes/kube-state-metrics $ make container
cp -r * /tmp/tmp.0ZsFjUTGBW
GOOS=linux GOARCH=arm64 CGO_ENABLED=0 go build -o /tmp/tmp.0ZsFjUTGBW/kube-state-metrics
# github.com/kubernetes/kube-state-metrics
./main.go:71:3: cannot use collectors.RegisterCronJobCollector (type func(""k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""k8s.io/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string)) as type func(""github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""github.com/kubernetes/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string) in map value
./main.go:72:3: cannot use collectors.RegisterDaemonSetCollector (type func(""k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""k8s.io/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string)) as type func(""github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""github.com/kubernetes/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string) in map value
./main.go:73:3: cannot use collectors.RegisterDeploymentCollector (type func(""k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""k8s.io/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string)) as type func(""github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""github.com/kubernetes/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string) in map value
./main.go:74:3: cannot use collectors.RegisterJobCollector (type func(""k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""k8s.io/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string)) as type func(""github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""github.com/kubernetes/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string) in map value
./main.go:75:3: cannot use collectors.RegisterLimitRangeCollector (type func(""k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""k8s.io/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string)) as type func(""github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""github.com/kubernetes/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string) in map value
./main.go:76:3: cannot use collectors.RegisterNodeCollector (type func(""k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""k8s.io/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string)) as type func(""github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""github.com/kubernetes/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string) in map value
./main.go:77:3: cannot use collectors.RegisterPodCollector (type func(""k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""k8s.io/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string)) as type func(""github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""github.com/kubernetes/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string) in map value
./main.go:78:3: cannot use collectors.RegisterReplicaSetCollector (type func(""k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""k8s.io/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string)) as type func(""github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""github.com/kubernetes/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string) in map value
./main.go:79:3: cannot use collectors.RegisterReplicationControllerCollector (type func(""k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""k8s.io/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string)) as type func(""github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""github.com/kubernetes/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string) in map value
./main.go:80:3: cannot use collectors.RegisterResourceQuotaCollector (type func(""k8s.io/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""k8s.io/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string)) as type func(""github.com/kubernetes/kube-state-metrics/vendor/github.com/prometheus/client_golang/prometheus"".Registerer, ""github.com/kubernetes/kube-state-metrics/vendor/k8s.io/client-go/kubernetes"".Interface, string) in map value
./main.go:80:3: too many errors
Makefile:52: recipe for target '.container-arm64' failed
make: *** [.container-arm64] Error 2
```
",closed,False,2018-02-26 23:11:56,2018-02-27 08:10:01
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/375,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/375,fix port usage misunderstanding,"Fix #373 

/cc @brancz 
ping @juliantaylor

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/375)
<!-- Reviewable:end -->
",closed,True,2018-02-27 07:19:36,2018-02-28 13:12:46
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/issues/376,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/376,Enable k8s-ci-bot,"I think we need to enable k8s-ci-bot for kube-state-metrics. We have already have a root OWNERS file under the root directory of kube-state-metrics.

@cblecker Any thing we need to do to make it happen just like what @DirectXMan12 and you do for heapster in https://github.com/kubernetes/heapster/pull/1963?",closed,False,2018-02-27 10:09:46,2018-02-27 18:23:47
kube-state-metrics,tomwilkie,https://github.com/kubernetes/kube-state-metrics/issues/377,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/377,Initialise ksm_scrape_error_total metrics with zeros,Its generally considered bad practice to have missing metrics: https://prometheus.io/docs/practices/instrumentation/#avoid-missing-metrics,closed,False,2018-02-27 12:38:55,2018-02-28 10:49:48
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/378,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/378,fix telemetry metrics values to 0 when not happen,"Fix #377 

/cc @brancz 
Ping @tomwilkie

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/378)
<!-- Reviewable:end -->
",closed,True,2018-02-28 05:37:22,2018-02-28 10:57:56
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/379,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/379,add secret and configmap metrics,"Fix #370

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/379)
<!-- Reviewable:end -->
",closed,True,2018-02-28 14:34:11,2018-04-04 09:27:56
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/380,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/380,update go version to 1.10,"This PR will update Golang version to 1.9 as 1.10 has been released. We should use the newer golang version. :)

/cc @brancz

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/380)
<!-- Reviewable:end -->
",closed,True,2018-03-01 04:30:46,2018-03-14 04:23:59
kube-state-metrics,huskerdhf,https://github.com/kubernetes/kube-state-metrics/issues/381,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/381,kube-state-metrics authentication/authorization,"Hello,

Within this add-on server, is there any means of enabling authentication and authorization on the kube-state-metrics interface? Similar to the cAdvisor interface there is a significant amount of sensitive information exposed by kube-state-metrics which in a production environment preferably would not be accessible by just anyone or anything running on the K8s cluster. The main K8s project now provides a means of fully securing the cAdvisor /metrics endpoint and it would seem the same general type of functionality would be useful for this project if it is not already available.

If not is there any plan to add such a capability to kube-state-metrics? And if so is there a planned timeframe when that capability will be available?

Alternatively are there any documented design patterns for adding authentication/authorization to the kube-state-metrics interface outside the project?

Thanks,

DaveF",closed,False,2018-03-03 00:30:33,2018-03-05 18:20:55
kube-state-metrics,mindw,https://github.com/kubernetes/kube-state-metrics/pull/382,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/382,Add kube_daemonset_labels,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kube-state-metrics/382)
<!-- Reviewable:end -->
",closed,True,2018-03-03 21:17:50,2018-03-04 01:45:53
kube-state-metrics,pickledrick,https://github.com/kubernetes/kube-state-metrics/pull/383,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/383,Add annotations to namespace collector,,closed,True,2018-03-06 10:20:27,2018-03-08 13:43:59
kube-state-metrics,nlamirault,https://github.com/kubernetes/kube-state-metrics/pull/384,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/384,Fix: default ports,The defaults arguments are not ``8080`` and ``8081`` (See [here](https://github.com/kubernetes/kube-state-metrics/blob/release-1.2/main.go#L155) and [here](https://github.com/kubernetes/kube-state-metrics/blob/release-1.2/main.go#L157)),closed,True,2018-03-07 09:58:38,2018-03-07 16:04:18
kube-state-metrics,tcolgate,https://github.com/kubernetes/kube-state-metrics/pull/385,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/385,Add _labels for pv and pvc.,"This adds kube_persistentvolume_labels, and
kube_persistentvolumeclaim_lables. These are useful for associating the
pv and pvc with the app they were deployed for (via labels)",closed,True,2018-03-08 10:09:48,2018-03-09 02:23:13
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/386,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/386,update namespace doc,"This PR updates namespace doc. No code change.

/cc @brancz ",closed,True,2018-03-08 14:11:17,2018-03-14 04:21:40
kube-state-metrics,tcolgate,https://github.com/kubernetes/kube-state-metrics/pull/387,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/387,Remove namespace from PersistentVolume metrics,Remove namespace from PersistentVolume metrics as PersistentVolume is not namespace scoped.,closed,True,2018-03-08 16:56:00,2018-03-09 13:21:11
kube-state-metrics,derekwaynecarr,https://github.com/kubernetes/kube-state-metrics/issues/388,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/388,Add taint support,"A node whose volume is stuck attaching for more than X minutes is automatically tainted.

Ability to alert on this taint would be useful to couple automated remedy to reboot or repair the node or do other corrective action where appropriate.

Surfacing node status conditions helps for issues derived from node problem detector but more problems are likely to actually be surfaced via taints.",closed,False,2018-03-09 03:28:51,2018-03-14 14:12:04
kube-state-metrics,tcolgate,https://github.com/kubernetes/kube-state-metrics/pull/389,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/389,Add Pod persistentvolumeclaim binding information.,"This introduces two metrics that are useful when monitoring disk usage
of a pod. They allow you to determine the persistentvolumeclaims
associated with a specific pod, and how they are bound.

Without this pods can only be loosely associated with volumes through
labels.

I looked at providing metrics on other attached volume types, however
this is not useful at this time as the kubelet only exports statistics
for PVC mounted volumes. Most volumes can/should be mounted via a PVC
anyway so this is a relatively minor limitation.",closed,True,2018-03-09 08:50:20,2018-03-09 09:22:12
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/390,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/390,fix cronjob parse,"This PR will fix cronjob parse error.

As with [Kubernetes cronjob parse](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/cronjob/utils.go#L92-L108), we should also use `cron.Parse` instead of `cron.ParseStandard`.

/cc @brancz ",closed,True,2018-03-09 10:29:51,2018-03-09 14:52:27
kube-state-metrics,derekwaynecarr,https://github.com/kubernetes/kube-state-metrics/pull/391,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/391,Add metrics for node taints,"Fixes https://github.com/kubernetes/kube-state-metrics/issues/388

",closed,True,2018-03-10 05:18:21,2018-03-15 02:16:02
kube-state-metrics,normanjoyner,https://github.com/kubernetes/kube-state-metrics/pull/392,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/392,fix misspelling of Kubernetes,Fixes a typo of Kubernetes in the README,closed,True,2018-03-10 15:33:50,2018-03-11 02:15:12
kube-state-metrics,dohnto,https://github.com/kubernetes/kube-state-metrics/issues/393,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/393,HPAs do not respect --namespace argument,"### kube-state-metrics version
```
version.Version{GitCommit:""3e5e2d7"", BuildDate:""2018-03-10T21:06:00Z"", Release:""v1.2.0"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""linux/amd64""}
```

### How to reproduce
```
$ ./kube-state-metrics --kubeconfig ~/.kube/config  --telemetry-port 8081 --port 8080 --collectors=horizontalpodautoscalers,nodes  --namespace=my-ns
# in separate shell
$  curl localhost:8080/metrics | grep hpa
kube_hpa_status_desired_replicas{hpa=""typocorrection"",namespace=""my-ns""} 1
kube_hpa_status_desired_replicas{hpa=""variants"",namespace=""other-ns""} 2
```

### Expected result

No `kube_hpa_*` metrics from other namespace than `my-ns`.
",closed,False,2018-03-10 21:10:33,2018-03-11 02:24:10
kube-state-metrics,dohnto,https://github.com/kubernetes/kube-state-metrics/pull/394,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/394,Fix namespacing of HPA,Fixes #393,closed,True,2018-03-10 21:10:46,2018-03-11 02:24:10
kube-state-metrics,dohnto,https://github.com/kubernetes/kube-state-metrics/pull/395,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/395,[RFC] Allow to specify multiple namespace.,"**I am no golang professional so I welcome any feedback or improvement tips. Also I am not very familiar with kubernetes golang library so maybe I am trying to solve it too naively. Please tell me.**

### What problem is it trying to solve
See #368 for rationale.

### What is this feature?
Allow to pass value of `--namespace` as a list to allow to monitor multiple namespaces. E. g.
```
$ ./kube-state-metrics # all namespaces
$ ./kube-state-metrics --namespace=kube-system # single namespace
$ ./kube-state-metrics --namespace=kube-system,default,ingress-nginx # multiple namespaces
```

### Is it backwards compatible?
Yes
",closed,True,2018-03-10 22:12:30,2018-03-14 02:11:35
kube-state-metrics,aramalipoor,https://github.com/kubernetes/kube-state-metrics/pull/396,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/396,collectors: Report pod's completion time,"This metric alongside `pod_start_time` can help determine total Pod's lifetime. Useful for example to get average duration it takes Openshift's BuildConfigs to build images.

Example query:
```promql
avg without(pod, namespace)
  (kube_pod_completion_time{pod=~"".*-[0-9]+-build""} 
       - kube_pod_start_time{pod=~"".*-[0-9]+-build""})
```

The `pod_completion_time` calculation logic is based [this piece of code](https://github.com/openshift/origin-web-console/blob/57086c97efd8e5983d250d3aef110f61e0a5eab0/app/scripts/filters/resources.js#L1102-L1116) in [origin-web-console](https://github.com/openshift/origin-web-console)",closed,True,2018-03-12 18:13:31,2018-03-13 08:06:46
kube-state-metrics,aramalipoor,https://github.com/kubernetes/kube-state-metrics/pull/397,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/397,Add Unreleased section in CHANGELOG,As per https://keepachangelog.com/en/1.0.0/#effort and the [discussion here](https://github.com/kubernetes/kube-state-metrics/pull/396#discussion_r174003166),closed,True,2018-03-13 06:57:16,2018-03-13 08:56:57
kube-state-metrics,lyq628,https://github.com/kubernetes/kube-state-metrics/issues/398,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/398,Failed to create client: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory,"If no configuration about ca in k8s , just execute command:
1.kubectl create clusterrolebinding kube-state-metrics --clusterrole=cluster-admin --serviceaccount=monitoring:kube-state-metrics
2. kubectl create clusterrolebinding prometheus --clusterrole=cluster-admin --serviceaccount=monitoring:prometheus"",

kube-state-metrics fails to start and gives the tips:""Failed to create client: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory""。So is the token a MUST ?",closed,False,2018-03-14 07:23:00,2018-04-10 07:58:19
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/399,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/399,sanity check,"This PR
* add sanity check for `collectorSet` and `namespaceList`
* make `namespaceList`'s type to `string` to make it more user friendly to end users with `string` type.

/cc @brancz ",closed,True,2018-03-14 16:05:45,2018-03-17 01:49:18
kube-state-metrics,dohnto,https://github.com/kubernetes/kube-state-metrics/pull/400,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/400,Updated CHANGELOG,Updated changelog's unreleased section with #395 and #394.,closed,True,2018-03-14 19:30:35,2018-03-14 22:42:05
kube-state-metrics,charlesakalugwu,https://github.com/kubernetes/kube-state-metrics/issues/401,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/401,Running multiple replicas of kube-state-metrics,"Hi,

I was wondering if there are any recommendations you can offer to me on why I cannot run 3 replicas of kube-state-metrics. It seems to me like a stateless application dependent on getting data from the kube-apiserver. I think it should be ok to run multiple copies. Is this a wrong assumption?",closed,False,2018-03-16 10:49:24,2018-03-16 17:13:12
kube-state-metrics,ikruglov,https://github.com/kubernetes/kube-state-metrics/issues/402,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/402,two kube-state-metrics significantly deviate in reports over time,"Hi all,

I have very strong suspicion that kube-state-metrics never does resync and hence a long-running instance will deviate from the reality quite dramatically. At least this is what we see in production when we run two instances of kube-state-metrics watching same k8s cluster. They will end up showing different data quite soon.

I'll explain the problem using pods collector as an example. Please take the explanation with a graint of salt as I'm not big expert in kubernetes ecosystem.

So, pods collector uses [shared informer](https://github.com/kubernetes/kube-state-metrics/blob/master/collectors/pod.go#L200). The share informed gets created [here](https://github.com/kubernetes/kube-state-metrics/blob/master/collectors/collectors.go#L55).
```
func NewSharedInformerList(client rest.Interface, resource string, namespaces []string, objType runtime.Object) *SharedInformerList {
    sinfs := SharedInformerList{}
    for _, namespace := range namespaces {
        slw := cache.NewListWatchFromClient(client, resource, namespace, fields.Everything())
        sinfs = append(sinfs, cache.NewSharedInformer(slw, objType, resyncPeriod))
    }
    return &sinfs
}
```

One of the parameters which NewSharedInformer() receives is resyncPeriod (it's set to 5 min by default). As far as I understand the kubernetes model, resync period is here to limit potential inconsistency between kubernetes and a client (kube-state-metrics in our case). In other words, kube-state-metrics can deviate from the true state of affairs, but it's guaranteed to catch up within resync period. I'm no a big guru in kubernetes so my wording might not be precise, please excuse me in such case.

The piece of client-go which is responsible for syncing aclient with kube-apiservers is [Reflector](https://godoc.org/k8s.io/client-go/tools/cache#NewReflector). The condition which actually triggers a resync is [here](https://github.com/kubernetes/client-go/blob/master/tools/cache/reflector.go#L285). I'll quote it here:
```
            if r.ShouldResync == nil || r.ShouldResync() {
                glog.V(4).Infof(""%s: forcing resync"", r.name)
                if err := r.store.Resync(); err != nil {
                    resyncerrc <- err
                    return
                }
            }
```

So, whether reflector performs a resync depends on ShouldResync callback being set and the result of calling it. ShouldResync is populated through a sequence of passes which I don't want to mention here. The call of ShouldResync() also goes through the chain of calls and it ends up being in [shouldResync()](https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L436) of sharedProcessor. I'll also quite the function here:

```
func (p *sharedProcessor) shouldResync() bool {
    p.listenersLock.Lock()
    defer p.listenersLock.Unlock()

    p.syncingListeners = []*processorListener{}

    resyncNeeded := false
    now := p.clock.Now()
    for _, listener := range p.listeners {
        // need to loop through all the listeners to see if they need to resync so we can prepare any
        // listeners that are going to be resyncing.
        if listener.shouldResync(now) {
            resyncNeeded = true
            p.syncingListeners = append(p.syncingListeners, listener)
            listener.determineNextResync(now)
        }
    }
    return resyncNeeded
}
```

So, the function roughly implements following logic: go through every available processorListener and ask whether it needs a resync or not. If any of them needs resyncing then return true. If no one needs or there are no listeners then return false.

The ""there are no listeners"" is key here. The only place which I found where listeners are populated is [addListener()](https://github.com/kubernetes/client-go/blob/5f85fe426e7aa3c1df401a7ae6c1ba837bd76be9/tools/cache/shared_informer.go#L384) function. This function is only called twice inside [AddEventHandlerWithResyncPeriod()](https://github.com/kubernetes/client-go/blob/5f85fe426e7aa3c1df401a7ae6c1ba837bd76be9/tools/cache/shared_informer.go#L326).

These findings lead us to a simple result: if there is no EventHandler than there will be no resyncs.

And this is indeed what happens in kube-state-metrics. It simply created NewSharedInformer and doesn't add any watchers because of it following the different logic of listing pods directly through a [inf.GetStore().List()](https://github.com/kubernetes/kube-state-metrics/blob/master/collectors/pod.go#L204). I'll quote things here once again:
```
    pinfs := NewSharedInformerList(client, ""pods"", namespaces, &v1.Pod{})
    podLister := PodLister(func() (pods []v1.Pod, err error) {
        for _, pinf := range *pinfs {
            for _, m := range pinf.GetStore().List() {
                pods = append(pods, *m.(*v1.Pod))
            }
        }
        return pods, nil
    })
```

As an attempt to fix the problem I applied this patch to kube-state-metrics:
```
diff --git a/collectors/collectors.go b/collectors/collectors.go
index 875b218..3a59e93 100644
--- a/collectors/collectors.go
+++ b/collectors/collectors.go
@@ -52,7 +52,9 @@ func NewSharedInformerList(client rest.Interface, resource string, namespaces []
        sinfs := SharedInformerList{}
        for _, namespace := range namespaces {
                slw := cache.NewListWatchFromClient(client, resource, namespace, fields.Everything())
-               sinfs = append(sinfs, cache.NewSharedInformer(slw, objType, resyncPeriod))
+               inf := cache.NewSharedInformer(slw, objType, resyncPeriod)
+               inf.AddEventHandler(cache.ResourceEventHandlerFuncs{AddFunc: func(obj interface{}) {}})
+               sinfs = append(sinfs, inf)
        }
        return &sinfs
 }
```

This was enough to start seeing periodical ""forcing resync"" in logs when running kube-state-metrics with -v=4.

```
# note that resyncPeriod is set to 5 sec here
$ ./kube-state-metrics --v=4 --kubeconfig=kubeconfig --logtostderr --host=127.0.0.1 --port=80 --telemetry-host=127.0.0.1 --telemetry-port=81 --collectors=pods
I0318 21:38:15.768770   16454 main.go:236] Using all namespace
I0318 21:38:15.773566   16454 main.go:274] Testing communication with server
I0318 21:38:15.814821   16454 main.go:279] Running with Kubernetes cluster version: v1.9. git version: v1.9.3. git tree state: clean. commit: d2835416544f298c919e2ead3be3d0864b52323b. platform: linux/amd64
I0318 21:38:15.814903   16454 main.go:281] Communication with server successful
I0318 21:38:15.815600   16454 pod.go:198] collect pod with v1
I0318 21:38:15.815626   16454 main.go:290] Starting kube-state-metrics self metrics server: 127.0.0.1:18081
I0318 21:38:15.815707   16454 main.go:360] Active collectors: pods
I0318 21:38:15.815714   16454 main.go:315] Starting metrics server: 127.0.0.1:18080
I0318 21:38:15.816033   16454 reflector.go:202] Starting reflector *v1.Pod (5s) from k8s.io/kube-state-metrics/collectors/collectors.go:65
I0318 21:38:15.816086   16454 reflector.go:240] Listing and watching *v1.Pod from k8s.io/kube-state-metrics/collectors/collectors.go:65
I0318 21:38:20.860003   16454 reflector.go:286] k8s.io/kube-state-metrics/collectors/collectors.go:65: forcing resync
I0318 21:38:25.860295   16454 reflector.go:286] k8s.io/kube-state-metrics/collectors/collectors.go:65: forcing resync
I0318 21:38:30.860556   16454 reflector.go:286] k8s.io/kube-state-metrics/collectors/collectors.go:65: forcing resync
I0318 21:38:35.860835   16454 reflector.go:286] k8s.io/kube-state-metrics/collectors/collectors.go:65: forcing resync

```
I have been running patched kube-state-metrics in production for a couple of days and didn't notice any deviation in reported data. So, it seems that the patch works. However, I would like somebody to double check my findings. I'm also not sure that the patch is the best possible solution.

Base on my observation of how kube-state-metrics can be wrong in production, I think this is an important bug to get the fix soon. There might be also logical problems in client-go. At least the need to create a handler for an informer to periodically resync seems questionable for me. But, I'm not a big expert so I'm not going to judge much here.",closed,False,2018-03-20 21:52:34,2018-04-11 21:30:38
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/403,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/403,collectors: Add NOP eventhandler to ensure we perform resyncs,"Fixes #402 

@andyxning ",closed,True,2018-03-21 15:52:47,2018-04-28 16:09:04
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/404,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/404,CHANGELOG.md: Add v1.3.0-rc.0 entry,"We've accumulated a number of features over the past month, and since we've traditionally cut releases right around Kubernetes releases, here it goes.

I encourage everyone to test this so we can find bugs, should there be any new ones introduced.

@andyxning ",closed,True,2018-03-23 14:29:02,2018-03-26 12:04:18
kube-state-metrics,r4j4h,https://github.com/kubernetes/kube-state-metrics/issues/405,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/405,NodeNetworkUnavailable missing from kube_node_status_condition,"
According to the Changelog version [1.0.0-rc1](https://github.com/kubernetes/kube-state-metrics/blob/aaf8e2fa716010762a30d675e06fdcb68f942040/CHANGELOG.md#v100-rc1--2017-08-02) several metrics, including `kube_node_status_network_unavailable`, were collapsed into the metric `kube_node_status_condition`.

Initially reported in #258, `NodeNetworkUnavailable` [exists in this project's tests](https://github.com/kubernetes/kube-state-metrics/blob/master/collectors/node_test.go#L206) but in my instances of kube-state-metrics the condition=""NetworkUnavailable"" never appears.

It seems like it was intended for this to be present through the new metric but a regression was introduced around the `kube_node_status_condition` merger so this Issue is about restoring it.

I send my metrics into Prometheus, the following PromQL query:

`sum by(condition) (kube_node_status_condition{job=""metrics/prometheus-kube-state-metrics""})`

results in:

```
{condition=""MemoryPressure""} | 10
{condition=""OutOfDisk""} | 10
{condition=""Ready""} | 10
{condition=""DiskPressure""} | 10
```

Based on conversation in #258 this may be from upstream Kubernetes getting rid of the metric and kube-state-metrics simply relaying what Kubernetes gives. If that is the case, then the test code is indeed just from the legacy days when it was provided and could be replaced with a current one like OutOfDisk or just left as is, letting these issues inform anyone in the future of what happened.

I am thinking this is likely the case as my nodes give the following statuses:

```
status:
  conditions:
  - lastHeartbeatTime: 2018-03-23T23:50:08Z
    lastTransitionTime: 2018-03-23T14:04:28Z
    message: kubelet has sufficient disk space available
    reason: KubeletHasSufficientDisk
    status: ""False""
    type: OutOfDisk
  - lastHeartbeatTime: 2018-03-23T23:50:08Z
    lastTransitionTime: 2018-03-23T14:04:28Z
    message: kubelet has sufficient memory available
    reason: KubeletHasSufficientMemory
    status: ""False""
    type: MemoryPressure
  - lastHeartbeatTime: 2018-03-23T23:50:08Z
    lastTransitionTime: 2018-03-23T14:04:28Z
    message: kubelet has no disk pressure
    reason: KubeletHasNoDiskPressure
    status: ""False""
    type: DiskPressure
  - lastHeartbeatTime: 2018-03-23T23:50:08Z
    lastTransitionTime: 2018-03-23T14:04:48Z
    message: kubelet is posting ready status
    reason: KubeletReady
    status: ""True""
    type: Ready
```

Notably all those reported from my kube-state-metrics are present, but networking is not.

So things likely are working as intended, but I wanted to submit this Issue for closure on the matter from #258 because I let it go stale and don't have permissions to reopen it.
",closed,False,2018-03-23 23:56:13,2018-03-27 06:30:46
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/406,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/406,fix cronjob parse,"This should fix cronjob schedule parse for next schedule time calculation.

Sorry for changing this  logic over and over again. But, it should work now. 

The reason for the next schedule time ut test is not idempotent in all time zone is that:
* the time for the same unix timestamp is different in different time zone
* the cronjob is parsed in local time zone 

 For example, if the schedule cronjob config is `0 */6 * * *` and the last scheduled time zone is . In GMT+8, i.e., ""Asia/Shanghai"". 
* schedule time in local: ""2018-03-11 12:34:56 +0800 CST""
* next schedule time in local: ""2018-03-11 18:00:00 +0800 CST""
* next schedule time in utc: ""1.5207624e+09""

Whereas in the time zone is UTC, i.e., travis CI
* schedule time in local: ""2018-03-11 04:34:56 +0000 UTC""
* next schedule time in local: ""2018-03-11 06:00:00 +0000 UTC""
* next schedule time in utc: ""1.520748e+09""

As for now, [cronjob always use five elements tuple to represent the schedule cron config](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#creating-a-cron-job), kube-state-metrics should be in consensus.",closed,True,2018-03-26 06:18:22,2018-03-26 11:37:49
kube-state-metrics,pvanderlinden,https://github.com/kubernetes/kube-state-metrics/issues/407,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/407,No metric for pods which are force deleted/stopped,"When you delete a pod, or it get's deleted due to a new version, sometimes containers don't shutdown properly, and just hang. Kubernetes will force kill those when this happens after a grace period.

It would be good to have metrics on this, so it can be addressed, but I can't find anything like that, including `kube_pod_container_status_terminated_reason` which doesn't seem to know this. Is there anything like this, or is this a missing feature?",closed,False,2018-03-27 11:03:57,2018-08-08 11:00:38
kube-state-metrics,derekwaynecarr,https://github.com/kubernetes/kube-state-metrics/issues/408,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/408,pod collector should surface all request and limit resources,"The pod collector exposes a named metric for each standard resource.

The pod collector should support the ability to expose metrics for all request and limits on a pod/container including extended resources.  This is important for handling resources that are introduced via device plugins.

@brancz - this was a topic that came up in resource mgmt f2f discussion.  we want to engage with sig-instrumentation to find the right approach.",closed,False,2018-03-30 17:17:31,2018-05-24 13:56:33
kube-state-metrics,r0fls,https://github.com/kubernetes/kube-state-metrics/issues/409,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/409,"Cronjob support, docs question","Is this section of the docs still correct?
>If users want to enable this feature when kubernetes version larger than 1.7, It must be configured, with the following parameter setting for apiserver. --runtime-config=batch/v2alpha1=true

It seems that given https://github.com/kubernetes/kube-state-metrics/commit/20c6b1f78a1174cbd95272bc72183d6f96c64db7

Enabling `batch/v2alpha1` in the api server is no longer needed.",closed,False,2018-04-02 18:14:59,2018-04-04 05:59:08
kube-state-metrics,spirrello,https://github.com/kubernetes/kube-state-metrics/issues/410,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/410,Not recognizing pods in an Unknown state,"Hi,

We had two pods in our cluster go into an unknown state however kube-state-metrics isn't recognizing this.  See below:

Output from curl and kubectl:

kube_pod_status_phase{namespace=""kube-system"",phase=""Unknown"",pod=""filebeat-ht37s""} 0
kube_pod_status_phase{namespace=""kube-system"",phase=""Unknown"",pod=""filebeat-j1ppj""} 0

kubectl get pods -n kube-system | grep Unknown
filebeat-ht37s                              1/1       Unknown   3          103d
filebeat-j1ppj                              1/1       Unknown   5          103d


Kube-state-metrics version: 1.2
Docker version: 1.12
K8s version: 1.5.4
OS flavor: Ubuntu 16.04 LTS",closed,False,2018-04-03 12:52:27,2018-04-28 01:23:49
kube-state-metrics,r0fls,https://github.com/kubernetes/kube-state-metrics/pull/411,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/411,update readme for cronjob collector,fix #409 ,closed,True,2018-04-03 18:01:47,2018-04-07 14:31:43
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/412,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/412,Cut v1.3.0 stable,"After a period of 12 days of testing the release candidate, no bugs were reported, so we're promoting it to be the stable release.

Once merged, I will push the new image to the quay.io repository. @piosz and or @loburm please stand by for the gcr.io image.

@andyxning 

/cc @fabxc @ant31 @mxinden @derekwaynecarr @ironcladlou ",closed,True,2018-04-04 09:15:54,2018-04-12 11:45:46
kube-state-metrics,pdecat,https://github.com/kubernetes/kube-state-metrics/pull/413,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/413,Add permissions to list and watch ConfigMaps and Secrets,"Required since https://github.com/kubernetes/kube-state-metrics/pull/379

Without that, the following errors are triggered:

```
E  k8s.io/kube-state-metrics/collectors/collectors.go:62: Failed to list *v1.Secret: secrets is forbidden: User ""system:serviceaccount:monitoring:kube-state-metrics"" cannot list secrets at the cluster scope: Unknown user ""system:serviceaccount:monitoring:kube-state-metrics"" 
E  k8s.io/kube-state-metrics/collectors/collectors.go:62: Failed to list *v1.ConfigMap: configmaps is forbidden: User ""system:serviceaccount:monitoring:kube-state-metrics"" cannot list configmaps at the cluster scope: Unknown user ""system:serviceaccount:monitoring:kube-state-metrics"" 
```

Note: here, kube-state-metrics is deployed in a custom `monitoring` namespace.",closed,True,2018-04-04 09:31:38,2018-04-04 13:09:18
kube-state-metrics,dlespiau,https://github.com/kubernetes/kube-state-metrics/issues/414,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/414,kube-state-metrics keeps failed pods around for kube_pod_* metrics,"Some `kube_pod_*` metrics exist for Failed pods:
```
kube_pod_info
kube_pod_owner
kube_pod_labels
kube_pod_created
kube_pod_start_time
```

`kube_status_phase` shows that kube-state-metrics is populating this group of metrics with information about the failed (non running) pods:
![screenshot from 2018-04-04 13-35-16](https://user-images.githubusercontent.com/7986/38307913-46b05688-380d-11e8-93d2-a62c434891ee.png)

`kubectl` is indeed confirming there's only one pod running (and so are the `kube_pod_container_*` metrics).

I would expect the metrics above to only show `Pending|Running` pods, maybe `Unknown` but I'm not sure when the last value is set and seems like it could be a transient state more than anything else.",closed,False,2018-04-04 12:50:15,2018-04-12 09:21:12
kube-state-metrics,dohnto,https://github.com/kubernetes/kube-state-metrics/pull/415,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/415,Refactor e2e #1,,closed,True,2018-04-05 15:12:28,2018-04-07 17:23:54
kube-state-metrics,dannyk81,https://github.com/kubernetes/kube-state-metrics/issues/416,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/416,kms 1.3.0 fails to parse certificate due to Go version,"Similar to https://github.com/prometheus/prometheus/issues/3983

For certificates with non-fqdn values in dnsName fields, applications built with certain Go versions (1.9.<something> -> 1.10) will fail to communicate with the apiserver.

```
F0406 16:58:08.867531       1 main.go:245] Failed to create client: ERROR communicating with apiserver: Get https://10.82.16.1:443/version: tls: failed to parse certificate from server: x509: cannot parse dnsName ""K8s foo bar""
```

Since ""K8s foo bar"" is not an fqdn, tls can't parse it won't allow to establish a connection.

This issue was fixed/mitigated in Go 1.10.1
https://groups.google.com/forum/m/#!msg/golang-announce/IkPkOF8JqLs/TFBbWHJYAwAJ",closed,False,2018-04-06 17:08:28,2018-04-13 18:49:40
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/417,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/417,fix kubernetes accelerator deprecation procedure,"Fix https://github.com/kubernetes/kube-state-metrics/pull/411#discussion_r179186482
>>kubernetes accelerator feature support is deprecated in version v1.11.

> Kubernetes accelerator support is already deprecated in 1.10. The PR to completely remove it is merged in master, so it will be completely removed in 1.11.



/cc @mindprince @brancz ",closed,True,2018-04-07 14:31:16,2018-07-05 12:19:51
kube-state-metrics,dohnto,https://github.com/kubernetes/kube-state-metrics/pull/418,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/418,Extend e2e tests,"Addressing #290. I tried to made commits readable as possible, can squash if prefered.

* I refactored e2e testsuite allowing user to overwrite whether he wants to install minikube & kubectl and also choose minikube's driver (locally when testing I want to use virtualbox and I don't want the script to download minikube and kubectl over and over to my system path).

* I added `cronjob daemonset hpa job limitrange persistentvolumeclaim replicationcontroller resourcequota statefulset` resources which are created

* I added a simple check that expected metrics are exposed

",closed,True,2018-04-07 17:02:08,2018-04-18 05:18:50
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/419,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/419,use alpine as base image,"This PR will move the base image for kube-state-metrics from `scratch` to `alpine`. Alpine has many useful binaries, such as `sh`. This will be helpful when we need to `exec` into an running kube-state-metrics container and run some debug command.

Last but not the least is that alpine image is small enough thus the kube-state-metircs image will not increase much.",closed,True,2018-04-08 06:12:43,2018-04-10 02:26:06
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/420,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/420,fix pod unknown stats,"Fix #410 

This PR will adds another pod unknown situation where `kubectl` uses to mark a pod in Unknown state.",closed,True,2018-04-08 10:58:40,2018-04-10 02:27:50
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/421,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/421,add go version check,"Fix #416 

This PR will ad a go version check for `make build` to help us mitigate the problem encountered in https://github.com/kubernetes/kube-state-metrics/issues/416",closed,True,2018-04-09 02:22:56,2018-04-12 02:22:42
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/422,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/422,use golang image to compile ksm,"Fix #421

This PR will add support for using golang container to compile kube-state-metrics.",closed,True,2018-04-11 03:32:14,2018-04-11 16:16:51
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/423,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/423,Add note about kube-state-metrics exposing raw data,"This closes #414

cc @andyxning ",closed,True,2018-04-11 08:41:23,2018-04-12 09:18:56
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/424,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/424,pin minikube v0.25.2,Minikube has released v0.26.0 and it seems that e2e test for kube-state-metrics is not compatible with the latest minikube release. Let's first pin minikube version to v0.25.2 and file an issue in minikube to track this.,closed,True,2018-04-11 09:34:00,2018-04-11 15:13:41
kube-state-metrics,discordianfish,https://github.com/kubernetes/kube-state-metrics/pull/425,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/425,Bump golang version to 1.10.1,"Fixes #257 (probably)

I've tried debugging #257 by rebuilding ksm with some more debugging options but after building it with go1.10.1

- memory usage dropped from 300MB-1Gi to pretty stable 55-65MB
- scrape duration went from 3s-timeout to 0.3s-1s.",closed,True,2018-04-11 11:07:25,2018-04-11 15:35:01
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/426,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/426,force bootstrapper to localkube for minikube,Force minikube use localkube as the bootstrapper instead of kubeadm. For more info: https://github.com/kubernetes/minikube/issues/2704,closed,True,2018-04-11 16:10:40,2018-07-11 08:52:23
kube-state-metrics,ayushpateria,https://github.com/kubernetes/kube-state-metrics/pull/427,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/427,Add total resources scraped metric,This adds `ksm_resources_scraped_total` which counts total resources scraped since the start of the metrics server.,closed,True,2018-04-11 18:42:16,2018-09-16 16:42:17
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/428,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/428,CHANGELOG.md: Add entry for v1.3.1,"@andyxning 

/cc @discordianfish 

@piosz and @loburm stand by to push the new image to v1.3.1",closed,True,2018-04-12 07:24:40,2018-04-12 08:55:31
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/429,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/429,Cut 1.3.1,"@andyxning 

/cc @dohnto @discordianfish

@piosz @loburm please stand by to push the v1.3.1 image to gcr",closed,True,2018-04-12 10:05:57,2018-04-21 12:15:48
kube-state-metrics,elcuyi,https://github.com/kubernetes/kube-state-metrics/pull/430,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/430,Expose Pod Annotations as Prometheus labels,,closed,True,2018-04-13 10:53:53,2019-01-06 11:07:30
kube-state-metrics,ryanmcnamara,https://github.com/kubernetes/kube-state-metrics/issues/431,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/431,Job success and fail metrics don't appear if kube-state-metrics was offline,"If a k8s job is created + succeeds or fails while kube-state-metrics is offline, kube-state-metrics will not increment eith the `kubernetes_state.job.succeeded` or the `kubernetes_state.job.failed` metric, even after kube-state-metrics comes back online. The jobs still exist as ""completed"" in k8s.

Additionally, the same thing happens if a job is created and completes very quickly (suppose it is created and completed 10s later), under certain circumstances. I believe this is related to kube-state-metrics (default) 20s poll",closed,False,2018-04-14 18:00:40,2018-05-02 11:05:39
kube-state-metrics,jmcarp,https://github.com/kubernetes/kube-state-metrics/pull/432,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/432,Clarify apiserver override example.,"* Drop `--in-cluster` flag, which doesn't exist as of latest release
* Pass `--apiserver` flag to match instructions",closed,True,2018-04-15 22:00:04,2018-04-16 02:29:59
kube-state-metrics,mwhittington21,https://github.com/kubernetes/kube-state-metrics/pull/433,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/433,Adds time_to_schedule and time_to_start metrics,"This PR adds two metrics with the aim of tracking the beginning of the pod lifecycle better. They are named `kube_pod_time_to_schedule` and `kube_pod_time_to_start`.

I tried to get the results I wanted by just adding/subtracting various existing metrics in Prometheus but there's a bit of conditional logic required to make the most sense out of the created vs started timestamps for a pod object.",closed,True,2018-04-16 01:12:39,2018-04-21 12:38:00
kube-state-metrics,ewbankkit,https://github.com/kubernetes/kube-state-metrics/pull/434,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/434,Fix documentation typo 'Heaspter' -> 'Heapster',,closed,True,2018-04-16 19:36:55,2018-04-16 19:49:59
kube-state-metrics,discordianfish,https://github.com/kubernetes/kube-state-metrics/issues/435,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/435,Add HPA conditions,Similar to `kube_node_status_condition` there should be `kube_hpa_status_condition` exposing the conditions of the HPA.,closed,False,2018-04-17 09:28:19,2018-04-18 15:36:05
kube-state-metrics,discordianfish,https://github.com/kubernetes/kube-state-metrics/pull/436,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/436,Add hpa conditions,"Fixed after discussion in Slack, thanks to @DirectXMan12 
This now uses a explicitly versioned client as recommended anyways.

---
Not working yet, need to understand the versioning and would welcome somebody pointing me in the right direction to solve this in a backward compatible way. The HPA conditions are part of the HPA status since v2beta1. When I update the import though, I get this error:

```
E0417 12:51:00.051078   17716 reflector.go:386] k8s.io/kube-state-metrics/collectors/collectors.go:62: expected type *v2beta1.HorizontalPodAutoscaler, but watch event object had type *v1.HorizontalPodAutoscaler
```",closed,True,2018-04-17 10:51:24,2018-04-19 09:27:53
kube-state-metrics,dohnto,https://github.com/kubernetes/kube-state-metrics/pull/437,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/437,Updated changelog according to changes in #418,Updated changelog according to changes in #418 ,closed,True,2018-04-18 05:13:09,2018-04-18 05:39:01
kube-state-metrics,cofyc,https://github.com/kubernetes/kube-state-metrics/pull/438,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/438,Move gatherAndCompare and related code to separate package and expose it,"hi, 

`gatherAndCompare` function is really useful to test `prometheus.Collector`.  I have used it [in kubelet collectors](https://github.com/kubernetes/kubernetes/pull/59170/files#diff-18db6236f1e4cf727350b7f4dafa9e50R17). Now I am working on exposing metrics in local volume provisioner: https://github.com/kubernetes-incubator/external-storage/pull/721. I suggest to move `gatherAndCompare` to separate package and expose it. Then I can vendor it instead of copying.
What do you think?",closed,True,2018-04-18 06:44:55,2018-07-03 02:31:47
kube-state-metrics,pdecat,https://github.com/kubernetes/kube-state-metrics/pull/439,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/439,Fix markdown rendering of possible reason tag values,"The rendered lists of possible reason tag values are truncated after the first element for the `kube_pod_container_status_terminated_reason` and `kube_pod_container_status_waiting_reason` metrics.

The pipe character needs escaping as already done for other metrics, e.g. `kube_pod_status_phase`.",closed,True,2018-04-18 12:16:58,2018-04-18 12:31:00
kube-state-metrics,andrewsykim,https://github.com/kubernetes/kube-state-metrics/issues/440,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/440,pod resource metrics should account for pod conditions,"I have a cluster where kube-state-metrics reports that the total requested memory on a node is greater than the allocatable memory of the node. 
```
sum(kube_pod_container_resource_requests_memory_bytes{node=""node-name""}) >
sum(kube_node_status_capacity_memory_bytes{node=""node-name""})
```

From a quick glance at the code it seems like kube-state-metrics collects the requested memory of a pod regardless of its condition (running, completed, evicted). So in the event a kubernetes job that requests a lot of resources leaves pods in the `Completed` state, the memory used by those pods are included in `kube_pod_container_resource_requests_memory_bytes{node=""node-name""}` until Kubernetes decides to delete the pod resource. 

I think the correct behaviour is to either only collect pods that are running or expose the pod conditions as labels. Happy to help put in the work to get this fixed once we agree on what is the best step forward. ",closed,False,2018-04-19 16:11:01,2018-04-23 15:19:19
kube-state-metrics,discordianfish,https://github.com/kubernetes/kube-state-metrics/pull/441,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/441,Fix hpa condition labels,The labels were switched accidentially. This fixes it.,closed,True,2018-04-24 17:11:46,2018-04-24 20:59:13
kube-state-metrics,ZhiqinYang,https://github.com/kubernetes/kube-state-metrics/pull/442,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/442,add pod resource ,"sometimes we only need to pod resource,  but the container resources for limit cannot get when we don't add limit in yaml.",closed,True,2018-04-26 03:07:57,2018-07-02 03:27:11
kube-state-metrics,unterstein,https://github.com/kubernetes/kube-state-metrics/issues/443,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/443,Is v1.3.1 officially released?,"Hey 👋 

I am fairly enjoying this image released on `https://quay.io/repository/coreos/kube-state-metrics?tag=latest&tab=tags`, thank you very much! I wondered if version 1.3.1 is officially released, because it is not tagged as `latest` in the image repo and not declared as latest version in the `latest version` section of the readme. If it is officially released, would it be possible to also tag the v1.3.1 as latest? This would be awesome!

Thanks :)",closed,False,2018-04-26 14:03:34,2018-05-28 11:30:32
kube-state-metrics,unterstein,https://github.com/kubernetes/kube-state-metrics/pull/444,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/444,Update README.md to reference 1.3.1 as latest release,As discussed in #443 version 1.3.1 is the latest official release and should be referenced in the README.md,closed,True,2018-04-26 14:17:19,2018-04-26 14:50:27
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/445,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/445,add support for quay latest image tag,"Partially fix #443 

This PR adds two functionalities:
* add the new `quay-push` target to push images to quay.io
* add the support for tag the `latest` tag when release with a new version

BTW, i do not think it is the right way to use `latest` tag for any docker images. But since kube-state-metrics has been released under support for `latest` tag, we should keep this release procedure.",closed,True,2018-04-27 08:48:12,2018-04-27 10:01:33
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/446,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/446,refactor to toward kubernetes way,"This PR will refactor kube-state-metrics to be a more kubernetes way. Mainly:
* move `collectors`/`version` packages under `pkg` package
* refactor flag command arguments configuration to `options` package",closed,True,2018-04-27 10:28:57,2018-05-15 15:04:10
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/447,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/447,add ISSUE and pr guildline,/cc @andyxning ,closed,True,2018-04-30 13:27:35,2018-04-30 16:06:17
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/448,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/448,update Compatibility matrix,,closed,True,2018-04-30 13:37:34,2018-05-17 02:00:50
kube-state-metrics,doktoroblivion,https://github.com/kubernetes/kube-state-metrics/issues/449,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/449,kube-state-metrics support for oidc authentication providers," /kind feature

**What happened**:
Attempting to deploy kube-state-metrics container into our environment the function threw and error, which at first I took to be a BUG, but am thinking its probably a FEATURE request now after going through much of the code.  The message received when launching the container:
```
$ kube-state-metrics --port=8080 --telemetry-port=8081 --kubeconfig=/home/egriffin/.bluemix/plugins/container-service/clusters/apiconnect-dev-1/kube-config-dal10-apiconnect-dev-1.yml
I0430 13:18:38.117467   17878 main.go:225] Using default collectors
I0430 13:18:38.117574   17878 main.go:239] Using all namespace
F0430 13:18:38.120900   17878 main.go:248] Failed to create client: No Auth Provider found for name ""oidc""
```
**What you expected to happen**:
I expected the container to just launch, but I see there is no reference to the *client-go/plugin/pkg/client/auth/oidc* plugin in the code (main.go) or called routines within client-go.

**How to reproduce it (as minimally and precisely as possible)**:
Using a Bluemix or similar environment build and deploy the container within it using the provided configuration file, which will contain something like the following:
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority: ca-dal10-apiconnect-dev-1.pem
    server: https://***.**.***.***:22998
  name: apiconnect-dev-1
contexts:
- context:
    cluster: apiconnect-dev-1
    namespace: default
    user: *******@us.ibm.com
  name: apiconnect-dev-1
current-context: apiconnect-dev-1
kind: Config
preferences: {}
users:
- name: *******@us.ibm.com
  user:
    auth-provider:
      config:
        client-id: bx
        client-secret: bx
        id-token: ******************
        idp-issuer-url: https://iam.ng.bluemix.net/kubernetes
        refresh-token: ********************
      name: oidc
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:55:54Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.11-2+fa6873d3e386d7"", GitCommit:""fa6873d3e386d7ead42923b24aea3b76e74395a3"", GitTreeState:""clean"", BuildDate:""2018-04-17T08:10:40Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kube-state-metrics image version
```
v1.3.0 / 2018-04-04
```
",closed,False,2018-04-30 19:07:38,2018-05-06 16:22:51
kube-state-metrics,luarx,https://github.com/kubernetes/kube-state-metrics/issues/450,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/450,Differences between metrics-server repo and kube-state-metrics repo,"I know that this is not a bug neither a request but I don't find a forum where I can ask this question,

Maybe it's a stupid question but... what is the differences between this repo and kubernetes/kube-state-metrics (https://github.com/kubernetes/kube-state-metrics/) repo?",closed,False,2018-05-02 23:56:01,2018-05-03 15:16:14
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/451,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/451,add generic pod resource request and limit,"**What this PR does / why we need it**:
This PR
* Adds `disable-pod-none-generic-resource-metrics` command line argument to kube-state-metrics to disable old not labeled resource metrics.
* Adds generic new `kube_pod_container_resource_requests ` and `kube_pod_container_resource_limits `.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #408 

",closed,True,2018-05-03 10:52:35,2018-05-24 15:33:09
kube-state-metrics,doktoroblivion,https://github.com/kubernetes/kube-state-metrics/pull/452,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/452,kube-state-metrics support for oidc authentication providers #449,"**What this PR does / why we need it**:
It fixes an issue with auth provider plugins for oidc

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
https://github.com/kubernetes/kube-state-metrics/issues/449

",closed,True,2018-05-04 17:59:50,2018-05-06 16:22:51
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/453,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/453,update kube-state-metrics version to v1.3.1,"**What this PR does / why we need it**:
This PR updates the kube-state-metrics version to v1.3.1

",closed,True,2018-05-05 00:51:53,2018-05-07 02:19:46
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/454,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/454,add support for differenct auth provider,"**What this PR does / why we need it**:
This PR adds support for all auth providers to kube-state-metrics.
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #452 
Fixes #449 

",closed,True,2018-05-06 03:36:21,2018-05-07 02:20:25
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/455,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/455,Release 1.3,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #443
 
",closed,True,2018-05-07 15:01:03,2018-05-28 11:30:32
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/456,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/456,fix collectors argument panic,"**What this PR does / why we need it**:
This PR fixes panics when `--collectors` command line argument is specified. This is a regression bug in #446.

This PR adds unit tests for command line argument parse and `CollectorSet` `Set` method. 
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #446 

",closed,True,2018-05-16 04:13:55,2018-05-16 15:17:20
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/issues/457,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/457,Add support for custom resource definition,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
As with the popular about operators, CRDs are used more and more frequently in Kubernetes. We should try to add support for collecting metrics about CRDs.

But, as CRDs are defined by different users and may vary from user to user, we may need to extract some common metrics or fields.

Maybe we should work with people from https://github.com/operator-framework as it may become  the de facto sdks to develop operators.

I am not quite sure what to do for now, just propose this issue and hope to learn some suggestions from others.",closed,False,2018-05-17 12:59:38,2018-08-16 11:44:42
kube-state-metrics,ptagr,https://github.com/kubernetes/kube-state-metrics/issues/458,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/458,kube_pod_container_resource_requests_cpu_cores shows evicted pods ,"Just noticed that the `kube_pod_container_resource_requests_cpu_cores` shows `evicted` pods causing issues in trying to monitor the status of just running pods. 

I tried looking at `kube_pod_labels` to see if I can ignore `evicted` pods but that does not show it as well. ",open,False,2018-05-21 01:12:25,2019-04-05 08:26:51
kube-state-metrics,raja-gola,https://github.com/kubernetes/kube-state-metrics/issues/459,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/459,Kube-state-metrics:  http: proxy error: context canceled,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug



**What happened**:
Seeing following error log in quay.io/coreos/kube-rbac-proxy:v0.3.0
```
$ kubectl --context=prod  logs kube-state-metrics-2405493521-7d362 -c kube-rbac-proxy-main
2018/05/22 05:35:26 http: proxy error: context canceled
2018/05/22 05:35:56 http: proxy error: context canceled
2018/05/22 05:36:26 http: proxy error: context canceled
2018/05/22 05:36:56 http: proxy error: context canceled
2018/05/22 05:37:26 http: proxy error: context canceled
2018/05/22 05:37:56 http: proxy error: context canceled
2018/05/22 05:38:26 http: proxy error: context canceled
2018/05/22 05:38:56 http: proxy error: context canceled
2018/05/22 05:39:26 http: proxy error: context canceled
2018/05/22 05:39:56 http: proxy error: context canceled
2018/05/22 05:40:26 http: proxy error: context canceled
2018/05/22 05:40:56 http: proxy error: context canceled
2018/05/22 05:41:26 http: proxy error: context canceled
2018/05/22 05:41:56 http: proxy error: context canceled
2018/05/22 05:42:26 http: proxy error: context canceled
2018/05/22 05:42:56 http: proxy error: context canceled
2018/05/22 05:43:26 http: proxy error: context canceled
2018/05/22 05:43:56 http: proxy error: context canceled
2018/05/22 05:44:26 http: proxy error: context canceled
2018/05/22 05:44:56 http: proxy error: context canceled
2018/05/22 05:45:26 http: proxy error: context canceled
2018/05/22 05:45:56 http: proxy error: context canceled
2018/05/22 05:46:26 http: proxy error: context canceled
2018/05/22 05:46:56 http: proxy error: context canceled
2018/05/22 05:47:26 http: proxy error: context canceled
2018/05/22 05:47:56 http: proxy error: context canceled
2018/05/22 05:48:26 http: proxy error: context canceled
2018/05/22 05:48:56 http: proxy error: context canceled
2018/05/22 05:49:26 http: proxy error: context canceled
2018/05/22 05:49:56 http: proxy error: context canceled
2018/05/22 05:50:26 http: proxy error: context canceled
2018/05/22 05:50:56 http: proxy error: context canceled
```

**What you expected to happen**:
No error logs

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
$ kubectl --context=prod version
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:**""v1.7.8**"", GitCommit:""bc6162cc70b4a39a7f39391564e0dd0be60b39e9"", GitTreeState:""clean"", BuildDate:""2017-10-05T06:54:19Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.8+coreos.0"", GitCommit:""bc6162cc70b4a39a7f39391564e0dd0be60b39e9"", GitTreeState:""clean"", BuildDate:""2017-10-11T20:39:17Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kube-state-metrics image version
v0.3.0
",closed,False,2018-05-22 05:54:34,2018-07-20 21:47:56
kube-state-metrics,piaoyu,https://github.com/kubernetes/kube-state-metrics/issues/460,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/460,NodeLost can't be Feedback to the  pod kube_pod_container_status ,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
> /kind feature


**What happened**:
NodeLost can't be Feedback to the  pod kube_pod_container_status 
```
kube-system           log-sender-4b46h                                        1/1       NodeLost                0          83d
```
**What you expected to happen**:
kube_pod_container_status can check the NodeLost status

**How to reproduce it (as minimally and precisely as possible)**:
make  a node  crash

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
#kubectl version
Client Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.0-dirty-20180502"", GitCommit:""2255ac8bd29ca31e1afa51c542709758af9ff98e"", GitTreeState:""dirty"", BuildDate:""2018-05-02T17:06:36Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.0-dirty-20180502"", GitCommit:""2255ac8bd29ca31e1afa51c542709758af9ff98e"", GitTreeState:""dirty"", BuildDate:""2018-05-02T17:06:36Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}

- Kube-state-metrics image version
kube-state-metrics:v1.3.1
",closed,False,2018-05-23 10:54:42,2018-11-02 14:43:38
kube-state-metrics,while1malloc0,https://github.com/kubernetes/kube-state-metrics/issues/461,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/461,Applying CPU limits to kube-state-metrics causes rapid memory consumption,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
Applying CPU limits to a pod running kube-state-metrics with no namespace flag causes unbounded memory allocation. This continues until the pod is OOM-killed by Kubernetes. This seems to happen reliably whenever CPU limits are applied. With no CPU limits, or with the `--namespace` flag supplied, the pod stays well under its memory limit. I observed this by applying a limit, running `htop`, and watching the process memory grow until it is killed.

The particular cluster is fairly large. Running `kubectl get all --all-namespaces | wc -l` returns 1375, so it seems reasonable that quite a bit memory would have to be allocated, as explained in [this issue](https://github.com/kubernetes/kube-state-metrics/issues/257), but I can't seem to figure out why applying CPU limits would cause such rapid and unbounded memory allocation. 

**What you expected to happen**:
Memory allocation does not grow as rapidly. 

**How to reproduce it (as minimally and precisely as possible)**:
Deploy a Kubernetes Deployment containing CPU resource limits to a cluster. Here's the deployment file that I'm seeing this behavior with: 

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: kube-state-metrics
  replicas: 1
  template:
    metadata:
      labels:
        k8s-app: kube-state-metrics
    spec:
      serviceAccountName: kube-state-metrics
      containers:
      - name: kube-state-metrics
        image: quay.io/coreos/kube-state-metrics:v1.2.0
        ports:
        - name: http-metrics
          containerPort: 8080
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        resources:
          limits:
            # removing this CPU limit reliably solves memory limit issues
            cpu: 200m
            memory: 4Gi
          requests:
            cpu: 200m
            memory: 4Gi
```

**Anything else we need to know?**


**Environment**:
- Kubernetes version (use `kubectl version`): 
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.2"", GitCommit:""81753b10df112992bf51bbc2c2f85208aad78335"", GitTreeState:""clean"", BuildDate:""2018-04-27T09:22:21Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.2"", GitCommit:""81753b10df112992bf51bbc2c2f85208aad78335"", GitTreeState:""clean"", BuildDate:""2018-04-
27T09:10:24Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kube-state-metrics image version: I've reproduced this issue with v1.2.0 and v1.3.0
",closed,False,2018-05-23 22:39:23,2018-07-03 15:26:22
kube-state-metrics,while1malloc0,https://github.com/kubernetes/kube-state-metrics/pull/462,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/462,Add note about CPU limits,"**What this PR does / why we need it**:
This PR adds a note about imposing CPU limitations on kube-state-metrics, and the consequent memory consumption that results. It explains the consequence of imposing too low a CPU limit, the cause of unbounded memory consumption in low-CPU environments, and offer a possible fix to users who are seeing unbounded memory consumption when use kube-state-metrics. This PR was created in response to https://github.com/kubernetes/kube-state-metrics/issues/461 as temporary advice until the recommended resources can be revised. 

Fix #461 ",closed,True,2018-05-24 14:24:29,2018-07-03 15:26:22
kube-state-metrics,jessfraz,https://github.com/kubernetes/kube-state-metrics/issues/463,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/463,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",closed,False,2018-05-24 14:37:02,2018-05-31 09:28:58
kube-state-metrics,mindw,https://github.com/kubernetes/kube-state-metrics/pull/464,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/464,remove namepspace reference from persistentvolumes docs.,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
remove namespace reference from persistentvolumes docs
",closed,True,2018-05-26 17:21:46,2018-05-28 05:32:46
kube-state-metrics,liuzhi1986,https://github.com/kubernetes/kube-state-metrics/issues/465,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/465,deployment state error,"When I create a deployment with 1 replicas. I got this:
```
kube-state-metrics-76cf49fd6f-k2z6j                          2/2       Running   0          11m       10.10.32.10     cn-hangzhou.i-bp10ebr5qg3ycsr8wq19
kube-state-metrics-7c7686879d-4d2bj                          0/2       Pending   0          11m       <none>          <none>
```
My yaml  file like this:
```
apiVersion: apps/v1beta2
# Kubernetes versions after 1.9.0 should use apps/v1
# Kubernetes versions before 1.8.0 should use apps/v1beta1 or extensions/v1beta1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: kube-state-metrics
  replicas: 1
  template:
    metadata:
      labels:
        k8s-app: kube-state-metrics
    spec:
      serviceAccountName: kube-state-metrics
      containers:
      - name: kube-state-metrics
        image: lzcy5151/kube-state-metrics:v1.2.0 
        ports:
        - name: http-metrics
          containerPort: 8080
        - name: telemetry
          containerPort: 8081
        command: [""/kube-state-metrics"", ""--port=8080"", ""--telemetry-port=8081""]
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
      - name: addon-resizer
        image: lzcy5151/addon-resizer-amd64:2.1
        resources:
          limits:
            cpu: 100m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 30Mi
        env:
          - name: MY_POD_NAME            valueFrom:
              fieldRef:                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE            valueFrom:
              fieldRef:                fieldPath: metadata.namespace
        command:          - /pod_nanny
          - --container=kube-state-metrics

```
I try to delete and recreate the deployment or delete the Pending pod, but the pending pod still here.",closed,False,2018-05-28 02:06:42,2019-01-01 02:00:28
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/466,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/466,add generic node resource capacity and allocatable,"**What this PR does / why we need it**:
* Adds `disable-node-none-generic-resource-metrics` command line argument to kube-state-metrics to disable old not labeled resource metrics.
* Adds generic new `kube_node_status_capacity` and `kube_node_status_allocatable`.


",closed,True,2018-05-28 05:17:04,2018-06-08 02:27:23
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/467,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/467,Cherry-Pick: CHANGELOG.md: Add entry for v1.3.1,"**What this PR does / why we need it**:
This PR cherry-pick 1.3.1 release not commit to master branch
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #455 
Fixes #443

",closed,True,2018-05-28 10:01:59,2018-05-29 02:31:54
kube-state-metrics,cabrinoob,https://github.com/kubernetes/kube-state-metrics/issues/468,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/468,kube_pod_container_status_waiting_reason strange behaviour,"Hi,
I'am using prometheus to watch the state of running pods on my k8s cluster. I'm using the `kube_pod_container_status_waiting_reason` metric to do this.

For the test purpose, I create a deployment with a non-existing image in it to force error to raise: 

    $ kubectl run foo --image foo

Then, on my prometheus UI, I launch this query : 

```
kube_pod_container_status_waiting_reason{reason=~""ContainerCreating|CrashLoopBackOff|ErrImagePull|ImagePullBackOff""} > 0
```

during the first minute I have this result : 

```
kube_pod_container_status_waiting_reason{app=""prometheus"",chart=""prometheus-6.3.0"",component=""kube-state-metrics"",container=""foo"",heritage=""Tiller"",instance=""100.97.57.7:8080"",job=""kubernetes-service-endpoints"",kubernetes_name=""my-release-prometheus-kube-state-metrics"",kubernetes_namespace=""prometheus"",namespace=""jung"",pod=""foo-6db855bd79-wb2rs"",reason=""ContainerCreating"",release=""my-release""}
```
So, `kube-state-metric` reports that my pod is in ""ContainerCreating"" state

Then, during about 1 minute I have this result : 

```
kube_pod_container_status_waiting_reason{app=""prometheus"",chart=""prometheus-6.3.0"",component=""kube-state-metrics"",container=""foo"",heritage=""Tiller"",instance=""100.97.57.7:8080"",job=""kubernetes-service-endpoints"",kubernetes_name=""my-release-prometheus-kube-state-metrics"",kubernetes_namespace=""prometheus"",namespace=""jung"",pod=""foo-6db855bd79-wb2rs"",reason=""ErrImagePull"",release=""my-release""}
```
`kube-state-metric` reports that my pod is now in ""ErrImagePull"" state (as expected)

My problem is that this status does not persist more than 1 or 2 minutes, because if I refresh my query, I have a ""no data"" response while my deployment is still in ImagePullBackOff state.

Is it a normal behaviour?

Thank you for your help

",closed,False,2018-05-28 12:49:11,2018-05-28 13:26:39
kube-state-metrics,davidxia,https://github.com/kubernetes/kube-state-metrics/pull/469,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/469,Fix typo in README,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

fixes a small typo in the README",closed,True,2018-05-30 15:54:29,2018-05-30 16:13:32
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/470,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/470,add SECURITY_CONTACTS,"**What this PR does / why we need it**:
This PR adds SECURITY_CONTACTS file.
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #463 

",closed,True,2018-05-31 03:34:02,2018-05-31 09:35:31
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/471,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/471,Dockerfile: Remove `VOLUME` directive.,"The `VOLUME` directive causes Docker to provision host storage to cover
the implicitly requested volume mount. For security reasons this may be
disallowed by Kubernetes clusters, causing kube-state-metrics containers
to fail to be created.

Besides that, this directive does not do anything and to my knowledge
never did.

@andyxning 

cc @ironcladlou ",closed,True,2018-05-31 07:21:24,2018-05-31 09:27:34
kube-state-metrics,Ewocker,https://github.com/kubernetes/kube-state-metrics/issues/472,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/472,kube_node_status_phase not found,"### kind bug

**What happened**:
Could not find metric kube_node_status_phase in metrics

**What you expected to happen**:
I should be there as part of the node-metrics doc shows.

**How to reproduce it (as minimally and precisely as possible)**:
Go to the prometheus UI and type this in the Expression and you will see lots of thing showing up but not this metric.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): 1.9
- Kube-state-metrics image version v1.3.1
",closed,False,2018-06-04 22:39:32,2018-06-05 06:04:08
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/473,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/473,using defined variables instead of magic strings,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

using defined variables instead of magic strings for prometheus metrics lables

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-06-07 02:44:18,2018-06-08 13:33:06
kube-state-metrics,derekwaynecarr,https://github.com/kubernetes/kube-state-metrics/issues/474,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/474,node collector should expose dynamic volume limits,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
1.11 adds alpha support for dynamic volume limits as a counted resource.
https://github.com/kubernetes/kubernetes/pull/64154

**What you expected to happen**:
this value should be exposed via the node collector to support monitoring for exhaustion of volumes.",closed,False,2018-06-07 13:58:34,2018-07-03 08:30:19
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/475,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/475,prefer protobuf instead of just json format,"**What this PR does / why we need it**:
This PR will prefer protobuf format when communicating with apiserver instead of just json.
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #264 

",closed,True,2018-06-08 07:09:40,2018-06-25 09:00:44
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/476,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/476,avoid dirty hack to prometheus client,"**What this PR does / why we need it**:
This PR avoids dirty code change hack to prometheus client.


",closed,True,2018-06-08 08:36:01,2018-06-12 08:56:38
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/477,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/477,add  current status for each metrics,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Added  current status for each metrics，We define three stages for each metric.
Stable | Experimental | Deprecated
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-06-08 14:16:49,2018-06-12 02:52:03
kube-state-metrics,jpds,https://github.com/kubernetes/kube-state-metrics/issues/478,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/478,Jobs reported as restarted in kube_pod_container_status_restarted_total,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

I have a series of cron jobs in Kubernetes, and the following Prometheus rule (taken from here[1]):

```
- alert: PodFrequentlyRestarting
  expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
  for: 10m
  labels:
    severity: warning
  annotations:
    description: Pod {{$labels.namespace}}/{{$labels.pod}} was restarted {{$value}} times within the last hour
    summary: Pod is restarting frequently
```

My jobs spin up, and are fine:

```
$ kubectl get pods -n production-app -w
...
job1-1528893000-pmdxv      0/1       Completed   0          28m
job1-1528804800-vphrp    0/1       Completed   0          1d
job1-1528848000-cg4zn    0/1       Completed   0          12h
job2-1528848000-rnzmg   0/1       Completed   0          12h
job2-1528869600-qqwxd   0/1       Completed   0          6h
...
```

For some reason, alerts for this metric fire even those none of the pods have restarted and all is fine.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): 1.9.3
- Kube-state-metrics image version: 1.3.1

[1]: https://github.com/poseidon/typhoon/blob/0764bd30b5682931ab50348fedfee72798965c51/addons/prometheus/rules.yaml#L296",closed,False,2018-06-13 13:00:50,2018-10-22 13:38:22
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/479,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/479,Information sinking,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Move deprecated info into inner area

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-06-14 22:26:26,2018-06-15 02:59:29
kube-state-metrics,like-inspur,https://github.com/kubernetes/kube-state-metrics/issues/480,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/480,can't see ReplicationController metric,"I have install kubernetes-state-metric v1.3.1 on kubernetes v1.9.5 cluster, but I can't see some metrics like kube_replicationcontroller_*. I don't know why, so need some help. Thank you! @gnufied @ncdc @willb @rphillips @mml ",closed,False,2018-06-20 08:45:07,2018-07-02 02:57:57
kube-state-metrics,trondhindenes,https://github.com/kubernetes/kube-state-metrics/issues/481,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/481,Document resource limit configuration,"/kind feature


**What happened**:
I notice that kube-state-metrics has its on magic in terms of resource governing thru the ""pod-nanny"" sidecar. It's unclear to me how to tune resource settings for kube-state-metrics, since I gather it's not done using the regular `resources` attribute on the deployment.

**What you expected to happen**:
I'd like to find clear documentation on how to set up memory/cpu limits using the supplied kubernetes manifests.
",closed,False,2018-06-22 17:58:03,2018-07-12 08:54:26
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/482,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/482,adjust client-go User-Agent,"**What this PR does / why we need it**:
This RP will customize User-Agent client-go uses. This will make the User-Agent more useful and be in more Kubernetes's format:
`kube-state-metrics/v1.3.0 (linux/amd64) kube-state-metrics/64fdc24`

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-06-24 03:33:29,2018-06-25 09:50:28
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/483,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/483,bump kubernetes to 1.11,"**What this PR does / why we need it**:
This PR will update kubernetes/client-go to v1.11.0.

This PR will also deprecate in-tree nvidia support related alpha metrics.",closed,True,2018-07-02 08:42:04,2018-07-03 08:08:38
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/484,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/484,Add dynamic volume,"**What this PR does / why we need it**:
This PR will adds support for dynamic volume resource in node and pod request and limit level.
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #474 
",closed,True,2018-07-02 10:22:50,2018-08-01 13:19:10
kube-state-metrics,AdamDang,https://github.com/kubernetes/kube-state-metrics/pull/485,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/485,Typo fix:PersistentVolumeClaim->PersistentVolume,"In the file persistentvolume-metrics.md, the title is # PersistentVolumeClaim Metrics, here should be # PersistentVolume Metrics",closed,True,2018-07-06 07:30:28,2018-07-06 07:40:18
kube-state-metrics,WakeupTsai,https://github.com/kubernetes/kube-state-metrics/issues/486,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/486,Report the setting of service,"Hi, is it possible to report the port setting of k8s service? Such like TargetPort and NodePort.
Really thanks.
",closed,False,2018-07-06 09:20:12,2018-12-06 06:54:53
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/487,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/487,collectors: Change job label to job_name,"**What this PR does / why we need it**:

The job label is a reserved label in Prometheus, thus colliding with
this label causes Prometheus to automatically change this label upon
ingestion.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #338 

cc @andyxning @dannyk81 
",closed,True,2018-07-06 12:08:00,2018-07-18 08:54:44
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/488,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/488,Allow white- and blacklisting metrics to be exposed,"**What this PR does / why we need it**:

In large Kubernetes clusters single metrics can have a significant effect on the total number of metrics exposed by a single scrape. This PR allows white or blacklisting metrics to be exposed.

@andyxning ",closed,True,2018-07-06 13:14:59,2018-07-24 07:20:32
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/489,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/489,Upgrade go version,"**What this PR does / why we need it**:

This PR updates to the latest version of go which includes fixes to TLS and strings packages.

@andyxning ",closed,True,2018-07-06 13:18:25,2018-07-07 00:47:20
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/490,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/490,add description about pod nanny,"**What this PR does / why we need it**:
This PR will add description about the `addon-resizer` component.
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #481

",closed,True,2018-07-09 07:24:36,2018-07-13 19:01:44
kube-state-metrics,gottwald,https://github.com/kubernetes/kube-state-metrics/pull/491,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/491,Fix typo in metric description,"**What this PR does / why we need it**:
Fixes a typo in the metric description kube_pod_container_resource_requests_memory_bytes
",closed,True,2018-07-10 20:31:27,2018-07-11 02:22:26
kube-state-metrics,gottwald,https://github.com/kubernetes/kube-state-metrics/pull/492,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/492,Update outdated links in PR template,"**What this PR does / why we need it**:
Update outdated links in PR template
",closed,True,2018-07-10 21:52:13,2018-07-11 02:26:26
kube-state-metrics,directionless,https://github.com/kubernetes/kube-state-metrics/issues/493,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/493,Repeated OOM'ing (perhaps due to a large number of namespaces),"/kind bug

**What happened**:

I'm running kube-state-metrics as part of [kube-prometheus](https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus) but it's repeatedly triggering an `OOMKilled`. 


I suspect this is because of the large number of namespaces we have. Some bits of information:

```
$ kubectl get ns | wc -l
     238

$ kubectl get nodes | wc -l
      47

$ kubectl get pods --all-namespaces | wc -l
    4008

$ kubectl get secrets --all-namespaces | wc -l
    8313

```

The resource request and limits are: `{ ""cpu"": ""188m"",  ""memory"": ""5290Mi"" }`. (Unfortunately, I'm having trouble getting resource utilization before the oom)

**What you expected to happen**:

Not OOM

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.7"", GitCommit:""dd5e1a2978fd0b97d9b78e1564398aeea7e7fe92"", GitTreeState:""clean"", BuildDate:""2018-04-19T00:05:56Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10+"", GitVersion:""v1.10.4-gke.2"", GitCommit:""eb2e43842aaa21d6f0bb65d6adf5a84bbdc62eaf"", GitTreeState:""clean"", BuildDate:""2018-06-15T21:48:39Z"", GoVersion:""go1.9.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kube-state-metrics image version
```
""quay.io/coreos/kube-state-metrics:v1.3.1""
```",closed,False,2018-07-13 18:49:14,2019-03-20 11:12:58
kube-state-metrics,aojea,https://github.com/kubernetes/kube-state-metrics/issues/494,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/494,Missing CONTRIBUTING.md file,"All K8s subrepositories should have a CONTRIBUTING.md file, which at the minimum should point to https://github.com/kubernetes/community/blob/master/contributors/guide/README.md. Care should be taken that all information is in sync with the contributor guide.

Subrepositories may also have contributing guidelines specific to that repository. They should be explicitly documented and explained in the CONTRIBUTING.md

Ref:  https://github.com/kubernetes/community/issues/1832",closed,False,2018-07-16 09:23:35,2018-07-24 07:21:26
kube-state-metrics,tomwilkie,https://github.com/kubernetes/kube-state-metrics/issues/495,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/495,'job' label on Job metrics clashed with Prometheus job label,"See https://github.com/kubernetes-monitoring/kubernetes-mixin/pull/46

Perhaps we should consider changing it to `job_name`?",closed,False,2018-07-17 10:16:19,2018-07-18 08:54:36
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/496,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/496,add CONTRIBUTING.md,"**What this PR does / why we need it**:
This PR adds CONTRIBUTING.md file to kube-state-metrics
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #494 

",closed,True,2018-07-17 10:30:04,2018-07-24 09:31:47
kube-state-metrics,penglongli,https://github.com/kubernetes/kube-state-metrics/issues/497,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/497,Add current resource usage for per container of pod?,"### Description
In a kubernetes cluster, there probably have multiple projects(namespace). 

The cAdvisor can provide the metrics of running containers on every node. **But it will provides all projects' containers' metrics.** I don't want it.

### What i want
I only want get containers' request resource、limit resource、current usage resource from a single namespace.

And kube-state-metrics's metrics don't show the current resource usage.  Can you add the current usage to it?

",closed,False,2018-07-20 09:26:25,2019-03-14 13:54:25
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/issues/498,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/498,Performance Optimization Proposal,"/kind improvement

I have been brainstorming with @brancz on how we can improve kube-state-metrics' performance  in terms of response time and memory usage.

I am welcoming any feedback on this [Optimization Proposal](https://docs.google.com/document/d/1Z1q84680vG56SKDi1AMu0x3lFe4bi40DcpMCQDylyMg/edit?usp=sharing).

Related issues: https://github.com/kubernetes/kube-state-metrics/issues/493 https://github.com/kubernetes/kube-state-metrics/issues/257",closed,False,2018-07-23 17:47:22,2019-01-26 10:29:57
kube-state-metrics,The-smooth-operator,https://github.com/kubernetes/kube-state-metrics/issues/499,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/499,metrics kube_pod_container_resource_* missing for some pods,"/kind bug

**What happened**:
kube-state-metrics is not exposing kube_pod_container_resource_requests_* metrics for every pod, but just for some of them. 
For example, listing pods running on kube-system namespace
```
$ kubectl get pods --namespace=kube-system
NAME                                                  READY     STATUS    RESTARTS   AGE
aws-node-hkmvv                                        1/1       Running   0          24d
aws-node-qt4ff                                        1/1       Running   0          24d
calico-node-dd2kz                                     1/1       Running   0          4d
calico-node-vd75j                                     1/1       Running   0          4d
calico-typha-75667d89cb-jffb6                         1/1       Running   0          4d
calico-typha-horizontal-autoscaler-78f747b679-8jw88   1/1       Running   0          10d
kube-dns-7cc87d595-tfzb9                              3/3       Running   0          25d
kube-proxy-7w9r7                                      1/1       Running   0          24d
kube-proxy-vq9wz                                      1/1       Running   0          24d
kube2iam-565vd                                        1/1       Running   0          11d
kube2iam-5wqr8                                        1/1       Running   0          11d
```

However when querying Prometheus, I only get the information for kube-dns:
```
kube_pod_container_resource_requests_memory_bytes{exported_namespace=""kube-system""}

kube_pod_container_resource_requests_memory_bytes{container=""dnsmasq"",endpoint=""http-metrics"",exported_namespace=""kube-system"",exported_pod=""kube-dns-7cc87d595-tfzb9"",instance=""10.0.0.180:8080"",job=""kube-state-metrics"",namespace=""monitoring"",node=""ip-10-0-1-100.us-west-2.compute.internal"",pod=""kube-state-metrics-59599f757-wpr9d"",service=""kube-state-metrics""} | 20971520
-- | --
kube_pod_container_resource_requests_memory_bytes{container=""kubedns"",endpoint=""http-metrics"",exported_namespace=""kube-system"",exported_pod=""kube-dns-7cc87d595-tfzb9"",instance=""10.0.0.180:8080"",job=""kube-state-metrics"",namespace=""monitoring"",node=""ip-10-0-1-100.us-west-2.compute.internal"",pod=""kube-state-metrics-59599f757-wpr9d"",service=""kube-state-metrics""} | 73400320
kube_pod_container_resource_requests_memory_bytes{container=""sidecar"",endpoint=""http-metrics"",exported_namespace=""kube-system"",exported_pod=""kube-dns-7cc87d595-tfzb9"",instance=""10.0.0.180:8080"",job=""kube-state-metrics"",namespace=""monitoring"",node=""ip-10-0-1-100.us-west-2.compute.internal"",pod=""kube-state-metrics-59599f757-wpr9d"",service=""kube-state-metrics""} | 20971520
```

**What you expected to happen**:
To see kube_pod_container_resource_* metric for all pods in all namespaces

**How to reproduce it (as minimally and precisely as possible)**:
Not sure how to reproduce it from scratch, because also seems no one else is having this problem


**Environment**:
- Kubernetes version (use `kubectl version`): 1.11
- Kube-state-metrics image version: 1.3.1
",closed,False,2018-07-24 12:14:00,2018-07-24 12:45:42
kube-state-metrics,balous,https://github.com/kubernetes/kube-state-metrics/issues/500,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/500,kube_cronjob_next_schedule_time has time in the past,"**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
I've got a cron job with:
- lastScheduleTime: 2018-07-24T00:00:00Z
- schedule: 0 2 * * *
- creationTimestamp: 2018-06-06T13:55:15Z

For this cronjob, current value of *kube_cronjob_next_schedule_time* metric is 1532397600 (now it is 1532433916, so it is in the past). When formated for my timezone (+0200), the time is 2018-07-24 02:00:00 which is equal to the time the job was run tonight.

**What you expected to happen**:
*kube_cronjob_next_schedule_time* should contain time in the future (i.e. 2018-07-25 02:00:00).

**How to reproduce it (as minimally and precisely as possible)**:
It happens to all my cron jobs so I suppose it is enough to create a cronjob and check *kube_cronjob_next_schedule_time*'s value.

**Anything else we need to know?**:
I've also played with another job, which was created 47 days ago and was suspended since the beginning. I unsuspended it, checked the metric and got 47.75 days in the past.

Also note that the cluster's hosts timezone is set to CEST.

**Environment**:
- Kubernetes version (use `kubectl version`): N/A
- Kube-state-metrics image version 1.3.1, 1.2.0 and 1.1.0 (not tested the earlier versions)
",closed,False,2018-07-24 12:20:09,2018-12-23 14:40:29
kube-state-metrics,ehashman,https://github.com/kubernetes/kube-state-metrics/issues/501,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/501,"Consider renaming ""pod"" labels to ""pod_name"" and ""container"" labels to ""container_name""","/kind feature

Currently, the `kube_pod_*` metrics use the label ""pod"" to refer to the name of the pod in the timeseries and ""container"" to refer to container names within the pod.

Cadvisor, however, uses ""pod_name"" to refer to the names of pods and ""container_name"" to refer to names of containers. 

This difference makes it difficult to do joins on KSM metrics and cadvisor metrics, leading me to write queries like this one (which calculates the percentage of real utilization of quota per container -- namespace and podname templated in Grafana):

```
sum(irate(container_cpu_usage_seconds_total{namespace=""$namespace"",pod_name=""$podname"",container_name!=""POD""}[1m])) by (container_name) / sum(label_join(kube_pod_container_resource_requests_cpu_cores{namespace=""$namespace"",pod=""$podname""}, ""container_name"", """", ""container"")) by (container_name)
```

With normalized labels I could just write

```
sum(irate(container_cpu_usage_seconds_total{namespace=""$namespace"",pod_name=""$podname"",container_name!=""POD""}[1m])) by (container_name) / sum(kube_pod_container_resource_requests_cpu_cores{namespace=""$namespace"",pod=""$podname""}) by (container_name)
```

Is it possible to rename the ""pod"" labels to match ""pod_name"" and ""container"" labels to match ""container_name"" to normalize between these different data sources? I can certainly write a patch for this.

**Why propose this here rather than telling cadvisor to match KSM?**

Because e.g. ""container_name"" is a more descriptive label name than using ""container"" to refer to the name of a container :)

**Environment**:
- Kubernetes version (use `kubectl version`):  1.8.7-21 with internal patches 
- Kube-state-metrics image version: 1.2.0",closed,False,2018-07-30 16:19:54,2018-08-23 16:44:55
kube-state-metrics,thedebugger,https://github.com/kubernetes/kube-state-metrics/issues/502,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/502,kube state metrics doesn't report init container pod metrics,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:
Can't see pod container metrics for init containers.

**What you expected to happen**:
Should see pod container metrics for init container like the ""main"" container

**How to reproduce it (as minimally and precisely as possible)**:
deploy a pod with an init container and check for ""kube_pod_container_status_terminated"" metric. It doesn't contain init metrics.

**Anything else we need to know?**:
The [code right](https://github.com/kubernetes/kube-state-metrics/blob/master/pkg/collectors/pod.go#L383) now only looks at ContainerStatus which doesn't provide status for InitContainers

**Environment**:
- All Kubernetes version
- All Kube-state-metrics image version",closed,False,2018-08-01 23:26:01,2019-01-03 08:54:29
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/503,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/503,Cut v1.4.0-rc.0,"Assuming everything goes well, we will be aiming for Monday the 13th of August for the stable 1.4.0 release. I encourage everyone to test this so we can find bugs, should there be any new ones introduced.

/assign @andyxning 

@piosz @loburm please stand by to push the container images to gcr.io once this is merged.",closed,True,2018-08-06 06:57:43,2018-08-13 09:52:27
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/504,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/504,add myself to review,"Recommend yourself as an reviewer

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Recommend yourself as an reviewer

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-08-08 08:51:54,2018-08-10 05:52:24
kube-state-metrics,nicholasgibson2,https://github.com/kubernetes/kube-state-metrics/issues/505,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/505,Missing metric in k8s v1.9.5,"/kind bug

**What happened**:
[Node Metrics](https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/node-metrics.mdl) are not present after upgrading Kubernetes from v1.6.4 -> v1.9.5

**What you expected to happen**:
Node Metrics should be present

**How to reproduce it (as minimally and precisely as possible)**:
view kube-state-metrics endpoint running on K8s v1.9.5

**Anything else we need to know?**:

**Environment**:
- Kubernetes version: v1.9.5
- Kube-state-metrics image version: v1.3.1
",closed,False,2018-08-08 23:18:34,2018-08-09 20:28:51
kube-state-metrics,dohnto,https://github.com/kubernetes/kube-state-metrics/pull/506,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/506,Added init containers pod metrics.,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** 
Fixes #502 

",closed,True,2018-08-11 22:31:17,2018-10-02 21:46:55
kube-state-metrics,WakeupTsai,https://github.com/kubernetes/kube-state-metrics/issues/507,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/507,Show pod uid in the kube_pod_info metrics,"/kind feature

Hi, I'm wondering is it possible to show the pod uid as the label in the kube_pod_info metrics?

Thanks.",closed,False,2018-08-13 04:23:38,2018-08-28 05:06:55
kube-state-metrics,WakeupTsai,https://github.com/kubernetes/kube-state-metrics/pull/508,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/508,Add uid info in kube_pod_info metric,"**What this PR does / why we need it**:
This PR will add pod uid in the kube_pod_info metrics.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #507 
",closed,True,2018-08-13 09:41:36,2018-08-28 05:06:55
kube-state-metrics,loburm,https://github.com/kubernetes/kube-state-metrics/pull/509,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/509,Update release documentation,Adds information related to staging container registry that should be used for release candidate images.,closed,True,2018-08-13 09:50:50,2018-08-16 06:46:17
kube-state-metrics,n-rook,https://github.com/kubernetes/kube-state-metrics/issues/510,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/510,There should be a way to get the Deployment of a ReplicaSet,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

/kind feature

I was looking for a way to connect Pods to their Deployment. I can use the `kube_pod_owner` metric to connect Pods to their owning ReplicaSet. But, I do not see a way to go from a ReplicaSet to its Deployment.

I'm not an expert in either Prometheus or Kubernetes, but I think the best way to do this would be to add a `kube_replicaset_owner` metric similar to `kube_pod_owner`, because that would be consistent with the existing design.

**Environment**:
- Kubernetes version (use `kubectl version`): client v1.9.7, server v1.9.6-gke.2
- Kube-state-metrics image version: quay.io/coreos/kube-state-metrics:v1.3.0
",closed,False,2018-08-13 18:53:53,2018-09-05 14:47:58
kube-state-metrics,vidhy,https://github.com/kubernetes/kube-state-metrics/issues/511,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/511,Wrong metric for kube_daemonset_status_number_misscheduled ,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:
I am able to see the kube_daemonset_status_number_misscheduled metric value is 1 for few of my daemonsets. But when i check the daemonsets and corresponding pods they are running properly on all the worker nodes. 

I guess It is because of the cluster autoscaler. New node gets added and removed dynamically. Probabaly that is not being reflected in the metrics exposed.

Unable to understand what does this metric mean. It would be better if we have explanation on the metrics being exposed.

**What you expected to happen**:
kube_daemonset_status_number_misscheduled  metric value to remain 0

**How to reproduce it (as minimally and precisely as possible)**:
Setup a kube cluster with cluster autoscaler enabled. Have a daemonset. Autoscale the cluster nodes by increasing replicas of any pod. Check the kube_daemonset_status_number_misscheduled  metric.

**Anything else we need to know?**:


**Environment**:
- Kubernetes version (use `kubectl version`):
1.9.6
- Kube-state-metrics image version
v1.2.0
",closed,False,2018-08-14 07:16:29,2018-09-03 03:05:48
kube-state-metrics,vielktus,https://github.com/kubernetes/kube-state-metrics/issues/512,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/512,No Cronjobs metrics were exposed,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug


**What happened**:
I installed kube-state-metrics to my Minikube using prometheus-operator. Prometheus can discover this target. I even ssh to MInikube and do a curl to the target link to get the list of metrics. However, the list returned without any Cronjobs metrics.
Do I need to add some args to running kube-state-metrics container to expose Cronjob metrics ? 

**What you expected to happen**:
Prometheus can scrape Cronjob metrics like this https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/cronjob-metrics.md

**How to reproduce it (as minimally and precisely as possible)**:
```helm install -n kube-prometheus helm/kube-prometheus

**Anything else we need to know?**:
The current version I use in Prometheus Operator for kube-state-metrics is v1.2.0. I tried with 1.3.1 but still no luck

**Environment**:
- Kubernetes version (use `kubectl version`): 1.10
- Kube-state-metrics image version: v1.2.0 and 1.3.1
",closed,False,2018-08-15 04:08:28,2018-08-15 09:30:39
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/513,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/513,main_test.go: Introduce overarching benchmark test,"**What this PR does / why we need it**:

This patch adds a simple go benchmark test, injecting Kubernetes objects
and simulating scrape requests. It uses the Kubernetes client-go fake
client. Alongside comes some refactoring of each collectors structure
using informer factories to be compatible with the fake client.

The patch lays the groundwork to make future performance optimizations
comparable with past versions.

How to run test:
`go test -race -bench  . -memprofile=mem.out -cpuprofile=cpu.out`

Relates to https://github.com/kubernetes/kube-state-metrics/issues/498",closed,True,2018-08-15 12:11:26,2018-08-23 09:38:39
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/issues/514,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/514,introduce CRD service monitor,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature
> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:


**What you expected to happen**:
 If CRD service could be monitored , it could detect the health status of crd to identify problems.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version
",closed,False,2018-08-17 02:51:50,2019-01-14 08:11:47
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/515,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/515,introduce CustomResourceDefinitions monitor target,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
If CRD service could be monitored , it could detect the health status of crd to identify problems.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #https://github.com/kubernetes/kube-state-metrics/issues/514

/cc @brancz @andyxning ",open,True,2018-08-17 02:53:55,2019-01-14 01:46:28
kube-state-metrics,wangxy518,https://github.com/kubernetes/kube-state-metrics/pull/516,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/516,Update CONTRIBUTING.md,the url has been changed.,closed,True,2018-08-17 03:02:20,2018-08-19 09:37:53
kube-state-metrics,Betula-L,https://github.com/kubernetes/kube-state-metrics/issues/517,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/517,Wrong pod status for  kube_pod_status_phase,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

/metrics shows that Pod status is `Running`
```
kube_pod_status_phase{namespace=""dsg"",phase=""Running"",pod=""dsg-040941c4649742d61916-84d9b9bf8f-5f7rn""} 1
kube_pod_status_phase{namespace=""xchat"",phase=""Running"",pod=""xchat-1aba9051c2bc56c8b711-6898df559d-xm46j""} 1
```
Whereas, actual Pod status is `Terminating` or `CrashLoopBackOff`.
```
kubectl get po --all-namespaces|grep -v Running

xchat           xchat-1aba9051c2bc56c8b711-6898df559d-xm46j          0/1       Terminating        1          17d
dsg             dsg-040941c4649742d61916-84d9b9bf8f-5f7rn            0/1       CrashLoopBackOff   33         5h

```
**What you expected to happen**:

kube-state-metric should export Pod status as `Terminating` or `Unknown` rather than `Running`

**How to reproduce it (as minimally and precisely as possible)**:

Create pods with status `Terminating` or `CrashLoopBackOff`.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.1"", GitCommit:""d4ab47518836c750f9949b9e0d387f20fb92260b"", GitTreeState:""clean"", BuildDate:""2018-04-12T14:26:04Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}

Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.2"", GitCommit:""81753b10df112992bf51bbc2c2f85208aad78335"", GitTreeState:""clean"", BuildDate:""2018-04-27T09:10:24Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kube-state-metrics image version

commit 43fcd3cf48f9ab6d0361210c5debb4e2aa1d2fc0 (tag: v1.3.1, origin/release-1.3)
",closed,False,2018-08-17 08:29:16,2018-08-17 09:01:41
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/issues/518,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/518,rename heapster  to metrics-server,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
Heapster is deprecated, Consider using metrics-server and a third party metrics pipeline to gather Prometheus-format metrics instead. Is it necessary?
/cc @brancz 

> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version
",closed,False,2018-08-17 09:54:57,2018-08-28 03:24:46
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/519,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/519,Heapster with supplementary information,"Heapster is deprecated, it need to be updated with supplementary information which would be insteaded by metrics-server.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes https://github.com/kubernetes/kube-state-metrics/issues/518

",closed,True,2018-08-20 01:54:38,2018-08-28 03:24:46
kube-state-metrics,andyxning,https://github.com/kubernetes/kube-state-metrics/pull/520,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/520,add kube_replicatset_owner metric,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
This PR adds `kube_replicaset_owner` metric.
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #510 

",closed,True,2018-08-20 06:41:29,2018-08-27 05:23:31
kube-state-metrics,MIBc,https://github.com/kubernetes/kube-state-metrics/pull/521,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/521,Fix a typo,,closed,True,2018-08-20 07:42:06,2018-08-20 09:47:21
kube-state-metrics,MovieStoreGuy,https://github.com/kubernetes/kube-state-metrics/issues/522,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/522,No metrics being exported,"Hey team,

I have deployed the tag v1.3.1 to my cluster in hope that my datadog deployment will forward metrics, I have v1.1.0 on a different cluster and it is exporting metrics that I want.

What I am testing is that given the 1.3.1 deployment, I should get metrics however I have noticed this:

```
I0820 12:38:28.028289       1 main.go:222] Using default collectors
I0820 12:38:28.028442       1 main.go:236] Using all namespace
W0820 12:38:28.028507       1 client_config.go:529] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0820 12:38:28.030052       1 main.go:274] Testing communication with server
I0820 12:38:28.037109       1 main.go:279] Running with Kubernetes cluster version: v1.9+. git version: v1.9.6-gke.2. git tree state: clean. commit: cb151369f60073317da686a6ce7de36abe2bda8d. platform: linux/amd64
I0820 12:38:28.037142       1 main.go:281] Communication with server successful
I0820 12:38:28.037516       1 pod.go:198] collect pod with v1
I0820 12:38:28.037585       1 main.go:290] Starting kube-state-metrics self metrics server: 0.0.0.0:8081
I0820 12:38:28.037614       1 replicaset.go:73] collect replicaset with extensions/v1beta1
I0820 12:38:28.037665       1 replicationcontroller.go:79] collect replicationcontroller with v1
I0820 12:38:28.037698       1 job.go:108] collect job with batch/v1
I0820 12:38:28.037734       1 hpa.go:73] collect hpa with autoscaling/v1
I0820 12:38:28.037777       1 endpoint.go:71] collect endpoint with v1
I0820 12:38:28.037814       1 cronjob.go:89] collect cronjob with batch/v1beta1
I0820 12:38:28.037855       1 statefulset.go:95] collect statefulset with apps/v1beta1
I0820 12:38:28.037893       1 persistentvolume.go:62] collect persistentvolume with v1
I0820 12:38:28.037942       1 daemonset.go:92] collect daemonset with extensions/v1beta1
I0820 12:38:28.037982       1 limitrange.go:55] collect limitrange with v1
I0820 12:38:28.038008       1 service.go:65] collect service with v1
I0820 12:38:28.038029       1 namespace.go:76] collect namespace with v1
I0820 12:38:28.038065       1 deployment.go:111] collect deployment with extensions/v1beta1
I0820 12:38:28.038172       1 node.go:134] collect node with v1
I0820 12:38:28.038221       1 resourcequota.go:53] collect resourcequota with v1
I0820 12:38:28.038252       1 persistentvolumeclaim.go:75] collect persistentvolumeclaim with v1
I0820 12:38:28.038284       1 secret.go:71] collect secret with v1
I0820 12:38:28.038316       1 configmap.go:55] collect configmap with v1
I0820 12:38:28.038349       1 main.go:360] Active collectors: pods,replicasets,replicationcontrollers,jobs,horizontalpodautoscalers,endpoints,cronjobs,statefulsets,persistentvolumes,daemonsets,limitranges,services,namespaces,deployments,nodes,resourcequotas,persistentvolumeclaims,secrets,configmaps
I0820 12:38:28.038403       1 main.go:315] Starting metrics server: 0.0.0.0:8080
```

After the metric server has started, nothing else appears in the logs unlike my other deployment.
Is this expected or is there anything else I need to do?",closed,False,2018-08-20 12:59:00,2018-08-20 13:38:45
kube-state-metrics,snegivulcan,https://github.com/kubernetes/kube-state-metrics/issues/523,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/523,Missing metrics ( kube_job_* & others ),"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**: BUG REPORT

/kind bug


**What happened**: Several metrics are missing like `kube_job_*` 

**What you expected to happen**: Expect to see all the metrics as documented in the wiki

**How to reproduce it (as minimally and precisely as possible)**: Deployed kube-state-metrics v1.3.1 into GKE ( GCP's managed Kubernetes service ). I used the kubernetes specs as given in this repository, but without Role and Role bindings.


**Anything else we need to know?**:

**Environment**: GKE. 
- Kubernetes version (use `kubectl version`): 1.9.7+
- Kube-state-metrics image version: v1.3.1
",closed,False,2018-08-21 04:52:01,2018-09-03 03:05:12
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/524,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/524,*: cut v1.4.0,"After a testing period of 16 days, there were no additional bugs found or features introduced.

@andyxning @zouyee 

cc @mxinden 

@loburm @piosz please stand by to push the final v1.4.0 image to gcr.io once this PR is merged and the tag is pushed.",closed,True,2018-08-22 11:57:03,2018-08-31 11:12:39
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/525,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/525,bump addon-resizer image version,"

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
bump addon-resizer image version

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-08-23 09:47:20,2018-08-24 01:17:43
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/526,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/526,Merge Release 1.4 back into master,@andyxning @zouyee ,closed,True,2018-08-23 10:00:53,2018-08-27 16:54:31
kube-state-metrics,du2016,https://github.com/kubernetes/kube-state-metrics/issues/527,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/527,user self define labels from resource labels or fields,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:

for example ,some times ,user want to hold pod labels such as a label named product,use it to send mail to email group

**What you expected to happen**:

user can self add  fields or labels to metrics labels

**How to reproduce it (as minimally and precisely as possible)**:
for example ,some times ,user want to hold pod labels,such as a label named product,use it  send mail to email group
",closed,False,2018-08-24 02:39:45,2018-08-28 02:06:39
kube-state-metrics,Kami-no,https://github.com/kubernetes/kube-state-metrics/issues/528,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/528,namespace quota info feature,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature


**What happened**:
There is no info about namespace quota

**What you expected to happen**:
Namespace quota info. It makes capacity planing for namespace simple.",closed,False,2018-08-25 13:46:10,2018-08-26 08:26:08
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/529,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/529,Merge Release 1.4 back into master,"This pull request supersedes #526 .

@andyxning @zouyee 

cc @mxinden ",closed,True,2018-08-27 16:54:07,2018-08-28 03:06:33
kube-state-metrics,2rs2ts,https://github.com/kubernetes/kube-state-metrics/issues/530,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/530,Expose CronJob FailedNeedsStart state in some way,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**: We had a test cluster that was down for a long time due to a power outage, so some CronJobs failed to start and entered a FailedNeedsStart state. It was a while until we noticed this.

**What you expected to happen**: We expected that there would be some metric for this but it seems not. Maybe I am just missing something so please tell me if so.

**How to reproduce it (as minimally and precisely as possible)**: I believe you can repro by creating a cronjob to be run soon, then shutting down your entire control plane until that time passes, but I have not bothered to do that.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): 1.10.5
- Kube-state-metrics image version: 1.3.1
",open,False,2018-08-27 23:04:23,2019-01-22 18:56:43
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/531,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/531,remove unused func,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
remove unused func

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-08-28 08:42:47,2018-08-28 10:51:52
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/532,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/532,Adding test files for local check,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Adding test files for local check

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes https://github.com/kubernetes/kube-state-metrics/issues/533

",closed,True,2018-08-29 02:26:02,2019-03-08 13:34:25
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/issues/533,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/533,add prepare script for local test,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->
If we would like to test ksm in local env, test steps need to be done locally.

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
/kind feature


**What happened**:

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version
",closed,False,2018-08-29 02:28:57,2019-01-26 04:48:46
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/534,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/534, pkg/collectors: Introduce MetricsStore updated by Reflectors,"**What this PR does / why we need it**:

As discussed in #498 kube-state-metrics faces the following two performance
issues when deployed in bigger Kubernetes environments:

1. Long response time on metrics scrape
2. High memory usage

This patch introduces a custom client go store, MetricsStore. It is updated by a
corresponding reflector. Instead of generating the Prometheus custom metrics on
demand, metrics are generated on new Kubernetes object arrival.

This pull request represents an early *work-in-progress* state to enable early
feedback. It targets the *performance-optimizations* branch, instead of *master*.

Known ToDos:

- Re-enable multi namespace support
- Re-enable white/black listing
- Adjust remaining `collectors/*_test.go`

**Premature benchmarking results**:

1000 configmaps, 1000 pods, 100 scrapes:





```shell
$ git checkout master
$ go test -bench .                    
goos: linux
goarch: amd64
pkg: k8s.io/kube-state-metrics
BenchmarkKubeStateMetrics-4            1        10267655552 ns/op
PASS
ok      k8s.io/kube-state-metrics       10.282s

```

[master_cpu.pdf](https://github.com/kubernetes/kube-state-metrics/files/2345809/master_cpu.pdf)
[master_mem.pdf](https://github.com/kubernetes/kube-state-metrics/files/2345810/master_mem.pdf)

```shell
$ git checkout metricsstore
$ go test -bench .
goos: linux
goarch: amd64
pkg: k8s.io/kube-state-metrics
BenchmarkKubeStateMetrics-4            1        1876562818 ns/op
--- BENCH: BenchmarkKubeStateMetrics-4
        main_test.go:39: starting kube-state-metrics benchmark with fixtureMultiplier 1000 and requestCount 100
PASS
ok      k8s.io/kube-state-metrics       1.892s
```

[metricsstore_cpu.pdf](https://github.com/kubernetes/kube-state-metrics/files/2345825/metricsstore_cpu.pdf)
[metricsstore_mem.pdf](https://github.com/kubernetes/kube-state-metrics/files/2345826/metricsstore_mem.pdf)







**Which issue(s) this PR fixes**:

Relates to https://github.com/kubernetes/kube-state-metrics/issues/498


//CC @metalmatze @brancz @squat @s-urbaniak

",closed,True,2018-09-03 14:13:26,2018-10-16 14:14:04
kube-state-metrics,jutley,https://github.com/kubernetes/kube-state-metrics/pull/535,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/535,Add kube_pod_container_status_last_terminated_reason,"**What this PR does / why we need it**:
This PR introduces a new metric: `kube_pod_container_status_last_terminated_reason`. 

Currently, we have the `kube_pod_container_status_terminated_reason`, but this always returns to 0 once a container starts back up. This means that we will only have a couple data points, if any at all, around the reason for a container termination. As a result, we cannot alert when a container crashes for a specific reason (we'd like to alert based on OOMs). 

This is brought up in this issue: https://github.com/kubernetes/kube-state-metrics/issues/344

**Which issue(s) this PR fixes**:
Fixes #344 

",closed,True,2018-09-04 20:11:22,2019-04-01 08:20:41
kube-state-metrics,abinet,https://github.com/kubernetes/kube-state-metrics/issues/536,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/536,Pod labels in metrics,"Currently `namespace`, `pod` are default labels provided in the metrics.
I would like to print some additional pod labels in the metrics's output to have an ability to group on later in prometheus / grafana. Is it possible?

",closed,False,2018-09-11 08:28:29,2018-09-18 07:08:26
kube-state-metrics,MIBc,https://github.com/kubernetes/kube-state-metrics/pull/537,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/537,Change error msg,"**What this PR does / why we need it**:
The test creates two resources(pod, configmap), not only pod added.


",closed,True,2018-09-13 07:25:15,2018-09-13 07:26:19
kube-state-metrics,MIBc,https://github.com/kubernetes/kube-state-metrics/pull/538,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/538,Change error message,"**What this PR does / why we need it**:
The test creates two resources(pod, configmap), not only pod added.
",closed,True,2018-09-13 07:39:10,2018-09-13 08:08:51
kube-state-metrics,abinet,https://github.com/kubernetes/kube-state-metrics/pull/539,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/539,add an example of joining the metrics in docs,"This PR adds an example for joining the metrics as per discussion in https://github.com/kubernetes/kube-state-metrics/issues/536

Fix #536 ",closed,True,2018-09-14 14:53:04,2018-09-18 07:13:50
kube-state-metrics,snegivulcan,https://github.com/kubernetes/kube-state-metrics/issues/540,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/540,kube_node_info has wrong instance info,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**: BUG REPORT

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

I run `kube_node_info` in my 3 node k8s cluster and it the returns following results

```
kube_node_info{container_runtime_version=""docker://17.3.2"",instance=""gke-boshupgrade-default-pool-abcdefgh-jj46"",job=""kubernetes-pods"",k8s_app=""kube-state-metrics"",kernel_version=""4.4.111+"",kubelet_version=""v1.9.7-gke.6"",kubeproxy_version=""v1.9.7-gke.6"",node=""gke-boshupgrade-default-pool-abcdefgh-jj46"",os_image=""Container-Optimized OS from Google"",pod_template_hash=""3967561728"",provider_id=""gce://ss-infrastructure/us-west1-a/gke-boshupgrade-default-pool-abcdefgh-6ct4""}       1
kube_node_info{container_runtime_version=""docker://17.3.2"",instance=""gke-boshupgrade-default-pool-abcdefgh-jj46"",job=""kubernetes-pods"",k8s_app=""kube-state-metrics"",kernel_version=""4.4.111+"",kubelet_version=""v1.9.7-gke.6"",kubeproxy_version=""v1.9.7-gke.6"",node=""gke-boshupgrade-default-pool-abcdefgh-jj46"",os_image=""Container-Optimized OS from Google"",pod_template_hash=""3967561728"",provider_id=""gce://ss-infrastructure/us-west1-a/gke-boshupgrade-default-pool-abcdefgh-91dz""}       1
kube_node_info{container_runtime_version=""docker://17.3.2"",instance=""gke-boshupgrade-default-pool-abcdefgh-jj46"",job=""kubernetes-pods"",k8s_app=""kube-state-metrics"",kernel_version=""4.4.111+"",kubelet_version=""v1.9.7-gke.6"",kubeproxy_version=""v1.9.7-gke.6"",node=""gke-boshupgrade-default-pool-abcdefgh-jj46"",os_image=""Container-Optimized OS from Google"",pod_template_hash=""3967561728"",provider_id=""gce://ss-infrastructure/us-west1-a/gke-boshupgrade-default-pool-abcdefgh-jj46""}
```

Note - the node/instance are same for all 3 entries. But i do see provider_id has right node info.

**What you expected to happen**:

I expect the instance to contain the actual correct node value

**How to reproduce it (as minimally and precisely as possible)**:

Running a 3 nodes K8s cluster in using GKS in Google Cloud. Have KubeStateMetrics ( v1.4.0 ) as a Kubernetes deployment.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): 

```
root@1b0a13227276:/vulcan-platform-tools# kubectl version
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.2"", GitCommit:""bb9ffb1654d4a729bb4cec18ff088eacc153c239"", GitTreeState:""clean"", BuildDate:""2018-08-07T23:17:28Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.7-gke.6"", GitCommit:""9b635efce81582e1da13b35a7aa539c0ccb32987"", GitTreeState:""clean"", BuildDate:""2018-08-16T21:33:47Z"", GoVersion:""go1.9.3b4"", Compiler:""gc"", Platform:""linux/amd64""}

```

- Kube-state-metrics image version
v1.4.0
",closed,False,2018-09-16 00:57:19,2019-03-07 13:11:12
kube-state-metrics,thedebugger,https://github.com/kubernetes/kube-state-metrics/pull/541,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/541,Add init container metrics,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #502

",open,True,2018-09-18 06:55:25,2019-03-18 17:12:34
kube-state-metrics,xrl,https://github.com/kubernetes/kube-state-metrics/issues/542,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/542,Process errors with i/o timeout against the kube api endpoint,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**: I installed kube-state-metrics from kube-prometheus and it has been in a restart loop, timing out while talking to the kube API.

**What you expected to happen**: kube-state-metrics should not time out talking to the kube API.

**How to reproduce it (as minimally and precisely as possible)**:

Use this container definitions:

```
      - args:
        - --host=127.0.0.1
        - --port=8081
        - --telemetry-host=127.0.0.1
        - --telemetry-port=8082
        # - --apiserver
        image: quay.io/coreos/kube-state-metrics:v1.4.0
        name: kube-state-metrics
        resources:
          limits:
            cpu: 100m
            memory: 150Mi
          requests:
            cpu: 100m
            memory: 150Mi
```

The full deployment, which I'm running verbatim, is here: https://github.com/coreos/prometheus-operator/blob/master/contrib/kube-prometheus/manifests/kube-state-metrics-deployment.yaml

Once that's installed, then run `k get pods`:

```
$ k get pods
NAME                                  READY     STATUS             RESTARTS   AGE
alertmanager-main-0                   1/2       CrashLoopBackOff   1081       3d
grafana-5b68464b84-mg6pf              1/1       Running            0          3d
kube-state-metrics-594cc76cfc-dpqfb   3/4       CrashLoopBackOff   767        2d
node-exporter-4lr5b                   2/2       Running            0          3d
node-exporter-5qzl4                   2/2       Running            0          3d
node-exporter-kddxg                   2/2       Running            0          3d
node-exporter-mb4xz                   2/2       Running            0          3d
node-exporter-n44vr                   2/2       Running            0          3d
node-exporter-wntnh                   2/2       Running            0          3d
prometheus-k8s-0                      3/3       Running            1          3d
prometheus-k8s-1                      3/3       Running            1          3d
prometheus-operator-587d64f4c-tkl4w   1/1       Running            0          3d
```

and then look at the logs:

```
$ k logs kube-state-metrics-594cc76cfc-dpqfb -c kube-state-metrics
I0921 15:26:29.547189       1 main.go:76] Using default collectors
I0921 15:26:29.547238       1 main.go:90] Using all namespace
I0921 15:26:29.547248       1 main.go:96] No metric whitelist or blacklist set. No filtering of metrics will be done.
W0921 15:26:29.547270       1 client_config.go:552] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0921 15:26:29.547962       1 main.go:145] Testing communication with server
F0921 15:26:59.548419       1 main.go:112] Failed to create client: ERROR communicating with apiserver: Get https://10.43.168.1:443/version?timeout=32s: dial tcp 10.43.168.1:443: i/o timeout
```


**Anything else we need to know?**:

I have confirmed from a standalone pod that I can access the kube cluster:

```
# curl -vk https://kubernetes.default.svc
* Rebuilt URL to: https://kubernetes.default.svc/
*   Trying 10.43.168.1...
* TCP_NODELAY set
* Connected to kubernetes.default.svc (10.43.168.1) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Request CERT (13):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Certificate (11):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=kubernetes-master
*  start date: Sep 11 17:15:28 2018 GMT
*  expire date: Sep 10 17:15:28 2028 GMT
*  issuer: CN=kubernetes
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x55c98221c8e0)
> GET / HTTP/2
> Host: kubernetes.default.svc
> User-Agent: curl/7.58.0
> Accept: */*
>
* Connection state changed (MAX_CONCURRENT_STREAMS updated)!
< HTTP/2 401
< content-type: application/json
< www-authenticate: Basic realm=""kubernetes-master""
< content-length: 165
< date: Fri, 21 Sep 2018 15:31:20 GMT
<
{
  ""kind"": ""Status"",
  ""apiVersion"": ""v1"",
  ""metadata"": {

  },
  ""status"": ""Failure"",
  ""message"": ""Unauthorized"",
  ""reason"": ""Unauthorized"",
  ""code"": 401
* Connection #0 to host kubernetes.default.svc left intact
}
```

**Environment**:
- Kubernetes version (use `kubectl version`):

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-10T11:44:36Z"", GoVersion:""go1.11"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.7"", GitCommit:""0c38c362511b20a098d7cd855f1314dad92c2780"", GitTreeState:""clean"", BuildDate:""2018-08-20T09:56:31Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kube-state-metrics image version: `1.4.0` or `1.3.1` they both fail the same way.
",closed,False,2018-09-21 15:35:09,2018-09-22 14:12:05
kube-state-metrics,ehashman,https://github.com/kubernetes/kube-state-metrics/issues/543,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/543,KSM is unable to authenticate to cluster in 1.3.x+ releases,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

When I run kube-state-metrics on any release 1.3.0+, it launches, but when it attempts to scrape any resources, I get the error:

```
User ""system:anonymous"" cannot list <resource>s at the cluster scope: No policy matched.
```

However, all the various clusterbindingroles are set up correctly for the cluster. I am not sure why KSM is unable to authenticate.

The correct token is definitely loaded inside the KSM Pod and I've successfully used it to authenticate as the ServiceAccount against the API server with a curl command (it matches ""system:serviceaccount:kube-system:kube-state-metrics"" in this case). For some reason, it seems that KSM is ignoring the token?

**What you expected to happen**:

KSM successfully authenticates with the API server and scrapes the cluster. This does work correctly in the 1.2.0 release (but I believe that certificate verification might not be working properly in that release, as I didn't have to load in our custom CA certs to get KSM 1.2.0 to work correctly).

**How to reproduce it (as minimally and precisely as possible)**:

I'm not really sure how to minimally reproduce this; it affects all our clusters (prod + QA) but there's nothing particularly special about them. We have RBAC enabled.

**Environment**:
- Kubernetes version (use `kubectl version`): 1.8.7
- Kube-state-metrics image version: 1.3.0, 1.3.1, 1.4.0",closed,False,2018-09-25 15:29:54,2018-10-09 01:48:46
kube-state-metrics,Deepak1100,https://github.com/kubernetes/kube-state-metrics/issues/544,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/544,kube_service_info matrics should show external-ip assign to service,"
> Uncomment only one, leave it on its own line: 

> /kind feature


**What happened**:
i am trying to get somehow get external-ip attribute of service in prometheus so i can map it with cloudwatch exporter matrics for that elb. but i didn't find any matrics giving me this information.(whic have service and its elb attached)

```
NAME         TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)        AGE
app-external   LoadBalancer   172.20.19.131    asdfasdfa-asdfasd-asdfasf.us-east-1.elb.amazonaws.com   443:31255/TCP                                                                      4d
```
**What you expected to happen**:
If there is any matrics which can give me all service info like external-ip then i can use it on multiple places
**How to reproduce it (as minimally and precisely as possible)**:
just add annotation while creating service which will cretate elb and add it as its externap-ip attribute
```
service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
```
and check kube_service_info or kube_service_label any other metrics and there is no way to find external-ip of service.

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-10T11:44:36Z"", GoVersion:""go1.11"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-28T20:13:43Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kube-state-metrics image version
```
gcr.io/google_containers/kube-state-metrics:v1.2.0
```
",closed,False,2018-09-26 07:15:23,2018-11-09 13:53:06
kube-state-metrics,tomwilkie,https://github.com/kubernetes/kube-state-metrics/pull/545,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/545,"Export stateful sets status.{current_revision,update_revision}","Signed-off-by: Tom Wilkie <tom.wilkie@gmail.com>

**What this PR does / why we need it**:

~Turns out probably not necessary, was looking to build an alert for non-updated pods, `kube_statefulset_status_replicas_updated != kube_statefulset_replicas`.~

Nope, above query only works for statefulsets with updated.  You need `max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)` to find stateful sets with updates that haven't been rolled out.

",closed,True,2018-09-26 15:08:44,2018-11-15 11:45:03
kube-state-metrics,gregory-lyons,https://github.com/kubernetes/kube-state-metrics/issues/546,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/546,Add metrics for PodDisruptionBudget objects,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:

**What you expected to happen**:
It would be cool to have metrics for PDB objects for visibility into availability and possible disruptions. I can start working on this if it sounds like a reasonable proposal.

**How to reproduce it (as minimally and precisely as possible)**:

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version
",closed,False,2018-09-26 22:37:24,2018-10-19 08:18:38
kube-state-metrics,tasdikrahman,https://github.com/kubernetes/kube-state-metrics/issues/547,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/547,"metric: kube_pod_container_status_waiting_reason missing label reason=""CrashLoopBackOff"" and reason=""ImagePullBackOff""","** BUG REPORT **:

/kind bug

**What happened**:

I recently upgraded 
- kube-state-metrics from version `1.2.0` to `1.4.0` 
- addon-resizer from version `1.0` to `1.7`

**What you expected to happen**:

On my older k8s cluster (v1.8.9+coreos.0), where I have made the same upgrade. I can see the metric 

```
kube_pod_container_status_waiting_reason{container=""traefik-ingress-lb"",namespace=""kube-system"",pod=""traefik-ingress-controller-qhzwn"",reason=""ErrImagePull""} 0
kube_pod_container_status_waiting_reason{container=""traefik-ingress-lb"",namespace=""kube-system"",pod=""traefik-ingress-controller-qhzwn"",reason=""ImagePullBackOff""} 0
kube_pod_container_status_waiting_reason{container=""traefik-ingress-lb"",namespace=""kube-system"",pod=""traefik-ingress-controller-sjbxf"",reason=""ContainerCreating""} 0
kube_pod_container_status_waiting_reason{container=""traefik-ingress-lb"",namespace=""kube-system"",pod=""traefik-ingress-controller-sjbxf"",reason=""CrashLoopBackOff""} 0
```

being emitted, but on my newer k8s cluster(v1.9.4+coreos.0), I can't see the label `reason=""CrashLoopBackOff""` being added to the metric `kube_pod_container_status_waiting_reason`. 

```
kube_pod_container_status_waiting_reason{container=""node-exporter"",namespace=""monitoring"",pod=""node-exporter-zdzxf"",reason=""ContainerCreating""} 0
kube_pod_container_status_waiting_reason{container=""node-exporter"",namespace=""monitoring"",pod=""node-exporter-zdzxf"",reason=""ErrImagePull""} 0
kube_pod_container_status_waiting_reason{container=""node-exporter"",namespace=""monitoring"",pod=""node-exporter-zfbdt"",reason=""ContainerCreating""} 0
kube_pod_container_status_waiting_reason{container=""node-exporter"",namespace=""monitoring"",pod=""node-exporter-zfbdt"",reason=""ErrImagePull""} 0
```

where the labels `CrashLoopBackOff` and `ImagePullBackOff ` are missing. The only thing which is different from the two clusters is the version of k8s 

**How to reproduce it (as minimally and precisely as possible)**:

deploy the below two manifest files on the respective k8s cluster versions to see the missing label on the metric `kube_pod_container_status_waiting_reason `

**Anything else we need to know?**:

manifest files for the two clusters

```
##  k8s cluster (v1.8.9+coreos.0)
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""5""
  creationTimestamp: null
  generation: 1
  labels:
    k8s-app: kube-state-metrics
  name: kube-state-metrics
  selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/kube-state-metrics
spec:
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kube-state-metrics
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        k8s-app: kube-state-metrics
    spec:
      containers:
      - image: quay.io/coreos/kube-state-metrics:v1.4.0
        imagePullPolicy: IfNotPresent
        name: kube-state-metrics
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 8081
          name: telemetry
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: 116m
            memory: 132Mi
          requests:
            cpu: 116m
            memory: 132Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      - command:
        - /pod_nanny
        - --container=kube-state-metrics
        - --cpu=100m
        - --extra-cpu=1m
        - --memory=100Mi
        - --extra-memory=2Mi
        - --threshold=5
        - --deployment=kube-state-metrics
        env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        image: gcr.io/google_containers/addon-resizer:1.7
        imagePullPolicy: IfNotPresent
        name: addon-resizer
        resources:
          limits:
            cpu: 100m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 30Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      nodeSelector:
        node-role.kubernetes.io/node-ops: """"
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: kube-state-metrics
      serviceAccountName: kube-state-metrics
      terminationGracePeriodSeconds: 30
```

and the newer cluster would be

```
## v1.9.4+coreos.0
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  creationTimestamp: null
  generation: 1
  labels:
    k8s-app: kube-state-metrics
  name: kube-state-metrics
  selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/kube-state-metrics
spec:
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kube-state-metrics
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        k8s-app: kube-state-metrics
    spec:
      containers:
      - image: quay.io/coreos/kube-state-metrics:v1.4.0
        imagePullPolicy: IfNotPresent
        name: kube-state-metrics
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 8081
          name: telemetry
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: 119m
            memory: 138Mi
          requests:
            cpu: 119m
            memory: 138Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      - command:
        - /pod_nanny
        - --container=kube-state-metrics
        - --cpu=100m
        - --extra-cpu=1m
        - --memory=100Mi
        - --extra-memory=2Mi
        - --threshold=5
        - --deployment=kube-state-metrics
        env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        image: gcr.io/google_containers/addon-resizer:1.7
        imagePullPolicy: IfNotPresent
        name: addon-resizer
        resources:
          limits:
            cpu: 100m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 30Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: kube-state-metrics
      serviceAccountName: kube-state-metrics
      terminationGracePeriodSeconds: 30
```

**Environment**:
- Kubernetes version (use `kubectl version`): 1.8.9 and 1.9.4
- Kube-state-metrics image version: 1.4.0
",closed,False,2018-09-28 09:01:40,2019-03-09 19:02:19
kube-state-metrics,sylr,https://github.com/kubernetes/kube-state-metrics/issues/548,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/548,"Add counters by terminated reason for deployment, daemonset and statefulset","It would be really useful to be able to track accurately the total of pod terminations by reason for deployment/daemonset/statefulset, i.e:

kube_deployment_pod_status_terminated_reason_total{}
kube_daemonset_pod_status_terminated_reason_total{}
kube_statefulset_pod_status_terminated_reason_total{}

/kind feature

",closed,False,2018-09-28 10:48:41,2019-02-25 12:14:36
kube-state-metrics,ehashman,https://github.com/kubernetes/kube-state-metrics/pull/549,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/549,Add changelog note for breaking change in #371,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

Adds a note about breaking changes to the CHANGELOG for the 1.2.0 -> 1.3.0 upgrade.

The changelog does not currently warn about the deprecation of the `--in-cluster` flag, meaning that users on 1.2.0 releases and earlier who use `--apiserver` with the default `--in-cluster=true` flag have authentication break on upgrade.

**Which issue(s) this PR fixes**:

Fixes #543.

**Other notes:**

The behaviour from 1.2.0 can be emulated on upgrade by removing the `--apiserver` flag and setting the environment variable `KUBERNETES_SERVICE_HOST` to the API server value.
",closed,True,2018-09-28 21:32:06,2018-10-09 01:48:46
kube-state-metrics,olivierboudet,https://github.com/kubernetes/kube-state-metrics/issues/550,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/550,error while sending encoded metrics: write: broken pipe,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:

Some metrics are not collected. The following error happen for each scraping interval : 
`E1001 07:14:23.558510       1 main.go:52] [error while sending encoded metrics: write tcp 10.8.13.78:8080->10.8.0.27:51354: write: broken pipe]
`

**What you expected to happen**:

No error at all and all metrics collected.

**How to reproduce it (as minimally and precisely as possible)**:

This is my deployment configuration : 
```
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
      component: kube-state-metrics
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: prometheus
        component: kube-state-metrics
    spec:
      containers:
      - args:
        - --v=4
        image: quay.io/coreos/kube-state-metrics:v1.4.0
        imagePullPolicy: IfNotPresent
        name: prometheus-kube-state-metrics
        ports:
        - containerPort: 8080
          name: metrics
          protocol: TCP
        resources:
          limits:
            cpu: 10m
            memory: 128Mi
          requests:
            cpu: 10m
            memory: 16Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: prometheus-kube-state-metrics
      serviceAccountName: prometheus-kube-state-metrics
      terminationGracePeriodSeconds: 30
```

**Anything else we need to know?**:

This is the log with command line argument --v=4 : 
```
I1001 07:11:16.958662       1 deployment.go:181] collected 53 deployments
I1001 07:11:14.958533       1 persistentvolumeclaim.go:126] collected 0 persistentvolumeclaims
I1001 07:11:21.957911       1 pod.go:282] collected 93 pods
E1001 07:11:28.157221       1 main.go:52] [error while sending encoded metrics: write tcp 10.8.13.78:8080->10.8.0.27:50328: write: broken pipe]
I1001 07:12:12.943334       1 persistentvolumeclaim.go:126] collected 22 persistentvolumeclaims
I1001 07:12:12.957211       1 daemonset.go:159] collected 4 daemonsets
I1001 07:12:12.957307       1 hpa.go:137] collected 0 hpas
I1001 07:12:12.957342       1 resourcequota.go:103] collected 0 resourcequotas
I1001 07:12:12.957951       1 job.go:181] collected 9 jobs
I1001 07:12:12.958132       1 node.go:206] collected 4 nodes
I1001 07:12:12.958169       1 replicationcontroller.go:143] collected 0 replicationcontrollers
I1001 07:12:12.959056       1 replicaset.go:135] collected 75 replicasets
I1001 07:12:13.359632       1 deployment.go:181] collected 53 deployments
I1001 07:12:13.359879       1 namespace.go:125] collected 9 namespaces
I1001 07:12:13.363003       1 statefulset.go:151] collected 17 statefulsets
I1001 07:12:13.558844       1 service.go:119] collected 92 services
I1001 07:12:13.559436       1 configmap.go:110] collected 44 configmaps
I1001 07:12:14.857507       1 cronjob.go:151] collected 2 cronjobs
I1001 07:12:14.858860       1 secret.go:128] collected 74 secrets
I1001 07:12:16.959586       1 pod.go:282] collected 93 pods
I1001 07:12:17.358140       1 endpoint.go:130] collected 94 endpoints
I1001 07:12:17.757231       1 persistentvolume.go:119] collected 22 persistentvolumes
I1001 07:12:18.057245       1 limitrange.go:101] collected 1 limitranges
E1001 07:12:24.158422       1 main.go:52] [error while sending encoded metrics: write tcp 10.8.13.78:8080->10.8.0.27:50658: write: broken pipe]
I1001 07:13:13.260075       1 endpoint.go:130] collected 94 endpoints
I1001 07:13:13.260158       1 limitrange.go:101] collected 1 limitranges
I1001 07:13:13.260315       1 cronjob.go:151] collected 2 cronjobs
I1001 07:13:13.657648       1 secret.go:128] collected 74 secrets
I1001 07:13:13.658485       1 persistentvolumeclaim.go:126] collected 22 persistentvolumeclaims
I1001 07:13:13.660651       1 hpa.go:137] collected 0 hpas
I1001 07:13:13.660694       1 resourcequota.go:103] collected 0 resourcequotas
I1001 07:13:14.057347       1 job.go:181] collected 9 jobs
I1001 07:13:14.058320       1 node.go:206] collected 4 nodes
I1001 07:13:14.060030       1 replicaset.go:135] collected 75 replicasets
I1001 07:13:14.060059       1 replicationcontroller.go:143] collected 0 replicationcontrollers
I1001 07:13:14.460392       1 statefulset.go:151] collected 17 statefulsets
I1001 07:13:14.857319       1 daemonset.go:159] collected 4 daemonsets
I1001 07:13:14.857414       1 namespace.go:125] collected 9 namespaces
I1001 07:13:14.857986       1 persistentvolume.go:119] collected 22 persistentvolumes
I1001 07:13:14.859108       1 deployment.go:181] collected 53 deployments
I1001 07:13:15.759209       1 configmap.go:110] collected 44 configmaps
I1001 07:13:15.760053       1 service.go:119] collected 92 services
I1001 07:13:17.759649       1 pod.go:282] collected 93 pods
E1001 07:13:23.957200       1 main.go:52] [error while sending encoded metrics: write tcp 10.8.13.78:8080->10.8.0.27:51026: write: broken pipe]
I1001 07:14:12.945895       1 persistentvolumeclaim.go:126] collected 22 persistentvolumeclaims
I1001 07:14:12.957150       1 hpa.go:137] collected 0 hpas
I1001 07:14:12.957223       1 resourcequota.go:103] collected 0 resourcequotas
I1001 07:14:12.958188       1 node.go:206] collected 4 nodes
I1001 07:14:12.958888       1 replicaset.go:135] collected 75 replicasets
I1001 07:14:12.958923       1 replicationcontroller.go:143] collected 0 replicationcontrollers
I1001 07:14:13.459363       1 job.go:181] collected 9 jobs
I1001 07:14:13.460220       1 statefulset.go:151] collected 17 statefulsets
I1001 07:14:13.460434       1 namespace.go:125] collected 9 namespaces
I1001 07:14:13.857340       1 daemonset.go:159] collected 4 daemonsets
I1001 07:14:14.457685       1 configmap.go:110] collected 44 configmaps
I1001 07:14:14.457702       1 persistentvolume.go:119] collected 22 persistentvolumes
I1001 07:14:14.459542       1 endpoint.go:130] collected 94 endpoints
I1001 07:14:14.459606       1 limitrange.go:101] collected 1 limitranges
I1001 07:14:14.757502       1 cronjob.go:151] collected 2 cronjobs
I1001 07:14:13.960645       1 service.go:119] collected 92 services
I1001 07:14:13.958269       1 deployment.go:181] collected 53 deployments
I1001 07:14:15.557780       1 secret.go:128] collected 74 secrets
I1001 07:14:17.458212       1 pod.go:282] collected 93 pods
E1001 07:14:23.558510       1 main.go:52] [error while sending encoded metrics: write tcp 10.8.13.78:8080->10.8.0.27:51354: write: broken pipe]
I1001 07:15:12.943546       1 configmap.go:110] collected 44 configmaps
I1001 07:15:12.943864       1 cronjob.go:151] collected 2 cronjobs
I1001 07:15:13.257172       1 secret.go:128] collected 74 secrets
I1001 07:15:13.358669       1 resourcequota.go:103] collected 0 resourcequotas
I1001 07:15:13.358978       1 job.go:181] collected 9 jobs
I1001 07:15:13.359382       1 persistentvolumeclaim.go:126] collected 22 persistentvolumeclaims
I1001 07:15:13.657335       1 daemonset.go:159] collected 4 daemonsets
I1001 07:15:13.657375       1 hpa.go:137] collected 0 hpas
I1001 07:15:13.657708       1 service.go:119] collected 92 services
I1001 07:15:14.159346       1 replicationcontroller.go:143] collected 0 replicationcontrollers
I1001 07:15:13.658273       1 statefulset.go:151] collected 17 statefulsets
I1001 07:15:13.658507       1 namespace.go:125] collected 9 namespaces
I1001 07:15:13.658937       1 persistentvolume.go:119] collected 22 persistentvolumes
I1001 07:15:13.659335       1 node.go:206] collected 4 nodes
I1001 07:15:13.659908       1 replicaset.go:135] collected 75 replicasets
I1001 07:15:14.158058       1 endpoint.go:130] collected 94 endpoints
I1001 07:15:14.158244       1 limitrange.go:101] collected 1 limitranges
I1001 07:15:15.159389       1 deployment.go:181] collected 53 deployments
I1001 07:15:17.058359       1 pod.go:282] collected 93 pods
E1001 07:15:23.357200       1 main.go:52] [error while sending encoded metrics: write tcp 10.8.13.78:8080->10.8.0.27:51682: write: broken pipe]
I1001 07:16:12.946221       1 service.go:119] collected 92 services
I1001 07:16:13.057191       1 limitrange.go:101] collected 1 limitranges
I1001 07:16:12.957515       1 configmap.go:110] collected 44 configmaps
I1001 07:16:13.157262       1 cronjob.go:151] collected 2 cronjobs
I1001 07:16:13.158252       1 secret.go:128] collected 74 secrets
I1001 07:16:13.957264       1 hpa.go:137] collected 0 hpas
I1001 07:16:13.957631       1 resourcequota.go:103] collected 0 resourcequotas
I1001 07:16:13.958618       1 job.go:181] collected 9 jobs
I1001 07:16:13.959588       1 persistentvolumeclaim.go:126] collected 22 persistentvolumeclaims
I1001 07:16:13.959899       1 daemonset.go:159] collected 4 daemonsets
I1001 07:16:13.959924       1 replicationcontroller.go:143] collected 0 replicationcontrollers
I1001 07:16:14.359188       1 deployment.go:181] collected 53 deployments
I1001 07:16:15.559565       1 statefulset.go:151] collected 17 statefulsets
I1001 07:16:15.559855       1 namespace.go:125] collected 9 namespaces
I1001 07:16:15.957343       1 persistentvolume.go:119] collected 22 persistentvolumes
I1001 07:16:16.057800       1 node.go:206] collected 4 nodes
I1001 07:16:16.460051       1 endpoint.go:130] collected 94 endpoints
I1001 07:16:16.460951       1 replicaset.go:135] collected 75 replicasets
I1001 07:16:18.359939       1 pod.go:282] collected 93 pods
E1001 07:16:26.257246       1 main.go:52] [error while sending encoded metrics: write tcp 10.8.13.78:8080->10.8.0.27:52000: write: broken pipe]
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.2"", GitCommit:""81753b10df112992bf51bbc2c2f85208aad78335"", GitTreeState:""clean"", BuildDate:""2018-04-27T09:22:21Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.7-gke.5"", GitCommit:""9b635efce81582e1da13b35a7aa539c0ccb32987"", GitTreeState:""clean"", BuildDate:""2018-08-02T23:42:40Z"", GoVersion:""go1.9.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kube-state-metrics image version : quay.io/coreos/kube-state-metrics:v1.4.0
",closed,False,2018-10-01 07:23:22,2018-12-27 08:38:00
kube-state-metrics,gregory-lyons,https://github.com/kubernetes/kube-state-metrics/pull/551,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/551,Add PodDisruptionBudget metrics,"**What this PR does / why we need it**:
Adds metrics for PodDisruptionBudget

**Which issue(s) this PR fixes** :
Fixes #546 

",closed,True,2018-10-02 05:02:40,2018-10-19 08:18:38
kube-state-metrics,lostick,https://github.com/kubernetes/kube-state-metrics/pull/552,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/552,Update add on resizer,"**What this PR does / why we need it**:
• Update addon-resizer with latest changes
• Bump minimum requests & limits as I have observed that kube-state-metrics pods often get killed on our kubernetes clusters with the minimum resources.

",closed,True,2018-10-05 21:01:34,2018-10-08 02:50:02
kube-state-metrics,colmose,https://github.com/kubernetes/kube-state-metrics/pull/553,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/553,Fix link href,"**What this PR does / why we need it**:
One of the hrefs in your README is broken currently. 

This updates with the correct link reference

",closed,True,2018-10-10 15:35:43,2018-10-24 18:32:07
kube-state-metrics,EmilyM1,https://github.com/kubernetes/kube-state-metrics/issues/554,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/554,Add metric for access mode for persistent volume claims ,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
Feature


/kind feature


**What happened**:

**What you expected to happen**:

It would be helpful to be able to see access modes on persistent volume claims.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version
",closed,False,2018-10-10 18:51:46,2019-02-20 21:23:59
kube-state-metrics,EmilyM1,https://github.com/kubernetes/kube-state-metrics/issues/555,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/555,Add metric for persistent volume size,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
Feature

 /kind feature


**What happened**:

**What you expected to happen**:
It would be helpful to see the size of persistent volumes.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version
",closed,False,2018-10-10 21:56:45,2019-02-22 06:08:13
kube-state-metrics,hegdedarsh,https://github.com/kubernetes/kube-state-metrics/issues/556,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/556,Kube-State-Metrics doesnt show any kubernetes related metrics ,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
>  bug
> /kind feature


**What happened**:
kube-state-metrics is running fine, and when i open it, i get the metrics, but i dont see any kubernetes related metrics for this, doesnt these get enabled by default, or do we need to configure something.

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (1.12.1)
- Kube-state-metrics image version (1.4)
",closed,False,2018-10-12 01:22:15,2019-03-15 10:13:59
kube-state-metrics,alapidas,https://github.com/kubernetes/kube-state-metrics/issues/557,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/557,Next release date?,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

Wondering when the next release will be, looking to pick up https://github.com/kubernetes/kube-state-metrics/commit/d171c3e440ab4a492f7fc966fdbf98cb26417783

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version
",closed,False,2018-10-12 16:16:27,2018-11-30 09:55:31
kube-state-metrics,AjayTripathy,https://github.com/kubernetes/kube-state-metrics/issues/558,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/558,Documentation slightly wrong for GKE cluster-admin-binding,"

**What happened**:
Documentation issue only:
kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud info | grep Account | cut -d '[' -f 2 | cut -d ']' -f 1) does not work as expected, due to a gcloud / IAM case sensitivity bug 

**What you expected to happen**:

> Ajays-MBP-2:kube-state-metrics atripathy$ kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud info | grep Account | cut -d '[' -f 2 | cut -d ']' -f 1)
> clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-binding created

This should have sufficed.

Ajays-MBP-2:kube-state-metrics atripathy$ gcloud info | grep Account | cut -d '[' -f 2 | cut -d ']' -f 1
4tripathy@gmail.com

Note that this returns 4tripathy@gmail.com, but my IAM is actually 4Tripathy@gmail.com , and IAMs are case sensitive. Proof in pudding:

> kubectl apply -f kubernetes/
> clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
> deployment.apps/kube-state-metrics created
> rolebinding.rbac.authorization.k8s.io/kube-state-metrics created
> serviceaccount/kube-state-metrics created
> service/kube-state-metrics created
> Error from server (Forbidden): error when creating ""kubernetes/kube-state-metrics-cluster-role.yaml"": clusterroles.rbac.authorization.k8s.io ""kube-state-metrics"" is forbidden: attempt to grant extra privileges: [PolicyRule{Resources:[""configmaps""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""configmaps""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""secrets""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""secrets""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""nodes""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""nodes""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""pods""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""pods""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""services""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""services""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""resourcequotas""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""resourcequotas""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""replicationcontrollers""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""replicationcontrollers""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""limitranges""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""limitranges""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""persistentvolumeclaims""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""persistentvolumeclaims""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""persistentvolumes""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""persistentvolumes""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""namespaces""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""namespaces""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""endpoints""], APIGroups:[""""], Verbs:[""list""]} PolicyRule{Resources:[""endpoints""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""daemonsets""], APIGroups:[""extensions""], Verbs:[""list""]} PolicyRule{Resources:[""daemonsets""], APIGroups:[""extensions""], Verbs:[""watch""]} PolicyRule{Resources:[""deployments""], APIGroups:[""extensions""], Verbs:[""list""]} PolicyRule{Resources:[""deployments""], APIGroups:[""extensions""], Verbs:[""watch""]} PolicyRule{Resources:[""replicasets""], APIGroups:[""extensions""], Verbs:[""list""]} PolicyRule{Resources:[""replicasets""], APIGroups:[""extensions""], Verbs:[""watch""]} PolicyRule{Resources:[""statefulsets""], APIGroups:[""apps""], Verbs:[""list""]} PolicyRule{Resources:[""statefulsets""], APIGroups:[""apps""], Verbs:[""watch""]} PolicyRule{Resources:[""cronjobs""], APIGroups:[""batch""], Verbs:[""list""]} PolicyRule{Resources:[""cronjobs""], APIGroups:[""batch""], Verbs:[""watch""]} PolicyRule{Resources:[""jobs""], APIGroups:[""batch""], Verbs:[""list""]} PolicyRule{Resources:[""jobs""], APIGroups:[""batch""], Verbs:[""watch""]} PolicyRule{Resources:[""horizontalpodautoscalers""], APIGroups:[""autoscaling""], Verbs:[""list""]} PolicyRule{Resources:[""horizontalpodautoscalers""], APIGroups:[""autoscaling""], Verbs:[""watch""]}] user=&{4Tripathy@gmail.com  [system:authenticated] map[user-assertion.cloud.google.com:[AGKDXmqHQCiuI5hD1t3PsVXh71YaUiVh5vn9/vxeCyHASpcCn874WfC61v4iotOV9G5gMq22wdFzEs0uApoGf1o0Jdzu+mYc8+qZYcevBqeXhVbhNLQa0TFOUcxP5L2LQVsDsgmSoo2A2v9QZadNt7UNPaXgY3dPHuBvIQY0ysPQ53nxMKbkKf5+/A+WFLxNrm0DkqGGJp6XSdVAz4jd0xz5S0EXLUYLOF5JpYE=]]} ownerrules=[PolicyRule{Resources:[""selfsubjectaccessreviews"" ""selfsubjectrulesreviews""], APIGroups:[""authorization.k8s.io""], Verbs:[""create""]} PolicyRule{NonResourceURLs:[""/api"" ""/api/*"" ""/apis"" ""/apis/*"" ""/healthz"" ""/swagger-2.0.0.pb-v1"" ""/swagger.json"" ""/swaggerapi"" ""/swaggerapi/*"" ""/version""], Verbs:[""get""]}] ruleResolutionErrors=[]
> Error from server (Forbidden): error when creating ""kubernetes/kube-state-metrics-role.yaml"": roles.rbac.authorization.k8s.io ""kube-state-metrics-resizer"" is forbidden: attempt to grant extra privileges: [PolicyRule{Resources:[""pods""], APIGroups:[""""], Verbs:[""get""]} PolicyRule{Resources:[""deployments""], ResourceNames:[""kube-state-metrics""], APIGroups:[""extensions""], Verbs:[""get""]} PolicyRule{Resources:[""deployments""], ResourceNames:[""kube-state-metrics""], APIGroups:[""extensions""], Verbs:[""update""]}] user=&{4Tripathy@gmail.com  [system:authenticated] map[user-assertion.cloud.google.com:[AGKDXmqHQCiuI5hD1t3PsVXh71YaUiVh5vn9/vxeCyHASpcCn874WfC61v4iotOV9G5gMq22wdFzEs0uApoGf1o0Jdzu+mYc8+qZYcevBqeXhVbhNLQa0TFOUcxP5L2LQVsDsgmSoo2A2v9QZadNt7UNPaXgY3dPHuBvIQY0ysPQ53nxMKbkKf5+/A+WFLxNrm0DkqGGJp6XSdVAz4jd0xz5S0EXLUYLOF5JpYE=]]} ownerrules=[PolicyRule{Resources:[""selfsubjectaccessreviews"" ""selfsubjectrulesreviews""], APIGroups:[""authorization.k8s.io""], Verbs:[""create""]} PolicyRule{NonResourceURLs:[""/api"" ""/api/*"" ""/apis"" ""/apis/*"" ""/healthz"" ""/swagger-2.0.0.pb-v1"" ""/swagger.json"" ""/swaggerapi"" ""/swaggerapi/*"" ""/version""], Verbs:[""get""]}] ruleResolutionErrors=[]
> 

bleh, okay, let's back up. Went to go check my IAM and noted it was 4Tripathy@gmail.com, so I went ahead and gave that a shot

> Ajays-MBP-2:kube-state-metrics atripathy$ kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=4Tripathy@gmail.com
> clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-binding created
>
> Ajays-MBP-2:kube-state-metrics atripathy$ kubectl apply -f kubernetes/
> clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics unchanged
> clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
> deployment.apps/kube-state-metrics unchanged
> rolebinding.rbac.authorization.k8s.io/kube-state-metrics unchanged
> role.rbac.authorization.k8s.io/kube-state-metrics-resizer created
> serviceaccount/kube-state-metrics unchanged
> service/kube-state-metrics unchanged
> 

It works.

**How to reproduce it (as minimally and precisely as possible)**:
Add a username to GKE with capital letters and repeat the above :)

**Anything else we need to know?**:
Recommend that for GKE users, the documentation has some instructions about checking your IAM instead of the one-liner.

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version

Actually, as the bug lies with IAM/gcloud interaction, likely Kubernetes version is not relevant.

> gcloud --version
> Google Cloud SDK 220.0.0
> app-engine-python 1.9.77
> bq 2.0.34
> cloud-datastore-emulator 2.0.2
> core 2018.10.08
> gcloud 
> gsutil 4.34
",closed,False,2018-10-13 06:03:58,2018-10-23 09:42:07
kube-state-metrics,estahn,https://github.com/kubernetes/kube-state-metrics/issues/559,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/559,kube_pod_container_resource_requests reports for removed pod,"/kind bug

**What happened**:

Using the following PromQL we tried to see pod resource usage:
```
kube_pod_container_resource_requests{node=""ip-10-10-123-123.ap-southeast-2.compute.internal"",resource=""cpu"",container=""foobar""}
```

The container doesn't exist on the host anymore but still reports a metric > 0.

**What you expected to happen**:

The gauge should go down to 0.

**How to reproduce it (as minimally and precisely as possible)**:

Just using the above example and by removing the pod/container.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): ```Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.0"", GitCommit:""0ed33881dc4355495f623c6f22e7dd0b7632b7c0"", GitTreeState:""clean"", BuildDate:""2018-09-28T15:20:58Z"", GoVersion:""go1.11"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.7"", GitCommit:""0c38c362511b20a098d7cd855f1314dad92c2780"", GitTreeState:""clean"", BuildDate:""2018-08-20T09:56:31Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}```
- Kube-state-metrics image version: ```quay.io/coreos/kube-state-metrics:v1.4.0```
",closed,False,2018-10-17 03:56:56,2018-10-17 10:35:07
kube-state-metrics,jhelbling,https://github.com/kubernetes/kube-state-metrics/pull/560,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/560,missed required rbac permissions,"
Version: kube-state-metrics:v1.4.0


kubectl version


Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.0"", GitCommit:""0ed33881dc4355495f623c6f22e7dd0b7632b7c0"", GitTreeState:""clean"", BuildDate:""2018-09-28T15:18:13Z"", GoVersion:""go1.11"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.8"", GitCommit:""7eab6a49736cc7b01869a15f9f05dc5b49efb9fc"", GitTreeState:""clean"", BuildDate:""2018-09-14T15:54:20Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}


```
E1017 06:46:05.408015       1 reflector.go:205] k8s.io/kube-state-metrics/pkg/collectors/collectors.go:91: Failed to list *v1beta1.StatefulSet: statefulsets.apps is forbidden: User ""system:serviceaccount:kube-system:kube-state-metrics"" cannot list statefulsets.apps at the cluster scope
E1017 06:46:05.408210       1 reflector.go:205] k8s.io/kube-state-metrics/pkg/collectors/collectors.go:91: Failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User ""system:serviceaccount:kube-system:kube-state-metrics"" cannot list replicationcontrollers at the cluster scope
E1017 06:46:05.410535       1 reflector.go:205] k8s.io/kube-state-metrics/pkg/collectors/collectors.go:91: Failed to list *v2beta1.HorizontalPodAutoscaler: horizontalpodautoscalers.autoscaling is forbidden: User ""system:serviceaccount:kube-system:kube-state-metrics"" cannot list horizontalpodautoscalers.autoscaling at the cluster scope``
```",closed,True,2018-10-17 06:53:35,2018-10-17 06:58:45
kube-state-metrics,Siva10,https://github.com/kubernetes/kube-state-metrics/issues/561,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/561,How to get the uptime of a particular deployment ? kube_deployment_created seems to be wrong,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->
/kind bug

**I'm able to see the right `Creation time` of a particular deployment in the k8s dashboard but the time in the metric `kube_deployment_created`seems to be wrong**:

**I expected the metric kube_deployment_created to be same as the creation time seen for a deployment in the k8s dashboard of a cluster**:

**This issue can be reproduced by creating a deployment in a cluster and take note of the creation time and see if its the same as the one appearing for the deployment in a result of the metric kube_deployment_created**:


**I'm trying to know the uptime of a particular deployment, pls point me to any other way of getting it**:

**Environment**:
- Kubernetes version (use `kubectl version`): 1.10.3
- Kube-state-metrics image version: 1.4.0
",closed,False,2018-10-17 09:58:04,2018-10-23 10:12:13
kube-state-metrics,LiliC,https://github.com/kubernetes/kube-state-metrics/pull/562,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/562,README: Update compatibility matrix to include 1.12,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Update compatibility matrix to include 1.12. 
Note: I tested latest release on a 1.12 cluster. 

",closed,True,2018-10-17 10:36:12,2018-10-17 12:18:22
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/563,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/563,pkg/options: Add flag to enable gzip encoding for responses,"**What this PR does / why we need it**:

Dependent on the environment users might or might not want to encode the
responses of kube-state-metrics via gzip independent of the
`Accept-Header` that the client (Prometheus) sends.

This patch adds a corresponding command line flag.

As I am still evaluating whether to enable or disable gzip by default, this is a *work-in-progress* pull request. Thoughts are welcome.

",closed,True,2018-10-17 12:10:29,2018-10-24 14:26:09
kube-state-metrics,AjayTripathy,https://github.com/kubernetes/kube-state-metrics/pull/564,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/564,update documentation to reflect case-sensitivity issues in gcloud,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Better documentation is always good. Took me about two hours to figure out why the referenced one-liner wasn't working for me, hopefully I can save someone else a little time.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #558

",closed,True,2018-10-18 22:03:44,2018-10-23 09:42:07
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/565,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/565,README.md: Remove reference to Prometheus client_golang library,"**What this PR does / why we need it**:

With the recent performance optimization, the Prometheus client_golang
library was replaced by custom logic. This patch removes the outdated
reference in the README.md.

//CC @LiliC

",closed,True,2018-10-22 08:38:20,2018-10-24 15:48:09
kube-state-metrics,beorn7,https://github.com/kubernetes/kube-state-metrics/pull/566,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/566,Pre-allocate the metrics slice in `GetAll`,"**What this PR does / why we need it**:

This method is the heaviest allocator. Pre-allocation will not help a
lot, but I guess every little helps... (Plus, it's really easy.)

**Which issue(s) this PR fixes**

It's related to #498 . See analysis there.

@mxinden 

",closed,True,2018-10-22 19:58:33,2018-10-23 10:15:52
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/567,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/567,pkg/metrics: Tune NewMetrics function,"**What this PR does / why we need it**:

This patch tunes the `NewMetrics` function based on optimizations discussed in https://github.com/prometheus/common/pull/148 and https://github.com/kubernetes/kube-state-metrics/issues/498#issuecomment-431882415 (//CC @beorn7).

- Use strings.Builder minimizing memory allocations.

- Use strconv to render float64 in combination with a []byte buffer pool
instead of fmt.Sprintf reducing memory allocations.

- Only sort labels of metrics in test framework, not by default (e.g. in
production).

Benchmark patch:

```bash
go get golang.org/x/tools/cmd/benchcmp

git checkout perf-exp~1
go test -v -bench=NewMetric  -run=^$  -benchmem ./pkg/metrics/... > old.txt

git checkout perf-exp
go test -v -bench=NewMetric  -run=^$  -benchmem ./pkg/metrics/... > new.txt

benchcmp old.txt new.txt

benchmark                           old ns/op     new ns/op     delta
BenchmarkNewMetric/value-1-4        2652          612           -76.92%
BenchmarkNewMetric/value-35.7-4     2706          755           -72.10%

benchmark                           old allocs     new allocs     delta
BenchmarkNewMetric/value-1-4        32             6              -81.25%
BenchmarkNewMetric/value-35.7-4     32             6              -81.25%

benchmark                           old bytes     new bytes     delta
BenchmarkNewMetric/value-1-4        1616          528           -67.33%
BenchmarkNewMetric/value-35.7-4     1616          528           -67.33%
```



",closed,True,2018-10-24 08:26:35,2018-10-29 15:17:26
kube-state-metrics,juzhao,https://github.com/kubernetes/kube-state-metrics/issues/568,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/568,kube_pod_status_phase treats Completed pod as Running," /kind bug

**What happened**:
pods under openshift-infra project
NAME READY STATUS RESTARTS AGE
hawkular-cassandra-1-d9cq5 1/1 Running 0 5h
hawkular-cassandra-2-clwx8 1/1 Running 1 5h
hawkular-metrics-9l2ws 1/1 Running 0 5h
hawkular-metrics-schema-g2k48 0/1 Completed 1 5h
heapster-78gxb 1/1 Running 0 5h

hawkular-metrics-schema pod is created by cron job, and its status is Completed
queried
kube_pod_status_phase{phase=""Running"",namespace=""openshift-infra""}
in prometheus UI.

it treats hawkular-metrics-schema-g2k48 status as Running, not Completed.
![image](https://user-images.githubusercontent.com/22743651/47434545-5c384e80-d7d5-11e8-8f81-6f449f410edf.png)


**What you expected to happen**:
Should treat Completed as Completed, not Running

**How to reproduce it (as minimally and precisely as possible)**:
As one simple test, we can create pod by job, when the job is finished, the pod's status will be changed to Completed

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""11+"", GitVersion:""v1.11.0+d4cacc0"", GitCommit:""d4cacc0"", GitTreeState:""clean"", BuildDate:""2018-10-23T18:24:29Z"", GoVersion:""go1.10.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11+"", GitVersion:""v1.11.0+d4cacc0"", GitCommit:""d4cacc0"", GitTreeState:""clean"", BuildDate:""2018-10-23T18:24:29Z"", GoVersion:""go1.10.2"", Compiler:""gc"", Platform:""linux/amd64""}

- Kube-state-metrics image version
v1.4.0
",closed,False,2018-10-24 13:40:39,2018-11-30 07:37:50
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/issues/569,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/569,kube-state-metrics breaking release aka 2.0,"We have accumulated a number of deprecated metrics and odd behaviors that I believe may justify a 2.0 release. I'd like to take this issue to discuss whether people think this is a good idea and collect what we would potentially like to break should we do a breaking release.

Off the top of my head breaking changes I would like to do:

* remove deprecated metrics
* rename black-/whitelist to allow/deny-list
* use same ports in all cases (currently the flag defaults to 80/81, but the dockerfile specifies 8080 and 8081)
* rename hpa metrics to use full `horizontalpodautoscaler` nomenclature, to match the rest of the exposed metrics
* rename `--namespace` flag to `--namespaces`
* remove non-identifying labels from pod metrics (eg. [node labels](https://github.com/kubernetes/kube-state-metrics/blob/aed94850b839d111d15308c97e1db7ada8017c6f/internal/collector/pod.go#L511))

I would see a breaking release at least 3 months out there, as I would like to validate the [performance optimizations](https://github.com/kubernetes/kube-state-metrics/issues/498) independently first. Further thoughts?

@andyxning @zouyee @mxinden ",open,False,2018-10-24 15:45:18,2019-04-04 09:04:59
kube-state-metrics,chuonglh,https://github.com/kubernetes/kube-state-metrics/issues/570,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/570,kube_pod_container_status_terminated_reason not show pod OOMkilled correctly,"Currently, we have noticed that metric kube_pod_container_status_terminated_reason not show correctly metric when pods is OOMkilled.

<img width=""671"" alt=""screen shot 2018-10-25 at 11 17 45"" src=""https://user-images.githubusercontent.com/19338075/47475859-b114ad00-d847-11e8-98a3-abeaecdfcc22.png"">
<img width=""670"" alt=""screen shot 2018-10-25 at 11 17 51"" src=""https://user-images.githubusercontent.com/19338075/47475861-b1ad4380-d847-11e8-8b39-3769cce58898.png"">
<img width=""667"" alt=""screen shot 2018-10-25 at 11 18 08"" src=""https://user-images.githubusercontent.com/19338075/47475862-b245da00-d847-11e8-8b46-21b190bf7aea.png"">

Pod is killed at 8:00 when reach the mem limit but the status still 0 in pod status",closed,False,2018-10-25 04:19:48,2018-11-09 02:52:49
kube-state-metrics,patstrom,https://github.com/kubernetes/kube-state-metrics/pull/571,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/571,Add external name/ip and load balancer ingress to service collector,"

**What this PR does / why we need it**:

Add some more metrics about services. While these metrics don't change much having them collected by kube-state-metrics would allow admins to share e.g AWS ELB DNS names associated with services on platforms like Grafana.


Fixes #544
",closed,True,2018-10-25 16:12:01,2018-11-19 12:21:52
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/572,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/572,pkg/metrics: Write newline as byte with strings.Builder,"**What this PR does / why we need it**:

Write newline with `WriteByte` for higher performance.

Origins from https://github.com/kubernetes/kube-state-metrics/pull/567#discussion_r228943917

//CC @brancz @beorn7 ",closed,True,2018-10-29 15:17:07,2018-10-29 15:31:38
kube-state-metrics,ebabani,https://github.com/kubernetes/kube-state-metrics/issues/573,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/573,Question: adding ingress metrics?,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:

Hi. Wanted to check what the maintainers think about exposing ingress metrics. 

I saw that there was a PR (https://github.com/kubernetes/kube-state-metrics/pull/225) that was closed. 

In our case, the use case is to alert on possible duplicate ingress paths created for the same host (undefined behaviour on our system).

I can think of a number of other use cases, like alerting of exposed test paths on prod.

Let me know what you think, and if it's worth it to re-open that PR, or make a new one adding ingress metrics. 

Thanks",closed,False,2018-10-29 21:22:36,2019-02-06 09:20:15
kube-state-metrics,bluechips23,https://github.com/kubernetes/kube-state-metrics/issues/574,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/574,Feature Request: Include Deployment and Pod Annotations,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature


**What happened**: Deployment and Pod Annotations are not included today in kube_state_metrics

**What you expected to happen**: Include Deployment and Pod Annotations


**Anything else we need to know?**: 
Deployment and Pod annotations contain some additional metadata information which are important. These metadata could be included when generating alerts using Prometheus. For instance, if the annotation has contact information (email addresses, etc), then Prometheus can fetch those to send alerts. Labels have a 63 character limit and do not support characters such as ""@"", "","" or "";"", making them unsuitable to include any email addresses. Annotations seem like an ideal place to store these information.

Please consider to see if deployment and pod annotations can be included.
",closed,False,2018-10-30 17:47:28,2019-03-30 19:17:15
kube-state-metrics,LiliC,https://github.com/kubernetes/kube-state-metrics/pull/575,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/575,Making kube-state-metrics a library,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
`kube-state-metrics` has many useful parts which could be used as library, this is the first step towards that.

For now only the functions are exposed and some helper functions moved to a different place. In the long term I would prefer to end with the following structure:
```
pkg
├── collector // functions needed to create a collector, this would be used in the builder pkg and externally 
├── internal
│   └── builder // content of the current upstream resource collectors which would consume the collector pkg
├── metrics // what is currently there but also move the utils/helper functions from collectors. Also the metricHandler interface as that would be useful to expose and reuse elsewhere.

```
Any thoughts on the above are welcome, I can also open an issue to discuss it?
",closed,True,2018-11-01 09:42:30,2018-11-02 14:28:30
kube-state-metrics,pammi22,https://github.com/kubernetes/kube-state-metrics/issues/576,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/576,resource usage metrics export issue,"Kube state metrics not exporting **kube_pod_container_resource_requests_cpu_cores** and **kube_pod_container_resource_requests_memory_bytes** for the pods whose resource limit is not defined.

i am getting all other metrics perfectly apart from these two metrics that too only in case if the resource limit of a pod is not defined.   is it a bug ?",closed,False,2018-11-01 12:11:11,2018-11-13 09:26:21
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/577,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/577,pkg/collectors: Renable white-/blacklisting and HELP text,"**What this PR does / why we need it**:

- Reenable feature to filter the exposition of metric families by their
name. This is now done at startup time, thereby not slowing down the
critical path.

- Expose metrics grouped by their metric family and prefix each metric
family with their HELP text. In the future this can easily be extended
to also expose the TYPE text.

(- This also paved the path for sorting the exposed metrics in a
zero-cost manner if this is wanted (//CC @beorn7).)

Relates to https://github.com/kubernetes/kube-state-metrics/issues/557


",closed,True,2018-11-01 14:34:57,2018-11-27 15:36:09
kube-state-metrics,naseemkullah,https://github.com/kubernetes/kube-state-metrics/pull/578,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/578,Adding CreateContainerConfigError as possible reason for container no…,"

**What this PR does / why we need it**:

For alerts based off `kube_pod_container_status_waiting_reason` metric to go off in event of `CreateContainerConfigError`

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes https://github.com/prometheus/prometheus/issues/4815

",closed,True,2018-11-06 15:38:36,2018-11-07 15:13:02
kube-state-metrics,LiliC,https://github.com/kubernetes/kube-state-metrics/issues/579,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/579,Feature: Make  kube-state-metrics a library,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What:**
This is a follow up from https://github.com/kubernetes/kube-state-metrics/pull/575. Some functions were exposed in that PR, but would like to continue the disucsion to make more of a usuable library.

**How:**
```
pkg
├── collector // functions needed to create a collector, this would be used in the builder pkg and externally 
├── internal
│   └── builder // content of the current upstream resource collectors which would consume the collector pkg
├── metrics // what is currently there but also move the utils/helper functions from collectors. Also the metricHandler interface as that would be useful to expose and reuse elsewhere.

```

cc @mxinden @brancz ",closed,False,2018-11-08 13:24:29,2019-01-23 16:34:45
kube-state-metrics,aisensiy,https://github.com/kubernetes/kube-state-metrics/issues/580,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/580,[Question] No real cpu / memory usage report is given?,https://github.com/kubernetes/kube-state-metrics/blob/master/Documentation/pod-metrics.md This document shows no `cpu` or `memory` usage for current state? So...I can not collect any real time data for pods or containers?,closed,False,2018-11-08 16:39:41,2018-11-10 03:44:38
kube-state-metrics,mooncak,https://github.com/kubernetes/kube-state-metrics/pull/581,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/581,Fix typos: cleaup -> cleanup,"Signed-off-by: mooncake <xcoder@tenxcloud.com>

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Fix typos: cleaup -> cleanup

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-11-10 16:04:16,2018-11-12 14:00:07
kube-state-metrics,charlieegan3,https://github.com/kubernetes/kube-state-metrics/pull/582,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/582,Correct readme Deployment -> Development typo,"

**What this PR does / why we need it**:
This anchor link wasn't working - it was pointing to a non-existent ID.

",closed,True,2018-11-10 22:08:43,2018-11-11 11:19:18
kube-state-metrics,omerlh,https://github.com/kubernetes/kube-state-metrics/issues/583,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/583,Missing metrics about pods in status failed and the reason,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug

/kind feature

**What happened**:
I have a few pods in failed state, due to out of CPU, for example:
```
Status:             Failed
Reason:             OutOfcpu
```
I couldn't find a metric I can use to monitor pods in this state. By looking on the code, it seems that this state is not collected - only waiting or terminated.

**What you expected to happen**:
Be able to monitor how many pods are in this state and why (by the reason).

**How to reproduce it (as minimally and precisely as possible)**:
Create a cluster with pods in failed status

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): `Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-09T17:53:03Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}`
- Kube-state-metrics image version: `quay.io/coreos/kube-state-metrics:v1.4.0z
",open,False,2018-11-12 13:59:44,2019-01-08 10:49:50
kube-state-metrics,r0bj,https://github.com/kubernetes/kube-state-metrics/issues/584,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/584,Not running pod still listed as running,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

Finished pod (eg. `mw-cj-delete-articles-by-prefix-1542086820-7khfh`):
```
kubectl -n prod get pod mw-cj-delete-articles-by-prefix-1542086820-7khfh
NAME                                               READY     STATUS    RESTARTS   AGE
mw-cj-delete-articles-by-prefix-1542086820-7khfh   0/1       Error     0          1d
```

But pod is still listed as running in metrics, eg:
- kube_pod_container_status_running
```
kube_pod_container_status_running{pod=""mw-cj-delete-articles-by-prefix-1542086820-7khfh""}

kube_pod_container_status_running{container=""php"",endpoint=""https-main"",instance=""10.200.225.223:8443"",job=""kube-state-metrics"",namespace=""prod"",pod=""mw-cj-delete-articles-by-prefix-1542086820-7khfh"",service=""kube-state-metrics""}
```
- kube_pod_container_resource_requests
```
kube_pod_container_resource_requests{pod=""mw-cj-delete-articles-by-prefix-1542086820-7khfh""}

kube_pod_container_resource_requests{container=""php"",endpoint=""https-main"",instance=""10.200.225.223:8443"",job=""kube-state-metrics"",namespace=""prod"",node=""k8s-worker-s31"",pod=""mw-cj-delete-articles-by-prefix-1542086820-7khfh"",resource=""cpu"",service=""kube-state-metrics"",unit=""core""}
kube_pod_container_resource_requests{container=""php"",endpoint=""https-main"",instance=""10.200.225.223:8443"",job=""kube-state-metrics"",namespace=""prod"",node=""k8s-worker-s31"",pod=""mw-cj-delete-articles-by-prefix-1542086820-7khfh"",resource=""memory"",service=""kube-state-metrics"",unit=""byte""}
```

In this case correct calculation resources of running containers is impossible.

**What you expected to happen**:

Only running container are listed in metrics like `kube_pod_container_status_running`.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-09T18:02:47Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-09T17:53:03Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kube-state-metrics image version: v1.4.0",closed,False,2018-11-14 19:38:50,2018-11-22 16:18:25
kube-state-metrics,gregory-lyons,https://github.com/kubernetes/kube-state-metrics/issues/585,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/585,Collisions between metrics of same name but different namespace,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug


**What happened**:
As of #534, metric stores use a map keyed by object name:
https://github.com/mxinden/kube-state-metrics/blob/234d788a0d5cfac2b5bf33153afd3b944c39b220/pkg/metrics_store/metrics_store.go#L41

This a poor key choice given that Kubernetes allows you to have objects with the [same name across different namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#when-to-use-multiple-namespaces). For any objects with duplicate names, kube-state-metrics now reports metrics for only one of them at a time.

We observed this problem in our cluster, as many of our object names are duplicated across namespaces. We have seen that kube-state-metrics switches which object it is reporting, possibly due to watches expiring/refreshing at staggered times, but only ever one object at once.

**What you expected to happen**:
kube-state-metrics should display metrics for all objects (some combination or hash of name+namespace might be a better key).

**How to reproduce it (as minimally and precisely as possible)**:
Create two objects with same name in different namespaces, and observe that kube-state-metrics only reports one.
```
glyons@mbp-005245 ~ $ kubectl create ns ns1
namespace ""ns1"" created
glyons@mbp-005245 ~ $ kubectl create ns ns2
namespace ""ns2"" created
glyons@mbp-005245 ~ $ kubectl create secret generic -n ns1 my-secret
secret ""my-secret"" created
glyons@mbp-005245 ~ $ kubectl create secret generic -n ns2 my-secret
secret ""my-secret"" created
glyons@mbp-005245 ~ $ $GOPATH/bin/kube-state-metrics --port=8080 --telemetry-port=8081 --host=localhost --kubeconfig=/Users/glyons/.kube/config &
[1] 70921
glyons@mbp-005245 ~ $ curl -s localhost:8080/metrics | grep my-secret
kube_secret_info{namespace=""ns1"",secret=""my-secret""} 1
kube_secret_type{namespace=""ns1"",secret=""my-secret"",type=""Opaque""} 1
kube_secret_created{namespace=""ns1"",secret=""my-secret""} 1.542316991e+09
kube_secret_labels{namespace=""ns1"",secret=""my-secret""} 1
kube_secret_metadata_resource_version{namespace=""ns1"",resource_version=""4437"",secret=""my-secret""} 1
glyons@mbp-005245 ~ $ kubectl get secrets --all-namespaces | grep my-secret
ns1           my-secret                                        Opaque                                0         2m
ns2           my-secret                                        Opaque                                0         2m
```

**Environment**:
- Kubernetes version (use `kubectl version`): 1.10
- Kube-state-metrics image version: Custom-built version with HEAD at https://github.com/kubernetes/kube-state-metrics/commit/a166def64d0b9add5851a95171239bca382215fb
",closed,False,2018-11-15 21:30:37,2018-11-16 12:50:03
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/586,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/586,pkg/metrics_store: Cache objects by uid not name,"**What this PR does / why we need it**:

Previously the metrics store would cache metrics of Kubernetes objects
by object name. A name is not unique as two namespaces can each have an
object with the same name.

Instead cache objects by their uid which is unique across the cluster.

Thanks @gregory-lyons for the catch.

**Which issue(s) this PR fixes**

Fixes https://github.com/kubernetes/kube-state-metrics/issues/585

This probably relates to the missing metrics referenced in https://github.com/kubernetes/kube-state-metrics/issues/498#issuecomment-431931146.

",closed,True,2018-11-16 12:43:27,2018-11-16 18:35:39
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/587,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/587,main_test.go: Extend pod fixture and fix bench set bytes,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

Extending pod fixture for better benchmarking.


",closed,True,2018-11-22 10:04:05,2018-11-22 15:19:42
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/588,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/588,pkg/builder: Make collector order deterministic,"**What this PR does / why we need it**:

Instead of iterating a map of enabled collectors, iterate a sorted
slice to achieve determinism across scrapes.

Having a consistent order in the metrics output enables Proemetheus to
apply optimizations during metric parsing and ingestion.
",closed,True,2018-11-22 10:09:21,2018-11-22 15:21:26
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/589,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/589,pkg/collectors: Refactor Cron Job and Config Map collector,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

This patch refactors the cron job and config map collector to the format introduced in https://github.com/kubernetes/kube-state-metrics/pull/577.
",closed,True,2018-11-22 10:12:37,2018-11-26 14:02:37
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/590,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/590,pkg/collectors: Refactor deployment and daemonset,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

This patch refactors the daemon set and deployment collector to the format introduced in #577.
",closed,True,2018-11-22 17:17:37,2018-11-23 08:53:36
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/591,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/591,pkg/collectors: Refactor endpoint and hpa collector,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

This patch refactors the endpoint and horizontal pod autoscaler collector to the format introduced in #577.


",closed,True,2018-11-23 08:43:40,2018-11-28 18:00:54
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/592,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/592,pkg/collectors: Refactor job and limit range collector,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
This patch refactors the job and limit range collector to the format introduced in #577.
",closed,True,2018-11-23 10:42:49,2018-11-23 13:36:57
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/593,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/593,pkg/collectors: Refactor namespace and node collector,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

This patch refactors the namespace and node collector to the format introduced in #577.
",closed,True,2018-11-26 12:04:40,2018-11-26 12:41:00
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/594,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/594,pkg/collectors: Refactor persistent volume & claim collector,"

**What this PR does / why we need it**:

This patch refactors the persistent volume and persistent volume claim collector to the format introduced in #577.
",closed,True,2018-11-26 14:30:06,2018-11-26 15:24:57
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/595,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/595,pkg/collectors: Refactor pod disruption budget and replica set collector,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

This patch refactors the pod disruption budget and replica set collector to the format introduced in #577.
",closed,True,2018-11-27 13:50:46,2018-11-27 15:38:28
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/596,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/596,pkg/collectors: Refactor replication controller and resource quota collector,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

This patch refactors the replication controller and resource quota collector to the format introduced in #577.
",closed,True,2018-11-27 16:26:33,2018-11-28 13:47:18
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/597,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/597,pkg/collectors: Refactor secret and stateful set collector,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

This patch refactors the secret and stateful set collector to the format introduced in #577.
",closed,True,2018-11-28 09:39:29,2018-11-28 10:22:43
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/598,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/598,*: Reintroduce TYPE text,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

Prometheus exposes the [type](https://prometheus.io/docs/concepts/metric_types/) of a metric family as a header line. This patch reintroduces this feature to the performance optimized version.

This patch is based on https://github.com/kubernetes/kube-state-metrics/pull/591. Once that is merged, this can follow along.

",closed,True,2018-11-28 16:58:15,2018-11-29 11:39:30
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/599,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/599,Merge performance optimizations into master,"**What this PR does / why we need it**:

Given that all collectors are now updated to the new performance optimized architecture, we can go ahead and merge the optimizations into master. What do you all think?

",closed,True,2018-11-29 11:41:47,2018-12-14 16:05:44
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/issues/600,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/600,main.go: Pass cancellable context to collector builder,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:

In `main.go` we pass a `context.TODO()` to the collectors builder.

https://github.com/kubernetes/kube-state-metrics/blob/af34d12ac49e6e7431e44efcfc9b41fddce95453/main.go#L77

**What you expected to happen**:

Instead we should pass a cancellable context that is cancelled on shutdown.

Originated here https://github.com/kubernetes/kube-state-metrics/pull/593#discussion_r236237555.",closed,False,2018-11-29 11:49:41,2019-02-18 15:21:38
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/601,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/601,Merge performance optimization into master,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

Given that all collectors are now updated to the new performance optimized architecture, we can go ahead and merge the optimizations into master. What do you all think?

This is rebased onto current master to get rid of the merge commits and the thereby implied CLA issues. For more info see https://github.com/kubernetes/kube-state-metrics/pull/599.

",closed,True,2018-11-29 14:55:51,2018-12-14 16:06:04
kube-state-metrics,sstarcher,https://github.com/kubernetes/kube-state-metrics/issues/602,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/602,[feature] calculation of job runtime,"Currently completion time and start time exist.  Is there any easy way to currently calculate runtime with those 2 metrics?

",closed,False,2018-11-29 19:52:24,2018-11-30 15:36:53
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/607,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/607,*: Cut v1.5.0-alpha.0,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

Cut a first v1.5.0 alpha release.

**Which issue(s) this PR fixes**:

Fixes https://github.com/kubernetes/kube-state-metrics/issues/557

",closed,True,2018-11-30 09:18:12,2018-11-30 13:36:12
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/608,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/608,tests: Add stability notice to lib pkg,"
**What this PR does / why we need it**:

This patch adds a stability notice to the `tests/lib` package making sure no one depends on kube-state-metrics as a library and expects a stable interface.

While we are already in the package, it also adds a license header to the `lib_test.go` file.

For now this pull request targets the `release-1.5` branch. This can later on be merged back into `master`.
",closed,True,2018-11-30 16:13:31,2018-11-30 18:14:29
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/609,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/609,tests: Add stability notice to lib pkg - back to master,"**What this PR does / why we need it**:

As I can't merge `release-1.5` back into `master` due to the @k8s-merge-robot merge commit that violates the CLA check, this is a follow up pull request to https://github.com/kubernetes/kube-state-metrics/pull/608 at least sharing the base commit.
",closed,True,2018-11-30 22:02:35,2018-12-05 15:35:52
kube-state-metrics,Chenditang,https://github.com/kubernetes/kube-state-metrics/pull/610,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/610,Add metric for persistent volume claims accessmodes.,"**What this PR does / why we need it**:
Add metric for persistent volume claims accessmodes.
```
# HELP kube_persistentvolumeclaim_spec_accessmodes The accessmodes requested by the persistent volume claim.
# TYPE kube_persistentvolumeclaim_spec_accessmodes gauge
kube_persistentvolumeclaim_spec_accessmodes{namespace=""default"",persistentvolumeclaim=""testpvc"",accessmodes=""ReadWriteOnce""} 0
kube_persistentvolumeclaim_spec_accessmodes{namespace=""default"",persistentvolumeclaim=""testpvc"",accessmodes=""ReadOnlyMany""} 0
kube_persistentvolumeclaim_spec_accessmodes{namespace=""default"",persistentvolumeclaim=""testpvc"",accessmodes=""ReadWriteMany""} 1
```

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #554",open,True,2018-12-04 08:39:05,2019-03-01 17:15:37
kube-state-metrics,whypro,https://github.com/kubernetes/kube-state-metrics/issues/611,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/611,Feature request: HA for kube-state-metrics,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:

Currently, kube-state-metrics is running only one instance in the cluster. If node become down, metrics will be disappeared for minutes before a new instance is ready. 

Thanks for @brancz 's suggestion in https://github.com/kubernetes/kube-state-metrics/issues/147#issuecomment-305193110. There are two ways described to make kube-state-metrics HA:

> 1. use a pessimistic approach when querying for kube-state-metrics, and perform for example doing something like:
>     max by(namespace, pod) (kube_pod_container_status_restarts)
> 3. Instead of whitebox monitoring kube-state-metrics put it behind a service and load balance to the instances.

However, if using method 1, we should modify most of our rules. If using method 2, we should configure a blackbox exporter or add additional-scrape-config to prometheus instance. The latter one will increase a little maintenance burden and have some inconsistent problems .

**What you expected to happen**:

So, how about using standby strategy to implement HA. For examples:

1. There are 3 kube-state-metrics replicas
2. ksm 1 ready and acquire leader lease successfully
3. ksm 1 start serve some metrics
4. ksm 2 and ksm 3 become ready and try to acqure lease but failed
5. ksm 2 and ksm 3 enter standby mode, wait for lease and serve empty metrics 
6. ksm 1 down, ksm 2 acquire lease, start serve metrics immediately, ksm 3 keep waiting.
7. a new instance ksm 4 is created and ready. it become standby mode because acquire lease failed.

It would ensure multiple kube-state-metrics instances expose single metric at the same time. And maybe it could be implemented by leaderelection pkg of k8s client-go conveniently.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version
",open,False,2018-12-04 11:23:40,2019-01-22 22:22:28
kube-state-metrics,sta-szek,https://github.com/kubernetes/kube-state-metrics/issues/612,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/612,Wrong values in labels (taken from prometheus pod instead of application pod),"/kind bug

Hi, first of all I tried to find solution but could not. Don't know if this is bug or feature but definitely stopped my work so I decided to ask for help. Is that correct repo for the issue?

**What happened**:
Metrics have values from prometheus pod / prometheus-prometheus-kube-state-metrics instead of application pod. Example:
```
{
app=""prometheus"",
chart=""prometheus-5.0.1"",
component=""kube-state-metrics"",
container=""cloud-microservice-template"",
heritage=""Tiller"",
instance=""xxxxxxx:8080"",
job=""kubernetes-service-endpoints"",
kubernetes_name=""prometheus-prometheus-kube-state-metrics"",
kubernetes_namespace=""monitoring"",
namespace=""default"",
pod=""cloud-microservice-template-598556f97-xbr7q"",
pod_label_app=""prometheus"",
pod_label_component=""kube-state-metrics"",
pod_label_maintainer_team=""infrastructure"",
pod_label_pod_template_hash=""3455604768"",
pod_label_release=""prometheus"",release=""prometheus""
}
```
In above example only `container` and `pod` are correct. The `pod_label_maintainer_team` is taken from prometheus instead of cloud-microservice-template deployment and because of that I cannot do correct mappings.

**What you expected to happen**:
All label values will be taken from application pod

**How to reproduce it (as minimally and precisely as possible)**:
Run prometheus helm chart, configure kubernetes-service-endpoint job: 
```
      - job_name: 'kubernetes-service-endpoints'

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
            replacement: pod_label_$1
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.2"", GitCommit:""17c77c7898218073f14c8d573582e8d2313dc740"", GitTreeState:""clean"", BuildDate:""2018-10-24T06:54:59Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:05:37Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kube-state-metrics image version
`k8s.gcr.io/kube-state-metrics:v1.1.0`",closed,False,2018-12-04 16:36:15,2018-12-05 16:54:54
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/613,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/613,Introduce sharding (and experimental auto-sharding),"**What this PR does / why we need it**:

This PR introduces sharding capabilities for kube-state-metrics, as well as the experimental feature to have kube-state-metrics automatically detect sharding, by discovering its nominal position within a StatefulSet and using the number of replicas in the StatefulSet as the total number of shards. I added a further description of the mechanism to the documentation.

These features will allow kube-state-metrics to be easily auto-scaled with the horizontal-pod-autoscaler by using its latency of `/metrics` requests to be lower than a certain threshold, otherwise scaling it up, as the latency is directly related to the amount of objects a shard needs to handle and the more replicas in a setup, the lower the number of objects, therefore a lower latency, as each shard handles 1/nth of objects in the cluster.

kube-state-metrics is well optimized, but in order to scale it infinitely and on commodity hardware it is necessary to shard the work it needs to do.

Note, I do not intend to ship these features in the already started release series of 1.5.x of kube-state-metrics, but rather release following it.

@andyxning @zouyee @mxinden @s-urbaniak @squat @metalmatze ",open,True,2018-12-08 23:17:28,2019-03-07 15:53:14
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/614,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/614,exchange heapster status,"

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
remove heapster description using metrics-server instead.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-12-11 02:20:05,2018-12-26 09:15:46
kube-state-metrics,zouyee,https://github.com/kubernetes/kube-state-metrics/pull/615,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/615,fix go fmt check,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
fix go fmt check

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-12-11 03:03:35,2018-12-11 05:09:00
kube-state-metrics,MIBc,https://github.com/kubernetes/kube-state-metrics/pull/616,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/616,Add labels in Exposed Metrics documentation,"
**What this PR does / why we need it**:
Documentation lacks some labels in node and pod metrics .

",closed,True,2018-12-11 07:24:48,2018-12-12 02:58:01
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/617,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/617,*: Cut v1.5.0-beta.0,"**What this PR does / why we need it**:

Cut v1.5.0-beta.0 of kube-state-metrics. I am not entirely sure what the time between alpha and beta was in the past but given that there are no bug reports so far I would suggest moving on to a beta release.

What are your thoughts?

",closed,True,2018-12-11 14:03:28,2018-12-12 02:02:52
kube-state-metrics,MIBc,https://github.com/kubernetes/kube-state-metrics/pull/618,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/618,Clear TODO comment in builder.go,"

**What this PR does / why we need it**:
Clear comment --- ""TODO : What if not ok?"". Because the collector's name has been checked in CollectorSet.Set function. It will exit if collector' name is not in DefaultCollectors. 
As a result, all collectors in Build.enabledCollectors will find its constructor function. 
So I think it doesn't exist ""not ok"" situation.

",closed,True,2018-12-13 08:01:27,2018-12-14 15:54:59
kube-state-metrics,MIBc,https://github.com/kubernetes/kube-state-metrics/pull/619,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/619,Delete it,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2018-12-14 06:50:00,2018-12-14 06:50:10
kube-state-metrics,LiliC,https://github.com/kubernetes/kube-state-metrics/pull/620,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/620,pkg/collectors: Remove leftover commented code,"**What this PR does / why we need it**:
As the `buildStatefulSetCollector` function has been refactored this code
can be removed.

cc @mxinden 


",closed,True,2018-12-20 14:53:36,2019-01-14 11:51:29
kube-state-metrics,svend,https://github.com/kubernetes/kube-state-metrics/pull/621,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/621,Cleanup whitespace in Kuberernetes manifest files,"Remove trailing whitespace and trailing newlines. Some of the manifests had trailing blank lines, others did not. Remove all trailing blank lines for consistency.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

Cleans whitespace in manifest files for consistency. No other changes.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #",closed,True,2018-12-21 18:12:11,2018-12-27 10:13:09
kube-state-metrics,idanlevin,https://github.com/kubernetes/kube-state-metrics/issues/622,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/622,Add support for clusters using PodSecurityPolicy,"**Is this a BUG REPORT or FEATURE REQUEST?**:
Feature Request
> Uncomment only one, leave it on its own line: 
>
> /kind bug
/kind feature


**What happened**:
When deploying to a cluster with PodSecurityPolicy enabled, the deployment fails to start with the following message:

```
message: 'pods ""monitoring-kube-state-metrics-7d5d8f9446-"" is forbidden: unable to validate against any pod security policy: []'
reason: FailedCreate
```

**What you expected to happen**:
The deployment should succeed

**How to reproduce it (as minimally and precisely as possible)**:
1. Create a cluster with PodSecurityPolicy enabled
2. Try to deploy the _kube-state-metrics_ application

**Anything else we need to know?**:
This can be solved by:
1. Create a dedicated PodSecurityPolicy for the service
2. Add to kube-state-metrics clusterrole the permissions to use the PSP - 
```
- apiGroups:
  - extensions
  resourceNames:
  - kube-state-metrics
  resources:
  - podsecuritypolicies
  verbs:
  - use
```

**Environment**:
- Kubernetes version (use `kubectl version`): 1.12.1
- Kube-state-metrics image version: v1.4.0
",closed,False,2018-12-26 12:00:02,2019-01-07 16:00:25
kube-state-metrics,ankon,https://github.com/kubernetes/kube-state-metrics/issues/623,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/623,ksm_resources_per_scrape no longer reported in telemetry metrics in 1.5.0.beta-0?,"/kind bug

**What happened**:

Noticed that I no longer see a graph derived from the `ksm_resources_per_scrape` metric, and using a `kubectl port-forward` on the telemetry port can see that no `ksm_` metrics are reported:
~~~~
# HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile=""0""} 5.9083e-05
go_gc_duration_seconds{quantile=""0.25""} 8.6843e-05
go_gc_duration_seconds{quantile=""0.5""} 0.000107557
go_gc_duration_seconds{quantile=""0.75""} 0.000141701
go_gc_duration_seconds{quantile=""1""} 0.078269036
go_gc_duration_seconds_sum 31.042589783
go_gc_duration_seconds_count 23037
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 67
# HELP go_info Information about the Go environment.
# TYPE go_info gauge
go_info{version=""go1.11.2""} 1
# HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.
# TYPE go_memstats_alloc_bytes gauge
go_memstats_alloc_bytes 5.7134624e+07
# HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed.
# TYPE go_memstats_alloc_bytes_total counter
go_memstats_alloc_bytes_total 5.95577483752e+11
# HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table.
# TYPE go_memstats_buck_hash_sys_bytes gauge
go_memstats_buck_hash_sys_bytes 2.091064e+06
# HELP go_memstats_frees_total Total number of frees.
# TYPE go_memstats_frees_total counter
go_memstats_frees_total 3.857933406e+09
# HELP go_memstats_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started.
# TYPE go_memstats_gc_cpu_fraction gauge
go_memstats_gc_cpu_fraction 0.00018497385816140357
# HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata.
# TYPE go_memstats_gc_sys_bytes gauge
go_memstats_gc_sys_bytes 1.1698176e+07
# HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use.
# TYPE go_memstats_heap_alloc_bytes gauge
go_memstats_heap_alloc_bytes 5.7134624e+07
# HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used.
# TYPE go_memstats_heap_idle_bytes gauge
go_memstats_heap_idle_bytes 2.70442496e+08
# HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use.
# TYPE go_memstats_heap_inuse_bytes gauge
go_memstats_heap_inuse_bytes 6.36928e+07
# HELP go_memstats_heap_objects Number of allocated objects.
# TYPE go_memstats_heap_objects gauge
go_memstats_heap_objects 215789
# HELP go_memstats_heap_released_bytes Number of heap bytes released to OS.
# TYPE go_memstats_heap_released_bytes gauge
go_memstats_heap_released_bytes 2.5776128e+08
# HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system.
# TYPE go_memstats_heap_sys_bytes gauge
go_memstats_heap_sys_bytes 3.34135296e+08
# HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection.
# TYPE go_memstats_last_gc_time_seconds gauge
go_memstats_last_gc_time_seconds 1.5466097257945971e+09
# HELP go_memstats_lookups_total Total number of pointer lookups.
# TYPE go_memstats_lookups_total counter
go_memstats_lookups_total 0
# HELP go_memstats_mallocs_total Total number of mallocs.
# TYPE go_memstats_mallocs_total counter
go_memstats_mallocs_total 3.858149195e+09
# HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures.
# TYPE go_memstats_mcache_inuse_bytes gauge
go_memstats_mcache_inuse_bytes 6912
# HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system.
# TYPE go_memstats_mcache_sys_bytes gauge
go_memstats_mcache_sys_bytes 16384
# HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures.
# TYPE go_memstats_mspan_inuse_bytes gauge
go_memstats_mspan_inuse_bytes 1.186056e+06
# HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system.
# TYPE go_memstats_mspan_sys_bytes gauge
go_memstats_mspan_sys_bytes 1.523712e+06
# HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place.
# TYPE go_memstats_next_gc_bytes gauge
go_memstats_next_gc_bytes 6.9154432e+07
# HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations.
# TYPE go_memstats_other_sys_bytes gauge
go_memstats_other_sys_bytes 889024
# HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator.
# TYPE go_memstats_stack_inuse_bytes gauge
go_memstats_stack_inuse_bytes 1.409024e+06
# HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator.
# TYPE go_memstats_stack_sys_bytes gauge
go_memstats_stack_sys_bytes 1.409024e+06
# HELP go_memstats_sys_bytes Number of bytes obtained from system.
# TYPE go_memstats_sys_bytes gauge
go_memstats_sys_bytes 3.5176268e+08
# HELP go_threads Number of OS threads created.
# TYPE go_threads gauge
go_threads 17
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 5221.69
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 1.048576e+06
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 11
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 1.11935488e+08
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.54548098584e+09
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 4.13028352e+08
~~~~

The log of kube-state-metrics contains repeatedly this stanza, which I assume is ""just fine"" as I can see other kube-state-metrics metrics just fine.
~~~~
E0104 13:48:11.054600       1 reflector.go:205] k8s.io/kube-state-metrics/pkg/collectors/builder.go:517: Failed to list *v2beta1.HorizontalPodAutoscaler: the server could not find the requested resource
E0104 13:48:11.601297       1 reflector.go:205] k8s.io/kube-state-metrics/pkg/collectors/builder.go:517: Failed to list *v1beta1.CronJob: the server could not find the requested resource
E0104 13:48:11.897050       1 reflector.go:205] k8s.io/kube-state-metrics/pkg/collectors/builder.go:517: Failed to list *v1beta1.PodDisruptionBudget: Forbidden: ""/apis/policy/v1beta1/poddisruptionbudgets?limit=500&resourceVersion=0"" (get poddisruptionbudgets.policy)
~~~~

**What you expected to happen**:

`ksm_resources_per_scrape` is reported.

**How to reproduce it (as minimally and precisely as possible)**:

Unclear. Last thing I remember was that I updated from 1.4.0 to 1.5.0.beta-0.

**Environment**:
- Kubernetes version (use `kubectl version`): 
~~~~
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.2"", GitCommit:""922a86cfcd65915a9b2f69f3f193b8907d741d9c"", GitTreeState:""clean"", BuildDate:""2017-07-21T08:23:22Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.8+coreos.0"", GitCommit:""4578c9a7285b3c6fa0a014e810b9ebe687205ff8"", GitTreeState:""clean"", BuildDate:""2017-10-04T09:08:47Z"", GoVersion:""go1.7.6"", Compiler:""gc"", Platform:""linux/amd64""}
~~~~
- Kube-state-metrics image version: 
~~~~
    Image:		quay.io/coreos/kube-state-metrics:v1.5.0-beta.0
    Image ID:		docker-pullable://quay.io/coreos/kube-state-metrics@sha256:aa95378c1114b5ada039073b0602cbdc1a71a54a4f08e2b4e610b2fd4ca44ec5
~~~~
",closed,False,2019-01-04 14:00:28,2019-01-10 12:23:10
kube-state-metrics,gjtempleton,https://github.com/kubernetes/kube-state-metrics/pull/624,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/624,Correct typo on new metric name,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Correct metric name in CHANGELOG.md from kube_replictaset_owner -> kube_replicaset_owner see #520 for reason it's been misnamed


",closed,True,2019-01-04 15:40:45,2019-01-06 08:34:33
kube-state-metrics,byxorna,https://github.com/kubernetes/kube-state-metrics/pull/625,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/625,fix labels for job metrics,"**What this PR does / why we need it**:

https://github.com/kubernetes/kube-state-metrics/pull/487 changed the `job` label on some metrics to `job_name`. This PR updates docs to reflect the correct label names for `kube_job_*` metrics.


",closed,True,2019-01-04 21:51:36,2019-01-06 05:21:19
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/626,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/626,version bumps and minor improvements in Dockerfile,"Signed-off-by: tariqibrahim <tariq181290@gmail.com>


**What this PR does / why we need it**:

Minor improvements and staying up to date.

",closed,True,2019-01-05 04:52:30,2019-01-15 06:04:51
kube-state-metrics,MarcusNoble,https://github.com/kubernetes/kube-state-metrics/pull/627,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/627,Added additional labels for pod metrics,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Add additional labels to the `kube_pod_status_ready` and `kube_pod_status_scheduled` metrics to provide the `host_ip`, `pod_ip`, `uid` and `node`.

This brings the values more in line with the `kube_pod_info` metric.

**Which issue(s) this PR fixes**:
N/A

",closed,True,2019-01-08 12:10:39,2019-01-08 13:45:37
kube-state-metrics,mytxyang,https://github.com/kubernetes/kube-state-metrics/issues/628,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/628,need build new v1.4.0 image with golang 1.11.4,"**Is this a BUG REPORT or FEATURE REQUEST?**:

> /kind feature

**What happened**:
we need kube-state-metrics v1.4.0 to upgrade with golang 1.11.4 to fix 3 security vulnerabilities:
https://groups.google.com/forum/#!topic/golang-announce/Kw31K8G7Fi0

Currently `gcr.io/google_containers/kube-state-metrics:v1.4.0` cannot pass security vulnerabilities.

**What you expected to happen**:
Please help to build `gcr.io/google_containers/kube-state-metrics:v1.4.0` with golang 1.11.4.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):1.11.0
- Kube-state-metrics image version: v1.4.0
",closed,False,2019-01-10 03:02:02,2019-01-14 05:27:15
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/629,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/629,Cut 1.5.0,"After an additional testing period of 30 days, there were no additional bugs found or features introduced. Due to no bugs being reported over an in total 41 days period, I feel no more pre-releases are necessary for a stable release.

This release's focus was a large architectural change in order to improve performance and resource usage of kube-state-metrics drastically. Special thanks to @mxinden for his hard work on this! See the changelog of the pre-releases for more detailed information and related pull requests.

@metalmatze @s-urbaniak @mxinden @squat @andyxning @zouyee 

@loburm @piosz @kawych @DirectXMan12 please stand by to publish the gcr.io image.",closed,True,2019-01-10 10:42:45,2019-01-14 15:25:56
kube-state-metrics,ankon,https://github.com/kubernetes/kube-state-metrics/pull/630,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/630,Remove the mentioning of `ksm_scrape_error_total` and `ksm_resources_per_scrape` metrics,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

It removes outdated documentation.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #623

",closed,True,2019-01-10 11:09:48,2019-01-11 08:53:18
kube-state-metrics,ankon,https://github.com/kubernetes/kube-state-metrics/pull/631,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/631,Remove the obsolete metrics code,"These stopped getting populated with 234d788a0d5cfac2b5bf33153afd3b944c39b220, and
cannot be restored easily.

Related-to: #623

_This is intended for after 1.5.0, as per https://github.com/kubernetes/kube-state-metrics/pull/629#issuecomment-453057404_

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

It removes obsolete code that could be misleading when reading this in the future.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:


",closed,True,2019-01-10 11:11:02,2019-01-14 11:21:08
kube-state-metrics,LiliC,https://github.com/kubernetes/kube-state-metrics/pull/632,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/632,*: Move collectors pkg to internal dir,"Rename the existing functions that are used in collectors to collector
pkg.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Expose helper functions that can be used outside of kube-state-metrics and move collector package `collectors` to internal package. The `internal` directory is in the root of the project, this is so we can call this in the `main.go` file, import of a path containing the element “internal” is disallowed if the importing code is outside the tree rooted at the parent of the “internal” directory.

This is what we need from kube-state-metrics, but if @wanghaoran1988 needs something more, I can adjust things. Also open for naming discussions, etc. :)

cc @mxinden @brancz 

**Which issue(s) this PR fixes**:
Fixes https://github.com/kubernetes/kube-state-metrics/issues/579

",closed,True,2019-01-15 17:58:12,2019-03-05 08:35:01
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/633,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/633,Deduplicate metric name,"**What this PR does / why we need it**:

Instead of defining the metric name both in the metric family generator
as well as the metric itself, with this patch the metric name is
injected into the metric family by the metric family generator.
",closed,True,2019-01-16 12:15:44,2019-01-28 13:42:11
kube-state-metrics,brancz,https://github.com/kubernetes/kube-state-metrics/pull/634,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/634,OWNERS: Add @mxinden,"Given @mxinden probably knows this code base the best right now. It's only fair to have him be an owner. Congrats!

@andyxning @zouyee ",closed,True,2019-01-16 15:03:51,2019-01-17 11:31:12
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/635,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/635,clarify the import location of Store interface in comments,Minor correction in comments to indicate that we are importing from client-go and not upstream k8s :),closed,True,2019-01-17 22:42:42,2019-01-19 03:35:19
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/636,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/636,run gofmt -s on all of the source files,Running gofmt simplify on all the files,closed,True,2019-01-19 03:34:12,2019-01-22 01:46:44
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/637,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/637,tests: Add check-license script,"**What this PR does / why we need it**:

Add a script to check, whether each golang file has a license header.",closed,True,2019-01-21 15:08:17,2019-01-22 13:35:57
kube-state-metrics,kminehart,https://github.com/kubernetes/kube-state-metrics/pull/638,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/638,Add replicaset label metrics,"<!--  Thanks for sending a pull request!  Here are some tips for you:
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**: This PR adds the `kube_replicaset_labels` metric. In some edge case scenarios this can be useful.

In our environment, we heavily use labels for tenancy. Developers want to be able to see how often they ""deploy"" an update; this can normally be done using `kube_deployment_created`, unless the deployment is updated. When the deployment is updated, a new replicaset is created, so we use `kube_replicaset_created`, but we can not determine what team owns the replicaset without being able to access its labels

I'm having minikube issues so I'm unable to run the e2e tests locally :sweat_smile: ",closed,True,2019-01-22 17:01:19,2019-01-23 09:21:06
kube-state-metrics,LiliC,https://github.com/kubernetes/kube-state-metrics/pull/639,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/639,*: Set e2e test env variables in .travis.yaml,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
If you run the make target for e2e tests locally this will by default presume you are running in CI and reinstall minikube, etc.

This adds the env variables to `.travis.yaml` instead and updates the documentation.


",closed,True,2019-01-24 14:18:49,2019-01-24 15:39:36
kube-state-metrics,r0fls,https://github.com/kubernetes/kube-state-metrics/pull/640,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/640,add ingress labels,"**What this PR does / why we need it**:
Add an Ingress collector. There is currently no ingress collector.

**Which issue(s) this PR fixes**:
Fixes #573 
",closed,True,2019-01-24 23:27:09,2019-03-29 11:44:23
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/641,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/641,use the latest patch release of go 1.11,Updating to go 1.11.5 since it has security fixes.,closed,True,2019-01-27 01:17:30,2019-01-28 01:53:16
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/642,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/642,remove dependency on kubernetes/kubernetes,"It is considered best practice to not rely on kubernetes/kubernetes as a dependency as it's not really meant for external consumption.

Moreover, the methods and variables imported in this project are simple logic which can just reside in the project itself IMO.

This PR addresses that",closed,True,2019-01-27 01:43:10,2019-01-31 15:28:35
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/643,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/643,tests: Add benchmark compare script and include in CI,"**What this PR does / why we need it**:

Add a script that runs all benchmark tests on the current git ref and
the last release and add it to the Makefile and .travis.yml file.

This does not enforce certain benchmark results, but at least gives people the chance to easily compare their changes performance-wise.

Idea came up here: https://github.com/kubernetes/kube-state-metrics/pull/633#issuecomment-458084905",closed,True,2019-01-28 13:41:35,2019-01-29 12:08:25
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/644,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/644,pkg/metric/generator: Refactor GenerateFunc call and return pointer,"**What this PR does / why we need it**:

**pkg/metric/generator: Introduce Generate method calling GenerateFunc**

Instead of making an external component responsible of adding the family name to
each family, enforce this in the generator via the `Generate` function.

**pkg/metric/generator: Make GenerateFunc return pointer to Family**

Instead of returning a Family right away, make GenerateFunc return a pointer to
a Family, matching the convention across the k8s project and removing the need
to do so later in the code path.
",closed,True,2019-01-28 15:44:20,2019-02-04 18:44:29
kube-state-metrics,mgoodness,https://github.com/kubernetes/kube-state-metrics/pull/645,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/645,Enable use as kustomize base layer,"**What this PR does / why we need it**:

- Primarily adds a `kustomization.yaml` file to enable use as a [kustomize](https://github.com/kubernetes-sigs/kustomize) base layer.
- Also adds resources from the `apps` group to the Role and ClusterRole.
- Updates the Deployment to `apps/v1`
",closed,True,2019-01-28 20:29:42,2019-02-01 19:54:15
kube-state-metrics,eyalev,https://github.com/kubernetes/kube-state-metrics/pull/646,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/646,Get GCP email account from gcloud's 'value' format,"https://cloud.google.com/sdk/gcloud/reference/topic/formats

",closed,True,2019-01-29 11:15:11,2019-01-29 14:55:44
kube-state-metrics,wozniakjan,https://github.com/kubernetes/kube-state-metrics/pull/647,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/647,Remove accidentally committed file,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
It looks to me the `internal/collector/filename` may have been accidentally committed here
https://github.com/kubernetes/kube-state-metrics/pull/632/files#diff-13f62cf8c67fedfe14d52d72048702db

It is empty and I couldn't find it being used anywhere, unit tests pass well without it :)

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
none, just a cleanup

cc: @LiliC 
",closed,True,2019-01-29 14:59:08,2019-01-29 15:14:33
kube-state-metrics,mrueg,https://github.com/kubernetes/kube-state-metrics/pull/648,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/648,Makefile: Add local build target,"Signed-off-by: Manuel Rüger <manuel@rueg.eu>

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Adds a local-build target, which allows to be agnostic of Docker and allow building it in one's own golang container.
",closed,True,2019-01-29 17:30:04,2019-01-30 21:46:57
kube-state-metrics,wanghaoran1988,https://github.com/kubernetes/kube-state-metrics/issues/649,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/649,Add collector for CSR,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature
As a SRE, I'd like monitor the csr status, like if it's pending for a long time, and not approved, there will be problem, eg. the node create csr for it's client/server certs, pending will cause the node outage.

**What happened**:

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version
",closed,False,2019-01-31 03:44:31,2019-03-24 20:07:05
kube-state-metrics,wanghaoran1988,https://github.com/kubernetes/kube-state-metrics/pull/650,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/650,Add collector for csr,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #649 

",closed,True,2019-01-31 11:34:03,2019-03-24 20:08:08
kube-state-metrics,wozniakjan,https://github.com/kubernetes/kube-state-metrics/pull/651,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/651,[WIP] Native pod termination counter - proposed implementation,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
I would like to continue discussion from https://github.com/kubernetes/kube-state-metrics/issues/344. While using `LastTerminationState` allows to catch more OOMKills, it still misses those where during the scrape period container dies more than once with different termination state. I am not entirely sure this implementation would follow conventions set by other metrics but it does allow never to miss an OOMKill because it can increment own counter on each observed pod change.

It is split into two commits
- first one allowing unit tests to take more than one object
- implementing the observed termination counter

Looking forward to any comments and hoping we can come up with good solution together :)

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2019-01-31 16:45:30,2019-02-04 18:34:50
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/652,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/652,use the appsv1 package as it is more appropriate for deployment runtimeObject,"Using the `""k8s.io/api/apps/v1""` for the deployment object.",closed,True,2019-02-01 06:37:51,2019-02-02 16:11:11
kube-state-metrics,jenciso,https://github.com/kubernetes/kube-state-metrics/issues/653,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/653,Installation process give me error due Kustomization.yaml file,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
Error during the installation of ksm

**What you expected to happen**:
The easy installation process should terminate correctly. But when I apply the kubectl command, it show me an error. It is due the kustomization.yaml file

**How to reproduce it (as minimally and precisely as possible)**:
`kubectl apply -f kubernetes`
show this error: 
error: error validating ""kubernetes/kustomization.yaml"": error validating data: [apiVersion not set, kind not set]; if you choose to ignore these errors, turn validation off with --validate=false

**Anything else we need to know?**:
The kustomization.yaml file was added 4 days ago and it resides into the kubernetes directoy, is for that reason that apply command give this error. @mgoodness 

**Environment**: Kubernetes 1.13.2
- Kubernetes version (use `kubectl version`): 1.13.2
- Kube-state-metrics image version 1.5.0
",closed,False,2019-02-01 18:57:43,2019-02-04 19:07:08
kube-state-metrics,mgoodness,https://github.com/kubernetes/kube-state-metrics/pull/654,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/654,Move kustomization.yaml,"Signed-off-by: Michael Goodness <mike.goodness@ticketmaster.com>

**What this PR does / why we need it**:

`kubectl apply -f kubernetes/` is broken, as `kustomization.yaml` isn't (yet) something kubectl can parse. This moves it to the root of the repo.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #653 

",closed,True,2019-02-01 20:05:30,2019-02-04 19:07:08
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/655,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/655,correct some typographical mistakes in kube-state-metrics,Fixing some typos :),closed,True,2019-02-03 05:55:34,2019-02-04 16:20:20
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/656,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/656,documentation fixes in kube-state-metrics,Documentation improvements,closed,True,2019-02-04 09:01:08,2019-02-04 19:00:51
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/657,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/657,documentation fixes: i)minor improvements ii)add new hpa labels in do…,@brancz FYI,closed,True,2019-02-04 19:00:34,2019-02-04 19:14:40
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/issues/658,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/658,Change dependency management tool for kube-state-metrics,"Godeps is too old and we should really try changing it to a more mature tool like `dep` or `go.mod`

My vote is for dep. Here are the reasons why

i) `dep` is the more mature tool (for now) and easy to use.
ii) A good number of OSS projects use `dep`
iii) `go.mod` still hasn't reached that level of maturity yet and we should wait until newer versions for Go are release. Moreover, `go.mod` provides an easy transition out of `dep`. Once go modules achieves good stability and adoption rates, the conversion should be easy.

Thoughts @brancz @andyxning @mxinden @zouyee ?",closed,False,2019-02-04 20:37:17,2019-02-07 12:01:40
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/659,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/659,add go modules support for kube-state-metrics,Go mod support for kube-state-metrics. Removes Godeps.  Fixes #658 ,closed,True,2019-02-05 17:09:45,2019-02-07 16:49:29
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/660,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/660,README.md: Reword raw data section,"

**What this PR does / why we need it**:

Improve wording in README.md


",closed,True,2019-02-06 15:51:12,2019-02-07 10:27:42
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/661,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/661,cleanup api groups and add ingresses to the cluster role RBAC,Using appsv1 for replicasets and daemonset as it's more appropriate. Adding also adding ingresses to the extensions RBAC,closed,True,2019-02-07 21:44:03,2019-02-08 08:26:18
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/662,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/662,adding the CI check for go modules consistency,Updated Travis CI to check for go mod consistency.,closed,True,2019-02-07 22:01:06,2019-02-08 17:09:14
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/663,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/663,add the ingress resource to the extensions apigroup section of RBAC,,closed,True,2019-02-08 08:31:04,2019-02-08 10:39:20
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/664,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/664,add shellcheck to the ci and minor cleanup of bash scripts in kube_state_metrics,,closed,True,2019-02-09 02:39:34,2019-02-12 16:32:16
kube-state-metrics,vivekj11,https://github.com/kubernetes/kube-state-metrics/issues/665,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/665,How kube-metrics will be fetched by Prometheus if it is available on a different node,"


**What happened**:
I have a k8s cluster on which I deployed Kube-state-metrics and Prometheus. Things worked fine, Prometheus was able to fetch all metrics.
Now I want to keep Prometheus on a different server(not a part of the cluster). when I checked Kube-state-metrics, they are not available externally. 
So how can I forward all metrics from my k8s cluster node using Kube-state-metrics to my Prometheus 
which is hosted on a different node.

I am looking for some endpoint which I can put in my prometheus.yml file so it can start scraping metrics from this cluster node. 
",closed,False,2019-02-10 15:09:00,2019-02-11 10:17:30
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/666,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/666,Changed the collector name from ingress to ingresses to match the convention,"The cmd parameter for the ingress collector should be ""ingresses"" since all the other apigroup resources go by their plurals for their collector names.

This change should be safe IMO since the latest release does not include ingresses as yet.",closed,True,2019-02-11 03:39:53,2019-02-11 10:34:23
kube-state-metrics,wfernandes,https://github.com/kubernetes/kube-state-metrics/issues/667,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/667,Documentation of metrics-server is misleading,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
This is a bug report for the documentation section [kube-state-metrics vs. metrics-server(Heapster)](https://github.com/kubernetes/kube-state-metrics#kube-state-metrics-vs-metrics-serverheapster).

Although, the metrics-server is known as a light-weight heapster, I feel that the documentation has some misleading points. For example in the kube-state-metrics README it says,

> metrics-server(Heapster) is a project which fetches metrics (such as CPU and memory utilization) from the Kubernetes API server and nodes and sends them to various time-series backends such as InfluxDB or Google Cloud Monitoring.

This is incorrect when looking at:
- the [docs of the metrics-server](https://github.com/kubernetes/community/blob/5b2e4f19565f62c5af3803d82d21fc6fb8d7d7d8/contributors/design-proposals/instrumentation/metrics-server.md#design) 
> It will be a cluster level component which periodically scrapes metrics from all Kubernetes nodes served by Kubelet through Summary API. Then metrics will be aggregated, stored in memory (see Scalability limitations) and served in Metrics API format.

- and the [monitoring pipeline design proposal](https://github.com/kubernetes/community/blob/5b2e4f19565f62c5af3803d82d21fc6fb8d7d7d8/contributors/design-proposals/instrumentation/monitoring_architecture.md#core-metrics-pipeline)
> These sources [kubelet, resource estimator] are scraped by a component we call metrics-server which is like a slimmed-down version of today's Heapster. metrics-server stores locally only latest values and has **no sinks**

**What you expected to happen**:

Perhaps this section should focus on the differences between kube-state-metrics and metrics-server and ignore references to heapster since it is going to be deprecated anyways.

Sections like.

> metrics-server(Heapster) only needs to fetch, format and forward metrics that already exist, in particular from Kubernetes components, and write them into sinks, which are the actual monitoring systems

> metrics-server(Heapster) is a project which fetches metrics (such as CPU and memory utilization) from the Kubernetes API server and nodes and sends them to various time-series backends such as InfluxDB or Google Cloud Monitoring.

should be deleted.
",closed,False,2019-02-11 04:20:24,2019-02-21 17:20:33
kube-state-metrics,dengceltics,https://github.com/kubernetes/kube-state-metrics/issues/668,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/668,Stateful Set Metrics can't found,"I first got the latest version of the Mirror(quay.io/coreos/kube-state-metrics:v1.5.0). 
state-metrics-deployment.yaml:
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: monitoring
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      serviceAccountName: kube-state-metrics
      containers:
      - name: kube-state-metrics
        image: quay.io/coreos/kube-state-metrics:v1.5.0
        ports:
        - containerPort: 8080

Why i can get the newest metrics such as kube_pod_container_status_last_terminated_reason,but can't find the metrics related statefulset,such as kube_statefulset_status_replicas.",closed,False,2019-02-12 09:52:43,2019-02-12 17:06:54
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/669,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/669,update Dockerfile to use latest alpine,"Use the latest alpine linux base image. v3.9 was released last month

More details: http://dl-cdn.alpinelinux.org/alpine/v3.9/",closed,True,2019-02-12 19:29:44,2019-02-13 04:43:27
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/670,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/670,fix the release documentation,Minor doc fixes for release stuff.,closed,True,2019-02-13 03:48:29,2019-02-18 10:25:08
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/671,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/671,use cancelable context instead of context.TODO(),Fixes #600 ,closed,True,2019-02-13 18:22:17,2019-02-20 05:15:11
kube-state-metrics,andre-franco,https://github.com/kubernetes/kube-state-metrics/issues/672,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/672,kube-state-metrics tries to start 2 pods on Master node,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
kube-state-metrics deployment tries to start 2 pods when I schedule it to run on a master node despite replicas being set to 1.
**What you expected to happen**:
That only one pod is scheduled on a master node.
**How to reproduce it (as minimally and precisely as possible)**:
Deploy kube-state-metrics using nodeSelector and tolerations as below:

```
nodeSelector:
  beta.kubernetes.io/os: linux
  kubernetes.io/hostname: master-node1
tolerations:
- effect: NoSchedule
  key: node-role.kubernetes.io/master
```

**Anything else we need to know?**:
One pods runs normally and the other pod which wasn't even supposed to be scheduled stays in a Pending state because it ignores the tolerations I set and tries to schedule it on a master node anyway because of nodeSelector:

```
[root@master-node1 ~]# kubectl get pods -n monitoring-prometheus -o wide | grep kube-state-metrics
kube-state-metrics-784996ddd9-vd2bk    4/4     Running   0          17h   10.244.1.69     master-node1   <none>           <none>
kube-state-metrics-798cbf4476-lxvsl    0/4     Pending   0          17h   <none>          <none>                             <none>           <none>
```

Here's the output of kubectl describe pod on the Pending pod:

```
Name:               kube-state-metrics-798cbf4476-lxvsl
Namespace:          monitoring-prometheus
Priority:           0
PriorityClassName:  <none>
Node:               <none>
Labels:             app=kube-state-metrics
                    pod-template-hash=798cbf4476
Annotations:        <none>
Status:             Pending
IP:
Controlled By:      ReplicaSet/kube-state-metrics-798cbf4476
Containers:
  kube-rbac-proxy-main:
    Image:      quay.io/coreos/kube-rbac-proxy:v0.4.1
    Port:       8443/TCP
    Host Port:  0/TCP
    Args:
      --logtostderr
      --secure-listen-address=:8443
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:8081/
    Limits:
      cpu:     20m
      memory:  40Mi
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-state-metrics-token-rdj26 (ro)
  kube-rbac-proxy-self:
    Image:      quay.io/coreos/kube-rbac-proxy:v0.4.1
    Port:       9443/TCP
    Host Port:  0/TCP
    Args:
      --logtostderr
      --secure-listen-address=:9443
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:8082/
    Limits:
      cpu:     20m
      memory:  40Mi
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-state-metrics-token-rdj26 (ro)
  kube-state-metrics:
    Image:      quay.io/coreos/kube-state-metrics:v1.5.0
    Port:       <none>
    Host Port:  <none>
    Args:
      --host=127.0.0.1
      --port=8081
      --telemetry-host=127.0.0.1
      --telemetry-port=8082
    Limits:
      cpu:     112m
      memory:  330Mi
    Requests:
      cpu:        112m
      memory:     330Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-state-metrics-token-rdj26 (ro)
  addon-resizer:
    Image:      quay.io/coreos/addon-resizer:1.0
    Port:       <none>
    Host Port:  <none>
    Command:
      /pod_nanny
      --container=kube-state-metrics
      --cpu=100m
      --extra-cpu=2m
      --memory=150Mi
      --extra-memory=30Mi
      --threshold=5
      --deployment=kube-state-metrics
    Limits:
      cpu:     50m
      memory:  30Mi
    Requests:
      cpu:     10m
      memory:  30Mi
    Environment:
      MY_POD_NAME:       kube-state-metrics-798cbf4476-lxvsl (v1:metadata.name)
      MY_POD_NAMESPACE:  monitoring-prometheus (v1:metadata.namespace)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-state-metrics-token-rdj26 (ro)
Conditions:
  Type           Status
  PodScheduled   False
Volumes:
  kube-state-metrics-token-rdj26:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-state-metrics-token-rdj26
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
                 kubernetes.io/hostname=master-node1
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>
```

Here's the yaml file I'm using to deploy it:

```
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  labels:
    app: kube-state-metrics
  name: kube-state-metrics
  namespace: monitoring-prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      containers:
      - args:
        - --logtostderr
        - --secure-listen-address=:8443
        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
        - --upstream=http://127.0.0.1:8081/
        image: quay.io/coreos/kube-rbac-proxy:v0.4.1
        name: kube-rbac-proxy-main
        ports:
        - containerPort: 8443
          name: https-main
        resources:
          limits:
            cpu: 20m
            memory: 40Mi
          requests:
            cpu: 10m
            memory: 20Mi
      - args:
        - --logtostderr
        - --secure-listen-address=:9443
        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
        - --upstream=http://127.0.0.1:8082/
        image: quay.io/coreos/kube-rbac-proxy:v0.4.1
        name: kube-rbac-proxy-self
        ports:
        - containerPort: 9443
          name: https-self
        resources:
          limits:
            cpu: 20m
            memory: 40Mi
          requests:
            cpu: 10m
            memory: 20Mi
      - args:
        - --host=127.0.0.1
        - --port=8081
        - --telemetry-host=127.0.0.1
        - --telemetry-port=8082
        image: quay.io/coreos/kube-state-metrics:v1.5.0
        name: kube-state-metrics
        resources:
          limits:
            cpu: 100m
            memory: 150Mi
          requests:
            cpu: 100m
            memory: 150Mi
      - command:
        - /pod_nanny
        - --container=kube-state-metrics
        - --cpu=100m
        - --extra-cpu=2m
        - --memory=150Mi
        - --extra-memory=30Mi
        - --threshold=5
        - --deployment=kube-state-metrics
        env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        image: quay.io/coreos/addon-resizer:1.0
        name: addon-resizer
        resources:
          limits:
            cpu: 50m
            memory: 30Mi
          requests:
            cpu: 10m
            memory: 30Mi
      nodeSelector:
        beta.kubernetes.io/os: linux
        kubernetes.io/hostname: master-node1
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: kube-state-metrics
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.2"", GitCommit:""cff46ab41ff0bb44d8584413b598ad8360ec1def"", GitTreeState:""clean"", BuildDate:""2019-01-10T23:35:51Z"", GoVersion:""go1.11.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.2"", GitCommit:""cff46ab41ff0bb44d8584413b598ad8360ec1def"", GitTreeState:""clean"", BuildDate:""2019-01-10T23:28:14Z"", GoVersion:""go1.11.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kube-state-metrics image version
v1.5.0",open,False,2019-02-14 12:35:12,2019-03-21 06:41:52
kube-state-metrics,r0fls,https://github.com/kubernetes/kube-state-metrics/pull/673,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/673,add pvc access mode metric,"**What this PR does / why we need it**: Adds PVC access mode as metric `kube_persistentvolumeclaim_access_mode`

**Which issue(s) this PR fixes**:
Fixes #554 

",closed,True,2019-02-15 23:40:51,2019-02-20 21:23:59
kube-state-metrics,r0fls,https://github.com/kubernetes/kube-state-metrics/pull/674,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/674,add persistent volume capacity metric,"**What this PR does / why we need it**: Add `kube_persistentvolume_capacity` to track PV capacity.

**Which issue(s) this PR fixes**:
Fixes #555

",closed,True,2019-02-16 00:22:50,2019-02-22 06:08:13
kube-state-metrics,wfernandes,https://github.com/kubernetes/kube-state-metrics/pull/675,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/675,Update metric-server documentation in README,"
<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Updates README documentation that avoids potential confusion for readers trying to understand all the various observability components of kubernetes.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #667 

",closed,True,2019-02-18 22:00:49,2019-02-21 17:20:33
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/676,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/676,use klog instead of glog,Being good K8s citizens :),closed,True,2019-02-20 04:03:34,2019-02-20 12:13:21
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/677,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/677,docs: rename the Documentation folder to docs,`docs` seem to be unanimously chosen folder name for Documentation in GitHub open source projects.,closed,True,2019-02-20 05:07:38,2019-02-22 14:57:01
kube-state-metrics,akshatgit,https://github.com/kubernetes/kube-state-metrics/issues/678,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/678,Graphite integration,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature


**What happened**:
We use graphite as the default tool to store system and application metrics in our infra. It would be great if we could directly send metrics using kube-state-metrics to an external graphite endpoint. 
**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Kube-state-metrics image version
",open,False,2019-02-21 07:13:07,2019-02-21 10:28:21
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/679,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/679,add tariq1890 to the reviewers list,cc @brancz @mxinden ,closed,True,2019-02-21 15:19:54,2019-02-21 17:21:47
kube-state-metrics,jmreicha,https://github.com/kubernetes/kube-state-metrics/issues/680,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/680,Missing quay.io/gcr arm64 images?,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

I see there are instructions in the `Makefile` for building and pushing multiarch images, but don't actually see the images on the registries, just the amd64 image.

```
manifest-tool inspect k8s.gcr.io/kube-state-metrics:v1.5.0
k8s.gcr.io/kube-state-metrics:v1.5.0: manifest type: application/vnd.docker.distribution.manifest.v2+json
      Digest: sha256:37c4ddfa160043a7fd73703b7cae41cf38b692ca19c1dff722293c90a386b59f
Architecture: amd64
          OS: linux
    # Layers: 2
      layer 1: digest = sha256:cd784148e3483c2c86c50a48e535302ab0288bebd587accf40b714fffd0646b3
      layer 2: digest = sha256:cff0390a13ca215dcab67321b8e7d4a84955bb5fa0fd39de426d4621bc864c02
```

**What you expected to happen**:

I was expecting to see manifests/images for different architectures (as listed in the Makefile).  Am I missing something here?",closed,False,2019-02-22 19:26:22,2019-03-01 03:03:18
kube-state-metrics,zuzzas,https://github.com/kubernetes/kube-state-metrics/pull/681,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/681,Export Job's owner,"**What this PR does / why we need it**:

This PR adds a Job's object owner metric similar to the Pod's owner. It is quite helpful when trying to correlate Jobs and CronJobs.
",closed,True,2019-02-25 09:10:52,2019-02-26 13:55:30
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/682,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/682,add collector for networkpolicies,,closed,True,2019-02-25 22:20:34,2019-03-11 17:25:06
kube-state-metrics,nlamirault,https://github.com/kubernetes/kube-state-metrics/issues/683,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/683,Error on a Kubernetes 1.13.3 cluster,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:

I deploy the application on a v1.13.3 cluster. And i've got an error when the container start:

**What you expected to happen**:

```
$ kubectl get pods -n kube-system -l k8s-app=kube-state-metrics  -o wide
NAME                                  READY   STATUS             RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
kube-state-metrics-7f9799c9c8-rzt4c   0/2     CrashLoopBackOff   12         8m56s   10.244.4.4   jarvis-node3   <none>           <none>

$ kubectl logs kube-state-metrics-7f9799c9c8-rzt4c -n kube-system -c kube-state-metrics
I0226 14:59:46.276097       1 main.go:80] Using default collectors
I0226 14:59:46.276562       1 main.go:88] Using all namespace
I0226 14:59:46.276617       1 main.go:124] metric white-blacklisting: blacklisting the following items: 
W0226 14:59:46.276780       1 client_config.go:552] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0226 14:59:46.286559       1 main.go:166] Testing communication with server
I0226 14:59:46.446532       1 main.go:171] Running with Kubernetes cluster version: v1.13. git version: v1.13.3. git tree state: clean. commit: 721bfa751924da8d1680787490c54b9179b1fed0. platform: linux/arm64
I0226 14:59:46.446716       1 main.go:173] Communication with server successful
I0226 14:59:46.461246       1 main.go:182] Starting kube-state-metrics self metrics server: 0.0.0.0:81
2019/02/26 14:59:46 listen tcp 0.0.0.0:81: bind: permission denied
```

**How to reproduce it (as minimally and precisely as possible)**:

deploy the YAML files from v1.5 release on a v1.13.3 cluster.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): 
```bash
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.3"", GitCommit:""721bfa751924da8d1680787490c54b9179b1fed0"", GitTreeState:""clean"", BuildDate:""2019-02-01T20:08:12Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/arm64""}
Server Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.3"", GitCommit:""721bfa751924da8d1680787490c54b9179b1fed0"", GitTreeState:""clean"", BuildDate:""2019-02-01T20:00:57Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/arm64""}
```
- Kube-state-metrics image version : v1.5.0
",closed,False,2019-02-26 15:01:32,2019-02-26 16:57:16
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/issues/684,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/684,Make container/E2e tests are not referring to the latest image tag of kube-state-metrics,"We are still extracting `v1.5.0-alpha` as the latest release version when running the `e2e` tests and `make container`

This is probably happening because `git describe --abbrev=0` returns the following
`v1.5.0-alpha.0`

We would need to push the latest tag `1.5.0` to the current master.

/cc @mxinden @brancz 
",closed,False,2019-02-27 02:31:58,2019-02-28 16:39:53
kube-state-metrics,benjaminhuo,https://github.com/kubernetes/kube-state-metrics/issues/685,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/685,resourcequota metrics missing in v1.5.0,"
![snip20190227_336](https://user-images.githubusercontent.com/18525465/53482846-1e1ca580-3abb-11e9-8b3f-20558ed0fd67.png)
![snip20190227_337](https://user-images.githubusercontent.com/18525465/53482848-1e1ca580-3abb-11e9-846c-d4608ae15420.png)
![snip20190227_338](https://user-images.githubusercontent.com/18525465/53482850-1eb53c00-3abb-11e9-9aed-fb91e6f5102c.png)


Is resourcequota metrics like kube_resourcequota removed in kube-state-metrics v1.5.0?
I'm using prometheus deploy by prometheus operator 0.27, but cannot find resourcequota related metrics anymore as I can in v1.3.2
If it's removed, how can I get it back?

Thanks very much
Ben",closed,False,2019-02-27 10:09:32,2019-02-27 15:32:19
kube-state-metrics,zuzzas,https://github.com/kubernetes/kube-state-metrics/pull/686,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/686,"Revert ""Do not export Job's owner metrics if Job's OwnerReference is …","This reverts commit 15b93c4706372623cd0bbba413f7ae461cce4406.

Based on @brancz's decision here: https://github.com/kubernetes/kube-state-metrics/issues/569#issuecomment-467393722
",closed,True,2019-02-27 11:18:35,2019-02-28 07:34:57
kube-state-metrics,cgetzen,https://github.com/kubernetes/kube-state-metrics/issues/687,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/687,Add time metrics,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug

/kind feature


**What happened**:
There is no way to track how long most pod/container events take. I would like to be able to track pull times, start times, etc.

**What you expected to happen**:
For all kube_pod_container_status_* events, I would like to have access to a kube_pod_container_status_*_time, in order to track how long events take.",open,False,2019-02-27 17:21:01,2019-03-01 17:57:55
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/688,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/688,replace git describe with VERSION file for maintaining version tagging,Fixes #684 ,closed,True,2019-02-28 15:24:17,2019-03-01 04:21:25
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/689,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/689,improve the variable name of the kube state metrics image tag in e2e shell script,Fixing variable name for readability and consistency.,closed,True,2019-02-28 15:41:10,2019-02-28 16:41:45
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/690,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/690,improve the release documentation,Release docs need to be updated given the changes in #688 ,closed,True,2019-03-01 04:04:00,2019-03-01 08:29:47
kube-state-metrics,estahn,https://github.com/kubernetes/kube-state-metrics/issues/691,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/691,kube_job_spec_active_deadline_seconds not present when job fails,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

* A Job has exceeded it's `activeDeadlineSeconds` and was terminated.
* I wanted to alert on these by using the following rule:

```
sum by(job_name) (kube_job_status_completion_time{job_name!=""""} - kube_job_status_start_time) > max by(job_name) (kube_job_spec_active_deadline_seconds{job_name!=""""})
```
This will not work as `kube_job_spec_active_deadline_seconds` is not populated for this failing job.

**What you expected to happen**:

Have specs available regardless of whether the job fails.

**How to reproduce it (as minimally and precisely as possible)**:

N/A

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.2"", GitCommit:""cff46ab41ff0bb44d8584413b598ad8360ec1def"", GitTreeState:""clean"", BuildDate:""2019-01-13T23:15:13Z"", GoVersion:""go1.11.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.6"", GitCommit:""b1d75deca493a24a2f87eb1efde1a569e52fc8d9"", GitTreeState:""clean"", BuildDate:""2018-12-16T04:30:10Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kube-state-metrics image version

```
quay.io/coreos/kube-state-metrics:v1.4.0
```",closed,False,2019-03-01 09:06:17,2019-03-04 22:20:42
kube-state-metrics,trobert2,https://github.com/kubernetes/kube-state-metrics/pull/692,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/692,Fixes typo in README.md from Secrects to Secrets,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide/pull-requests.md#the-pull-request-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

",closed,True,2019-03-01 15:17:54,2019-03-01 15:54:16
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/693,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/693,fix all golint and gosimple issues in kube-state-metrics,"Fix all golint issues in kube-state-metrics.

This should bring the goreportcard to 100%",closed,True,2019-03-02 18:04:32,2019-03-04 09:31:58
kube-state-metrics,bergerx,https://github.com/kubernetes/kube-state-metrics/issues/694,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/694,kube-state-metrics keep serving stale metrics after extended apiserver outage,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
We wanted to check the metrics for some pods and realised we don't have metrics for the pod in question, then check the deployment and other resources, no metrics are there, but some other pod metrics were available.
We checked the metrics exposed from the /metrics endpoint and we saw metrics exposed for pods which are deleted 2+ weeks ago.
So kube-state-metrics were reporting stale metrics for last 3 weeks.

Here are some relevant logs from kube-state-metrics pod:

```
2019-02-10T21:56:52.294138617Z E0210 21:56:52.293444       1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=124961, ErrCode=NO_ERROR, debug=""""
2019-02-10T21:56:52.295193557Z E0210 21:56:52.293956       1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=124961, ErrCode=NO_ERROR, debug=""""
2019-02-10T21:56:52.295465367Z E0210 21:56:52.294137       1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=124961, ErrCode=NO_ERROR, debug=""""
2019-02-10T21:56:52.29578788Z E0210 21:56:52.294170       1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=124961, ErrCode=NO_ERROR, debug=""""
... There are total of 17 `Unable to decode an event` logs within same second

2019-02-10T21:56:52.321674569Z W0210 21:56:52.321518       1 reflector.go:341] k8s.io/kube-state-metrics/pkg/collectors/collectors.go:91: watch of *v1.Endpoints ended with: very short watch: k8s.io/kube-state-metrics/pkg/collectors/collectors.go:91: Unexpected watch close - watch lasted less than a second and no items received
2019-02-10T21:56:52.322241891Z W0210 21:56:52.322003       1 reflector.go:341] k8s.io/kube-state-metrics/pkg/collectors/collectors.go:91: watch of *v1beta1.CronJob ended with: very short watch: k8s.io/kube-state-metrics/pkg/collectors/collectors.go:91: Unexpected watch close - watch lasted less than a second and no items received
... There are total of 17 `Unexpected watch close` logs within same second 

# here the logs pivoted to `TLS handshake timeout`
2019-02-10T21:57:03.330774532Z E0210 21:57:03.330640       1 reflector.go:205] k8s.io/kube-state-metrics/pkg/collectors/collectors.go:91: Failed to list *v1.Pod: Get https://msa-dev-az-xxxxxx.hcp.eastus.azmk8s.io:443/api/v1/pods?limit=500&resourceVersion=0: net/http: TLS handshake timeout
...
... # there are quite many `net/http: TLS handshake timeout` messages here until the logs stop
...
2019-02-10T21:57:25.366656235Z E0210 21:57:25.366563       1 reflector.go:205] k8s.io/kube-state-metrics/pkg/collectors/collectors.go:91: Failed to list *v1.PersistentVolumeClaim: Get https://msa-dev-az-xxxxxx.hcp.eastus.azmk8s.io:443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0: net/http: TLS handshake timeout
2019-02-10T21:57:25.367127153Z E0210 21:57:25.366998       1 reflector.go:205] k8s.io/kube-state-metrics/pkg/collectors/collectors.go:91: Failed to list *v2beta1.HorizontalPodAutoscaler: Get https://msa-dev--xxxxxx.hcp.eastus.azmk8s.io:443/apis/autoscaling/v2beta1/horizontalpodautoscalers?limit=500&resourceVersion=0: net/http: TLS handshake timeout
# this is the very last log line, current date is 2019-03-04 and still no logs, pod is still up and reporting stale metrics from that time, so last log line was 3 weeks ago
```


**What you expected to happen**:
kube-state-metrics should not be serving stale metrics more than a certain time (I'd say at most couple of minutes for our case).
Seems like the kube-state-metrics didn't try to reconnect to the kube-apiserver after having some issues with the kube-apiserver.
Instead of keep serving stale metrics kube-state-metrics should at least panic out our report unhealthy status.

**How to reproduce it (as minimally and precisely as possible)**:
We didn't reproduce it yet. I'll update if we are able to reproduce.

**Anything else we need to know?**:
Here is a summary of the deployment status:
```
$ ptc msai msa-dev:seed insight deployment/msa-monitoring-kube-state-metrics -n monitoring   -r
Deployment/msa-monitoring-kube-state-metrics[monitoring], created 2 months ago
  desired:1, existing:1, ready:1, updated:1, available:1
  Available:True for 2 months MinimumReplicasAvailable :'Deployment has minimum availability.' last update was 2 months ago
  Progressing:True for 2 months NewReplicaSetAvailable :'ReplicaSet ""msa-monitoring-kube-state-metrics-6bb98dc558"" has successfully progressed.' last update was 2 months ago
Pod/msa-monitoring-kube-state-metrics-6bb98dc558-tlfvb[monitoring], created 2 months ago
  Running BestEffort
  Initialized:True for 2 months
  Ready:True for 4 days
  ContainersReady:True
  PodScheduled:True for 2 months
  Container: kube-state-metrics deploys quay.io/coreos/kube-state-metrics:v1.4.0
    ready:True
    state: running for 2 months
```

**Environment**:
- Kubernetes version (use `kubectl version`): `v1.11.5` --> `Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.5"", GitCommit:""753b2dbc622f5cc417845f0ff8a77f539a4213ea"", GitTreeState:""clean"", BuildDate:""2018-11-26T14:31:35Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}`
- Kube-state-metrics image version: `quay.io/coreos/kube-state-metrics:v1.4.0`

",open,False,2019-03-04 16:33:48,2019-03-05 13:29:29
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/695,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/695,lint: add golangci-lint to the CI pipeline of kube-state-metrics,"Follow up PR of #693 
This PR also fixes goimports(as of go 1.11) issues reported by golangci-lint

/cc @brancz @andyxning ",closed,True,2019-03-04 18:13:55,2019-03-05 02:16:30
kube-state-metrics,jesusvazquez,https://github.com/kubernetes/kube-state-metrics/pull/696,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/696,[Documentation] Add join metric example for running pods requesting memory,"**What this PR does / why we need it**:

Adds a new example to the **Join Metrics** section in the documentation. 

I believe its a useful example for the users since knowing how much memory is being taken up by running pods is a common pattern.

**Which issue(s) this PR fixes** 
This PR is not fixing https://github.com/kubernetes/kube-state-metrics/issues/458 but its related. The label `phase` won't be added to pod metrics so showing an example of how to take advantage of joining metrics to workaround this might be useful for the community.

",closed,True,2019-03-05 20:57:52,2019-03-11 12:53:27
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/issues/697,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/697,Discuss cutting kube-state-metrics v1.6.0,"Given that the last release was cut ~2 month ago and *master* being ahead of *release-1.5* by 107 commits, what do you all think of cutting a *v1.6.0* release?",open,False,2019-03-07 13:50:28,2019-03-15 08:43:35
kube-state-metrics,joelsmith,https://github.com/kubernetes/kube-state-metrics/pull/698,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/698,Update embargo doc link in SECURITY_CONTACTS and change PST to PSC,See https://github.com/kubernetes/security/issues/8 for more information,closed,True,2019-03-08 18:11:11,2019-03-11 06:23:27
kube-state-metrics,okke-formsma,https://github.com/kubernetes/kube-state-metrics/issues/699,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/699,Export readiness and liveness metrics,"/kind feature
Currently there is no way to monitor liveness and readiness probe failures using kube-state-metrics. I would very much like to be able to see these for every pod.
",open,False,2019-03-13 11:43:31,2019-03-15 10:36:32
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/700,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/700,update go version to 1.12.x,,closed,True,2019-03-15 18:55:26,2019-03-23 17:45:13
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/701,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/701,use kubernetes 1.14 dependencies in kube-state-metrics,This PR uses the latest k8s 1.14.0 deps.,closed,True,2019-03-18 04:07:20,2019-04-02 11:58:36
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/702,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/702,*: Cut v1.6.0-rc.0,"**Which issue(s) this PR fixes**:
Relates to https://github.com/kubernetes/kube-state-metrics/issues/697

",open,True,2019-03-19 13:17:47,2019-04-05 14:58:22
kube-state-metrics,dengceltics,https://github.com/kubernetes/kube-state-metrics/issues/703,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/703,some metrics such as kube_pod_container_ cannot scrape,,open,False,2019-03-21 03:52:41,2019-03-22 09:58:29
kube-state-metrics,chancez,https://github.com/kubernetes/kube-state-metrics/issues/704,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/704,Support for PersistentVolume cloud volumeID metric/label,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

I'm not sure how this should work; but it would be useful to have a metric or label added to PV metrics that indicates the cloud volumeID of a volume. It's provider specific; so I'm not sure the best way to go about it.

At the end of the day, I'm expecting a new metric (or many metrics) with the value of `spec.gcePersistentDisk.pdName` from a GCE PV or `spec. awsElasticBlockStore.volumeID` from an AWS PV stored in a label.",open,False,2019-03-21 16:29:49,2019-04-01 09:14:46
kube-state-metrics,lachlancooper,https://github.com/kubernetes/kube-state-metrics/pull/705,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/705,Fix metric labels for kube_job_failed,**What this PR does / why we need it**: Fixes an inconsistency with the way the `condition` label key is applied to the `kube_job_failed` metric.,closed,True,2019-03-24 23:52:47,2019-04-04 13:46:14
kube-state-metrics,liuwenjiegithub,https://github.com/kubernetes/kube-state-metrics/issues/706,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/706,kube-static-metrics addon-resizer error,"ERROR: logging before flag.Parse: I0326 14:44:55.136801       1 pod_nanny.go:81] Watching namespace: kube-system, pod: kube-state-metrics-65f9c6fcdf-gcbrm, container: kube-state-metrics.
ERROR: logging before flag.Parse: I0326 14:44:55.136808       1 pod_nanny.go:82] storage: MISSING, extra_storage: 0Gi
ERROR: logging before flag.Parse: I0326 14:44:55.138236       1 pod_nanny.go:171] Failed to read data from config file ""MISSING/NannyConfiguration"": open MISSING/NannyConfiguration: no such file or directory, using default parameters
ERROR: logging before flag.Parse: I0326 14:44:55.138294       1 pod_nanny.go:109] cpu: 100m, extra_cpu: 1m, memory: 100Mi, extra_memory: 2Mi
ERROR: logging before flag.Parse: I0326 14:44:55.138315       1 pod_nanny.go:138] Resources: [{Base:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} ExtraPerNode:{i:{value:1 scale:-3} d:{Dec:<nil>} s:1m Format:DecimalSI} Name:cpu} {Base:{i:{value:104857600 scale:0} d:{Dec:<nil>} s:100Mi Format:BinarySI} ExtraPerNode:{i:{value:2097152 scale:0} d:{Dec:<nil>} s:2Mi Format:BinarySI} Name:memory}]


kubernetes 1.13.3
addon-resizer:1.8.3",open,False,2019-03-26 15:12:43,2019-04-01 15:43:06
kube-state-metrics,tariq1890,https://github.com/kubernetes/kube-state-metrics/pull/707,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/707,update the prometheus_client_golang dependency to the latest version: v0.9.2,Using the latest version of client_golang from prometheus.,closed,True,2019-03-28 20:13:58,2019-03-29 02:51:41
kube-state-metrics,flouthoc,https://github.com/kubernetes/kube-state-metrics/pull/708,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/708,kube_node_status_phase does not works. Updating Docs,"Update Docs for not supported metrics.

https://github.com/kubernetes/kube-state-metrics/issues/472


",closed,True,2019-04-02 11:29:14,2019-04-03 01:42:52
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/709,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/709,internal/collector+pkg/metrics_store: Reduce allocation & improve hot path,"This pull request includes two performance optimizations found while testing the kube-state-metrics *v1.6.0* release candidate. Given that v1.6.0-rc.0 doesn't seem to introduce any performance regressions, these patches don't need to be included in the release.

### 1. internal/collector: Preallocate metric slice length when known upfront

Profiling kube-state-metrics shows that a lot of CPU time is spent on
`runtime.growslice`. This is related to metric generation when
converting a Kuberntes object to a slice of metrics in
`metric.FamilyGenerator`s. Oftentimes the length of the metric slice is
known upfront.

When the needed size of the metrics slice is known upfront, allocate an
appropriately sized metric slice.

### 2. pkg/metrics_store: Cache byte slices instead of strings

When converting a string to a byte slice, in Golang would do a full copy
which introduces memory allocations. On an HTTP scrape request WriteAll
in the metrics store component is called, which writes all cached
metrics into the given io.Writer. io.Writer takes a byte slice, whereas
metrics are cached as strings. The connect the two, the metrics store
component converts the metric strings to byte slices. This results in
`runtime.stringtoslicebyte` [1] being a prominent CPU user.

Instead of caching metrics as strings, cache them as byte slices,
removing the additional translation from the hot-path.

### Test setup

#### Unit benchmark

The first patch gets us some reasonable memory and runtime optimizations. Running `internal.collector.BenchmarkPodCollector` with (*new*) and without (*old*) returns the following:

```
benchmark                   old ns/op     new ns/op     delta
BenchmarkPodCollector-4     25942         24475         -5.65%

benchmark                   old allocs     new allocs     delta
BenchmarkPodCollector-4     419            395            -5.73%

benchmark                   old bytes     new bytes     delta
BenchmarkPodCollector-4     20112         19544         -2.82%
```

#### End to end test

I have compared kube-state-metrics *v1.6.0-rc.0-0* (vanilla https://github.com/kubernetes/kube-state-metrics/pull/702),  *v1.6.0-rc.0-4* (vanilla + patch 1) and *v1.6.0-rc.0-5* (vanilla + patch 1 & 2) on a 9 node cluster with AWS m5.large compute instances. During the comparison a test script would create and delete pods ranging from 250 - 5000 pods total. In addition the cluster had 8000 secrets constantly. Kube-state-metrics instances are scraped every 2s.

##### Scrape duration

Both patches seem to be neutral in regards to the Prometheus scrape duration.

![image](https://user-images.githubusercontent.com/7047859/55469099-0928ba00-5605-11e9-83c7-dd310173db90.png)


##### CPU usage

Patch 1 seems to have only a subtle impact on the CPU usage, patch 2 constantly performs better, given that we only need to convert the metric string to a byte slice once and not on every scrape.

![image](https://user-images.githubusercontent.com/7047859/55468812-7e47bf80-5604-11e9-930b-344c1ca4aa41.png)





[1] https://golang.org/src/runtime/string.go?s=4063:4115#L145


",closed,True,2019-04-03 09:43:23,2019-04-05 12:17:20
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/710,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/710,go.mod: Pin k8s.io/client-go to v11.0.0,"The previous patch updating kube-state-metrics to Kubernetes v1.14 (https://github.com/kubernetes/kube-state-metrics/pull/701) resulted in k8s.io/client-go module set to `v2.0.0-alpha.0.0.20190313235726-6ee68ca5fd83+incompatible`. While `6ee68ca5fd83` is the right reference to client-go release (v11), `v2.0.0-alpha` is clearly not. The k8s.io/client-go code checked in into /vendor follows the commit sha, not the version.

This patch pins k8s.io/client-go to `v11.0.0` via `GO111MODULE=on go get k8s.io/client-go@v11.0`. As /vendor was already correctly updated with the previous patch, this patch does not include any changes to /vendor.

So far I have not been able to understand, why go modules would pin the version to an old tag, but vendor the new version. Maybe @tariq1890 do you have an idea?
",closed,True,2019-04-03 16:30:12,2019-04-05 10:07:05
kube-state-metrics,mxinden,https://github.com/kubernetes/kube-state-metrics/pull/711,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/711,README.md: Update compatibility matrix for kube-state-metrics v1.6,xref: https://github.com/kubernetes/kube-state-metrics/pull/702#issuecomment-478964510,closed,True,2019-04-03 16:33:15,2019-04-05 12:31:23
kube-state-metrics,sylr,https://github.com/kubernetes/kube-state-metrics/issues/712,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/712,Add label for priority class in kube_pod_info,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:

It would be nice to be able to have information about the priority class to write alerts that does not involve low priority pods.

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): 1.13.5
- Kube-state-metrics 1.5.0
",closed,False,2019-04-04 07:49:12,2019-04-05 08:39:18
kube-state-metrics,chenk008,https://github.com/kubernetes/kube-state-metrics/pull/713,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/713,Add label for priority class in kube_pod_info #712,"Add label for priority class in kube_pod_info

Fixes #712 

",closed,True,2019-04-04 17:36:44,2019-04-05 08:39:19
kube-state-metrics,luarx,https://github.com/kubernetes/kube-state-metrics/issues/714,https://api.github.com/repos/kubernetes/kube-state-metrics/issues/714,New metric for PVC resize status,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [KUBE-STATE-METRICS](https://github.com/kubernetes/kube-state-metrics) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature


**What happened**:
In Kubernetes v1.11 the persistent volume expansion feature is being promoted and we can use it to resize PVC. But kube-state-metrics doesn't have a metric to know if a PVC is ""**resizing**""/""**already resized but pending of a pod restart**""/""**resized**"".

It would be useful to know if a pod must be restarted so that Kubernetes can resize the filesystem of a persistent volume.

**What you expected to happen**:
Have a metric to know resize state of a PVC and use it to configure alerts of pods that must be restarted to be able to resize the volume.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): v1.11.9
- Kube-state-metrics image version
",open,False,2019-04-05 14:42:03,2019-04-05 14:42:27
