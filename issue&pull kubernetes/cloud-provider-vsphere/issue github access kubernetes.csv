name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
cloud-provider-vsphere,jessfraz,https://github.com/kubernetes/cloud-provider-vsphere/issues/1,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/1,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",closed,False,2018-05-24 14:43:19,2018-06-04 16:00:14
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/2,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/2,Enable Prow automation in the repo,"As subject says, Prow automation needs to be enabled for this repo before starting accepting PRs and such.",closed,False,2018-05-25 13:31:20,2018-06-04 16:44:57
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/pull/3,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/3,Add Fabio and Doug as approvers,Fixes: #1 ,closed,True,2018-06-04 13:02:14,2018-06-04 16:00:15
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/pull/4,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/4,Add initial repo layout with CCM prototype,"Create scaffolding for repo, with initial CCM prototype (thanks to cloud-provider-openstack)",closed,True,2018-06-06 11:47:10,2018-06-07 18:34:25
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/5,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/5,Create manifests to deploy CCM as pod or daemonset,For reference: https://github.com/kubernetes/cloud-provider-openstack/tree/master/manifests/controller-manager,closed,False,2018-06-12 16:33:42,2018-07-24 16:29:56
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/6,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/6,Enable the use of Kubernetes secrets to store passwords for CCM,"We should allow users to store infrastructure secrets in the right place.

/kind feature",closed,False,2018-06-19 13:14:18,2018-08-06 15:10:45
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/7,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/7,Import in-tree cloud provider code in codebase,"As opposed to vendoring the code from k/k.

/kind feature
",closed,False,2018-06-19 14:24:58,2018-07-24 09:11:36
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/8,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/8,Setup a CI job to push latest images to dockerhub,"Makefile has a `make images` target that builds a CCM container based on `photon:2.0`.

It would be great to get the ball rolling with CI creating a job that runs a build and pushes the resulting image to dockerhub.

/kind feature
",closed,False,2018-06-20 19:29:00,2018-09-26 15:53:30
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/9,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/9,Create CCM documentation as specified in KEP 0002,"ref: https://github.com/kubernetes/community/blob/master/keps/sig-cloud-provider/0002-cloud-controller-manager.md#implementation-detailsnotesconstraints

/kind feature",closed,False,2018-06-20 20:50:43,2019-03-06 17:33:37
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/10,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/10,Set up unit testing for CCM code,"This would ideally run a `make test` on the tide testing infrastructure, this issue is for the creation of the hook, `make test` can be a noop for now.

/kind feature
/help",closed,False,2018-06-20 20:54:24,2018-08-06 15:06:59
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/11,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/11,Refactor NodeManager to use PropertyCollector,"Currently state changes within vCenter are discovered by discovering/manually fetching the Moref and then polling the object for changes, we should refactor this code to use the `PropertyCollector` service offered in vCenter >4.1

/kind feature
",closed,False,2018-07-09 13:37:06,2018-08-23 00:04:05
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/pull/12,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/12,Add refactoring design doc,Add documentation for the refactoring work,closed,True,2018-07-10 13:35:37,2018-07-16 20:43:52
cloud-provider-vsphere,aojea,https://github.com/kubernetes/cloud-provider-vsphere/issues/13,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/13,Missing CONTRIBUTING.md file,"All K8s subrepositories should have a CONTRIBUTING.md file, which at the minimum should point to https://github.com/kubernetes/community/blob/master/contributors/guide/README.md. Care should be taken that all information is in sync with the contributor guide.

Subrepositories may also have contributing guidelines specific to that repository. They should be explicitly documented and explained in the CONTRIBUTING.md

Ref:  https://github.com/kubernetes/community/issues/1832",closed,False,2018-07-16 09:23:24,2018-07-19 07:55:43
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/pull/14,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/14,Initial working CCM prototype,"This is an initial stab at code refactoring, the provider is registering nodes successfully but code requires a lot of cleanup. It also imports code from in-tree as #7 requires.",closed,True,2018-07-16 20:53:48,2018-07-23 20:22:47
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/issues/15,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/15,Create vcpctl tool to facilitate CCM provisioning,"**FEATURE REQUEST**:

>
> /kind feature

Deploying a cloud provider on vSphere is a task that has many prerequisites, from creating a user with correct roles on vCenter, to creating a correct configuration for the service. When migrating users from the in-tree version there's also a need to convert configuration files and to make sure sensitive credentials are stored safely.

The tool should fulfill these needs:

- [x] Perform vSphere configuration health check:
  - disk.uuid set on esx

- [x] Create vSphere role with minimal set of permissions
- [x] Create vSphere solution user (generate keypair), to be used with CCM

",closed,False,2018-07-17 23:24:26,2018-09-11 10:15:56
cloud-provider-vsphere,nikhita,https://github.com/kubernetes/cloud-provider-vsphere/pull/16,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/16,Add CONTRIBUTING.md,"Fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/13
xref https://github.com/kubernetes/community/issues/1832

/assign frapposelli 

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-07-19 05:21:20,2018-07-19 08:00:45
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/issues/17,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/17,Allow Pushing Images with Latest Tag,"**Is this a BUG REPORT or FEATURE REQUEST?**:
 /kind feature

**What happened**:
When running `make upload-images`, docker images are pushed and tags based on the build hash.

**What you expected to happen**:
Having a tag on the build hash is fine, but I would also like to push with the tag `latest`. By default, I would be nice to push both the build tag and latest tag. Not ideal but acceptable would be optionally pushing a `latest` tag.

The reason why this is needed is because by not pushing with the `latest` tag means for each build I need to update my YAML to append the build tag (example `12a86808`) to the image name (ie `dvonthenen/vsphere-cloud-controller-manager:12a86808`).

**How to reproduce it (as minimally and precisely as possible)**:
Run `make upload-images`

**Anything else we need to know?**:
Nope

**Environment**:
- vsphere-cloud-controller-manager version: Based on https://github.com/kubernetes/cloud-provider-vsphere/pull/14/files
- OS (e.g. from /etc/os-release): CentOS 7.X
- Kernel (e.g. `uname -a`): meh
- Install tools:
- Others: Kubernetes 1.11.1
",closed,False,2018-07-20 17:10:43,2018-07-24 16:51:56
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/18,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/18,RBAC for DS and Pod Deployment of CCM,"**What this PR does / why we need it**:
Adds a simple pod-based deployment instead of using a DaemonSet.
Provider roles and bindings for RBAC that support both the DS and POD deployment.

**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/5

**Special notes for your reviewer**: None",closed,True,2018-07-23 18:56:09,2018-07-23 19:49:22
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/19,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/19,RBAC for DS and Pod Deployment of CCM,"**What this PR does / why we need it**:
Adds a simple pod-based deployment instead of using a DaemonSet.
Provider roles and bindings for RBAC that support both the DS and POD deployment.

**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/5

**Special notes for your reviewer**: None
",closed,True,2018-07-23 19:54:23,2018-07-23 19:54:40
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/20,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/20,RBAC for DS and Pod Deployment of CCM,"**What this PR does / why we need it**:
Adds a simple pod-based deployment instead of using a DaemonSet.
Provider roles and bindings for RBAC that support both the DS and POD deployment.

**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/5

**Special notes for your reviewer**: None
",closed,True,2018-07-23 19:55:10,2018-07-24 16:34:05
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/21,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/21,Allow Pushing Images with Latest Tag,"**What this PR does / why we need it**:
The reason why this is needed is because by not pushing with the latest tag means for each build I need to update my YAML to append the build tag (example 12a86808) to the image name (ie dvonthenen/vsphere-cloud-controller-manager:12a86808). Having latest allows users to deploy the latest cloud provider without having to modify the YAML file in the manifest directory every single time.

**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/17

**Special notes for your reviewer**: None
",closed,True,2018-07-24 15:22:00,2018-07-24 15:26:57
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/22,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/22,Allow using latest tag for docker push,"**What this PR does / why we need it**:
The reason why this is needed is by not pushing with the latest tag means for each build I need to update my YAML to append the build tag (example 12a86808) to the image name (ie dvonthenen/vsphere-cloud-controller-manager:12a86808). Having latest allows users to deploy the latest cloud provider without having to modify the YAML file in the manifest directory every single time.

**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/17

**Special notes for your reviewer**: None
",closed,True,2018-07-24 15:27:54,2018-07-24 17:00:19
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/23,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/23,Code cleanup,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:

The packages under `pkg/` have some dead code in them and are in general bad shape.

**What you expected to happen**:

A clean up to be perfomed: removal of dead code, code optimization, removal of redundant code paths.

**Anything else we need to know?**:

/cc @akutz",closed,False,2018-07-25 16:34:39,2018-07-27 23:00:58
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/issues/24,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/24,Committing vendor,"/kind feature

Have the project maintainers considered committing vendor? It tends to make project management much easier.",closed,False,2018-07-25 18:24:42,2018-07-28 17:07:01
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/25,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/25,docs: Update README with build tips,"**What this PR does / why we need it**: This patch updates the README file with a section about building the project locally. The new section outlines the correct location to which to clone the repository and the preferred method to build the provider.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: NA

**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```",closed,True,2018-07-26 15:15:12,2018-07-26 15:17:15
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/26,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/26,"fix: Sets Makefile default target to ""all: build""","**What this PR does / why we need it**: This patch updates the default target in the Makefile to be ""all"" which depends on ""build"". This is a standard target graph in Makefiles.

Currently the `Makefile`'s default target is `$(GOBIN)` which is a directory path. For many reasons directory targets do not make the best Makefile targets *or* dependencies (unless order-only), but this is also not a user-friendly choice for the default target. Executing `make` should not yield `nothing to be done` on a clean system with an existing `$GOPATH/bin` directory.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: NA

**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```",closed,True,2018-07-27 20:53:45,2018-08-09 16:52:00
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/27,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/27,Removing dead code,"**What this PR does / why we need it**: This patch fixes #23 and removes dead code. The following command was used to determine the code to remove:

```shell
$ gometalinter.v2 \
  --tests \
  --deadline=300s \
  --disable-all \
  --enable=deadcode \
  --enable=varcheck \
  --enable=structcheck \
  --enable=unparam \
  --enable=unused \
  $(for d in cluster cmd pkg; \
    do find ""${d}"" -type d; done | \
    sed 's~^~./~g' | tr '\n' ' ')
```

Please note that the above command does not use `./...` to imply all recursive packages. The `gometalinter` tool includes `vendor` when doing this, and that directory should be omitted. The output was as follows:

```shell
pkg/vclib/constants.go:55:1:warning: testNameNotFound is unused (deadcode)
pkg/cloudprovider/vsphere/types.go:117:2:warning: unused struct field k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere.NodeDetails.vm (structcheck)
pkg/cloudprovider/vsphere/instances.go:113:45:warning: parameter ctx is unused (unparam)
pkg/cloudprovider/vsphere/instances.go:113:66:warning: parameter user is unused (unparam)
pkg/cloudprovider/vsphere/instances.go:113:79:warning: parameter keyData is unused (unparam)
pkg/cloudprovider/vsphere/instances.go:119:37:warning: parameter ctx is unused (unparam)
pkg/cloudprovider/vsphere/instances.go:147:41:warning: parameter ctx is unused (unparam)
pkg/cloudprovider/vsphere/instances.go:339:47:warning: parameter ctx is unused (unparam)
pkg/vclib/pbm.go:88:74:warning: parameter dc is unused (unparam)
pkg/vclib/virtualmachine.go:413:143:warning: parameter result 0 (error) is never used (unparam)
pkg/cloudprovider/vsphere/types.go:117:2:warning: field vm is unused (U1000) (unused)
```

The following warnings were ignored as false-positives:

* `testNameNotFound` was listed as unused, but it **is** used in tests. The linters apparently do not all consider Go test sources.
* The `unparam` warnings were ignored because they refer to altering function signatures that are: required by interfaces but not yet implemented and thus the parameters are unused 
* Go contexts which may not always be used *yet*, but is good practice to include for the future. 

All in all one change was made:

1. The same issue was discovered twice by different tools:
    * `pkg/cloudprovider/vsphere/types.go:117:2:warning: unused struct field k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere.NodeDetails.vm (structcheck)`
    * `pkg/cloudprovider/vsphere/types.go:117:2:warning: field vm is unused (U1000) (unused)`

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #23

**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```",closed,True,2018-07-27 21:50:22,2018-07-27 23:01:31
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/28,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/28,Add documentation to Gopkg.toml,"**What this PR does / why we need it**: This patch adds documentation on how to modify Gopkg.toml to the top of the `Gopkg.toml` file.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: NA

**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```",closed,True,2018-07-28 16:56:07,2018-07-30 00:17:57
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/issues/29,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/29,dep: Removing non-Go files from dependencies,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

FWIW, I added the following to the `Gopkg.toml` file and did a clean `dep ensure`:

```toml
[prune]
  non-go = true
  go-tests = true
  unused-packages = true
```

I wanted to see how much disk space was saved, if any, by removing non-Go files from the project's dependencies. The answer? Only about 2MB:

**Keep non-Go files**
```shell
$ du -m -d 0 .vendor
50	.vendor
```

**Remove non-Go files**
```shell
$ du -m -d 0 vendor
48	vendor
```

So for now, at least, it does not make sense to remove non-Go files from dependencies as the cost-savings is not worth the risk of losing contents that may be necessary for testing.

**What happened**: NA

**What you expected to happen**: NA

**How to reproduce it (as minimally and precisely as possible)**: NA

**Anything else we need to know?**: NA

**Environment**: NA",closed,False,2018-07-28 17:04:42,2018-12-25 19:32:24
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/30,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/30,Updates for building the provider locally,"**What this PR does / why we need it**: This PR includes three changes:

1. The `hack/make.sh` script has been updated to accept multiple targets
2. The `README` now includes information about why `dep` may hang and how to solve the issue
3. The `README` has been updated to include more detailed information about building the provider locally

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: NA

**Special notes for your reviewer**: Please note that the update to `hack/make.sh` was included in this PR due to not wanting to indicate in the build documentation that running `make` directly can accept multiple targets while using `hack/make.sh` did not.

**Release note**:
```release-note
* The ""hack/make.sh"" script now accepts multiple targets.
* The project's ""README"" now includes information about why the ""dep"" tool may hang and how to solve the issue.
```",closed,True,2018-07-28 17:56:02,2018-08-08 17:38:42
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/pull/31,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/31,Feature: Create vcpctl tool framework. (Stage 1) ,"- Add command and cli packageAdd vcpctl command and cli pacakge 

#15 


",closed,True,2018-07-31 10:51:32,2018-08-20 10:13:59
cloud-provider-vsphere,Rajat-0,https://github.com/kubernetes/cloud-provider-vsphere/pull/32,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/32,Adding code of conduct file,"Adding missing code-of-conduct.md file.
",closed,True,2018-08-02 09:08:22,2018-08-02 09:13:28
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/33,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/33,Implement Kubernetes Secret for Storing vCenter Creds,"**What this PR does / why we need it**:
This implements the ability to store vCenter credentials (username and password) in a Kubernetes Secret. 
- Multiple vCenter creds are supported within the secret.
- Configurable service account supported for multiple cloud-provider-vsphere instances.
- To maintain backward compatibility, credentials are obtained in the following order: secret, vCenter unique creds, global setting. If the secret doesn't exist, the behavior is to fall back on the old method of conf file.
- Documentation updated to help the user deploy using a secret.

**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/6

**Special notes for your reviewer**:
Tested the following scenarios on a 1.11.1 Kubernetes cluster:
- Secret containing vCenter creds without setting creds in vsphere.conf
- Secret containing vCenter creds and also setting creds in vsphere.conf (made sure Secret is used)
- Secret does not contain vCenter creds but creds set in vsphere.conf (creds from config are used)
- No Secret set, vCenter creds set within [vcenter ip] block
- No Secret set, vCenter creds set using Global

**Release note**:
None",closed,True,2018-08-03 15:32:21,2018-08-06 15:37:29
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/34,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/34,Implement unit tests for pkg/cloudprovider/vsphere,"We need unit tests for the cloud provider code to ensure we do not introduce breaking changes.

Currently `pkg/cloudprovider/vsphere` has no unit tests, we should implement an initial set of unit tests for the package.

These tests should be part of the `unit` Makefile target that is automatically run as part of the `test` target, which in turn is run by our CI job.

/kind feature
",closed,False,2018-08-07 13:28:08,2018-08-16 16:37:00
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/35,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/35,Use a better property for reconciliation between nodes and VMs,"The current in-tree provider code assumes that the CCM code is running on every node of the cluster and uses the VM UUID (fetched locally from `/sys/class/dmi/id/product_serial`) to perform reconciliation between the kubernetes node and its underlying VM.

With the out-of-tree provider, that assumption is not true anymore and the current code uses DNS names to perform reconciliation between node and VM, while this works in practice, it is suboptimal.

We should investigate a better way to implement this reconciliation.

/kind feature
",closed,False,2018-08-07 13:47:45,2018-09-04 09:35:17
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/36,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/36,Port Zones support to out-of-tree CCM,"Zones support is currently under review in the in-tree provider and will most likely be included in the 1.12 release (ref. kubernetes/kubernetes#66795).

This issue tracks the porting of the feature over to the out-of-tree CCM hosted in this repo.

/kind feature",closed,False,2018-08-07 16:00:53,2019-02-13 17:18:19
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/37,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/37,Implement E2E tests using vcsim,"We need end-to-end tests for the cloud provider code to provide functional testing against a real vSphere API endpoint.

Currently the CCM has no E2E tests, we should implement an initial set of E2E tests for it, these tests should be part of a new Makefile target that will automatically run as part of a new CI job.

- [x] Implement E2E tests using vcsim
- [x] Create `Makefile` target for E2E tests
- [x] Create a new job in `kubernetes/test-infra` for E2E tests and make it run at every PR

/kind feature",closed,False,2018-08-08 14:29:46,2019-04-03 16:15:30
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/38,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/38,Run conformance test for every patch version of Kubernetes and push results to testgrid,"As outlined in [KEP0018](https://github.com/kubernetes/community/blob/master/keps/sig-cloud-provider/0018-testgrid-conformance-e2e.md), SIG Cloud Provider is asking that every Kubernetes cloud provider reports conformance test results for every patch version of Kubernetes at the minimum. Running conformance tests against master and on pre-release versions are highly encouraged but will not be a requirement.

/kind feature
",closed,False,2018-08-08 15:56:22,2018-09-26 16:14:54
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/39,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/39,Unit Test Coverage and Refactor Obtaining k8s Secret,"**What this PR does / why we need it**:
This PR implements additional unit test coverage for CCM and also refactors obtaining k8s secret for the credential manager. The refactor was necessary in order to correctly utilize the k8s ""fake"" client for use in the unit tests.

**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/34

**Special notes for your reviewer**:
Tested refactored code for obtaining the k8s secret on a k8s 1.11.1 cluster.
Made sure the unit tests passed when running `make unit`.
Will identify if more unit tests can be added to `pkg/cloudprovider/vsphere` and create an additional issue if additional tests are added.

**Release note**:
None
",closed,True,2018-08-16 16:09:17,2018-08-16 16:40:23
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/40,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/40,Investigate using Bazel as build/test tool,"[Bazel](https://bazel.io) is used by Kubernetes as build tool, would be nice to understand pro/cons of adopting it vs. using `make`.

The outcome of this issue should be a document or a comment on this issue detailing pro/cons and potentially reaching a consensus on the path forward.

/kind feature",closed,False,2018-08-16 16:24:30,2018-12-19 15:33:47
cloud-provider-vsphere,embano1,https://github.com/kubernetes/cloud-provider-vsphere/issues/41,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/41,Merge and improve the documentation of the cloud provider,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature

/kind improvement

**What happened**:
Currently, the ownerhship and status of the in-tree cloud provider documentation is unclear. Also, it ties the cloud provider to the storage related Project ""Hatchway"" (https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/), where a separation of concerns is perhaps better.

Going forward with both, in-tree and out-of-tree cloud provider, we should consolidate, re-host and clean up the documentation (supported versions, etc.). Also related tools and scripts, e.g. VCP UX deployment script, should be validated against the latest versions or marked as outdated as the current version (https://github.com/vmware/kubernetes/tree/enable-vcp-uxi) seems to be broken with K8s 1.11.

There's several open issues regarding the documentation, e.g. https://github.com/vmware/kubernetes/issues/479 and https://github.com/vmware/kubernetes/issues/491. Also, the current documentation seems to be incorrect with regards to the vsphere.conf on the workers: https://github.com/vmware/kubernetes/issues/501",open,False,2018-08-17 12:27:24,2019-03-06 18:57:45
cloud-provider-vsphere,AdamDang,https://github.com/kubernetes/cloud-provider-vsphere/pull/42,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/42,Fix some grammatical errors to make the doc better,Fix some grammatical errors to make the doc better,closed,True,2018-08-21 05:53:53,2018-08-24 08:51:38
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/43,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/43,Implements NodeManager and Reconcile Based on UUID,"**What this PR does / why we need it**:
This implements the NodeManager that handles management of existing nodes in a kubernetes cluster and also handles onboard new nodes. The reconciliation or the booking keeping on nodes has changed from using the DNS name to the SystemUUID (not to be confused with the VMware InstanceUUID).

**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/11 and https://github.com/kubernetes/cloud-provider-vsphere/issues/35

**Special notes for your reviewer**:
The in-tree provider used a single map to store looking up node information based DNS name and UUID. You could potentially craft ways of forcing a collision and possibly highjacking another nodes info. This implements two lookup maps one for DNS and one for UUID.

This also fixes an issue with gathering the creds from the kubernetes secret found in testing this feature.

Also adds a bunch more logging to understand what is going on in the provider both for verification and for future debugging/troubleshooting.

**Release note**:
Tested on kubernetes 1.11.2 with 3 worker nodes.
Tested adding a 4th worker node and made sure it was handled correctly.",closed,True,2018-08-22 20:10:33,2018-09-10 22:06:50
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/pull/44,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/44,Feature: Implement vcpctl create solution user for CCM (Stage 2),"- Add Client for delegating vim25 and ssoadmin
- Add Credential for client, session and sso
- Add CreateUserFunc for creating solution user and person user
- Implement creating default solution user and granting WSTrust
permission and Administrator role.



fixes #15 


**Special notes for your reviewer**:
Stage 2 for **vcpctl** implementation

**Release note**:
`NONE`
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write .
-->
```release-note
none
```
",closed,True,2018-08-23 00:57:43,2018-09-11 17:40:03
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/45,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/45,Add NodeManager Unit Tests,"**What this PR does / why we need it**:
Continue to add unit tests to the project.

**Special notes for your reviewer**:
Moved around Config initialization code to NodeManager to satisfy test order dependencies

**Release note**:
None",closed,True,2018-09-10 22:16:31,2018-09-24 17:29:12
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/pull/46,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/46,Feature: Create role and check vms on vSphere for enabling UUID (Stage 3 and 4),"- Add create default role with minimal privileges
- Traversal vms on vsphere, checking UUID and enable it if not.

#15 ",closed,True,2018-09-11 10:16:50,2018-09-14 19:10:40
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/issues/47,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/47,Add tests to vcpctl,"Tests are missing in current vcpctl at this moment. Tests need to cover functionality in `pkg/cli` package.

It also needs to investigate how to implement E2E tests for vcpctl.",open,False,2018-09-11 10:24:23,2019-03-06 16:41:16
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/issues/48,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/48,Add generating certs (key-pair) in vcpctl for provisioning CCM,"
`vcpctl` requires user to provide certificate by `--cert` flag in command line. There needs to add an option feature to generate key-pairs  

> Uncomment only one, leave it on its own line: 
>

> /kind feature


",open,False,2018-09-11 10:28:12,2019-03-06 16:41:16
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/49,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/49,Documentation clarification for deploying CCM,"**What this PR does / why we need it**:
Helps clarify certain assumptions made when deploying the CCM using the Pod or DaemonSet YAML.

**Special notes for your reviewer**:
None

**Release note**:
None
",closed,True,2018-09-11 15:41:24,2018-09-12 14:47:15
cloud-provider-vsphere,figo,https://github.com/kubernetes/cloud-provider-vsphere/pull/50,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/50,report node hostname,"both ip address and hostname needed be reported in node status

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
out-of-tree CCM reporting node hostname in node status.
this is necessary after the upstream change at #67714,
if not reporting, in the node status, it will only has node ip address, it will break tests and applications.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
https://github.com/kubernetes/kubernetes/issues/67714

**Special notes for your reviewer**:
we need to add test later similar as this one: 
https://github.com/kubernetes/kubernetes/pull/68017,
but since we are building tests for nodemanager, this can be part of nodemanager tests.


**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
None
```
/cc @frapposelli @dougm @dvonthenen 
",closed,True,2018-09-11 18:15:10,2018-09-12 16:11:38
cloud-provider-vsphere,figo,https://github.com/kubernetes/cloud-provider-vsphere/pull/51,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/51,remove --tty option when issue docker run,"the --tty option is incompatible with cron job setting.
will get en error saying: ""the input device is not a TTY"" if doing so.

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
this is to unblock the CI job (as cron job now) to build CCM image and upload to registry

**Which issue this PR fixes**
no PR filed. but we got the issue to run 'hack/make.sh' at cron job.

**Special notes for your reviewer**:
None

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
None
```

/cc @dougm @frapposelli 
",closed,True,2018-09-14 18:18:20,2018-09-14 18:23:02
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/52,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/52,Add documentation for vcpctl,"Write user-facing documentation for `vcpctl`, as a minimum:

- Describe its functions
- Walk the user through a new configuration from start to finish

/kind documentation
/assign @fanzhangio ",closed,False,2018-09-19 15:55:32,2018-09-26 14:51:20
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/53,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/53,Explore the creation of a Helm chart for deploying CCM,"The CCM could potentially be deployed through Helm for more flexibility, explore the creation of a Helm chart for it.

/kind feature
/assign @dvonthenen ",open,False,2018-09-19 16:20:15,2019-04-03 16:25:54
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/pull/54,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/54,Add doc for vcpctl,"#52 
",closed,True,2018-09-19 23:28:30,2018-09-25 04:02:14
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/55,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/55,Return descriptive vm size when calling InstanceType(),"Currently when calling `InstanceType()` we simply return `vsphere-vm`, this does not help the user in identifying the type of VM that the k8s node is running on.

As vSphere doesn't have t-shirt size instances, we should come up with a meaningful expression of the vm size to be reported as string.

Some ideas (assuming a VM with 4 vCPU and 8GB of RAM):

- `vsphere.4c8192m` 
- `vm-4cpu8192mem`
- `vm.4c8m`
- `vm.4c8gb`
- `vm.4cpu-8gb`
- `vsphere-vm.4cpu-8gb`

/kind feature
/cc @dvonthenen @dougm @akutz ",closed,False,2018-09-20 08:30:41,2019-03-19 00:34:21
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/56,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/56,Expose gRPC call to obtain k8s VMs,"**What this PR does / why we need it**:
Implements a gRPC endpoint to obtain a list of k8s nodes (master+workers) in a vSphere environment. Implemented based on the comment found here: https://github.com/kubernetes/cloud-provider-vsphere/pull/46#discussion_r217589906

These nodes returned are only nodes belonging to a kubernetes cluster. The gRPC call allows obtaining all nodes across all configured vCenters, nodes that belong within a given vCenter, and nodes in a specific Datacenter within a vCenter.

**Special notes for your reviewer**:
Added go tests for new functionality in node manager and also to test the gRPC server/client functionality.",closed,True,2018-09-20 17:37:11,2018-09-25 15:02:48
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/57,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/57,Default to gcr.io image registry,"**What this PR does / why we need it**:
This PR changes the behavior of building and pushing images to use the gcr.io image registry.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: NA

**Special notes for your reviewer**:
This PR includes a number of small fixes discovered while implementing the primary feature. However, the changes **are** isolated into individual commits.

**Release note**:
```release-note
NONE
```",closed,True,2018-09-23 08:59:18,2018-09-25 07:15:21
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/58,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/58,"Adds CCM user; fixes ""watch secret"" errors","**What this PR does / why we need it**:
This patch binds the user `cloud-controller-manager` to the role `system:cloud-controller-manager` in order to enable the use of a custom Kubernetes user and thus no longer needing to provide the CCM with the controller-manager's kubeconfig.

This patch also fixes the annoying ""watch secret"" failures that populate the logs. Adding the `watch` permission to the `secrets` resource for the `system:cloud-controller-manager` role and using the `cloud-controller-manager` user's `kubeconfig` will result in these errors being a thing of the past.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: NA

**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```",closed,True,2018-09-23 10:44:29,2018-09-25 00:59:40
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/pull/59,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/59,"Fix insecure as true by default, pass flag to client option",Quick fix about enable insecure by default. Pass insecure flag to client option,closed,True,2018-09-23 22:09:28,2018-09-25 04:02:26
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/60,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/60,Standardize and enforce a Go version through the repo,"Create a file with the version of Go to be used when building artifacts from this repo.

The file should be used as source of truth in Makefile and across all the scripts created, they should enforce the version contained in the file.

/assign @akutz ",open,False,2018-09-24 17:05:43,2019-03-06 16:41:16
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/61,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/61,Support for building and pushing the CCM via Travis-CI,"**What this PR does / why we need it**:
This PR builds the CCM on Travis-CI and pushes the image to the GCR registry where the latest image may be pulled with `docker pull gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager`

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: NA @frapposelli?

**Special notes for your reviewer**:
Hi reviewers. This PR should be a single commit, but #57 and #58 are not yet merged. Also, I will be removing the `feature/travis-ci` branch under the Travis-CI config's `allowed_branches` list in a separate PR. Until this is merged, that lets my fork work off of my PR branch.

The following secure, environment variable must be added to the project's travisci.com settings: `GCR_KEY_FILE`. This is a gzip'd, base64 encoded version of the key file for service account `gcr-travis-ci-push`, which has permissions to upload images to the GCR registry.

The Travis-CI settings should also define and environment variable called `OWNERS` with a regex of the owners. For example, `'^(frapposelli|akutz)$'`. Please note the enclosing, single quotes are important or else Travis-CI fails exports the environment variable. Only GitHub users in this env var can use the `ci-` commands in a commit message to override the conditions on which the stages depend.

**Release note**:
```release-note
NONE
```",closed,True,2018-09-24 21:30:09,2018-09-25 16:27:25
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/62,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/62,Disable the vSphere CCM API,"**What this PR does / why we need it**:
This disables the vSphere CCM API (default is disabled) until a mechanism to securely connect to the gRPC endpoint exists. This will be implemented in a future PR. Currently, this API should be used for testing purposes only.

**Special notes for your reviewer**:
This was originally implemented in order to facilitate development for a feature in vcpctl.",closed,True,2018-09-25 15:39:03,2018-09-25 21:14:51
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/63,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/63,Disable k8s Secret Listener if Secret info is not provided.,"**What this PR does / why we need it**:
This fixes the secret listener flooding the CCM logs if the secret listener is not configured correctly.

If not configured correctly, you will see the following message in the CCM log repeated over and over:
```
2018-09-20T12:09:16.424243246-05:00 stderr F E0920 17:09:16.424030       1 reflector.go:322] k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/informers/factory.go:130: Failed to watch *v1.Secret: unknown (get secrets)
```

**Special notes for your reviewer**:
Tested on 1.11.2",closed,True,2018-09-25 21:35:46,2018-09-26 14:40:12
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/issues/64,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/64,Bug: gopkg.toml should be updated with dependencies,"<!-- This form is for bug reports and feature requests! -->

**BUG REPORT**:
Some commits imported packages which not updated in Gopkg.toml. It will cause compile error in local dev env.  So, it would be better for updating the Gopkg.toml along with each PR commit.

> /kind bug

**What happened**:
For example: 
Gopkg.toml only contains minimal dependencies, like `k8s.io/api`, `k8s.io/apimachinery`, `k8s.io/client-go`, `k8s.io/kubernetes`, etc.

New added dependencies are missing, like `k8s.io/sample-controller`, etc.

**What you expected to happen**:
Update the toml file by adding each new dependency.

**Anything else we need to know?**:

**Environment**:
- vsphere-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-09-25 22:38:17,2019-03-18 15:10:48
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/pull/65,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/65,Add generating solution user key pair for login purpose,"#48 

**vcpctl** allows`--cert` flag to receive solution user certificate like`/path/to/certificate/k8s-vcp.crt`

1) if the cert exists, directly return error since VC does not allow multiple solution users use the same cert.
2) if the cert does not exist, invoke creating certificate function to generate key and cert file with the same id k8s-vcp in the same path directory as `--cert` specified.
3) the generated `k8s-vcp.crt` file will be decoded and assigned back to u.cert again.",closed,True,2018-09-25 22:52:39,2018-09-28 12:15:16
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/66,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/66,Feature/k8s conformance,"**What this PR does / why we need it**:
This PR enables Kubernetes e2e conformance tests for the CCM on all merges to master and provides the ability to enable e2e via a daily cron job (must be configured) in Travis-CI. 

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #38

**Special notes for your reviewer**:
The following environment variable must be added to the project's travisci.com settings: 

| Name | Secure | Description |
|-------|--------|-------------|
| `E2E_IMAGE` | | The image used to run the e2e workflow.Please set to `gcr.io/kubernetes-conformance-testing/yake2e` |
| `GCS_KEY_FILE` |  | A Google Cloud JSON key file that has permissions to upload the test results to the GCS buckets `k8s-conformance-vsphere` and `k8s-conformance-cloud-provider-vsphere` |
| `GCS_EMAIL` |  | The e-mail address associated with the user in the `GCS_KEY_FILE` |
| `OWNERS` |  | A single-quoted regular expression that lists the GitHub user names of the people that may use commit message keywords to trigger manual e2e runs. For example, `'^(frapposelli|akutz)$'` |
| `VMC_INFO` |  | A file that contains the credential information used to access VMC and AWS. Please see yake2e's [Quick Start](https://github.com/akutz/yake2e#quick-start) section for an example of the file. The file should be processed with `gzip -9c <FILE | base64` in order to set the environment variable with the file's contents. | 

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```",closed,True,2018-09-26 16:04:29,2018-09-26 16:14:54
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/67,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/67,General housekeeping/cleanup,"**What this PR does / why we need it**:
This PR contains the following changes:
- Changed the default port number on the API server to 43001. This was done in order to avoid application protocol collision. Old port was used in the example.
- Provided a mechanism to change the default binding for the API server. This is required for Helm Chart development
- Changed ENV variables for API disabling and binding to standardize the names. Now they are VSPHERE_API_DISABLE and VSPHERE_API_BINDING respectively.
- Fixed logging to standardize on glog. There were some places where fmt.XXXX and log.XXXX were used
- Fixed the glog.V(X) logging level to appropriate levels
- Fixed various variable names to standardize on naming in the Config object.

**Special notes for your reviewer**:
Tested on 1.11.2",closed,True,2018-09-26 16:33:51,2018-11-06 11:15:06
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/68,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/68,Fix for non-unique cluster names / disable PR builds in Travis-CI,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This patch adds a sha1sum'd repo slug to the end of the name of the cluster deployed by the CI process. This patch also disables PR builds in Travis-CI due to the limited jobs available and being used by the conformance tests. Otherwise PR builds will sit in the queue forever.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: NA

**Special notes for your reviewer**:

**Release note**:
```release-note
```
",closed,True,2018-09-26 17:13:19,2018-09-27 07:15:39
cloud-provider-vsphere,mooncak,https://github.com/kubernetes/cloud-provider-vsphere/pull/69,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/69,Fix typos issue in vsphere_test.go,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Fix typos issue in vsphere_test.go

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-09-27 16:51:09,2018-09-27 17:35:31
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/pull/70,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/70,Add cli tests for vcpctl,"Add unit tests for client and user 
Issue #47 

To run these tests, VC_TEST env should be set up
```
export VC_TEST_URL=<VC URL>
export VC_TEST_PASSWORD=<VC password>
export VC_TEST_USERNAME=<VC username>
```
@dougm @frapposelli ",closed,True,2018-09-28 05:03:25,2018-10-10 03:21:16
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/pull/71,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/71,Fix CCM container location in manifest,"This PR fixes the location for the CCM location in the provider manifests, the new location is pushed automatically through CI.

/kind feature
/cc @akutz",closed,True,2018-10-01 07:31:56,2018-10-01 09:11:43
cloud-provider-vsphere,fanzhangio,https://github.com/kubernetes/cloud-provider-vsphere/pull/72,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/72,Fix bug: vsphere/nodemanager.go failed by Error format,"Fix 'Error call has possible formatting directive %+v' error.
This is a blocking bug in CI

@frapposelli ",closed,True,2018-10-09 09:11:14,2018-10-10 03:20:14
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/73,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/73,csi: add plugin scaffold,"Add scaffold for CSI plugin that gives us something to start building and using for CI. This will be a no-op plugin first, with each method just being ""not implemented"", but allows other efforts to begin in parallel.",closed,False,2018-10-09 17:35:07,2018-10-22 19:10:24
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/74,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/74,csi: add CI for plugin using vcsim,"Want to be able to run tests on the CSI plugin on a local machine using only vcsim. This requires updates to vcsim as well, to support FCD.",open,False,2018-10-09 17:35:49,2019-03-06 16:41:17
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/75,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/75,csi: Add e2e tests for plugin that use VMC,"The goal here is have a set of tests that can run on VMC infrastructure that exercises E2E functionality of the CSI plugin.

To this end, the following tasks need to be completed:

- [ ] Add support to [sk8](https://github.com/vmware/simple-k8s-test-env) for deploying the CSI plugin
- [ ] Determine how to build an `e2e.test` binary from tests housed in this repo that use the existing K8s testing framework
- [ ] Determine which [E2E storage tests from the K8s repo](https://github.com/kubernetes/kubernetes/tree/master/test/e2e/storage/vsphere) can be run on VMC infrastructure. This will be a subset of what is there now, as most ""disruptive"" tests (e.g. those that reboot vCenter infrastructure components) cannot be run on VMC.
- [ ] Determine what/where/how to run ""destructive"" tests, or tests that require additional infrastructure that is hard to achieve on VMC (e.g. multi-vcenter testing, zones, etc.).
- [ ] Once prow jobs can launch tests onto VMC, run the E2E tests there and report status back to testgrid

These tests would be the ""real"" tests, that run on VMC against an actual vCenter instance.",open,False,2018-10-09 17:36:05,2019-03-06 22:44:21
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/76,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/76,csi: push plugin builds to GCR automatically.,"Just as is done for the vSphere CCM, want to have new container images of the CSI plugin pushed to GCR automatically as part of CI.",closed,False,2018-10-09 17:36:28,2018-11-28 20:06:43
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/77,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/77,csi: support for storage vMotion and SDRS,,closed,False,2018-10-09 17:37:16,2019-04-02 17:40:06
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/78,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/78,csi: support for datastore cluster(s),"Current in-tree solution does not support datastore clusters, only accessing a specific datastore.",closed,False,2018-10-09 17:37:32,2018-12-07 02:32:08
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/79,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/79,"csi: support for zone-local, non-shared datastores",,closed,False,2018-10-09 17:38:13,2019-02-13 17:02:28
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/80,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/80,csi: block volume support,,closed,False,2018-10-09 17:38:20,2019-04-02 14:38:38
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/81,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/81,csi: automatically promote/register existing virtual disk manager volumes to FCD,"In an upgrade scenario from in-tree to CSI, there may be existing volumes in use that were controlled by virtual disk manager. We want to promote these volumes by registering them as an FCD, which there is an API for.",closed,False,2018-10-09 17:38:43,2019-04-02 17:40:29
cloud-provider-vsphere,figo,https://github.com/kubernetes/cloud-provider-vsphere/pull/82,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/82,format log explicitly with Errorf() and Infof(),"when run `make test` with go1.11.1, will get error like:
`pkg/vclib/diskmanagers/vmdm.go:105: Verbose.Info call has possible
formatting directive %q`, those issues been ignored if using
go version 1.10.

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:


**Which issue this PR fixes** 
this will unlock the consistently failing CI test for all pull request
like #70 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```

/cc @frapposelli  @dougm @fanzhangio ",closed,True,2018-10-09 22:57:45,2018-10-09 23:03:23
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/83,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/83,Explore load balancer support on VMC,"vSphere does not come with a load balancer facility out of the box, but when running in VMC (a.k.a. VMware on AWS) we have access to the ELB service provided by AWS.

This issue should track an exploratory effort to scope the feasibility and the work needed to bring support for ELB in `cloud-provider-vsphere`.

/kind feature
/assign @akutz ",open,False,2018-10-10 15:44:32,2019-03-06 16:39:34
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/84,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/84,Add logo for helm chart,"**What this PR does / why we need it**:
It is highly recommended that Helm Chart implement having a logo. Adding this logo to be referenced by the vSphere CCM helm chart.",closed,True,2018-10-10 16:08:35,2018-11-06 11:15:05
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/85,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/85,Add empty CSI plugin,"This commit adds the structure for implementing the CSI plugin using
the structure promoted by GoCSI. A 'make vsphere-csi' now produces a
vsphere-csi binary at the top level, just like the CCM.

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: Starts the CSI implementation

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #73 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-10-16 17:38:17,2018-10-22 20:39:15
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/86,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/86,csi: design plugin to support multiple APIs,"When writing the CSI plugin, we need to make sure we can support a choice of backend APIs. The API can be chosen via config flag/var. We start by implementing FCD, but there may future APIs and it would nice to only have on official VMware plugin. Or we may need to add support for something older, like virtual disk manager. Good to keep our options open.
",closed,False,2018-10-17 15:39:17,2018-10-24 18:50:55
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/issues/87,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/87,Node resolution fails when other VMs have same VM-tools-reported guest host FQDNs,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
The CCM fails during node look up when a VM is cloned, even with a new UUID, or when VMs have the same names. Ive seen this occur from templates and OVFs. Most recently the CCM failed outright when I exported a VM as an OVF, imported that OVF as a new VM, and then stood up K8s on the new VM. Until I removed the original VM from the vSphere inventory, the CCM indicated that it could not resolve the node.

I **have** to think something is going on internal to vSphere itself that is causing issues when **one** or **more** of the following is true:

* Multiple VMs have the same names
* Multiple VMs have the same host name as reported by the VM tools
* VMs are deployed from the same template/OVF with new hardware, but have the same files, such as `/etc/machine-id`, that are read by VM tools and get added to the identity of the VM
* Orgremlins?

So my hypothesis seems to be close to something. Look at the UUIDs of the VMs:

```shell
[130]akutz@akutz-a01:Downloads$ govc vm.info -vm.ipath /SDDC-Datacenter/vm/Workloads/yakity-centos
Name:           yakity-centos
  Path:         /SDDC-Datacenter/vm/Workloads/yakity-centos
  UUID:         42301ab6-f495-1845-25ff-5190f281430a
  Guest name:   CentOS 7 (64-bit)
  Memory:       2048MB
  CPU:          1 vCPU(s)
  Power state:  poweredOn
  Boot time:    2018-10-17 21:40:26.36326 +0000 UTC
  IP address:   192.168.3.87
  Host:         10.2.32.4
```

```shell
[0]akutz@akutz-a01:Downloads$ govc vm.info -vm.ipath /SDDC-Datacenter/vm/Workloads/centos-ovf
Name:           centos-ovf
  Path:         /SDDC-Datacenter/vm/Workloads/centos-ovf
  UUID:         42301050-0acb-4e89-31d9-d10386eb4b6e
  Guest name:   CentOS 7 (64-bit)
  Memory:       2048MB
  CPU:          1 vCPU(s)
  Power state:  poweredOn
  Boot time:    2018-10-17 21:38:20.299679 +0000 UTC
  IP address:   192.168.3.52
  Host:         10.2.32.8
```

Now look at the errors from the CCM:

```shell
018-10-17T16:43:55.562713931-05:00 stderr F E1017 21:43:55.562586       1 node_controller.go:417] failed to find kubelet node IP from cloud provider
2018-10-17T16:44:05.571354049-05:00 stderr F I1017 21:44:05.571209       1 instances.go:107] instances.InstanceID() called with yakity.localdomain
2018-10-17T16:44:05.571374006-05:00 stderr F I1017 21:44:05.571230       1 instances.go:111] instances.InstanceID() CACHED with yakity.localdomain
2018-10-17T16:44:05.571379606-05:00 stderr F I1017 21:44:05.571236       1 instances.go:76] instances.NodeAddressesByProviderID() called with vsphere://42301050-0acb-4e89-31d9-d10386eb4b6e
2018-10-17T16:44:05.571384433-05:00 stderr F I1017 21:44:05.571241       1 instances.go:81] instances.NodeAddressesByProviderID() CACHED with 42301050-0acb-4e89-31d9-d10386eb4b6e
2018-10-17T16:44:05.571388477-05:00 stderr F E1017 21:44:05.571250       1 node_controller.go:417] failed to find kubelet node IP from cloud provider
```

The VM in question is `yakity-centos`. The CCM on `yakity-centos` *worked* when the following were true:

1. The VM `centos-ovf` was removed from the inventory
2. The VM `centos-ovf` was added back to the inventory with no information yet reported from VM tools since the VM had not yet been powered on

The CCM *failed* on `yakity-centos` when the following is true:

1. The VM `centos-ovf` is in the inventory and its host name has been reported by VM tools to be the same as the host name reported by `yakity-centos`

The `centos-ovf` VM doesnt even have to be powered on to cause `yakity-ovf` to fail. All that is required is for `centos-ovf` to have been powered on at least *once* so that VM tools reports `centos-ovf`s host name as the same as `yakity-ovf`.

When thats the case, the CCM fails.

This makes complete sense when viewed in the context of the problem we saw a month ago with VM/host names.

**What you expected to happen**:
For the CCM to resolve the node as usual.

**How to reproduce it (as minimally and precisely as possible)**:
I cannot be sure, but it seems related to VMs that report the same guest host names via VM tools. On our VMC environment I can reproduce it with the VMs `centos-ovf` and `yakity-centos`. Please do not touch these VMs without looping me in, however, as I'm using them at the moment.

**Anything else we need to know?**:
My mama says I've the prettiest brown eyes...

I think I found an issue, maybe? In [`instances.go`](https://github.com/kubernetes/cloud-provider-vsphere/blob/master/pkg/cloudprovider/vsphere/instances.go), the `NodeAddresses` function uses [`FindVMByName`](https://github.com/kubernetes/cloud-provider-vsphere/blob/master/pkg/cloudprovider/vsphere/instances.go#L63) where the `NodeAddressesByProviderID` function uses [`FindVMByUUID`](https://github.com/kubernetes/cloud-provider-vsphere/blob/master/pkg/cloudprovider/vsphere/instances.go#L85). Why would `FindVMByName` ever be used?

I think this is the culprit:

```go
// ExternalID returns the cloud provider ID of the instance identified by
// nodeName. If the instance does not exist or is no longer running, the
// returned error will be cloudprovider.InstanceNotFound.
//
// When nodeName identifies more than one instance, only the first will be
// considered.
func (i *instances) ExternalID(ctx context.Context, nodeName types.NodeName) (string, error) {
	glog.V(4).Info(""instances.ExternalID() called with "", nodeName)
	return i.InstanceID(ctx, nodeName)
}

// InstanceID returns the cloud provider ID of the instance identified by nodeName.
func (i *instances) InstanceID(ctx context.Context, nodeName types.NodeName) (string, error) {
	glog.V(4).Info(""instances.InstanceID() called with "", nodeName)

	// Check if node has been discovered already
	if node, ok := i.nodeManager.nodeNameMap[string(nodeName)]; ok {
		glog.V(2).Info(""instances.InstanceID() CACHED with "", string(nodeName))
		return node.UUID, nil
	}

	if err := i.nodeManager.DiscoverNode(string(nodeName), FindVMByName); err == nil {
		glog.V(2).Info(""instances.InstanceID() FOUND with "", string(nodeName))
		return i.nodeManager.nodeNameMap[string(nodeName)].UUID, nil
	}

	glog.V(4).Info(""instances.InstanceID() NOT FOUND with "", string(nodeName))
	return """", ErrNodeNotFound
}
```

`ExternalID` calls `InstanceID`, which always uses `FindVMByName`.

Maybe. I dont know. Im sure this all works 99.9999% of the time and theres just some weird edge case Im exacerbating. That and my lack of knowledge of the workflow probably means Im making mountains out of molehills and seeing bugs where there arent any.

**Environment**:
- vsphere-cloud-controller-manager version: latest
- OS (e.g. from /etc/os-release): 
```
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```

- Kernel (e.g. `uname -a`):
```shell
Linux yakity.localdomain 3.10.0-862.14.4.el7.x86_64 #1 SMP Wed Sep 26 15:12:11 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
```

- Install tools: yakity
- Others:",open,False,2018-10-17 23:25:30,2019-03-06 16:39:34
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/88,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/88,csi: add ability to support multiple backend APIs,"This commit introduces the X_CSI_VSPHERE_APINAME env var, which can be
used to specify the name of the API to use when talking to vCenter.
Currently the only supported API name is ""FCD"", and is the default. FCD
will be used if no API is specified. If one is specified, anything that
doesn't evaluate to FCD will produce an error.

Right now it is envisioned that alternate APIs will only require changes
to the Controller portion of the code. Identity and Node are likely to
remain the same, but this can be re-evaluated in the future.

fixes #86 

**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```
",closed,True,2018-10-23 15:42:00,2019-03-11 20:24:36
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/89,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/89,csi: read same config from file as is used with cloud provider,"When the config map is mounted to the container running the CSI plugin, the plugin should be capable of reading this file. This will allow the same config map to be used for the cloud provider and for CSI.

Since this is just a k/v flat file in the end, this does not make reading that config file K8s specific.

As much as we can, we want to be able to configure the plugin with env vars, but more advanced configs involving multiple vCenter's, for example, would be complicated through that method.",closed,False,2018-10-23 19:36:06,2018-11-06 10:56:56
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/90,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/90,Refactor configuration file for all 3 projects,Junking this...,closed,True,2018-10-25 20:26:07,2018-10-25 20:41:44
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/91,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/91,Refactor configuration gathering for all 3 projects,"**What this PR does / why we need it**:
This PR refactors the vsphere.conf format and parsing in the CCM project. The CSI implementation should leverage the same config file that the CCM project is using. It turns out that the vcpctl project was also using the same config file but was directly importing the entire CCM package just to parse the config. The config parsing code was isolated and this new package was placed in a common area that can be used by all three projects.

This refactor does also include basic capabilities to use ENVARs to configure vCenters. Although this method exists, it is highly discouraged not to be used in a production-like deployment because it can be very error prone. It's only to be used for dev/test purposes if at all.

For CSI to read the config:
```
import (
       vcfg ""k8s.io/cloud-provider-vsphere/pkg/config""
)

cfg, err := vcfg.ReadConfig(config)
if err != nil {
	return nil, err
}
```

There is also a helper function to setup up the connection objects used by vclib:
```
import (
       vcfg ""k8s.io/cloud-provider-vsphere/pkg/config""
)

//genrate connection map
vsphereInstanceMap := vcfg.GenerateInstanceMap(cfg)
```

If more things can be moved over, then we can address in a follow on PR. This is a decent foundation though.


**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/89

**Special notes for your reviewer**:
Tested CCM on 1.11.3
Tested CCM Helm Chart on 1.11.3
make test passes

**Release note**:
Special note that the [Network] section in the vsphere.conf file was removed since it was no longer being referenced anywhere. Documentation was updated to reflect this change. Existing vsphere.conf will need to remove that section otherwise you will get a parse error in the config file.
",closed,True,2018-10-25 20:34:53,2018-11-06 11:15:03
cloud-provider-vsphere,yeya24,https://github.com/kubernetes/cloud-provider-vsphere/pull/92,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/92,fix typos overriden -> overriden,"
**What this PR does / why we need it**:
fix two typos 
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
fix two typos : docs/deploying_cloud_provider_vsphere_with_rbac.md line 75 and 
                       manifests/controller-manager/vsphere.conf line 2
                       overriden -> overridden
",closed,True,2018-11-01 07:11:07,2018-11-01 17:58:46
cloud-provider-vsphere,mooncak,https://github.com/kubernetes/cloud-provider-vsphere/pull/93,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/93,Fix typos: doesnt -> does not,"Signed-off-by: mooncake <xcoder@tenxcloud.com>

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Fix typos: doesnt -> does not

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-04 09:28:54,2018-11-04 11:20:01
cloud-provider-vsphere,mooncak,https://github.com/kubernetes/cloud-provider-vsphere/pull/94,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/94,Fix doc typos: folowing->following requried->required,"Signed-off-by: mooncake <xcoder@tenxcloud.com>

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Fix doc typos: 
folowing->following 
requried->required

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-04 13:44:32,2018-11-04 15:23:46
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/95,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/95,Move pkg/vclib and pkg/config under pkg/common,"**What this PR does / why we need it**:
This PR is just an organizational packaging restructuring. We will be adding a couple common pkg in a subsequent future PRs placing 3-4 ""common"" folders under the pkg directory which makes it kind of confusing on what is going on in that folder. Pushing vclib and config under a ""common"" folder then clears up the pkg folder to only contain buildable components (namely vcpctl, CCM, and CSI) plus this ""common"" folder shared between these components.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**:
`make build` and `make test` works as expected",closed,True,2018-11-06 13:02:32,2018-11-07 10:04:48
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/pull/96,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/96,Restore Network configuration section and add deprecation notice,"This was causing issues with existing configmaps, this fix should restore continuity and adds a warning that the section has been deprecated

/cc @akutz",closed,True,2018-11-06 21:53:14,2018-11-06 22:00:50
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/97,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/97,"Move CredMgr to pkg/common, add DCOS/generic-CO support","**What this PR does / why we need it**:
Moved credential manager into pkg/common for CSI to reuse. Also, adds support for other container orchestrators like DCOS.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**:
Since this is a refactor plus additional implementation to support DCOS and other generic COs, the k8s workflow, docs, and user experience is unchanged.

Tested using k8s on 1.11.3 using k8s secret listener
Tested by mounting the k8s secret to the filesystem which simulates a DCOS deploy of CSI
`make test` functions correctly",closed,True,2018-11-07 11:56:24,2018-11-19 16:53:26
cloud-provider-vsphere,mooncak,https://github.com/kubernetes/cloud-provider-vsphere/pull/98,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/98,Fix typos: genrate -> generate,"Signed-off-by: mooncake <xcoder@tenxcloud.com>

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Fix typos: genrate -> generate

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-08 15:58:56,2018-11-08 16:04:34
cloud-provider-vsphere,mooncak,https://github.com/kubernetes/cloud-provider-vsphere/pull/99,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/99,Fix a batch of typos,"
<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Fix typos: attachs -> attaches

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-08 17:08:55,2018-11-14 17:21:05
cloud-provider-vsphere,mikeweiwei,https://github.com/kubernetes/cloud-provider-vsphere/pull/100,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/100,fix logging calls,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
f don't use formatted output,fix logging calls
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-13 10:44:03,2018-11-14 04:31:19
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/101,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/101,Refactor vSphere Connection into a Connection Manager for CSI reuse,"**What this PR does / why we need it**:
This refactors the use of vSphere connections into a connection manager so that CSI can easily reuse creating connections to vCenters. Cleans up common.Config so that it's only responsible for generating the config and moves the vSphereInstanceMap into the ConnMgr.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**:
Since this is a refactor, the k8s workflow, docs, and user experience are unchanged.

This change should make it super easy to start making API calls to govmomi:
```
import (
       vcfg ""k8s.io/cloud-provider-vsphere/pkg/config""
       cm ""k8s.io/cloud-provider-vsphere/pkg/connectionmanager""
)

//read the config
cfg, err := vcfg.ReadConfig(config)
if err != nil {
	return nil, err
}

//create a connection manager based on the config
//second param is a k8s secretListener, if nil, use the creds in vsphere.conf
connMgr := cm.NewConnectionManagerK8s(cfg, nil)
```

Tested using k8s on 1.11.3 using k8s secret listener
`make test` functions correctly",closed,True,2018-11-15 15:32:42,2018-11-19 16:53:25
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/102,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/102,Remove dead/stale code,"**What this PR does / why we need it**:
This removes dead/stale code that was carried/copied over from the in-tree provider which is no longer needed. This removes the networking section from the configuration file because it isn't needed by the out-of-tree provider. 

There was an original attempt to remove this dead/stale code but it broke a couple of projects that were using/deploying the CCM as apart of these other projects. Sorry about that! This is a second attempt to remove code that is no longer needed before we cut a first version/tag of the CCM.

The behavior of `gcfg` was changed such that if extra or missing sections/keyvalues are parsed, they will not produce a fatal error. Only invalid config file syntax are errors. This was tested with the legacy `network` section in the config file, did not produce a fatal error and the CCM was able to run successfully.

**Special notes for your reviewer**:
When these lines of code were reverted/added back into the CCM, the updates to the documentation, example YAML, the outstanding Helm chart, etc were not reverted along with it. It would be very nice to remove this code/property from the config so we don't create technical debt or legacy issues before we cut the first version.

If we decided not to merge this PR, we can close this PR but we would need to create a new PR to ""revert"" the documentation, example YAML, Helm chart, etc to match the existence of this property in the config file.

CC: @frapposelli @dougm @codenrhoden @akutz ",closed,True,2018-11-19 22:29:41,2018-11-20 21:43:08
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/103,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/103, Implements Startup/Setup Functionality for vSphere CSI Driver,"**What this PR does / why we need it**:
This implements the functionality in the vSphere CSI driver to Startup/Setup the service to:
- consume a vsphere.conf file and create it's corresponding Config object
- takes the previous Config object to generates corresponding vSphere connections via ConnectionManager
- make use of a new common pkg `kubernetes` to create a k8s client and secret listener for cred manager. More info below

Build/Deploy related updates:
- adds YAML files to conveniently deploy vSphere CSI image to a kubernetes cluster
- provides starter/WIP documentation to deploy vSphere CSI 
- updates the Makefile to build the docker CSI image and uploads to said container to the configured registry

CCM updates:
- makes use of the new InformerManager class that both CCM and CSI now use to generate a secret listener for cred manager.

Common packages updates:
- removed overlapping roles and role-bindings in the CCM's YAMLs
- adds a new `kubernetes` pkg which provides helper functions to create a kubernetes client based on various configuration options and implements a wrapper for all Informers
- the `New` func to create a ConnectionManager was consolidated to generate the specific configuration based on the vsphere.conf Config object.

**Which issue this PR fixes**: NA

**Testing performed**:
`make test` passes all tests
Tested vSphere Cloud Controller Manager on Kubernetes 1.11.3
Tested vSphere CSI on Kubernetes 1.11.3
- verified the k8s client is created successfully
- verified the connection manager was able to connect to all configured vcenters
- k8s secretlistener was used

**Special notes for your reviewer**:
Some common packages were refactored to be easily consumed by CCM and CSI. These changes do not impact the deployment, configuration files, or user experience for either projects.

The updates to the Makefile includes functionality to upload packages to the configured container registry. I am not sure if anything needs to be configured on the Google Container Registry to allow the vsphere-csi image to be loaded to it.
",closed,True,2018-11-27 17:26:29,2018-11-28 16:54:03
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/104,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/104,csi: Implement volume create/delete,"As part of implementing CSI spec functionality, first step is adding volume create and delete.

Right now this is simplistic, as it just goes to the identified datastore, rather than a datastore cluster.",closed,False,2018-11-28 00:14:40,2018-12-07 19:22:14
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/105,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/105,csi: implement Controller publish/unpublish,"As part of implementing CSI spec, add capability for Controller{Publish,Unpublish}. This functionality reaches out the vCenter and attaches/detaches the volume to the requested node.",closed,False,2018-11-28 00:16:28,2018-12-07 19:25:21
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/106,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/106,csi: Implement Node stage/unstage,"Implement Node{Stage,Unstage}.

This operation mounts and unmounts the attached volume to a plugin-specific mountpoint, but does not expost the volume to a workload (pod, container) yet.",closed,False,2018-11-28 00:18:00,2018-12-07 21:59:11
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/107,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/107,csi: Implement Node Publish/Unpublish,"Implement Node{Publish,Unpublish}Volume.

This API bind-mounts the given volume from the staged location to the workload location.",closed,False,2018-11-28 00:18:54,2018-12-08 00:13:39
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/108,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/108,Initial Implementation for CSI ListVolumes using FCD,"**What this PR does / why we need it**:
This PR implements ListVolumes for First Class Disks (FCD) on DatastoreClusters and Datastores.
- Datastore has a new function to ListDisks which returns all FCD on that datastore
- New class called DatastoreCluster which gets Properties, Child Datastores comprising the DatastoreCluster, and also ListDisks for FCD on the Datastores comprising the DatastoreCluster
- Datacenter has a function ListDisks which returns only unique FCDs on all DatastoreClusters and Datastores then sorts them if pagination is used.

NodeManager:
- Moves DiscoverNode core functionality into ConnectionManager

ConnectionManager:
- Provides a new function that finds the VC/DC combo that owns a given VM called WhichVCandDCByNodeId
- Provides a ""mock"" function that finds a VC/DC comobo based on a zone called WhichVCandDCByZone. *NOTE: this is a placeholder until we support multiple VC/DC*

Misc:
- Moved various constants and etc to a proper location in code.

**Which issue this PR fixes**: NA (does not have an issue for ListVolumes)

**Special notes for your reviewer**:
- tested CCM because of refactor of nodeManager.DiscoverNode now uses connectionManager. WhichVCandDCByNodeId
- tested CSI ListVolumes using FCD on a mix of DatastoreClusters and traditional Datastores. Tested: 1) returning all FCD, 2) tested the first pagination of FCD getting 10 out of 11, 3) then tested the remainder pagination getting 11 of 11.
- `make test` functions normally
- tested on k8s 1.11.3
- tested on vSphere 6.7U1

**Release note**:
Changes to CCM and CSI functionality does not impact the end user, documentation, or etc.",closed,True,2018-12-03 01:37:24,2018-12-07 02:39:31
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/109,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/109,Vendored BitBucket dependency,"**What this PR does / why we need it**:
This patch commits the vendored BitBucket dependency in order to prevent connection errors that sometimes occur when CI attempts to resolve and fetch the remote dependency using Mercurial.

**Which issue this PR fixes**:
Unblocks #108 

**Special notes for your reviewer**:
If this does not work, I will either close this PR or use it to commit all of vendor in order to resolve PR #108.",closed,True,2018-12-04 00:03:55,2018-12-04 00:13:25
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/110,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/110,Initial implementation for CSI create/delete,"**What this PR does / why we need it**:
Initial implementation for Create/Delete for vSphere CSI.
- Handles DatastoreClusters, VSAN datastores, and ""traditional"" Datastores
- added a comment/doc idea for how to implement zones

Other:
- connectiomanager implements WhichVCandDCByFCDId to retrieve which VC/DC owns a given FCD
- reverted recent changes to nodemanager.DiscoverNode() which was causing the go tests to fail
- refactored some ctx stuff on nodemanager and connectionmanager to handle some intermittent go test failures
- changed a few functions on Datacenter to return DatastoreInfo instead of Datastore
- updated go tests to reflect the above
- removed some unnecessary loops carried over from the in-tree version

**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/78 and https://github.com/kubernetes/cloud-provider-vsphere/issues/104

**Special notes for your reviewer**:
- tested CSI Create/Delete FCD on DatastoreClusters, VSAN datastores, and ""traditional"" Datastores
- `make test` functions normally
- tested on k8s 1.11.3
- tested on vSphere 6.7U1

**Release note**:
Changes to CCM and CSI functionality does not impact the end user, documentation, or etc.",closed,True,2018-12-04 22:45:09,2018-12-07 02:39:32
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/111,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/111,Update to CSI 1.0.0 spec,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: The CSI driver needs to be built of the CSI 1.0.0 spec, and this PR pulls that in along with an updated GoCSI

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: none

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-12-05 19:51:43,2018-12-05 21:04:15
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/112,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/112,Implement CSI Node Stage/Unstage,"This patch adds logic for CSI Node staging and unstaging. This takes a
pre-attached block volume, and mounts it to the given path in the
staging request. `gofsutil` is used to take care of the formatting,
mounting, and parsing of the mount tables.

<!-- Thanks for sending a pull request! -->

**Which issue this PR fixes**: fixes #106

**Special notes for your reviewer**: Tested this with node-only funcitonally. An FCD was previously attached to a node, and tested that I could idempotently stage and unstage that volume to a pre-created staging directory.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-12-06 19:34:10,2018-12-07 22:09:11
cloud-provider-vsphere,maplain,https://github.com/kubernetes/cloud-provider-vsphere/pull/113,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/113,update KEP link in README.md,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
update kep link in README.md because it's moved from kubernetes/community to kubernetes/enhancements

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
outdated link in README.md

**Special notes for your reviewer**:
https://github.com/kubernetes/community/blob/master/keps/sig-cloud-provider/0002-cloud-controller-manager.md
https://github.com/kubernetes/enhancements/blob/master/keps/sig-cloud-provider/0002-cloud-controller-manager.md

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-12-07 00:31:15,2018-12-07 01:09:13
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/114,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/114,Initial implementation for CSI Publish/Unpublish,"**What this PR does / why we need it**:
Initial implementation for Publish/Unpublish for vSphere CSI.
- Handles DatastoreClusters, VSAN datastores, and ""traditional"" Datastores

Other:
- changed existing DetachDisk to be idempotent

**Which issue this PR fixes**: fixes https://github.com/kubernetes/cloud-provider-vsphere/issues/105

**Special notes for your reviewer**:
- tested CSI Publish/Unpublish FCD on DatastoreClusters, VSAN datastores, and ""traditional"" Datastores
- `make test` functions normally
- tested on k8s 1.11.3
- tested on vSphere 6.7U1

**Release note**:
Changes to CCM and CSI functionality does not impact the end user, documentation, or etc.",closed,True,2018-12-07 03:21:45,2018-12-07 19:34:02
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/115,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/115,csi: should /etc/cloud/vsphere.conf be required?,"While testing CSI, I came across this and wanted to capture it before I forgot.

When starting the CSI plugin on a new node, I get an error:
```
$ ./vsphere-csi
FATA[0000] Failed to open /etc/cloud/vsphere.conf . Err: open /etc/cloud/vsphere.conf: no such file or directory
```

I was expecting to be able to set all the needed parameters via env var for my simple case, but if the referenced config file is not present, the plugin does not start up. Furthermore, when looking at the code [here](https://github.com/kubernetes/cloud-provider-vsphere/blob/a06c86025167f56f988f800c5f132bf1f778cbab/pkg/common/config/config.go#L344) It looks like the config file would have precedence over env vars. I'm used to the other way around -- using env vars to override something that may be in a config file.

I wanted to use this issue to clarify that behavior and see if we could agree on what is desired. And then document it.",closed,False,2018-12-07 05:16:03,2019-02-05 20:00:19
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/116,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/116,CSI Node Publish/Unpublish,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This patch implements NodePublishVolume and NodeUnpublishVolume.

The code assumes the volume has already been staged, since we support
the NODE_STAGE_UNSTAGE capability, and it is up to the CO to ensure
these semantics are followed. The volume is then bind-mounted from the
staging target to the target. The SP (storage plugin) does create the
target dir, and removes it upon unpublish, per the spec.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #107 

**Special notes for your reviewer**:
This PR needs to be rebased after the PR for node stage/unstage is merged (#112)

Tested on a node that had an FCD attached, then use the `csc` tool to repeated publish and unpublish the volume to various targets (testing for idempotency and ability to publish the same volume to multiple targets).

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-12-07 19:36:29,2018-12-08 16:09:57
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/117,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/117,Implement NodeGetInfo,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This patch implements the CSI NodeGetInfo endpoint. For our purposes
right now, this just needs to return the Node ID, which is actually the
VM UUID as read from /sysfs with a conversion applied.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
This PR builds off of two prior PRs, #107, and #106. Those will need to be merged first so this can be rebased on top of them.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Node
```
",closed,True,2018-12-07 20:21:48,2018-12-08 23:15:06
cloud-provider-vsphere,dougm,https://github.com/kubernetes/cloud-provider-vsphere/pull/118,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/118,Fix cloudprovider test races,"GuestFullName is the OS type, which is the same for all VMs. This means the FindByDnsName impl could randomly return the wrong VM.
Change to using the vm.Name which is unique in these test cases.

DiscoverNode had a race where wg.Done() could be called before wg.Add(), causing wg.Wait() to return earlier than expected.
",closed,True,2018-12-07 20:23:36,2018-12-07 20:37:48
cloud-provider-vsphere,dougm,https://github.com/kubernetes/cloud-provider-vsphere/pull/119,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/119,More fixes for cloudprovider test races,"- Protect read/write access to VSphereInstance.Conn field (in-tree does the same)

- Avoid use of runtime.SetFinalizer in tests as it randomly kicks in, logging out client sessions

- Fixup error message when vm search fails
",closed,True,2018-12-08 17:43:04,2018-12-08 22:44:02
cloud-provider-vsphere,dougm,https://github.com/kubernetes/cloud-provider-vsphere/pull/120,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/120,Add dvonthenen to OWNERS,,closed,True,2018-12-08 20:01:16,2018-12-08 22:31:12
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/121,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/121,Add empty response for ValidateVolumeCapabilities,"
<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: The ValidateVolumeCapabilities is required to be implemented for the
Controller service. This is the last missing required API. Right now it
just returns an empty response, which is allowed. Since the ""confirmed""
value is empty, this tells the CO that it has *not* confirmed that any
of the requested capabilties for the given volume have been confirmed.


**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**: This probably isn't strictly necessary for the Alpha version, but it at least brings us in line with all the **required** APIs for CSI. Returning nil here could have caused problems if a CO were to actually call this method.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-12-09 22:15:23,2018-12-11 18:36:12
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/122,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/122,[WIP] YAML updates for CSI controller/node,"**What this PR does / why we need it**:
TODO

**Which issue this PR fixes**: NA

**Special notes for your reviewer**:
",closed,True,2018-12-10 04:03:18,2018-12-14 21:03:00
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/123,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/123,Always create a Controller Service early,"

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: 
The previous code would skip creating a Controller Service if one was
not needed, and if it was needed, the service would would be created at
the end of BeforeServe. With GoCSI, this was too late because a check
was done to see that the ControllerService was not nil, and the previous
code told GoCSI that it was nil.

This patch changes GetController() to instantiate a ControllerService
struct early and return it, even though it is not initialize. To
facilitate this, New() not returns a nil struct, and Init() has been
added to actually configure the controller with the vSphere config file.

This patch also removes the calls for `log.Fatal*` - we need to avoid
the use of Fatal as it prevents GoCSI from performing automatic sock
file cleanup.
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-12-11 19:28:34,2019-03-11 20:24:38
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/124,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/124,Initial release of CSI YAML + Page83Data,"**What this PR does / why we need it**:
This provides YAML to deploy the vsphere-csi controller and node components for the driver. It also provides documentation updates to guide a user to test a basic pod for reference.

An additional change was made:
- provide the Page83Data in the PublishContext that will be needed by the node component to mount the appropriate device to the VM.
- NodeGetInfo was changed to use os.Hostname() and not getSystemUUID() because k8s uses the Hostname to uniquely identify nodes. Please see past PRs on the CCM.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**:
Was tested on:
k8s 1.13.0 cluster
vsphere 6.7u1
",closed,True,2018-12-17 22:55:55,2018-12-18 19:16:46
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/125,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/125,require page83 data in publish context for FCD,"

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: 
When using FCD, the Volume ID does not map to the UUID that shows up for
the disk in /dev/disk/by-id/wwn-0x*****. Instead, the UUID needs to be
passed by the controller in the publish context. This patch changes the
node service to look for that field in the publish context, and requires
it to work.

Since the Volume ID is no longer sufficient to look up the block device,
the Unstage and Unpublish methods change to looking up the underyling
block device by seeing what is mounted to the respective targets. If a
block device is mounted to the given target, it is assumed that it is
the block device corresponding to the Volume ID -- we no longer have a
way to confirm that.

See #124 for more context.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**: This needs a rebase after #124 is merged, as they both add the `page83data` attribute/constant.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-12-18 18:34:55,2019-03-11 20:24:46
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/126,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/126,Update CSI container image with FS utilities,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Install utilities for ext2/3/4, XFS, and BtrFS, along with lsblk.

`toybox` has to be uninstalled from Photon 2.0, otherwise it conflicts with `e2fsprogs` and others.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
I considered also installing `ca-certificates`, but chose not to since our YAML examples mount `/etc/ssl/certificates` from the host.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```",closed,True,2018-12-18 20:59:57,2019-03-11 20:24:52
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/127,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/127,Refactor Config package to allow for specialization,"**What this PR does / why we need it**:
Refactor pkg/common/config to allow for specialization or customization of a config file based on the components need. This allows a component to have a configuration that only pertains to it without requiring other components config to be aware of it.

An example of how this could be done is by creating a Config object that wraps the common one:
```
import (
 vcfg ""k8s.io/cloud-provider-vsphere/pkg/common/config""
)

struct MyComponent {
  vcfg.Config
  OtherField string
}
```

**Which issue this PR fixes**: NA

**Special notes for your reviewer**:
Tested both the CCM and CSI components on:
k8s 1.13.1 cluster with 10 worker nodes
vSphere 6.7u1
Used a multiple vCenters (2 to be exact) with multiple Datacenters (3 to be exact)
`make test` passes

**Release note**:
This is only a refactor of existing code and the changes to CCM and CSI functionality does not impact the end user, documentation, or etc.",closed,True,2019-01-10 22:40:19,2019-01-11 16:28:00
cloud-provider-vsphere,shahbour,https://github.com/kubernetes/cloud-provider-vsphere/issues/128,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/128,Move from intree to out of tree,"Hello

I am trying to move vsphere from intree to out of tree but when checking the [docs](https://github.com/kubernetes/cloud-provider-vsphere/tree/master/docs) I see that we can install `deploying_cloud_provider_vsphere_with_rbac.md` and  `deploying_csi_vsphere_with_rbac.md` , what is the differnece between them and should i install them both ?

Thanks
",closed,False,2019-01-14 12:56:26,2019-02-05 20:02:02
cloud-provider-vsphere,Dr-Dream,https://github.com/kubernetes/cloud-provider-vsphere/issues/129,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/129,"Failed to run CSI with incompatible version, csi-controller gone into unrecoverable state","<!-- This form is for bug reports and feature requests! -->

BUG REPORT

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
Instaled k8s v1.13.2.
cloud-provider=external.
vsphere.conf with secrets.

Installed cloud-controller-manager according docs
Works ok.

Installed csi according docs.
Started.

So far so good, added storage class 'vsphere-gold' and created pvc.
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-vsphere-csi-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: vsphere-gold
```

got error
kubectl --kubeconfig admin.conf -n kube-system logs -f vsphere-csi-controller-0 vsphere-csi-controller
```
time=""2019-01-21T15:19:45Z"" level=debug msg=""enabled context injector""
time=""2019-01-21T15:19:45Z"" level=debug msg=""init req & rep validation"" withSpec=false
time=""2019-01-21T15:19:45Z"" level=debug msg=""init implicit rep validation"" withSpecRep=false
time=""2019-01-21T15:19:45Z"" level=debug msg=""init req validation"" withSpecReq=false
time=""2019-01-21T15:19:45Z"" level=debug msg=""enabled request ID injector""
time=""2019-01-21T15:19:45Z"" level=debug msg=""enabled request logging""
time=""2019-01-21T15:19:45Z"" level=debug msg=""enabled response logging""
time=""2019-01-21T15:19:45Z"" level=debug msg=""enabled serial volume access""
time=""2019-01-21T15:19:45Z"" level=info msg=""configured: io.k8s.cloud-provider-vsphere.vsphere"" api=FCD mode=controller
time=""2019-01-21T15:19:45Z"" level=info msg=""identity service registered""
time=""2019-01-21T15:19:45Z"" level=info msg=""controller service registered""
time=""2019-01-21T15:19:45Z"" level=info msg=serving endpoint=""unix:///var/lib/csi/sockets/pluginproxy/csi.sock""
time=""2019-01-21T15:19:46Z"" level=debug msg=""/csi.v1.Identity/Probe: REQ 0001: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:19:46Z"" level=debug msg=""/csi.v1.Identity/Probe: REP 0001: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:19:46Z"" level=debug msg=""/csi.v1.Identity/GetPluginInfo: REQ 0002: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:19:46Z"" level=debug msg=""/csi.v1.Identity/GetPluginInfo: REP 0002: Name=io.k8s.cloud-provider-vsphere.vsphere, VendorVersion=v0.1.1, XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:19:46Z"" level=debug msg=""/csi.v1.Identity/GetPluginCapabilities: REQ 0003: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:19:46Z"" level=debug msg=""/csi.v1.Identity/GetPluginCapabilities: REP 0003: Capabilities=[service:<type:CONTROLLER_SERVICE > ], XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:19:46Z"" level=debug msg=""/csi.v1.Controller/ControllerGetCapabilities: REQ 0004: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:19:46Z"" level=debug msg=""/csi.v1.Controller/ControllerGetCapabilities: REP 0004: Capabilities=[rpc:<type:LIST_VOLUMES >  rpc:<type:CREATE_DELETE_VOLUME >  rpc:<type:PUBLISH_UNPUBLISH_VOLUME > ], XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:28:35Z"" level=debug msg=""/csi.v1.Identity/GetPluginCapabilities: REQ 0005: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:28:35Z"" level=debug msg=""/csi.v1.Identity/GetPluginCapabilities: REP 0005: Capabilities=[service:<type:CONTROLLER_SERVICE > ], XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:28:35Z"" level=debug msg=""/csi.v1.Controller/ControllerGetCapabilities: REQ 0006: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:28:35Z"" level=debug msg=""/csi.v1.Controller/ControllerGetCapabilities: REP 0006: Capabilities=[rpc:<type:LIST_VOLUMES >  rpc:<type:CREATE_DELETE_VOLUME >  rpc:<type:PUBLISH_UNPUBLISH_VOLUME > ], XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:28:35Z"" level=debug msg=""/csi.v1.Identity/GetPluginInfo: REQ 0007: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:28:35Z"" level=debug msg=""/csi.v1.Identity/GetPluginInfo: REP 0007: Name=io.k8s.cloud-provider-vsphere.vsphere, VendorVersion=v0.1.1, XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:28:35Z"" level=debug msg=""/csi.v1.Controller/CreateVolume: REQ 0008: Name=pvc-34370d70-1d91-11e9-8953-00505697cc54, CapacityRange=required_bytes:1073741824 , VolumeCapabilities=[mount:<fs_type:\""ext4\"" > access_mode:<mode:SINGLE_NODE_WRITER > ], Parameters=map[parent_type:Datastore parent_name:ESXi-CL1-VV1 (RAID 50)], XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
ERROR: logging before flag.Parse: E0121 15:28:40.988002       1 connection.go:63] Failed to create govmomi client. err: ServerFaultCode: Cannot complete login due to an incorrect user name or password.
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0x15a6d36]
```
According 'Cannot complete login due to an incorrect user name or password' i decided there is authentication issue.
Recreated cluster.
Added server, username and password to vsphere.conf
Installed ccm and csi.
Created pvc and got:
```
kubectl --kubeconfig admin.conf -n kube-system logs -f vsphere-csi-controller-0 vsphere-csi-controller
time=""2019-01-21T15:54:15Z"" level=debug msg=""enabled context injector""
time=""2019-01-21T15:54:15Z"" level=debug msg=""init req & rep validation"" withSpec=false
time=""2019-01-21T15:54:15Z"" level=debug msg=""init implicit rep validation"" withSpecRep=false
time=""2019-01-21T15:54:15Z"" level=debug msg=""init req validation"" withSpecReq=false
time=""2019-01-21T15:54:15Z"" level=debug msg=""enabled request ID injector""
time=""2019-01-21T15:54:15Z"" level=debug msg=""enabled request logging""
time=""2019-01-21T15:54:15Z"" level=debug msg=""enabled response logging""
time=""2019-01-21T15:54:15Z"" level=debug msg=""enabled serial volume access""
time=""2019-01-21T15:54:15Z"" level=info msg=""configured: io.k8s.cloud-provider-vsphere.vsphere"" api=FCD mode=controller
time=""2019-01-21T15:54:15Z"" level=info msg=""identity service registered""
time=""2019-01-21T15:54:15Z"" level=info msg=""controller service registered""
time=""2019-01-21T15:54:15Z"" level=info msg=serving endpoint=""unix:///var/lib/csi/sockets/pluginproxy/csi.sock""
time=""2019-01-21T15:54:16Z"" level=debug msg=""/csi.v1.Identity/Probe: REQ 0001: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:54:16Z"" level=debug msg=""/csi.v1.Identity/Probe: REP 0001: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:54:16Z"" level=debug msg=""/csi.v1.Identity/GetPluginInfo: REQ 0002: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:54:16Z"" level=debug msg=""/csi.v1.Identity/GetPluginInfo: REP 0002: Name=io.k8s.cloud-provider-vsphere.vsphere, VendorVersion=v0.1.1, XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:54:16Z"" level=debug msg=""/csi.v1.Identity/GetPluginCapabilities: REQ 0003: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:54:16Z"" level=debug msg=""/csi.v1.Identity/GetPluginCapabilities: REP 0003: Capabilities=[service:<type:CONTROLLER_SERVICE > ], XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:54:16Z"" level=debug msg=""/csi.v1.Controller/ControllerGetCapabilities: REQ 0004: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:54:16Z"" level=debug msg=""/csi.v1.Controller/ControllerGetCapabilities: REP 0004: Capabilities=[rpc:<type:LIST_VOLUMES >  rpc:<type:CREATE_DELETE_VOLUME >  rpc:<type:PUBLISH_UNPUBLISH_VOLUME > ], XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:58:19Z"" level=debug msg=""/csi.v1.Identity/GetPluginCapabilities: REQ 0005: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:58:19Z"" level=debug msg=""/csi.v1.Identity/GetPluginCapabilities: REP 0005: Capabilities=[service:<type:CONTROLLER_SERVICE > ], XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:58:19Z"" level=debug msg=""/csi.v1.Controller/ControllerGetCapabilities: REQ 0006: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:58:19Z"" level=debug msg=""/csi.v1.Controller/ControllerGetCapabilities: REP 0006: Capabilities=[rpc:<type:LIST_VOLUMES >  rpc:<type:CREATE_DELETE_VOLUME >  rpc:<type:PUBLISH_UNPUBLISH_VOLUME > ], XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:58:19Z"" level=debug msg=""/csi.v1.Identity/GetPluginInfo: REQ 0007: XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:58:19Z"" level=debug msg=""/csi.v1.Identity/GetPluginInfo: REP 0007: Name=io.k8s.cloud-provider-vsphere.vsphere, VendorVersion=v0.1.1, XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
time=""2019-01-21T15:58:19Z"" level=debug msg=""/csi.v1.Controller/CreateVolume: REQ 0008: Name=pvc-5ee40328-1d95-11e9-8858-0050569742c5, CapacityRange=required_bytes:1073741824 , VolumeCapabilities=[mount:<fs_type:\""ext4\"" > access_mode:<mode:SINGLE_NODE_WRITER > ], Parameters=map[parent_name:ESXi-CL1-VV1 (RAID 50) parent_type:Datastore], XXX_NoUnkeyedLiteral={}, XXX_sizecache=0""
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0x15a6d36]

goroutine 76 [running]:
k8s.io/cloud-provider-vsphere/vendor/github.com/vmware/govmomi/vslm.NewObjectManager(...)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/vmware/govmomi/vslm/object_manager.go:41
k8s.io/cloud-provider-vsphere/pkg/common/vclib.(*DatastoreInfo).GetFirstClassDiskInfo(0xc00072e270, 0x1e3c700, 0xc0006071d0, 0xc0006453b0, 0x28, 0x1, 0x0, 0x0, 0x41199c)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/pkg/common/vclib/datastore.go:197 +0x86
k8s.io/cloud-provider-vsphere/pkg/common/vclib.(*Datacenter).GetFirstClassDisk(0xc0000a6228, 0x1e3c700, 0xc0006071d0, 0xc0000a5280, 0x16, 0x1bd6372, 0x9, 0xc0006453b0, 0x28, 0x1, ...)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/pkg/common/vclib/datacenter.go:518 +0x1ab
k8s.io/cloud-provider-vsphere/pkg/csi/service/fcd.(*controller).CreateVolume(0xc00000ab80, 0x1e3c700, 0xc0006071d0, 0xc0005b83f0, 0xc00000ab80, 0xc000620e01, 0xc000185390)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/pkg/csi/service/fcd/controller.go:132 +0x4a0
k8s.io/cloud-provider-vsphere/vendor/github.com/container-storage-interface/spec/lib/go/csi._Controller_CreateVolume_Handler.func1(0x1e3c700, 0xc0006071d0, 0x1b46320, 0xc0005b83f0, 0x0, 0x1e2a300, 0xc000620e40, 0x0)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/container-storage-interface/spec/lib/go/csi/csi.pb.go:4579 +0x86
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/serialvolume.(*interceptor).createVolume(0xc0000b0d80, 0x1e3c700, 0xc0006071d0, 0xc0005b83f0, 0xc000620c60, 0xc000620c80, 0x0, 0x0, 0x0, 0x0)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/serialvolume/serial_volume_locker.go:162 +0x161
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/serialvolume.(*interceptor).handle(0xc0000b0d80, 0x1e3c700, 0xc0006071d0, 0x1b46320, 0xc0005b83f0, 0xc000620c60, 0xc000620c80, 0x180, 0x14d, 0x0, ...)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/serialvolume/serial_volume_locker.go:90 +0x37d
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/serialvolume.(*interceptor).handle-fm(0x1e3c700, 0xc0006071d0, 0x1b46320, 0xc0005b83f0, 0xc000620c60, 0xc000620c80, 0xc00036a0c0, 0xc000185558, 0x4c7ab7, 0xc00036a0c0)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/serialvolume/serial_volume_locker.go:71 +0x73
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/utils.ChainUnaryServer.func2.1.1(0x1e3c700, 0xc0006071d0, 0x1b46320, 0xc0005b83f0, 0x14d, 0x0, 0xc00036a0c0, 0x0)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/utils/utils_middleware.go:99 +0x63
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/logging.(*interceptor).handleServer.func1(0x1e18160, 0xc0000b0c80, 0xc000185670, 0x1)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/logging/logging_interceptor.go:84 +0x49
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/logging.(*interceptor).handle(0xc0000b0ce0, 0x1e3c700, 0xc0006071d0, 0x1c0b16f, 0x1f, 0x1b46320, 0xc0005b83f0, 0xc000185720, 0xc000000064, 0x412da5, ...)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/logging/logging_interceptor.go:130 +0xc6
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/logging.(*interceptor).handleServer(0xc0000b0ce0, 0x1e3c700, 0xc0006071d0, 0x1b46320, 0xc0005b83f0, 0xc000620c60, 0xc000620ca0, 0xc0003f57c0, 0x0, 0xc00023a000, ...)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/logging/logging_interceptor.go:83 +0xe0
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/logging.(*interceptor).handleServer-fm(0x1e3c700, 0xc0006071d0, 0x1b46320, 0xc0005b83f0, 0xc000620c60, 0xc000620ca0, 0x10, 0x17ddfe0, 0x316da01, 0xffffffffffffffff)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/logging/logging_interceptor.go:58 +0x73
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/utils.ChainUnaryServer.func2.1.1(0x1e3c700, 0xc0006071d0, 0x1b46320, 0xc0005b83f0, 0x8, 0x0, 0x0, 0x30)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/utils/utils_middleware.go:99 +0x63
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/requestid.(*interceptor).handleServer(0xc000045860, 0x1e3c700, 0xc0006071d0, 0x1b46320, 0xc0005b83f0, 0xc000620c60, 0xc000620cc0, 0xc000333918, 0x4d7b4b, 0x1ab6f40, ...)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/requestid/request_id_injector.go:83 +0x28e
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/requestid.(*interceptor).handleServer-fm(0x1e3c700, 0xc0006071d0, 0x1b46320, 0xc0005b83f0, 0xc000620c60, 0xc000620cc0, 0x1880ba0, 0xc0003f57b0, 0x1e3c700, 0xc0006071d0)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware/requestid/request_id_injector.go:24 +0x73
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/utils.ChainUnaryServer.func2.1.1(0x1e3c700, 0xc0006071d0, 0x1b46320, 0xc0005b83f0, 0xc0006071d0, 0x20, 0x20, 0x1a6cc00)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/utils/utils_middleware.go:99 +0x63
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi.(*StoragePlugin).injectContext(0xc000416000, 0x1e3c700, 0xc000606ea0, 0x1b46320, 0xc0005b83f0, 0xc000620c60, 0xc000620ce0, 0x0, 0x0, 0xc00023a000, ...)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware.go:226 +0xa7
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi.(*StoragePlugin).injectContext-fm(0x1e3c700, 0xc000606ea0, 0x1b46320, 0xc0005b83f0, 0xc000620c60, 0xc000620ce0, 0x854efa, 0x1a6cc00, 0xc000620d00, 0xc000620c60)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/middleware.go:22 +0x73
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/utils.ChainUnaryServer.func2.1.1(0x1e3c700, 0xc000606ea0, 0x1b46320, 0xc0005b83f0, 0x31ed738, 0xc000333ae8, 0x40c1d8, 0x20)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/utils/utils_middleware.go:99 +0x63
k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/utils.ChainUnaryServer.func2(0x1e3c700, 0xc000606ea0, 0x1b46320, 0xc0005b83f0, 0xc000620c60, 0xc000620c80, 0x1882e80, 0x31ed738, 0x1b7d5e0, 0xc00002c900)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/rexray/gocsi/utils/utils_middleware.go:106 +0xf9
k8s.io/cloud-provider-vsphere/vendor/github.com/container-storage-interface/spec/lib/go/csi._Controller_CreateVolume_Handler(0x1b0d460, 0xc00000ab80, 0x1e3c700, 0xc000606ea0, 0xc0005b8380, 0xc00000b340, 0x0, 0x0, 0x30, 0xc0003a6660)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/github.com/container-storage-interface/spec/lib/go/csi/csi.pb.go:4581 +0x158
k8s.io/cloud-provider-vsphere/vendor/google.golang.org/grpc.(*Server).processUnaryRPC(0xc000163180, 0x1e492e0, 0xc00066d000, 0xc00002c900, 0xc0003e90e0, 0x31bf9a0, 0x0, 0x0, 0x0)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/google.golang.org/grpc/server.go:1026 +0x4cd
k8s.io/cloud-provider-vsphere/vendor/google.golang.org/grpc.(*Server).handleStream(0xc000163180, 0x1e492e0, 0xc00066d000, 0xc00002c900, 0x0)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/google.golang.org/grpc/server.go:1252 +0x1308
k8s.io/cloud-provider-vsphere/vendor/google.golang.org/grpc.(*Server).serveStreams.func1.1(0xc00060c4f0, 0xc000163180, 0x1e492e0, 0xc00066d000, 0xc00002c900)
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/google.golang.org/grpc/server.go:699 +0x9f
created by k8s.io/cloud-provider-vsphere/vendor/google.golang.org/grpc.(*Server).serveStreams.func1
        /Users/trhoden/go/src/k8s.io/cloud-provider-vsphere/vendor/google.golang.org/grpc/server.go:697 +0xa1
```
After container failed permamnet with log on vsphere-csi-controller
`time=""2019-01-22T09:00:40Z"" level=fatal msg=""failed to listen"" error=""listen unix /var/lib/csi/sockets/pluginproxy/csi.sock: bind: address already in use""`

**What you expected to happen**:
Understable error messages in csi .
Recovery after failure,

**How to reproduce it (as minimally and precisely as possible)**:
Deploy k8s v1.13.1
Follow current instructions form docs.

**Anything else we need to know?**:
It runs on vCener 5.5.
logs from ccm
`I0122 09:12:06.986602       1 event.go:221] Event(v1.ObjectReference{Kind:""PersistentVolumeClaim"", Namespace:""default"", Name:""my-vsphere-csi-pvc"", UID:""d711fa42-1e23-11e9-8a47-0050569729eb"", APIVersion:""v1"", ResourceVersion:""164789"", FieldPath:""""}): type: 'Normal' reason: 'ExternalProvisioning' waiting for a volume to be created, either by external provisioner ""io.k8s.cloud-provider-vsphere.vsphere"" or manually created by system administrator
`
**Environment**:
- vsphere-cloud-controller-manager version:
gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager:latest
latest from google repository
- OS (e.g. from /etc/os-release):
```
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
`Linux k8s-master-0 3.10.0-957.1.3.el7.x86_64 #1 SMP Thu Nov 29 14:49:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux`
- Install tools:
```
yum info open-vm-tools
Installed Packages
Name        : open-vm-tools
Arch        : x86_64
Version     : 10.2.5
Release     : 3.el7
```

```
NAME                                     READY     STATUS             RESTARTS   AGE       IP             NODE
coredns-6fd7dbf94c-4brvg                 1/1       Running            0          18h       10.244.5.2     k8s-worker-2
coredns-6fd7dbf94c-blt57                 1/1       Running            0          18h       10.244.2.3     k8s-master-1
dns-autoscaler-5b4847c446-wqfs6          1/1       Running            0          18h       10.244.3.2     k8s-worker-1
kube-apiserver-k8s-master-0              1/1       Running            0          18h       10.100.0.110   k8s-master-0
kube-apiserver-k8s-master-1              1/1       Running            0          18h       10.100.0.111   k8s-master-1
kube-apiserver-k8s-master-2              1/1       Running            0          18h       10.100.0.112   k8s-master-2
kube-controller-manager-k8s-master-0     1/1       Running            0          18h       10.100.0.110   k8s-master-0
kube-controller-manager-k8s-master-1     1/1       Running            0          18h       10.100.0.111   k8s-master-1
kube-controller-manager-k8s-master-2     1/1       Running            0          18h       10.100.0.112   k8s-master-2
kube-flannel-6w66f                       2/2       Running            0          18h       10.100.0.115   k8s-worker-2
kube-flannel-fk85t                       2/2       Running            0          18h       10.100.0.110   k8s-master-0
kube-flannel-m7mhv                       2/2       Running            0          18h       10.100.0.112   k8s-master-2
kube-flannel-wzfx4                       2/2       Running            0          18h       10.100.0.111   k8s-master-1
kube-flannel-z2nb6                       2/2       Running            0          18h       10.100.0.114   k8s-worker-1
kube-flannel-zhxtq                       2/2       Running            0          18h       10.100.0.113   k8s-worker-0
kube-proxy-8ldbh                         1/1       Running            0          18h       10.100.0.110   k8s-master-0
kube-proxy-9sdts                         1/1       Running            0          18h       10.100.0.112   k8s-master-2
kube-proxy-cqq72                         1/1       Running            0          18h       10.100.0.113   k8s-worker-0
kube-proxy-rj8jt                         1/1       Running            0          18h       10.100.0.115   k8s-worker-2
kube-proxy-v2x8p                         1/1       Running            0          18h       10.100.0.114   k8s-worker-1
kube-proxy-wtx5z                         1/1       Running            0          18h       10.100.0.111   k8s-master-1
kube-scheduler-k8s-master-0              1/1       Running            0          18h       10.100.0.110   k8s-master-0
kube-scheduler-k8s-master-1              1/1       Running            0          18h       10.100.0.111   k8s-master-1
kube-scheduler-k8s-master-2              1/1       Running            0          18h       10.100.0.112   k8s-master-2
kubernetes-dashboard-8457c55f89-ghb8m    1/1       Running            0          18h       10.244.2.2     k8s-master-1
tiller-deploy-56794866d9-c7g5k           1/1       Running            0          18h       10.244.3.4     k8s-worker-1
vsphere-cloud-controller-manager-88s4g   1/1       Running            0          18h       10.100.0.111   k8s-master-1
vsphere-cloud-controller-manager-k9drk   1/1       Running            0          18h       10.100.0.112   k8s-master-2
vsphere-cloud-controller-manager-xjwwr   1/1       Running            0          18h       10.100.0.110   k8s-master-0
vsphere-csi-controller-0                 2/3       CrashLoopBackOff   6          16h       10.244.0.7     k8s-master-0
vsphere-csi-node-l2c5h                   2/2       Running            0          18h       10.100.0.115   k8s-worker-2
vsphere-csi-node-q4xw5                   2/2       Running            0          18h       10.100.0.114   k8s-worker-1
vsphere-csi-node-v8vsj                   2/2       Running            0          18h       10.100.0.113   k8s-worker-0
```
- Others:
",closed,False,2019-01-22 09:15:21,2019-02-13 15:31:52
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/130,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/130,Implement Zone Support on CCM,"**What this PR does / why we need it**:
This implements the zone support that currently exists within the in-tree provider.

Additional notes:
- Updates to the documentation to note atypical behaviors.
- Removed sections of YAML that were overriding permissions of the service account used for CCM that was causing the zones functionality to not work. It turns out these sections aren't required anyways.
- Moved GetUUIDFromProviderID and ConvertK8sUUIDtoNormal out of NodeManager to a common util file so that other code can use without requiring an instance of NodeManager.
- Fixed bug in NodeManager which was causing intermittent failure of zones discovery in a multi-vCenter configuration. Side effect was that the wrong VC address would be assigned/associated to a given node. Fix is moving the declaration of `var datacenterObjs []*vclib.Datacenter` to adjust its scope. Turns out this issue also exists in the in-tree provider. Will file a separate PR for k/k.

**Which issue this PR fixes**:
https://github.com/kubernetes/cloud-provider-vsphere/issues/36

**Special notes for your reviewer**:
Tested CCM on:
k8s 1.13.2 cluster with 10 worker nodes in 3 zones
vSphere 6.7u1
Deployed a test pod which targeted a particular zone
Used a multiple vCenters (2 to be exact) with multiple Datacenters (3 to be exact)
`make test` passes

**Release note**:
For any tests (like e2e or etc), the removal of sections of YAML is important for the zones functionality. Please take note of these changes.",closed,True,2019-01-23 15:43:40,2019-01-24 15:00:02
cloud-provider-vsphere,andrewsykim,https://github.com/kubernetes/cloud-provider-vsphere/pull/131,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/131,fix error handling bug in node discovery,"**What this PR does / why we need it**:
Due to a bug in how we handle errors in node discovery, the vSphere provider would panic because we try to fetch NodeAddresses from a nil pointer. Panic discovered by @SandeepPissay, thank you! 

**Release note**:
```release-note
NONE
```
",closed,True,2019-01-25 01:23:50,2019-01-25 14:51:12
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/132,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/132,Fixes ci/latest conformance tests by adding support for custom sk8 URL,"This patch fixes the `ci/latest` conformance tests by adding support for a custom sk8 URL. This URL is the script used by the image that runs the e2e conformance tests.

@frapposelli, prior to merging this PR you need to add the following the projects TravisCI.org settings:

```shell
SK8_URL=https://raw.githubusercontent.com/vmware/simple-k8s-test-env/v0.1.1/yakity.sh
```",closed,True,2019-01-26 19:00:28,2019-01-26 21:21:27
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/133,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/133,Remove K8s 1.10 / add 1.13 to CI,"This PR removes Kubernetes 1.10 from the list of GA releases tested against `master` and with the nightly cron job. In its place K8s 1.13 has been added. The prevailing CI strategy is to test `ci/latest` and the three, previous GA release trains, which are currently `1.13`, `1.12`, and `1.11`.

cc @figo",closed,True,2019-01-28 14:58:30,2019-01-28 18:01:14
cloud-provider-vsphere,divyenpatel,https://github.com/kubernetes/cloud-provider-vsphere/issues/134,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/134,`kubectl get pv` is not listing dynamically created PVs by vSphere CSI Driver,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
> /kind bug


**What happened**:
Dynamically provisioned persistence volume using vSphere CSI Driver does not get listed with `kubectl get pv` command.

```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.2"", GitCommit:""cff46ab41ff0bb44d8584413b598ad8360ec1def"", GitTreeState:""clean"", BuildDate:""2019-01-10T23:35:51Z"", GoVersion:""go1.11.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.2"", GitCommit:""cff46ab41ff0bb44d8584413b598ad8360ec1def"", GitTreeState:""clean"", BuildDate:""2019-01-10T23:28:14Z"", GoVersion:""go1.11.4"", Compiler:""gc"", Platform:""linux/amd64""}
```

```
# kubectl get pv
No resources found.
```

Strange thing is `Kubectl describe pv` is listing the PV.

```
# kubectl describe pv
Name:            pvc-b4d9f41b-2441-11e9-9c0e-005056a43de5
Labels:          <none>
Annotations:     pv.kubernetes.io/provisioned-by: io.k8s.cloud-provider-vsphere.vsphere
Finalizers:      []
StorageClass:    my-vsphere-fcd-class
Status:          Pending
Claim:           default/my-vsphere-csi-pvc
Reclaim Policy:  Delete
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        5Gi
Node Affinity:   <none>
Message:         
Source:
    Type:              CSI (a Container Storage Interface (CSI) volume source)
    Driver:            io.k8s.cloud-provider-vsphere.vsphere
    VolumeHandle:      167ba236-784f-4237-b17e-37dc519bf8b0
    ReadOnly:          false
    VolumeAttributes:      datacenter=vcqaDC
                           name=pvc-b4d9f41b-2441-11e9-9c0e-005056a43de5
                           parent_name=sharedVmfs-0
                           parent_type=Datastore
                           storage.kubernetes.io/csiProvisionerIdentity=1548817403656-8081-io.k8s.cloud-provider-vsphere.vsphere
                           type=First Class Disk
                           vcenter=10.161.155.44
Events:                <none>
```

If I specify PV name with `kubectl get pv pvc-b4d9f41b-2441-11e9-9c0e-005056a43de5`, I can see PV is getting listed.

```
# kubectl get pv pvc-b4d9f41b-2441-11e9-9c0e-005056a43de5
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                        STORAGECLASS           REASON   AGE
pvc-b4d9f41b-2441-11e9-9c0e-005056a43de5   5Gi        RWO            Delete           Pending   default/my-vsphere-csi-pvc   my-vsphere-fcd-class            13h
```

**What you expected to happen**:
`kubectl get pv` should list PVs created by `io.k8s.cloud-provider-vsphere.vsphere`

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- vsphere-cloud-controller-manager version:
gcr.io/cloud-provider-vsphere/vsphere-csi:v0.1.1
gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager:latest

- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-01-30 17:00:44,2019-02-16 00:00:54
cloud-provider-vsphere,divyenpatel,https://github.com/kubernetes/cloud-provider-vsphere/issues/135,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/135,Dynamically provisioned Persistence Volume is not getting bound with its PV Claim.,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
> /kind bug


**What happened**:
Dynamically provisioned Persistence Volume is not getting bound with its PV Claim.

Storage Class
---------------
```
# cat sc.yaml 
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: my-vsphere-fcd-class
  annotations:
    storageclass.kubernetes.io/is-default-class: ""true""
provisioner: io.k8s.cloud-provider-vsphere.vsphere
parameters:
  parent_type: ""Datastore""
  parent_name: ""sharedVmfs-0""
```

Persistence Volume Claim
----------------------------
```
# cat pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-vsphere-csi-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: my-vsphere-fcd-class
```

```
# kubectl create -f sc.yaml 
storageclass.storage.k8s.io/my-vsphere-fcd-class created

# kubectl create -f pvc.yaml 
persistentvolumeclaim/my-vsphere-csi-pvc created
```


PV Claim and PV remains in the Pending State and does not get bound.
------------------------------------------------------------------------
```
# kubectl get pvc
NAME                 STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS           AGE
my-vsphere-csi-pvc   Pending                                      my-vsphere-fcd-class   21s


# kubectl describe pvc my-vsphere-csi-pvc
Name:          my-vsphere-csi-pvc
Namespace:     default
StorageClass:  my-vsphere-fcd-class
Status:        Pending
Volume:        
Labels:        <none>
Annotations:   volume.beta.kubernetes.io/storage-provisioner: io.k8s.cloud-provider-vsphere.vsphere
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      
Access Modes:  
VolumeMode:    Filesystem
Events:
  Type       Reason                 Age                From                                                                                                 Message
  ----       ------                 ----               ----                                                                                                 -------
  Normal     Provisioning           55s                io.k8s.cloud-provider-vsphere.vsphere_vsphere-csi-controller-0_a0ab202d-243b-11e9-90a7-0a580af40102  External provisioner is provisioning volume for claim ""default/my-vsphere-csi-pvc""
  Normal     ProvisioningSucceeded  14s                io.k8s.cloud-provider-vsphere.vsphere_vsphere-csi-controller-0_a0ab202d-243b-11e9-90a7-0a580af40102  Successfully provisioned volume pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5
  Normal     ExternalProvisioning   10s (x5 over 55s)  persistentvolume-controller                                                                          waiting for a volume to be created, either by external provisioner ""io.k8s.cloud-provider-vsphere.vsphere"" or manually created by system administrator
Mounted By:  my-csi-app
```


```
# kubectl describe pv pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5
Name:            pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5
Labels:          <none>
Annotations:     pv.kubernetes.io/provisioned-by: io.k8s.cloud-provider-vsphere.vsphere
Finalizers:      []
StorageClass:    my-vsphere-fcd-class
Status:          Pending
Claim:           default/my-vsphere-csi-pvc
Reclaim Policy:  Delete
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        5Gi
Node Affinity:   <none>
Message:         
Source:
    Type:              CSI (a Container Storage Interface (CSI) volume source)
    Driver:            io.k8s.cloud-provider-vsphere.vsphere
    VolumeHandle:      0cb048c8-0f2a-4b8a-bbc4-8f1fec1b1fe6
    ReadOnly:          false
    VolumeAttributes:      datacenter=vcqaDC
                           name=pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5
                           parent_name=sharedVmfs-0
                           parent_type=Datastore
                           storage.kubernetes.io/csiProvisionerIdentity=1548817403656-8081-io.k8s.cloud-provider-vsphere.vsphere
                           type=First Class Disk
                           vcenter=10.161.155.44
Events:                <none>
```


Logs
-----

**csi-provisioner logs**
```
I0130 18:05:52.034637       1 controller.go:605] create volume rep: {CapacityBytes:5368709120 VolumeId:0cb048c8-0f2a-4b8a-bbc4-8f1fec1b1fe6 VolumeContext:map[parent_name:sharedVmfs-0 type:First Class Disk vcenter:10.161.155.44 datacenter:vcqaDC name:pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5 parent_type:Datastore] ContentSource:<nil> AccessibleTopology:[] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0}
I0130 18:05:52.034715       1 controller.go:666] successfully created PV {GCEPersistentDisk:nil AWSElasticBlockStore:nil HostPath:nil Glusterfs:nil NFS:nil RBD:nil ISCSI:nil Cinder:nil CephFS:nil FC:nil Flocker:nil FlexVolume:nil AzureFile:nil VsphereVolume:nil Quobyte:nil AzureDisk:nil PhotonPersistentDisk:nil PortworxVolume:nil ScaleIO:nil Local:nil StorageOS:nil CSI:&CSIPersistentVolumeSource{Driver:io.k8s.cloud-provider-vsphere.vsphere,VolumeHandle:0cb048c8-0f2a-4b8a-bbc4-8f1fec1b1fe6,ReadOnly:false,FSType:ext4,VolumeAttributes:map[string]string{datacenter: vcqaDC,name: pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5,parent_name: sharedVmfs-0,parent_type: Datastore,storage.kubernetes.io/csiProvisionerIdentity: 1548817403656-8081-io.k8s.cloud-provider-vsphere.vsphere,type: First Class Disk,vcenter: 10.161.155.44,},ControllerPublishSecretRef:nil,NodeStageSecretRef:nil,NodePublishSecretRef:nil,}}
I0130 18:05:52.034819       1 controller.go:1026] provision ""default/my-vsphere-csi-pvc"" class ""my-vsphere-fcd-class"": volume ""pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5"" provisioned
I0130 18:05:52.034836       1 controller.go:1040] provision ""default/my-vsphere-csi-pvc"" class ""my-vsphere-fcd-class"": trying to save persistentvolume ""pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5""
I0130 18:06:22.040682       1 controller.go:1052] provision ""default/my-vsphere-csi-pvc"" class ""my-vsphere-fcd-class"": failed to save persistentvolume ""pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5"": Timeout: request did not complete within requested timeout 30s
I0130 18:06:32.041055       1 controller.go:1040] provision ""default/my-vsphere-csi-pvc"" class ""my-vsphere-fcd-class"": trying to save persistentvolume ""pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5""
I0130 18:06:32.046834       1 controller.go:1044] provision ""default/my-vsphere-csi-pvc"" class ""my-vsphere-fcd-class"": persistentvolume ""pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5"" already exists, reusing
I0130 18:06:32.046992       1 controller.go:1088] provision ""default/my-vsphere-csi-pvc"" class ""my-vsphere-fcd-class"": succeeded
I0130 18:06:32.047058       1 event.go:221] Event(v1.ObjectReference{Kind:""PersistentVolumeClaim"", Namespace:""default"", Name:""my-vsphere-csi-pvc"", UID:""aa2fd750-24b9-11e9-9c0e-005056a43de5"", APIVersion:""v1"", ResourceVersion:""115696"", FieldPath:""""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-aa2fd750-24b9-11e9-9c0e-005056a43de5
```

**vsphere-csi-controller logs**

```
ERROR: logging before flag.Parse: W0130 18:05:51.189849       1 connection.go:77] Creating new client session since the existing session is not valid or not authenticated
ERROR: logging before flag.Parse: E0130 18:05:51.580279       1 datacenter.go:520] GetFirstClassDiskByName failed. Err: No vSphere disk ID/Name found
```

**kube-controller-manager logs**

```
I0130 18:12:06.149093       1 event.go:221] Event(v1.ObjectReference{Kind:""PersistentVolumeClaim"", Namespace:""default"", Name:""my-vsphere-csi-pvc"", UID:""aa2fd750-24b9-11e9-9c0e-005056a43de5"", APIVersion:""v1"", ResourceVersion:""115696"", FieldPath:""""}): type: 'Normal' reason: 'ExternalProvisioning' waiting for a volume to be created, either by external provisioner ""io.k8s.cloud-provider-vsphere.vsphere"" or manually created by system administrator
I0130 18:12:21.149188       1 event.go:221] Event(v1.ObjectReference{Kind:""PersistentVolumeClaim"", Namespace:""default"", Name:""my-vsphere-csi-pvc"", UID:""aa2fd750-24b9-11e9-9c0e-005056a43de5"", APIVersion:""v1"", ResourceVersion:""115696"", FieldPath:""""}): type: 'Normal' reason: 'ExternalProvisioning' waiting for a volume to be created, either by external provisioner ""io.k8s.cloud-provider-vsphere.vsphere"" or manually created by system administrator
I0130 18:12:36.149247       1 event.go:221] Event(v1.ObjectReference{Kind:""PersistentVolumeClaim"", Namespace:""default"", Name:""my-vsphere-csi-pvc"", UID:""aa2fd750-24b9-11e9-9c0e-005056a43de5"", APIVersion:""v1"", ResourceVersion:""115696"", FieldPath:""""}): type: 'Normal' reason: 'ExternalProvisioning' waiting for a volume to be created, either by external provisioner ""io.k8s.cloud-provider-vsphere.vsphere"" or manually created by system administrator
```

**What you expected to happen**:

PV Claim should get bind with dynamically created PV.

**Environment**:
--------------------

**kube-controller-manager**: `k8s.gcr.io/kube-controller-manager:v1.13.2`

**vsphere-csi-controller**: `quay.io/k8scsi/csi-provisioner:v1.0.1`

**vsphere-cloud-controller-manager**: `gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager:latest`

**vsphere-csi-node**: `quay.io/k8scsi/csi-node-driver-registrar:v1.0.1`

",closed,False,2019-01-30 18:25:25,2019-02-15 23:58:56
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/136,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/136,Initial Implementation for CSI multi-VC/DC Support,"**What this PR does / why we need it**:
This implements mutli-vCenter and multi-Datacenter support in the CSI driver components.
This also indirectly implements the second half of Zones support for CSI by way of supporting multi-VC/DC and adding code to support CreateVolume in a particular region/zone through the use of the native kubernetes topology support in CSI/k8s.

Some notable changes:
- Enables VOLUME_ACCESSIBILITY_CONSTRAINTS to allow the topology to get passed to the CSI driver. Note that the native topology support works for 1.12.X and higher. Capability exists in code to support 1.11.X and below.
- Moved implementation of nodemanager.DiscoverNode() into the pkg/common/connectionmanager to be reused by other components which is why you see a lot of code deletion in `nodemanager.go`.
- As a part of the move DiscoverNode() in the CM, add a `list.go` (example: grab all DCs in a config), `search.go` (where DiscoverNode() now lives along with DiscoverFcd()), and `zones.go` (handles object look up, like Datacenters or clusters, based on tags/zones) to be reused by CCM and CSI which is why you see a lot of additional code/files in `connectionmanager.go`
- refactored the Zones Interface in the CCM to make use of the common code above which is why you see a lot of the deletion of code in `zones.go`
- implements GLOG logging in the CSI components in `controller.go` based on CSI Debug being enabled. YAML is updated as well.
- updated CSI YAML to remove unnecessary properties (SSL config, kubeconfig, etc) that were actually interfering with permissions related to the CSI service account 
- updated CSI YAML to put place holders for zones

**TODOs**:
I have not created the images I plan on posting in the  docs/deploying_ccm_and_csi_with_multi_dc_vc_aka_zones.md documentation, but I will go back and fill that out later. I thought it was better to get this PR out for review and update with the diagrams/pictures later.

**Which issue this PR fixes**:
https://github.com/vmware/cna-upstream-planning/issues/323

**Special notes for your reviewer**:
Tested CCM on:
k8s 1.13.1 cluster with 10 worker nodes in 3 zones
vSphere 6.7u1
Deployed a test pod which targeted a particular zone using a persistent volume
Used a multiple vCenters (2 to be exact) with multiple Datacenters (3 to be exact)
`make test` passes

**Release note**:
The single VC/DC configuration should be backward compatible and all older single VC/DC based vsphere.conf should still work with this updated code.
",closed,True,2019-01-30 21:33:57,2019-02-07 20:42:32
cloud-provider-vsphere,andrewsykim,https://github.com/kubernetes/cloud-provider-vsphere/issues/137,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/137,Investigate using k8s.io/klog instead of glog,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
We should investigate using k8s.io/klog instead of glog. `klog` is a fork of `glog` so we should validate that there are no breaking changes in the way we use glog if we were to switch. ",closed,False,2019-02-05 20:35:34,2019-02-12 20:23:47
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/issues/138,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/138,Only Return/Report IPs Bound to a VMware vNIC,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
Consider using this method, which only returns IP address that have a corresponding vSphere virtual NIC device: https://github.com/vmware/govmomi/blob/e560c7bb1e709826a897c9da4baad0944c2cdda0/object/virtual_machine.go#L254-L259

Also see discussion in: [kubernetes/kubernetes#73721](https://github.com/kubernetes/kubernetes/pull/73721)

**What you expected to happen**:
Only IPs bound to a VMware vNIC will be reported.

**How to reproduce it (as minimally and precisely as possible)**:
 

**Anything else we need to know?**:
No

**Environment**:
Should be applied to all environments",closed,False,2019-02-06 14:53:32,2019-03-12 18:54:29
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/issues/139,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/139,csi: Check for min-VC and min-Cluster support,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
The CSI driver currently only supports 6.5 and higher.

**What you expected to happen**:
Give a warning to the user if the VC or the cluster attached to a zone is pre-6.5 and produce an error. The number of configurations that can be created are infinite... so attempt to detect easy/simple unsupported configurations.

**How to reproduce it (as minimally and precisely as possible)**:
Use an older VC configuration (a negative test).

**Anything else we need to know?**:
Nope

**Environment**:
NA
",closed,False,2019-02-06 18:50:22,2019-02-13 15:30:22
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/140,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/140,Switch from using glog to klog,"**What this PR does / why we need it**:
The kubernetes community is moving away from glog to klog. This updates all components (CCM, CSI, CLI) used to klog.

**Which issue this PR fixes**:
https://github.com/kubernetes/cloud-provider-vsphere/issues/137

**Special notes for your reviewer**:
Tested CCM and CSI on:
- k8s 1.13.1 cluster with 10 worker nodes in 3 zones
- vSphere 6.7u1

**Release note**:
Internal package change
",closed,True,2019-02-12 15:32:03,2019-02-12 19:35:03
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/141,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/141,Check for min vCenter version,"**What this PR does / why we need it**:
This checks the minimum vCenter API version (which is 6.5) on the CSI FCD controller. If unsupported, the controller will print an error message and will fail to start the controller service.

I had hopes that we could check clusters within DCs, but quickly realized that there could be clusters that are being used to manage non-Kubernetes earmarked clusters that can be of an older ESX version. Only check that can be done without going through a bunch of hoops is checking the vCenter API version. Docs already outline the support matrix for the CSI driver.

**Which issue this PR fixes**:
https://github.com/kubernetes/cloud-provider-vsphere/issues/139

**Special notes for your reviewer**:
Added go tests to check the boundary cases.

Tested CCM and CSI on:
- k8s 1.13.1 cluster with 10 worker nodes in 3 zones
- vSphere 6.7u1

**Release note**:
NA",closed,True,2019-02-12 20:27:50,2019-02-14 15:26:06
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/issues/142,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/142,CCM >v0.1.0 fails on Photon2 OS,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
Running CCM:latest on Photon2 OS results in a panic.

**What you expected to happen**:
No panic.

**How to reproduce it (as minimally and precisely as possible)**:
Use CCM:latest on Photon2 OS.

**Anything else we need to know?**:
When discovering node information, the CCM on Photon2 OS panics:

```shell
2019-02-14T00:28:28.77211952Z stderr F I0214 00:28:28.772000       1 flags.go:27] FLAG: --address=""127.0.0.1""
2019-02-14T00:28:28.772228035Z stderr F I0214 00:28:28.772189       1 flags.go:27] FLAG: --allocate-node-cidrs=""false""
2019-02-14T00:28:28.772298274Z stderr F I0214 00:28:28.772266       1 flags.go:27] FLAG: --allow-untagged-cloud=""false""
2019-02-14T00:28:28.77236737Z stderr F I0214 00:28:28.772325       1 flags.go:27] FLAG: --alsologtostderr=""false""
2019-02-14T00:28:28.772413658Z stderr F I0214 00:28:28.772389       1 flags.go:27] FLAG: --bind-address=""0.0.0.0""
2019-02-14T00:28:28.772476189Z stderr F I0214 00:28:28.772444       1 flags.go:27] FLAG: --cert-dir=""/var/run/kubernetes""
2019-02-14T00:28:28.772532703Z stderr F I0214 00:28:28.772497       1 flags.go:27] FLAG: --cidr-allocator-type=""RangeAllocator""
2019-02-14T00:28:28.772613304Z stderr F I0214 00:28:28.772579       1 flags.go:27] FLAG: --cloud-config=""/etc/cloud/cloud-provider.conf""
2019-02-14T00:28:28.772668646Z stderr F I0214 00:28:28.772645       1 flags.go:27] FLAG: --cloud-provider=""vsphere""
2019-02-14T00:28:28.772731503Z stderr F I0214 00:28:28.772700       1 flags.go:27] FLAG: --cluster-cidr=""""
2019-02-14T00:28:28.772775354Z stderr F I0214 00:28:28.772752       1 flags.go:27] FLAG: --cluster-name=""kubernetes""
2019-02-14T00:28:28.772841335Z stderr F I0214 00:28:28.772808       1 flags.go:27] FLAG: --concurrent-service-syncs=""1""
2019-02-14T00:28:28.772885695Z stderr F I0214 00:28:28.772862       1 flags.go:27] FLAG: --configure-cloud-routes=""true""
2019-02-14T00:28:28.772948713Z stderr F I0214 00:28:28.772918       1 flags.go:27] FLAG: --contention-profiling=""false""
2019-02-14T00:28:28.772994519Z stderr F I0214 00:28:28.772969       1 flags.go:27] FLAG: --controller-start-interval=""0s""
2019-02-14T00:28:28.77306038Z stderr F I0214 00:28:28.773026       1 flags.go:27] FLAG: --feature-gates=""""
2019-02-14T00:28:28.773115022Z stderr F I0214 00:28:28.773081       1 flags.go:27] FLAG: --help=""false""
2019-02-14T00:28:28.773178693Z stderr F I0214 00:28:28.773146       1 flags.go:27] FLAG: --http2-max-streams-per-connection=""0""
2019-02-14T00:28:28.773222896Z stderr F I0214 00:28:28.773199       1 flags.go:27] FLAG: --kube-api-burst=""30""
2019-02-14T00:28:28.773286761Z stderr F I0214 00:28:28.773255       1 flags.go:27] FLAG: --kube-api-content-type=""application/vnd.kubernetes.protobuf""
2019-02-14T00:28:28.773334447Z stderr F I0214 00:28:28.773308       1 flags.go:27] FLAG: --kube-api-qps=""20""
2019-02-14T00:28:28.773395451Z stderr F I0214 00:28:28.773364       1 flags.go:27] FLAG: --kubeconfig=""/etc/cloud/kubeconfig""
2019-02-14T00:28:28.773439326Z stderr F I0214 00:28:28.773416       1 flags.go:27] FLAG: --leader-elect=""false""
2019-02-14T00:28:28.773503286Z stderr F I0214 00:28:28.773472       1 flags.go:27] FLAG: --leader-elect-lease-duration=""15s""
2019-02-14T00:28:28.773547767Z stderr F I0214 00:28:28.773524       1 flags.go:27] FLAG: --leader-elect-renew-deadline=""10s""
2019-02-14T00:28:28.773611681Z stderr F I0214 00:28:28.773581       1 flags.go:27] FLAG: --leader-elect-resource-lock=""endpoints""
2019-02-14T00:28:28.773655454Z stderr F I0214 00:28:28.773632       1 flags.go:27] FLAG: --leader-elect-retry-period=""2s""
2019-02-14T00:28:28.773720892Z stderr F I0214 00:28:28.773688       1 flags.go:27] FLAG: --log-backtrace-at="":0""
2019-02-14T00:28:28.773829952Z stderr F I0214 00:28:28.773775       1 flags.go:27] FLAG: --log-dir=""""
2019-02-14T00:28:28.773876082Z stderr F I0214 00:28:28.773852       1 flags.go:27] FLAG: --log-flush-frequency=""5s""
2019-02-14T00:28:28.773938182Z stderr F I0214 00:28:28.773908       1 flags.go:27] FLAG: --logtostderr=""true""
2019-02-14T00:28:28.773982401Z stderr F I0214 00:28:28.773959       1 flags.go:27] FLAG: --master=""""
2019-02-14T00:28:28.774063766Z stderr F I0214 00:28:28.774017       1 flags.go:27] FLAG: --min-resync-period=""12h0m0s""
2019-02-14T00:28:28.774113027Z stderr F I0214 00:28:28.774084       1 flags.go:27] FLAG: --node-monitor-period=""5s""
2019-02-14T00:28:28.774165879Z stderr F I0214 00:28:28.774136       1 flags.go:27] FLAG: --node-status-update-frequency=""5m0s""
2019-02-14T00:28:28.774218087Z stderr F I0214 00:28:28.774194       1 flags.go:27] FLAG: --node-sync-period=""0s""
2019-02-14T00:28:28.774277964Z stderr F I0214 00:28:28.774247       1 flags.go:27] FLAG: --port=""10253""
2019-02-14T00:28:28.774320546Z stderr F I0214 00:28:28.774298       1 flags.go:27] FLAG: --profiling=""false""
2019-02-14T00:28:28.774382383Z stderr F I0214 00:28:28.774352       1 flags.go:27] FLAG: --route-reconciliation-period=""10s""
2019-02-14T00:28:28.77442518Z stderr F I0214 00:28:28.774402       1 flags.go:27] FLAG: --secure-port=""0""
2019-02-14T00:28:28.774501596Z stderr F I0214 00:28:28.774470       1 flags.go:27] FLAG: --stderrthreshold=""2""
2019-02-14T00:28:28.774545089Z stderr F I0214 00:28:28.774522       1 flags.go:27] FLAG: --tls-cert-file=""""
2019-02-14T00:28:28.774613902Z stderr F I0214 00:28:28.774577       1 flags.go:27] FLAG: --tls-cipher-suites=""[]""
2019-02-14T00:28:28.77465987Z stderr F I0214 00:28:28.774636       1 flags.go:27] FLAG: --tls-min-version=""""
2019-02-14T00:28:28.774750594Z stderr F I0214 00:28:28.774691       1 flags.go:27] FLAG: --tls-private-key-file=""""
2019-02-14T00:28:28.774792968Z stderr F I0214 00:28:28.774769       1 flags.go:27] FLAG: --tls-sni-cert-key=""[]""
2019-02-14T00:28:28.774852869Z stderr F I0214 00:28:28.774826       1 flags.go:27] FLAG: --use-service-account-credentials=""true""
2019-02-14T00:28:28.774911876Z stderr F I0214 00:28:28.774869       1 flags.go:27] FLAG: --v=""2""
2019-02-14T00:28:28.774957351Z stderr F I0214 00:28:28.774929       1 flags.go:27] FLAG: --version=""false""
2019-02-14T00:28:28.775022704Z stderr F I0214 00:28:28.774988       1 flags.go:27] FLAG: --vmodule=""""
2019-02-14T00:28:28.778289143Z stderr F W0214 00:28:28.778237       1 authentication.go:55] Authentication is disabled
2019-02-14T00:28:28.778384283Z stderr F I0214 00:28:28.778316       1 insecure_serving.go:49] Serving insecurely on 127.0.0.1:10253
2019-02-14T00:28:28.83044425Z stderr F I0214 00:28:28.830086       1 node_controller.go:89] Sending events to api server.
2019-02-14T00:28:28.840868055Z stderr F I0214 00:28:28.840537       1 pvlcontroller.go:107] Starting PersistentVolumeLabelController
2019-02-14T00:28:28.840914136Z stderr F I0214 00:28:28.840549       1 controller_utils.go:1025] Waiting for caches to sync for persistent volume label controller
2019-02-14T00:28:28.85045527Z stderr F E0214 00:28:28.850239       1 controllermanager.go:240] Failed to start service controller: the cloud provider does not support external load balancers
2019-02-14T00:28:28.850484857Z stderr F I0214 00:28:28.850259       1 controllermanager.go:264] Will not configure cloud provider routes for allocate-node-cidrs: false, configure-cloud-routes: true.
2019-02-14T00:28:28.854729651Z stderr F I0214 00:28:28.854662       1 node_controller.go:329] This node c01.988ba0c.sk8 is registered without the cloud taint. Will not process.
2019-02-14T00:28:28.940747455Z stderr F I0214 00:28:28.940697       1 controller_utils.go:1032] Caches are synced for persistent volume label controller
2019-02-14T00:28:31.715090811Z stderr F I0214 00:28:31.714993       1 node_controller.go:329] This node c01.988ba0c.sk8 is registered without the cloud taint. Will not process.
2019-02-14T00:28:32.915922485Z stderr F E0214 00:28:32.914877       1 connection.go:63] Failed to create govmomi client. err: ServerFaultCode: Cannot complete login due to an incorrect user name or password.
2019-02-14T00:28:33.045949609Z stderr F E0214 00:28:33.045675       1 datacenter.go:107] Unable to find VM by UUID. VM UUID: bd013042-b08d-992f-af31-5c15b5c37e1e
2019-02-14T00:28:33.045969735Z stderr F E0214 00:28:33.045688       1 search.go:170] Error while looking for vm=bd013042-b08d-992f-af31-5c15b5c37e1e(byUUID) in vc=vcenter.sddc-54-70-161-229.vmc.vmware.com and datacenter=SDDC-Datacenter: No VM found
2019-02-14T00:28:33.04597453Z stderr F E0214 00:28:33.045704       1 nodemanager.go:110] WhichVCandDCByNodeId failed. Err: No VM found
2019-02-14T00:28:33.04599851Z stderr F E0214 00:28:33.045845       1 runtime.go:66] Observed a panic: ""invalid memory address or nil pointer dereference"" (runtime error: invalid memory address or nil pointer dereference)
2019-02-14T00:28:33.046010315Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:72
2019-02-14T00:28:33.046014808Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:65
2019-02-14T00:28:33.046018915Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:51
2019-02-14T00:28:33.046022523Z stderr F /home/travis/.gimme/versions/go1.11.linux.amd64/src/runtime/asm_amd64.s:522
2019-02-14T00:28:33.046025738Z stderr F /home/travis/.gimme/versions/go1.11.linux.amd64/src/runtime/panic.go:513
2019-02-14T00:28:33.046028805Z stderr F /home/travis/.gimme/versions/go1.11.linux.amd64/src/runtime/panic.go:82
2019-02-14T00:28:33.046031631Z stderr F /home/travis/.gimme/versions/go1.11.linux.amd64/src/runtime/signal_unix.go:390
2019-02-14T00:28:33.046035174Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere/nodemanager.go:114
2019-02-14T00:28:33.046038409Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere/nodemanager.go:69
2019-02-14T00:28:33.046041743Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere/cloud.go:158
2019-02-14T00:28:33.046045236Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere/cloud.go:73
2019-02-14T00:28:33.046048002Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache/controller.go:195
2019-02-14T00:28:33.046053755Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache/shared_informer.go:554
2019-02-14T00:28:33.04605665Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:203
2019-02-14T00:28:33.04605956Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:203
2019-02-14T00:28:33.046070896Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache/shared_informer.go:548
2019-02-14T00:28:33.046074413Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133
2019-02-14T00:28:33.046077578Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134
2019-02-14T00:28:33.046080513Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88
2019-02-14T00:28:33.046083291Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache/shared_informer.go:546
2019-02-14T00:28:33.046086438Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache/shared_informer.go:390
2019-02-14T00:28:33.046089239Z stderr F /home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:71
2019-02-14T00:28:33.046092753Z stderr F /home/travis/.gimme/versions/go1.11.linux.amd64/src/runtime/asm_amd64.s:1333
2019-02-14T00:28:33.049215968Z stderr F panic: runtime error: invalid memory address or nil pointer dereference [recovered]
2019-02-14T00:28:33.049239569Z stderr F 	panic: runtime error: invalid memory address or nil pointer dereference
2019-02-14T00:28:33.049243349Z stderr F [signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x1839d3b]
2019-02-14T00:28:33.049246251Z stderr F 
2019-02-14T00:28:33.049249676Z stderr F goroutine 76 [running]:
2019-02-14T00:28:33.049252851Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)
2019-02-14T00:28:33.049259035Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:58 +0x108
2019-02-14T00:28:33.049266212Z stderr F panic(0x1b0f300, 0x37ab9d0)
2019-02-14T00:28:33.049269252Z stderr F 	/home/travis/.gimme/versions/go1.11.linux.amd64/src/runtime/panic.go:513 +0x1b9
2019-02-14T00:28:33.049282887Z stderr F k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere.(*NodeManager).DiscoverNode(0xc000115ea0, 0xc0004de6c0, 0x24, 0x0, 0x42e286, 0x1e1efa0)
2019-02-14T00:28:33.049286771Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere/nodemanager.go:114 +0x11b
2019-02-14T00:28:33.049289834Z stderr F k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere.(*NodeManager).RegisterNode(0xc000115ea0, 0xc000971080)
2019-02-14T00:28:33.049294573Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere/nodemanager.go:69 +0x137
2019-02-14T00:28:33.049297605Z stderr F k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere.(*VSphere).nodeAdded(0xc000500ff0, 0x1e99fe0, 0xc000971080)
2019-02-14T00:28:33.049300889Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere/cloud.go:158 +0xb2
2019-02-14T00:28:33.049304311Z stderr F k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere.(*VSphere).nodeAdded-fm(0x1e99fe0, 0xc000971080)
2019-02-14T00:28:33.049307197Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/pkg/cloudprovider/vsphere/cloud.go:73 +0x3e
2019-02-14T00:28:33.049310057Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnAdd(0xc00044bfc0, 0x0, 0xc00044bfd0, 0x1e99fe0, 0xc000971080)
2019-02-14T00:28:33.049313538Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache/controller.go:195 +0x49
2019-02-14T00:28:33.04931644Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache.(*processorListener).run.func1.1(0x42b3c3, 0xc000858dc8, 0xa0)
2019-02-14T00:28:33.049319879Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache/shared_informer.go:554 +0x21d
2019-02-14T00:28:33.04932285Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait.ExponentialBackoff(0x989680, 0x3ff0000000000000, 0x3fb999999999999a, 0x5, 0xc000b47e18, 0x42aed2, 0xc00048bd90)
2019-02-14T00:28:33.049325836Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:203 +0x9c
2019-02-14T00:28:33.049328468Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache.(*processorListener).run.func1()
2019-02-14T00:28:33.049331326Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache/shared_informer.go:548 +0x89
2019-02-14T00:28:33.049334028Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil.func1(0xc000858f68)
2019-02-14T00:28:33.049336756Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133 +0x54
2019-02-14T00:28:33.049339886Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc000b47f68, 0xdf8475800, 0x0, 0x1ab1001, 0xc000820540)
2019-02-14T00:28:33.04934325Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134 +0xbe
2019-02-14T00:28:33.049346033Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait.Until(0xc000858f68, 0xdf8475800, 0xc000820540)
2019-02-14T00:28:33.049348757Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88 +0x4d
2019-02-14T00:28:33.049359492Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache.(*processorListener).run(0xc0007f1600)
2019-02-14T00:28:33.049362318Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache/shared_informer.go:546 +0x8d
2019-02-14T00:28:33.049365211Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache.(*processorListener).run-fm()
2019-02-14T00:28:33.04937107Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/client-go/tools/cache/shared_informer.go:390 +0x2a
2019-02-14T00:28:33.049375914Z stderr F k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1(0xc0009a3720, 0xc0006ec9c0)
2019-02-14T00:28:33.049378766Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:71 +0x4f
2019-02-14T00:28:33.049381527Z stderr F created by k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait.(*Group).Start
2019-02-14T00:28:33.049384732Z stderr F 	/home/travis/gopath/src/k8s.io/cloud-provider-vsphere/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:69 +0x62
```

I think the issue may be caused by [lines 108-114 in `nodemanager.go`](https://github.com/kubernetes/cloud-provider-vsphere/blob/8f0ef6aca235605cb842a0d4247aa72a27bd8b39/pkg/cloudprovider/vsphere/nodemanager.go#L108-L114) not preventing `vmDI` from being accessed when it is presumably `nil`.

Apparently the following two patches created a merge conflict and the incorrect logic found its way into the final product:

1. Initial Implementation for CSI multi-VC/DC Support - https://github.com/kubernetes/cloud-provider-vsphere/commit/da8021e3116155db5a5323fefa90aa7f4cfa7a49
2. Switch from using glog to klog - https://github.com/kubernetes/cloud-provider-vsphere/commit/0f7fb7458970a02f711b79b6c01b67b47d1dbddb

In `nodemanager.go`, line 110, the error handling in the glog/klog patch doesnt prevent an error finding the VM from continuing with the function. However, the CSI/DC patch *does*.

The problem may still be a Photon-specific issue, but errors did not used to result in a panic. Now they do. It can be safely assumed that the inability to find a VM causes a secondary effort, which is in fact succeeding on Photon.

Ive noticed on Photon that `/sys/class/dmi/id/product_uuid` and `/sys/class/dmi/id/product_serial` are both the same endianness:

```shell
[0]root@c01:~$ cat /sys/class/dmi/id/product_uuid
423001BD-8DB0-2F99-AF31-5C15B5C37E1E

[0]root@c01:~$ cat /sys/class/dmi/id/product_serial
VMware-42 30 01 bd 8d b0 2f 99-af 31 5c 15 b5 c3 7e 1e
```

On CentOS and in Debian the files contain UUIDs where one is big endian and the other is little. In fact, theres code *in* the CCM to translate this. Apparently the result in the CCM on Photon is the UUID in the *correct* format being translated to the *incorrect* format.

Even on CCM:0.1.0 running on Photon2 OS, where the panic does not occur, the initial lookup by UUID fails for the exact same reason:

```shell
2019-02-13T23:27:05.669661653Z stderr F E0213 23:27:05.669474       1 datacenter.go:97] Unable to find VM by UUID. VM UUID: bd013042-b08d-992f-af31-5c15b5c37e1e
2019-02-13T23:27:05.669677637Z stderr F E0213 23:27:05.669485       1 nodemanager.go:241] Error while looking for vm=bd013042-b08d-992f-af31-5c15b5c37e1e(byUUID) in vc=vcenter.sddc-54-70-161-229.vmc.vmware.com and datacenter=SDDC-Datacenter: No VM found
2019-02-13T23:27:05.669688376Z stderr F I0213 23:27:05.669495       1 nodemanager.go:246] Did not find node bd013042-b08d-992f-af31-5c15b5c37e1e in vc=vcenter.sddc-54-70-161-229.vmc.vmware.com and datacenter=SDDC-Datacenter
2019-02-13T23:27:05.715118213Z stderr F I0213 23:27:05.715037       1 nodemanager.go:280] Found node c01.988ba0c.sk8 as vm=VirtualMachine:vm-16044 in vc=vcenter.sddc-54-70-161-229.vmc.vmware.com and datacenter=SDDC-Datacenter
2019-02-13T23:27:05.715204209Z stderr F I0213 23:27:05.715151       1 nodemanager.go:282] Hostname: c01.988ba0c.sk8 UUID: 423001bd-8db0-2f99-af31-5c15b5c37e1e
2019-02-13T23:27:05.71527719Z stderr F I0213 23:27:05.715234       1 instances.go:114] instances.InstanceID() FOUND with c01.988ba0c.sk8
2019-02-13T23:27:05.715358423Z stderr F I0213 23:27:05.715307       1 instances.go:79] instances.NodeAddressesByProviderID() CACHED with 423001bd-8db0-2f99-af31-5c15b5c37e1e
```

The only difference is that in this case no panic occurs and thus the VM is resolved by name.

I think its pretty clear what the problem is. I also think the bug introduced recently only reveals an underlying problem with Photon2 OS -- the two files should not be the same endianness. At least thats not how both CentOS and Debian behave. For example, here is CentOS:

```shell
[0]root@c01-ccm-ci-latest-adc83b1-86:~$ cat /etc/os-release 
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

[0]root@c01-ccm-ci-latest-adc83b1-86:~$ uname -a
Linux c01-ccm-ci-latest-adc83b1-86.vmware.ci 3.10.0-862.11.6.el7.x86_64 #1 SMP Tue Aug 14 21:49:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

[0]root@c01-ccm-ci-latest-adc83b1-86:~$ cat /sys/class/dmi/id/product_serial 
VMware-42 30 97 bd 95 b0 be 1e-22 d2 fc 82 c4 46 9a 90

[0]root@c01-ccm-ci-latest-adc83b1-86:~$ cat /sys/class/dmi/id/product_uuid 
BD973042-B095-1EBE-22D2-FC82C4469A90
```

While `42 30 97 bd 95 b0 be 1e-22 d2 fc 82 c4 46 9a 90` and `BD973042-B095-1EBE-22D2-FC82C4469A90` may look different, they're the same value formatted differently. This is what the CCM expects. Because the CCM code expects this, on Photon2 OS the CCM is unknowingly converting a *valid* UUID into an invalid one.

**Environment**:
- vsphere-cloud-controller-manager version: 
```
gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager@sha256:deba22958d2d0556b726ee4b4fb75c0d708f0d07b004f705fef7e2262af4974b
```
- OS (e.g. from /etc/os-release):
```shell
NAME=""VMware Photon OS""
VERSION=""2.0""
ID=photon
VERSION_ID=2.0
PRETTY_NAME=""VMware Photon OS/Linux""
ANSI_COLOR=""1;34""
HOME_URL=""https://vmware.github.io/photon/""
BUG_REPORT_URL=""https://github.com/vmware/photon/issues""
```
- Kernel (e.g. `uname -a`):
```shell
Linux c01.988ba0c.sk8 4.9.140-3.ph2-esx #1-photon SMP Sat Jan 19 14:32:14 UTC 2019 x86_64 Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz GenuineIntel GNU/Linux
```

- Install tools:
sk8

- Others:",closed,False,2019-02-13 23:59:02,2019-02-22 17:33:52
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/143,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/143,Bug fix for NodeManager and Photon 2 UUID,"**What this PR does / why we need it**:
This fixes:
- a crash in the NodeManager when the VM is not found via UUID introduced accidentally with this PR https://github.com/kubernetes/cloud-provider-vsphere/pull/136 Missing a return statement: https://github.com/kubernetes/cloud-provider-vsphere/pull/143/files#diff-b7f52468613fc2d2fc4b257bed84a46cR138
- a bug with the way Photon 2 reports the UUID which is then passed from the kubelet to the CCM. It does not conform to the way the RHEL, Ubuntu, and etc reports it. The reporting format of the UUID has been fixed in Photon 3 though.

**Which issue this PR fixes**:
https://github.com/kubernetes/cloud-provider-vsphere/issues/142

**Special notes for your reviewer**:
Added go tests to check UUID revert.

Tested CCM and CSI on:
- k8s 1.13.1 cluster with 10 worker nodes in 3 zones
- vSphere 6.7u1
- OS: RHEL

**Release note**:
NA",closed,True,2019-02-14 19:32:16,2019-02-14 21:43:19
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/144,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/144,Project Health improvements for 2019 H1,"This issue tracks improvements to the health of the vSphere Cloud Provider / CSI project. These improvements are paramount to the success of this project and to foster a healthy community of contributors.

Areas of improvements for 2019 H1:

* [ ]  Perform bi-weekly issue grooming instead of monthly.
* [ ]  Create a Developer guide to enable new contributors.
* [ ]  Work to get more PRs and participation from parties outside of VMware.
* [ ]  Make this repo the go-to choice for new Kubernetes users on vSphere.
* [ ]  Add a more complete documentation to adhere to [Cloud Provider documentation KEP](https://github.com/kubernetes/enhancements/pull/827)
* [ ]  Create a quickstart docs in the repo
* [ ]  Continue working on an easy-button deployment model for users.
* [ ]  Improve unit tests and integration tests, especially between CCM and CSI.
* [ ]  Continue to actively participate in SIG-Cloud Provider
* [ ]  Improve release model and automate the release process
* [ ]  Work on migrating the CSI project into a dedicate repo for better release process/automation and issue tracking
",open,False,2019-02-15 17:59:28,2019-02-15 18:02:50
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/145,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/145,Clarification on CCM and CSI Documentation,"**What this PR does / why we need it**:
This attempts to clarify what setup is required for both the CCM and CSI in order for those components to function properly.

**Which issue this PR fixes**:
NA

**Special notes for your reviewer**:
NA

**Release note**:
NA",closed,True,2019-02-19 17:33:10,2019-02-19 17:42:49
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/issues/146,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/146,Provide documentation on how to run CCM and CSI with Kubeadm,"As `kubeadm` is the standard way to deploy Kubernetes on operating systems, we should provide instructions on how to deploy CCM and CSI when using it.

/kind feature
",open,False,2019-02-19 17:40:48,2019-03-06 18:57:20
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/147,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/147,Part 1: Add CSI go tests,"**What this PR does / why we need it**:
We need more CSI tests! Tests added:
- Complete controller go test (Create, List, Publish, Unpublish, Delete)
- Test ListVolumes boundary conditions mainly around pagination
- Test ListVoumes order
- Test for Zones support using 2 Datacenters

Additional changes found while added go tests:
- updated govmomi to latest to fix a bug in device Key/UnitNumber in the older version
- Removed some klog.V(X) to always display since they are very useful debug statements to have all the time
- Changed some klog.V(X) values as some weren't needed as often and others were
- Found a bug in GetAllDatastoreClusters in which returns an error when no DatastoreClusters exist causing the entire ListVolumes to fail
- Changed ListVolumes to be on a 0-based index instead of 1-based. This is used for the NextToken when doing pagination of volumes. 1-based was too confusing.

**Which issue this PR fixes**:
NA

**Special notes for your reviewer**:
Tested CCM and CSI on:
k8s 1.13.2 cluster with 10 worker nodes in 3 zones
vSphere 6.7u1
Deployed a test pod which targeted a particular zone using a persistent volume
Used a multiple vCenters (2 to be exact) with multiple Datacenters (3 to be exact)
`make test` passes

**Release note**:
NA
",closed,True,2019-02-22 22:58:38,2019-02-25 20:06:47
cloud-provider-vsphere,dougm,https://github.com/kubernetes/cloud-provider-vsphere/pull/148,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/148,Use vcsim SearchIndex in nodemanager test,"When this test was written, vcsim did not have a FindByDnsName implementation.
Now that it does, there's no need to add our own FindByDnsName method, just
need to set the vm guest.hostName property to the value used in the call to
vclib's GetVMByDNSName().

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

/kind cleanup

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-02-25 17:53:15,2019-02-25 19:07:22
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/149,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/149,Part 2: More go tests,"**What this PR does / why we need it**:
Final PR for go testing. Tests added:
- Now testing the instances interface
- Now testing ConnectionManager... testing all the ways we look up or find things. Meaning we test list, search, and zone-based searches. New `_test.go` file for each category. These are used everywhere in both CCM and CSI.
- configFromEnvOrSim now supported multi-DC environment
- NodeManager now testing using multi-DC
- Improved the ListVolumes test for ordering
- Add ProviderID tests

Refactored:
- NodeManager and CCM Zones go tests don't need to initialize using newVSphere(). Was overkill and did a bunch of mostly unnecessary stuff.

**Which issue this PR fixes**:
NA

**Special notes for your reviewer**:
Since all changes were to `go tests` code only. Just made sure that `make test` passed as excepted.

**Release note**:
NA
",closed,True,2019-02-27 16:06:01,2019-02-27 22:35:26
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/150,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/150,Fix broken cron builds that rely on fork=true,"**What this PR does / why we need it**:
This patch fixes an issue where Travis-CI's conditional ""fork"" no longer is set to ""true"" when a build is triggered by cron.

Many thanks to @imkin and @figo for helping to discover this issue.

The underlying cause appears to be related to https://github.com/bioconda/bioconda-recipes/issues/6809. However, the original configuration was working past the point that the linked problem was discovered. It appears to be a regression in Travis-CI. An issue will be opened with their support team to alert them to the problem.

**Which issue this PR fixes**:
NA

**Special notes for your reviewer**:
NA

**Release note**:
NA",closed,True,2019-03-02 23:19:52,2019-03-04 18:16:22
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/151,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/151,e2e: Sonobuoy,"**What this PR does / why we need it**:
This patch updates the e2e conformance tests to use sk8's Sonobuoy refactor. Before merging the PR, the following environment variables need to be configured in the project's Travis-CI settings:

| Name | Value |
|-------|------|
| `SK8_URL` | `https://raw.githubusercontent.com/vmware/simple-k8s-test-env/v0.2.0/sk8.sh` |
| `E2E_IMAGE` | `gcr.io/kubernetes-conformance-testing/sk8e2e:v20190304-v0.2.0` |
| `KUBE_CONFORMANCE_IMAGE` | `akutz/kube-conformance:v1.13.4` |

Related to:
1. https://github.com/vmware/simple-k8s-test-env/issues/9
2. https://github.com/vmware/simple-k8s-test-env/issues/10
3. https://github.com/vmware/simple-k8s-test-env/issues/11

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: Hi there. How are you today?

**Release note**: NA",closed,True,2019-03-04 20:46:07,2019-03-04 21:06:15
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/issues/152,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/152,Please update Travis-CI env vars / fixes Aggregation layer e2e conformance test,"Hi @frapposelli,

Please update the following Travis-CI environment variables:

| Name | Value |
|-------|------|
| `SK8_URL` | `https://raw.githubusercontent.com/vmware/simple-k8s-test-env/v0.2.1/sk8.sh` |
| `E2E_IMAGE` | `gcr.io/kubernetes-conformance-testing/sk8e2e:v20190304-v0.2.1-1-g6d7e0f0` |

The new script and image support the Kubernetes aggregation layer, thereby fixing the broken e2e conformance test `[sig-api-machinery] Aggregator [It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]`.

Related to https://github.com/vmware/simple-k8s-test-env/pull/10 and https://github.com/vmware/simple-k8s-test-env/pull/12.

cc @figo @imkin ",closed,False,2019-03-05 01:55:05,2019-03-05 07:49:08
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/153,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/153,CSI ginkgo,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This patch adds Ginkgo tests for the CSI Plugin that can be exercised
when the plugin has not been configured (meaning, no vCenter
interaction).

These tests start an actual CSI plugin and communicate with it over
gRPC. The connection is in-memory thanks to the memconn package,
which makes testing simpler and faster.

The reference to ""-tags unit"" has been removed from the Makefile's
""unit"" target, as no Go source files had a unit build constraint in
them. This call was ineffective.

In a separate commit, the reference to glog is removed from Gopkg.toml. This is no longer a direct dependency, and does not belong in Gopkg.toml anymore.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
`make test` passes

This is actually a first step on the way to E2E tests. This covers what we can test on a ""real"" CSI plugin without configuring it and deploying it on vSphere infrastructure. Next step will be a set of tests that can be run on any (Linux) node that has access to vCenter and perform the appropriate storage functionality on a single node. After that will come consuming the K8s testing framework to test functionality within K8s for a full E2E test.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-03-07 23:34:48,2019-03-11 20:24:53
cloud-provider-vsphere,joelsmith,https://github.com/kubernetes/cloud-provider-vsphere/pull/154,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/154,Update embargo doc link in SECURITY_CONTACTS and change PST to PSC,See https://github.com/kubernetes/security/issues/8 for more information,closed,True,2019-03-08 17:52:40,2019-03-09 00:16:19
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/155,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/155,Add e2e integration tests using vcSim and Kind,"**What this PR does / why we need it**:
This patch adds e2e integration tests that load the CCM image into a Kubernetes cluster turned up with Kind and the vCenter simulator. Execute the tests with:

```shell
$ make -C test/integration
```

The tests depend on the following binaries locally: `kind`, `kubectl`, and `docker`. Kind may be installed with `go get -u sigs.k8s.io/kind`.

Many thanks to @BenTheElder, @andrewsykim, @munnerz, and @neolit123.

**TODO**:
* Migrate all the shell logic into an actual Ginkgo-based Go test using Kind's golang library
* Refactor the test and the project to use Bazel for caching
* Have better checks than printing the CCM log at the end
* Add a README to the root of the `test` directory and perhaps the `integration` directory as well with documentation on how everything works. For now this PR and the default targets of the Makefiles will serve as the documentation.
* This PR is loosely related to https://github.com/kubernetes/kubernetes/pull/75229. Please note this PR doesn't tell Kind to configure nodes with `--cloud-provider=external`. This is because Kind's CNI plug-in, Weave, won't start properly if the kubelet cannot report the node address. Therefore the cluster is turned up with no cloud provider configured and then the cloud provider taint is manually applied. Whether this solution is long-term is yet to be decided.

**Example Test Run**
```shell
$ make -C test/integration
make build-ccm-image
make[1]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
GOOS=linux make -C ../.. image-controller-manager
make[2]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere'
make[3]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere'
dep ensure -v && touch vendor
make[3]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere'
CGO_ENABLED=0 GOOS=linux go build \
	-ldflags ""-w -s -X 'main.version=a0a42c15'"" \
	-o vsphere-cloud-controller-manager \
	cmd/vsphere-cloud-controller-manager/main.go
cp vsphere-cloud-controller-manager cluster/images/controller-manager
docker build -t gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager:a0a42c15 cluster/images/controller-manager
Sending build context to Docker daemon  54.31MB
Step 1/3 : FROM photon:2.0
 ---> 37c525935c86
Step 2/3 : ADD vsphere-cloud-controller-manager /bin/
 ---> f9d6d95bd6dc
Step 3/3 : CMD [""/bin/vsphere-cloud-controller-manager""]
 ---> Running in 9f62eb9dccd9
Removing intermediate container 9f62eb9dccd9
 ---> 6a4cb71b8042
Successfully built 6a4cb71b8042
Successfully tagged gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager:a0a42c15
docker tag gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager:a0a42c15 gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager:latest
rm cluster/images/controller-manager/vsphere-cloud-controller-manager
make[2]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere'
make[1]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
make cluster-up
make[1]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
kind create cluster --config ""kind-config.yaml"" --name ""ccm-integration-test""
Creating cluster ""ccm-integration-test"" ...
  Ensuring node image (kindest/node:v1.13.3) 
  [control-plane] Creating node container  
  [control-plane] Fixing mounts  
  [control-plane] Configuring proxy 
  [control-plane] Starting systemd  
  [control-plane] Waiting for docker to be ready  
  [control-plane] Pre-loading images  
  [worker] Creating node container  
  [worker] Fixing mounts  
  [worker] Configuring proxy 
  [worker] Starting systemd  
  [worker] Waiting for docker to be ready  
  [worker] Pre-loading images  
  [control-plane] Creating the kubeadm config file  
  [control-plane] Starting Kubernetes (this may take a minute)  
  [worker] Joining worker node to Kubernetes  
Cluster creation complete. You can now use the cluster with:

export KUBECONFIG=""$(kind get kubeconfig-path --name=""ccm-integration-test"")""
kubectl cluster-info
make[1]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
make load-ccm-image
make[1]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
kind load docker-image --name ""ccm-integration-test"" gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager:a0a42c15
make[1]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
make deploy-vcsim
make[1]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
export KUBECONFIG=""$(kind get kubeconfig-path --name ""ccm-integration-test"")"" && \
kubectl -n kube-system apply -f ../vcsim/deployment.yaml
serviceaccount/vcsim created
statefulset.apps/vcsim created
service/vcsim created
make[1]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
make create-vms
make[1]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
./create-vms.sh ""ccm-integration-test""
waiting for vcsim.......................................ok
creating vcsim vm:
  name=ccm-integration-test-control-plane
  fqdn=ccm-integration-test-control-plane
  ipv4=172.17.0.2
   mac=02:42:ac:11:00:02
  uuid=04f419fe-8688-4066-a16d-cbc4193d5b93
  suid=fe19f404-8886-6640-a16d-cbc4193d5b93
creating vcsim vm:
  name=ccm-integration-test-worker
  fqdn=ccm-integration-test-worker
  ipv4=172.17.0.3
   mac=02:42:ac:11:00:03
  uuid=04f419fe-8688-4066-a16d-cbc4193d5b94
  suid=fe19f404-8886-6640-a16d-cbc4193d5b94
make[1]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
make deploy-ccm
make[1]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
export KUBECONFIG=""$(kind get kubeconfig-path --name ""ccm-integration-test"")"" && \
kubectl -n kube-system create configmap cloud-config --from-file=vsphere.conf && \
kubectl -n kube-system create -f secrets.yaml && \
kubectl -n kube-system apply -f ../../manifests/controller-manager/cloud-controller-manager-roles.yaml && \
kubectl -n kube-system apply -f ../../manifests/controller-manager/cloud-controller-manager-role-bindings.yaml && \
sed 's~gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager:latest~gcr.io/cloud-provider-vsphere/vsphere-cloud-controller-manager:a0a42c15~g' <../../manifests/controller-manager/vsphere-cloud-controller-manager-pod.yaml | kubectl -n kube-system apply -f -
configmap/cloud-config created
secret/vccm created
clusterrole.rbac.authorization.k8s.io/system:cloud-controller-manager created
clusterrolebinding.rbac.authorization.k8s.io/system:cloud-controller-manager created
serviceaccount/cloud-controller-manager created
pod/vsphere-cloud-controller-manager created
service/vsphere-cloud-controller-manager created
make[1]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
make taint-nodes
make[1]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
export KUBECONFIG=""$(kind get kubeconfig-path --name ""ccm-integration-test"")"" && \
for name in $(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'); do \
  kubectl taint nodes ""${name}"" node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule; \
done
node/ccm-integration-test-control-plane tainted
node/ccm-integration-test-worker tainted
make[1]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
make log-ccm
make[1]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
export KUBECONFIG=$(kind get kubeconfig-path --name ""ccm-integration-test"") && \
while ! kubectl -n kube-system logs vsphere-cloud-controller-manager >/dev/null 2>&1; do sleep 1; done && \
sleep 3 && \
kubectl -n kube-system logs vsphere-cloud-controller-manager
I0309 00:48:36.584904       1 flags.go:27] FLAG: --address=""0.0.0.0""
I0309 00:48:36.584988       1 flags.go:27] FLAG: --allocate-node-cidrs=""false""
I0309 00:48:36.585010       1 flags.go:27] FLAG: --allow-untagged-cloud=""false""
I0309 00:48:36.585031       1 flags.go:27] FLAG: --alsologtostderr=""false""
I0309 00:48:36.585053       1 flags.go:27] FLAG: --bind-address=""0.0.0.0""
I0309 00:48:36.585095       1 flags.go:27] FLAG: --cert-dir=""/var/run/kubernetes""
I0309 00:48:36.585110       1 flags.go:27] FLAG: --cidr-allocator-type=""RangeAllocator""
I0309 00:48:36.585161       1 flags.go:27] FLAG: --cloud-config=""/etc/cloud/vsphere.conf""
I0309 00:48:36.585182       1 flags.go:27] FLAG: --cloud-provider=""vsphere""
I0309 00:48:36.585202       1 flags.go:27] FLAG: --cluster-cidr=""""
I0309 00:48:36.585223       1 flags.go:27] FLAG: --cluster-name=""kubernetes""
I0309 00:48:36.585243       1 flags.go:27] FLAG: --concurrent-service-syncs=""1""
I0309 00:48:36.585265       1 flags.go:27] FLAG: --configure-cloud-routes=""true""
I0309 00:48:36.585281       1 flags.go:27] FLAG: --contention-profiling=""false""
I0309 00:48:36.585301       1 flags.go:27] FLAG: --controller-start-interval=""0s""
I0309 00:48:36.585324       1 flags.go:27] FLAG: --feature-gates=""""
I0309 00:48:36.585357       1 flags.go:27] FLAG: --help=""false""
I0309 00:48:36.585382       1 flags.go:27] FLAG: --http2-max-streams-per-connection=""0""
I0309 00:48:36.585403       1 flags.go:27] FLAG: --kube-api-burst=""30""
I0309 00:48:36.585424       1 flags.go:27] FLAG: --kube-api-content-type=""application/vnd.kubernetes.protobuf""
I0309 00:48:36.585635       1 flags.go:27] FLAG: --kube-api-qps=""20""
I0309 00:48:36.585669       1 flags.go:27] FLAG: --kubeconfig=""""
I0309 00:48:36.585723       1 flags.go:27] FLAG: --leader-elect=""true""
I0309 00:48:36.585755       1 flags.go:27] FLAG: --leader-elect-lease-duration=""15s""
I0309 00:48:36.585830       1 flags.go:27] FLAG: --leader-elect-renew-deadline=""10s""
I0309 00:48:36.585852       1 flags.go:27] FLAG: --leader-elect-resource-lock=""endpoints""
I0309 00:48:36.585907       1 flags.go:27] FLAG: --leader-elect-retry-period=""2s""
I0309 00:48:36.585945       1 flags.go:27] FLAG: --log-backtrace-at="":0""
I0309 00:48:36.585968       1 flags.go:27] FLAG: --log-dir=""""
I0309 00:48:36.585984       1 flags.go:27] FLAG: --log-flush-frequency=""5s""
I0309 00:48:36.586005       1 flags.go:27] FLAG: --logtostderr=""true""
I0309 00:48:36.586020       1 flags.go:27] FLAG: --master=""""
I0309 00:48:36.586047       1 flags.go:27] FLAG: --min-resync-period=""12h0m0s""
I0309 00:48:36.586065       1 flags.go:27] FLAG: --node-monitor-period=""5s""
I0309 00:48:36.586115       1 flags.go:27] FLAG: --node-status-update-frequency=""5m0s""
I0309 00:48:36.586137       1 flags.go:27] FLAG: --node-sync-period=""0s""
I0309 00:48:36.586151       1 flags.go:27] FLAG: --port=""10253""
I0309 00:48:36.586173       1 flags.go:27] FLAG: --profiling=""false""
I0309 00:48:36.586201       1 flags.go:27] FLAG: --route-reconciliation-period=""10s""
I0309 00:48:36.586222       1 flags.go:27] FLAG: --secure-port=""0""
I0309 00:48:36.586264       1 flags.go:27] FLAG: --stderrthreshold=""2""
I0309 00:48:36.586279       1 flags.go:27] FLAG: --tls-cert-file=""""
I0309 00:48:36.586293       1 flags.go:27] FLAG: --tls-cipher-suites=""[]""
I0309 00:48:36.586324       1 flags.go:27] FLAG: --tls-min-version=""""
I0309 00:48:36.586342       1 flags.go:27] FLAG: --tls-private-key-file=""""
I0309 00:48:36.586360       1 flags.go:27] FLAG: --tls-sni-cert-key=""[]""
I0309 00:48:36.586383       1 flags.go:27] FLAG: --use-service-account-credentials=""false""
I0309 00:48:36.586432       1 flags.go:27] FLAG: --v=""2""
I0309 00:48:36.586462       1 flags.go:27] FLAG: --version=""false""
I0309 00:48:36.586543       1 flags.go:27] FLAG: --vmodule=""""
W0309 00:48:36.586816       1 client_config.go:552] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
W0309 00:48:36.589631       1 authentication.go:55] Authentication is disabled
I0309 00:48:36.589684       1 insecure_serving.go:49] Serving insecurely on [::]:10253
I0309 00:48:36.594197       1 leaderelection.go:185] attempting to acquire leader lease  kube-system/cloud-controller-manager...
I0309 00:48:36.618196       1 leaderelection.go:194] successfully acquired lease kube-system/cloud-controller-manager
I0309 00:48:36.619512       1 event.go:221] Event(v1.ObjectReference{Kind:""Endpoints"", Namespace:""kube-system"", Name:""cloud-controller-manager"", UID:""129e493d-4205-11e9-9daa-02427a3787a5"", APIVersion:""v1"", ResourceVersion:""626"", FieldPath:""""}): type: 'Normal' reason: 'LeaderElection' ccm-integration-test-worker_129b3356-4205-11e9-b0a1-0242b996fa24 became leader
I0309 00:48:36.621428       1 node_controller.go:89] Sending events to api server.
I0309 00:48:36.622495       1 pvlcontroller.go:107] Starting PersistentVolumeLabelController
I0309 00:48:36.622522       1 controller_utils.go:1025] Waiting for caches to sync for persistent volume label controller
E0309 00:48:36.623170       1 controllermanager.go:240] Failed to start service controller: the cloud provider does not support external load balancers
I0309 00:48:36.623220       1 controllermanager.go:264] Will not configure cloud provider routes for allocate-node-cidrs: false, configure-cloud-routes: true.
E0309 00:48:36.641647       1 connection.go:63] Failed to create govmomi client. err: ServerFaultCode: Login failure
I0309 00:48:36.673085       1 nodemanager.go:114] Discovered VM using normal UUID format
I0309 00:48:36.688189       1 node_controller.go:373] Adding node label from cloud provider: beta.kubernetes.io/instance-type=vsphere-vm
I0309 00:48:36.696186       1 nodemanager.go:114] Discovered VM using normal UUID format
I0309 00:48:36.722832       1 controller_utils.go:1032] Caches are synced for persistent volume label controller
I0309 00:48:36.746236       1 node_controller.go:421] Successfully initialized node ccm-integration-test-control-plane with cloud provider
I0309 00:48:36.750818       1 node_controller.go:373] Adding node label from cloud provider: beta.kubernetes.io/instance-type=vsphere-vm
I0309 00:48:36.790370       1 node_controller.go:421] Successfully initialized node ccm-integration-test-worker with cloud provider
I0309 00:48:36.790546       1 node_controller.go:329] This node ccm-integration-test-control-plane is registered without the cloud taint. Will not process.
I0309 00:48:36.790682       1 node_controller.go:329] This node ccm-integration-test-control-plane is registered without the cloud taint. Will not process.
I0309 00:48:36.790780       1 node_controller.go:329] This node ccm-integration-test-worker is registered without the cloud taint. Will not process.
I0309 00:48:36.792137       1 node_controller.go:329] This node ccm-integration-test-worker is registered without the cloud taint. Will not process.
make[1]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
make cluster-down
make[1]: Entering directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
kind delete cluster --name ""ccm-integration-test""
Deleting cluster ""ccm-integration-test"" ...
make[1]: Leaving directory '/Users/akutz/Projects/go/src/k8s.io/cloud-provider-vsphere/test/integration'
```

**Which issue this PR fixes**:
Related to https://github.com/kubernetes/cloud-provider-vsphere/issues/37#issuecomment-470282003

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-09 00:49:41,2019-03-14 17:56:26
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/156,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/156,Go module support; require Go 1.11+; new Makefile,"**What this PR does / why we need it**:
This patch introduces the following:

* Support for Go modules
* Requires Go 1.11+
* A refactored and simplified Makefile
* Travis-CI now uses Xenial
* Parallel builds

The project should no longer be cloned to a GOPATH, instead the project can be cloned anywhere and `make` will build the vSphere cloud controller manager and CSI controller binaries.

Several of the older cross-build targets still exist, but no longer require Gox. Instead `make cross-build` and `make x-dist` build the OS/Arch specific binaries and package them.

Building the image is still restricted to ""linux_amd64"", but now the image is also built into a tar file to make testing even simpler using Kind's ability to load image tar streams directly into the Kubernetes cluster.

Travis-CI is now configured to use Xenial images to build the sources. This is necessary due to a change in BitBucket's security restrictions concerning Mercurial - http://bit.ly/hg-on-travis-failing.

Please see [this Travis-CI build](https://travis-ci.com/akutz/cloud-provider-vsphere/jobs/183664542) for an example of the process in action.

The Makefile now fully supports parallel builds. For example, to package distributions for all available build targets with a parallelism of three, simply execute:

```shell
$ make x-dist -j 3
```

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-10 02:07:57,2019-03-11 16:05:31
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/issues/157,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/157,Fix lint errors,"/kind bug

**What happened**:
The `lint` target reveal many errors/suggestions with the code:

```shell
$ make lint
go list ./... | xargs golint -set_exit_status | sed 's~/Users/akutz/Projects/cloud-provider-vsphere~.~'
./cmd/vcpctl/main.go:53:1: exported function RunMain should have comment or be unexported
./cmd/vcpctl/provision/provision.go:69:1: exported function AddProvision should have comment or be unexported
./cmd/vcpctl/provision/provision.go:86:1: exported function RunProvision should have comment or be unexported
./pkg/cli/cli.go:36:1: exported function ParseConfig should have comment or be unexported
./pkg/cli/client.go:30:6: exported type ClientOption should have comment or be unexported
./pkg/cli/client.go:39:1: exported method ClientOption.NewClient should have comment or be unexported
./pkg/cli/client.go:92:1: exported method ClientOption.GetClient should have comment or be unexported
./pkg/cli/client.go:99:1: exported method ClientOption.Userinfo should have comment or be unexported
./pkg/cli/client.go:107:6: exported type Credential should have comment or be unexported
./pkg/cli/client.go:115:6: exported type VCCMSecret should have comment or be unexported
./pkg/cli/client.go:120:1: exported method ClientOption.LoadCredential should have comment or be unexported
./pkg/cli/user.go:40:6: exported type User should have comment or be unexported
./pkg/cli/user.go:52:1: exported method User.Run should have comment or be unexported
./pkg/cli/user.go:97:6: exported type RolePermission should have comment or be unexported
./pkg/cli/user.go:116:6: exported type Role should have comment or be unexported
./pkg/cli/util.go:25:1: exported function ReadContent should have comment or be unexported
./pkg/cli/util.go:40:1: comment on exported function IsClusterNode should be of the form ""IsClusterNode ...""
./pkg/cli/test/vcsim.go:25:1: exported function NewServiceInstance should have comment or be unexported
./pkg/cloudprovider/vsphere/cloud.go:36:2: exported const ProviderName should have comment (or a comment on this block) or be unexported
./pkg/cloudprovider/vsphere/cloud.go:62:1: exported method VSphere.Initialize should have comment or be unexported
./pkg/cloudprovider/vsphere/cloud.go:88:1: exported method VSphere.LoadBalancer should have comment or be unexported
./pkg/cloudprovider/vsphere/cloud.go:93:1: exported method VSphere.Instances should have comment or be unexported
./pkg/cloudprovider/vsphere/cloud.go:98:1: exported method VSphere.Zones should have comment or be unexported
./pkg/cloudprovider/vsphere/cloud.go:103:1: exported method VSphere.Clusters should have comment or be unexported
./pkg/cloudprovider/vsphere/cloud.go:108:1: exported method VSphere.Routes should have comment or be unexported
./pkg/cloudprovider/vsphere/cloud.go:113:1: exported method VSphere.ProviderName should have comment or be unexported
./pkg/cloudprovider/vsphere/cloud.go:117:1: exported method VSphere.ScrubDNS should have comment or be unexported
./pkg/cloudprovider/vsphere/cloud.go:121:1: exported method VSphere.HasClusterID should have comment or be unexported
./pkg/cloudprovider/vsphere/instances.go:30:2: comment on exported const NodeNotFoundErrMsg should be of the form ""NodeNotFoundErrMsg ...""
./pkg/cloudprovider/vsphere/nodemanager.go:35:6: exported type FindVM should have comment or be unexported
./pkg/cloudprovider/vsphere/nodemanager.go:38:2: exported const FindVMByUUID should have comment (or a comment on this block) or be unexported
./pkg/cloudprovider/vsphere/nodemanager.go:41:2: comment on exported const VCenterNotFoundErrMsg should be of the form ""VCenterNotFoundErrMsg ...""
./pkg/cloudprovider/vsphere/nodemanager.go:132:1: exported method NodeManager.DiscoverNode should have comment or be unexported
./pkg/cloudprovider/vsphere/types.go:48:1: comment on exported type NodeInfo should be of the form ""NodeInfo ..."" (with optional leading article)
./pkg/cloudprovider/vsphere/types.go:58:6: exported type DatacenterInfo should have comment or be unexported
./pkg/cloudprovider/vsphere/types.go:63:6: exported type VCenterInfo should have comment or be unexported
./pkg/cloudprovider/vsphere/types.go:68:6: exported type NodeManager should have comment or be unexported
./pkg/cloudprovider/vsphere/util.go:25:2: exported const ProviderPrefix should have comment (or a comment on this block) or be unexported
./pkg/cloudprovider/vsphere/util.go:28:1: exported function GetUUIDFromProviderID should have comment or be unexported
./pkg/cloudprovider/vsphere/util.go:32:1: comment on exported function ConvertK8sUUIDtoNormal should be of the form ""ConvertK8sUUIDtoNormal ...""
./pkg/cloudprovider/vsphere/server/server.go:33:6: exported type NodeManagerInterface should have comment or be unexported
./pkg/cloudprovider/vsphere/server/server.go:37:6: exported type GRPCServer should have comment or be unexported
./pkg/common/config/config.go:33:2: exported const DefaultRoundTripperCount should have comment (or a comment on this block) or be unexported
./pkg/common/config/config.go:82:6: func name will be used as config.ConfigFromEnv by other packages, and that stutters; consider calling this FromEnv
./pkg/common/config/types.go:72:1: comment on exported type VirtualCenterConfig should be of the form ""VirtualCenterConfig ..."" (with optional leading article)
./pkg/common/connectionmanager/connectionmanager.go:31:6: exported type FindVM should have comment or be unexported
./pkg/common/connectionmanager/connectionmanager.go:34:2: exported const FindVMByUUID should have comment (or a comment on this block) or be unexported
./pkg/common/connectionmanager/connectionmanager.go:37:2: don't use ALL_CAPS in Go names; use CamelCase
./pkg/common/connectionmanager/connectionmanager.go:38:2: don't use ALL_CAPS in Go names; use CamelCase
./pkg/common/connectionmanager/connectionmanager.go:40:2: don't use ALL_CAPS in Go names; use CamelCase
./pkg/common/connectionmanager/connectionmanager.go:41:2: don't use ALL_CAPS in Go names; use CamelCase
./pkg/common/connectionmanager/connectionmanager.go:44:1: exported function NewConnectionManager should have comment or be unexported
./pkg/common/connectionmanager/connectionmanager.go:114:1: exported method ConnectionManager.Connect should have comment or be unexported
./pkg/common/connectionmanager/connectionmanager.go:155:1: exported method ConnectionManager.Logout should have comment or be unexported
./pkg/common/connectionmanager/connectionmanager.go:166:1: exported method ConnectionManager.Verify should have comment or be unexported
./pkg/common/connectionmanager/connectionmanager.go:179:1: exported method ConnectionManager.VerifyWithContext should have comment or be unexported
./pkg/common/connectionmanager/search.go:43:30: method WhichVCandDCByNodeId should be WhichVCandDCByNodeID
./pkg/common/connectionmanager/search.go:213:1: exported method ConnectionManager.WhichVCandDCByFCDId should have comment or be unexported
./pkg/common/connectionmanager/types.go:40:6: type VmDiscoveryInfo should be VMDiscoveryInfo
./pkg/common/connectionmanager/zones.go:502:1: exported method ConnectionManager.LookupZoneByMoref should have comment or be unexported
./pkg/common/credentialmanager/credentialmanager.go:130:1: exported method SecretCache.GetSecret should have comment or be unexported
./pkg/common/credentialmanager/credentialmanager.go:136:1: exported method SecretCache.UpdateSecret should have comment or be unexported
./pkg/common/credentialmanager/credentialmanager.go:142:1: exported method SecretCache.UpdateSecretFile should have comment or be unexported
./pkg/common/credentialmanager/credentialmanager.go:148:1: exported method SecretCache.GetCredential should have comment or be unexported
./pkg/common/credentialmanager/types.go:26:6: exported type SecretCache should have comment or be unexported
./pkg/common/credentialmanager/types.go:33:6: exported type Credential should have comment or be unexported
./pkg/common/credentialmanager/types.go:38:6: exported type SecretCredentialManager should have comment or be unexported
./pkg/common/kubernetes/types.go:26:6: exported type InformerManager should have comment or be unexported
./pkg/common/vclib/constants.go:19:6: exported type FindFCD should have comment or be unexported
./pkg/common/vclib/constants.go:22:2: exported const FindFCDByID should have comment (or a comment on this block) or be unexported
./pkg/common/vclib/datacenter.go:362:1: exported method Datacenter.GetAllDatastoreClusters should have comment or be unexported
./pkg/common/vclib/datacenter.go:441:1: exported method Datacenter.CreateFirstClassDisk should have comment or be unexported
./pkg/common/vclib/datacenter.go:505:1: exported method Datacenter.GetFirstClassDisk should have comment or be unexported
./pkg/common/vclib/datacenter.go:539:1: exported method Datacenter.GetAllFirstClassDisks should have comment or be unexported
./pkg/common/vclib/datacenter.go:594:1: exported method Datacenter.DoesFirstClassDiskExist should have comment or be unexported
./pkg/common/vclib/datacenter.go:613:1: exported method Datacenter.DeleteFirstClassDisk should have comment or be unexported
./pkg/common/vclib/datastore.go:163:1: receiver name dsi should be consistent with previous receiver name di for DatastoreInfo
./pkg/common/vclib/datastore.go:196:1: receiver name dsi should be consistent with previous receiver name di for DatastoreInfo
./pkg/common/vclib/fcd.go:23:6: exported type ParentDatastoreType should have comment or be unexported
./pkg/common/vclib/fcd.go:26:2: exported const TypeDatastore should have comment (or a comment on this block) or be unexported
./pkg/common/vclib/utils.go:211:1: exported function ExistsInList should have comment or be unexported
./pkg/common/vclib/virtualmachine.go:383:1: exported method VirtualMachine.GetVMUUID should have comment or be unexported
./pkg/common/vclib/virtualmachine.go:394:1: exported method VirtualMachine.GetVMNodeName should have comment or be unexported
./pkg/csi/service/node.go:369:12: if block ends with a return statement, so drop this else and outdent its block
./pkg/csi/service/fcd/constants.go:20:2: comment on exported const MbInBytes should be of the form ""MbInBytes ...""
./pkg/csi/service/fcd/constants.go:23:2: comment on exported const GbInBytes should be of the form ""GbInBytes ...""
./pkg/csi/service/fcd/constants.go:26:2: comment on exported const DefaultGbDiskSize should be of the form ""DefaultGbDiskSize ...""
./pkg/csi/service/fcd/constants.go:32:2: comment on exported const AttributeFirstClassDiskType should be of the form ""AttributeFirstClassDiskType ...""
./pkg/csi/service/fcd/constants.go:34:2: exported const AttributeFirstClassDiskName should have comment (or a comment on this block) or be unexported
./pkg/csi/service/fcd/constants.go:44:2: comment on exported const LabelZoneFailureDomain should be of the form ""LabelZoneFailureDomain ...""
./pkg/csi/service/fcd/vsphere_helper.go:34:2: don't use ALL_CAPS in Go names; use CamelCase
./pkg/csi/service/fcd/vsphere_helper.go:34:2: exported const NUM_OF_CONNECTION_ATTEMPTS should have comment (or a comment on this block) or be unexported
./pkg/csi/service/fcd/vsphere_helper.go:35:2: don't use ALL_CAPS in Go names; use CamelCase
./pkg/csi/service/fcd/vsphere_helper.go:37:2: don't use ALL_CAPS in Go names; use CamelCase
./pkg/csi/service/fcd/vsphere_helper.go:38:2: don't use ALL_CAPS in Go names; use CamelCase
Found 95 lint suggestions; failing.
```

**What you expected to happen**:
The linter should not return any suggestions.

**How to reproduce it (as minimally and precisely as possible)**:
The reproduction requires PR https://github.com/kubernetes/cloud-provider-vsphere/pull/156. The following command uses that PR on top of `master` to lint the project:
```shell
$ cd $(mktemp -d) && \
  git clone https://github.com/akutz/cloud-provider-vsphere . && \
  git checkout feature/go-mod-support && \
  git remote add upstream https://github.com/kubernetes/cloud-provider-vsphere && \
  git fetch upstream && \
  git rebase upstream/master && \
  make lint
```

**Anything else we need to know?**: NA

**Environment**: NA",closed,False,2019-03-10 17:44:36,2019-03-12 16:10:40
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/issues/158,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/158,Fix vet errors,"/kind bug

**What happened**:
The `vet` target reveal many errors/suggestions with the code:

```shell
$ make vet
go vet ./...
# k8s.io/cloud-provider-vsphere/pkg/cli
pkg/cli/cli.go:117: result of fmt.Errorf call not used
pkg/cli/cli.go:121: result of fmt.Errorf call not used
pkg/cli/cli.go:124: result of fmt.Errorf call not used
make: *** [Makefile:144: vet] Error 2
```

**What you expected to happen**:
The linter should not return any suggestions.

**How to reproduce it (as minimally and precisely as possible)**:
The reproduction requires PR https://github.com/kubernetes/cloud-provider-vsphere/pull/156. The following command uses that PR on top of `master` to vet the project:
```shell
$ cd $(mktemp -d) && \
  git clone https://github.com/akutz/cloud-provider-vsphere . && \
  git checkout feature/go-mod-support && \
  git remote add upstream https://github.com/kubernetes/cloud-provider-vsphere && \
  git fetch upstream && \
  git rebase upstream/master && \
  make vet
```

**Anything else we need to know?**: NA

**Environment**: NA",closed,False,2019-03-10 17:46:52,2019-03-12 16:10:40
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/159,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/159,Make Env Vars take precedence over config file,"
<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
The way that the config file was parsed and loaded previously was done
in a way that when reading a vSphere.conf file, env vars were checked
first, all errors from that were ignored, then values were loaded from
file and the config was validated.

Env vars should take precedence. This patch changes it so that when
reading a config file, that file is loaded, any valid env vars overwrite
the values from the file, and then the whole composite config is
validated.

It was also true (in the case of CSI) that a vsphere.conf file was
always required. This makes future testing difficult, as for basic
cases, it should be possible to configure the CSI plugin via Env vars
only. This patch makes that possible.

It was previously possible that if you did use Env Var only for config,
that an invalid config was returned without error. This patch fixes that
as well.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-03-10 18:44:33,2019-03-11 20:16:39
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/issues/160,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/160,"The test ""TestGRPCServerClient"" is flaky on Prow","/kind bug
/assign @dvonthenen 

**What happened**:
The test `TestGRPCServerClient` fails occasionally on Prow.

**What you expected to happen**:
For the test to not fail.

**How to reproduce it (as minimally and precisely as possible)**:
I don't know. Please see https://prow.k8s.io/job-history/kubernetes-jenkins/pr-logs/directory/pull-cloud-provider-vsphere-test for a list of the failed tests, specifically builds [385](https://prow.k8s.io/view/gcs/kubernetes-jenkins/pr-logs/pull/cloud-provider-vsphere/156/pull-cloud-provider-vsphere-test/385) and [387](https://prow.k8s.io/view/gcs/kubernetes-jenkins/pr-logs/pull/cloud-provider-vsphere/156/pull-cloud-provider-vsphere-test/387).

**Anything else we need to know?**: NA

**Environment**: NA",closed,False,2019-03-10 22:20:11,2019-03-11 21:54:22
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/161,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/161,Address flaky TestGRPCServerClient in `make test`,"**What this PR does / why we need it**:
Fixes the flaky gRPC server test. This is done by adding a simple gRPC call `GetVersion()` which will be needed later on anyways to obtain the version of the API and then just after server creation attempts to call that API to verify that the server is actually reachable in the `Start()` call. Also adds functionality to gracefully shutdown the server.

An example of the race condition is shown below in a run of `make test` and also the recovery of it.
```
=== RUN   TestGRPCServerNodes
I0311 09:58:12.297779   28646 server.go:126] API_VERSION: 0.0.1
--- PASS: TestGRPCServerNodes (0.00s)
=== RUN   TestGRPCServerVersion
W0311 09:58:12.299536   28646 server.go:121] could not getversion: rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = ""transport: Error while dialing dial tcp :43001: connect: connection refused""
I0311 09:58:13.301226   28646 server.go:126] API_VERSION: 0.0.1
--- PASS: TestGRPCServerVersion (1.00s)
PASS
```

**Which issue this PR fixes**: https://github.com/kubernetes/cloud-provider-vsphere/issues/160

**Special notes for your reviewer**:
None

**Release note**:
None
",closed,True,2019-03-11 17:04:31,2019-03-12 16:19:59
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/162,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/162,Image for building the binaries,"**What this PR does / why we need it**:
This patch introduces an image that already has a Go module cache produced from a recent execution of ""make deps"" against the ""master"" branch of this repository. This speeds up builds exponentially when executing ""hack/make.sh"" since only delta changes are downloaded for dependencies. For example:

```shell
$ hack/make.sh build
latest: Pulling from cloud-provider-vsphere/golang-1.12
Digest: sha256:8ab7713835daaf7f0e141f355f6f87d40c1fb8f78f46a7a1e7eb056dc069cae9
Status: Image is up to date for gcr.io/cloud-provider-vsphere/golang-1.12:latest
CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -ldflags '-extldflags ""-static"" -w -s -X ""main.version=7612a0af""' -o vsphere-cloud-controller-manager.linux_amd64 cmd/vsphere-cloud-controller-manager/main.go
CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -ldflags '-extldflags ""-static"" -w -s -X ""k8s.io/cloud-provider-vsphere/pkg/csi/service.version=7612a0af""' -o vsphere-csi.linux_amd64 cmd/vsphere-csi/main.go
```

If a variant of `cloud-provider-vsphere/golang-1.12` isn't available for the requested Golang version, then the default `golang` image is used at the requested version.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-11 17:27:12,2019-03-15 00:24:57
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/163,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/163,"Fix format, vet, lint, and logic errors","**What this PR does / why we need it**:
This patch fixes:
* Outstanding format issues revealed when running `make fmt`
* Outstanding vet issues revealed when running `make vet` (#158)
* Over 100 linter issues revealed when running `make lint` (#157)
* A logic error in the function `ExistsInList`

**Which issue this PR fixes** 
fixes #157, fixes #158

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-12 00:05:27,2019-03-12 16:10:40
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/164,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/164,Add env var to disable CSI K8s client,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR will make it possible (may not be all that's needed) to run the [CSI-Sanity test suite](https://github.com/kubernetes-csi/csi-test) against our CSI plugin outside of a K8s pod.

CSI plugins are intended to be CO agnostic. The current CSI plugin was
always initializing a K8s client and informer, which would cause plugin
initialization to fail if it was not running within a K8s pod.

This patch adds a new env var, X_CSI_DISABLE_K8S_CLIENT which can be set to
""true"" (or any other truthy value) to disable said initialization.

This patch also changes some methods around the K8s client and informer
to no longer use a pointer to a Go interface, as Go interfaces are
already pointers.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
`pkg/csi/envvars.go` was moved to `pkg/csi/types/envvars.go` to avoid circular imports.

The struct `pkg/csi/service/fcd/controller.controller` no longs holds a reference to the Kubernetes clientset or the informer, as these were never used again outside of the `Init` function.


**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-03-12 00:22:37,2019-03-14 15:39:42
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/pull/165,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/165,Only report IPs bound to vNICs,"**What this PR does / why we need it**:
We only report the IP address bound to the vNIC. As shown below in testing...

```
[vonthd@k8smaster csi]$ kubectl describe node k8sworker1.local
Name:               k8sworker1.local
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=vsphere-vm
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=k8s-region-eu
                    failure-domain.beta.kubernetes.io/zone=k8s-region-eu-all
                    kubernetes.io/hostname=k8sworker1.local
...
Addresses:
  ExternalIP:  10.160.137.75
  InternalIP:  10.160.137.75
  Hostname:    k8sworker1.local
```

**Which issue this PR fixes**: https://github.com/kubernetes/cloud-provider-vsphere/issues/138

**Special notes for your reviewer**:
NA

**Release note**:
NA
",closed,True,2019-03-12 16:47:15,2019-03-13 13:34:15
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/166,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/166,Enable linting on Prow,"**What this PR does / why we need it**:
This patch introduces support for linting in a pre-submit, blocking Prow job. The Makefile's `check` target now invokes `hack/check.sh`, which in turn invokes `make fmt`, `make vet`, and `make lint`, translating their results into the JUnit report that is consumable by the K8s test-grid.

This [gist](https://gist.github.com/akutz/42d1dc17d64edab19dbc1c11fc9fbaef) has an example of the `make check` command as well as the JUnit report the command produces.

The report is written to `${ARTIFACTS}/junit_check.xml`.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-12 17:30:12,2019-03-14 17:56:32
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/167,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/167,Support for running e2e against the CCM and CSI controllers from Prow,"**What this PR does / why we need it**:
# Continuous integration

The image `gcr.io/cloud-provider-vsphere/ci` is used by Prow jobs to build, test, and deploy the CCM and CSI providers.

## The CI workflow

Prow jobs are configured to perform the following steps:

| Job type | Linters | Build binaries | Unit test | Build images | Integration test | Deploy images | Conformance test |
|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| Presubmit |  |  |  |  |  | | |
| Postsubmit |  |  |  |  |  |  | |
| Periodic |  |  |  |  | | |  |

## Up-to-date sources

When running on Prow the jobs map the current sources into the CI container. That may be simulated locally by running the examples from a directory containing the desired sources and providing the `docker run` command with the following flags:

* `-v ""$(pwd)"":/go/src/k8s.io/cloud-provider-vsphere`

## Docker-in-Docker

Several of the jobs require Docker-in-Docker. To mimic that locally there are two options:

1. [Provide the host's Docker to the container](#provide-the-hosts-docker-to-the-container) 
2. [Run the Docker server inside the container](#run-the-docker-server-inside-the-container)

### Provide the host's Docker to the container

While Prow jobs [run the Docker server inside the container](#run-the-docker-server-inside-the-container), this option provides a low-cost (memory, disk) solution for testing locally. This option is enabled by running the examples from a directory containing the desired sources and providing the `docker run` command with the following flags:

* `-v /var/run/docker.sock:/var/run/docker.sock`
* `-e ""PROJECT_ROOT=$(pwd)""`
* `-v ""$(pwd)"":/go/src/k8s.io/cloud-provider-vsphere`

Please note that this option is only available when using a local copy of the sources. This is because all of the paths known to Docker will be of the local host system, not from the container. That's also why it's necessary to provide the `PROJECT_ROOT` environment variable -- it indicates to certain recipes the location of specific files or directories relative to the local sources on the host system.

### Run the Docker server inside the container
This is option that Prow jobs utilize and is also the method illustrated by the examples below. Please keep in mind that using this option locally requires a large amount of memory and disk space available to Docker:

| Type | Minimum Requirement |
|------|---------------------|
| Memory | 8GiB |
| Disk | 200GiB |

For Windows and macOS systems this means adjusting the size of the Docker VM disk and the amount of memory the Docker VM is allowed to use.

Resources notwithstanding, running the Docker server inside the container also requires providing the `docker run` command with the following flags:

* `--privileged`

## Check the sources

To check the sources run the following command:

```shell
$ docker run -it --rm \
  -e ""ARTIFACTS=/out"" -v ""$(pwd)"":/out \
  gcr.io/cloud-provider-vsphere/ci \
  make check
```

The above command will create the following files in the working directory:

* `junit_check.xml`

## Build the CCM and CSI binaries

The CI image is built with Go module and build caches from a recent build of the project's `master` branch. Therefore the CI image can be used to build the CCM and CSI binaries in a matter of seconds:

```shell
$ docker run -it --rm \
  -e ""BIN_OUT=/out"" -v ""$(pwd)"":/out \
  gcr.io/cloud-provider-vsphere/ci \
  make build
```

The above command will create the following files in the working directory:

* `vsphere-cloud-controller-manager.linux_amd64`
* `vsphere-csi.linux_amd64`

## Execute the unit tests

```shell
$ docker run -it --rm \
  gcr.io/cloud-provider-vsphere/ci \
  make unit-test
```

## Build the CCM and CSI images

Building the CCM and CSI images inside another image requires Docker-in-Docker (DinD):

```shell
$ docker run -it --rm --privileged \
  gcr.io/cloud-provider-vsphere/ci \
  make build-images
```

## Execute the integration tests
The project's integration tests leverage Kind, a solution for turning up a Kubernetes cluster using Docker:

```shell
$ docker run -it --rm --privileged \
  gcr.io/cloud-provider-vsphere/ci \
  make integration-test
```

Running the integration tests with the container providing the Docker server is **severely** taxing on the host system's resources. It is **highly** recommended, for purposes of local development, to opt to provide Docker to the container by bind mounting the host's Docker socket into the container. Please note this also requires using local sources and setting `PROJECT_ROOT`:

```shell
$ docker run -it --rm \
  -e ""PROJECT_ROOT=$(pwd)"" \
  -v ""$(pwd)"":/go/src/k8s.io/cloud-provider-vsphere \
  -v /var/run/docker.sock:/var/run/docker.sock \
  gcr.io/cloud-provider-vsphere/ci \
  make integration-test
```

## Deploy the CCM and CSI images
Pushing the images requires bind mounting a GCR key file into the container and setting the environment variable `GCR_KEY_FILE` to inform the deployment process the location of the key file:

```shell
$ docker run -it --rm --privileged \
  -e ""GCR_KEY_FILE=/keyfile.json"" -v ""$(pwd)/keyfile.json"":/keyfile.json \
  gcr.io/cloud-provider-vsphere/ci \
  make push-images
```

## Execute the conformance tests
Running the e2e conformance suite not only requires DinD but also an environment variable file that provides the information required to turn up a Kubernetes cluster against which the e2e tests are executed. For example:

```shell
VSPHERE_SERVER='vcenter.com'
VSPHERE_USERNAME='myuser'
VSPHERE_PASSWORD='mypass'
VSPHERE_DATACENTER='/dc1'
VSPHERE_RESOURCE_POOL=""/dc1/host/Cluster-1/Resources/mypool""
VSPHERE_DATASTORE=""/dc1/datastore/mydatastore""
VSPHERE_FOLDER=""/dc1/vm/myfolder""
```

If the vSphere endpoint is hosted in the VMware Cloud (VMC) on AWS then the file can also contain AWS access credentials to provide external access to the Kubernetes cluster:

```shell
AWS_ACCESS_KEY_ID='mykey'
AWS_SECRET_ACCESS_KEY='mysecretkey'
AWS_DEFAULT_REGION='myregion'
```

Finally, the configuration file can also include details that define the shape of the Kubernetes cluster as well as influence how and what e2e tests are executed:

```shell
CLOUD_PROVIDER='external'
E2E_FOCUS='\\[Conformance\\]'
E2E_SKIP='Alpha|\\[(Disruptive|Feature:[^\\]]+|Flaky)\\]'
K8S_VERSION='ci/latest'
NUM_BOTH='1'
NUM_CONTROLLERS='1'
NUM_WORKERS='1'
KUBE_CONFORMANCE_IMAGE='akutz/kube-conformance:latest'
```

Once the environment variable file is created, the conformance tests may be executed with:

```shell
$ docker run -it --rm --privileged \
  -e ""ARTIFACTS=/out"" -v ""$(pwd)"":/out \
  --env-file config.env \
  gcr.io/cloud-provider-vsphere/ci \
  make conformance-test
```

The above command will create the following files in the working directory:

```shell
$ ls -al
total 1936
drwxr-xr-x  13 akutz  staff   416B Mar 15 17:26 ./
drwxr-xr-x   3 akutz  staff    96B Mar 15 17:21 ../
-rw-r--r--   1 akutz  staff   599B Mar 15 17:21 build-info.json
-rw-r--r--   1 akutz  staff   8.4K Mar 15 17:26 e2e.log
drwxr-xr-x@  4 akutz  staff   128B Mar 15 17:26 hosts/
-rw-r--r--   1 akutz  staff   935K Mar 15 17:26 junit_01.xml
drwxr-xr-x@  5 akutz  staff   160B Mar 15 17:26 meta/
drwxr-xr-x@  4 akutz  staff   128B Mar 15 17:26 plugins/
drwxr-xr-x@  3 akutz  staff    96B Mar 15 17:26 podlogs/
drwxr-xr-x@  4 akutz  staff   128B Mar 15 17:26 resources/
-rw-r--r--@  1 akutz  staff   4.0K Mar 15 17:26 servergroups.json
-rw-r--r--@  1 akutz  staff   253B Mar 15 17:26 serverversion.json
-rw-r--r--   1 akutz  staff   281B Mar 15 17:25 terraform-output-vars.txt
```

The `build-info.json` file includes information about the build:

```json
{
  ""cluster-name"": ""prow-fb3ba63"",
  ""k8s-version"": ""ci/latest"",
  ""num-both"": ""1"",
  ""num-controllers"": ""1"",
  ""num-workers"": ""1"",
  ""cloud-provider"": ""external"",
  ""e2e-focus"": ""should provide DNS for the cluster[[:space:]]{0,}\\[Conformance\\]"",
  ""e2e-skip"": ""Alpha|\\[(Disruptive|Feature:[^\\]]+|Flaky)\\]"",
  ""kube-conformance-image"": ""akutz/kube-conformance:latest"",
  ""config-env"": ""/config.env"",
  ""gcr-key-file"": ""/keyfile.json""
}
```

The file `terraform-output.txt` includes information about the cluster that was turned up:

```shell
controllers = [
    192.168.3.207
]
controllers-with-kubelets = 1
etcd = https://discovery.etcd.io/8c6a3f14571bf6892d370c325a428d9b
external_fqdn = sk8lb-ee7dff5-6b4c40f39f288cf1.elb.us-west-2.amazonaws.com
kubeconfig = data/prow-fb3ba63/kubeconfig
workers = [
    192.168.3.182
]
```

And finally, the files `e2e.log` and `junit_01.xml` are the log for the e2e  execution and the file parsed by the K8s test grid.

The remaining files are created by Sonobuoy during the test execution.


cc @figo @frapposelli @dougm @dvonthenen 

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-15 22:39:49,2019-03-19 22:00:22
cloud-provider-vsphere,frapposelli,https://github.com/kubernetes/cloud-provider-vsphere/pull/168,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/168,Add descriptive instanceType to nodes,"**What this PR does / why we need it**:

This PR adds support for returning a more descriptive InstanceType for nodes, instead of returning a static string.

**Which issue this PR fixes** 

Fixes: #55

**Special notes for your reviewer**:


**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```",closed,True,2019-03-18 10:27:51,2019-03-19 08:26:44
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/169,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/169,Updates to CI and conformance work,"**What this PR does / why we need it**:
This patch includes several changes related to the CI and conformance workflow:

## ci: Require the Docker image registry include
This patch changes the behavior of including the Docker image registry makefile in the root makefile. If the include is not present an error will occur.

## ci: Simplify the VERSION variable
This patch simplifies the Makefile VERSION to use ?= instead of ifndef.

## ci: Print CI image version
This patch adds a target for printing the version of the CI image.

## ci: Rm Makefile from CI build cache; LDFLAGS
This patch removes the need to copy the Makefile when creating the build cache for the CI image in order to not invalidate the cache due to a Makefile change. To support this the LDFLAGS have been externalized to ""hack/make/ldflags.txt"" so the Makefile and CI Dockerfile both use the same flags.

## ci: Use sk8e2e as a stage instead of copying
This patch uses sk8e2e as a stage instead of copying directly from it. This allows the setting of the sk8e2e image as a build arg.

## ci: Add clean-d target to remove marker files
This patch introduces the ""clean-d"" target as a way to quickly remove all of the project's marker files.

## ci: Add deploy target
This patch adds a new target named ""deploy"" to make it easier to execute the deploy job from Prow.

## e2e: Fix quick-conformance-test target
This patch fixes the quick-conformance-test target by removing the unnecessary quotes from around E2E_FOCUS.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-20 16:47:45,2019-03-20 17:15:45
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/170,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/170,Explore splitting CSI plugin into separate repo,"/kind cleanup


We should look at whether we want to move the CSI plugin to it's own separate repo. Having it in the same repo as the CCM means they are coupled for releases, and makes versioning and tagging more complicated. We originally put CSI in here with the CCM because we followed [KEP 02](https://github.com/kubernetes/enhancements/blob/master/keps/sig-cloud-provider/20180530-cloud-controller-manager.md#repository-requirements), but this is worth a revisit. Early discussions with @andrewsykim indicate SIG-cloud provider may be open to changing this.

Assuming we *do* want to split out, two questions arise:

What is the location for the new repo? k8s-sigs has been the most common suggestion so far.

Since the CSI repo will need to import the common code from the CCM, should we move the commong code (e.g. `cloud-provider-vsphere/pkg/common`) to a separate repo as well? This would likely need to be hosted under the VMware org rather than Kubernetes.",open,False,2019-03-20 19:33:13,2019-04-03 14:52:33
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/issues/171,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/171,csi: update kubernetes-csi docs,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind documentation

The docs at https://kubernetes-csi.github.io/docs/drivers.html are out of date re: csi-vSphere, an need to be updated to point to the right place.
",open,False,2019-03-20 19:35:10,2019-03-29 21:36:17
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/172,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/172,[WIP] Test the CI jobs,"/hold

This patch is fake and should not be merged. It's for testing the CI jobs.

**What this PR does / why we need it**: You don't.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: Hi there.

**Release note**: NA",closed,True,2019-03-22 20:52:20,2019-03-29 18:15:22
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/173,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/173,Fix linting issues,"**What this PR does / why we need it**:
This patch fixes a few outstanding linting issues.

cc @codenrhoden 

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-22 21:31:03,2019-03-22 21:36:12
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/174,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/174,ci: Update the version of sk8e2e to fix CI bug,"**What this PR does / why we need it**:
This patch updates the version of sk8e2e used in the CI image to fix an issue when running conformance tests on Prow. The underlying issue is described at vmware/simple-k8s-test-env@eb10f99.

The CI image for this commit is already built and published at `gcr.io/cloud-provider-vsphere/ci:f2a0d372`.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-27 23:01:04,2019-03-27 23:40:49
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/175,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/175,ci: Fix CI conformance with new network ID,"**What this PR does / why we need it**:
This patch accounts for the changes that occurred as the result of the VMC NSX migration. The networks were relocated, and so the network ID of the network to which VMs are deployed changed.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-28 19:51:34,2019-03-28 23:29:57
cloud-provider-vsphere,codenrhoden,https://github.com/kubernetes/cloud-provider-vsphere/pull/176,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/176,Add Block Volume support,"

**What this PR does / why we need it**:
Adds block volume support for the CSI plugin

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #80 

**Special notes for your reviewer**:
This patch adds support for CSI's Block Volume access type. There are
some rough edges around the CSI spec when it comes to working with block
volumes and NodeStageVolume and NodeUnstageVolume, but these are worked
around for now.

For NodeStageVolume, we implement this as a noop for Block. The call
isn't necessary for Block volume, but it is mandated by the spec.

For NodeUnstageVolume, we would also do it as a noop, but there is no
context with the call to determine whether the volume in question is
block or mount access. So instead we rely on the requirement on the CO
to have created a staging-target directory. If that exists but nothing
is mounted to it, we assume it was block and move on. This is safe
because it is the CO's job to create and delete the staging target dir,
whereas it is the SP's job to create and delete the target path.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-03-29 20:08:30,2019-04-02 15:22:12
cloud-provider-vsphere,akutz,https://github.com/kubernetes/cloud-provider-vsphere/pull/177,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/177,ci: Update sk8e2e image in CI image,"**What this PR does / why we need it**:
Updates the sk8e2e image in the CI image.

**Which issue this PR fixes**: NA

**Special notes for your reviewer**: NA

**Release note**: NA",closed,True,2019-03-30 20:14:05,2019-04-02 14:50:36
cloud-provider-vsphere,Elegant996,https://github.com/kubernetes/cloud-provider-vsphere/issues/178,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/178,Err: No vSphere disk ID/Name found,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:
Volume failed to attach to pod after it was restarted.

**What you expected to happen**:
Volume to successfully reattach as the `PersistentVolumeClaim` and `PersistentVolume` still exist as well as the `vmdk`.

**How to reproduce it (as minimally and precisely as possible)**:
After a grace period (a day or two?), scale the replicas of Deployment/StatefulSet to 0 that has a storage volume attached through the CSI driver. Set the scale back to its original value and CSI driver should fail to attach the volume on the new pod(s):

> Warning  FailedAttachVolume  5s (x4 over 6m12s)   attachdetach-controller                AttachVolume.Attach failed for volume ""pvc-2e07359a-4f90-11e9-a939-000c29616bad"" : rpc error: code = Internal desc = WhichVCandDCByFCDId(927ef76f-2312-4a4e-b634-8fbb13134462) failed. Err: No vSphere disk ID/Name found

Please note that this is independent of the node that the pod previously resided on. The disk does not appear to unmount from the node in vCenter either which seems problematic.

**Anything else we need to know?**:
`csi-attacher` logs:
>I0403 05:37:54.655419       1 controller.go:173] Started VA processing ""csi-94a7031ea7c96543a069a44a5cba3b8f4db0aa7f2cd7f6800610578ba01e3b63""
I0403 05:37:54.655541       1 csi_handler.go:93] CSIHandler: processing VA ""csi-94a7031ea7c96543a069a44a5cba3b8f4db0aa7f2cd7f6800610578ba01e3b63""
I0403 05:37:54.655565       1 csi_handler.go:120] Attaching ""csi-94a7031ea7c96543a069a44a5cba3b8f4db0aa7f2cd7f6800610578ba01e3b63""
I0403 05:37:54.655915       1 csi_handler.go:259] Starting attach operation for ""csi-94a7031ea7c96543a069a44a5cba3b8f4db0aa7f2cd7f6800610578ba01e3b63""
I0403 05:37:54.656107       1 csi_handler.go:388] Saving attach error to ""csi-94a7031ea7c96543a069a44a5cba3b8f4db0aa7f2cd7f6800610578ba01e3b63""
I0403 05:37:54.660047       1 controller.go:139] Ignoring VolumeAttachment ""csi-94a7031ea7c96543a069a44a5cba3b8f4db0aa7f2cd7f6800610578ba01e3b63"" change
I0403 05:37:54.660383       1 csi_handler.go:398] Saved attach error to ""csi-94a7031ea7c96543a069a44a5cba3b8f4db0aa7f2cd7f6800610578ba01e3b63""
I0403 05:37:54.660413       1 csi_handler.go:103] Error processing ""csi-94a7031ea7c96543a069a44a5cba3b8f4db0aa7f2cd7f6800610578ba01e3b63"": failed to attach: persistentvolume ""pvc-7df261c8-4c50-11e9-a573-000c29616bad"" not found

`vsphere-csi-controller` logs:
>I0403 05:43:21.955579       1 datacenter.go:615] DoesFirstClassDiskExist(927ef76f-2312-4a4e-b634-8fbb13134462): NOT FOUND
E0403 05:43:21.955607       1 search.go:329] Error while looking for FCD=<nil> in vc=10.0.10.25 and datacenter=homelab: No vSphere disk ID/Name found
time=""2019-04-03T05:43:21Z"" level=error msg=""WhichVCandDCByFCDId(927ef76f-2312-4a4e-b634-8fbb13134462) failed. Err: No vSphere disk ID/Name found""

**Environment**:
- vsphere-cloud-controller-manager version: `v0.1.1` and `master`
- OS (e.g. from /etc/os-release): Fedora 28
- Kernel (e.g. `uname -a`): 4.18.8-200.fc28.x86_64
- Install tools:
- Others:
",open,False,2019-04-03 05:40:46,2019-04-03 19:53:18
cloud-provider-vsphere,dvonthenen,https://github.com/kubernetes/cloud-provider-vsphere/issues/179,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/179,Investigate Node Affinity Scheduling,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
Based in part on the work done here:
https://github.com/vmware/vsphere-affinity-scheduling-plugin ",open,False,2019-04-03 18:19:06,2019-04-03 18:19:14
cloud-provider-vsphere,RaceFPV,https://github.com/kubernetes/cloud-provider-vsphere/issues/180,https://api.github.com/repos/kubernetes/cloud-provider-vsphere/issues/180,Attached PVC vmdks get deleted when a node is removed,"When a node is removed via the provider, there are cases where the still attached PVCs can get deleted from disk as well, causing a loss of data and the vmdk file. This then causes k8s to fail to bring up the container on a new node as the underlying PVC is gone.

This can be resolved by having the vm deleted but not deleting all of the PVC vmdk files, or by flagging the PVC vmdks to not delete with vm on creation.

This has cost the loss of data including sql databases etc without warning as there are no prompts or stop telling you what actions will occur on node removal/deletion.",open,False,2019-04-05 00:30:03,2019-04-05 00:30:03
