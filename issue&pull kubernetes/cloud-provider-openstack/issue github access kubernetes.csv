name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
cloud-provider-openstack,hogepodge,https://github.com/kubernetes/cloud-provider-openstack/pull/1,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/1,Merge working repository and history to new upstream,Merge the master branch from dims/openstack-cloud-controller-manager to kubernetes/cloud-provider-openstack.,closed,True,2018-03-21 15:22:10,2018-03-21 15:30:39
cloud-provider-openstack,calebamiles,https://github.com/kubernetes/cloud-provider-openstack/pull/2,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/2,Merge OpenStack Cloud Provider upstream,taken from @dims ,closed,True,2018-03-21 18:25:00,2018-03-21 19:40:14
cloud-provider-openstack,calebamiles,https://github.com/kubernetes/cloud-provider-openstack/pull/3,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/3,Update LICENSE,,closed,True,2018-03-21 19:37:24,2018-03-27 23:02:43
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/4,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/4,[WIP] just testing CI jobs,,closed,True,2018-03-22 00:39:41,2018-03-22 10:15:41
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/5,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/5,Install glide in makefile,,closed,True,2018-03-22 02:14:41,2018-03-22 02:33:05
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/6,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/6,make vet fmt is broken,"```
$ make vet fmt
cd /Users/dims/go/src/git.openstack.org/openstack/openstack-cloud-controller-manager && go vet ./...
# git.openstack.org/openstack/openstack-cloud-controller-manager/pkg/volume/cinder/provisioner
pkg/volume/cinder/provisioner/iscsi_test.go:61:4: err declared but not used
pkg/volume/cinder/provisioner/iscsi_test.go:105:4: err declared but not used
pkg/volume/cinder/provisioner/iscsi_test.go:153:4: err declared but not used
pkg/volume/cinder/provisioner/rbd_test.go:78:4: err declared but not used
vet: typecheck failures
make: *** [vet] Error 2
```",closed,False,2018-03-22 02:34:15,2018-03-23 05:30:04
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/7,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/7,Replace git.openstack.org/openstack/openstack-cloud-controller-manager with k8s.io/cloud-provider-openstack,We need to find and replace all instances of the earlier namespace we used to use with the new one.,closed,False,2018-03-22 02:35:37,2018-03-28 00:17:05
cloud-provider-openstack,hogepodge,https://github.com/kubernetes/cloud-provider-openstack/pull/8,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/8,Updated package references from git.openstack.org to k8s.io,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

This PR updates package references from git.openstack.org to k8s.io.

**Which issue this PR fixes** 

fixes #7 

**Special notes for your reviewer**:

related PR : kubernetes/k8s.io#108

**Release note**:

```release-note 
Updated package references to k8s.io/cloud-provider-openstack/*
```",closed,True,2018-03-22 04:52:37,2018-03-29 17:24:31
cloud-provider-openstack,hogepodge,https://github.com/kubernetes/cloud-provider-openstack/pull/9,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/9,Fix make vet fmt,"
<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

Several tests had unused err variables. Removed them to fix `make vet fmt`

**Which issue this PR fixes**
fixes #6 

**Special notes for your reviewer**:

**Release note**:

<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-03-22 05:14:23,2018-03-29 17:24:33
cloud-provider-openstack,flaper87,https://github.com/kubernetes/cloud-provider-openstack/pull/10,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/10,Volunteer to review and collaborate,"As promised, I'm putting my name out there to help review and maintain this cloud provider.

I'm not sure what the right process to update `OWNERS` is but here's a try :)",closed,True,2018-03-22 05:27:07,2018-03-23 21:17:22
cloud-provider-openstack,gonzolino,https://github.com/kubernetes/cloud-provider-openstack/pull/11,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/11,Add Daniel Gonzalez (gonzolino) as reviewer,,closed,True,2018-03-22 10:37:25,2018-03-23 18:16:02
cloud-provider-openstack,strigazi,https://github.com/kubernetes/cloud-provider-openstack/pull/12,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/12,Add strigazi to reviewers,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-03-22 10:53:36,2018-03-23 21:06:46
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/13,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/13,Use best practices to run external CCM,"_From @dims on March 21, 2018 18:57_

This list is from @andrewsykim 

* use `hostnetwork: true` so you don't depend on CNI to initialize nodes
* use `dnsPolicy: Default` since kube-dns likely doesn't tolerate the `node.cloudprovider.kubernetes.io/uninitialized` taint
* tolerate `node-role.kubernetes.io/master` since you probably want it running on master nodes
* use --leader-elect if you want HA

_Copied from original issue: dims/openstack-cloud-controller-manager#92_",closed,False,2018-03-22 10:56:48,2018-08-19 12:39:36
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/14,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/14,Use best practices to run external CCM,"_From @dims on March 21, 2018 18:57_

This list is from @andrewsykim 

* use `hostnetwork: true` so you don't depend on CNI to initialize nodes
* use `dnsPolicy: Default` since kube-dns likely doesn't tolerate the `node.cloudprovider.kubernetes.io/uninitialized` taint
* tolerate `node-role.kubernetes.io/master` since you probably want it running on master nodes
* use --leader-elect if you want HA

_Copied from original issue: dims/openstack-cloud-controller-manager#92_",closed,False,2018-03-22 10:57:04,2018-03-22 10:57:27
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/15,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/15,Document how to setup RBAC with Keystone Authentication,"_From @dims on March 16, 2018 21:50_

See note from Saverio on his blog post : https://cloudblog.switch.ch/2018/02/02/openstack-keystone-authentication-for-your-kubernetes-cluster/
```
I will be correctly authenticated by keystone that will verify my identity, but I will have no authorization to do anything:
```

_Copied from original issue: dims/openstack-cloud-controller-manager#87_",closed,False,2018-03-22 10:58:08,2018-10-29 16:11:16
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/16,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/16,Unable to mount Openstack Cinder Volume in a pod,"_From @walteraa on March 2, 2018 20:56_

<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug



**What happened**:
Using Openstack cloud provider, I am not able to mount my Cinder volumes in pods. I am getting this error in my pod events:
```
Events:
  Type     Reason                 Age   From                               Message
  ----     ------                 ----  ----                               -------
  Normal   Scheduled              16s   default-scheduler                  Successfully assigned mongo-controller-5sktj to walter-atmosphere-minion
  Normal   SuccessfulMountVolume  16s   kubelet, walter-atmosphere-minion  MountVolume.SetUp succeeded for volume ""default-token-7cx2x""
  Warning  FailedMount            16s   kubelet, walter-atmosphere-minion  MountVolume.SetUp failed for volume ""walter-test"" : mount failed: exit status 32
Mounting command: systemd-run
Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/7df75a03-1e58-11e8-93a7-fa163ec86641/volumes/kubernetes.io~cinder/walter-test --scope -- mount -o bind /var/lib/kubelet/plugins/kubernetes.io/cinder/mounts/ea7e96fe-24cb-40f3-9fb3-420ac7ac1752 /var/lib/kubelet/pods/7df75a03-1e58-11e8-93a7-fa163ec86641/volumes/kubernetes.io~cinder/walter-test
Output: Running scope as unit run-r488c59ffc9324542af0c41f646f6ff99.scope.
mount: special device /var/lib/kubelet/plugins/kubernetes.io/cinder/mounts/ea7e96fe-24cb-40f3-9fb3-420ac7ac1752 does not exist
  Warning  FailedMount  15s  kubelet, walter-atmosphere-minion  MountVolume.SetUp failed for volume ""walter-test"" : mount failed: exit status 32
Mounting command: systemd-run
``` 

My `openstack-cloud-provider` is showing the following error:

```
ERROR: logging before flag.Parse: I0302 20:36:33.026783       1 openstack_instances.go:46] Claiming to support Instances
ERROR: logging before flag.Parse: I0302 20:36:38.029334       1 openstack_instances.go:46] Claiming to support Instances
ERROR: logging before flag.Parse: I0302 20:36:43.035928       1 openstack_instances.go:46] Claiming to support Instances
(...)
```

Is it important to know that, before this error, I was getting another error:
```
ERROR: logging before flag.Parse: E0302 18:34:19.759493       1 reflector.go:205] git.openstack.org/openstack/openstack-cloud-controller-manager/vendor/k8s.io/kubernetes/pkg/controller/cloud/pvlcontroller.go:109: Failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User ""system:serviceaccount:kube-system:pvl-controller"" cannot list persistentvolumes at the cluster scope
```

Then, I work around it by runnig the following commands:
```
kubectl create clusterrolebinding --user system:serviceaccount:kube-system:default kube-system-cluster-admin-1 --clusterrole cluster-admin
kubectl create clusterrolebinding --user system:serviceaccount:kube-system:pvl-controller kube-system-cluster-admin-2 --clusterrole cluster-admin
kubectl create clusterrolebinding --user system:serviceaccount:kube-system:cloud-node-controller kube-system-cluster-admin-3 --clusterrole cluster-admin
kubectl create clusterrolebinding --user system:serviceaccount:kube-system:cloud-controller-manager kube-system-cluster-admin-4 --clusterrole cluster-admin
kubectl create clusterrolebinding --user system:serviceaccount:kube-system:shared-informers kube-system-cluster-admin-5 --clusterrole cluster-admin
kubectl create clusterrolebinding --user system:kube-controller-manager  kube-system-cluster-admin-6 --clusterrole cluster-admin
```



**What you expected to happen**:
I expect that my Cinder Openstack volume could be mounted in my pod.

**How to reproduce it (as minimally and precisely as possible)**:
- Deploy the `openstack-cloud-provider` in your cluster by running the command `kubectl create -f https://raw.githubusercontent.com/dims/openstack-cloud-controller-manager/master/manifests/controller-manager/openstack-cloud-controller-manager-ds.yaml`
  - I made sure that it works by creating an internal service LoadBalancer and it works fine for me.
  - I should to did a workaround(creating the permissive bind) mentioned before, because my controller wasn't able to access the persistent volume API.
- Create a volume in OpenStack
  - I created it by running the command `openstack volume create walter-test --size 10`, which gave me a volume:
```
+---------------------+------------------------------------------------------------------+
| Field               | Value                                                            |
+---------------------+------------------------------------------------------------------+
| attachments         | []                                                               |
| availability_zone   | cinderAZ_1                                                       |
| bootable            | false                                                            |
| consistencygroup_id | None                                                             |
| created_at          | 2018-03-02T20:17:31.408441                                       |
| description         | None                                                             |
| encrypted           | False                                                            |
| id                  | ea7e96fe-24cb-40f3-9fb3-420ac7ac1752                             |
| multiattach         | False                                                            |
| name                | walter-test                                                      |
| properties          |                                                                  |
| replication_status  | disabled                                                         |
| size                | 10                                                               |
| snapshot_id         | None                                                             |
| source_volid        | None                                                             |
| status              | creating                                                         |
| type                | None                                                             |
| updated_at          | None                                                             |
| user_id             | f0cc6d2bcea9d6fe9c2b68264e7d9343c537323c0243d068a0eb119c05fc3c45 |
+---------------------+------------------------------------------------------------------+
```
 - I've created the following resources:
```
---

apiVersion: ""v1""
kind: ""PersistentVolume""
metadata:
  name: ""walter-test""
spec:
  storageClassName: cinder
  capacity:
    storage: ""5Gi""
  accessModes:
    - ""ReadWriteOnce""
  cinder:
    fsType: ext4
    volumeID: ea7e96fe-24cb-40f3-9fb3-420ac7ac1752

---

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: atmosphere-pv-claim
spec:
  storageClassName: cinder
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

---

apiVersion: v1
kind: ReplicationController
metadata:
  labels:
    name: mongo
  name: mongo-controller
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: mongo
    spec:
      volumes:
        - name: atmosphere-storage
          persistentVolumeClaim:
           claimName: atmosphere-pv-claim
      containers:
      - image: mongo
        name: mongo
        ports:
        - name: mongo
          containerPort: 27017
          hostPort: 27017
        volumeMounts:
            - name: atmosphere-storage
              mountPath: /data/db
```
- Now you can check the pod stucked in ""ContainerCreating"" status
```
ubuntu@walter-atmosphere:~$ kubectl get pods
NAME                     READY     STATUS              RESTARTS   AGE
mongo-controller-5sktj   0/1       ContainerCreating   0          21m
```

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: `dims/openstack-cloud-controller-manager:0.1.0`
- OS (e.g. from /etc/os-release): Ubuntu
- Kernel (e.g. `uname -a`): `Linux walter-atmosphere 4.4.0-116-generic #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux`
- Install tools: `kubeadm`
- Others:


_Copied from original issue: dims/openstack-cloud-controller-manager#81_",closed,False,2018-03-22 10:58:51,2018-04-09 17:50:17
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/17,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/17,Test internal and external LBaaS scenarios with Octavia,"_From @dims on February 7, 2018 18:59_

Test internal and external LBaaS scenarios with Octavia

_Copied from original issue: dims/openstack-cloud-controller-manager#71_",closed,False,2018-03-22 10:59:07,2018-04-12 13:50:54
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/18,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/18,Add unit tests for authentication and authorization hooks,"_From @dims on January 31, 2018 3:40_

PoC is sorely lacking tests

_Copied from original issue: dims/openstack-cloud-controller-manager#53_",closed,False,2018-03-22 10:59:19,2018-03-29 14:43:43
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/19,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/19,Use same configuration loading as cloud provider for provisioner and flex volume driver,"_From @dims on January 29, 2018 23:47_

Each of the executables have command lines and different ways to load cloud config etc. Let's use the one from cloud provider and standardize on it.

_Copied from original issue: dims/openstack-cloud-controller-manager#47_",closed,False,2018-03-22 10:59:34,2018-04-09 17:51:02
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/20,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/20,Document how to run openstack-cloud-controller-manager with hack/local-up-cluster.sh,"_From @dims on January 19, 2018 2:21_

There is support for `EXTERNAL_CLOUD_PROVIDER=true` when you also add `CLOUD_PROVIDER` and `CLOUD_CONFIG` for local-up-cluster.sh, we should document how one can easily build a test/dev environment using local-up-cluster.sh

_Copied from original issue: dims/openstack-cloud-controller-manager#34_",closed,False,2018-03-22 11:00:35,2018-07-18 01:07:20
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/21,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/21,Keystone auth token used for authentication openstack cloudprovider resources,"_From @dims on January 16, 2018 16:30_

see https://github.com/kubernetes/kubernetes/issues/14809

_Copied from original issue: dims/openstack-cloud-controller-manager#27_",closed,False,2018-03-22 11:00:51,2018-08-19 12:39:36
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/22,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/22,Run conformance test suite with openstack-cloud-controller-manager enabled,"_From @dims on January 16, 2018 16:27_

See if we break something?

_Copied from original issue: dims/openstack-cloud-controller-manager#26_",closed,False,2018-03-22 11:01:14,2018-04-09 17:53:18
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/23,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/23,Can we do new scenarios for volumes with Bare Metal?,"_From @dims on January 16, 2018 16:25_

 based on Flex Volume PoC with cinder : https://gist.github.com/dims/ad5afbd657014b0bba17b40228cba32e

_Copied from original issue: dims/openstack-cloud-controller-manager#25_",closed,False,2018-03-22 11:01:29,2018-04-09 17:52:48
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/24,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/24,use secrets to specify the openstack auth info,"_From @dims on January 15, 2018 20:13_

will need this fixed first: https://github.com/dims/openstack-cloud-controller-manager/issues/8

_Copied from original issue: dims/openstack-cloud-controller-manager#13_",closed,False,2018-03-22 11:01:42,2018-08-23 15:16:46
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/25,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/25,RBAC rules needed for running as pod/daemonset,"_From @dims on January 15, 2018 20:0_

hack we can use for now is ... we need a better way

```
# Hack for RBAC for all for the new cloud-controller process, we need to do better than this
cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:default kube-system-cluster-admin-1 --clusterrole cluster-admin
cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:pvl-controller kube-system-cluster-admin-2 --clusterrole cluster-admin
cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:cloud-node-controller kube-system-cluster-admin-3 --clusterrole cluster-admin
cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:cloud-controller-manager kube-system-cluster-admin-4 --clusterrole cluster-admin
cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:shared-informers kube-system-cluster-admin-5 --clusterrole cluster-admin
cluster/kubectl.sh create clusterrolebinding --user system:kube-controller-manager  kube-system-cluster-admin-6 --clusterrole cluster-admin
cluster/kubectl.sh create clusterrolebinding --user system:serviceaccount:kube-system:attachdetach-controller kube-system-cluster-admin-7 --clusterrole cluster-admin
cluster/kubectl.sh set subject clusterrolebinding system:node --group=system:nodes
```

_Copied from original issue: dims/openstack-cloud-controller-manager#12_",closed,False,2018-03-22 11:01:55,2018-03-27 04:12:03
cloud-provider-openstack,zioproto,https://github.com/kubernetes/cloud-provider-openstack/pull/26,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/26,Volunteer to review and collaborate, I'm putting my name out there to help review and maintain this cloud provider.,closed,True,2018-03-22 11:53:22,2018-03-24 00:06:29
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/27,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/27,Add Lingxian Kong(lingxiankong) in reviewer list,"I have been active developer in OpenStack projects (PTL of Qinling project), and we (Catalyst Cloud) are deploying Magnum in our OpenStack based public cloud, we are very interested in the integration between OpenStack and Kubernetes. We are willing to offer help.
",closed,True,2018-03-22 11:59:28,2018-03-24 00:06:12
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/28,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/28,Add code cleanup utils - gofmt and golint,"* Run `make fmt` or `make lint` to run these
* Use hack/update-gofmt.sh to fix problems with fmt
",closed,True,2018-03-22 12:34:24,2018-03-22 18:49:04
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/29,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/29,Volunteer to review and collaborate,"I'm putting my name out there to help review openstack cloud provider.
",closed,True,2018-03-22 14:20:27,2018-03-24 00:05:41
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/pull/30,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/30,add me to owners file,"As asked in slack, here it is :)",closed,True,2018-03-22 14:30:44,2018-03-23 19:06:02
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/31,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/31,Prettify identity code,"This patch makes the code look better and conform to the standards of the go language.

1. All variables var_name were renamed to varName.
2. Comments were added for all public facing entities.
3. Entities that are not used outside of the module were declared as private.
4. Several small typos were fixed as well.
",closed,True,2018-03-22 14:56:39,2018-03-23 19:53:03
cloud-provider-openstack,dklyle,https://github.com/kubernetes/cloud-provider-openstack/pull/32,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/32,Adding dklyle to help review and maintain,"
",closed,True,2018-03-22 15:31:55,2018-03-22 17:00:06
cloud-provider-openstack,hogepodge,https://github.com/kubernetes/cloud-provider-openstack/pull/33,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/33,"Added hogepodge to OWNERS, alphabetized listing","
<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

Added hogepodge to OWNERS and alphabetized contributor list.

**Which issue this PR fixes**

NONE

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```NONE
```",closed,True,2018-03-22 16:14:41,2018-03-29 17:24:35
cloud-provider-openstack,hogepodge,https://github.com/kubernetes/cloud-provider-openstack/pull/34,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/34,Remove MAINTAINERS.md,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

The Kubernetes development community favors OWNERS with approver and reviewer entries over maintainers file and entries. Remove outdated and unmaintained MAINTAINERS.md file.

**Which issue this PR fixes** 

NONE

**Special notes for your reviewer**:

https://github.com/kubernetes/community/blob/master/community-membership.md#maintainer

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```
NONE
```",closed,True,2018-03-22 16:55:51,2018-03-29 17:24:52
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/35,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/35,mrhillsman volunteer as reviewer,Signed-off-by: Melvin Hillsman <mrhillsman@gmail.com>,closed,True,2018-03-22 18:35:03,2018-03-22 19:09:04
cloud-provider-openstack,edisonxiang,https://github.com/kubernetes/cloud-provider-openstack/pull/36,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/36,Add edisonxiang to review and collaborate,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: 
Thanks very much.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-03-23 07:04:54,2018-03-23 18:09:02
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/37,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/37,Add tests for Keystone Authenticator,,closed,True,2018-03-23 17:43:07,2018-03-24 17:27:02
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/38,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/38,Remove useless error handler,"It is a duplicate of another handler, located several lines above. 
Therefore, this code will never be executed.",closed,True,2018-03-23 17:51:19,2018-03-23 21:25:04
cloud-provider-openstack,hogepodge,https://github.com/kubernetes/cloud-provider-openstack/pull/39,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/39,Batch adding OWNERS to prevent merge conflicts,"
<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

We asked new additions to the OWNERS files to submit their additions individually, but with the reviews stuck in constant conflict this patch adds all of the current approvals to one file.

https://github.com/kubernetes/cloud-provider-openstack/pull/10
https://github.com/kubernetes/cloud-provider-openstack/pull/12
https://github.com/kubernetes/cloud-provider-openstack/pull/26
https://github.com/kubernetes/cloud-provider-openstack/pull/27
https://github.com/kubernetes/cloud-provider-openstack/pull/29

**Which issue this PR fixes**

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```
NONE
```",closed,True,2018-03-23 19:20:45,2018-03-29 17:24:28
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/issues/40,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/40,consider using non-root users in container images,"
**Is this a BUG REPORT or FEATURE REQUEST?**: FEATURE

 /kind feature


**What happened**: All this repository containers https://github.com/kubernetes/cloud-provider-openstack/blob/master/cluster/images/ are using root user in docker images. 

**What you expected to happen**: I except that we should not use root user if not really needed. 

**How to reproduce it (as minimally and precisely as possible)**: `docker run -it <image> whoami`

",closed,False,2018-03-23 22:05:38,2018-03-27 04:17:03
cloud-provider-openstack,ZhengGu0304,https://github.com/kubernetes/cloud-provider-openstack/pull/41,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/41,Merge pull request #2 from kubernetes/master,"policy.json is not required for authentication

it's only used when authentication webhook is enabled

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-03-24 02:26:40,2018-03-29 09:57:33
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/pull/42,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/42,use occm without root,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: default user for cloud controller manager 1001 which is not uid of root.

**Which issue this PR fixes**: fixes #40 partly

**Special notes for your reviewer**: 

**Release note**:
```release-note
```
",closed,True,2018-03-24 12:06:28,2018-03-27 04:17:03
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/pull/43,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/43,remove filtering by server status,"**What this PR does / why we need it**: Originally these filtering was removed in https://github.com/kubernetes/kubernetes/pull/59931 but there is still left one function which is used only in ccm.

**Which issue this PR fixes**: fixes #

**Special notes for your reviewer**: This is little bit stupid now, but I was thinking that we need to have similar behavior in this ccm than what we have in normal controller-manager. So I fixed this function that is used in ccm part. Sorry for making this now, it would have been better to implement in same PR.

So this PR means that kubernetes cluster node is not deleted from cluster if the instance still exist in openstack. If instance is deleted from openstack - the node is deleted from cluster as well

**Release note**:

```release-note
NONE
```

/assign anguslees",closed,True,2018-03-24 12:17:41,2018-03-26 01:18:01
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/44,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/44,Barbican as a Kubernetes KMS provider,"Similar to https://github.com/Azure/kubernetes-kms

background research:
- https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
- https://github.com/kubernetes/kubernetes/pulls?utf8=%E2%9C%93&q=is%3Apr+++EncryptionConfig+
- https://github.com/kubernetes/kubernetes/pulls?utf8=%E2%9C%93&q=is%3Apr+++experimental-encryption-provider-config
",closed,False,2018-03-26 13:06:51,2018-12-24 20:45:55
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/45,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/45,Kubernetes RBAC <-> OpenStack Keystone - sync,"Can we write a controller (CRD?) to make it easier to refresh RBAC info from Keystone information?

cc @Fedosin ",closed,False,2018-03-26 13:09:29,2018-06-28 13:10:22
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/46,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/46,Cluster-autoscaler support for OpenStack cloud provider,Track work in https://github.com/kubernetes/autoscaler/issues/734 and serve as a redirect for anyone looking for the support here in this repo,closed,False,2018-03-26 13:16:09,2019-02-07 22:18:30
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/47,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/47,Periodic CI job in OpenStack foundation infrastructure,"Can we test external cloud provider with kubespray in OpenStack infra?

cc @pabelanger ",closed,False,2018-03-26 13:24:37,2018-09-07 17:11:55
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/48,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/48,Enable external cloud provider in Magnum,"cc @strigazi 

magnum currently uses the k/k in-tree cloud provider. we should consider adding a new CI job that uses the code from this repository

http://codesearch.openstack.org/?q=cloud-provider&i=nope&files=&repos=magnum",closed,False,2018-03-26 13:26:37,2018-09-07 17:11:54
cloud-provider-openstack,dixudx,https://github.com/kubernetes/cloud-provider-openstack/pull/49,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/49,add dixudx to OWNERS,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
/assign @dims 
**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
None
```
",closed,True,2018-03-26 14:58:42,2018-03-27 01:29:48
cloud-provider-openstack,gonzolino,https://github.com/kubernetes/cloud-provider-openstack/pull/50,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/50,Remove 'systemctl status' call from devstack plugin,"Calling 'systemctl status' halts execution on systems that have
SYSTEMD_PAGER set or do not use the --no-pager option.

**What this PR does / why we need it**:
Using the k8s-cloud-provider devstack plugin in ubuntu 16.04 (I guess other distributions are affected as well) causes the devstack run to be stuck in a pagination view of `systemctl status docker`.
Since the call shouldn't be needed AFAIK, I propose to remove it.

As an alternative, we could also replace the call with `systemctl --no-pager status docker` to prevent the usage of a pager.",closed,True,2018-03-26 15:30:28,2018-03-27 02:27:03
cloud-provider-openstack,v1k0d3n,https://github.com/kubernetes/cloud-provider-openstack/pull/51,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/51,OWNERS: add user v1k0d3n as reviewer,"This change adds v1k0d3n as a reviewer, if the project is willing.

- v1k0d3n

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: This PR adds a reviewer to the OWNERS file.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes # N/A

**Special notes for your reviewer**: N/A

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```N/A
```
",closed,True,2018-03-26 16:19:42,2018-03-26 18:26:01
cloud-provider-openstack,rootfs,https://github.com/kubernetes/cloud-provider-openstack/pull/52,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/52,add rootfs to OWNERS,"**What this PR does / why we need it**:
add rootfs as volunteer 

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```

@dims 
",closed,True,2018-03-26 17:51:34,2018-03-26 19:47:02
cloud-provider-openstack,grahamhayes,https://github.com/kubernetes/cloud-provider-openstack/pull/53,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/53,WIP: Minor devstack plugin changes,"<!-- Thanks for sending a pull request! -->

# **STILL WIP - will update as I walk through the install**

**What this PR does / why we need it**:

* Swap ``$BASE_DIR/tools/test-setup.sh`` and
  ``$BASE_DIR/tools/install-distro-packages.sh`` so that go is
  installed in time
* Ensure systemctl uses ``--no-pager`` to avoid pauses when running
  interactively
* Move the generation of the bindep package list to the ``$BASE_DIR``

**Special notes for your reviewer**:
*STILL WIP - will update as I walk through the install*

**Release note**:
```NONE
```
",closed,True,2018-03-26 19:58:33,2018-04-08 23:40:45
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/54,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/54,Kubectl exec-based client auth provider for Keystone,"We pulled out code from main repository related to the kubectl client side plugin in this repository:
https://github.com/kubernetes/kubernetes/tree/master/staging/src/k8s.io/client-go/plugin/pkg/client/auth/openstack

However the code needs to be enabled as a exec based auth provider to actually work, see:
https://github.com/kubernetes/kubernetes/pull/59495

Once that is done, we need to remove the code from the main k/k repository.

cc @Fedosin @flaper87 @knikolla ",closed,False,2018-03-27 00:01:28,2018-04-27 19:23:22
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/55,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/55,RBAC info for external cloud provider / CCM,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

CCM has a couple of ClusterRoleBinding/ClusterRole(s) that it needs
for cloud-controller-manager, cloud-node-controller, pvl-controller
and shared-informers service accounts.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: 
Fixes #25

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-03-27 00:29:17,2018-03-27 04:12:03
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/56,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/56,Authorization failed because of int type value returned instead of bool,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
When using k8s-keystone-auth container for authorization, kubectl failed with the error: `Error from server (InternalError): an error on the server (""Internal Server Error: \""/api/v1/namespaces/default/pods?limit=500\"": v1beta1.SubjectAccessReview: Status: v1beta1.SubjectAccessReviewStatus: Allowed: ReadBool: expect t or f,
 but found 1, error found in #10 byte of ...|llowed\"": 1\n  }\n}|..., bigger context ...|mestamp\"": null\n  },\n  \""status\"": {\n    \""allowed\"": 1\n  }\n}|..."") has prevented the request from succeeding (get pods)`

**What you expected to happen**:
After sourced openstack credential, kubectl should work

**How to reproduce it (as minimally and precisely as possible)**:
- k8s-keystone-auth pod is running
- config `--authentication-token-webhook-config-file=/etc/kubernetes/pki/webhookconfig.yaml`, `--authorization-webhook-config-file=/etc/kubernetes/pki/webhookconfig.yaml` and `--authorization-mode=Node,Webhook,RBAC` for kube-apiserver.yaml
- source openstack credential
- kubectl get pods

**Anything else we need to know?**:
no

**Environment**:
- openstack-cloud-controller-manager version: 0ac2e6af2609dfe6f291a3a03991e38eb490fec1
- OS (e.g. from /etc/os-release): 
```bash
NAME=""Ubuntu""
VERSION=""16.04.3 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.3 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
```
- Kernel (e.g. `uname -a`):
```
Linux lingxian-k8s-master 4.4.0-112-generic #135-Ubuntu SMP Fri Jan 19 11:48:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Others: NA
",closed,False,2018-03-27 12:09:07,2018-03-28 16:28:05
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/57,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/57,Update latest k/k - Mar 27,"Sync a couple of commits

70c2baaff8 fix isnotfound
97b4110d9f Stabilize openstack_test when running against real cloud",closed,True,2018-03-27 16:25:33,2018-03-27 22:35:27
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/58,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/58,Add Keystone authorizer tests,,closed,True,2018-03-27 18:25:48,2018-03-28 15:59:01
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/59,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/59,Update master branch to use k/k 1.10,"we are currently vendoring k/k 1.9, we should move to 1.10",closed,False,2018-03-27 23:09:51,2018-04-08 00:05:59
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/60,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/60,Use bool type for 'allowed' field in authorization response,"**What this PR does / why we need it**:
To fix the bug in keystone webhook authorization

**Which issue this PR fixes**:
fixes #56

**Special notes for your reviewer**:
The k8s authorizer is expecting a bool type value from WebhookAuthorizer response.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
NONE
",closed,True,2018-03-27 23:13:08,2018-03-29 07:01:25
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/61,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/61,Update vendored dependencies to new Kubernetes 1.10 release,,closed,True,2018-03-28 02:10:05,2018-03-29 04:48:03
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/issues/62,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/62,Add full example for kubernetes.io/cinder provisioner,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
Add full kubernetes.io/cinder provisioner example, make it easy for beginner to understand the difference with openstack.org/standalone-cinder provisioner.

**What you expected to happen**:
Understand kubernetes.io/cinder and openstack.org/standalone-cinder scenario.

**How to reproduce it (as minimally and precisely as possible)**:
NA

**Anything else we need to know?**:
NA

**Environment**:
NA
",closed,False,2018-03-28 03:32:22,2018-03-28 16:33:06
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/pull/63,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/63,Add full example for kubernetes.io/cinder provisioner,"**What this PR does / why we need it**:
Add kubernetes.io/cinder example, make it easy for beginner to understand the difference with openstack.org/standalone-cinder provisioner.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*:
fixes #62

**Special notes for your reviewer**:
NA

**Release note**:
NA
",closed,True,2018-03-28 03:32:53,2018-03-29 01:59:41
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/64,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/64,Support flexible authorization policy configuration,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
Currently, the authorization policy configuration is not scalable, it's painful for the admin to set the correct rules. For example, it's impossible to config the rule that allow users with a specific role and in a specific project. 

**What you expected to happen**:
As k8s admin, I need flexible authorization policy configuration like the following:
```
[
  {
    ""resource"": {
      ""verb"": [""get"", ""list"", ""watch""],
      ""resource"": ""*"",
      ""version"": ""*"",
      ""namespace"": ""*""
    },
    ""match"": [
      {
        ""type"": ""role"",
        ""value"": [""k8s-admin"", ""k8s-viewer"", ""k8s-editor""]
      },
      {
        ""type"": ""project"",
        ""value"": [""c1f7910086964990847dc6c8b128f63c""]
      },
    ]
  }
]
```

**How to reproduce it (as minimally and precisely as possible)**:
NONE

**Anything else we need to know?**:
NONE

**Environment**:
NONE
",closed,False,2018-03-28 06:05:06,2018-04-03 01:27:06
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/65,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/65,Add Helm charts for examples/manifests,Suggestion from @kfox1111 at https://github.com/kubernetes/cloud-provider-openstack/pull/63#issuecomment-376942870 ,closed,False,2018-03-28 16:11:02,2018-08-29 15:37:54
cloud-provider-openstack,aglitke,https://github.com/kubernetes/cloud-provider-openstack/pull/66,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/66,Add Adam Litke to the OWNERS file,"Hi all.  I just noticed that the standalone-cinder provisioner that I've been maintaining in the kubernetes incubator has been copied over here.  Adding my name to the list of collaborators.

Signed-off-by: Adam Litke <alitke@redhat.com>

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-03-28 17:42:14,2018-03-29 02:33:04
cloud-provider-openstack,hogepodge,https://github.com/kubernetes/cloud-provider-openstack/pull/67,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/67,Add document to help getting started with development,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

This PR outlines basic requirements for setting up a development environment for `cloud-provider-openstack`

**Which issue(s) this PR fixes**: 
Fixes #20 

**Special notes for your reviewer**:

Advice on making this more widely usable, based on your own experiences, are welcome.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```",closed,True,2018-03-28 22:46:33,2018-07-18 01:07:20
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/68,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/68,validator to check openstack connection and functionality,"Similar to https://github.com/cloudfoundry-incubator/cf-openstack-validator for Testing how CF uses OpenStack (fog based ruby code)

example output - http://paste.openstack.org/show/716654/",closed,False,2018-03-28 23:03:05,2018-09-07 14:08:55
cloud-provider-openstack,jianglingxia,https://github.com/kubernetes/cloud-provider-openstack/pull/69,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/69,fix clound to cloud,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
fix clound to cloud
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-03-29 06:48:04,2018-03-29 06:57:05
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/issues/70,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/70,Migrate from glide to dep,"Now we use [Glide](https://github.com/Masterminds/glide) for dependency management, but the project is not actively developing and the authors recommend to migrate to [dep](https://github.com/Masterminds/glide#golang-dep) until it's too late.",closed,False,2018-03-29 14:45:02,2018-04-10 00:10:09
cloud-provider-openstack,hichtakk,https://github.com/kubernetes/cloud-provider-openstack/issues/71,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/71,kubectl fails with openstack auth-provider,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
kubectl fails with openstack auth-provider.

```
$ kubectl get node                                                                                                                                                     
panic: assignment to entry in nil map                                                                                                                                  
                                                                                                                                                                       
goroutine 1 [running]:                                                                                                                                                 
k8s.io/kubernetes/vendor/k8s.io/client-go/plugin/pkg/client/auth/openstack.newOpenstackAuthProvider(0xc420984720, 0x1b, 0x0, 0x2a41500, 0xc4202d74e0, 0x1e08701, 0x6, 0
xc4207a74c8, 0xae9d24)  
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/client-go/plugin/pkg/client/auth/openstack/openstack.go:137 +0x25a
k8s.io/kubernetes/vendor/k8s.io/client-go/rest.GetAuthProvider(0xc420984720, 0x1b, 0xc420255a60, 0x2a41500, 0xc4202d74e0, 0x0, 0x0, 0x0, 0x0)
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/client-go/rest/plugin.go:72 +0x114       
k8s.io/kubernetes/vendor/k8s.io/client-go/rest.(*Config).TransportConfig(0xc42015ba40, 0xc42036f5de, 0x1, 0x0)                       
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/client-go/rest/transport.go:63 +0x66c    
k8s.io/kubernetes/vendor/k8s.io/client-go/rest.TransportFor(0xc42015ba40, 0xc420404900, 0xc42036f5de, 0x1, 0x0)                      
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/client-go/rest/transport.go:40 +0x2f     
k8s.io/kubernetes/vendor/k8s.io/client-go/rest.UnversionedRESTClientFor(0xc42015ba40, 0x0, 0x0, 0x4116ad)                            
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/client-go/rest/config.go:235 +0x9b       
k8s.io/kubernetes/vendor/k8s.io/client-go/discovery.NewDiscoveryClientForConfig(0xc42015af00, 0xc4207829a0, 0x0, 0x0)                
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/client-go/discovery/discovery_client.go:378 +0x9b
k8s.io/kubernetes/pkg/kubectl/cmd/util.(*discoveryFactory).DiscoveryClient(0xc420429c40, 0x41e886, 0xc4209981e0, 0x7f6bd03c37e0, 0x429b40)
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/util/factory_client_access.go:113 +0xa1
k8s.io/kubernetes/pkg/kubectl/cmd/util.(*ring0Factory).DiscoveryClient(0xc420046300, 0xc420297560, 0x7f6bd03d453d, 0x8, 0xab)        
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/util/factory_client_access.go:186 +0x34
k8s.io/kubernetes/pkg/kubectl/cmd/util.(*ring1Factory).CategoryExpander(0xc420046330, 0xc420046330, 0xc4209981e0)                    
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/util/factory_object_mapping.go:113 +0x66
k8s.io/kubernetes/pkg/kubectl/cmd/util.(*ring2Factory).NewBuilder(0xc420429c60, 0x0)                                                 
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/util/factory_builder.go:156 +0x16b     
k8s.io/kubernetes/pkg/kubectl/cmd/util.(*factory).NewBuilder(0xc420046360, 0xd411031)                                                
        <autogenerated>:1 +0x3d  
k8s.io/kubernetes/pkg/kubectl/resource.(*Builder).Flatten(...)    
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/resource/get.go:235                    
k8s.io/kubernetes/pkg/kubectl/cmd/resource.(*GetOptions).Run(0xc4200de840, 0x2a6f640, 0xc420046360, 0xc42017dd40, 0xc4201602c0, 0x1, 0x1, 0x0, 0x0)
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/resource/get.go:247 +0xfd              
k8s.io/kubernetes/pkg/kubectl/cmd/resource.NewCmdGet.func1(0xc42017dd40, 0xc4201602c0, 0x1, 0x1)                                     
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/resource/get.go:149 +0x115             
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute(0xc42017dd40, 0xc420160260, 0x1, 0x1, 0xc42017dd40, 0xc420160260) 
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:603 +0x234    
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0xc420174000, 0x5000107, 0x0, 0xffffffffffffffff)                
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:689 +0x2fe    
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute(0xc420174000, 0xc420046360, 0x2a42440)                            
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:648 +0x2b     
k8s.io/kubernetes/cmd/kubectl/app.Run(0x0, 0x0)                   
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubectl/app/kubectl.go:41 +0xd5                    
main.main()                      
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubectl/kubectl.go:27 +0x26                        
```

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:
Run these commands bellow according to [keystone auth document](https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-keystone-webhook-authenticator-and-authorizer.md)
```
kubectl config set-credentials openstackuser --auth-provider=openstack
kubectl config set-context --cluster=kubernetes --user=openstackuser openstackuser@kubernetes
kubectl config use-context openstackuser@kubernetes
```

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):  
   Ubuntu 16.04.3 LTS
- Kernel (e.g. `uname -a`):  
   4.4.0-112-generic
- Install tools:
- Others:  
   kubectl v1.9.2
",closed,False,2018-03-30 02:42:08,2018-04-06 09:52:36
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/issues/72,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/72,Create cinder volume failed with default availability zone 'nova',"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
create cinder volume failed against a specific cloud provider due to invalid parameters caused by incorrect availability zone, because the default value of 'nova' [1] in this repository doesn't match the real case in cloud provider.

[1] https://github.com/kubernetes/cloud-provider-openstack/blob/master/pkg/volume/cinder/volumeservice/actions.go#L75

**What you expected to happen**:
create cinder volume succeed by change the default value of availability zone to empty.

**How to reproduce it (as minimally and precisely as possible)**:
1. install openstack cinder standalone service
2. create a k8s cluster via local-up-cluster.sh
3. run the cinder-provisioner binary built from this repository
4. run test from this repository, kubectl apply -f examples/persistent-volume-provisioning/cinder/cinder-full.yaml

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: latest
- OS (e.g. from /etc/os-release): Ubuntu 16.04.4 LTS
- Kernel (e.g. `uname -a`): Linux 4.4.0-116-generic
- Install tools: git, make
- Others:
",closed,False,2018-03-30 02:52:51,2018-03-31 12:25:03
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/pull/73,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/73,Change default az to empty when create cinder volume,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #72 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-03-30 03:03:46,2018-04-02 01:40:56
cloud-provider-openstack,AnalogJ,https://github.com/kubernetes/cloud-provider-openstack/issues/74,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/74,"workaround for ""x509: failed to load system roots and no roots provided"" issue on CoreOS","/kind bug

**What happened**:
When deploying Kubernetes on CoreOS using `kubeadm` and a config file specifying `cloud-provider: openstack` the controller-manager will consistently die with the following error (from `docker logs`):

```
error building controller context: cloud provider could not be initialized: could not init cloud provider ""openstack"": Post https://URL/v2.0/tokens: x509: failed to load system roots and no roots provided
```

On CoreOS the `/etc/ssl/certs` files are all symlinks to `/usr/share/ca-certificates`

When the kuberenets controller container runs it seems unable to actually read the files in that directory, and throws an error

**What you expected to happen**:
The controller manager does not throw an error and die. 

**How to reproduce it (as minimally and precisely as possible)**:
```
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration

# pass in the openstack configuration to Kubernetes.
cloudProvider: openstack
apiServerExtraArgs:
    cloud-provider: openstack
    cloud-config: /etc/cloud/bootstrap/cloud.conf
apiServerExtraVolumes:
- name: oscloudcfg
  hostPath: /etc/cloud/bootstrap/
  mountPath: /etc/cloud/bootstrap/

controllerManagerExtraArgs:
    cloud-provider: openstack
    cloud-config: /etc/cloud/bootstrap/cloud.conf
controllerManagerExtraVolumes:
- name: oscloudcfg
  hostPath: /etc/cloud/bootstrap/
  mountPath: /etc/cloud/bootstrap/
```


**Anything else we need to know?**:
The workaround is to add the following sections to your `kubeadm.conf` file:

```
apiServerExtraVolumes:
- name: ca-certs
  hostPath: /usr/share/ca-certificates/
  mountPath: /etc/ssl/certs/
controllerManagerExtraVolumes:
- name: ca-certs
  hostPath: /usr/share/ca-certificates/
  mountPath: /etc/ssl/certs/
```

**Environment**:
- openstack-cloud-controller-manager version: N/A
- OS (e.g. from /etc/os-release): 

```NAME=""Container Linux by CoreOS""
ID=coreos
VERSION=1632.3.0
VERSION_ID=1632.3.0
BUILD_ID=2018-02-14-0338
```

- Kernel (e.g. `uname -a`):

Linux kubestack-controller0 4.14.19-coreos #1 SMP Wed Feb 14 03:18:05 UTC 2018 x86_64 Intel Core Processor (Haswell, no TSX) GenuineIntel GNU/Linux

- Install tools: kubeadm
- Others:
",closed,False,2018-03-30 16:32:59,2018-09-10 10:14:46
cloud-provider-openstack,ghugo,https://github.com/kubernetes/cloud-provider-openstack/pull/75,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/75,Add ghugo to review and collaborate,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Adds ghugo (gagehugo) to the list of reviewers/approvers.

**Special notes for your reviewer**:
I've been following the kubernetes/keystone webhook for a while and as a core contributor to keystone I would like to help review and contribute to this project.",closed,True,2018-03-30 19:35:19,2018-04-09 02:23:50
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/76,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/76,Update .zuul.yaml with correct job name,Signed-off-by: Melvin Hillsman <mrhillsman@gmail.com>,closed,True,2018-03-30 21:34:07,2018-03-31 02:33:04
cloud-provider-openstack,liu-sheng,https://github.com/kubernetes/cloud-provider-openstack/pull/77,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/77,Add more testing scenarios jobs against K8S+OpenStack,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
- Add jobs trigger for triggering all the jobs by commenting ""/retest"" or ""/test all"" under a PR
- Add jobs trigger for triggering unit tests job by commenting ""/test cloud-provider-openstack-unit"" under a PR
- Add jobs trigger for triggering K8S+LBaaS(Octavia) scenario testing by commenting ""/test cloud-provider-openstack-lb"" under a PR
- Add jobs trigger for triggering K8S+keystone authentication/authorization scenario testing by comment ""/test cloud-provider-openstack-auth"" under a PR

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-03-31 07:20:24,2018-04-05 17:02:08
cloud-provider-openstack,jianglingxia,https://github.com/kubernetes/cloud-provider-openstack/issues/78,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/78,standalone-cinder-provisioner deployment create pod status CrashLoopBackOff not running,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug
> Uncomment only one, leave it on its own line: 
 /kind bug


**What happened**:
I deploy a cluster in bare-metal enviroment,then create the deployment named standalone-cinder-provisioner,but created pod not running and status CrashLoopBackOff! is the image problem?
the image is me use :docker pull quay.io/external_storage/standalone-cinder-provisioner:latest
the deployment yaml:

apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: standalone-cinder-provisioner
  labels:
    app: standalone-cinder-provisioner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: standalone-cinder-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: standalone-cinder-provisioner
    spec:
      containers:
      - name: standalone-cinder-provisioner
        image: ""quay.io/external_storage/standalone-cinder-provisioner:latest""
        imagePullPolicy: IfNotPresent
        env:
        - name: OS_CINDER_ENDPOINT
          value: http://10.1.101.1:8776/v2
        - name: OS_USERNAME
          value: k8s
        - name: OS_TENANT_NAME
          value: kubernetes

[root@localhost:/home/ubuntu/yaml-baremetal]$ kubectl get po
NAME                                             READY     STATUS             RESTARTS   AGE
standalone-cinder-provisioner-6797c5db6b-rtjqc   0/1       CrashLoopBackOff   19         1h

the kubelet log:

I0403 02:33:00.122888   20622 kuberuntime_manager.go:739] checking backoff for container ""standalone-cinder-provisioner"" in pod ""standalone-cinder-provisioner-6797c5db6b-rtjqc_default(87835eee-3699-11e8-bfec-4c09b4b0c25b)""
I0403 02:33:00.122967   20622 kuberuntime_manager.go:749] Back-off 5m0s restarting failed container=standalone-cinder-provisioner pod=standalone-cinder-provisioner-6797c5db6b-rtjqc_default(87835eee-3699-11e8-bfec-4c09b4b0c25b)
E0403 02:33:00.122995   20622 pod_workers.go:182] Error syncing pod 87835eee-3699-11e8-bfec-4c09b4b0c25b (""standalone-cinder-provisioner-6797c5db6b-rtjqc_default(87835eee-3699-11e8-bfec-4c09b4b0c25b)""), skipping: failed to ""StartContainer"" for ""standalone-cinder-provisioner"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=standalone-cinder-provisioner pod=standalone-cinder-provisioner-6797c5db6b-rtjqc_default(87835eee-3699-11e8-bfec-4c09b4b0c25b)""

**What you expected to happen**:
the deployment created pod can running
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:k8sv1.8.5
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-04-02 10:25:19,2018-04-05 18:50:09
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/79,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/79,Keystone authorization improvement,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
- Support flexible authorization policy configuration like the following:
  ```
  [
    {
      ""resource"": {
        ""verbs"": [
          ""get"",
          ""list"",
          ""watch""
        ],
        ""resources"": [
          ""pods""
        ],
        ""version"": ""*"",
        ""namespace"": ""default""
      },
      ""match"": [
        {
          ""type"": ""role"",
          ""values"": [
            ""k8s-admin"",
            ""k8s-viewer"",
            ""k8s-editor""
          ]
        },
        {
          ""type"": ""project"",
          ""values"": [
            ""c1f7910086964990847dc6c8b128f63c""
          ]
        }
      ]
    }
  ]
  ```

- Change the example policy file and the doc accordingly

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #64

**Special notes for your reviewer**: None
",closed,True,2018-04-02 23:35:47,2018-04-03 03:16:43
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/issues/80,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/80,"Get creating vm as TargetNode, that cause TestRoutes case failed","**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
In unit test case TestRoutes, if the first vm in vm list is creating status, the vm do not allocate IP address yet, that will cause ErrNoAddressFound in following logic.

https://github.com/kubernetes/cloud-provider-openstack/blob/master/pkg/cloudprovider/providers/openstack/openstack_routes_test.go#L50

**What you expected to happen**:
Use running vm as TargetNode, for example: the vm that unit test is running in.

**How to reproduce it (as minimally and precisely as possible)**:
It happen in OpenLab integration tests, it is test result and log.
http://logs.openlabtesting.org/logs/77/77/f26a6e3b931cabf01e68eea02c93b64fd3d11908/cloud-provider-openstack-all/cloud-provider-openstack-unittest/3f9e57e/

**Anything else we need to know?**:
NA

**Environment**:
- openstack-cloud-controller-manager version: master
",closed,False,2018-04-03 03:10:13,2018-04-03 22:48:52
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/issues/81,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/81,TestVolumes failed caused by volume status did not change to expected status after 30s,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
Run TestVolumes failed caused by volume status did not change to expected status after 30s, however according to the logs [1][2], we found it doesn't wait at all to check the expected volume status and execute post commands immediately rather than wait for 30s to check status. Wonder if this a test case bug.

[1] http://80.158.23.49/logs/5/5/4c8c41c009fb5543116c1130f37de49f7eed2556/cloud-provider-openstack-all/cloud-provider-openstack-unittest/7d1c865/job-output.txt.gz
[2] http://80.158.23.49/logs/5/5/133543ee3dd987b533f8f62fd98f99d8a995377e/check-generic-cloud/cloud-provider-openstack-unittest/27e5859/job-output.txt.gz

**What you expected to happen**:
Wait 30s to check the expected volume status.

**How to reproduce it (as minimally and precisely as possible)**:
Run TestVolumes unit test

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-04-03 05:09:34,2018-04-03 22:48:41
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/82,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/82,Fix reliability of TestRoutes and TestVolumes,"In TestRoutes, let's make sure we get an active instance and use
that. If the instance is not active, then it will not have an ip address
and hence fail.

In TestVolumes, we should actually wait for a total of 30 seconds. We
are bailing out in one second right now. Make sure we sleep for the
initial delay before we start as well.
",closed,True,2018-04-03 12:53:50,2018-04-04 01:52:47
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/83,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/83,Run depend during `make check`,"looks like something changed in the CI system, so we need to make sure we pull the dependencies before we run `make check`",closed,True,2018-04-03 20:33:53,2018-04-03 22:46:44
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/84,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/84,Better environment vars support for standalone cinder service,"during some refactoring, lost support for OS_CINDER_ENDPOINT, so adding
that back and also pick up other env vars.",closed,True,2018-04-04 02:13:50,2018-04-04 02:26:07
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/85,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/85,getConfig should work when config file is absent,"When the config file is not present, we should just use what is in the
env variables. during some refactoring, we lost that ability.",closed,True,2018-04-04 03:46:12,2018-04-04 08:49:08
cloud-provider-openstack,gonzolino,https://github.com/kubernetes/cloud-provider-openstack/pull/86,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/86,Migrate from Glide to dep for dependency management,"**What this PR does / why we need it**:
This PR migrates from Glide to dep as dependency management tool as suggested by the Glide authors: https://github.com/Masterminds/glide#golang-dep

There is still work to be done for the migration to finish, e.g. remove glide.yaml and glide.lock, remove usage of glide in all scripts, etc.

I replaced all usages of glide in the Makefile, but I think this still needs thorough testing. 

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #70 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Migrate from Glide to dep as dependency management tool.
```",closed,True,2018-04-04 16:43:10,2018-04-10 00:10:09
cloud-provider-openstack,j-griffith,https://github.com/kubernetes/cloud-provider-openstack/pull/87,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/87,Migrate from Glide to Dep,"This change moves off of Glide and on to Dep and makes the
necessary changes to the Makefile as well.

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR moves off of Glide (as per Glide teams recommendations) and on to dep

**Which issue this PR fixes** 
fixes #70 

**Special notes for your reviewer**:
This should be a ""drop in"" replacement / upgrade.  This change includes the necessary updates to the Makefile.  We shouldn't need any special args for `go test` or `ensure` other than what's been added.  I've verified we run the same tests in the case of unit and coverage runs but I could certainly have an error in there for a keen reviewer to catch.


```release-note
```
",closed,True,2018-04-04 19:48:02,2018-04-04 21:07:36
cloud-provider-openstack,yanndegat,https://github.com/kubernetes/cloud-provider-openstack/issues/88,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/88,external network identified as internal,"/kind feature


**What happened**:
public ipv4 node address identified as Internal 

**What you expected to happen**:
public ipv4 node address identified as External 

**How to reproduce it (as minimally and precisely as possible)**:
Don't name your openstack public network ""public""
Create an instance with an IP on this network
",closed,False,2018-04-05 13:21:26,2018-04-09 17:40:15
cloud-provider-openstack,yanndegat,https://github.com/kubernetes/cloud-provider-openstack/issues/89,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/89,Filter instances ipv4/6 addresses,"/kind feature


**What happened**:
my nodes are identified either by their ipv4 addr or their ipv6, according 
to the numerical order of the adresses.
Once added to the the nodeAddresses list, the type of the addr is lost.

**What you expected to happen**:
something more consistent & at least either ipv4 addrs or ipv6 addrs

**How to reproduce it (as minimally and precisely as possible)**:
setup an openstack network with ipv4 & ipv6 support such as your instances
get 2 addresses.
",closed,False,2018-04-05 13:28:30,2018-04-09 17:40:03
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/90,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/90,Update latest from k8s master apr 5,Sync with master,closed,True,2018-04-05 18:13:15,2018-04-05 18:31:08
cloud-provider-openstack,j-griffith,https://github.com/kubernetes/cloud-provider-openstack/issues/91,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/91,CSI Plugin needs to check for existing volumes with same name on create,"
kind bug

**What happened**:
The CSI spec states that the CreateVolume call is idempotent and should not create duplicate volumes if/when called multiple times with the same Volume Name.  The current implementation does not check for existing Volumes with the specified Name and thus allows creation of duplicate named Volumes.

**What you expected to happen**:
The CSI plugin should respond with the existing Volume and NOT create a new Volume.

**How to reproduce it (as minimally and precisely as possible)**:
Run the CSI Plugin and issue a create call multiple times with the same name, then check out the Cinder List and you'll see multiple volumes with the same name.

**Anything else we need to know?**:
PR is in progress by jgriffith

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-04-05 20:30:32,2018-04-09 17:38:10
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/92,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/92,[WIP] Dummy PR for testing CI jobs,Just testing...,closed,True,2018-04-05 21:33:37,2018-04-08 23:40:05
cloud-provider-openstack,yanndegat,https://github.com/kubernetes/cloud-provider-openstack/pull/93,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/93,Add support for networking opts,"- allows to customize public network name
  according to the cloud provider

Fixes #88

/assign @dims 

**Special notes for your reviewer**:

first impl. 
don't know where to update the docs

",closed,True,2018-04-06 07:58:25,2018-04-06 09:11:26
cloud-provider-openstack,yanndegat,https://github.com/kubernetes/cloud-provider-openstack/pull/94,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/94,Add support for networking opts,"- allows to customize public network name
  according to the cloud provider
- allows disabling ipv6 at config level

Fixes #89 #88

/assign @dims 
",closed,True,2018-04-06 09:10:12,2018-04-09 10:55:41
cloud-provider-openstack,flaper87,https://github.com/kubernetes/cloud-provider-openstack/issues/95,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/95,Allow for the CSI cinder driver to interact with nodes not managed by OpenStack,"<!-- This form is for bug reports and feature requests! -->

/kind feature

The CSI driver requires the Kubernetes cluster to be running in an OpenStack cloud. This expectation leaves out scenarios, where Kubernetes is not running as part of a tenant in an OpenStack managed VM.

It should be possible to refactor the plugin in a way that we can provide support for both, instance and BM, volume attachments either as part of an OpenStack tenant or not.

The goal is to support the following scenarios:

Kubernetes running side-by-side with OpenStack:
- 2 separate clouds (hardware) that can talk to each other.
- Kubernetes has access to the OpenStack cloud and wants to use Cinder for volume management

Kubernetes running inside an OpenStack VM:
- This is the scenario that is currently supported

Kubernetes running on a BM node as part of an OpenStack tenant:
- This may work with the current implementation but I haven't fully tested it.

It's possible that we may need some changes in the current CSI implementation to be able to gather more information about the target node in advance.See this issue for more info: https://github.com/container-storage-interface/spec/issues/216

That said, I believe this could be implemented, perhaps not in the most ideal way, with what we have today in gophercloud and the CPO implementation.",closed,False,2018-04-06 11:52:28,2018-11-01 11:17:15
cloud-provider-openstack,AdamDang,https://github.com/kubernetes/cloud-provider-openstack/pull/96,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/96,Correct the returned err message,"Line 192:  return nil, fmt.Errorf(""volume directory: %v does not exists"", targetMountDir)
does not exists->does not exist",closed,True,2018-04-06 12:19:17,2018-05-01 03:01:44
cloud-provider-openstack,warmchang,https://github.com/kubernetes/cloud-provider-openstack/pull/97,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/97,Fix the path to the cinder sample.,"**What this PR does / why we need it**: Fix the path to the cinder sample.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-04-06 14:52:09,2018-04-07 15:44:09
cloud-provider-openstack,j-griffith,https://github.com/kubernetes/cloud-provider-openstack/pull/98,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/98,Check for existing names on csi CreateVolume,"**What this PR does / why we need it:

The CSI spec states that it's up to the Plugin to enforce idempotency
on Create and that we do NOT create duplicate named volumes if the same
name is specified in a subsequent Create request.

This patch adds some things to check that:
1. A GetVolumesByName function to openstack_volumes
2. A check in CreateVolume prior to issuing Create to
   see if any volumes with the given Name exist prior
   to Create

If we find a single volume with the given Name we return it.

If we find multiple volumes with the given Name we don't really
have enough info currently to *know* what the right thing is, so
we return an Error.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` 
fixes #91 

**Special notes for your reviewer**:

",closed,True,2018-04-06 16:43:49,2018-04-09 17:38:11
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/99,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/99,copy doc from main repo,"from
https://github.com/kubernetes/website/blob/master/docs/concepts/cluster-administration/cloud-providers.md
",closed,True,2018-04-06 18:38:35,2018-04-06 19:11:10
cloud-provider-openstack,jianglingxia,https://github.com/kubernetes/cloud-provider-openstack/issues/100,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/100,the way cinder-endpoint to deploy provisioner that the image standalone-cinder-provisioner not support,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug
> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:
I use the cinder-endpoint way to deploy the provisioner,but found the image that quay.io/external_storage/standalone-cinder-provisioner:latest not updated and not supproted! then I plan to make a image use the master branch code,and clone the cloud-provider-openstack code then command the make,but found need glide,then install glide and make again,but found the log,can help me or I maybe wrong in some place,thanks

[INFO]	--> Fetching k8s.io/apiextensions-apiserver
[INFO]	--> Fetching k8s.io/kube-openapi
[INFO]	--> Fetching k8s.io/kubernetes
[WARN]	Unable to checkout golang.org/x/net
[ERROR]	Update failed for golang.org/x/net: Cannot detect VCS
[WARN]	Unable to checkout golang.org/x/crypto
[ERROR]	Update failed for golang.org/x/crypto: Cannot detect VCS
[WARN]	Unable to checkout golang.org/x/sys
[ERROR]	Update failed for golang.org/x/sys: Cannot detect VCS
[WARN]	Unable to checkout golang.org/x/text
[ERROR]	Update failed for golang.org/x/text: Cannot detect VCS
[WARN]	Unable to checkout google.golang.org/genproto
[ERROR]	Update failed for google.golang.org/genproto: Cannot detect VCS
[WARN]	Unable to checkout golang.org/x/time
[ERROR]	Update failed for golang.org/x/time: Cannot detect VCS
[WARN]	Unable to checkout google.golang.org/grpc
[ERROR]	Update failed for google.golang.org/grpc: Cannot detect VCS

**What you expected to happen**:
can provider a new image ?
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-04-08 09:22:35,2018-04-09 17:39:11
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/issues/101,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/101,"Support to launch specified binary in local-up-cluster.sh, like: cinder-provisioner and so on","**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
We have several binaries in repo:
- openstack-cloud-controller-manager
- cinder-provisioner
- cinder-csi-plugin
- cinder-flex-volume-driver
- k8s-keystone-auth

Now local-up-cluster.sh only support to launch and config **openstack-cloud-controller-manager**, is there any plan to support launching all of these above binaries in local-up-cluster.sh? That will help OpenLab to reduce the complexity of automation script, like: https://github.com/theopenlab/openlab-zuul-jobs/blob/master/playbooks/cloud-provider-openstack-acceptance-test-keystone-authentication-authorization/run.yaml#L86-L88 and https://github.com/theopenlab/openlab-zuul-jobs/blob/master/playbooks/cloud-provider-openstack-acceptance-test-keystone-authentication-authorization/run.yaml#L92-L98

**What you expected to happen**:
We can control to launch specified binary with some environment variables in local-up-cluster.sh, like: EXTERNAL_CLOUD_PROVIDER_BINARY do now.

**How to reproduce it (as minimally and precisely as possible)**:
NA

**Anything else we need to know?**:
NA

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-04-08 10:02:01,2018-09-06 13:44:56
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/102,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/102,Add conformance e2e test pipeline,"We had our first e2e all green run here:
https://github.com/kubernetes/cloud-provider-openstack/pull/92#issuecomment-379556696

",closed,True,2018-04-08 15:32:55,2018-04-09 15:32:09
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/103,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/103,Create a periodic job for e2e conformance test and upload results to test grid,"Now that we have the e2e conformance test run against PR(s), we need a CI job that runs the same thing periodically (6 hours? 12 hours?) Here are some notes:

- We need a periodic job for Kubernetes/Kubernetes master against cloud-provider-openstack master (so we know if we break something)
- We need a periodic job for Kubernetes/Kubernetes release-1.10 branch against cloud-provider-openstack master (se we make sure we don't break any one who wants to use our code against the last release)
- Need to create a GCS bucket and upload files for each run, so we can display the results in kubernetes test-grid. See details in [1]

Nits:
- test_results.html is empty as the junit xml file is under kubernetes directory (it's junit_01.xml)

Links:
- [1] https://docs.google.com/document/d/1lGvP89_DdeNO84I86BVAU4qY3h2VCRll45tGrpyx90A/edit?ts=5a8486c1",closed,False,2018-04-08 22:06:22,2018-04-11 00:14:56
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/104,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/104,Standardize docker image repo location,"Let's use a single repo for all the docker images from cloud-provider-openstack:
https://hub.docker.com/u/k8scloudprovider/dashboard/",closed,True,2018-04-09 02:11:35,2018-04-10 10:11:09
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/105,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/105,Need CI jobs to build and push docker images to a central repository,How about this? https://hub.docker.com/u/k8scloudprovider/dashboard/,closed,False,2018-04-09 02:13:11,2018-09-06 19:50:56
cloud-provider-openstack,ghugo,https://github.com/kubernetes/cloud-provider-openstack/pull/106,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/106,Add ghugo as reviewer,"**What this PR does / why we need it:**
Adds ghugo (gagehugo) to the list of reviewers/approvers.

**Special notes for your reviewer:**
I've been following the kubernetes/keystone webhook for a while and as a core contributor to keystone I would like to help review and contribute to this project.",closed,True,2018-04-09 02:29:59,2018-04-10 08:42:10
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/pull/107,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/107,Add periodic jobs for E2E conformance tests,"**What this PR does / why we need it**:

Add periodic jobs for E2E conformance tests
- test against kubernetes master
- test against kubernetes latest release(v1.10.0)

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #103 

**Special notes for your reviewer**:
@dims 

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-04-10 01:56:05,2018-04-11 06:18:38
cloud-provider-openstack,flaper87,https://github.com/kubernetes/cloud-provider-openstack/issues/108,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/108,Document what languages plugins can be written in,"This issue is to discuss (and then document) our willingness to host plugins that are written in other programming languages.

There are cases when using a different programming language would make the implementation cleaner, simple and more stable. An example of this can be found in the following discussion related to the CSI plugin: https://github.com/kubernetes/cloud-provider-openstack/issues/95#issuecomment-380024052

- Is it ok for us to host/test code that is not Go?
- Would the current members and reviewers feel comfortable with maintaining such code?
- Can we provide testing pipelines for such code?

Starting this issue to have a broader discussion on this topic. I'll make sure to document the consensus.
",closed,False,2018-04-10 08:55:43,2018-09-07 16:10:55
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/pull/109,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/109,Add pull policy for driver-registrar image to support local image built from latest source code,"Fix #110 
Add pull policy for driver-registrar image to support local image built from latest source code",closed,True,2018-04-10 11:21:18,2018-04-11 03:13:33
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/issues/110,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/110,Add pull policy for driver-registrar image to support local image built from latest source code,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
/kind feature


**What happened**:
Currently, during the OpenLab CI job the driver-registrar image is always pulled from remote(docker hub) which may be obsolete against the source code where it is built from, so this issue wants to add a pull policy to support local image built from latest source code.

**What you expected to happen**:
We can keep using the latest image in OpenLab CI job.

**How to reproduce it (as minimally and precisely as possible)**:

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-04-10 11:36:32,2018-04-10 20:27:11
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/pull/111,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/111,Keep pipeline job and comment consistent,"**What this PR does / why we need it**:
In Kubernetes/OpenStack integration scenario, we should keep pipeline
name, job name and trigger comment consistence, that build consist user
experience for developer from k8s community, and easy to understand how
to trigger the specific jobs when the job failed.

- update all of pipeline name, job name and trigger comment for k8s jobs
- run all of jobs in separate pipeline when PR commit and update, and
  report to github independently
- branch-v1.10.0 should be just branch-v1.10

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes theopenlab/openlab#46

**Special notes for your reviewer**:
@dims 

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-04-11 08:19:03,2018-04-13 06:39:09
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/112,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/112,Squash unnecessary ERROR prefixes in logs,"we were getting some bad prefixes to actual logging because of the way flags are setup. just squash them. 

```
ERROR: logging before flag.Parse: W0411 08:27:21.065165   52319 client_config.go:533] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
ERROR: logging before flag.Parse: W0411 08:27:21.065296   52319 client_config.go:538] error creating inClusterConfig, falling back to default config: unable to load in-cluster configuration, KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PO
```",closed,True,2018-04-11 13:39:51,2018-04-11 15:05:55
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/113,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/113,Tolerate unknown sections / variables,"We have a few data structures to load Configuration, until we have
a single package, let's make sure we can read a cloud conf file with
extra sections. (example cinder csi plugin fails when we feed it a
cloud.conf with LoadBalancer section)
",closed,True,2018-04-11 17:52:01,2018-04-12 02:00:58
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/114,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/114,OpenStack Octavia based ingress controller,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
It could be a great addon to have an ingress controller option implemented based on Octavia service for the OpenStack cloud provider, especially those who already deployed Octavia, just like how GKE does [here](https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer). FYI, as [Octavia official doc](https://docs.openstack.org/octavia/latest/reference/introduction.html) said: Octavia will fully replace Neutron LBaaS as the load balancing solution for OpenStack.

**What you expected to happen**:
Deploy an ingress-controller with OpenStack Octavia as backend.

**How to reproduce it (as minimally and precisely as possible)**:
None

**Anything else we need to know?**:
None

**Environment**:
None
",closed,False,2018-04-11 22:56:54,2018-04-30 23:47:18
cloud-provider-openstack,jianglingxia,https://github.com/kubernetes/cloud-provider-openstack/issues/115,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/115,why the os-initialize_connection Initiator not same VM caused the volume always attaching,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:
the standalone cinder provisioner created the pv and volume,but the volume always attaching,I found the cinder api.log is the os-initialize_connection Initiator not same VM! the place I not corrected ?thanks!

[root@localhost:/etc/iscsi]$ kubectl get po
NAME                                             READY     STATUS    RESTARTS   AGE
standalone-cinder-provisioner-5d85c99899-w6vx7   1/1       Running   0          1h

[root@localhost:/etc/iscsi]$ kubectl get sc
NAME                 PROVISIONER
standard (default)   openstack.org/standalone-cinder

[root@localhost:/etc/iscsi]$ kubectl get secret
NAME                    TYPE                                  DATA      AGE
default-token-dl44v     kubernetes.io/service-account-token   3         10d
standard-cephx-secret   kubernetes.io/rbd                     1         1d

[root@localhost:/etc/iscsi]$ kubectl get pvc
NAME              STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
keystone-sc-pvc   Bound     pvc-b4b87e3b-3e72-11e8-a4fc-4c09b4b0c25b   1Gi        RWO            standard       37m

the cinder volume status:
![image](https://user-images.githubusercontent.com/25783555/38668534-b5ae370c-3e76-11e8-99a2-4010e7f6b164.png)

the cinder api.log
![image](https://user-images.githubusercontent.com/25783555/38668621-e173e210-3e76-11e8-87f6-253c78e1de27.png)

and the cinder volume.log
![image](https://user-images.githubusercontent.com/25783555/38668757-2f2fcd20-3e77-11e8-8397-bd32c536eabd.png)
![image](https://user-images.githubusercontent.com/25783555/38668795-45ced8b4-3e77-11e8-97b2-75ffe7d7238a.png)
![image](https://user-images.githubusercontent.com/25783555/38668993-cdd8843a-3e77-11e8-9f07-3277083ab767.png)

**What you expected to happen**:
the volume attached the vm success
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**: k8sv1.8.5
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-04-12 09:35:55,2018-09-15 13:15:19
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/116,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/116,Remove unnecessary MAINTAINERS.md file,since we are using OWNERS file we don't need this file,closed,True,2018-04-12 20:32:52,2018-04-13 01:17:59
cloud-provider-openstack,edisonxiang,https://github.com/kubernetes/cloud-provider-openstack/pull/117,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/117,bump csi manifests to v0.2.0,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Since kubernetes csi images has bumped to v0.2.0,
we should update them in csi cinder manifests.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-04-13 03:12:40,2018-04-13 09:49:00
cloud-provider-openstack,kartoch,https://github.com/kubernetes/cloud-provider-openstack/issues/118,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/118,Filter main ip address(es),"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:

Goal: have a way to configure Openstack cloud provider to use a specific address for each node.

More precisely, following kubernetes/kubernetes#62163 and kubernetes/cloud-provider-openstack#89, it is now possible to disable IPv6 addresses. But what if a node has two (or more) IPv4 and/or IPv6 addresses ? For instance let's have the following configuration:

```
  status:
    addresses:
    - address: 10.1.0.45
      type: InternalIP
    - address: 2001:41d0:302:1100::a:1720
      type: InternalIP
    - address: 54.38.91.250
      type: InternalIP
    - address: k8s-worker-1
      type: Hostname
```

The order of addresses is based on comparison between the address string values. So the first one (10.1.0.45) is ideally the main address used by kubernetes.

But it is more problematic for the following ones:

```
  status:
    addresses:
    - address: 15.13.37.45
      type: InternalIP
    - address: 192.168.27.34
      type: InternalIP
    - address: k8s-worker-1
      type: Hostname
```

Even if there is no IPv6 address, the public address is used as the kubernetes main address, which is something not really secure, especially if there is a private network.

**What you expected to happen**:

Openstack cloud provider must be provide a way to force that a specific address is the main address.",closed,False,2018-04-13 08:19:33,2018-04-13 12:13:44
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/pull/119,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/119,[DNM] OpenLab integration job test PR,"Please do not merge the PR, we need a PR to trigger jobs, it will be closed when everything looks good.",closed,True,2018-04-13 09:12:26,2018-04-17 02:55:26
cloud-provider-openstack,flaper87,https://github.com/kubernetes/cloud-provider-openstack/pull/120,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/120,Enter DEST dir before running dep ensure,"If we don't enter the `$(DEST)` dir, `dep` will fail saying that the `cloud-provider-openstack` dir is not in one of the known `GOPATH/src` paths.

This commit also forces the creation of the symlink, otherwise it'll fail saying that a symlink exists already.",closed,True,2018-04-13 12:16:18,2018-04-25 05:16:03
cloud-provider-openstack,flaper87,https://github.com/kubernetes/cloud-provider-openstack/pull/121,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/121,Allow for the CSI cinder driver to interact with nodes not managed by OpenStack,"This PR addresses #95 

Sending it as a WIP to be able to discuss the architecture and implementation as things evolve.

**Release note**:

```release-note
```
",closed,True,2018-04-13 12:20:52,2018-10-08 14:58:02
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/122,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/122,Running Kubernetes IPv6 CI tests on OpenStack cloud,"Based one from @leblancd 

Let's start with the conformance test suite and see what it would take to enable IPv6 based job
- We need to check if vexxhost can support IPv6 in the VM(s) we use 
   * seems like a possibility looking at http://logs.openlabtesting.org/logs/19/119/4800ccab78624506960c42559b61a48a7360c473/cloud-provider-openstack-acceptance-test-e2e-conformance/cloud-provider-openstack-acceptance-test-e2e-conformance/8c6cbbd/zuul-info/zuul-info.ubuntu-xenial-vexxhost.txt
- If we can't use t, we will have to use devstack VM (and enable kuryr?)
- Next, they use kubeadm with following config, we have to translate this to local-up-cluster (need to check if images built from master is good enough too)
```
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
kubernetesVersion: 1.10.0
api:
  advertiseAddress: fd00::100
networking:
  serviceSubnet: fd00:1234::/110
unifiedControlPlaneImage: diverdane/hyperkube-amd64:v1.9.0-beta.0.ipv6.2
tokenTTL: 0s
nodeName: kube-master
```
- After that they use the following snippet to run tests:
```
export KUBECONFIG=/home/openstack/.kube/config
export KUBE_MASTER=local
export KUBE_MASTER_IP=""[fd00:1234::1]:443""
export KUBERNETES_CONFORMANCE_TEST=n
cd $GOPATH/src/k8s.io/kubernetes
go run hack/e2e.go -- --provider=local --v --test --test_args=""--host=https://[fd00:1234::1]:443 --ginkgo.focus=Networking|Services --ginkgo.skip=IPv4|Networking-Performance|Federation|preserve\ssource\spod|session\saffinity:\sudp|functioning\sNodePort --num-nodes=2""
```
",closed,False,2018-04-13 13:48:41,2018-11-14 11:23:43
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/issues/123,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/123,add support to proxy protocol v1 in octavia lbaas,"**Is this a BUG REPORT or FEATURE REQUEST?**: FEATURE

/kind feature


**What happened**: Currently when we create lbaas resources, in 100% of use-cases at least I am using protocol TCP if my server is listening https. This is really problem if we want to get end-user ip address through to POD. 

**What you expected to happen**: I except that lbaas could forward end-user ip address somehow to pod. This can be done using proxy protocol in octavia lbaas (and then pod should have proxy protocol support as well, then it will parse the ip address from the headers). 




",closed,False,2018-04-16 08:43:18,2018-09-13 10:25:49
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/124,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/124,Create a function to receive keystone tokens on the client side,"This code adds new function GetToken inside ""keystone"" module that should be used
in the executable file for the client's credential exec plugin [1].

It allows to work in both interactive and non-interactive modes.
In the first case, the user will be prompted to enter data from the console,
in the second the data will be read from the related environment variables.

[1] https://kubernetes.io/docs/admin/authentication/#client-go-credential-plugins
",closed,True,2018-04-16 14:42:18,2018-04-27 15:43:34
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/125,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/125,Remove DEST variable redeclaration in the Makefile,,closed,True,2018-04-16 19:47:55,2018-04-18 23:12:00
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/126,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/126,Create an executable to receive keystone tokens on the client side,"This code adds new executable that should be used as a command for the client's
credential exec plugin [1].

It allows to work in both interactive and non-interactive modes.
In the first case, the user will be prompted to enter data from the console,
in the second the data will be read from the related environment variables.

[1] https://kubernetes.io/docs/admin/authentication/#client-go-credential-plugins

fixes #54 ",closed,True,2018-04-17 19:55:35,2018-04-27 19:23:22
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/pull/127,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/127,Switch to new OpenLab unit test job in release-1.9 branch,"**What this PR does / why we need it**:
openstack-cloud-controller-manager-unittest have been deprecated
in OpenLab side, use cloud-provider-openstack-unittest to instead,
all of these job name have been updated in master, only release-1.9
branch should be updated. This change do not impact real code of
cloud-provider-openstack.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
Related-Bug: theopenlab/openlab#21

**Special notes for your reviewer**:
@dims ",closed,True,2018-04-18 09:15:22,2018-04-19 01:19:26
cloud-provider-openstack,gonzolino,https://github.com/kubernetes/cloud-provider-openstack/pull/128,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/128,Small fixes in controller-manager manifests,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Fix some small issues in controller-manager manifests

DaemonSet manifest:
- Remove double hostNetwork attribute
- Set nodeSelector to only schedule pods on master nodes

Pod manifest:
- Set correct component label",closed,True,2018-04-19 12:01:25,2018-04-25 00:48:02
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/issues/129,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/129,Remove duplicate extraction of a key from connectionInfo dict in flexvolume functionality,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
The connection_info key has been extracted twice [1][2] in flexvolume functionality code, which leads to an interface conversion error:
panic: interface conversion: interface {} is nil, not map[string]interface {}

[1] https://github.com/kubernetes/cloud-provider-openstack/blob/master/pkg/flexvolume/cinder_client.go#L115
[2] https://github.com/kubernetes/cloud-provider-openstack/blob/master/pkg/flexvolume/cinder_client.go#L120

**What you expected to happen**:
The flexvolume functionality works correctly.
**How to reproduce it (as minimally and precisely as possible)**:
Call the flexvolume mount method.

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-04-20 06:23:06,2018-04-27 01:46:19
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/pull/130,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/130,Remove duplicate extraction of a key from connectionInfo dict in flexvolume functionality,"volume functionality

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #129 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-04-20 06:33:38,2018-04-27 01:46:19
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/issues/131,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/131,Add cinder flexvolume test example,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
/kind feature


**What happened**:
There is no cinder flexvolume test example, we should add one.
**What you expected to happen**:
There is no cinder flexvolume test example, we should add one.
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-04-23 08:36:48,2018-05-02 10:12:18
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/pull/132,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/132,Add cinder flexvolume test example,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #131 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-04-23 08:41:55,2018-05-03 01:44:25
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/133,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/133,Add octavia-ingress-controller,"**What this PR does / why we need it**:
An ingress controller implementation based on OpenStack Octavia service. A demo: <https://youtu.be/ASSUMDvH_aE>

**Which issue this PR fixes**: 
fixes #114

**Special notes for your reviewer**:
This is the initial commit that can only be used for PoC purpose, a lot of things need to be improved such as:

- Add documentation for how to deploy and test the ingress controller.
- The unit test is missing
- Add functional test
- TLS support
- Service name support

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Add OpenStack(Octavia) based ingress controller implementation.
```
",closed,True,2018-04-23 13:10:45,2018-05-01 00:02:21
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/pull/134,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/134,Add cinder flexvolume test job,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Add a job to check cinder flexvolume functionality for each PR.
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes theopenlab/openlab#33

**Special notes for your reviewer**:
Depends on #130 #132 
**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-04-24 02:27:55,2018-05-04 00:14:54
cloud-provider-openstack,j-griffith,https://github.com/kubernetes/cloud-provider-openstack/pull/135,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/135,Enable building without ln into k8s.io directory,"
**What this PR does / why we need it**:
This PR modifies the Makefile a bit to give us some flexibility and lay some groundwork for future improvements.  The current Makefile does some work to set a GOPATH and then link the current directory in to a k8s.io directory.  This mostly works, but there can be some problems if you for example aren't in a valid gopath, or if you have multiple development repos being worked on simultaneously.

What this change does is instead takes out the GOPATH setup and symbolic linking, and rather requires that you have the code checked out in a valid GOPATH.  We check for this condition in the Makefile and inform the user with an error if this is not the case.

One other thing we do is move all of the built binaries into a single _out directory, just to help with organization a bit.  This update will also allow us to move the build process into a Container more easily if we decide we'd like to go that way.
",closed,True,2018-04-24 15:46:57,2018-04-26 04:42:55
cloud-provider-openstack,j-griffith,https://github.com/kubernetes/cloud-provider-openstack/issues/136,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/136,Add Snapshot support to Cinder/CSI," /kind feature

We're getting close to a consensus for the Snapshots spec in CSI:
    https://github.com/container-storage-interface/spec/pull/207

Of course we'll want to implement this in the CSI driver here when it lands, but also in the meantime we'll need to implement the provider layer as well.

So, might as well start implementing an openstack_snapshots pkg and at least roughing in the CSI implementation. 

",closed,False,2018-04-24 20:32:56,2018-10-21 07:54:12
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/pull/137,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/137,DoNotMerge K8s job benchmark for PR 133,"We need this PR to demonstrate the OpenLab k8s jobs are behaving normally on kubernetes/cloud-provider-openstack master branch, since there might be a bug in `make` progress brought by PR #133 , and the k8s job logs under this PR can be helpful to identify the errors and correct them.

Also I reproduced the erros locally, here are the logs for `make openstack-cloud-controller-manager` on master branch and #133 in the same environment respectively, hoping to be helpful
master : http://paste.openstack.org/show/719881/
#133 : http://paste.openstack.org/show/719883/",closed,True,2018-04-25 07:31:54,2018-04-28 08:52:30
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/138,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/138,Disable test-lb-octavia openlab job,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

Test environment is flaky. details at https://github.com/theopenlab/openlab/issues/54

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-04-25 12:07:37,2018-04-25 19:06:03
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/139,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/139,[WIP][DNM] Octavia ingress controller,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

/hold

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-04-25 12:23:44,2018-04-26 01:10:56
cloud-provider-openstack,xmik,https://github.com/kubernetes/cloud-provider-openstack/issues/140,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/140,This node <name> is registered without the cloud taint. Will not process.,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug


**What happened**:
A k8s node was initialized, but then cloud-provider-openstack says that it will not process:
```
ERROR: logging before flag.Parse: I0425 15:47:59.280128 1 openstack.go:628] Claiming to support Zones
ERROR: logging before flag.Parse: I0425 15:47:59.280101 1 node_controller.go:344] Adding node label from cloud provider: beta.kubernetes.io/instance-type=312048105
ERROR: logging before flag.Parse: I0425 15:47:59.553145 1 node_controller.go:371] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/region=RegionOne
ERROR: logging before flag.Parse: I0425 15:48:00.355632 1 node_controller.go:392] Successfully initialized node k8s-testing-workstation-master-1 with cloud provider
ERROR: logging before flag.Parse: I0425 15:48:00.355658 1 node_controller.go:297] This node k8s-testing-workstation-master-1 is registered without the cloud taint. Will not process.
ERROR: logging before flag.Parse: I0425 15:48:00.357463 1 node_controller.go:297] This node k8s-testing-workstation-master-1 is registered without the cloud taint. Will not process.
```

**What you expected to happen**:
I expected cloud-provider-openstack to proceed after successful initialization of a node.

**How to reproduce it (as minimally and precisely as possible)**:
1. Deployed new k8s cluster (1 master node, 1 worker node).  Essential Pods are running: kube-apiserver, kube-scheduler, kube-controller-manager, kube-proxy. They are managed by kubelet, their manifests are in `/etc/kubernetes/manifests`.
1. Add taints to all 2 nodes:
```
$ kubectl taint nodes k8s-testing-workstation-master-1 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
node ""k8s-testing-workstation-master-1"" tainted
$ kubectl taint nodes k8s-testing-workstation-worker-1 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
node ""k8s-testing-workstation-worker-1"" tainted
$ kubectl describe node | grep Taints
Taints:             node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
Taints:             node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
```
2. Add a Pod manifest for cloud-provider-openstack into `/etc/kubernetes/manifests`, so that it is managed by kubelet too.
4. The Pod with cloud-provider-openstack is running. Log messages as above. And the taints are unset:
```
$ kubectl describe node | grep Taints
Taints:             <none>
Taints:             <none>
```

**Anything else we need to know?**:
Commandline options set:
```
containers:
  - name: openstack-cloud-controller-manager
    image: docker.io/k8scloudprovider/openstack-cloud-controller-manager:latest
    args:
      - /bin/openstack-cloud-controller-manager
      - --bind-address=0.0.0.0
      - --address=0.0.0.0
      - --cluster-name=k8s-testing
      - --cluster-cidr=10.33.0.0/16
      - --cloud-config=/srv/kubernetes/openstack.conf
      - --cloud-provider=openstack
      - --kubeconfig=/etc/kubernetes/master-kubeconfig.yaml
      - --kube-api-content-type=application/vnd.kubernetes.protobuf
      - --leader-elect=false
      - --profiling=false
      - --port=10253
      - --secure-port=0
      - --use-service-account-credentials=false
      - --v=2
      - --loglevel=2
```


**Environment**:
- openstack-cloud-controller-manager version:
```
/bin/openstack-cloud-controller-manager --version
Kubernetes v0.0.0-master+$Format:%h$
```
from docker image: `docker.io/k8scloudprovider/openstack-cloud-controller-manager:latest`.(Is there any tagged version?)
- OS (e.g. from /etc/os-release): Ubuntu 16.04
- Kernel (e.g. `uname -a`):  `Linux k8s-testing-workstation-master-1 4.4.0-45-generic #66-Ubuntu SMP Wed Oct 19 14:12:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
`
- Install tools: Private saltstack files + terraform
- Others:
   * no authorization/authentication set
   * k8s 1.10.1
",closed,False,2018-04-25 16:12:49,2018-05-17 13:26:07
cloud-provider-openstack,j-griffith,https://github.com/kubernetes/cloud-provider-openstack/pull/141,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/141,Add Snapshots to CSI Provider,"**What this PR does / why we need it**:
This PR adds the implementation for Snapshots support to the CSI/OpenStack provider.  CSI will hopefully be merging Snapshot support soon, and we'll want to update the CSI driver accordingly.

First though, we need to add Snapshot support to the OpenStack Provider for the CSI plugin.  We simply add Create, Delete and List and update our Interface for the OpenStack package.  We also add the new functions to the openstack_mock so that when we add the CSI Controller implementation we can add our tests.

**Which issue this PR fixes** *
partial #136 

**Special notes for your reviewer**:
This just exposes the Snapshot functions we need in Gophercloud

Rather than create another abstraction layer over the response types, I'm just leveraging the Snapshot type from Gophercloud directly.

",closed,True,2018-04-25 18:38:39,2018-04-27 01:40:18
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/142,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/142,Enforce running Makefile from $DEST,"We have had several problems:
- folks checking out to a random directory outside their GOPATH
- CI jobs checking out directory to github.com/kubernetes/xyz

both resulting in problems with build, problems updating dependencies, sometimes things work on the dev environment and fail in CI causing folks to scratch their heads.

So let's standardize on the best practice ""this git repo should be checked out under $GOPATH/src/k8s.io"" and enforce it. Makefile will bail out when the directory is not correct (either because we checked out to the wrong repo, bad symlinks or wrong PWD)",closed,True,2018-04-25 23:01:20,2018-04-26 16:54:02
cloud-provider-openstack,j-griffith,https://github.com/kubernetes/cloud-provider-openstack/pull/143,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/143,Provide a hack a script to run make in a Container,"**What this PR does / why we need it**:

This just adds a simple hack/make.sh script to run make directives in a
Docker container.  This means you can run build, test, vet etc on a
repod checked out into any directory on any machine that has Docker.
You aren't required to have a Golang environment or dev environment
setup on your system.

Some may find this useful to simplify the development and test process. 
It's not as useful for dev environments because you presumably will need
a proper Go env to do any dev work, but it's a very lightweight and easy
way to provide a testing environment or to generate builds without any
pre-requisites.
",closed,True,2018-04-26 16:39:12,2018-04-27 15:17:31
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/144,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/144,Setup a CI job to push latest images to dockerhub,"have `make images` to build images, we need to push them probably when our conformance tests pass.

https://hub.docker.com/u/k8scloudprovider/

",closed,False,2018-04-27 17:08:25,2018-06-29 08:11:31
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/145,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/145,Support OpenStack clouds with Keystone Federated Authentication,Please see https://github.com/gophercloud/gophercloud/issues/966,closed,False,2018-04-27 21:15:06,2018-09-24 23:02:53
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/pull/146,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/146,DoNotMerge test jobs after refactoring,,closed,True,2018-04-28 08:58:31,2018-05-04 18:10:45
cloud-provider-openstack,AdamDang,https://github.com/kubernetes/cloud-provider-openstack/pull/147,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/147,Typo fix: commited->committed,"Line 126: commited->committed
",closed,True,2018-04-30 15:25:20,2018-05-01 02:00:50
cloud-provider-openstack,AdamDang,https://github.com/kubernetes/cloud-provider-openstack/pull/148,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/148,Typo fix: occured->occurred,occured->occurred,closed,True,2018-05-01 05:13:00,2018-05-11 09:37:15
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/149,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/149,Document how to setup octavia ingress controller,"**Is this a BUG REPORT or FEATURE REQUEST?**: /kind bug

**What happened**: the octaiva ingress controller doc is missing

**What you expected to happen**: None

**How to reproduce it (as minimally and precisely as possible)**: None


**Anything else we need to know?**: None

**Environment**: None
",closed,False,2018-05-01 23:00:32,2018-05-09 02:54:53
cloud-provider-openstack,jsafrane,https://github.com/kubernetes/cloud-provider-openstack/issues/150,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/150,CSI driver should not rely on device path reported by OpenStack,"/kind bug

**What happened**:
I'm running CSI driver from this repo and my volume got attached as `/dev/vdc`, while CSI driver returned `DevicePath: /dev/vdb` as attachment metadata.

This volume cannot be mounted into a pod because `NodePublish` can't find the volume.

**What you expected to happen**:
The driver either reports DevicePath or it's able to find the volume on the node without trusting `DevicePath`.

**How to reproduce it (as minimally and precisely as possible)**:
I don't know what I did to OpenStack to attach the volume as /dev/vdc instead of /dev/vdb (there is no `vdb` device), but since that happened I can reproduce the bug reliably by just creating a pod.

**Environment**:
- openstack-cloud-controller-manager version:
  I don't have access to the actual servers...

```
# nova  version-list
Client supported API versions:
Minimum version 2.1
Maximum version 2.53

Server supported API versions:
+------+-----------+----------------------+-------------+---------+
| Id   | Status    | Updated              | Min Version | Version |
+------+-----------+----------------------+-------------+---------+
| v2.0 | SUPPORTED | 2011-01-21T11:33:21Z |             |         |
| v2.1 | CURRENT   | 2013-07-23T11:33:21Z | 2.1         | 2.38    |
+------+-----------+----------------------+-------------+---------+

# cinder version-list
Client supported API versions:
Minimum version 3.0
Maximum version 3.40

Server supported API versions:
+------+------------+----------------------+-------------+---------+
| Id   | Status     | Updated              | Min Version | Version |
+------+------------+----------------------+-------------+---------+
| v1.0 | DEPRECATED | 2016-05-02T20:25:19Z |             |         |
| v2.0 | SUPPORTED  | 2014-06-28T12:20:21Z |             |         |
| v3.0 | CURRENT    | 2016-02-08T12:20:21Z | 3.0         | 3.15    |
+------+------------+----------------------+-------------+---------+

```



- OS (e.g. from /etc/os-release): Red Hat Enterprise Linux Server 7.5
- Kernel (e.g. `uname -a`): 3.10.0-862.el7.x86_64
- Install tools: manually
",closed,False,2018-05-02 10:53:48,2018-09-29 13:21:07
cloud-provider-openstack,j-griffith,https://github.com/kubernetes/cloud-provider-openstack/issues/151,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/151,Cinder package has its own openstack pkg implementation,"
/kind improvement

Currently we seem to have 3 implementations of OpenStack providers in the repo:
./cloudprovider/providers/openstack
./csi/cinder/openstack
./provider/openstack

That means we're duplicating a number of things unnecessarily, not to mention there are some bug fixes/updates that don't propagate to all of the packages.

Ideally, we'd have a single top level provider package (provider/openstack) and Cinder plugins would import and use those instead of having their own implementations.  Same for the cloudprovider/provider/openstack, so perhaps consider cleaning up and improving the provider/openstack package and having each of the sub packages import/use that base.

",closed,False,2018-05-02 14:53:52,2018-12-14 16:12:14
cloud-provider-openstack,j-griffith,https://github.com/kubernetes/cloud-provider-openstack/issues/152,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/152,Restructure Cinder plugins,"/kind improvement
Looking at the structure of the Cinder CSI packages, trying to utilize the same plugin/package for both the standalone Cinder case and the full Cloud use case is a bit wonky.  It may be worth considering doing some consolidation and restructuring of the package to leverage shared code where possible and keep the different use cases explicitly unique.
",closed,False,2018-05-02 14:56:35,2018-06-12 17:09:25
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/153,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/153,Allow to read username and password from env variables,"Now, in case of interactive session, users have to enter their
logins and passwords every time in the interactive sessions.
This is not very convenient, so it's better to allow to store
this data in env variables and ask to enter the values only if
they are unset.

This commit adds two new flags for the client-keystone-auth executable:
--user-name and --password, which are defaulted to OS_USERNAME and
OS_PASSWORD respectively.

Addtionally, this commit fixes several small issues:
1. Since Stdout is redirected the executable cannot write data there,
   when the user is asked for his login or password. Therefore, Stderr
   should be used for the output in the interactive sessions.
2. The Exec plugin expects to get expiresAt data in RFC3339Nano format,
   otherwise it is ignored. So the executable converts it before printing.
3. Domain name was defaulted to the string ""default"". But it's better to
   get this value from OS_DOMAIN_NAME env variable.
",closed,True,2018-05-02 20:54:24,2018-05-14 13:46:25
cloud-provider-openstack,dixudx,https://github.com/kubernetes/cloud-provider-openstack/pull/154,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/154,add cloud-provider-openstack-format job,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Add a new test to

* run gofmt checking
* checking normal typos

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
Depends on theopenlab/openlab-zuul-jobs#200
/assign dims

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
add cloud-provider-openstack-format job
```
",closed,True,2018-05-03 07:31:21,2018-05-15 11:12:27
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/155,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/155,Makefile targets for cross build and distributions,"example:
`VERSION=0.1.0 make build-cross dist`

The command line above should build a bunch of tars and zips that can
be uploaded when we cut a release.

Code was borrowed from https://github.com/kubernetes/helm/blob/master/Makefile",closed,True,2018-05-03 14:44:50,2018-05-04 23:44:52
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/156,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/156,Switch to latest gophercloud,Moving both the main k/k repo and this repo to latest ,closed,True,2018-05-03 20:31:10,2018-05-04 23:36:52
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/157,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/157,Improve the config option for ingress controller,"**What this PR does / why we need it**:
Make the ingress controller configuration more readable

**Special notes for your reviewer**:
Before this change, the config file looks like:
```yaml
kubernetes:
    apiserverHost:
    kubeConfig: /etc/kubernetes/ingress-openstack.conf
openStack:
    username: demo
    password: password
    projectID: ${project_id}
    authURL: ${auth_url}/v3
    region: RegionOne
octavia:
    subnetID: ${subnet_id}
    allocateFloatingIP: true
    floatingIPNetwork: ${public_net_id}
```

After this change, it will be:
```yaml
kubernetes:
    apiserverHost:
    kubeconfig: /etc/kubernetes/ingress-openstack.conf
openstack:
    username: demo
    password: password
    project_id: 02c5e04d91b44987bb791d74a36654d4
    auth_url: http://10.0.19.151/identity/v3
    region: RegionOne
octavia:
    subnet_id: bfe392ad-aa5b-464b-acfa-7bb476e66bc6
    fip_network: 6e361672-2e8e-4273-b1cf-fbbef4be59e7
```
",closed,True,2018-05-04 06:09:42,2018-05-09 03:52:22
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/pull/158,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/158,Use default volume type in k8s cinder test,Closes theopenlab/openlab#59,closed,True,2018-05-04 07:53:26,2018-05-04 10:05:51
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/159,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/159,octavia-ingress-controller image build task is missing,"**Is this a BUG REPORT or FEATURE REQUEST?**: 
/kind bug

**What happened**:
octavia-ingress-controller image build task is missing in Makefile

**What you expected to happen**:
We should build image for octavia-ingress-controller just like other services.
",closed,False,2018-05-04 13:19:56,2018-05-07 10:31:52
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/160,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/160,Add octavia-ingress-controller image build task in Makefile,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Add octavia-ingress-controller image build task in Makefile

**Which issue this PR fixes**: 
fixes #159
",closed,True,2018-05-04 13:20:45,2018-05-07 10:36:45
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/161,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/161,Add doc for octavia-ingress-controller,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
octavia-ingress-controller doc is missing

**Which issue this PR fixes** :
fixes #149
",closed,True,2018-05-04 13:43:12,2018-05-09 03:52:32
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/162,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/162,Create OWNERS file in pkg/ingress,"As the author of octavia-ingress-controller, I'm willing to maintain this folder and keep improving the controller.",closed,True,2018-05-04 13:56:34,2018-05-07 10:22:00
cloud-provider-openstack,angao,https://github.com/kubernetes/cloud-provider-openstack/pull/163,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/163,Typo fix: fix some word error,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-05-07 06:57:09,2018-05-07 10:04:29
cloud-provider-openstack,mnaser,https://github.com/kubernetes/cloud-provider-openstack/pull/164,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/164,"Revert ""Disable test-lb-octavia openlab job""","**What this PR does / why we need it**:
Add CI coverage again

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
Fixes theopenlab/openlab#54

**Special notes for your reviewer**:
We run the infra behind this :)
This reverts commit c4027dc07560fa5f460566d4a154bcf1beb5dc60.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-05-07 20:45:29,2018-05-07 20:52:41
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/165,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/165,"Revert ""Disable test-lb-octavia openlab job""","Thanks to Mohammed Naser <mnaser@vexxhost.com>

This reverts commit c4027dc07560fa5f460566d4a154bcf1beb5dc60.


",closed,True,2018-05-07 20:52:19,2018-05-11 09:58:25
cloud-provider-openstack,angao,https://github.com/kubernetes/cloud-provider-openstack/pull/166,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/166,Typo fix: change occured to occrred,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-05-08 03:39:17,2018-05-08 05:45:25
cloud-provider-openstack,openstacker,https://github.com/kubernetes/cloud-provider-openstack/pull/167,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/167,Support env variables for more arguments,"Magnum team is try to integrate k8s-keystone-auth when creating
k8s cluster so that user can get keystone authN and authZ for k8s
out of box. However, Magnum is not running kubelet on master(for
flannel network driver) or doesn't allow running workingload on
master(for calico network driver). So we need a good way (env
variables) to pass in those necessary arguments. This patch adds
the env variables support for those must-have arguments.

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-05-09 23:24:21,2018-05-10 02:30:54
cloud-provider-openstack,openstacker,https://github.com/kubernetes/cloud-provider-openstack/pull/168,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/168,Fix ldflags format for build,"Currently, make build doesn't work for go1.9.1 and possibly other
older versions, user will see error like below when buiding:

//////////////////////////////////////////////////////////
CGO_ENABLED=0 GOOS=linux go build \
        -ldflags '""-w -s -X 'main.version=c206dbe2'""' \
        -o k8s-keystone-auth \
        cmd/k8s-keystone-auth/main.go
flag provided but not defined: -w -s -X main.version
......
Makefile:88: recipe for target 'k8s-keystone-auth' failed
make: *** [k8s-keystone-auth] Error 2
/////////////////////////////////////////////////////////

It ""works"" with go1.10. However, with the binary built on go1.10,
when run the command like './k8s-keystone-auth -v 1', you will see
the version is empty, because -ldflags is not well parsed due to
the outer single quotation marks. The patch removes the outer
single quotation mark to make it fully works for different go versions.

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-05-10 00:04:05,2018-05-10 02:38:54
cloud-provider-openstack,s0komma,https://github.com/kubernetes/cloud-provider-openstack/issues/169,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/169,after adding cloud config kubelet wont start ,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
Hi Guys, im trying to setup k8s persistent storage using cinder.
my k8s version is `1.9.4`
followed all the steps
we added below details on all the nodes of k8s including master and worker 
`--cloud-provider=openstack --cloud-config=/etc/kubernetes/cloud_config` --> the added to kubelet, controller and api server on all master nodes and only for kubelet on worker nodes (edited)
after doing so kubelet wont start

Kubelet logs are below
`May 10 20:10:00 control-plane-256024810-1-359235278 kubelet[12070]: I0510 20:10:00.036796   12070 server.go:305] Successfully initialized cloud provider: ""openstack"" from the config file: ""/etc/kubernetes/cloud_config""
May 10 20:10:00 control-plane-256024810-1-359235278 kubelet[12070]: I0510 20:10:00.036907   12070 openstack_instances.go:39] openstack.Instances() called
May 10 20:10:00 control-plane-256024810-1-359235278 kubelet[12070]: error: failed to run Kubelet: failed to get instances from cloud provider
May 10 20:10:00 control-plane-256024810-1-359235278 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
May 10 20:10:00 control-plane-256024810-1-359235278 systemd[1]: kubelet.service: Unit entered failed state.`

tried to see if nova list and cinder list are working from the vm and it does with same credentials
 
**What you expected to happen**:
Kubelet to come up 
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: using in tree 
- OS (e.g. from /etc/os-release): NAME=""Ubuntu""
VERSION=""16.04.3 LTS (Xenial Xerus)""
- Kernel (e.g. `uname -a`): inux control-plane-256024810-1-359235278 4.4.0-89-generic #112-Ubuntu SMP Mon Jul 31 19:38:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
- Others:
",closed,False,2018-05-10 20:38:37,2018-10-13 23:01:07
cloud-provider-openstack,j-griffith,https://github.com/kubernetes/cloud-provider-openstack/issues/170,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/170,Cinder has two modes and thus needs two CSI plugins,"**BUG/Community Discussion**:

We need two plugins for the standalone (cinderlite) and normal full Cloud (cinder) cases.
The differences in the Controller itself are very minimal; we SHOULD be able to just
make an abstraction in the client implementation.

NOTE there's a real problem here if we try and use the brick local-attach extension.
It didn't occur to me initially, but after Flaper ran into some of this it became clear.
This opens up a couple of problems:
1. Node needs access to Cinder control plane
2. Controler Publish can't be called resulting to difficulty with the abstraction

Current thoughts:

* Create a client package specifically for working with Cinder (both Cinder and Cinderlite)

* Node and Controller servers should be independent but don't have to be, we do need to
  distinguish here, keep in mind the model I'm working off of is a centralized controller
  and individual Node servers.  This also means that Publish is a 2 step process; first
  being the controller request which gathers the needed attach info (connection_info),
  followed by the NodePublish request to finalize things and make the actual connection on
  the Node.

  This is all light weight enough that we could just bundle/distribute in a single pkg
  but I'm not completely clear yet on how this is advertised back up to the CO (ie how to
  ensure my csi plugin running on a Node will NOT accept controller requests).  I'm unclear
  on this because in the current form I don't see a way to specify when launching that the
  pkg is a Node vs Control server but I suspect I'm just missing somethign obvious.

* Create a cinder and a cinderlite plugin
  cinder being the traditional full stack, and cinderlite being standalone; as mentioned
  above the controller can likely be shared, but the Node servers will be unique. 

Problems that this opens up:
1. Making connections on the nodes!!
   - Should we write our own pkg for volume connections?
   - Hack the brick extension to do what we want here?
     Could seperate the attach call and connection interests and provide a
     new call that takes the connection_info as input (I think this is the 
     right answer currently)
   - Reserve some ability to pass this info to the pod (ie kubevirt etc)

2. Keeping as much common code in the pkgs as possible
   - There's just enough difference between cinder and cinderlite from a
     CSI perspective to make it annoying.  We could use an interface model
     with a common impl and abstract the deltas out via a golang inteface?
   - Rely on the client impl for all of the controller differences and have
     unique Node servers
   - The only thing that makes me hesitant with using the client impl for a
     shared controller codebase is what the future might introduce to complicate
     things (ie don't want to revisit this again in 6 months)

Proposal: 
  Start working on the above suggested approach/refactor.  Initially we should probably
  do this as an independent pkg in the existing repo (or an external repo maybe?), and
  keep it as WIP status.  This means supporting the existing code that's there, this
  hopefully won't be a terrible challenge as CSI adoption is still early.
",closed,False,2018-05-11 16:49:35,2018-11-08 17:07:42
cloud-provider-openstack,openstacker,https://github.com/kubernetes/cloud-provider-openstack/pull/171,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/171,Improve the doc of k8s-keystone-auth,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-05-14 23:34:17,2018-05-16 09:58:29
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/172,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/172,Sync latest master to cpo may 14,Sync a few commits from master that did not make it here yet,closed,True,2018-05-14 23:48:58,2018-05-15 10:25:27
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/173,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/173,"PR #59323, fix bug and remove one api call, add node util dependency ",Picking up another commit that was missing from our repo,closed,True,2018-05-15 12:19:40,2018-05-15 18:50:29
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/174,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/174,Document what OpenStack authorizations are needed for the account specified in cloud config,"- Ability to run `openstack endpoint list`
- Ability to run `openstack service list`

( the above 2 are from https://github.com/kubernetes/cloud-provider-openstack/issues/169#issuecomment-388476937 )",closed,False,2018-05-15 13:54:49,2018-11-26 09:03:09
cloud-provider-openstack,piersharding,https://github.com/kubernetes/cloud-provider-openstack/issues/175,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/175,standalone-cinder-provisioner fails to allocate PersistentVolumeClaim,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 

/kind bug


**What happened**:
standalone-cinder-provisioner failed to allocate claim from a standalone Cinder instance (stuck in Pending state).
I can see the standalone-cinder-provisioner pickup the pvc create request:
```
I0515 19:48:28.573000       1 controller.go:492] Starting provisioner controller f04d44f3-5878-11e8-ad2a-b65d335b448a!
I0515 19:51:50.381817       1 controller.go:1167] scheduleOperation[lock-provision-default/myclaim[68960194-5879-11e8-9631-7cfe903a4aff]]
I0515 19:51:50.383588       1 controller.go:1167] scheduleOperation[lock-provision-default/myclaim[68960194-5879-11e8-9631-7cfe903a4aff]]
I0515 19:51:50.393835       1 leaderelection.go:156] attempting to acquire leader lease...
I0515 19:51:50.398264       1 leaderelection.go:178] successfully acquired lease to provision for pvc default/myclaim
I0515 19:51:50.398350       1 controller.go:1167] scheduleOperation[provision-default/myclaim[68960194-5879-11e8-9631-7cfe903a4aff]]
I0515 19:51:56.545811       1 controller.go:900] volume ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" for claim ""default/myclaim"" created
```

and I can see this pass through to stand alone Cinder (blockbox) and actually create the volume:
```
2018-05-15 20:51:50.688 23 DEBUG cinder.volume.manager [req-c0f6ac39-2685-4daa-95f8-cb052ee55c76 admin admin - - -] Task 'cinder.volume.flows.manager.create_volume.CreateVolumeFromSpecTask;volume:create' (c84bdd51-ca03-47a2-a185-1820c2c20aa7) transitioned into state 'RUNNING' from state 'PENDING' _task_receiver /var/lib/openstack/local/lib/python2.7/site-packages/taskflow/listeners/logging.py:194
2018-05-15 20:51:50.689 23 INFO cinder.volume.flows.manager.create_volume [req-c0f6ac39-2685-4daa-95f8-cb052ee55c76 admin admin - - -] Volume b994c226-3c98-433c-9b0f-2b86d7003074: being created as raw with specification: {'status': u'creating', 'volume_size': 1, 'volume_name': u'volume-b994c226-3c98-433c-9b0f-2b86d7003074'}
2018-05-15 20:51:50.690 23 DEBUG oslo_concurrency.processutils [req-c0f6ac39-2685-4daa-95f8-cb052ee55c76 admin admin - - -] Running cmd (subprocess): env LC_ALL=C lvcreate -T -V 1g -n volume-b994c226-3c98-433c-9b0f-2b86d7003074 cinder-volumes/cinder-volumes-pool execute /var/lib/openstack/local/lib/python2.7/site-packages/oslo_concurrency/processutils.py:372
2018-05-15 20:51:50.872 23 DEBUG oslo_concurrency.processutils [req-c0f6ac39-2685-4daa-95f8-cb052ee55c76 admin admin - - -] CMD ""env LC_ALL=C lvcreate -T -V 1g -n volume-b994c226-3c98-433c-9b0f-2b86d7003074 cinder-volumes/cinder-volumes-pool"" returned: 0 in 0.182s execute /var/lib/openstack/local/lib/python2.7/site-packages/oslo_concurrency/processutils.py:409
2018-05-15 20:51:50.874 23 DEBUG cinder.volume.manager [req-c0f6ac39-2685-4daa-95f8-cb052ee55c76 admin admin - - -] Task 'cinder.volume.flows.manager.create_volume.CreateVolumeFromSpecTask;volume:create' (c84bdd51-ca03-47a2-a185-1820c2c20aa7) transitioned into state 'SUCCESS' from state 'RUNNING' with result '{'status': u'creating', 'volume_size': 1, 'volume_name': u'volume-b994c226-3c98-433c-9b0f-2b86d7003074'}' _task_receiver /var/lib/openstack/local/lib/python2.7/site-packages/taskflow/listeners/logging.py:183
2018-05-15 20:51:50.875 23 DEBUG cinder.volume.manager [req-c0f6ac39-2685-4daa-95f8-cb052ee55c76 admin admin - - -] Task 'cinder.volume.flows.manager.create_volume.CreateVolumeOnFinishTask;volume:create, create.end' (280c3617-e162-4ea7-a5d8-6b36e3ead972) transitioned into state 'RUNNING' from state 'PENDING' _task_receiver /var/lib/openstack/local/lib/python2.7/site-packages/taskflow/listeners/logging.py:194
2018-05-15 20:51:50.881 23 INFO cinder.volume.flows.manager.create_volume [req-c0f6ac39-2685-4daa-95f8-cb052ee55c76 admin admin - - -] Volume volume-b994c226-3c98-433c-9b0f-2b86d7003074 (b994c226-3c98-433c-9b0f-2b86d7003074): created successfully
```
And the volume can be listed with 'cinder list':
```
ubuntu@k8s-master-0:~$ cinder list
+--------------------------------------+-----------+---------------------------------------------------------+------+-------------+----------+-------------+
| ID                                   | Status    | Name                                                    | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+---------------------------------------------------------+------+-------------+----------+-------------+
| b994c226-3c98-433c-9b0f-2b86d7003074 | attaching | cinder-dynamic-pvc-6899dcc8-5879-11e8-ad2a-b65d335b448a | 1    | -           | false    |             |
+--------------------------------------+-----------+---------------------------------------------------------+------+-------------+----------+-------------+
```
But then there appears to be some sort of timeout or miscommunication reported by the standalone-cinder-provisioner, and the cinder volume is then subsequently automatically deleted:
```
I0515 19:52:20.465708       1 leaderelection.go:204] stopped trying to renew lease to provision for pvc default/myclaim, timeout reached
I0515 19:52:26.558231       1 controller.go:921] failed to save volume ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" for claim ""default/myclaim"": Timeout: request did not complete within allowed duration
I0515 19:52:28.584805       1 controller.go:1167] scheduleOperation[provision-default/myclaim[68960194-5879-11e8-9631-7cfe903a4aff]]
I0515 19:52:36.564608       1 controller.go:921] failed to save volume ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" for claim ""default/myclaim"": persistentvolumes ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" already exists
I0515 19:52:43.585004       1 controller.go:1167] scheduleOperation[lock-provision-default/myclaim[68960194-5879-11e8-9631-7cfe903a4aff]]
I0515 19:52:43.593096       1 leaderelection.go:156] attempting to acquire leader lease...
I0515 19:52:43.595853       1 leaderelection.go:178] successfully acquired lease to provision for pvc default/myclaim
I0515 19:52:43.595902       1 controller.go:1167] scheduleOperation[provision-default/myclaim[68960194-5879-11e8-9631-7cfe903a4aff]]
I0515 19:52:46.569739       1 controller.go:921] failed to save volume ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" for claim ""default/myclaim"": persistentvolumes ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" already exists
I0515 19:52:56.574953       1 controller.go:921] failed to save volume ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" for claim ""default/myclaim"": persistentvolumes ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" already exists
I0515 19:52:58.585145       1 controller.go:1167] scheduleOperation[provision-default/myclaim[68960194-5879-11e8-9631-7cfe903a4aff]]
I0515 19:53:06.581022       1 controller.go:921] failed to save volume ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" for claim ""default/myclaim"": persistentvolumes ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" already exists
I0515 19:53:13.585304       1 controller.go:1167] scheduleOperation[provision-default/myclaim[68960194-5879-11e8-9631-7cfe903a4aff]]
I0515 19:53:13.656399       1 leaderelection.go:204] stopped trying to renew lease to provision for pvc default/myclaim, timeout reachedE0515 19:53:16.581166       1 controller.go:931] Error creating provisioned PV object for claim default/myclaim: persistentvolumes ""pvc-68960194-5879-11e8-9631-7cfe903a4aff"" already exists. Deleting the volume.
```

**What you expected to happen**:
I am trying to build a baremetal Kubernetes cluster that uses a stand alone instance of Cinder (no Keystone integration based on blockbox) as the backend volume manager serving lvm volumes.  I would expect the standalone-cinder-provisioner to  allocate the PersistentVolumeClaim and attach storage to appropriate node available for dependent Pod. 

**How to reproduce it (as minimally and precisely as possible)**:
Install stand alone Cinder (blockbox) as described in https://thenewstack.io/deploying-cinder-stand-alone-storage-service/, and https://github.com/openstack/cinder/blob/master/contrib/block-box/docker-compose.yml . 
Configure k8s as described here https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager including setting up and enabling Initializers and disabling PersistentVolumeLabel. 
Deploy standalone-cinder-provisioner as described https://github.com/kubernetes/cloud-provider-openstack/blob/master/manifests/provisioner/deployment.yaml, with the configuration modified for stand alone Cinder:
```
       ...
        - name: OS_CINDER_ENDPOINT
          value: http://{{ cluster_api_address }}:8776/v3
        - name: CINDER_ENDPOINT
          value: http://{{ cluster_api_address }}:8776/v3
        - name: CINDERCLIENT_BYPASS
          value: http://{{ cluster_api_address }}:8776/v3
        - name: OS_AUTH_SYSTEM
          value: noauth
        - name: OS_VOLUME_API_VERSION
          value: ""3.44""
        - name: OS_USERNAME
          value: admin
        - name: OS_PROJECT_ID
          value: admin
```

**Anything else we need to know?**:

standalone-cinder-provisioner image version:
`k8scloudprovider/cinder-provisioner                                         latest              5aa7587eb628        5 days ago          29.2MB`
stand alone Cinder (blockbox) was built from master at commit https://github.com/openstack/cinder/commit/f6cad81789d5816a2a0aa78da0a9b7c2ebf0fa33

I am unsure what other information I need to provide, so please ask if something else is required.  Thanks in advance for any help or advice.

**Environment**:
- openstack-cloud-controller-manager version: 
- OS (e.g. from /etc/os-release): 16.04.4 LTS (Xenial Xerus)
- Kernel (e.g. `uname -a`): Linux k8s-master-0 4.4.0-96-generic #119-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
- Install tools: kubeadm
- Others:  
- Kubernetes:
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.2"", GitCommit:""81753b10df112992bf51bbc2c2f85208aad78335"", GitTreeState:""clean"", BuildDate:""2018-04-27T09:22:21Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.2"", GitCommit:""81753b10df112992bf51bbc2c2f85208aad78335"", GitTreeState:""clean"", BuildDate:""2018-04-27T09:10:24Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}


EDIT: fixed typo
",closed,False,2018-05-15 22:57:35,2018-05-17 02:25:11
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/176,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/176,Restore pre 1.10 openstack instance naming behavior,Sync with the change from @liggitt in k/k master - https://github.com/kubernetes/kubernetes/pull/63903/commits,closed,True,2018-05-16 17:11:08,2018-05-16 19:08:55
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/177,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/177,Track coverage using codecov.io,"We don't really have a good idea about the code coverage using our unit tests. The first step is to update the Makefile, need to figure out how to add support to our Zuul v3 jobs to talk to codecov.io",closed,True,2018-05-16 18:29:37,2018-06-09 14:18:26
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/178,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/178,Add _dist and octavia binary to gitignore,A couple more things to ignore,closed,True,2018-05-17 01:32:54,2018-05-17 14:20:53
cloud-provider-openstack,aglitke,https://github.com/kubernetes/cloud-provider-openstack/pull/179,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/179,Cinder provisioner fixes,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: Apply a few fixes that have been queued while the provisioner code was moved to this repo from the kubernetes-incubator.  I would like to focus my development effort now on this repository instead of the old location.

**Which issue this PR fixes**: This PR fixes the provisioning flow when cinder.create takes longer than 5 seconds.  It also makes it easier to use certain iscsi storage where initiatorName is verified during volume attachment.  The rest of the commits make small fixes and refactors to prepare for the addition of cloning support.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Usability and reliability improvements to the cinder-provisioner.
```
",closed,True,2018-05-17 20:35:51,2018-05-22 13:59:03
cloud-provider-openstack,mkoderer,https://github.com/kubernetes/cloud-provider-openstack/pull/180,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/180,Install also make if not installed,"**What this PR does / why we need it**:
Fixes issue that install-distro-packages.sh checks for make but
never installs it.",closed,True,2018-05-18 09:02:27,2018-05-23 21:42:34
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/181,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/181,Update Kubernetes webhook auth documentaion,"Since kubectl v1.11 we will use ""exec"" [1]  mode to receive user tokens
from Keystone. This patch add documentation about how to configure
kubectl to be able to use this feature.

[1] https://kubernetes.io/docs/admin/authentication/#client-go-credential-plugins",closed,True,2018-05-22 13:12:55,2018-05-25 18:43:34
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/pull/182,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/182,Manila external provisioner,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Adds Manila external provisioner

**Special notes for your reviewer**:
Originated from [kubernetes-incubator/external-storage](https://github.com/kubernetes-incubator/external-storage). Some of the advantages are: ability to handle different share types, OpenStack credentials are passed per-StorageClass instead of per-provisioner.
I'll add unit tests in the following commits, I'd just like to have some feedback if the design is ok.

Tested with a Ceph cluster + [csi-cephfs](https://github.com/ceph/ceph-csi) and Kubernetes' CephFS mounter.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```

/cc @dims ",closed,True,2018-05-22 16:14:55,2018-05-30 10:48:29
cloud-provider-openstack,aglitke,https://github.com/kubernetes/cloud-provider-openstack/pull/183,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/183,Add cloning via PVC annotation,"This PR enables provisioning new volumes as a clone of an existing PVC.  To
request a clone, the user should add a clone request annotation to the PVC:

```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: clone-claim
  annotations:
    k8s.io/CloneRequest: default/source-pvc
```

The provisioner will locate the indicated PVC and determine the underlying
associated cinder volume ID.  The provisioner will use this ID as the
source-vol-id parameter when provisioning the new volume resulting in a clone
at the storage level.

Cloning will only work if the cinder backend supports ""smart-cloning"".  This
means that the clone will complete without the need for cinder to perform a
bit-for-bit copy using the host.  Because of this, the cinder provisioner will
not attempt to clone unless the associated storage class contains the
parameter: ""smartclone"" to indicate that volumes provisioned from this storage
class can be cloned correctly.

If the provisioner was able to clone the volume it will apply the
'k8s.io/CloneOf' annotation to the PVC.
",closed,True,2018-05-22 18:49:50,2018-06-06 01:14:50
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/issues/184,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/184,not possible to specify connection limit when creating lbaas,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**: FEATURE

/kind feature


**What happened**: It is not possible to specify connection limit to LoadBalancer resource

**What you expected to happen**: I except that I could specify limit with service annotation

**How to reproduce it (as minimally and precisely as possible)**: Create service using type LoadBalancer, then check connection limit `neutron lbaas-listener-show <listenerid>`. In case of haproxy lbaas it will print -1, but because it is not defined the default value for haproxy is 2000. At least for us that is way too low. Currently we are using `neutron lbaas-listener-update --connection-limit 10000 <listenerid>` after the lbaas is created by kubernetes. It will update the connection limit without downtime. 

It seems that gophercloud createopts does not support connection limit:
https://github.com/gophercloud/gophercloud/blob/master/openstack/networking/v2/extensions/lbaas_v2/listeners/requests.go#L79

So it might be that it should be added there first, and after that create new service annotation and add value here https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/openstack/openstack_loadbalancer.go#L754 (similar place exist in this repository, the row number is not ofc same.)
",closed,False,2018-05-23 05:36:56,2018-05-25 20:32:32
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/pull/185,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/185,add possibility to specify connection limit in lbaas listener,"
**What this PR does / why we need it**: We need possibility to specify connection limits instead of default ones

**Which issue this PR fixes**: fixes #184

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-05-23 13:21:40,2018-05-25 20:32:33
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/186,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/186,Add SECURITY_CONTACTS for k/cpo,Adding Security Contacts to Repositories under Kubernetes per request on kubernetes-dev from @jessfraz ,closed,True,2018-05-24 00:13:33,2018-05-24 14:14:32
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/187,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/187,Add a conformance job for 1.11 release branch,So that we can add it to release blocking dashboard!,closed,True,2018-05-30 10:15:01,2018-05-30 12:54:44
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/pull/188,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/188,[WIP]Pip quiet install in Makefile,"**What this PR does / why we need it**:
Add ""--quiet"" option for pip install Mercurial to avoid
stdio to be interrupted, that cause incompleteness of OpenLab log.

**Special notes for your reviewer**:
@dims ",closed,True,2018-06-01 01:57:54,2018-06-04 03:14:54
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/189,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/189,ConfigMap support for k8s-keystone-auth,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**: /kind feature

**What happened**: N/A

**What you expected to happen**: Allow the admin user to define the authorization policy by using configmap.

**How to reproduce it (as minimally and precisely as possible)**: N/A

**Anything else we need to know?**: N/A
",closed,False,2018-06-01 09:14:50,2018-06-04 15:25:55
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/190,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/190,Add configmap support for k8s-keystone-auth,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: Add configmap support for k8s-keystone-auth

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #189

**Special notes for your reviewer**:
With this change, the k8s-keystone-auth can accept either policy config
file or a configmap name in 'kube-system' namespace, because in real
deployment, it's admin user's responsibility to update the policy file
for authorization.

In order to connect to k8s api, either a kubeconfig file or service
account should be provided.

The next step is to support on-the-fly change for the policy without
restarting k8s-keystone-auth service.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-06-01 09:16:17,2018-06-06 02:29:45
cloud-provider-openstack,xmik,https://github.com/kubernetes/cloud-provider-openstack/issues/191,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/191,Label not set: failure-domain.beta.kubernetes.io/zone,"/kind question

## TL;DR: Problem
This label is not being set on any node in my Kubernetes cluster:
```
failure-domain.beta.kubernetes.io/zone=nova
```
In result, a cinder volume (created from PVC) is created but not attached to any vm. 
This label should be set on a node by cloudprovider, [source](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domain-beta-kubernetes-io-zone).
Error from Pod waiting for this volume:
```
$ kubectl -n testing describe pod
Name:           nginx-vols-testing-c75cbd664-5m7qq
Namespace:      testing
# ...
Tolerations:     node.cloudprovider.kubernetes.io/uninitialized
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  25s (x322 over 1h)  default-scheduler  0/3 nodes are available: 1 node(s) didn't match node selector, 2 node(s) had no available volume zone.
```

Setting the label manually with:
```
kubectl label node/<node-name> failure-domain.beta.kubernetes.io/zone=nova
```
fixes it (volume becomes attached to a VM).

## Environment
I have deployed Kubernetes 1.10 cluster on OpenStack VMs, intend to use cinder for persistent volumes. Now, I am not sure which solution should I choose and I have trouble understanding [openstack-kubernetes-integration-options.md](https://github.com/kubernetes/cloud-provider-openstack/blob/v0.1.0/docs/openstack-kubernetes-integration-options.md).

Can you please correct me if I am wrong? The solutions (I think) are:
1. **In-tree OpenStack provider in Kubernetes repository**, the old way: use only kube-controller-manager (no new ccm containers needed). The flags to set for kubelet, kube-apiserver and kube-controller-manager: `--cloud-config=/path/to/cloud_config  --cloud-provider=openstack`
2. **Cloud Controller Manager (CCM) in Kubernetes repository**, the new, but temporary way: 
   * use kube-controller-manager, but set `--external-cloud-volume-plugin=openstack`, so that it has a role of: Volume controller, [source](https://kubernetes.io/docs/concepts/architecture/cloud-controller/#persistentvolumelabels-controller), but also set: `--cloud-config=/path/to/cloud_config  --cloud-provider=external`
   * and also use cloud-controller-manager with `--cloud-config=/path/to/cloud_config  --cloud-provider=openstack`. This is just one container from image `k8scloudprovider/openstack-cloud-controller-manager:v0.1.0`
3. **External OpenStack provider**, which is the same as the 2nd way, but we should also deploy one from those 3:
   * Cinder Standalone provisioner
   * Cinder Flex volume driver
   * Cinder CSI driver

My goal is to use Cinder CSI driver, because:
   * ""CSI is planned to become the primary volume plugin system for Kubernetes""  [source](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md)
   * In the long run we should prefer CSI over FlexVolume [source](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#working-with-out-of-tree-volume-plugin-options)
   * AFAIK Cinder Standalone provisioner is for those Kubernetes clusters that want to connect with external Cinder resources (and such a Kubernetes cluster doesn't need to be deployed on Openstack VMs).

But, since the CSI driver is WIP, [source](https://github.com/kubernetes/cloud-provider-openstack/blob/v0.1.0/docs/openstack-kubernetes-integration-options.md#cinder-csi-driver), I choose the 2nd way for now.




",closed,False,2018-06-03 16:57:47,2019-02-14 07:22:17
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/issues/192,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/192,Allow to synchronize Keystone projects with k8s namespaces,"To improve integration between keystone and k8s it's recommended to add a possibility to synchronize projects/namespaces.

In other words, if the user belongs to the project in Keystone, then when attempting to authenticate in Kubernetes using [k8s-keystone-auth](https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-keystone-webhook-authenticator-and-authorizer.md), a k8s namespace with the same name as the project id will automatically be created for him.

/kind feature",closed,False,2018-06-05 14:21:26,2018-06-20 20:23:06
cloud-provider-openstack,aglitke,https://github.com/kubernetes/cloud-provider-openstack/issues/193,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/193,Label to classify issues by component,"I would like to scrub github issues in this repo to identify which pertain to the standalone cinder provisioner.  In my own repo I would use a label ""standalone-cinder-provisioner"" or something.  Could we add this label or is there a better way to achieve this?",closed,False,2018-06-05 15:27:00,2018-06-05 17:18:11
cloud-provider-openstack,kairen,https://github.com/kubernetes/cloud-provider-openstack/pull/194,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/194,Fix broken link in README.md,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR will fix the broken link in README.md.

**Special notes for your reviewer**:
@dims ",closed,True,2018-06-06 02:06:12,2018-06-06 06:59:58
cloud-provider-openstack,openstacker,https://github.com/kubernetes/cloud-provider-openstack/issues/195,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/195,Improvements for k8s-keystone-auth,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature


**What happened**:

**What you expected to happen**:
In a perfect world, I would like to see:
1. k8s-keystone-auth can be run either standalone or as pod
2. k8s-keystone-auth can accept both policy file or configMap
3. Policies can be updated and effected on-the-fly
4. If k8s-keystone-auth is running as pod, it can work on worker node.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
  This is related to Magnum/k8s-keystone-auth integration.
",closed,False,2018-06-07 19:14:21,2018-06-14 15:29:29
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/196,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/196,Trivial: Fix the path of policy.json,"**What this PR does / why we need it**:

policy.json exists under examples/webhook/, not examples directly.
This fixes the path for avoiding confusion.

**Release note**: `NONE`
",closed,True,2018-06-08 18:36:18,2018-06-14 02:09:05
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/197,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/197,Support to change the policy configmap for k8s-keystone-auth on the fly,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Support to change the policy configmap for k8s-keystone-auth on the fly

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #195

**Special notes for your reviewer**:
- Watch the configmap change for authorization policy and apply to the
  service dynamically.
- Improve the shape of k8s-keystone-auth implementation

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
The authorization functionality of k8s-keystone-auth service can be running either by specifying a policy file or a configmap in kube-system namespace. If the configmap is provided, the authorization policy could be changed during the service running without service restart.
```
",closed,True,2018-06-10 08:51:29,2018-06-15 11:59:26
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/198,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/198,Allow synchronization between Keystone projects and K8s namespaces,"To improve integration between keystone and k8s it's recommended
to add a possibility to synchronize projects/namespaces.

In other words, if the user belongs to the project in Keystone, then, when
he attepts to authenticate in Kubernetes using k8s-keystone-auth, a k8s
namespace with the same name as the project id will automatically be
created for him.

To enable the feature two new arguments were added to the k8s-keystone-auth binary:
""--sync-config-file"" - points to a local file with sync configuration.
""--sync-configmap-name"" defines a configmap containing the configuration.
If any of them is set, new namespace will be automatically created when the user is
authenticated in Keystone.

fixes: #192 ",closed,True,2018-06-11 12:28:09,2018-06-20 20:23:07
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/199,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/199,Write the command name of webhook,"**What this PR does / why we need it**:

It was a little difficult to understand what command I should run
as a webhook process. This adds the command name of the webhook
for easy understanding.

**Release note**: `NONE`.
",closed,True,2018-06-12 00:40:36,2018-06-14 02:09:05
cloud-provider-openstack,AdamDang,https://github.com/kubernetes/cloud-provider-openstack/pull/200,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/200,Typo fix fromat->format/to to->to,"Line 326: fromat->format
Line 665: to to->to",closed,True,2018-06-12 05:17:29,2018-06-12 10:28:28
cloud-provider-openstack,zioproto,https://github.com/kubernetes/cloud-provider-openstack/pull/201,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/201,Add binary file name to .gitignore,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

In commit 61f07b188579c2a20b735db06258e3edd973306a we introduced a new binary file `client-keystone-auth` but we forgot to add it to the `.gitignore` like the other binaries


",closed,True,2018-06-12 08:34:01,2018-06-12 10:21:28
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/202,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/202,Upgrade pflag,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**: Remove the log ERROR message in each line
```
ubuntu@k8s-master:~$ kubectl logs k8s-keystone-auth-k8s-master -n kube-system
ERROR: logging before flag.Parse: I0612 09:06:00.524068       1 keystone.go:451] Creating kubernetes API client.
ERROR: logging before flag.Parse: W0612 09:06:00.524182       1 client_config.go:533] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
ERROR: logging before flag.Parse: W0612 09:06:00.524262       1 client_config.go:538] error creating inClusterConfig, falling back to default config: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
ERROR: logging before flag.Parse: E0612 09:06:00.524410       1 main.go:43] failed to get kubernetes client: invalid configuration: no configuration has been provided
```

First add dependency `github.com/spf13/pflag` to `Gopkg.toml`, then run `dep ensure` to auto-update Gopkg.lock
",closed,True,2018-06-12 10:34:42,2018-06-15 11:59:39
cloud-provider-openstack,zioproto,https://github.com/kubernetes/cloud-provider-openstack/issues/203,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/203,Broken dep goautoneg,"BUG REPORT

/kind bug


**What happened**:

During the build process it is not possible to download the dep `goautoneg`

```
(85/85) Failed to write bitbucket.org/ww/goautoneg@default
grouped write of manifest, lock and vendor: error while writing out vendor tree: failed to write dep tree: failed to export bitbucket.org/ww/goautoneg: no valid source could be created:
	failed to set up sources from the following URLs:
https://bitbucket.org/ww/goautoneg
: remote repository at https://bitbucket.org/ww/goautoneg does not exist, or is inaccessible
	failed to set up sources from the following URLs:
ssh://hg@bitbucket.org/ww/goautoneg
: remote repository at ssh://hg@bitbucket.org/ww/goautoneg does not exist, or is inaccessible
	failed to set up sources from the following URLs:
http://bitbucket.org/ww/goautoneg
: remote repository at http://bitbucket.org/ww/goautoneg does not exist, or is inaccessible
   1 # This file is autogenerated, do not edit; changes may be undone by the next 'dep ensure'.
	failed to set up sources from the following URLs:
https://bitbucket.org/ww/goautoneg
: remote repository at https://bitbucket.org/ww/goautoneg does not exist, or is inaccessible: remote: Not Found
fatal: repository 'https://bitbucket.org/ww/goautoneg/' not found
: exit status 128
	failed to set up sources from the following URLs:
ssh://git@bitbucket.org/ww/goautoneg
: remote repository at ssh://git@bitbucket.org/ww/goautoneg does not exist, or is inaccessible: Warning: Permanently added the RSA host key for IP address '104.192.143.1' to the list of known hosts.
git@bitbucket.org: Permission denied (publickey).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
: exit status 128
	failed to set up sources from the following URLs:
git://bitbucket.org/ww/goautoneg
: remote repository at git://bitbucket.org/ww/goautoneg does not exist, or is inaccessible: fatal: unable to connect to bitbucket.org:
bitbucket.org[0: 104.192.143.3]: errno=Operation timed out
bitbucket.org[1: 104.192.143.2]: errno=Operation timed out
bitbucket.org[2: 2401:1d80:1010::152]: errno=Operation timed out
bitbucket.org[3: 104.192.143.1]: errno=Operation timed out

: exit status 128
	failed to set up sources from the following URLs:
http://bitbucket.org/ww/goautoneg
: remote repository at http://bitbucket.org/ww/goautoneg does not exist, or is inaccessible: remote: Not Found
fatal: repository 'http://bitbucket.org/ww/goautoneg/' not found
: exit status 128
make: *** [depend] Error 1
```


**What you expected to happen**:

The dependency should be downloaded from the URL https://bitbucket.org/ww/goautoneg

**How to reproduce it (as minimally and precisely as possible)**:
`dep ensure -v`

**Anything else we need to know?**:

",closed,False,2018-06-12 13:03:51,2018-07-27 15:39:50
cloud-provider-openstack,zioproto,https://github.com/kubernetes/cloud-provider-openstack/pull/204,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/204,provide a download URL for goautoneg,"Provide a download URL for goautoneg

Fixes #203 


",closed,True,2018-06-12 13:04:56,2018-06-12 13:29:21
cloud-provider-openstack,aglitke,https://github.com/kubernetes/cloud-provider-openstack/pull/205,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/205,Document cloning in the standalone-cinder provisioner,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-06-12 17:59:38,2018-06-21 14:08:53
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/206,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/206,Add information about user's domain to the Extra data,"When user authenticates there is no information about his domain,
but having it could be useful for data synchronization. For instance,
when admin may want to disable the synchronization for some specific
domains.

This patch adds information about user's domain to the Extra data,
that can be found by ""alpha.kubernetes.io/identity/user/domain/id"" and
""alpha.kubernetes.io/identity/user/domain/name"" keys respectively.

Also missing unit tests were added to check unmarshaling of keystone
responses.",closed,True,2018-06-13 14:16:08,2018-06-19 10:39:33
cloud-provider-openstack,aglitke,https://github.com/kubernetes/cloud-provider-openstack/issues/207,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/207,Request new container build of cinder-provisioner to be pushed to docker,Hi.  There have been several recent fixes and enhancements to the cinder-provisioner.  Can someone trigger a build to update cinder-provisioner:latest to the current state of the code?  Thanks!,closed,False,2018-06-13 16:29:14,2018-06-13 17:11:13
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/208,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/208,Fix the config sort of k8s-keystone-auth,"**What this PR does / why we need it**:

On some environments, kube-apiserver restarts automatically just after
changing the manifest (/etc/kubernetes/manifests/kube-apiserver.yaml, etc).
Before restarting, k8s-keystone-auth process needs to exist.
So this changes the config sort of k8s-keystone-auth to make the condition
easily for readers.

**Release note**: `NONE`
",closed,True,2018-06-14 02:16:17,2018-06-14 15:18:29
cloud-provider-openstack,zioproto,https://github.com/kubernetes/cloud-provider-openstack/pull/209,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/209,make dep ensure verbose,"It takes a while to fetch everything to the network,
it is nice to give a progress indication of what is
going on
",closed,True,2018-06-15 12:02:59,2018-06-21 17:42:25
cloud-provider-openstack,RdL87,https://github.com/kubernetes/cloud-provider-openstack/issues/210,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/210,unexpected directory layout while running make,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:


 /kind bug



**What happened**:

After gave the make k8s-keystone-auth command, during the dep ensure phase i receive the following error:

CGO_ENABLED=0 GOOS=linux go build \
	-ldflags ""-w -s -X 'main.version=b4f24944-dirty'"" \
	-o k8s-keystone-auth \
	cmd/k8s-keystone-auth/main.go
unexpected directory layout:
	import path: github.com/golang/glog
	root: /home/dilallo/go/src
	dir: /home/dilallo/go/src/k8s.io/cloud-provider-openstack/vendor/github.com/golang/glog
	expand root: /home/dilallo/go/src
	expand dir: /home/dilallo/go/src/k8s.io/cloud-provider-openstack/vendor/github.com/golang/glog
	separator: /
Makefile:88: recipe for target 'k8s-keystone-auth' failed
make: *** [k8s-keystone-auth] Error 1


My GO version go1.10.3 linux/amd64

",closed,False,2018-06-19 14:07:35,2018-11-19 09:18:59
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/211,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/211,support for secrets,"https://github.com/kubernetes/kubernetes/pull/63826 got merged in main k/k. 

What's the best way to implement/support this? provisioner/csi/flex-volume?

",closed,False,2018-06-19 16:18:39,2018-11-16 18:17:19
cloud-provider-openstack,edisonxiang,https://github.com/kubernetes/cloud-provider-openstack/issues/212,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/212,Support Cinder API version configuration in CSI plugin,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**: 

There are lots of OpenStack users who are using Cinder v1 and Cinder v2 API. 
But now the Cinder CSI Plugin is only supporting Cinder V3.  For the end users, it has no way to set other Cinder API version like v1 and v2.

**What you expected to happen**:

Cinder version should be supported in Cinder CSI Plugin.
It can be configured like [kubernetes in tree cloud provider](https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/).

For example:
```
[BlockStorage]
bs-version=v2
```

**How to reproduce it (as minimally and precisely as possible)**:

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-06-20 01:50:35,2018-11-17 03:26:03
cloud-provider-openstack,edisonxiang,https://github.com/kubernetes/cloud-provider-openstack/pull/213,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/213,Support Cinder API version configuration,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

Support Cinder API version configuration in Cinder CSI Plugin.
It can be configured in the cloud.conf.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #212

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",open,True,2018-06-20 06:11:11,2019-04-02 15:29:03
cloud-provider-openstack,s0komma,https://github.com/kubernetes/cloud-provider-openstack/issues/214,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/214,"Kubelet: could not init cloud provider ""openstack"": 1:1: expected section header","<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
Hi Guys, im trying to setup k8s persistent storage using cinder.
my k8s version is `1.9.4`
followed all the steps
we added below details on all the nodes of k8s including master and worker 
`--cloud-provider=openstack --cloud-config=/etc/kubernetes/cloud_config` --> the added to kubelet, controller and api server on all master nodes and only for kubelet on worker nodes (edited)
after doing so kubelet wont start on master node, all worker nodes are up 

Kubelet logs are below
```-- The start-up result is done.
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.779199   21254 flags.go:52] FLAG: --address=""0.0.0.0""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.779786   21254 flags.go:52] FLAG: --allow-privileged=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.780036   21254 flags.go:52] FLAG: --alsologtostderr=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.780316   21254 flags.go:52] FLAG: --anonymous-auth=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.780584   21254 flags.go:52] FLAG: --application-metrics-count-limit=""100""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.780848   21254 flags.go:52] FLAG: --authentication-token-webhook=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.781151   21254 flags.go:52] FLAG: --authentication-token-webhook-cache-ttl=""2m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.781443   21254 flags.go:52] FLAG: --authorization-mode=""Webhook""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.781719   21254 flags.go:52] FLAG: --authorization-webhook-cache-authorized-ttl=""5m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.781963   21254 flags.go:52] FLAG: --authorization-webhook-cache-unauthorized-ttl=""30s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.782237   21254 flags.go:52] FLAG: --azure-container-registry-config=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.782494   21254 flags.go:52] FLAG: --boot-id-file=""/proc/sys/kernel/random/boot_id""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.782780   21254 flags.go:52] FLAG: --bootstrap-checkpoint-path=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.783084   21254 flags.go:52] FLAG: --bootstrap-kubeconfig=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.783355   21254 flags.go:52] FLAG: --cadvisor-port=""4194""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.783657   21254 flags.go:52] FLAG: --cert-dir=""/var/lib/kubelet/pki""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.783914   21254 flags.go:52] FLAG: --cgroup-driver=""cgroupfs""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.784176   21254 flags.go:52] FLAG: --cgroup-root=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.784433   21254 flags.go:52] FLAG: --cgroups-per-qos=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.784688   21254 flags.go:52] FLAG: --chaos-chance=""0""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.784956   21254 flags.go:52] FLAG: --client-ca-file=""/var/lib/kubelet/ca.pem""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785222   21254 flags.go:52] FLAG: --cloud-config=""/etc/kubernetes/cloud_config""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785486   21254 flags.go:52] FLAG: --cloud-provider=""openstack""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785509   21254 flags.go:52] FLAG: --cloud-provider-gce-lb-src-cidrs=""130.211.0.0/22,35.191.0.0/16,209.85.152.0/22,209.85.204.0/22""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785518   21254 flags.go:52] FLAG: --cluster-dns=""[172.16.56.10]""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785527   21254 flags.go:52] FLAG: --cluster-domain=""cluster.local""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785531   21254 flags.go:52] FLAG: --cni-bin-dir=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785535   21254 flags.go:52] FLAG: --cni-conf-dir=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785538   21254 flags.go:52] FLAG: --container-hints=""/etc/cadvisor/container_hints.json""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785542   21254 flags.go:52] FLAG: --container-runtime=""docker""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785546   21254 flags.go:52] FLAG: --container-runtime-endpoint=""unix:///var/run/dockershim.sock""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785550   21254 flags.go:52] FLAG: --containerd=""unix:///var/run/containerd.sock""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785554   21254 flags.go:52] FLAG: --containerized=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785558   21254 flags.go:52] FLAG: --contention-profiling=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785561   21254 flags.go:52] FLAG: --cpu-cfs-quota=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785564   21254 flags.go:52] FLAG: --cpu-manager-policy=""none""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785568   21254 flags.go:52] FLAG: --cpu-manager-reconcile-period=""10s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785572   21254 flags.go:52] FLAG: --docker=""unix:///var/run/docker.sock""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785575   21254 flags.go:52] FLAG: --docker-disable-shared-pid=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785579   21254 flags.go:52] FLAG: --docker-endpoint=""unix:///var/run/docker.sock""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785582   21254 flags.go:52] FLAG: --docker-env-metadata-whitelist=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785586   21254 flags.go:52] FLAG: --docker-only=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785589   21254 flags.go:52] FLAG: --docker-root=""/var/lib/docker""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785593   21254 flags.go:52] FLAG: --docker-tls=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785596   21254 flags.go:52] FLAG: --docker-tls-ca=""ca.pem""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785600   21254 flags.go:52] FLAG: --docker-tls-cert=""cert.pem""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785603   21254 flags.go:52] FLAG: --docker-tls-key=""key.pem""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785607   21254 flags.go:52] FLAG: --dynamic-config-dir=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785612   21254 flags.go:52] FLAG: --enable-controller-attach-detach=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785615   21254 flags.go:52] FLAG: --enable-custom-metrics=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785619   21254 flags.go:52] FLAG: --enable-debugging-handlers=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785622   21254 flags.go:52] FLAG: --enable-load-reader=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785625   21254 flags.go:52] FLAG: --enable-server=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785629   21254 flags.go:52] FLAG: --enforce-node-allocatable=""[pods]""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785637   21254 flags.go:52] FLAG: --event-burst=""10""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785641   21254 flags.go:52] FLAG: --event-qps=""5""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785644   21254 flags.go:52] FLAG: --event-storage-age-limit=""default=0""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785648   21254 flags.go:52] FLAG: --event-storage-event-limit=""default=0""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785651   21254 flags.go:52] FLAG: --eviction-hard=""imagefs.available<15%,memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785664   21254 flags.go:52] FLAG: --eviction-max-pod-grace-period=""0""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785668   21254 flags.go:52] FLAG: --eviction-minimum-reclaim=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785674   21254 flags.go:52] FLAG: --eviction-pressure-transition-period=""5m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785678   21254 flags.go:52] FLAG: --eviction-soft=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785682   21254 flags.go:52] FLAG: --eviction-soft-grace-period=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785685   21254 flags.go:52] FLAG: --exit-on-lock-contention=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785689   21254 flags.go:52] FLAG: --experimental-allocatable-ignore-eviction=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785692   21254 flags.go:52] FLAG: --experimental-allowed-unsafe-sysctls=""[]""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785696   21254 flags.go:52] FLAG: --experimental-bootstrap-kubeconfig=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785700   21254 flags.go:52] FLAG: --experimental-check-node-capabilities-before-mount=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785703   21254 flags.go:52] FLAG: --experimental-dockershim=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785707   21254 flags.go:52] FLAG: --experimental-dockershim-root-directory=""/var/lib/dockershim""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785711   21254 flags.go:52] FLAG: --experimental-fail-swap-on=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785714   21254 flags.go:52] FLAG: --experimental-kernel-memcg-notification=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785718   21254 flags.go:52] FLAG: --experimental-mounter-path=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785722   21254 flags.go:52] FLAG: --experimental-qos-reserved=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785726   21254 flags.go:52] FLAG: --fail-swap-on=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785729   21254 flags.go:52] FLAG: --feature-gates=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785734   21254 flags.go:52] FLAG: --file-check-frequency=""20s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785738   21254 flags.go:52] FLAG: --global-housekeeping-interval=""1m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785741   21254 flags.go:52] FLAG: --google-json-key=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785745   21254 flags.go:52] FLAG: --hairpin-mode=""promiscuous-bridge""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785748   21254 flags.go:52] FLAG: --healthz-bind-address=""127.0.0.1""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785752   21254 flags.go:52] FLAG: --healthz-port=""10248""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785756   21254 flags.go:52] FLAG: --host-ipc-sources=""[*]""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785774   21254 flags.go:52] FLAG: --host-network-sources=""[*]""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785778   21254 flags.go:52] FLAG: --host-pid-sources=""[*]""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785784   21254 flags.go:52] FLAG: --hostname-override=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785788   21254 flags.go:52] FLAG: --housekeeping-interval=""10s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785792   21254 flags.go:52] FLAG: --http-check-frequency=""20s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785795   21254 flags.go:52] FLAG: --image-gc-high-threshold=""85""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785799   21254 flags.go:52] FLAG: --image-gc-low-threshold=""80""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785802   21254 flags.go:52] FLAG: --image-pull-progress-deadline=""2m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785806   21254 flags.go:52] FLAG: --image-service-endpoint=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785809   21254 flags.go:52] FLAG: --init-config-dir=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785812   21254 flags.go:52] FLAG: --iptables-drop-bit=""15""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785815   21254 flags.go:52] FLAG: --iptables-masquerade-bit=""14""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785819   21254 flags.go:52] FLAG: --keep-terminated-pod-volumes=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785822   21254 flags.go:52] FLAG: --kube-api-burst=""10""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785826   21254 flags.go:52] FLAG: --kube-api-content-type=""application/vnd.kubernetes.protobuf""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785830   21254 flags.go:52] FLAG: --kube-api-qps=""5""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785833   21254 flags.go:52] FLAG: --kube-reserved=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785837   21254 flags.go:52] FLAG: --kube-reserved-cgroup=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785840   21254 flags.go:52] FLAG: --kubeconfig=""/var/lib/kubelet/kubelet.kubeconfig""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785845   21254 flags.go:52] FLAG: --kubelet-cgroups=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785848   21254 flags.go:52] FLAG: --lock-file=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785852   21254 flags.go:52] FLAG: --log-backtrace-at="":0""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785856   21254 flags.go:52] FLAG: --log-cadvisor-usage=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785859   21254 flags.go:52] FLAG: --log-dir=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785863   21254 flags.go:52] FLAG: --log-flush-frequency=""5s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785868   21254 flags.go:52] FLAG: --logtostderr=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785871   21254 flags.go:52] FLAG: --machine-id-file=""/etc/machine-id,/var/lib/dbus/machine-id""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785875   21254 flags.go:52] FLAG: --make-iptables-util-chains=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785879   21254 flags.go:52] FLAG: --manifest-url=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785882   21254 flags.go:52] FLAG: --manifest-url-header=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785889   21254 flags.go:52] FLAG: --master-service-namespace=""default""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785893   21254 flags.go:52] FLAG: --max-open-files=""1000000""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785899   21254 flags.go:52] FLAG: --max-pods=""110""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785903   21254 flags.go:52] FLAG: --maximum-dead-containers=""-1""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785906   21254 flags.go:52] FLAG: --maximum-dead-containers-per-container=""1""
Jun 21 08:20:22 control-plane-271098166-1-377715803 systemd[1]: kubelet.service: Unit entered failed state.
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785910   21254 flags.go:52] FLAG: --minimum-container-ttl-duration=""0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785924   21254 flags.go:52] FLAG: --minimum-image-ttl-duration=""2m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785928   21254 flags.go:52] FLAG: --network-plugin=""cni""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785931   21254 flags.go:52] FLAG: --network-plugin-mtu=""0""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785935   21254 flags.go:52] FLAG: --node-ip=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785938   21254 flags.go:52] FLAG: --node-labels=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785944   21254 flags.go:52] FLAG: --node-status-update-frequency=""10s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785948   21254 flags.go:52] FLAG: --non-masquerade-cidr=""10.0.0.0/8""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785951   21254 flags.go:52] FLAG: --oom-score-adj=""-999""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785956   21254 flags.go:52] FLAG: --pod-cidr=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785960   21254 flags.go:52] FLAG: --pod-infra-container-image=""gcr.docker.prod.walmart.com/google_containers/pause-amd64:3.0""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785964   21254 flags.go:52] FLAG: --pod-manifest-path=""/etc/kubernetes/manifests""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785968   21254 flags.go:52] FLAG: --pods-per-core=""0""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785971   21254 flags.go:52] FLAG: --port=""10250""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785974   21254 flags.go:52] FLAG: --protect-kernel-defaults=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785978   21254 flags.go:52] FLAG: --provider-id=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785981   21254 flags.go:52] FLAG: --read-only-port=""10255""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785984   21254 flags.go:52] FLAG: --really-crash-for-testing=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785988   21254 flags.go:52] FLAG: --register-node=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785991   21254 flags.go:52] FLAG: --register-schedulable=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.785994   21254 flags.go:52] FLAG: --register-with-taints=""node-role.kubernetes.io/master=:NoSchedule""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786004   21254 flags.go:52] FLAG: --registry-burst=""10""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786008   21254 flags.go:52] FLAG: --registry-qps=""5""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786011   21254 flags.go:52] FLAG: --require-kubeconfig=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786015   21254 flags.go:52] FLAG: --resolv-conf=""/etc/resolv.conf""
Jun 21 08:20:22 control-plane-271098166-1-377715803 systemd[1]: kubelet.service: Failed with result 'exit-code'.
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786019   21254 flags.go:52] FLAG: --rkt-api-endpoint=""localhost:15441""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786023   21254 flags.go:52] FLAG: --rkt-path=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786026   21254 flags.go:52] FLAG: --rkt-stage1-image=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786029   21254 flags.go:52] FLAG: --root-dir=""/var/lib/kubelet""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786032   21254 flags.go:52] FLAG: --rotate-certificates=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786036   21254 flags.go:52] FLAG: --runonce=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786039   21254 flags.go:52] FLAG: --runtime-cgroups=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786042   21254 flags.go:52] FLAG: --runtime-request-timeout=""15m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786046   21254 flags.go:52] FLAG: --seccomp-profile-root=""/var/lib/kubelet/seccomp""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786049   21254 flags.go:52] FLAG: --serialize-image-pulls=""true""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786053   21254 flags.go:52] FLAG: --stderrthreshold=""2""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786056   21254 flags.go:52] FLAG: --storage-driver-buffer-duration=""1m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786059   21254 flags.go:52] FLAG: --storage-driver-db=""cadvisor""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786063   21254 flags.go:52] FLAG: --storage-driver-host=""localhost:8086""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786066   21254 flags.go:52] FLAG: --storage-driver-password=""root""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786070   21254 flags.go:52] FLAG: --storage-driver-secure=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786074   21254 flags.go:52] FLAG: --storage-driver-table=""stats""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786077   21254 flags.go:52] FLAG: --storage-driver-user=""root""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786081   21254 flags.go:52] FLAG: --streaming-connection-idle-timeout=""4h0m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786084   21254 flags.go:52] FLAG: --sync-frequency=""1m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786087   21254 flags.go:52] FLAG: --system-cgroups=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786091   21254 flags.go:52] FLAG: --system-reserved=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786094   21254 flags.go:52] FLAG: --system-reserved-cgroup=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786097   21254 flags.go:52] FLAG: --tls-cert-file=""/var/lib/kubelet/kubelet-client.pem""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786101   21254 flags.go:52] FLAG: --tls-private-key-file=""/var/lib/kubelet/kubelet-client-key.pem""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786105   21254 flags.go:52] FLAG: --v=""10""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786109   21254 flags.go:52] FLAG: --version=""false""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786115   21254 flags.go:52] FLAG: --vmodule=""""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786119   21254 flags.go:52] FLAG: --volume-plugin-dir=""/usr/libexec/kubernetes/kubelet-plugins/volume/exec/""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786123   21254 flags.go:52] FLAG: --volume-stats-agg-period=""1m0s""
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.786938   21254 feature_gate.go:226] feature gates: &{{} map[]}
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.787016   21254 controller.go:114] kubelet config controller: starting controller
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.787022   21254 controller.go:118] kubelet config controller: validating combination of defaults and flags
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.809494   21254 mount_linux.go:208] Detected OS with systemd
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.811801   21254 iptables.go:589] couldn't get iptables-restore version; assuming it doesn't support --wait
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.813061   21254 server.go:182] Version: v1.9.4
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: I0621 08:20:22.813142   21254 feature_gate.go:226] feature gates: &{{} map[]}
Jun 21 08:20:22 control-plane-271098166-1-377715803 kubelet[21254]: error: failed to run Kubelet: could not init cloud provider ""openstack"": 1:1: expected section header
Jun 21 08:20:27 control-plane-271098166-1-377715803 systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Jun 21 08:20:27 control-plane-271098166-1-377715803 systemd[1]: Stopped Kubernetes Kubelet.
a
 
**What you expected to happen**:
Kubelet to come up 
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: using in tree 
- OS (e.g. from /etc/os-release): NAME=""Ubuntu""
VERSION=""16.04.3 LTS (Xenial Xerus)""
- Kernel (e.g. `uname -a`): Linux control-plane-271098166-1-377715803 4.4.0-89-generic #112-Ubuntu SMP Mon Jul 31 19:38:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
- Others:
",closed,False,2018-06-21 08:23:10,2018-06-21 17:41:13
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/215,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/215,Small README.md improvements,"Since glide dependency management was replaced by ""go dep"" [1], we need
to update README.md accordingly.

Also this patch improves the formatting of the file by following the
rules described in [2].
The rules that were violated:
- MD022 - Headings should be surrounded by blank lines;
- MD032 - Lists should be surrounded by blank lines;
- MD034 - Bare URL used.

[1] https://github.com/kubernetes/cloud-provider-openstack/pull/86
[2] https://github.com/DavidAnson/markdownlint/blob/master/doc/Rules.md
",closed,True,2018-06-21 14:34:20,2018-06-21 17:42:48
cloud-provider-openstack,sangeetg,https://github.com/kubernetes/cloud-provider-openstack/issues/216,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/216,Empty String for APIGroup is not suppoted.,"<!-- This form is for bug reports and feature requests! -->


**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug
/kind feature

**What happened**:
Kubernetes interprets value APIGroup = ""*"" as all API groups APIs, and empty string (APIGroup = """") as only Core API group APIs when defining authorization roles. The webhook authorization code (https://github.com/kubernetes/cloud-provider-openstack/blob/d22e37ede33bfe0bddeb7a9f57d13b74731105e0/pkg/identity/keystone/authorizer.go#L50) does not do Core only APIGroup validation.

**What you expected to happen**:
In policy file if version = """", then authorizer need to validate that only Core Group APIs are permitted. That is be able to only permit Core API group APIs when version = """" in policy file.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-06-22 18:21:47,2018-07-05 13:38:20
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/issues/217,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/217,Missing image manila-provisioner during uploading images in Makefile,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
upload image manila-provisioner failed when running Makefile, here is my output:
2018-06-24 01:54:44.993212 | ubuntu-xenial-vexxhost | docker push docker.io/k8scloudprovider/manila-provisioner:latest
2018-06-24 01:54:45.088064 | ubuntu-xenial-vexxhost | The push refers to a repository [docker.io/k8scloudprovider/manila-provisioner]
2018-06-24 01:54:45.088299 | ubuntu-xenial-vexxhost | An image does not exist locally with the tag: k8scloudprovider/manila-provisioner
2018-06-24 01:54:45.099917 | ubuntu-xenial-vexxhost | Makefile:254: recipe for target 'upload-images' failed
2018-06-24 01:54:45.100076 | ubuntu-xenial-vexxhost | make: *** [upload-images] Error 1
2018-06-24 01:54:45.379174 | ubuntu-xenial-vexxhost | ERROR
2018-06-24 01:54:45.379500 | ubuntu-xenial-vexxhost | {
2018-06-24 01:54:45.379589 | ubuntu-xenial-vexxhost |   ""delta"": ""0:02:29.734363"",
2018-06-24 01:54:45.379663 | ubuntu-xenial-vexxhost |   ""end"": ""2018-06-24 01:54:45.107716"",
2018-06-24 01:54:45.379730 | ubuntu-xenial-vexxhost |   ""failed"": true,
2018-06-24 01:54:45.379794 | ubuntu-xenial-vexxhost |   ""rc"": 2,
2018-06-24 01:54:45.379926 | ubuntu-xenial-vexxhost |   ""start"": ""2018-06-24 01:52:15.373353""
2018-06-24 01:54:45.379998 | ubuntu-xenial-vexxhost | }

[1] http://logs.openlabtesting.org/logs/periodic/github.com/kubernetes/cloud-provider-openstack/master/cloud-provider-openstack-acceptance-test-e2e-conformance-stable-branch-v1.11/d5bb635/job-output.txt.gz

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-06-25 09:00:19,2018-07-05 18:38:18
cloud-provider-openstack,animationzl,https://github.com/kubernetes/cloud-provider-openstack/pull/218,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/218,Missing image manila-provisioner during uploading images in Makefile,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This fixs the missing operation of image manila-provisioner building in Makefile to avoid image does not exist error during image uploading.
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #217 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-06-25 09:11:38,2018-07-09 02:09:26
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/219,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/219,Sync role assignments,"When new namespace is automatically created from a Keystone project, new role
bindings are also should be created to allow the user having the same rights
as in the keystone project.

fixes #45",closed,True,2018-06-25 18:16:20,2018-06-28 13:10:22
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/220,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/220,Ensure that authorizer supports core api group validation,"This patch adds unit tests verifying that if version = """" in the policy file,
then the authorizer must ensure that access is allowed to Core Group API
resources only.

fixes #216 ",closed,True,2018-06-26 14:45:45,2018-07-05 13:38:20
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/221,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/221,Format tables in manila provisioner doc,"Tables used to describe configuration options in Manila provisoner 
document are not easily readable, This patch corrects the 
table formatting

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR fixes table formatting in manila provisioner doc

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-07-02 06:27:35,2018-11-27 08:37:17
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/222,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/222,Support to differentiate cloud load balancers created for 'LoadBalancer' type services,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
In current k8s deployment on openstack, the only way to get all the load balancers created for the services of `LoadBalancer` type for a specific k8s cluster is:

- Get all the services in that k8s cluster
- 'Calculate' the load balancer name according to the code [here](https://github.com/kubernetes/kubernetes/blob/a3d30dc939c102be2eec3d7d5c7f33f63a1e9893/pkg/cloudprovider/cloud.go#L67)

which is not an easy task for the end users or operators.

**What you expected to happen**:
As an end users or operators, I can easily get all the load balancers created for a specific k8s cluster. 

Suggestion: As the cloud provider, we could set the param `--cluster-name` of controller manager for different k8s clusters and inject the cluster name in the load balancer description.

**How to reproduce it (as minimally and precisely as possible)**:
N/A

**Anything else we need to know?**:
N/A

**Environment**:
N/A
",closed,False,2018-07-03 02:05:31,2018-07-05 02:19:22
cloud-provider-openstack,gonzolino,https://github.com/kubernetes/cloud-provider-openstack/pull/223,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/223,Add cluster name to LB description,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Add cluster name to the description of Loadbalancers to help operators with the differentiation of clusters.
The cluster name is passed to the cloud-controller-manager using the `--cluster-name` parameter. If it is not set 'kubernetes' is used as default cluster name.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #222

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```
Add cluster name to LB description
```",closed,True,2018-07-03 12:46:59,2018-07-05 02:19:22
cloud-provider-openstack,hogepodge,https://github.com/kubernetes/cloud-provider-openstack/pull/224,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/224,Refactor the configuration guide,"
<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Refactoring of the configuration guide with the following features:
* Added a table of contents for easier navigation.
* Moved required and optional parameters into different sections.
* Alphabetized groups and keys (excepting Global).
* Replaces sample values with consistent placeholder text.

This is part of the effort to standardize documentation across
all cloud providers in collaboration with SIG-Cloud-Provider

**Special notes for your reviewer**:

This is a work in progress in collaboration with SIG-Cloud-Provider. Feedback from not just the OpenStack provider community, but other provider teams, is welcome.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
NONE
",closed,True,2018-07-03 22:43:10,2018-07-19 00:46:06
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/225,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/225,Add auth data sync documentation,"This commit adds documentation describing the use of the new feature for auth data synchronization.
",closed,True,2018-07-05 22:55:03,2018-07-19 00:47:20
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/pull/226,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/226,Manila: provisioning existing shares,"**What this PR does / why we need it**:
Makes it possible to use existing Manila shares in the manila external-provisioner: the user supplies the share name/ID and the share access ID in StorageClass params.
This has implications on dealing with `reclaimPolicy`: if the share has been created by the external-provisioner, it will be deleted if `reclaimPolicy: Delete`. If the user has supplied share name/ID, it won't delete the underlying share regardless of the reclaim policy.

Added 3 new PV annotations:
- Provision type: can be either dynamic or static (newly created share or an existing one)
- ShareSecretName, ShareSecretNamespace: secret ref for the share secrets

A big chunk of code deals with `shareoptions`, which now supports `value` tag with these options: `optional`, `default=`, `coalesce=`, `requires=`

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```

/cc @dims @flaper87 ",closed,True,2018-07-09 13:30:05,2018-07-19 09:40:57
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/issues/227,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/227,Reflect Kubernetes best practices on what a project should look like,https://github.com/kubernetes/kubernetes-template-project#kubernetes-template-project,closed,False,2018-07-09 17:24:49,2018-08-22 05:50:30
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/228,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/228,Add building way of k8s-keystone-auth binary,"**What this PR does / why we need it**:

This adds the building way of k8s-keystone-auth binary.

In addition, the follwoing two option names are similar and the
passed files are the same on the doc.
  --authorization-webhook-config-file
  --authentication-token-webhook-config-file
It is easy to cause confusion with current describing way.
This makes the description of authorization config flags align for
the above authentication way for easy understanding.

**Release note**: `NONE`
",closed,True,2018-07-12 00:34:39,2018-07-19 00:55:42
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/229,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/229,Merge multiple createKubernetesClient calls,"**What this PR does / why we need it**:

There were duplicated createKubernetesClient calls for conditions.
Then this merges them into a single call for the code cleanup.

**Release note**: `NONE`
",closed,True,2018-07-13 00:29:54,2018-07-19 00:55:47
cloud-provider-openstack,aojea,https://github.com/kubernetes/cloud-provider-openstack/issues/230,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/230,Missing CONTRIBUTING.md file,"All K8s subrepositories should have a CONTRIBUTING.md file, which at the minimum should point to https://github.com/kubernetes/community/blob/master/contributors/guide/README.md. Care should be taken that all information is in sync with the contributor guide.

Subrepositories may also have contributing guidelines specific to that repository. They should be explicitly documented and explained in the CONTRIBUTING.md

Ref:  https://github.com/kubernetes/community/issues/1832",closed,False,2018-07-16 09:23:23,2018-07-19 01:10:35
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/231,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/231,Add Contributor Guidelines doc,"This patch adds general contributing guidelines for cloud-provider-openstack.

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR adds document for general contributing guidelines to cloud-provider-openstack

**Which issue this PR fixes** : 
fixes #230 

",closed,True,2018-07-17 10:08:10,2018-11-27 08:37:22
cloud-provider-openstack,RdL87,https://github.com/kubernetes/cloud-provider-openstack/issues/232,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/232,Using k8s-keystone-auth with OpenStack Application Credentials,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature


**What happened**:

The k8s-keystone-auth component requires some values to be set from the user in the OS env.

**What you expected to happen**:

It should be very useful in order to simplify the user experience adding support also to the Application Credentials (https://docs.openstack.org/keystone/queens/user/application_credentials.html) OpenStack authentication.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-07-17 14:38:10,2018-10-08 14:55:35
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/233,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/233,Refactor: Reduce indents of resourceMatches(),"**What this PR does / why we need it**:

The methods had deep indents and it was difficult to understand
how to work on resourceMatches() and nonResourceMatches().
This reduces these indents for reading the code easily.

**Release note**: `NONE`
",closed,True,2018-07-17 23:05:28,2018-07-19 01:05:42
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/234,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/234,Sync with Kubernetes master repo,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

- We had commits in k/k that were not applied here. 10 commits in total.
- Switched from 10.x versions to 11.x versions for kubernetes dependencies
- Removed some unnecessary files (vendored them instead of having a copy that we need to keep in sync)
- Removed a temporary `getVolumeSource` and use the `getVolumeInfo` as in the main k/k master

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-07-19 01:51:49,2018-07-20 00:56:53
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/pull/235,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/235,Manila: trustee authentication + custom CAs,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

Adds support for trustee authentication and supplying custom CAs.
The OpenStack Secret may now contain:
- `os-trustID`, `os-trusteeID` and `os-trusteePassword` for trustee auth
- `os-certAuthority` and `os-TLSInsecure` for supplying a custom CA, and optionally allowing it to be insecure

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```

/cc @dims @hogepodge @flaper87 
",closed,True,2018-07-19 09:38:46,2018-07-19 15:19:52
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/236,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/236,Add keystone-webhook-auth sample for ubuntu 16.04,"**What this PR does / why we need it**:

Sometimes we had questions how to configure keystone-webhook-auth
on ubuntu 16.04, because that is a little far from current doc
explanation. So this adds the sample configuration for ubuntu 16.04
users.

**Release note**: `NONE`",closed,True,2018-07-20 21:48:32,2018-07-23 10:55:50
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/issues/237,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/237,k8s-keystone-auth outputs a lot of authorization failure logs,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

k8s-keystone-auth outputs a lot of authorization failure logs like:
```
I0720 22:07:33.529595     439 authorizer.go:197] Authorization failed, user: &user.DefaultInfo{Name:""system:kube-controller-manager"", UID:"""", Groups:[]string{""system:authenticated""}, Extra:map[string][]string(nil)}, attributes: authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc420461d40), Verb:""get"", Namespace:"""", APIGroup:"""", APIVersion:"""", Resource:"""", Subresource:"""", Name:"""", ResourceRequest:false, Path:""/apis/admissionregistration.k8s.io/v1beta1""}
I0720 22:07:33.533982     439 authorizer.go:197] Authorization failed, user: &user.DefaultInfo{Name:""system:kube-controller-manager"", UID:"""", Groups:[]string{""system:authenticated""}, Extra:map[string][]string(nil)}, attributes: authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc420448300), Verb:""get"", Namespace:"""", APIGroup:"""", APIVersion:"""", Resource:"""", Subresource:"""", Name:"""", ResourceRequest:false, Path:""/apis/apiextensions.k8s.io/v1beta1""}
I0720 22:07:33.537410     439 authorizer.go:197] Authorization failed, user: &user.DefaultInfo{Name:""system:kube-controller-manager"", UID:"""", Groups:[]string{""system:authenticated""}, Extra:map[string][]string(nil)}, attributes: authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc420461e80), Verb:""get"", Namespace:"""", APIGroup:"""", APIVersion:"""", Resource:"""", Subresource:"""", Name:"""", ResourceRequest:false, Path:""/apis/scheduling.k8s.io/v1beta1""}
I0720 22:07:34.194114     439 authorizer.go:197] Authorization failed, user: &user.DefaultInfo{Name:""system:kube-scheduler"", UID:"""", Groups:[]string{""system:authenticated""}, Extra:map[string][]string(nil)}, attributes: authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc420461f00), Verb:""get"", Namespace:""kube-system"", APIGroup:"""", APIVersion:""v1"", Resource:""endpoints"", Subresource:"""", Name:""kube-scheduler"", ResourceRequest:true, Path:""""}
I0720 22:07:34.197555     439 authorizer.go:197] Authorization failed, user: &user.DefaultInfo{Name:""system:kube-controller-manager"", UID:"""", Groups:[]string{""system:authenticated""}, Extra:map[string][]string(nil)}, attributes: authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc420448540), Verb:""get"", Namespace:""kube-system"", APIGroup:"""", APIVersion:""v1"", Resource:""endpoints"", Subresource:"""", Name:""kube-controller-manager"", ResourceRequest:true, Path:""""}
I0720 22:07:34.214007     439 authorizer.go:197] Authorization failed, user: &user.DefaultInfo{Name:""system:kube-scheduler"", UID:"""", Groups:[]string{""system:authenticated""}, Extra:map[string][]string(nil)}, attributes: authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc4204485c0), Verb:""update"", Namespace:""kube-system"", APIGroup:"""", APIVersion:""v1"", Resource:""endpoints"", Subresource:"""", Name:""kube-scheduler"", ResourceRequest:true, Path:""""}
I0720 22:07:34.223450     439 authorizer.go:197] Authorization failed, user: &user.DefaultInfo{Name:""system:kube-controller-manager"", UID:"""", Groups:[]string{""system:authenticated""}, Extra:map[string][]string(nil)}, attributes: authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc420448700), Verb:""update"", Namespace:""kube-system"", APIGroup:"""", APIVersion:""v1"", Resource:""endpoints"", Subresource:"""", Name:""kube-controller-manager"", ResourceRequest:true, Path:""""}
I0720 22:07:53.229992     439 authorizer.go:197] Authorization failed, user: &user.DefaultInfo{Name:""system:serviceaccount:kube-system:pod-garbage-collector"", UID:"""", Groups:[]string{""system:serviceaccounts"", ""system:serviceaccounts:kube-system"", ""system:authenticated""}, Extra:map[string][]string(nil)}, attributes: authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc420448840), Verb:""list"", Namespace:"""", APIGroup:"""", APIVersion:""v1"", Resource:""nodes"", Subresource:"""", Name:"""", ResourceRequest:true, Path:""""}
```

**What you expected to happen**:

These logs seem related to the Kubernetes controller processes and it seems unnecessary to be output.

**How to reproduce it (as minimally and precisely as possible)**:

We can reproduce this with the way of https://github.com/kubernetes/cloud-provider-openstack/pull/236

**Anything else we need to know?**:

I guess this kind of thing should be controlled with examples/webhook/policy.json and it would be necessary to update the file for this common situation.
In addition, the way to update examples/webhook/policy.json is not described on any document.
So we need to add the document also.

**Environment**:
- openstack-cloud-controller-manager version: the latest master (commit: 5bbf82c0054af6ad87b061fe9355bfe5a1b71f9a)
- OS (e.g. from /etc/os-release): VERSION=""16.04.4 LTS (Xenial Xerus)""
- Kernel (e.g. `uname -a`): Linux k8s-master 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- Install tools: kubeadm
- Others:
",closed,False,2018-07-20 22:17:37,2018-10-19 00:44:05
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/issues/238,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/238,k8s-keystone-auth: Necessary to add the way how to configure policy.json,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:

The k8s-keystone-auth document says
```
Copy the examples/webhook/policy.json and edit it to your needs.
```
but there is not any document which explains how to configure the policy.json file.
Then users need to investigate the code and edit the file.
That is hard to use the policy.json file for users.

**What you expected to happen**:

It is nice to have some related document which explains how to configure the policy.json file.

**Anything else we need to know?**:

This is related to https://github.com/kubernetes/cloud-provider-openstack/issues/237

**Environment**:
- openstack-cloud-controller-manager version: the latest master (commit: 5bbf82c0054af6ad87b061fe9355bfe5a1b71f9a)
- OS (e.g. from /etc/os-release): VERSION=""16.04.4 LTS (Xenial Xerus)""
- Kernel (e.g. `uname -a`): Linux k8s-master 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- Install tools: kubeadm
- Others:
",closed,False,2018-07-20 22:28:43,2018-10-19 00:44:27
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/239,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/239,Add documentation for k8s-keystone-auth,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Add the missing documentation for k8s-keystone-auth service. It only covers how to config and run the k8s-keystone-auth as a k8s service rather than the static pod.

**Which issue this PR fixes**: 
N/A

**Special notes for your reviewer**:
N/A

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-07-23 07:59:29,2018-07-24 00:10:24
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/240,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/240,client-keystone-auth: Project name is missing for authentication,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
client-keystone-auth doesn't work

**What you expected to happen**:
client-keystone-auth should work after kubeconfig configuration.

**How to reproduce it (as minimally and precisely as possible)**:
1. `kubectl config set-credentials openstackuser-clientauth`
2. Edit ~/.kube/config file, add the information client-keystone-auth needs

```yaml
users:
- name: openstackuser-clientauth
  user:
    exec:
      command: ""/home/ubuntu/client-keystone-auth""
      apiVersion: ""client.authentication.k8s.io/v1alpha1""
      env:
        - name: ""OS_USERNAME""
          value: ""demo""
        - name: ""OS_PASSWORD""
          value: ""password""
        - name: ""OS_PROJECT_NAME""
          value: ""demo""
        - name: ""OS_TENANT_NAME""
          value: ""demo""
        - name: ""OS_REGION_NAME""
          value: ""RegionOne""
        - name: ""OS_IDENTITY_API_VERSION""
          value: ""3""
        - name: ""OS_DOMAIN_NAME""
          value: ""default""
      args:
        - ""--keystone-url=http://10.140.81.86/identity/v3""
        - ""--domain-name=default""
```

3. In webhook service policy definition, allow user `demo` with `kube-viewer` role to get pods
4. Run `kubectl get pods`, the authorization failed.

```shell
$ kubectl get po
Keystone auth url is ""http://10.140.81.86/identity/v3""
Domain name is ""default""
User name is ""demo""
Password is set
Error from server (Forbidden): pods is forbidden: User ""demo"" cannot list pods in the namespace ""default""
```

The reason is the token client-keystone-auth gets from keystone is an unscoped token because the project name is not specified when fetching token.


",closed,False,2018-07-24 00:44:40,2018-07-27 13:37:57
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/241,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/241,Fix authentication error for client-auth-plugin,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Fix authentication error for client-auth-plugin

**Which issue this PR fixes**: 
fixes #240

**Special notes for your reviewer**:
With this patch, the client-keystone-auth could work the same with the traditional way. You can config the kubeconfig file as the following, you can use environment variables and don't need to add any params:

```yaml
users:
- name: openstackuser-clientauth
  user:
    exec:
      command: ""/home/ubuntu/client-keystone-auth""
      apiVersion: ""client.authentication.k8s.io/v1alpha1""
```

Then run `kubectl get pods`.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-07-24 00:49:10,2018-07-28 00:27:03
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/242,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/242,Add Support for CSI Spec v0.3.0 in cinder CSI plugin,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:


/kind feature


**What happened**:
 CSI Spec v0.3.0 is released, needs to add support for it
https://github.com/container-storage-interface/spec/releases/tag/v0.3.0

",closed,False,2018-07-25 07:10:16,2018-08-30 10:18:32
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/243,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/243,"Allow to provide user, domain and project ids to the client authenticator","In many cases it's necessary to use ids instead of names to provide information
for authentication.

This patch adds new options that allow to get the related id parameters from
OpenStack env variable, from command arguments and from the terminal.

Also this patch allows to leave project(id/name) empty, because these parameters
are not mandatory for authentication - in this case domain-scoped token will be
received.",closed,True,2018-07-27 17:37:28,2018-10-08 14:57:19
cloud-provider-openstack,xiaoxubeii,https://github.com/kubernetes/cloud-provider-openstack/pull/244,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/244,Fix that fail to resize pvc of cinder volume.,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Fix that fails to resize pvc of cinder volume. See also #[66687](https://github.com/kubernetes/kubernetes/issues/66687).

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: 
fixes #[66705](https://github.com/kubernetes/kubernetes/issues/66705)

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-07-31 07:12:18,2018-07-31 10:08:38
cloud-provider-openstack,ssfilatov,https://github.com/kubernetes/cloud-provider-openstack/issues/245,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/245,Associate FIP with LB port via LoadBalancerIP attribute,"**Is this a BUG REPORT or FEATURE REQUEST?**:

> /kind feature

I'd like to be able to associate allocated floating IP address with k8s service.
We could extend cloudprovider so that when LoadBalancerIP is mentioned in service spec, first we try to GET it and then associate with the loadbalancer port. If we fail to either GET it floating IP with the port, we fall back to creating.

I can provide a patch for this if you think it's an okay behavior.
",closed,False,2018-08-01 10:53:06,2018-08-27 01:59:40
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/246,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/246,Add Manila Provisioner binary to .gitignore,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR adds Manila Provisioner binary to .gitignore
",closed,True,2018-08-02 05:59:43,2018-11-27 08:38:10
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/247,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/247,Add Support for CSIv.0.3.0 in Cinder CSI plugin,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR adds support for CSI v.0.3.0 in cinder CSI plugin
https://github.com/container-storage-interface/spec/releases/tag/v0.3.0

**Which issue this PR fixes** : 
fixes #242 

**Release note**:
NONE.
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-08-02 06:46:33,2018-11-27 08:38:04
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/248,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/248,Add Toplogy support in Cinder CSI,"<!-- This form is for bug reports and feature requests! -->


**Is this a BUG REPORT or FEATURE REQUEST?**:

 /kind feature

**What happened**:
Implement Toplogy Support in Cinder CSI plugin.
A new toplogy API added In CSI v.0.3.0, It defines the accessibilty constraint(Regions, Zones) between a Node and Volume https://github.com/container-storage-interface/spec/releases/tag/v0.3.0.  It is good, if 
Cinder CSI Plugin can support this capabality
",closed,False,2018-08-02 07:31:34,2018-09-28 06:06:06
cloud-provider-openstack,npu21,https://github.com/kubernetes/cloud-provider-openstack/issues/249,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/249,Unable to mount volumes for pod with the pvc created by cinder-provisioner,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug


**What happened**:
I create a deployment like this
```
apiVersion: apps/v1beta2
kind: Deployment          
metadata:
  name: mysql-sonar
spec:
  replicas: 1                          
  selector:
    matchLabels:
      app: mysql-sonar                      
  template:
    metadata:
      labels:
        app: mysql-sonar
    spec:
      containers:                       
      - name: mysql-sonar
        image: harbor.iop.com:5000/library/iop/mysql:5.7.22
        args:
          - ""--ignore-db-dir=lost+found""      
        ports:
        - containerPort: 3306           
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: 123456XXX
        volumeMounts:
        - name: mysql-sonar
          mountPath: /var/mysql
      volumes:
      - name: mysql-sonar
        persistentVolumeClaim:
          claimName: mysql-sonar-pvc
```
the pvc mysql-sonar-pvc is create by cinder-provisioner
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-sonar-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: cinder-standard-iops
```
and it's bound
mysql-sonar-pvc                              Bound     pvc-4c853f30-9625-11e8-bfa3-0050568860dd   1Gi        RWO            cinder-standard-iops   13m
I can also see it's created successfully in cinder api

> Name:            pvc-4c853f30-9625-11e8-bfa3-0050568860dd
> Labels:          <none>
> Annotations:     cinderVolumeId=2ba19e9b-9430-44d2-8198-dbc6ca28acd5
>                  pv.kubernetes.io/provisioned-by=openstack.org/standalone-cinder
>                  standaloneCinderProvisionerIdentity=openstack.org/standalone-cinder
> StorageClass:    cinder-standard-iops
> Status:          Bound
> Claim:           default/mysql-sonar-pvc
> Reclaim Policy:  Delete
> Access Modes:    RWO
> Capacity:        1Gi
> Message:         
> Source:
>     Type:          RBD (a Rados Block Device mount on the host that shares a pod's lifetime)
>     CephMonitors:  [192.168.4.181:6789 192.168.4.182:6789 192.168.4.183:6789]
>     RBDImage:      volume-2ba19e9b-9430-44d2-8198-dbc6ca28acd5
>     FSType:        
>     RBDPool:       volumes
>     RadosUser:     cinder
>     Keyring:       /etc/ceph/keyring
>     SecretRef:     &{cinder-standard-iops-cephx-secret }
>     ReadOnly:      false
> Events:            <none>

here's the secret 
```
Name:         cinder-standard-iops-cephx-secret
Namespace:    default
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
key:  41 bytes
```
**What you expected to happen**:
the deployment are expected run successfully,however it failed at this step
![image](https://user-images.githubusercontent.com/11768607/43570026-e4783b62-966b-11e8-92f6-692676749cd1.png)
and here's the more detail kubernetes log
Aug 01 21:08:50 master2 kubelet[2490]: I0801 21:08:50.540947    2490 reconciler.go:217] operationExecutor.VerifyControllerAttachedVolume started for volume ""pvc-ea7ba804-955b-11e8-99c3-0050568860dd"" (UniqueName: ""kubernetes.io/rbd/[192.168.4.181:6789 192.168.4.182:6789 192.168.4.183:6789]:volume-98adc6e3-c108-4f62-9cb3-91aebab2094d"") pod ""mysql-sonar-7f75f65b64-sv5pn"" (UID: ""5643209b-95f0-11e8-905f-005056885e00"")
Aug 01 21:08:50 master2 kubelet[2490]: I0801 21:08:50.546307    2490 operation_generator.go:1121] Controller attach succeeded for volume ""pvc-ea7ba804-955b-11e8-99c3-0050568860dd"" (UniqueName: ""kubernetes.io/rbd/[192.168.4.181:6789 192.168.4.182:6789 192.168.4.183:6789]:volume-98adc6e3-c108-4f62-9cb3-91aebab2094d"") pod ""mysql-sonar-7f75f65b64-sv5pn"" (UID: ""5643209b-95f0-11e8-905f-005056885e00"") device path: """"
Aug 01 21:08:50 master2 kubelet[2490]: I0801 21:08:50.647216    2490 reconciler.go:262] operationExecutor.MountVolume started for volume ""pvc-ea7ba804-955b-11e8-99c3-0050568860dd"" (UniqueName: ""kubernetes.io/rbd/[192.168.4.181:6789 192.168.4.182:6789 192.168.4.183:6789]:volume-98adc6e3-c108-4f62-9cb3-91aebab2094d"") pod ""mysql-sonar-7f75f65b64-sv5pn"" (UID: ""5643209b-95f0-11e8-905f-005056885e00"")
Aug 01 21:08:50 master2 kubelet[2490]: I0801 21:08:50.647296    2490 operation_generator.go:447] MountVolume.WaitForAttach entering for volume ""pvc-ea7ba804-955b-11e8-99c3-0050568860dd"" (UniqueName: ""kubernetes.io/rbd/[192.168.4.181:6789 192.168.4.182:6789 192.168.4.183:6789]:volume-98adc6e3-c108-4f62-9cb3-91aebab2094d"") pod ""mysql-sonar-7f75f65b64-sv5pn"" (UID: ""5643209b-95f0-11e8-905f-005056885e00"") DevicePath """"
Aug 01 21:08:51 master2 kubelet[2490]: E0801 21:08:51.367069    2490 kubelet.go:1630] Unable to mount volumes for pod ""mysql-sonar-7f75f65b64-sv5pn_default(5643209b-95f0-11e8-905f-005056885e00)"": timeout expired waiting for volumes to attach/mount for pod ""default""/""mysql-sonar-7f75f65b64-sv5pn"". list of unattached/unmounted volumes=[mysql-sonar]; skipping pod
Aug 01 21:08:51 master2 kubelet[2490]: E0801 21:08:51.367159    2490 pod_workers.go:186] Error syncing pod 5643209b-95f0-11e8-905f-005056885e00 (""mysql-sonar-7f75f65b64-sv5pn_default(5643209b-95f0-11e8-905f-005056885e00)""), skipping: timeout expired waiting for volumes to attach/mount for pod ""default""/""mysql-sonar-7f75f65b64-sv5pn"". list of unattached/unmounted volumes=[mysql-sonar]

Any help will be appreciated very much

",closed,False,2018-08-02 07:49:30,2019-01-05 09:42:33
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/250,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/250,Add code-of-conduct.md,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Add code-of-conduct.md file as suggested by 
kubernetes template project
https://github.com/kubernetes/kubernetes-template-project#kubernetes-template-project

 fixes #227

**Release note**:
None.
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-08-02 09:57:10,2018-08-22 06:11:12
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/pull/251,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/251,Run conformance periodic jobs on specific branches for master,"**What this PR does / why we need it**:
It is not necessary to run both master and release 1.10 branch
cloud-provider-openstack to test against k8s master, v1.10 and v1.11
in OpenLab periodic jobs, that allocate lots of testing vm resources.
Add branches option into job define in order to limiting specific job
just run on specific branch.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-08-03 10:09:46,2018-08-07 06:45:28
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/pull/252,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/252,Run conformance periodic jobs on specific branches for 1.10,"**What this PR does / why we need it**:
It is not necessary to run both master and release 1.10 branch
cloud-provider-openstack to test against k8s master, v1.10 and v1.11
in OpenLab periodic jobs, that allocate lots of testing vm resources.
Add branches option into job define in order to limiting specific job
just run on specific branch.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-08-03 10:21:08,2018-08-10 07:08:46
cloud-provider-openstack,imdigitaljim,https://github.com/kubernetes/cloud-provider-openstack/pull/253,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/253,Add readable and configurable loadbalancer naming,"**What this PR does / why we need it**:
Currently LB's are named in Neutron based on what Google and AWS needs by using an in-tree cloudprovider function. Since this driver is now out of tree, we can make this a feature improve the experience for users on Horizon and API's and correlate the loadbalancer name and description in a more meaningful way.
",closed,True,2018-08-03 17:43:47,2018-08-03 17:59:03
cloud-provider-openstack,embik,https://github.com/kubernetes/cloud-provider-openstack/issues/254,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/254,Pod with PVC manged by cinder-standalone-provisioner trying to mount non-existent secret cinder-cephx-secret,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

The cinder-standalone-provisioner did not create a cephx-secret for my storage class and therefore the pod is stuck at `ContainerCreating`, missing the necessary secret.

I can see the Cinder volume itself though, so the provisioner is able to communicate with Cinder and has the necessary permissions to create volumes.

Logs from the cinder-standalone-provisioner after creating the pvc and pod:

```
I0808 07:57:42.755360       1 controller.go:1167] scheduleOperation[lock-provision-pv-test/myclaim[ba81ce18-9ae0-11e8-82ba-fa163e881e72]]
I0808 07:57:42.777259       1 controller.go:1167] scheduleOperation[lock-provision-pv-test/myclaim[ba81ce18-9ae0-11e8-82ba-fa163e881e72]]
I0808 07:57:42.815237       1 leaderelection.go:156] attempting to acquire leader lease...
I0808 07:57:42.832417       1 leaderelection.go:178] successfully acquired lease to provision for pvc pv-test/myclaim
I0808 07:57:42.833217       1 controller.go:1167] scheduleOperation[provision-pv-test/myclaim[ba81ce18-9ae0-11e8-82ba-fa163e881e72]]
I0808 07:57:47.088316       1 controller.go:1167] scheduleOperation[provision-pv-test/myclaim[ba81ce18-9ae0-11e8-82ba-fa163e881e72]]
I0808 07:57:47.088705       1 controller.go:900] volume ""pvc-ba81ce18-9ae0-11e8-82ba-fa163e881e72"" for claim ""pv-test/myclaim"" created
I0808 07:57:47.098794       1 controller.go:917] volume ""pvc-ba81ce18-9ae0-11e8-82ba-fa163e881e72"" for claim ""pv-test/myclaim"" saved
I0808 07:57:47.098913       1 controller.go:953] volume ""pvc-ba81ce18-9ae0-11e8-82ba-fa163e881e72"" provisioned for claim ""pv-test/myclaim""
I0808 07:57:48.892016       1 leaderelection.go:198] stopped trying to renew lease to provision for pvc pv-test/myclaim, task succeeded
```

Here's the pod's event section after creation:

```
Events:
  Type     Reason                  Age               From                                         Message
  ----     ------                  ----              ----                                         -------
  Warning  FailedScheduling        3m (x4 over 3m)   default-scheduler                            pod has unbound PersistentVolumeClaims (repeated 3 times)
  Normal   Scheduled               3m                default-scheduler                            Successfully assigned web to worker-02.<fqdn>
  Normal   SuccessfulAttachVolume  3m                attachdetach-controller                      AttachVolume.Attach succeeded for volume ""pvc-ba81ce18-9ae0-11e8-82ba-fa163e881e72""
  Normal   SuccessfulMountVolume   3m                kubelet, worker-02.<fqdn>                    MountVolume.SetUp succeeded for volume ""default-token-97xwh""
  Warning  FailedMount             3m (x24 over 3m)  kubelet, worker-02.<fqdn>                    MountVolume.NewMounter initialization failed for volume ""pvc-ba81ce18-9ae0-11e8-82ba-fa163e881e72"" : Couldn't get secret pv-test/cinder-cephx-secret err: secrets ""cinder-cephx-secret"" not found
```


**What you expected to happen**:

I guess the pod should either not try to mount the cephx-secret or the cephx-secret should have been created by the provisioner beforehand.

**How to reproduce it (as minimally and precisely as possible)**:

See section below. It's reproducable with our OpenStack environment.

**Anything else we need to know?**:

My Kubernetes resources:
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cinder-provisioner
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cinder-provisioner:cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: """"
    kind: ServiceAccount
    name: cinder-provisioner
    namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: standalone-cinder-provisioner
spec:
  selector:
    matchLabels:
      app: standalone-cinder-provisioner
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: standalone-cinder-provisioner
    spec:
      serviceAccountName: cinder-provisioner
      containers:
      - name: standalone-cinder-provisioner
        image: docker.io/k8scloudprovider/cinder-provisioner:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: OS_AUTH_URL
          value: <Auth Url>
        - name: OS_USERNAME
          value: <Username>
        - name: OS_PASSWORD
          value: <Password>
        - name: OS_TENANT_ID
          value: <Tenant>
        - name: OS_REGION_NAME
          value: <Region>
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: cinder
provisioner: openstack.org/standalone-cinder
```

And my test resources:

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi # pass here the size of the volume
  storageClassName: cinder
---
apiVersion: v1
kind: Pod
metadata:
  name: web
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          hostPort: 8081
          protocol: TCP
      volumeMounts:
        - mountPath: ""/usr/share/nginx/html""
          name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
```

**Environment**:
- openstack-cloud-controller-manager version: not in use; Image `docker.io/k8scloudprovider/cinder-provisioner:latest` used
- OS (e.g. from /etc/os-release): kubelet in Docker Image `Debian GNU/Linux 9 (stretch)`
- Kernel (e.g. `uname -a`): 4.14.44-coreos-r1
- Install tools: kubectl, see section above
- Others:
   - Kubernetes: 1.10.6
   - OpenStack: Ocata
",closed,False,2018-08-08 08:28:15,2019-02-11 10:43:18
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/255,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/255,Install dep instead of glide in getting started doc,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
cloud-provider-openstack uses dep for dependency management,
This  PR adds step to install dep in getting started document.",closed,True,2018-08-09 05:34:48,2018-11-27 08:38:00
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/256,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/256,Refer getting started document in contributing.md,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Update contributing.md with link of getting started document.",closed,True,2018-08-09 05:44:56,2018-11-27 08:38:02
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/257,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/257,Add OS_DOMAIN_NAME on the manifest,"**What this PR does / why we need it**:

If not specifying OS_DOMAIN_NAME on standalone-cinder-provisioner
manifest, the deployment is not deployed and the log is like:

  F0810 02:28:02.332034       1 main.go:86] Error creating Cinder
    provisioner: failed to get volume service: You must provide
    exactly one of DomainID or DomainName to authenticate by Username

So this adds the OS_DOMAIN_NAME on the manifest.

**Release note**: NONE
",closed,True,2018-08-10 02:36:54,2018-08-10 10:12:36
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/pull/258,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/258,Manila: update csi-cephfs backend to 0.3.0,"**What this PR does / why we need it**:
csi-cephfs has been updated to 0.3.0, this PR updates the manila-provisioner `ShareBackend` for csi-cephfs.

(unrelated) also adds a demo pod to `examples` - this will be used in the manil-provisioner acceptance test that is being prepared

**Release note**:
```release-note
NONE
```
",closed,True,2018-08-10 12:38:32,2018-08-14 02:23:27
cloud-provider-openstack,zioproto,https://github.com/kubernetes/cloud-provider-openstack/pull/259,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/259,Improve docs about auth data syncronization,"**What this PR does / why we need it**:

Improves documentation about Auth data synchronization between Keystone and Kubernetes


",closed,True,2018-08-10 14:49:31,2018-09-21 01:30:57
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/issues/260,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/260,"cinder-standalone: Failed to create a volume with ""Resource not found""","**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

To create a volume with PVC and default StorageClass on cinder-standalone provisioner, 
that was failed during a volume creation like the following cinder-standalone log:

```
I0810 22:06:01.796035       1 controller.go:492] Starting provisioner controller 918a1a6c-9ce9-11e8-8be4-0a580af401ce!
I0810 22:06:01.836948       1 controller.go:1167] scheduleOperation[lock-provision-default/cinder-claim[8a8b5234-9ce9-11e8-a146-fa163e420595]]
I0810 22:06:01.884646       1 leaderelection.go:156] attempting to acquire leader lease...
I0810 22:06:01.904134       1 leaderelection.go:178] successfully acquired lease to provision for pvc default/cinder-claim
I0810 22:06:01.904339       1 controller.go:1167] scheduleOperation[provision-default/cinder-claim[8a8b5234-9ce9-11e8-a146-fa163e420595]]
E0810 22:06:02.098094       1 actions.go:71] Failed to create a 1 GiB volume: Resource not found
E0810 22:06:02.098348       1 provisioner.go:184] Failed to create volume
E0810 22:06:02.100136       1 controller.go:895] Failed to provision volume for claim ""default/cinder-claim"" with StorageClass ""gold"": Resource not found
E0810 22:06:02.101034       1 goroutinemap.go:150] Operation for ""provision-default/cinder-claim[8a8b5234-9ce9-11e8-a146-fa163e420595]"" failed. No retries permitted until 2018-08-10 22:06:02.600670206 +0000 UTC m=+1.271658978 (durationBeforeRetry 500ms). Error: ""Resource not found""
I0810 22:06:03.961259       1 leaderelection.go:198] stopped trying to renew lease to provision for pvc default/cinder-claim, task failed
```

**What you expected to happen**:

PVC operation should be successful.

**How to reproduce it (as minimally and precisely as possible)**:

1. Create cinder-standalone
```
$ cat deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: standalone-cinder-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: standalone-cinder-provisioner
    spec:
      serviceAccountName: standalone-cinder-provisioner
      hostAliases:
      - ip: ""192.168.1.1""
        hostnames:
        - ""iaas-ctrl""
      containers:
      - name: standalone-cinder-provisioner
        image: docker.io/k8scloudprovider/cinder-provisioner:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: OS_AUTH_URL
          value: http://iaas-ctrl:5000/v3
        - name: OS_USERNAME
          value: admin
        - name: OS_PASSWORD
          value: XXXXXXXXXXXXXXX
        - name: OS_TENANT_ID
          value: 682e74f275fe427abd9eb6759f3b68c5
        - name: OS_REGION_NAME
          value: RegionOne
        - name: OS_DOMAIN_NAME
          value: Default
$ kubectl create -f deployment.yaml
```
2. Create a default StorageClass
```
$ cat storage-class.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: gold
  annotations:
    storageclass.kubernetes.io/is-default-class: ""true""
provisioner: openstack.org/standalone-cinder
parameters:
  type: fast
  availability: nova
$ kubectl create -f storage-class.yaml
```
3. Create a PVC
```
$ cat pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cinder-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
$ kubectl create -f pvc.yaml
```

**Environment**:
- openstack-cloud-controller-manager version: latest
- OS (e.g. from /etc/os-release): Ubuntu 16.04.5 LTS
- Kernel (e.g. `uname -a`): 4.4.0-130-generic
- Install tools: kubeadm
- Others: OpenStack Queens
",closed,False,2018-08-10 22:18:45,2018-08-10 23:27:21
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/261,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/261,Replace internal provisioner name with external one,"**What this PR does / why we need it**:

External provisioner name is ""openstack.org/standalone-cinder"" which
is hardcoded in this repo. And ""kubernetes.io/cinder"" is the one of
kubernetes/kubernetes in-tree provisioner which is already deprecated.
So this replaces the old name with a new one which is the external
cloud-provider-openstack to avoid confusion.
In addition, this removes the volume type from the example because
the provisioner cannot create a volume if the volume type doesn't
exist on Cinder side and it is hard to debug it.

**Release note**: NONE
",closed,True,2018-08-11 02:20:36,2018-08-21 20:44:45
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/262,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/262,Skip auth data synchronization in the case of unscoped tokens,"When a user sends an unscoped token to the server, Keystone still can authenticate
him, but doesn't include information about user's project and roles in the response.

In this case we have to skip the synchronization part, to prevent possible errors.",closed,True,2018-08-13 11:42:09,2018-08-23 02:22:40
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/263,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/263,Start using client.authentication.k8s.io/v1beta1 api for the client auth plugin,"Recently client authentication api was updated from v1alpha1 to v1beta1.
https://github.com/kubernetes/client-go/commit/f58a8f4fd079a6e2d027afeba4eb8c2ef3a233f5

This commit switches keystone-client-auth on the new api and removes the outdated
one.

The difference between two versions is that in alpha interactive mode was checked
in client-go and provided to the plugin with the environment variable
KUBERNETES_EXEC_INFO. In beta the plugin uses a TTY check to determine if it's
appropriate to prompt a user interactively. That's why KUBERNETES_EXEC_INFO is
not used anymore and can be removed.

Another change is that ExecCredentialSpec struct doesn't contain any fields now,
which means the plugin shouldn't write anything in this section during printing
response output.

https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins
",closed,True,2018-08-13 17:33:10,2018-09-21 01:30:23
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/264,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/264,Loadbalancer: Use cascading delete when using Octavia,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
The current `EnsureLoadBalancerDeleted` method involves too many API calls to the load balancing service.

**What you expected to happen**:
Octavia already supports [cascading deletion](https://developer.openstack.org/api-ref/load-balancer/v2/index.html#remove-a-load-balancer) for the load balancer.

**How to reproduce it (as minimally and precisely as possible)**:
N/A

**Anything else we need to know?**:
gophercloud also supports the feature.

**Environment**:
- openstack-cloud-controller-manager version: master

",closed,False,2018-08-16 00:54:01,2018-08-22 05:56:53
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/265,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/265,Cascading delete loadbalancer resources for Octavia,"**What this PR does / why we need it**:
Cascading delete loadbalancer resources for Octavia when deleting LoadBalancer type service.

**Which issue this PR fixes**: 
fixes #264

**Special notes for your reviewer**:
Reasons to use cascading delete:

- Reduce API calls to Octavia from 10+ to only 1. More API calls means
  more exception risk.
- Simplify the code logic

Tested on my k8s environment on top of OpenStack.

**Release note**:
NONE
",closed,True,2018-08-16 00:56:07,2018-08-22 07:26:13
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/266,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/266,Update Cinder CSI driver to 0.3.0,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR adds support for CSI v.0.3.0 in cinder CSI plugin
https://github.com/container-storage-interface/spec/releases/tag/v0.3.0

**Which issue this PR fixes** : 
fixes #242

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-08-16 10:11:28,2018-11-27 08:38:24
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/pull/267,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/267,Adds ZUUL job for manila-provisioner,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Adds a ZUUL job for manila-provisioner acceptance test
also adds manila-provisioner binary to `.gitignore`

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-08-16 11:57:59,2019-02-12 20:53:00
cloud-provider-openstack,AdamDang,https://github.com/kubernetes/cloud-provider-openstack/pull/268,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/268,Typo fix: guidlines->guidelines,"Line 141: guidlines->guidelines
",closed,True,2018-08-21 05:39:01,2018-08-23 09:19:36
cloud-provider-openstack,jsoyee,https://github.com/kubernetes/cloud-provider-openstack/issues/269,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/269,keystone URL got truncated in k8s-keystone-auth webhook and client,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
Hi All, then i follow the instructions setup k8s-keystone-auth webhook, the services got lunched successfully, followed by the latest version of instruction, we put client binary together with kubectl, when try to do authentication, the keystone URL got truncated. 
my keystone URL: **https://ci22sj-keystone-srv.xxx.com/identity/v3**

[root@deploy sjc02]# kubectl get all
Please enter project name: PROD-Collaboration Cloud Container PaaS
An error occurred: Post **https://ci22sj-keystone-sr/v3/auth/tokens**: dial tcp: lookup ci22sj-keystone-sr on 192.168.65.1:53: no such host
Please enter project name: An error occurred: Post https://ci22sj-keystone-sr/v3/auth/tokens: dial tcp: lookup ci22sj-keystone-sr on 192.168.65.1:53: no such host
Please enter project name: An error occurred: Post https://ci22sj-keystone-sr/v3/auth/tokens: dial tcp: lookup ci22sj-keystone-sr on 192.168.65.1:53: no such host
Please enter project name: An error occurred: Post https://ci22sj-keystone-sr/v3/auth/tokens: dial tcp: lookup ci22sj-keystone-sr on 192.168.65.1:53: no such host

Service log
[root@dfw02-rancher-cnl-etcd01 kubernetes]# kubectl --kubeconfig=kubeconfig.yaml logs service/k8s-keystone-auth-service -n kube-system
Found 2 pods, using pod/k8s-keystone-auth-6457b696cd-9dmnr
W0821 16:48:16.070283       1 config.go:73] Argument --sync-config-file or --sync-configmap-name missing. Data synchronization between Keystone and Kubernetes is disabled.
I0821 16:48:16.070408       1 keystone.go:526] Creating kubernetes API client.
W0821 16:48:16.070417       1 client_config.go:533] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0821 16:48:16.079293       1 keystone.go:543] Kubernetes API client created, server version v1.11
I0821 16:48:16.182751       1 keystone.go:93] ConfigMaps synced and ready
I0821 16:48:16.182799       1 keystone.go:101] Starting webhook server...
I0821 16:48:16.183751       1 keystone.go:155] ConfigMap created or updated, will update the authorization policy.
I0821 16:48:16.183814       1 keystone.go:171] Authorization policy updated.
I0821 16:53:57.773743       1 logs.go:49] http: TLS handshake error from 10.42.0.0:48770: EOF
**Pod logs**
[root@dfw02-rancher-cnl-etcd01 kubernetes]#  kubectl --kubeconfig=kubeconfig.yaml logs pod/k8s-keystone-auth-6457b696cd-7dl99 -n kube-system
W0821 16:48:22.446126       1 config.go:73] Argument --sync-config-file or --sync-configmap-name missing. Data synchronization between Keystone and Kubernetes is disabled.
I0821 16:48:22.446246       1 keystone.go:526] Creating kubernetes API client.
W0821 16:48:22.446257       1 client_config.go:533] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0821 16:48:22.468818       1 keystone.go:543] Kubernetes API client created, server version v1.11
I0821 16:48:22.571908       1 keystone.go:93] ConfigMaps synced and ready
I0821 16:48:22.571990       1 keystone.go:101] Starting webhook server...
I0821 16:48:22.572314       1 keystone.go:155] ConfigMap created or updated, will update the authorization policy.
I0821 16:48:22.572413       1 keystone.go:171] Authorization policy updated.

[root@dfw02-rancher-cnl-etcd01 kubernetes]#  kubectl --kubeconfig=kubeconfig.yaml logs pod/k8s-keystone-auth-6457b696cd-9dmnr -n kube-system
W0821 16:48:16.070283       1 config.go:73] Argument --sync-config-file or --sync-configmap-name missing. Data synchronization between Keystone and Kubernetes is disabled.
I0821 16:48:16.070408       1 keystone.go:526] Creating kubernetes API client.
W0821 16:48:16.070417       1 client_config.go:533] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0821 16:48:16.079293       1 keystone.go:543] Kubernetes API client created, server version v1.11
I0821 16:48:16.182751       1 keystone.go:93] ConfigMaps synced and ready
I0821 16:48:16.182799       1 keystone.go:101] Starting webhook server...
I0821 16:48:16.183751       1 keystone.go:155] ConfigMap created or updated, will update the authorization policy.
I0821 16:48:16.183814       1 keystone.go:171] Authorization policy updated.


**What you expected to happen**:
there should be some logs printed saying if there are some problem this webhook is facing.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
[root@dfw02-rancher-cnl-etcd01 kubernetes]# kubectl --kubeconfig=kubeconfig.yaml version
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.2"", GitCommit:""bb9ffb1654d4a729bb4cec18ff088eacc153c239"", GitTreeState:""clean"", BuildDate:""2018-08-07T23:17:28Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.1"", GitCommit:""b1b29978270dc22fecc592ac55d903350454310a"", GitTreeState:""clean"", BuildDate:""2018-07-17T18:43:26Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
[root@dfw02-rancher-cnl-etcd01 kubernetes]#
 image: k8scloudprovider/k8s-keystone-auth

- OS (e.g. from /etc/os-release):
[root@dfw02-rancher-cnl-etcd01 ~]# cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

[root@dfw02-rancher-cnl-etcd01 ~]#

- Kernel (e.g. `uname -a`):
[root@dfw02-rancher-cnl-etcd01 ~]# uname -a
Linux dfw02-rancher-cnl-etcd01 3.10.0-514.21.1.el7.x86_64 #1 SMP Thu May 25 17:04:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

- Install tools: Rancher2.0.7
- Others:
",closed,False,2018-08-21 17:01:28,2019-02-01 07:12:50
cloud-provider-openstack,hyperbolic2346,https://github.com/kubernetes/cloud-provider-openstack/pull/270,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/270,Adding snap building support to client-keystone-auth plugin.,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Adds snap building to build a snap for the client-keystone-auth plugin. [Snaps](https://snapcraft.io) are a new package system by Ubuntu. Snaps will run on almost any distro and it's super easy to update them. Snaps will update on the user's machine periodically, so the users will always have the latest version that you publish.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
I have also created a [launchpad team](https://launchpad.net/~cloud-provider-openstack) and a [snap store snap](https://dashboard.snapcraft.io/snaps/client-keystone-auth/) for this. The team owns the [launchpad project](https://launchpad.net/cloud-provider-openstack) which watches the github repo and builds snaps automagically. I will finish setting this up once the snapcraft.yaml is in the repo. Another option is to bail on the launchpad builders and use travis to build the snap, which you can read about [here](https://docs.snapcraft.io/build-snaps/ci-integration). The launchpad builders will release the snap to the edge channel only. It is up to something(travis?) to validate those snaps and release them to the stable channel.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Added snap support for the client-keystone-auth binary
```
",closed,True,2018-08-21 18:04:13,2018-08-22 08:29:17
cloud-provider-openstack,hyperbolic2346,https://github.com/kubernetes/cloud-provider-openstack/issues/271,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/271,Templates for Keystone auth are only embedded in docs and not in their own directory,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> /kind feature

yaml files for the keystone auth are only available in the docs and not in a template directory somewhere. It would be much easier to deploy if these files were somewhere that a clone would grab them.

To be clear, I'm looking for things like the service and deployment. It would be nice to also have an example configmap and secrets.",closed,False,2018-08-21 18:10:06,2018-11-19 20:49:05
cloud-provider-openstack,hyperbolic2346,https://github.com/kubernetes/cloud-provider-openstack/pull/272,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/272,Pulling docker for the build,"**What this PR does / why we need it**:
Fixing the snap build process. The launchpad builders didn't like using docker, so I switched it out to just build directly.",closed,True,2018-08-22 19:03:51,2018-08-22 20:49:52
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/issues/273,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/273,keep floatingip after lbaas is deleted,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**: FEATURE

/kind feature


**What happened**: currently if we have service with type: LoadBalancer, it will provision floatingip randomly to lbaas service. However, when we delete that service it will delete floatingip as well.

**What you expected to happen**: I except that we could define what floatingip we should use (that is in separate issue #245). Also I do except that we could say that ""please do not delete the floatingip, I still want to use that ip address"". So I propose new annotation `loadbalancer.openstack.org/keep-floatingip` which does not execute row https://github.com/kubernetes/cloud-provider-openstack/blob/master/pkg/cloudprovider/providers/openstack/openstack_loadbalancer.go#L1448

currently if we want keep floatingip, it should be deassociated BEFORE service delete

",closed,False,2018-08-23 07:48:56,2018-09-08 14:33:13
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/pull/274,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/274,implement keep floatingip annotation,"**What this PR does / why we need it**: we need possibility to keep floatingip after service delete

**Which issue this PR fixes**: fixes #273 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Now it is possible to keep service type: LoadBalancer floatingip address in openstack by defining annotation `loadbalancer.openstack.org/keep-floatingip` to service.
```
",closed,True,2018-08-23 09:04:04,2018-09-08 14:33:23
cloud-provider-openstack,FengyunPan2,https://github.com/kubernetes/cloud-provider-openstack/pull/275,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/275,Make OpenStack cloud provider report a node hostname address,"Cherry pick #67748
Cloud-provider-reported addresses are authoritative in 1.12, this
preserves default behavior that used the internal dns name as
the ""Hostname"" address on Node status.

**Release note**:

```release-note
The OpenStack cloud provider now reports a `Hostname` address type for nodes
```
",closed,True,2018-08-24 01:46:31,2018-08-24 08:59:03
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/pull/276,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/276,try to search floatingip first before creating it,"**Which issue this PR fixes**: fixes #245

**Special notes for your reviewer**:

Tested with following yaml:
```
kind: Service
apiVersion: v1
metadata:
  name: demoapp-lb1
spec:
  loadBalancerIP: ""10.222.134.126""
  type: LoadBalancer
  selector:
    name: demoapp-test
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5000
---
kind: Service
apiVersion: v1
metadata:
  name: demoapp-lb2
spec:
  type: LoadBalancer
  selector:
    name: demoapp-test
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5000
---
kind: Service
apiVersion: v1
metadata:
  name: demoapp-lb3
spec:
  loadBalancerIP: ""10.222.134.121""
  type: LoadBalancer
  selector:
    name: demoapp-test
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5000
```

Where demoapp-lb1 ip ""10.222.134.126"" did not exist in openstack project before 
demoapp-lb2 dynamic floating provisioning
demoapp-lb3 ip ""10.222.134.121"" existed in openstack project before 

The result:

```
kubectl get svc -o wide
NAME          TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)        AGE       SELECTOR
demoapp-lb1   LoadBalancer   10.254.243.90    <pending>        80:31827/TCP   3m        name=demoapp-test
demoapp-lb2   LoadBalancer   10.254.103.125   10.222.134.106   80:32469/TCP   3m        name=demoapp-test
demoapp-lb3   LoadBalancer   10.254.144.138   10.222.134.121   80:30042/TCP   3m        name=demoapp-test
```

demoapp-lb1 is in pending because I do not have permission to specify floatingips in floatingip create in our openstack.

**Release note**:

```release-note
NONE
```
",closed,True,2018-08-24 08:42:25,2018-08-27 01:59:40
cloud-provider-openstack,hyperbolic2346,https://github.com/kubernetes/cloud-provider-openstack/pull/277,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/277,First pass at adding templates for keystone auth,"**What this PR does / why we need it**:
Adding templates from the documentation(with some rbac changes). This makes it easier for people to deploy by cloning the repo.
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
https://github.com/kubernetes/cloud-provider-openstack/issues/271
**Special notes for your reviewer**:
RBAC changes to create a new service account specifically for keystone auth. Not sure where the tests are or if you want to use these templates in those tests.
**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Added keystone auth templates to examples/webhook.
```
",closed,True,2018-08-24 21:52:41,2018-10-04 13:52:49
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/pull/278,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/278,implement InstanceShutdownByProviderID to openstack,"k/k https://github.com/kubernetes/kubernetes/pull/67982
",closed,True,2018-08-29 17:30:27,2018-08-30 01:51:23
cloud-provider-openstack,AdamDang,https://github.com/kubernetes/cloud-provider-openstack/pull/279,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/279,Correct the typos in error messages,"Fix a typo in message,
Line 371&394:  alloted->allotted
",closed,True,2018-08-30 07:15:21,2018-08-31 09:25:14
cloud-provider-openstack,stieler-it,https://github.com/kubernetes/cloud-provider-openstack/issues/280,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/280,Kubernetes nodes lose InternalIP and ExternalIP temporarily,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**: InternalIP and ExternalIP are lost for some nodes (in our case: 2 of 5 are broken). I am not sure if this is related to the cloud provider.

```
>kubectl describe node intern-master1
...
Addresses:
  Hostname:  intern-master1

>kubectl describe node intern-worker1
...
Addresses:
  InternalIP:  10.0.0.15
  ExternalIP:  (hidden)
  Hostname:    intern-worker1
```

This leads to problems, e.g. when trying to create an L4 load balancer:
`Error creating load balancer (will retry): failed to ensure load balancer for service gitlab/gitlab-nginx-ingress-controller: error getting address for node intern-master1: no address found for host`

**What you expected to happen**: I expect both InternalIP and ExternalIP not to change if infrastructure doesn't change

**How to reproduce it (as minimally and precisely as possible)**: I can't tell it just ""happened"". Last week it worked, today it doesn't.

**Anything else we need to know?**: I saw a similar bug report here https://github.com/Azure/acs-engine/issues/3503

**Environment**:
- openstack-cloud-controller-manager version: N/A
- OS (e.g. from /etc/os-release): RancherOS v1.4.0
- Kernel (e.g. `uname -a`): Linux intern-worker1 4.14.32-rancher2 #1 SMP Fri May 11 11:30:31 UTC 2018 x86_64 GNU/Linux
- Install tools: Rancher 2.0.x with RKE
- Others: Kubernetes: Client v1.11.2, Server v1.11.1",closed,False,2018-09-01 10:23:33,2019-02-08 15:50:43
cloud-provider-openstack,praseodym,https://github.com/kubernetes/cloud-provider-openstack/issues/281,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/281,Cinder provisioner doesn't work on VMware Integrated OpenStack (VIO),"/kind bug

The external Cinder provisioner doesn't work on VMware Integrated OpenStack (VIO), whereas the Kubernetes in-tree OpenStack cloud provider [handles this platform just fine](https://github.com/kubernetes/kubernetes/blob/3a8a7114fa22bd687302d31548bb393e2de9f0cd/pkg/cloudprovider/providers/openstack/openstack_volumes.go#L475-L501). The external cloud provider logs [""Unsupported persistent volume type: VMDK""](https://github.com/kubernetes/cloud-provider-openstack/blob/5bbf82c0054af6ad87b061fe9355bfe5a1b71f9a/pkg/volume/cinder/provisioner/mapper.go#L45) and never manages to mount the volume.

**Environment**:
- openstack-cloud-controller-manager version: 0.2.0
- Platform: VMware Integrated OpenStack (VIO) / ESXi
- OS (e.g. from /etc/os-release): Ubuntu 18.04.1
- Install tools: kubeadm
",open,False,2018-09-02 21:10:54,2019-03-16 13:42:07
cloud-provider-openstack,zioproto,https://github.com/kubernetes/cloud-provider-openstack/pull/282,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/282,Get a token using Keystone Application Credential,"**What this PR does / why we need it**:

This PR makes possibile to login using Keystone Application Credentials.
fixes #232 

**Special notes for your reviewer**:

This PR requires this gophercloud PR to be merged first 
https://github.com/gophercloud/gophercloud/pull/1224

",closed,True,2018-09-06 13:07:25,2018-10-08 14:55:35
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/283,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/283,Update to latest 1.12 release dependencies,"- sync latest changes to cinder-csi
- pick up changes from: https://github.com/kubernetes-csi/drivers/tree/master/pkg/cinder
- pick up changes from main k/k repo
- update docker/docker and docker/distribution to newer versions",closed,True,2018-09-09 21:52:17,2018-09-27 17:59:45
cloud-provider-openstack,zioproto,https://github.com/kubernetes/cloud-provider-openstack/pull/284,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/284,OWNERS: remove zioproto (myself),"Removing my self from the OWNERS group.
I am moving to a new job where I will not work with Openstack, so I will not have time and resources to help with this project.

thanks and cheers !
",closed,True,2018-09-10 06:00:26,2018-09-21 01:26:35
cloud-provider-openstack,zioproto,https://github.com/kubernetes/cloud-provider-openstack/issues/285,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/285,client-keystone-auth: using a wrong password will return a wrong error message,"This is a bug report.
/kind bug

**What happened**:


Using `./client-keystone-auth` with correct credential will return the following JSON:
```
{
	""apiVersion"": ""client.authentication.k8s.io/v1alpha1"",
	""kind"": ""ExecCredential"",
	""status"": {
		""token"": ""secret"",
		""expirationTimestamp"": ""2018-09-13T09:48:51Z""
	}
}
```

However if there is a typo in the password the following json is returned:
```
{
	""apiVersion"": ""client.authentication.k8s.io/v1alpha1"",
	""kind"": ""ExecCredential"",
	""spec"": {
		""response"": {
			""code"": 401,
			""header"": {},
		},
	}
}
```
When this is returned to `kubectl` the user see this error message:
```
$ kubectl get pods
No resources found.
Unable to connect to the server: getting credentials: decoding stdout: couldn't get version/kind; json parse error: invalid character '}' looking for beginning of object key string
```

**What you expected to happen**:

The user should a meaningfull error, like `Authorization failed`

**How to reproduce it (as minimally and precisely as possible)**:

Try to authenticate with a wrong password of your keystone user.

",closed,False,2018-09-13 08:57:52,2019-03-04 22:08:03
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/286,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/286,[WIP] Barbican as Key Manager Service for Kubernetes,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR adds Feature to use Barbican as Key manager service for kubernetes

**Which issue this PR fixes**: 
fixes #44 

**Special notes for your reviewer**:
This Plugin cannot be used in production until[1] in barbican gets implemented.
As per Kubernetes documents[2], we need to return encrypted dek to kubernetes, It stores encrypted dek in etcd, only kek is stored in Key Manager Service. OpenStack Barbican currently does not support to return encrypted secret, It only returns secret id. This Plugin returns the secret id to Kubernetes rather than encrypted secret. It results in problem of storing encrypted data at two different locations and since there is no notification from kuberenetes about secret deletion, user has to manually delete secrets stored in barbican. Once feature[1] gets implemented barbican needs to store only single kek and encrypted secret will be stored in kubernetes etcd only.
[1] https://review.openstack.org/#/c/598389/
[2] https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
`NONE`.

",closed,True,2018-09-14 08:57:33,2018-10-08 14:56:09
cloud-provider-openstack,xgerman,https://github.com/kubernetes/cloud-provider-openstack/issues/287,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/287,TLS support for Octavia ingress controller,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
Feature Request
> Uncomment only one, leave it on its own line: 
>
> /kind bug
/kind feature


**What happened**:
OpenStack Octavia ingress controller should support TLS now as there is Barbican support in gopher cloud/openstack cloud provider.

**What you expected to happen**:
Annotations for TLS support need to be implemented.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2018-09-17 21:36:21,2019-03-24 15:50:16
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/288,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/288,Add debug output for cfg,"spend a couple of time in finding why the error:
Cloud provider could not be initialized: could not init
cloud provider ""openstack"": Authentication failed

turn out if we can log those with -v flag it will be helpful

",closed,True,2018-09-19 05:11:52,2018-09-19 05:38:42
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/289,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/289,Add debug output for cfg options,"spend a couple of time in finding why the error:
Cloud provider could not be initialized: could not init
cloud provider ""openstack"": Authentication failed

turn out if we can log those with -v flag it will be helpful

",closed,True,2018-09-19 05:39:20,2019-02-22 09:00:16
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/290,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/290,Enhance comment about document,"Per openstack update, some links should be updated as well for metadata services
",closed,True,2018-09-20 07:32:42,2018-09-27 13:44:49
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/291,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/291,Avoid ERROR: logging before flag.Parse in UT,"Try to avoid those error in UT output:

ok      k8s.io/cloud-provider-openstack/pkg/cloudprovider/providers/openstack   (cached)
ERROR: logging before flag.Parse: I0920 15:25:14.382972   31350 driver.go:49] Driver: csi-cinderplugin version: 0.3.0
ERROR: logging before flag.Parse: I0920 15:25:14.383122   31350 driver.go:80] Enabling controller service capability: CREATE_DELETE_VOLUME
ERROR: logging before flag.Parse: I0920 15:25:14.383135   31350 driver.go:80] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
ERROR: logging before flag.Parse: I0920 15:25:14.383147   31350 driver.go:92] Enabling volume access mode: SINGLE_NODE_WRITER
ERROR: logging before flag.Parse: I0920 15:25:14.383161   31350 driver.go:49] Driver: csi-cinderplugin version: 0.3.0
ERROR: logging before flag.Parse: I0920 15:25:14.383170   31350 driver.go:80] Enabling controller service capability: CREATE_DELETE_VOLUME
ERROR: logging before flag.Parse: I0920 15:25:14.383192   31350 driver.go:80] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
ERROR: logging before flag.Parse: I0920 15:25:14.383202   31350 driver.go:92] Enabling volume access mode: SINGLE_NODE_WRITER

",closed,True,2018-09-20 07:33:34,2018-09-21 13:04:20
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/issues/292,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/292,ask for domain name when using openrc from openstack devstack,"/kind bug


root@kvm-016922:~# env | grep DOMAIN
OS_USER_DOMAIN_NAME=Default
OS_PROJECT_DOMAIN_NAME=Default

root@kvm-016922:~# ./client-keystone-auth
Please enter domain name: Default

from https://docs.openstack.org/releasenotes/python-mistralclient/queens.html
we knew we need set OS_USER or OS_PROJECT
",closed,False,2018-09-25 07:31:38,2019-02-22 09:01:30
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/293,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/293,fix nits in client-keystone-auth,fix some nits,closed,True,2018-09-25 08:36:36,2018-10-01 20:02:47
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/294,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/294,Add/Update more CI jobs,"- Fixup 1.11 as we have a release-1.11 branch
- Add new job for 1.12

Change-Id: Ida552559ad4dc130586654d666bdf347d8221720

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-09-25 15:28:54,2018-09-25 16:15:39
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/295,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/295,Fix some document related to csc,"csc document (cinder plugin) is not up-to-date, this
patch tries to fix some of them, follow up patch will do
more update.
",closed,True,2018-09-26 07:24:53,2018-09-27 10:24:25
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/296,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/296,Add doc to indicate the roles/bindings need to be created,"The doc is not enough for the cloud-controller-manager to startup


",closed,True,2018-09-27 07:34:55,2018-09-27 10:33:39
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/297,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/297,Cinder CSI CI Tests are failing ,"

**What happened**:
Cinder CSI CI Tests are failing with the error ""attacher.MountDevice failed: driver name csi-cinderplugin not found in the list of registered CSI drivers"".
It is happening after PR https://github.com/kubernetes/kubernetes/pull/68200/, which defualts enable
KubeletPluginsWatcher, we need to update yaml files for driver-registrar sidecar container as described here https://kubernetes-csi.github.io/docs/print.html#kubelet-plugin-watcher

Also After the PR https://github.com/kubernetes/kubernetes/pull/67110, Adding NodeGetInfo call in node server is required

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-09-27 09:18:21,2018-09-27 10:58:14
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/298,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/298,Fix csi test,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR Fixes CSI CI Tests, It updates driver registrar configurations and add NodeGetInfo, NodeGetCapabilties rpc calls

**Which issue this PR fixes** : fixes #297 

**Special notes for your reviewer**:

**Release note**:
 `NONE`.

",closed,True,2018-09-27 09:35:47,2018-11-27 08:37:56
cloud-provider-openstack,mooncak,https://github.com/kubernetes/cloud-provider-openstack/pull/299,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/299,Fix typos issue in attacher.go,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Fix typos issue in attacher.go 

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-09-27 16:46:22,2018-10-08 15:06:01
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/300,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/300,Sync from Kubernetes master,"
- https://github.com/kubernetes/kubernetes/commit/093e231289f8cfabe7cb18500b73c6b12606da80
- https://github.com/kubernetes/kubernetes/commit/8f7fcd5adc9c8fd8dfd6c932b76076c870af5487",closed,True,2018-09-27 19:58:47,2018-09-28 14:11:49
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/301,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/301,Add Features in Cinder CSI Driver ,"<!-- This form is for bug reports and feature requests! -->

/kind feature


**What happened**:
With CSI v0.3.0 New Plugin Capabilties are added including Snapshot and Topology API, This features needs to be included in Cinder CSI driver.

Following CSI Interface are not implemented in cinder csi driver
 **Identity Service** 
 **Controller Service** 
    - ListVolumes, GetCapacity, ValidateVolumeCapabilities , ControllerGetCapabilities, CreateSnapshot, 
       DeleteSnapshot, ListSnapshots 
 **Node Service** 
    - NodeStageVolume , NodeUnstageVolume , NodeGetVolumeStats , NodeGetCapabilities

We need to check which interface we can implement, if not possible to implement we can atleast add them with default implementation.

https://github.com/container-storage-interface/spec/blob/master/spec.md

",closed,False,2018-09-28 06:03:33,2019-03-06 06:12:28
cloud-provider-openstack,vasartori,https://github.com/kubernetes/cloud-provider-openstack/issues/302,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/302,Create more functionals healthmonitors,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
FEATURE REQUEST

/kind feature

**What happened**:
ExternalLoadBalancers when created with option healthCheckNodePort only do a simple tcpcheck, this is not good on environments with a high traffic.

**What you expected to happen**:
Using annotations, we can ""copy"" the healthcheck of endpoints to a healthmonitor.

",closed,False,2018-09-28 13:58:22,2019-02-25 15:17:40
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/303,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/303,Add detail info about controller public/unpublish,"Add detail info about controller publish/unpublish

This doc does not have enough info.
Adding more description for someone who is familiar with
openstack but not with cloud provider.",closed,True,2018-09-29 04:01:40,2018-10-01 21:07:17
cloud-provider-openstack,ranasheel2000,https://github.com/kubernetes/cloud-provider-openstack/pull/304,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/304,Adds url to log new bug in contributing.md,"This commit adds URL for logging new issues related to
cloud-provider-openstack.

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
This PR adds URL for logging new issue in this repository.
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
None.
**Special notes for your reviewer**:
None.
**Release note**:
None
",closed,True,2018-09-29 18:49:31,2018-09-30 14:10:07
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/305,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/305,Set e2e to master until cpo 1.12 branch/tag is cut,"Signed-off-by: Melvin Hillsman <mrhillsman@gmail.com>

**What this PR does / why we need it**:
Update periodic job runs to use master branch since there is not a 1.12 branch/tag yet. Currently the 1.12 conformance does not run due to branch 1.11 being used.

@dims 
",closed,True,2018-10-01 04:04:27,2018-10-01 12:34:44
cloud-provider-openstack,xlucas,https://github.com/kubernetes/cloud-provider-openstack/issues/306,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/306,Wrong entrypoint in cinder-csi-plugin image,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug


**What happened**:

When trying to run cinder-csi-plugin from manifests, the following pods are in the `RunContainerError` state:
- csi-attacher-cinderplugin
- csi-nodeplugin-cinderplugin
- csi-provisioner-cinderplugin
- csi-secret-cinderplugin


Logs show the error is the same for all pods and pertains to the `cinder` container running the `cinder-csi-plugin` image:
```
container_linux.go:247: starting container process caused ""exec: \""--nodeid=<hostname>\"": executable file not found in $PATH""
```

**What you expected to happen**:

Pods should be in the `Running` state.

**How to reproduce it (as minimally and precisely as possible)**:

1. Replace the base64 version of cloud-config in `manifests/cinder-csi-plugin/csi-secret-cinderplugin.yaml` with your own.

2.  Run `kubectl -f apply manifests/cinder-csi-plugin`

**Anything else we need to know?**: It sounds like the image `docker.io/k8scloudprovider/cinder-csi-plugin:latest` has not been built like the others and the entrypoint is `/bin/cinder-csi-plugin` instead of `/cinder-csi-plugin` so it fails.

Changing the container args in each manifest from:
```yaml
- name: cinder
  image: docker.io/k8scloudprovider/cinder-csi-plugin:latest
  args :
    - --nodeid=$(NODE_ID)
    - --endpoint=$(CSI_ENDPOINT)
    - --cloud-config=$(CLOUD_CONFIG)
```

To:
```yaml
- name: cinder
  image: docker.io/k8scloudprovider/cinder-csi-plugin:latest
  args :
    - /bin/cinder-csi-plugin
    - --nodeid=$(NODE_ID)
    - --endpoint=$(CSI_ENDPOINT)
    - --cloud-config=$(CLOUD_CONFIG)
```

Is a workaround to this issue.

**Environment**:
- openstack-cloud-controller-manager version: 0.2.0
- OS (e.g. from /etc/os-release): Debian 9
- Kernel (e.g. `uname -a`): `Linux 4.9.0-7-amd64 #1 SMP Debian 4.9.110-3+deb9u1 (2018-08-03) x86_64 GNU/Linux`
",closed,False,2018-10-01 11:57:47,2018-10-02 10:27:50
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/307,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/307,Fix up path to executable for cinder-csi-plugin,"Change-Id: I4d7f4781bdcb4b6427de1363c680d2fb21de2753

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

Let us be explicit about the path of the binary for the cinder-csi-plugin

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: 
fixes #306

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-10-01 15:06:36,2018-10-02 10:27:50
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/308,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/308,Batch update members in octavia-ingress-controller,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Use batch operation API to reduce API calls to Octavia

**Special notes for your reviewer**:
<https://developer.openstack.org/api-ref/load-balancer/v2/index.html#batch-update-members>

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-10-02 15:18:26,2018-10-06 03:14:43
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/309,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/309,octavia-ingress-controller: Support backend service port name,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
The current implementation only supports service port number.

**What you expected to happen**:
Support service port number and port name as well.
",closed,False,2018-10-02 16:30:46,2018-10-08 15:06:10
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/310,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/310,[Ingress] Support to specify backend service port name,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Support to specify backend service port name

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*:
fixes #309

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-10-02 16:34:16,2018-10-10 13:54:18
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/311,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/311,octavia-ingress-controller: Octavia resources get recreated when the controller restarts,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
Octavia resources get recreated when the controller restarts

**What you expected to happen**:
Don't re-create the resources if the ingress doesn't change

**How to reproduce it (as minimally and precisely as possible)**:
1. the octavia-ingress-controller is up and running
2. create an ingress in k8s cluster
3. restart the octavia-ingress-controller service
4. the existing ingress is handled during the restart, some octavia resources get deleted and recreated.

",closed,False,2018-10-02 16:38:45,2018-10-14 13:11:27
cloud-provider-openstack,xlucas,https://github.com/kubernetes/cloud-provider-openstack/issues/312,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/312,Failed to find suitable endpoint from service catalog,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug


**What happened**: When I try to run the nginx example for the cinder CSI plugin, volume provisionning fails with the given error in the csi-provisionner container of the csi-provisioner-cinderplugin statefulset:

```
I1002 16:50:19.335338       1 controller.go:1167] scheduleOperation[provision-default/csi-pvc-cinderplugin[3ecb435b-c663-11e8-bace-fa163e19759e]]
E1002 16:50:19.692558       1 controller.go:895] Failed to provision volume for claim ""default/csi-pvc-cinderplugin"" with StorageClass ""csi-sc-cinderplugin"": rpc error: code = Unknown desc = No suitable endpoint could be found in the se
rvice catalog.
E1002 16:50:19.693439       1 goroutinemap.go:150] Operation for ""provision-default/csi-pvc-cinderplugin[3ecb435b-c663-11e8-bace-fa163e19759e]"" failed. No retries permitted until 2018-10-02 16:50:20.193333785 +0000 UTC m=+77.968082186 (
durationBeforeRetry 500ms). Error: ""rpc error: code = Unknown desc = No suitable endpoint could be found in the service catalog.""
I1002 16:50:21.355575       1 leaderelection.go:198] stopped trying to renew lease to provision for pvc default/csi-pvc-cinderplugin, task failed
```

The nginx pod being stuck in this state:

```
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  0s (x11 over 12s)  default-scheduler  pod has unbound PersistentVolumeClaims (repeated 2 times)
```

And the associated persistent volume claim stays in the pending state:

```
NAME                   STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS          AGE
csi-pvc-cinderplugin   Pending                                       csi-sc-cinderplugin   22m
```

**What you expected to happen**:

The volume should be properly provisionned.

**How to reproduce it (as minimally and precisely as possible)**:

1. Create `cloud-config` with
```ini
[Global]
region=<region>
username=<username>
password=<password>
auth-url=<auth_v3_url>
domain-name=<domain-name>

[BlockStorage]
bs-version=v2
```
2. `kubectl --namespace=kube-system create configmap cloud-config --from-file=cloud-config`
3. `kubectl apply -f manifests/controller-manager/openstack-cloud-controller-manager-ds.yaml`
4. `base64 -w0 < cloud-config`
5. Replace the result of the above command in `manifests/cinder-csi-plugin/csi-secret-cinderplugin.yaml`, `data.cloud.conf` field.
6. `kubectl apply -f manifests/cinder-csi-plugin`
7. `kubectl apply -f examples/cinder-csi-plugin`

**Anything else we need to know?**: 
- I've tried multiple combinations within the `[Global]` section of the cloud-config to no avail (v2/v3, with or without the tenant-name/tenant-id, domain-name/domain-id...)
- I've made sure nodes can join the provider's openstack services and can successfully list catalog services by running `python-openstackclient` on the underlying VMs using the same exact information than `cloud-config`.
- I've verified both secrets and configmaps volumes are properly mounted with the right content by running `kubectl exec <cloud-controller-manager-ds> cat /etc/cloud/cloud-config` and `kubectl exec <cinder-csi-provisionner-pod> cinder cat /etc/config/cloud.conf`. 

**Environment**:
- kubernetes: 1.11.3
- openstack-cloud-controller-manager version: 0.2.0
- OS (e.g. from /etc/os-release): Debian 9
- Kernel (e.g. `uname -a`): `Linux master-0 4.9.0-7-amd64 #1 SMP Debian 4.9.110-3+deb9u1 (2018-08-03) x86_64 GNU/Linux`
",open,False,2018-10-02 18:18:37,2019-03-19 08:20:26
cloud-provider-openstack,ranasheel2000,https://github.com/kubernetes/cloud-provider-openstack/pull/313,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/313,Uniform command prompt,"This commit adds uniform command prompt.

**What this PR does / why we need it**:
This PR updates prompt as $ for uniformity with a little bit of explanation.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
None.

**Special notes for your reviewer**:
None.
**Release note**:
None.
",closed,True,2018-10-03 05:57:59,2018-10-08 15:14:20
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/314,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/314,Update to Kubernetes 1.12.0 release,"Change-Id: I9bb8a1bcb47a08bf76fa19e3a83ae38e30a6dbe1

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-10-05 01:40:21,2018-10-05 19:17:16
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/315,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/315,Set the 1.10 branch to use 1.11 branch for 1.11 periodic job versus 1.10,"**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
Signed-off-by: Melvin Hillsman <mrhillsman@gmail.com>",closed,True,2018-10-07 00:02:55,2018-10-07 01:31:20
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/316,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/316,Fix up formatting for go 1.11.1,build harness etc have moved to newer go version. We need to switch as well,closed,True,2018-10-08 12:36:06,2018-10-08 13:25:04
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/317,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/317,Export some methods in anticipation of a standalone cinder provider,"We seem to have a longer runway for removing the cinder in-tree volume
provider. These changes will help us experiment with vendoring code from
cloud-provider-openstack back into kubernetes/kubernetes.

Change-Id: Ie49fe43ebc8b68295bbeb3a192a948dfbe6cd975

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-10-09 12:05:41,2018-10-09 18:00:50
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/318,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/318,Automated cherry pick of #317: Export some methods in anticipation of a standalone cinder,"Cherry pick of #317 on release-1.12.

#317: Export some methods in anticipation of a standalone cinder",closed,True,2018-10-09 12:58:25,2018-10-09 18:00:46
cloud-provider-openstack,niuzhenguo,https://github.com/kubernetes/cloud-provider-openstack/pull/319,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/319,Add Manila Provisioner binary to Makefile clean,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Adds manila-provisioner to `make clean`
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-10-10 03:00:21,2018-10-10 09:41:29
cloud-provider-openstack,mirake,https://github.com/kubernetes/cloud-provider-openstack/pull/320,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/320,Typo fix: exisiting -> existing,"Signed-off-by: Rui Cao <ruicao@alauda.io>
",closed,True,2018-10-10 12:29:57,2018-10-10 15:22:31
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/321,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/321,Add volume list functions,"previously volume list will show something like:
not implemented

now it will have

$ csc controller list-volumes  --endpoint tcp://127.0.0.1:10000
""05477623-0355-4229-80d0-dd695f40d748""  2147483648
""c0a0c1da-c4aa-49e0-8900-f67dbfb01dfd""  1073741824




",closed,True,2018-10-11 04:30:19,2018-10-25 02:58:55
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/322,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/322,[Ingress] Do not recreate octavia resources when the controller restarts,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
If the ingress resource doesn't change, nothing needs to do when the
ingress controller restarts. In order to do that we need to store
the ingress resource version information in the load balancer's
description field.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #311

**Special notes for your reviewer**:
With this PR, when the ingress controller restarts, for existing ingress that not changed, you can see the log below:
```
INFO[0000] ingress created or updated, will create or update octavia resources  ingress=default/test-octavia-ingress
DEBU[0000] loadbalancer exists                           name=k8s-default-test-octavia-ingress-lb
INFO[0000] loadbalancer active                           id=d15108bd-1f4a-49d5-8e8a-5c5fb80720d9 name=k8s-default-test-octavia-ingress-lb
INFO[0000] ingress not change                            ingress=test-octavia-ingress
```

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
The octavia ingress controller stores the ingress ResourceVersion in the description field of the load balancer in Octavia in order to avoid any change unnecessarily.
```
",closed,True,2018-10-11 13:25:50,2018-10-15 01:08:11
cloud-provider-openstack,flaper87,https://github.com/kubernetes/cloud-provider-openstack/issues/323,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/323,Use standard clouds.yaml file as config file,"/kind feature

We're currently migrating the `cluster-api-provider-openstack` config format from a custom implementation to using `clouds.yaml`, which is a known format in the OpenStack community.

https://github.com/kubernetes-sigs/cluster-api-provider-openstack/issues/16

Just like in the `cluster-api-provider-openstack` case, it'd be great if we could port the `cloud-provider-openstack` config format to use `clouds.yaml`. This would not only make both projects consistent but it'd also bring in a known format in the openstack community making the deployment of this provider easier to operators.

We could port the implementation that we're working on into `cloud-provider-openstack` and then ventor it in `cluster-api-provider-openstack` making the former a dependency for the latter.",closed,False,2018-10-11 15:17:57,2019-01-08 08:20:06
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/324,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/324,octavia-ingress-controller: Can not differentiates octavia resources for ingresses of different k8s clusters,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
If there are more than one k8s clusters for the same tenant in the cloud, it will be problematic for the ingresses with the same name, because only one load balancer is created.

**What you expected to happen**:
There should be different load balancers for the same name ingresses in different clusters.

**How to reproduce it (as minimally and precisely as possible)**:
1. Create two k8s clusters for the same tenant in Magnum.
2. Make sure the octavia-ingress-controller is up and running in both clusters.
3. Create an ingress with the same name in each cluster
4. In OpenStack, you can see there is only one load balancer created.
",closed,False,2018-10-12 02:47:47,2018-10-15 03:20:59
cloud-provider-openstack,pblaas,https://github.com/kubernetes/cloud-provider-openstack/issues/325,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/325,csi-nodeplugin-cinderplugin can't find instance-id,"> /kind bug
/kind feature


**What happened**:
While deploying `https://github.com/kubernetes/cloud-provider-openstack/tree/master/manifests/cinder-csi-plugin`

csi-nodeplugin-cinderplugin keeps crashing due to not able to find specific content.
```
I1012 11:04:30.120752       1 connection.go:140] GRPC error: rpc error: code = Unknown desc = open /var/lib/cloud/data/instance-id: no such file or directory
E1012 11:04:30.120773       1 main.go:159] rpc error: code = Unknown desc = open /var/lib/cloud/data/instance-id: no such file or directory
```

**What you expected to happen**:
/var/lib/cloud/data/instance-id should be filled with proper information from the OpenStack metadata service.

**How to reproduce it (as minimally and precisely as possible)**:
Deploy the cinder-csi-plugin on kubernetes on an OpenStack Ocata platform.

**Anything else we need to know?**:
There was a short discussion on the slack channel:
`https://kubernetes.slack.com/archives/C0LSA3T7C/p1539342540000100`

**Environment**:
- openstack-cloud-controller-manager version: Latest
- OS (e.g. from /etc/os-release): 
```
NAME=""Container Linux by CoreOS""
ID=coreos
VERSION=1745.7.0
VERSION_ID=1745.7.0
BUILD_ID=2018-06-14-0909
```
- Kernel (e.g. `uname -a`): `Linux k8s-k8scluster-node10 4.14.48-coreos-r2 #1 SMP Thu Jun 14 08:23:03 UTC 2018 x86_64 Intel Core Processor (Broadwell) GenuineIntel GNU/Linux`

- Install tools: `https://github.com/pblaas/nagoya`
- Others:
",closed,False,2018-10-12 13:42:07,2018-10-24 06:00:16
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/326,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/326,New url for golint,"Change-Id: I33942bf37e03889c52a0215f39d1a4b5e264a7ff

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-10-14 00:08:49,2018-10-14 13:00:47
cloud-provider-openstack,pblaas,https://github.com/kubernetes/cloud-provider-openstack/issues/327,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/327,Missing RBAC rules on the openstack-cloud-controller-manager daemonset manifest ,"/kind feature


**What happened**:
Deploying the daemonset of the openstack-cloud-controller-manager `https://github.com/kubernetes/cloud-provider-openstack/tree/master/manifests/controller-manager`  on a cluster with RBAC enabled won't render a usable k8s cluster due to the fact the taints are not removed from the nodes by the cloud controller manager due to missing RBAC permissions.

**What you expected to happen**:
The daemon set manifest  should be completed with required RBAC rules.

**How to reproduce it (as minimally and precisely as possible)**:
Deploy the daemonset manifest `https://github.com/kubernetes/cloud-provider-openstack/blob/master/manifests/controller-manager/openstack-cloud-controller-manager-ds.yaml` on a kubernetes cluster with RBAC enabled.

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: latest
",closed,False,2018-10-14 10:00:11,2018-10-14 15:58:22
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/328,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/328,[Ingress] Add cluster name in the config,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Adding cluster name in the octavia-ingress-conroller config file.

The cluster name is used for differentiate octavia resources created
for different k8s clusters of different tenants.

**Which issue this PR fixes**:
fixes #324

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-10-15 01:15:57,2018-10-15 03:40:06
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/329,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/329,octavia-ingress-controller: Node sync improvement,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Use batch members update operation to save the api calls to Octavia.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: 

**Special notes for your reviewer**:
This PR doesn't fix any bug, just improve the node sync operation and some trivial fixes for the log.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-10-15 03:43:56,2018-10-15 20:25:42
cloud-provider-openstack,hogepodge,https://github.com/kubernetes/cloud-provider-openstack/pull/330,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/330,Update Pull Request Template,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Updating the Pull Request Template


**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```NONE
```
",closed,True,2018-10-15 16:15:02,2018-10-26 13:31:16
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/331,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/331,Get NodeID from Metadata Service as fallback,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Currently cinder csi plugin gets nodeID from cloud-init mounted data, but the plugin fails if the instance image does not have cloud-init or data is at some other location, This commit adds to get the data from metadata service as a fallback option.

**Which issue this PR fixes** : fixes #325 

**Special notes for your reviewer**:
As of now separate code is written in cinder-csi-plugin to get metadata from openstack metadata service, In future we should import some common functionality from provider/openstack 

**Release note**:
`NONE`
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-10-16 08:06:32,2018-11-27 08:37:50
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/332,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/332,Update to k/k 1.13.0-alpha.1 version,"Change-Id: Ie9c510c6901d2259aa9fff539403a13136ec3f62

<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Update to kubernetes 1.13.0-alpha.1 version
```
",closed,True,2018-10-16 16:33:58,2018-10-16 19:02:32
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/333,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/333,[Ingress] Use TCP protocol for the lb,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
Use TCP protocol when creating the listener and pool.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
NONE
",closed,True,2018-10-17 02:13:53,2018-10-17 10:52:26
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/334,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/334,octavia-ingress-controller: The ingress event information is missing,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**: 
/kind feature

**What happened**: 
Can not see the ingress event using `kubectl get ing <name>` 

**What you expected to happen**: 
I'd like to see the following information:
```
Events:
  Type    Reason  Age   From                      Message
  ----    ------  ----  ----                      -------
  Normal  CREATE  5m    nginx-ingress-controller  Ingress default/test-nginx-ingress-svc-notfound
  Normal  UPDATE  5m    nginx-ingress-controller  Ingress default/test-nginx-ingress-svc-notfound
```

**How to reproduce it (as minimally and precisely as possible)**:
1. Create an ingress after octavia-ingress-controller is deployed.
2. `kubectl get ing <name>`
",closed,False,2018-10-17 02:44:33,2018-10-17 14:18:07
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/335,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/335,[Ingress] Add event support,"**What this PR does / why we need it**:
Add event support for octavia-ingress-controller.

**Which issue this PR fixes**:
fixes #334

**Special notes for your reviewer**:
Following are the events happened during an ingress creation:
```
Events:
  Type    Reason    Age   From                          Message
  ----    ------    ----  ----                          -------
  Normal  Creating  1m    openstack-ingress-controller  Ingress default/test-octavia-ingress-full
  Normal  Updated   4s    openstack-ingress-controller  Successfully associated IP address 172.24.4.14 to ingress default/test-octavia-ingress-full
  Normal  Created   3s    openstack-ingress-controller  Ingress default/test-octavia-ingress-full
```

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Support ingress events in octavia-ingress-controller.
```
",closed,True,2018-10-17 12:13:31,2018-10-17 22:45:30
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/336,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/336,Add capabilities,"**What this PR does / why we need it**:
This PR will add more doc about the controller capabilities ,user should know the 
capability of the csi controller ,e.g what's it can do 

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
N/A
**Special notes for your reviewer**:
N/A
**Release note**:
no release notes needed because it's doc change
",closed,True,2018-10-24 08:30:39,2018-11-15 03:37:08
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/337,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/337,Add create snapshot csi,"<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:
this is a new feature for csi's create snapshot function
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
relate to #301 
**Special notes for your reviewer**:

**Release note**:
```
now the CSI cinder support create snapshot function, you can use 
csc controller create-snapshot --endpoint xxx --source-volume yyy zzz
to create a new snapshot in openstack cloud through new csi interface
```
",closed,True,2018-10-26 07:24:23,2018-10-31 08:26:53
cloud-provider-openstack,hogepodge,https://github.com/kubernetes/cloud-provider-openstack/pull/338,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/338,Add adisky as reviewer to OWNERS file,"**What this PR does / why we need it**:
Adds adisky as reviewer to OWNERS file.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```NONE
```
",closed,True,2018-10-27 01:19:36,2018-10-29 20:00:26
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/339,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/339,Run e2e Conformance tests on v1.12,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->
**What this PR does / why we need it**:
Update zuul.yaml to run e2e Conformance tests on v1.12

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
None.

**Special notes for your reviewer**:

**Release note**:
`NONE`.
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
````release-note
````

",closed,True,2018-10-31 05:21:22,2018-11-27 08:37:45
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/340,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/340,Update CSI Manifests,"
<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
csi provisioner, attacher and driver-registrar v0.3.0 image last updated 3 months back, we should run csi CI against latest attacher, provisioner, driver-registrar.
In the CI scripts, we are building against latest but not testing with the latest build.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
`NONE`.
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-10-31 06:23:00,2018-11-27 08:37:43
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/341,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/341,Cinder CSI Raw block volume support,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What you expected to happen**:
CSI Raw block volume support has been added in v1.11 as alpha feature, we should add same support to cinder csi driver
",open,False,2018-10-31 07:35:07,2019-01-09 05:21:40
cloud-provider-openstack,mvladev,https://github.com/kubernetes/cloud-provider-openstack/pull/342,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/342,fix GetLabelsForVolume to handle only Cinder volumes,"**What this PR does / why we need it**: 
When the `openstack-cloud-controller-manager` is running with PV label initializing controller
and NFS volume is created, it causes nill reference error.

**Which issue this PR fixes** 
`OpenStack` version of kubernetes/kubernetes#68996

**Special notes for your reviewer**:
This is a cherry-pick of kubernetes/kubernetes#70459

**Release note**:
```release-note
NONE
```
",closed,True,2018-10-31 08:09:34,2018-11-11 00:44:31
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/343,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/343,Add snapshot into csi,"**What this PR does / why we need it**:
This PR increase the functions for csi cinder volume including
snapshot create, delete and list
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
no issue 
**Special notes for your reviewer**:

```release-note
Add snapshot into csi cinder volume
./csc controller create-snapshot, delete-snapshot and list-snapshots is supported in this patch
```
",closed,True,2018-10-31 09:15:49,2018-10-31 09:16:00
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/344,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/344,[csi] add snapshot support,"Add snapshot function for csi including:
add snapshot
delete snapshot
list snapshot


**What this PR does / why we need it**:
add snapshot functions for csi cinder volume
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
not a bug fix
**Special notes for your reviewer**:

**Release note**:

```
add snapshot csi functions, create-snapshot, delete-snapshot and list-snapshot are added 
```
",closed,True,2018-10-31 09:40:56,2019-01-07 03:25:32
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/345,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/345,Update CSI Docs,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
CSIPersistentVolume and MountPropagation Feature gates are default
enabled from kubernetes v1.10, updating CSI docs for the same

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
Reference: https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/

**Release note**:
`NONE`.
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-10-31 11:06:28,2018-11-27 08:37:37
cloud-provider-openstack,adsl123gg,https://github.com/kubernetes/cloud-provider-openstack/issues/346,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/346,Docs error in Loadbalancers example README.MD,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug


**What happened**:
use cloud-provider-openstack/examples/loadbalancers/internal-http-nginx.yaml to create **internal** loadbalancer failed, cloud-provider-openstack create a **external** loadbalancer.

**What you expected to happen**:
use cloud-provider-openstack/examples/loadbalancers/internal-http-nginx.yaml to create **INTERNAL** loadbalancer successfully.

**How to reproduce it (as minimally and precisely as possible)**:
when you run k8s and cloud-provider-openstack successful, run ""kubectl create -f examples/loadbalancers/internal-http-nginx.yaml""

**REASON:**
the code in openstack_loadbalancer.go assume LB is external, it read service annotation ""service.beta.kubernetes.io/openstack-internal-load-balancer"" to create internal or external LB. But the internal-http-nginx.yaml and external-http-nginx.yaml use annotations in pos, not service, so the yaml file should be changed to below content.
`---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: internal-http-nginx-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
---
kind: Service
apiVersion: v1
metadata:
  name: internal-http-nginx-service
  annotations:
    service.beta.kubernetes.io/openstack-internal-load-balancer: ""true""
spec:
  selector:
    app: nginx
  type: LoadBalancer
  ports:
  - name: http
    port: 80
    targetPort: 80`

",closed,False,2018-11-03 07:44:36,2018-12-13 13:48:50
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/347,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/347,Ensure cloudprovider/ does not depend on k8s.io/kubernetes,"Change-Id: Ie9db3ac132aa4cfb7351eb513f9df5b4f3623521

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-05 04:33:54,2018-11-06 21:54:41
cloud-provider-openstack,mooncak,https://github.com/kubernetes/cloud-provider-openstack/pull/348,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/348,Fix typos: connot->cannot concurrrent->concurrent,"Signed-off-by: mooncake <xcoder@tenxcloud.com>

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Fix typos: connot->cannot concurrrent->concurrent

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-06 15:30:28,2018-11-10 14:22:28
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/349,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/349,Update to k/k v1.13.0-alpha.3,"Change-Id: Ic00a00dd6bfe02939bbbab0bde4cafdec527ee32

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-06 23:09:33,2018-11-10 18:34:13
cloud-provider-openstack,iamemilio,https://github.com/kubernetes/cloud-provider-openstack/pull/350,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/350,Add support for reading clouds.yaml,"**What this PR does / why we need it**:  enables use of clouds.yaml in place of environment variables

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: Fixes: https://github.com/kubernetes/cloud-provider-openstack/issues/323



**Special notes for your reviewer**: First pr in this repo :1st_place_medal: 

@flaper87 
",closed,True,2018-11-07 20:22:20,2019-01-08 13:51:13
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/351,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/351,Update to v1.13.0-beta.0,"Change-Id: Ic00a00dd6bfe02939bbbab0bde4cafdec527ee32

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Periodic update to the latest k/k milestone

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Update to Kubernetes milestone v1.13-beta.0
```
",closed,True,2018-11-10 18:36:59,2018-11-12 12:52:19
cloud-provider-openstack,adsl123gg,https://github.com/kubernetes/cloud-provider-openstack/issues/352,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/352,Add feature for scale cluster worker node in openstack," /kind feature

Now cloud-provider-openstack can provide Loadbalance/PersistentVolume/router for k8s, but if k8s want to scale up worker nodes, now cloud-provider-openstack can do nothing. As we all know, openstack can provide VM, so why not provide VMs to k8s for cloud-provider-openstack. I think it's a very useful function, and also there are a project called [machine-controller](https://github.com/kubermatic/machine-controller), which has implement the function of asking openstack VMs and let the VMs join k8s cluster, so we can integrate the machine-controller project with cloud-provider-openstack.

main function for this feature:
add/delete/update worker nodes to k8s cluster from openstack
",closed,False,2018-11-11 12:14:02,2019-04-02 09:30:01
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/353,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/353,Remove unnucessary log and adjust some logic,"actually, err is not nil means node is not """",so
no need to double check here to make logic complicate

**What this PR does / why we need it**:
minor refactory
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
minor refactory to remove complex 
**Release note**:

```release-note
NONE
```
",open,True,2018-11-12 12:29:30,2019-02-18 14:19:45
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/354,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/354,Switch from glog to klog,"Change-Id: Ic58df1ddede507686ab1319ee7c960c48b5dfa80

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-11-12 23:50:57,2018-11-30 15:49:00
cloud-provider-openstack,mikeweiwei,https://github.com/kubernetes/cloud-provider-openstack/pull/355,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/355,fix logging calls,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**: 

If don't use formatted output,fix logging calls 

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-13 05:13:45,2018-11-28 22:08:01
cloud-provider-openstack,ricolin,https://github.com/kubernetes/cloud-provider-openstack/pull/356,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/356,Add me (ricolin) as reviewer,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:

```release-note
```
",closed,True,2018-11-13 06:27:14,2018-11-15 03:25:54
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/357,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/357,Add me (jichenjc) into reviewer,"

**What this PR does / why we need it**:
add as reviewer should be ok ? (Just +1/-1 ?)

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```
",closed,True,2018-11-14 05:54:26,2018-11-27 16:43:52
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/358,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/358,Update Cinder CSI Driver Name,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
It updates cinder csi driver name in domain name notation format, as suggested (not enforced )by CSI specs.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
 `NONE`.
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-11-14 08:02:59,2018-11-15 05:08:18
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/359,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/359,[Cinder CSI] Remove Dependency from csi-common package,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:
Cinder CSI Driver heavily depends on https://github.com/kubernetes-csi/drivers/tree/master/pkg/csi-common, we should remove its dependency from csi-common package as it is not suggested for production deployments and not maintained actively.

To Remove its Dependency we need to develop -
- Non Blocking Grpc server, To run controller, node, Identity services
- Identity Service


",closed,False,2018-11-14 08:13:22,2019-01-17 19:37:21
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/360,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/360,Implement a common library for openstack cloud provider,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
/kind feature


**What happened**:
Currently all Drivers(octavia, manila, cinder etc) have there own implemention for openstack related stuffs(Mainly Authentication and Configuration), It would be good, if we have a common library that all drivers can use. It will help to reduce maintenance efforts and code duplicacy.

",open,False,2018-11-14 08:27:55,2019-03-14 09:50:17
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/issues/361,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/361,report size when create volume again with same name,"<!-- This form is for bug reports and feature requests! -->

/kind bug



**What happened**:
root@kvm-017407:~# ~/go/src/github.com/rexray/gocsi/csc/csc controller create-volume --endpoint tcp://127.0.0.1:10000 CSIVolumeName1
""27aea4f0-9a93-4b99-9396-6a26a5bbf6e7""  1073741824      ""availability""=""nova""

root@kvm-017407:~# ~/go/src/github.com/rexray/gocsi/csc/csc controller create-volume --endpoint tcp://127.0.0.1:10000 CSIVolumeName1
""27aea4f0-9a93-4b99-9396-6a26a5bbf6e7""  0       ""availability""=""nova""


**What you expected to happen**:
0 ==>1073741824      
**How to reproduce it (as minimally and precisely as possible)**
every time

**Anything else we need to know?**:

**Environment**:
master branch
",closed,False,2018-11-15 05:38:19,2018-12-07 02:45:36
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/362,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/362,Add size into output,"when create a volume with name already there,
we don't give the size of the volume in output and lack of log.


**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
Fix bug #361 
**Special notes for your reviewer**:

**Release note**:

```release-note
create volume with same name will get the actual size other than 0 now
```
",closed,True,2018-11-15 05:39:48,2018-11-28 11:08:50
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/363,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/363,update format check scripts,"per
https://github.com/kubernetes/kubernetes/blob/843a67b215dcfa1781820a797c4c759e42e81d3a/hack/verify-gofmt.sh#L52-L56

**What this PR does / why we need it**:
sync verify fmt scripts with k8s

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
per commit message
**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```
",closed,True,2018-11-16 05:23:33,2018-11-27 14:31:38
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/364,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/364,Fix CSI CI (Update CSI Sidecar versions),"
<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Cinder CSI is breaking with sidecar containers update to CSIv1.0.0
Using older version until cinder driver updated to 1.0.0

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
`NONE`
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-11-16 08:02:56,2018-11-27 08:47:26
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/365,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/365,Update Cinder CSI driver to CSI v1.0.0,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:
All CSI sidecar containers and kubernetes  are updated to CSI v1.0.0, we should soon upgrade cinder CSI drive to 1.0.0
https://github.com/container-storage-interface/spec/releases/tag/v1.0.0

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-11-16 08:08:30,2019-02-12 20:45:57
cloud-provider-openstack,takaishi,https://github.com/kubernetes/cloud-provider-openstack/issues/366,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/366,Support client certificate,"/kind feature

Hello, when we have OpenStack endpoint with client certificate, we can't use `client-keystone-auth`.
I want `client-keystone-auth` support  `OS_CERT` and `OS_KEY` option like openstack cli.
",closed,False,2018-11-18 03:13:34,2019-01-14 14:43:33
cloud-provider-openstack,takaishi,https://github.com/kubernetes/cloud-provider-openstack/pull/367,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/367,client-keystone-auth supports client certificates,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**: `client-keystone-auth` supports client certificate with `OS_CERT` and `OS_KEY` options.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #366

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-11-18 03:19:39,2019-01-15 04:58:18
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/368,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/368,Keep up with k/k 1.13 tags - 1.13 beta.1,"Change-Id: I19d4067800dc95c5e0be57710565c31026dbd70c

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-20 02:33:17,2018-11-29 13:53:38
cloud-provider-openstack,chaosaffe,https://github.com/kubernetes/cloud-provider-openstack/pull/369,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/369,Add @chaosaffe to OWNERS,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Adds @chaosaffe to `OWNERS`
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
N/A
**Special notes for your reviewer**:
N/A
**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```

/cc @dims
/assign @flaper87 ",closed,True,2018-11-20 11:32:51,2018-12-31 13:22:16
cloud-provider-openstack,tsmetana,https://github.com/kubernetes/cloud-provider-openstack/pull/370,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/370,Manila: Add StorageClass parameter to specify NFS share client,"The NFS share created by Manila provisioner is by default read/write by the world. This is not always welcome behaviour. I have added a new StorageClass option that allows to specify the NFS clients that would have access to the share.

Moreover the default value ('0.0.0.0/0') doesn't work with Ganesha NFS server as it doesn't seem to like the '/0' part so I changed the default to '0.0.0.0'

```release-note
Manila provisioner recognizes a new StorageClass option to specify allowed NFS share client.
```
",closed,True,2018-11-20 13:51:52,2018-11-28 10:07:11
cloud-provider-openstack,hyperbolic2346,https://github.com/kubernetes/cloud-provider-openstack/pull/371,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/371,Fixing rbac template.,"**What this PR does / why we need it**:
Fixes the template used for rbac for the webhook keystone auth
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-20 21:28:50,2018-11-27 16:00:34
cloud-provider-openstack,ainmosni,https://github.com/kubernetes/cloud-provider-openstack/issues/372,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/372,Use OpenStack metadata service to acquire instance ID,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
Currently, when setting up the provider on a machine with a hostname that differs from the name in OpenStack the provider can't find the instance ID. The provider should use the [metadata service](https://docs.openstack.org/nova/rocky/user/metadata-service.html)  to acquire this ID to make this process more reliable.

Note that `/var/lib/cloud/data` is less reliable, as operating systems like container Linux have moved away from cloud-init.",closed,False,2018-11-22 14:21:44,2018-12-14 04:03:00
cloud-provider-openstack,ainmosni,https://github.com/kubernetes/cloud-provider-openstack/issues/373,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/373,Use OpenStack metadata service to acquire instance ID in cinder CSI plugin,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature


**What happened**:
The cinder CSI plugin currently acquires its instanceID from `/var/lib/cloud/data/instance-id`. This path will not always be available (Container Linux moved from cloud-config to ignition, which does not write that file). To make this more reliable, the plugin should use the [metadata service](https://docs.openstack.org/nova/rocky/user/metadata-service.html).

This issue is related to #372 but for a different component.",closed,False,2018-11-22 14:29:11,2018-12-04 07:44:28
cloud-provider-openstack,ainmosni,https://github.com/kubernetes/cloud-provider-openstack/issues/374,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/374,Document migration from internal to external cloud provider,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:
Currently, there's no documentation on how to migrate from the OpenStack cloudprovider that's integrated in the controller-manager. As there are a few potential pitfalls, having a ""tutorial"" style document would be quite useful and would increase the adoption of the external cloud provider.

Some things that this should include:
* Prerequisite steps to take before migrating.
* Differences in behaviour between the external and internal providers.
  * For example, the internal provider changes the node name (if it differs from the OpenStack name) while the external one doesn't.
* Order of the installation of components, and apiserver/controller-manager/kubelet configuration changes.
* Steps to verify that the migration was successful.",open,False,2018-11-22 15:17:22,2019-03-22 16:04:13
cloud-provider-openstack,ainmosni,https://github.com/kubernetes/cloud-provider-openstack/issues/375,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/375,Loadbalancer examples have the annotations on the wrong resource,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:

The annotations in examples/loadbalancers are set on the `Deployment` not on the `Service`. This causes every loadbalancer to be an external one (at least in my setup). To have any effect, the annotations have to be made on the `Service`.

These annotations hint to the provider if the loadbalancer should grab a floating IP for the loadbalancer or if its meant to remain internal.

**How to reproduce it (as minimally and precisely as possible)**:
Just apply the `internal-http-nginx.yaml` and you'll find that it will make an external one.",closed,False,2018-11-22 15:35:06,2018-11-27 14:23:18
cloud-provider-openstack,ainmosni,https://github.com/kubernetes/cloud-provider-openstack/pull/376,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/376,Move loadbalancer annotations to the service,"For the loadbalancer annotations to have any effect, they need to be in
the service definition not in the deployment.

**What this PR does / why we need it**:

It moves the loadbalancer annotations in the loadbalancers examples to the right resource. These annotations hint to the provider if the loadbalancer should grab a floating IP for the loadbalancer or if its meant to remain internal.

**Which issue this PR fixes**:
 fixes #375 ",closed,True,2018-11-22 15:37:14,2018-11-27 14:23:19
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/377,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/377,Implement Barbican as KMS provider for kubernetes,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
This PR adds Feature to use Barbican as Key manager service for kubernetes

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #44

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-11-23 11:17:17,2019-01-07 11:32:19
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/issues/378,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/378,create LB without node (master and taint only) leads to Internal Server Error from openstack,"<!-- This form is for bug reports and feature requests! -->

/kind bug


**What happened**:
Just a minor update, to provide more info when create LB member failed
**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
openstack devstack (created from commit e3e9ea299601665a295e31a98e90dd9587165850)
Nov 5
",closed,False,2018-11-26 03:05:00,2018-11-28 10:53:00
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/379,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/379,Add a member number check,"No members to be updated and calling openstack API will leads to
update error, instead of provide internal error to end user
we can provide more helpful info here.

Closes: #378


**What this PR does / why we need it**:
enhancing log output when no node existing and creating a LB
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
fixes #378 
**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```
",closed,True,2018-11-26 03:11:26,2018-11-28 10:53:01
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/380,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/380,Enhance event of ingress,"when failed to update, still get a 'updated' event
28s         Normal    Updating   Ingress   Ingress default/test-ingress1
28s         Warning   Failed     Ingress   Failed to update openstack.....
28s         Normal    Updated    Ingress   Ingress default/test-ingress1

**What this PR does / why we need it**:
avoid incorrect event generation

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
minor changes
**Special notes for your reviewer**:

**Release note**:

```release-note
NONE
```
",closed,True,2018-11-27 07:47:12,2018-11-28 11:30:17
cloud-provider-openstack,mape90,https://github.com/kubernetes/cloud-provider-openstack/issues/381,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/381,[Cinder CSI] Race condition in volume deletion between attacher and provisioner,"/kind bug
**What happened**:
When volumes are deleted. Both detaching and deletion are executed same time. This causes race condition where volume can be deleted before detacher found out that it was detached. 
After this the detaching will never succeed and `VolumeAttachment` is forever try to detach the volume.

**What you expected to happen**:
If we are detaching volume and we do not find the volume it should be quite safe to assume that it isn't attached to the host VM.

**How to reproduce it (as minimally and precisely as possible)**:
Create big deployment with multiple pods with volumes and delete them. Looping this should eventually cause this problem. Also if volume is long time in detaching state (over 1min timer that `WaitDiskDetached` is waiting) and is finally detached it might be that provisioner deletes it before attacher retries to detach it.

**Anything else we need to know?**:
One solution could be to catch 404 error from `os.GetVolume(volumeID)` and then assume that volume is detached if we get that error while detaching.


**Environment**:
csi-attacher:v0.3.0
csi-provisioner:v0.3.1
cinder-csi-plugin:v0.3.0",closed,False,2018-11-27 14:16:42,2019-01-11 14:37:54
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/pull/382,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/382,manila-provisioner: added mounter StorageClass parameter for csi-cephfs,"**What this PR does / why we need it**:
Adds a new StorageClass param for the csi-cephfs backend, users can now choose whether to use the Ceph kernel client or Ceph FUSE driver for mounting.

```release-note
Manila provisioner recognizes a new StorageClass option to specify mount tool for csi-cephfs backend
```
",closed,True,2018-11-29 09:36:15,2018-11-29 10:11:39
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/383,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/383,Switch to Kubernetes 1.13.0 release,"Change-Id: Id29f12f2ae97f8afbc6e756ddd2e69ebb923b253

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

Update to 1.13 RC1 

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-11-29 13:54:18,2018-12-04 17:02:49
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/pull/384,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/384,manila-provisioner: added mounter option for csi-cephfs backend,"**What this PR does / why we need it**:
Adds a new StorageClass param for the csi-cephfs backend, users can now choose whether to use the Ceph kernel client or Ceph FUSE driver for mounting.

```release-note
Manila provisioner recognizes a new StorageClass option to specify mount tool for csi-cephfs backend
```
",closed,True,2018-11-29 14:11:42,2018-12-17 07:55:06
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/385,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/385,Add doc for csi cinder usage,"some guide on how to use the csi plugin in the doc

**What this PR does / why we need it**:
enhance the doc
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:

```release-note
NONE
```
",closed,True,2018-11-30 06:54:54,2018-12-19 11:21:21
cloud-provider-openstack,zhiyxu,https://github.com/kubernetes/cloud-provider-openstack/issues/386,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/386,Persistent Volume provision of external OpenStack provider doesn't work normally,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug


**What happened**:
Hi Guys

I'm trying to setup Kubernetes Persistent Volume with Cinder, for the cluster is established on Openstack, my k8s version is ```1.9.8```

I've successfully tested In-tree OpenStack provider, by adding ```--cloud-provider``` and ```--cloud-config``` in ```kubelet```, ```kube-apiserver```, ```kube-controller-manager```, but I found out-of-tree [cloud controller manager](https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/) is the future trend, so I do the following configuration as docs:

- remove ```--cloud-provider``` in ```kube-apiserver``` and ```kube-controller-manager```, use ```--cloud-provider=external``` in all ```kubelet``` instead

- add admission controller ```DefaultStorageClass```,```Initializers``` and ```--runtime-config=admissionregistration.k8s.io/v1alpha1``` flag in ```kube-api-server```

- add ```cloud-config``` file in ```/etc/cloud/cloud-config``` of all master nodes

```
[Global]
domain-name = default
username = xxx
password = xxx
auth-url = xxx
tenant-id = xxx
region = RegionOne

[BlockStorage]
bs-version = v2
ignore-volume-az = yes
```

- create ```ClusterRole```, ```ClusterRoleBinding``` and ```configmap```

```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/master/cluster/addons/rbac/cloud-controller-manager-roles.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/master/cluster/addons/rbac/cloud-controller-manager-role-bindings.yaml
kubectl create configmap cloud-config --from-file=cloud-config -n kube-system
```

- config openstack-cloud-controller-manager-ds.yaml under cloud-provider-openstack/manifests/controller-manager and deploy

```
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cloud-controller-manager
  namespace: kube-system
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: openstack-cloud-controller-manager
  namespace: kube-system
  labels:
    k8s-app: openstack-cloud-controller-manager
spec:
  selector:
    matchLabels:
      k8s-app: openstack-cloud-controller-manager
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        k8s-app: openstack-cloud-controller-manager
    spec:
      nodeSelector:
        node-role.kubernetes.io/master: ""true""
      tolerations:
      - key: node.cloudprovider.kubernetes.io/uninitialized
        operator: ""Equal""
        value: ""true""
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: ""Equal""
        value: ""true""
        effect: NoSchedule
      serviceAccountName: cloud-controller-manager
      containers:
        - name: openstack-cloud-controller-manager
          image: k8scloudprovider/openstack-cloud-controller-manager:latest
          args:
            - /bin/openstack-cloud-controller-manager
            - --v=2
            - --cloud-config=/etc/cloud/cloud-config
            - --cloud-provider=openstack
            - --use-service-account-credentials=true
            - --address=127.0.0.1
            - --kubeconfig=/etc/kubernetes/config
          volumeMounts:
            - mountPath: /etc/pki
              name: k8s-certs
              readOnly: true
            - mountPath: /etc/ssl/certs
              name: ca-certs
              readOnly: true
            - mountPath: /etc/kubernetes/config
              name: kubeconfig
              readOnly: true
            - mountPath: /etc/cloud
              name: cloud-config-volume
              readOnly: true
            - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
              name: flexvolume-dir
          resources:
            requests:
              cpu: 200m
      hostNetwork: true
      volumes:
      - hostPath:
          path: /etc/libexec/kubernetes/kubelet-plugins/volume/exec
          type: DirectoryOrCreate
        name: flexvolume-dir
      - hostPath:
          path: /etc/pki
          type: DirectoryOrCreate
        name: k8s-certs
      - hostPath:
          path: /etc/ssl/certs
          type: DirectoryOrCreate
        name: ca-certs
      - hostPath:
          path: /etc/kubernetes/config
          type: FileOrCreate
        name: kubeconfig
      - name: cloud-config-volume
        configMap:
          name: cloud-config
```


- deploy InitializerConfiguration

```
kind: InitializerConfiguration
apiVersion: admissionregistration.k8s.io/v1alpha1
metadata:
  name: pvlabel.kubernetes.io
initializers:
  - name: pvlabel.kubernetes.io
    rules:
    - apiGroups:
      - """"
      apiVersions:
      - ""*""
      resources:
      - persistentvolumes
```

all the process works fine, but when I [deploy pods](https://gist.github.com/zhiyxu/dcb2e38a00bce1d940363d35c52eb39c) using PV and PVC, whether statically or dynamically, the cinder volume just cannot bind the correspoing node.

I go over the logs of openstack-cloud-controller-manager and got some error msg in node_controller:

```
$ kubectl logs -n kube-system openstack-cloud-controller-manager-bn279
I1130 04:46:35.631628       1 flags.go:33] FLAG: --address=""0.0.0.0""
I1130 04:46:35.631689       1 flags.go:33] FLAG: --allocate-node-cidrs=""false""
I1130 04:46:35.631695       1 flags.go:33] FLAG: --allow-untagged-cloud=""false""
I1130 04:46:35.631698       1 flags.go:33] FLAG: --alsologtostderr=""false""
I1130 04:46:35.631702       1 flags.go:33] FLAG: --authentication-kubeconfig=""""
I1130 04:46:35.631705       1 flags.go:33] FLAG: --authentication-skip-lookup=""false""
I1130 04:46:35.631708       1 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl=""10s""
I1130 04:46:35.631713       1 flags.go:33] FLAG: --authorization-always-allow-paths=""[/healthz]""
I1130 04:46:35.631720       1 flags.go:33] FLAG: --authorization-kubeconfig=""""
I1130 04:46:35.631723       1 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl=""10s""
I1130 04:46:35.631726       1 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl=""10s""
I1130 04:46:35.631729       1 flags.go:33] FLAG: --bind-address=""127.0.0.1""
I1130 04:46:35.631733       1 flags.go:33] FLAG: --cert-dir=""/var/run/kubernetes""
I1130 04:46:35.631736       1 flags.go:33] FLAG: --cidr-allocator-type=""RangeAllocator""
I1130 04:46:35.631739       1 flags.go:33] FLAG: --client-ca-file=""""
I1130 04:46:35.631742       1 flags.go:33] FLAG: --cloud-config=""/etc/cloud/cloud-config""
I1130 04:46:35.631745       1 flags.go:33] FLAG: --cloud-provider=""openstack""
I1130 04:46:35.631747       1 flags.go:33] FLAG: --cluster-cidr=""""
I1130 04:46:35.631750       1 flags.go:33] FLAG: --cluster-name=""kubernetes""
I1130 04:46:35.631753       1 flags.go:33] FLAG: --concurrent-service-syncs=""1""
I1130 04:46:35.631757       1 flags.go:33] FLAG: --configure-cloud-routes=""true""
I1130 04:46:35.631760       1 flags.go:33] FLAG: --contention-profiling=""false""
I1130 04:46:35.631763       1 flags.go:33] FLAG: --controller-start-interval=""0s""
I1130 04:46:35.631766       1 flags.go:33] FLAG: --controllers=""[*]""
I1130 04:46:35.631769       1 flags.go:33] FLAG: --external-cloud-volume-plugin=""""
I1130 04:46:35.631772       1 flags.go:33] FLAG: --feature-gates=""""
I1130 04:46:35.631777       1 flags.go:33] FLAG: --help=""false""
I1130 04:46:35.631780       1 flags.go:33] FLAG: --http2-max-streams-per-connection=""0""
I1130 04:46:35.631785       1 flags.go:33] FLAG: --kube-api-burst=""30""
I1130 04:46:35.631788       1 flags.go:33] FLAG: --kube-api-content-type=""application/vnd.kubernetes.protobuf""
I1130 04:46:35.631791       1 flags.go:33] FLAG: --kube-api-qps=""20""
I1130 04:46:35.631799       1 flags.go:33] FLAG: --kubeconfig=""/etc/kubernetes/config""
I1130 04:46:35.631802       1 flags.go:33] FLAG: --leader-elect=""true""
I1130 04:46:35.631805       1 flags.go:33] FLAG: --leader-elect-lease-duration=""15s""
I1130 04:46:35.631808       1 flags.go:33] FLAG: --leader-elect-renew-deadline=""10s""
I1130 04:46:35.631811       1 flags.go:33] FLAG: --leader-elect-resource-lock=""endpoints""
I1130 04:46:35.631813       1 flags.go:33] FLAG: --leader-elect-retry-period=""2s""
I1130 04:46:35.631816       1 flags.go:33] FLAG: --log-backtrace-at="":0""
I1130 04:46:35.631821       1 flags.go:33] FLAG: --log-dir=""""
I1130 04:46:35.631825       1 flags.go:33] FLAG: --log-flush-frequency=""5s""
I1130 04:46:35.631827       1 flags.go:33] FLAG: --logtostderr=""true""
I1130 04:46:35.631830       1 flags.go:33] FLAG: --master=""""
I1130 04:46:35.631833       1 flags.go:33] FLAG: --min-resync-period=""12h0m0s""
I1130 04:46:35.631836       1 flags.go:33] FLAG: --node-monitor-period=""5s""
I1130 04:46:35.631839       1 flags.go:33] FLAG: --node-status-update-frequency=""5m0s""
I1130 04:46:35.631842       1 flags.go:33] FLAG: --node-sync-period=""0s""
I1130 04:46:35.631845       1 flags.go:33] FLAG: --port=""10253""
I1130 04:46:35.631848       1 flags.go:33] FLAG: --profiling=""false""
I1130 04:46:35.631851       1 flags.go:33] FLAG: --requestheader-allowed-names=""[]""
I1130 04:46:35.631855       1 flags.go:33] FLAG: --requestheader-client-ca-file=""""
I1130 04:46:35.631858       1 flags.go:33] FLAG: --requestheader-extra-headers-prefix=""[x-remote-extra-]""
I1130 04:46:35.631862       1 flags.go:33] FLAG: --requestheader-group-headers=""[x-remote-group]""
I1130 04:46:35.631866       1 flags.go:33] FLAG: --requestheader-username-headers=""[x-remote-user]""
I1130 04:46:35.631870       1 flags.go:33] FLAG: --route-reconciliation-period=""10s""
I1130 04:46:35.631873       1 flags.go:33] FLAG: --secure-port=""10258""
I1130 04:46:35.631876       1 flags.go:33] FLAG: --stderrthreshold=""2""
I1130 04:46:35.631879       1 flags.go:33] FLAG: --tls-cert-file=""""
I1130 04:46:35.631882       1 flags.go:33] FLAG: --tls-cipher-suites=""[]""
I1130 04:46:35.631885       1 flags.go:33] FLAG: --tls-min-version=""""
I1130 04:46:35.631889       1 flags.go:33] FLAG: --tls-private-key-file=""""
I1130 04:46:35.631891       1 flags.go:33] FLAG: --tls-sni-cert-key=""[]""
I1130 04:46:35.631895       1 flags.go:33] FLAG: --use-service-account-credentials=""true""
I1130 04:46:35.631898       1 flags.go:33] FLAG: --v=""2""
I1130 04:46:35.631901       1 flags.go:33] FLAG: --version=""false""
I1130 04:46:35.631906       1 flags.go:33] FLAG: --vmodule=""""
I1130 04:46:35.976116       1 serving.go:293] Generated self-signed cert (/var/run/kubernetes/cloud-controller-manager.crt, /var/run/kubernetes/cloud-controller-manager.key)
W1130 04:46:36.320611       1 authentication.go:280] Cluster doesn't provide requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W1130 04:46:36.323406       1 plugins.go:114] WARNING: openstack built-in cloud provider is now deprecated. Please use 'external' cloud provider for openstack: https://github.com/kubernetes/cloud-provider-openstack
I1130 04:46:36.547016       1 secure_serving.go:116] Serving securely on 127.0.0.1:10258
I1130 04:46:36.547354       1 deprecated_insecure_serving.go:50] Serving insecurely on [::]:10253
I1130 04:46:36.547604       1 leaderelection.go:187] attempting to acquire leader lease  kube-system/cloud-controller-manager...
I1130 04:46:36.567085       1 leaderelection.go:196] successfully acquired lease kube-system/cloud-controller-manager
I1130 04:46:36.567769       1 event.go:221] Event(v1.ObjectReference{Kind:""Endpoints"", Namespace:""kube-system"", Name:""cloud-controller-manager"", UID:""eb3aa868-f45a-11e8-9e47-fa163e46faea"", APIVersion:""v1"", ResourceVersion:""136968"", FieldPath:""""}): type: 'Normal' reason: 'LeaderElection' pv-cluster-evan.master-10.182.70.162.novalocal_eb39ec2b-f45a-11e8-9c3b-fa163e46faea became leader
I1130 04:46:36.632724       1 node_controller.go:89] Sending events to api server.
I1130 04:46:36.674907       1 pvlcontroller.go:112] Starting PersistentVolumeLabelController
I1130 04:46:36.675133       1 controller_utils.go:1027] Waiting for caches to sync for persistent volume label controller
E1130 04:46:36.710234       1 controllermanager.go:227] Failed to start service controller: the cloud provider does not support external load balancers
I1130 04:46:36.710458       1 controllermanager.go:251] Will not configure cloud provider routes for allocate-node-cidrs: false, configure-cloud-routes: true.
I1130 04:46:36.775409       1 controller_utils.go:1034] Caches are synced for persistent volume label controller
E1130 04:46:36.985764       1 node_controller.go:368] failed to set node provider id: failed to get instance ID from cloud provider: instance not found
E1130 04:46:37.107110       1 node_controller.go:419] NodeAddress: Error fetching by providerID: ProviderID """" didn't match expected format ""openstack:///InstanceID"" Error fetching by NodeName: failed to find object
E1130 04:46:37.256736       1 node_controller.go:368] failed to set node provider id: failed to get instance ID from cloud provider: instance not found
E1130 04:46:37.372633       1 node_controller.go:419] NodeAddress: Error fetching by providerID: ProviderID """" didn't match expected format ""openstack:///InstanceID"" Error fetching by NodeName: failed to find object
E1130 04:46:37.512457       1 node_controller.go:368] failed to set node provider id: failed to get instance ID from cloud provider: instance not found
E1130 04:46:37.819174       1 node_controller.go:419] NodeAddress: Error fetching by providerID: ProviderID """" didn't match expected format ""openstack:///InstanceID"" Error fetching by NodeName: failed to find object
```

Did I miss some procedures or where did I go wrong?

Thank you so much!

**What you expected to happen**:

Out-of-tree OpenStack Cloud Controller Manager functioning well.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: latest
- OS (e.g. from /etc/os-release): CoreOS
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2018-11-30 07:04:44,2019-03-14 22:39:44
cloud-provider-openstack,attardi,https://github.com/kubernetes/cloud-provider-openstack/issues/387,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/387,rename client-keystone-auth to kubectl-keystone-auth,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
/kind feature


**What happened**:
According to Keystone conventions (https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/), plugins should be named with prefix kubectl-, therefore client-keystone-auth should be renamed as kubectl-keystone-auth.

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:

",open,False,2018-12-02 11:07:46,2019-03-17 23:13:51
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/issues/391,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/391,Delete a node then readd it in LB ingress has issue,"/kind bug



**What happened**:
root@kvm-017407:~/go/src/k8s.io/cloud-provider-openstack# kubectl get ing
NAME            HOSTS         ADDRESS       PORTS   AGE
test-ingress1   foo.bar.com   172.24.4.22   80      44h


root@kvm-017407:~/go/src/k8s.io/cloud-provider-openstack# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
kvm-017407   Ready    master   49d   v1.12.1
kvm-017734   Ready    <none>   21m   v1.12.2

kubectl delete node
delete node (17734) but LB doesn't have any update, I think we should consider to update openstack member 

Also
has following log
time=""2018-12-05T02:53:36Z"" level=info msg=""Detected change in list of current cluster nodes. New node set: []""
time=""2018-12-05T02:53:37Z"" level=error msg=""Failed to handle ingress"" ingress=test-ingress1

means we should avoid doing up in this case (node = [])



",closed,False,2018-12-05 03:16:41,2019-01-25 02:29:27
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/392,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/392,avoid update loadbalancer member if new node is [],"avoid update loadbalancer member if new node is []

**What this PR does / why we need it**:
when new node is [], the update loadbalancer member will lead to some issue 
**Which issue this PR fixes** 
fixes #391 



```release-note
NONE
```
",closed,True,2018-12-05 03:38:11,2019-01-25 02:29:27
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/393,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/393,Add Document to Install and Use Barbican KMS Plugin,"**What this PR does / why we need it**:
Doucment to Install and Use barbican KMS plugin

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
`NONE`
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```

",closed,True,2018-12-07 08:33:58,2018-12-24 20:46:01
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/issues/394,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/394,add csi-manila,"With CSI 1.0 out, we can start drafting some initial implementation for csi-manila using the ""proxy architecture"" as discussed offline. If there's no current work in progress by somebody else, I'll be glad to get this one started.

I don't have a PR ready yet but till then it would be great if we got the dependencies for 1.0 in (https://github.com/kubernetes/cloud-provider-openstack/issues/365).

cc @hogepodge @flaper87 

/kind feature
",open,False,2018-12-07 19:51:21,2019-04-03 00:47:29
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/395,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/395,Add client-keystone-auth to release assets,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**: 
client-keystone-auth binary is missing from the release.

**What you expected to happen**:
Include client-keystone-auth binary in the release

**How to reproduce it (as minimally and precisely as possible)**:
N/A

**Anything else we need to know?**:
N/A
",open,False,2018-12-11 01:59:29,2019-03-11 03:05:56
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/396,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/396,Trivial Fix: update incorrect log,"the log indicate wrong info, fix them

**What this PR does / why we need it**:
fix log
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:

```release-note
None
```
",closed,True,2018-12-11 05:21:13,2018-12-13 11:44:01
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/397,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/397,"add a check when the volume detach failed,","add a check when the volume detach failed, if it's a volume
not found, ignore the error and return false

**What this PR does / why we need it**:
Fix bug 381, add a check for the volume not found error for a race
**Which issue this PR fixes** 
fixes #381 
**Special notes for your reviewer**:

**Release note**:

```release-note
None
```
",closed,True,2018-12-11 06:01:39,2019-01-11 14:37:54
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/398,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/398,"Fix part of bug 372, try to use metadata to retrieve data first","first, let's try use metadata to retrieve data then use cloud-init data
after all, the data in metadata/config drive won't be changed but
it's likely the data in cloud-init generated might be changed.




**What this PR does / why we need it**:
use metadata/config drive first then cloud-init generated data

**Which issue this PR fixes** *
fixes #372 
**Special notes for your reviewer**:

**Release note**:
```release-note
None
```
",closed,True,2018-12-11 06:44:25,2018-12-14 04:03:01
cloud-provider-openstack,brtknr,https://github.com/kubernetes/cloud-provider-openstack/pull/399,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/399,Move loadbalancer annotation to service in README file,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**: The README is out of date compared to the yams file.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
Fixes #346 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-12-11 14:42:01,2018-12-13 13:49:43
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/400,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/400,Exclude irrelavent Files from CI jobs,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:
All CI jobs run in cloud-provider-openstack, for document changes also.

**What you expected to happen**:
we should exclude following files/directories from CI jobs
docs/
*.md
OWNERS
SECURITY_CONTACTS
",closed,False,2018-12-14 07:53:04,2018-12-17 16:04:58
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/401,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/401,Do not run jobs for changes to documentation,"```yaml
irrelevant-files:
  - ^docs/.*$
  - ^.*\.md$
  - ^OWNERS$
  - ^SECURITY_CONTACTS$
```

Fixes: kubernetes/cloud-provider-openstack#400

Signed-off-by: Melvin Hillsman <mrhillsman@gmail.com>

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-12-15 02:48:02,2018-12-18 02:02:11
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/402,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/402,Fix pvl controller role,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Add `patch` and `update` permission to pvl-controller role.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
The pvl-controller needs to update the pv's labels with region and zone information.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-12-17 11:37:19,2018-12-17 13:30:06
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/403,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/403,Fix cloud controller manager related role,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Fix cloud-controller-manager role name and add some resource permission

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-12-17 23:09:35,2018-12-18 03:22:43
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/404,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/404,Fix cloud controller manager role,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Fix cloud-controller-manager role name and add configmaps resource permission

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-12-18 03:24:05,2018-12-18 22:36:39
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/405,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/405,Fix getting instance availability zone from Nova,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Fix the code to set the correct availability zone for the instance, otherwide the pvc cannot bind to the pod because the zone doesn't match(using `ignore-volume-az=false`).

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-12-18 05:37:23,2018-12-18 21:00:16
cloud-provider-openstack,strigazi,https://github.com/kubernetes/cloud-provider-openstack/issues/406,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/406,Support for sssd in client-keystone-auth,"/kind feature

client-keystone-auth should support authentication with sssd.

OS_PROJECT_DOMAIN_ID=default
OS_IDENTITY_PROVIDER=sssd
OS_PROJECT_NAME=<NAME>
OS_MUTUAL_AUTH=<OPTION>
OS_IDENTITY_API_VERSION=3
OS_AUTH_TYPE=v3fedkerb
OS_PROTOCOL=kerberos
OS_AUTH_URL=<URL>",open,False,2018-12-18 08:58:06,2019-03-21 13:21:12
cloud-provider-openstack,mogaika,https://github.com/kubernetes/cloud-provider-openstack/issues/407,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/407,Multi-nic node ip must correspond to kubelet api listening address,"/kind bug

**What happened**:
openstack cloud provider replaces nodes internal ip with openstack node instance addresses, but kubelet api of node can be bounded only to one of these addresses (e.g. for security purposes). Typical scenario: first nic - for deployment, second nic- for k8s network and kubelet --address parameter equals to the second nic ip. In this case cluster can receive wrong address of node kubelet api (because usually selects first nic ip as cluster ip) and features like ""kubectl logs"" and ""kubectl proxy"" will not work. 

**What you expected to happen**:
Possibility to declare control-network in cloud config file.
Or declare kubelet listen addresses per node.

**How to reproduce it (as minimally and precisely as possible)**:
Use multiple nics for kubernetes nodes and set kubelet --address parameter to one of them
Check that wrong ip was assigned to internal-ip field in `kubectl get nodes -o wide` output
Try to `kubectl exec` to pod that was sheduled to node with wrong ip.

**Anything else we need to know?**:
from opentack-cloud-provider logs
`Error patching node with cloud ip addresses = [failed to patch status ""{\""status\"":{\""$setElementOrder/addresses\"":[{\""type\"":\""InternalIP\""},{\""type\"":\""ExternalIP\""},{\""type\"":\""InternalIP\""}],\""addresses\"":[{\""address\"":\""172.16.10.9\""type\"":\""InternalIP\""},{\""type\"":\""ExternalIP\""},{\""type\"":\""InternalIP\""}],\""addresses\"":[{\""address\"":\""172.16.10.95\"",\""type\"":\""InternalIP\""},{\""address\"":\""192.168.10.102\"",\""type\"":\""InternalIP\""}]}}"" for node ""cmp1.****"": Node ""cmp1.****"" is invalid: status.addresses[1]: Duplicate value: core.NodeAddress{Type:""InternalIP"", Address:""192.168.10.102""}]`

**Environment**:
- openstack-cloud-controller-manager version: 
- OS (e.g. from /etc/os-release): Ubuntu 16.04.1 LTS (Xenial Xerus)
- Kernel (e.g. `uname -a`): Linux ctl03 4.4.0-36-generic #55-Ubuntu SMP Thu Aug 11 18:01:55 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- Install tools: saltstack
- Node networks:
nic1 - 192.168.10.0/24
nic2 - 172.16.10.0/24
+ floating ip as external network - 10.13.0.0/16 
- Example of status.addresses of node:
```
    addresses:
    - address: 192.168.10.94
      type: InternalIP
    - address: 10.13.250.32
      type: ExternalIP
    - address: 172.16.10.112
      type: InternalIP
```
- Output of `kubectl get nodes -o wide`:
```root@ctl03:~# kubectl get nodes -o wide
NAME         STATUS   ROLES    AGE    VERSION                    INTERNAL-IP      EXTERNAL-IP    OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
cmp0.****    Ready    node     12h    v1.12.3-2+bc2e990ce4e046   192.168.10.98    10.13.250.12   Ubuntu 16.04.1 LTS   4.4.0-36-generic   docker://1.13.1
cmp1.****    Ready    node     14h    v1.12.3-2+bc2e990ce4e046   172.16.10.95     10.13.250.26   Ubuntu 16.04.1 LTS   4.4.0-36-generic   docker://1.13.1
ctl01.****   Ready    master   4d3h   v1.12.3-2+bc2e990ce4e046   192.168.10.104   10.13.250.25   Ubuntu 16.04.1 LTS   4.4.0-36-generic   docker://1.13.1
ctl02.****   Ready    master   14h    v1.12.3-2+bc2e990ce4e046   192.168.10.92    10.13.250.19   Ubuntu 16.04.1 LTS   4.4.0-36-generic   docker://1.13.1
ctl03.****   Ready    master   4d3h   v1.12.3-2+bc2e990ce4e046   192.168.10.94    10.13.250.32   Ubuntu 16.04.1 LTS   4.4.0-36-generic   docker://1.13.1```",closed,False,2018-12-18 14:37:50,2019-01-21 20:30:37
cloud-provider-openstack,wongma7,https://github.com/kubernetes/cloud-provider-openstack/pull/408,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/408,Parse manila provisioner flags,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**: make manila provisioner parse flags (--provisioner). Currently it is ignoring them because nil is passed to flag.CommandLine.Parse(nil). Calling flag.Parse should be the same thing (see https://golang.org/pkg/flag/#pkg-variables) as calling it with the non-nil args

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-12-18 19:47:09,2018-12-19 11:31:28
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/409,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/409,Fix the ccm manifest file and RBAC definition,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Remove the dependency of KCM's kubeconfig file and modify the RBAC roles for CCM.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-12-18 23:11:15,2018-12-19 05:38:37
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/410,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/410,Update doc: using-controller-manager-with-kubeadm,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Update documentation using-controller-manager-with-kubeadm.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-12-19 02:34:24,2018-12-19 05:38:15
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/411,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/411,enhance ccm configuration file,"to make the version consistent
to make the network settings by default



**What this PR does / why we need it**:
to make ccm create have network settings by default
to make sample use consistent version to kubeadm

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

```release-note
None
```
",closed,True,2018-12-21 07:23:07,2018-12-24 20:57:50
cloud-provider-openstack,netcho,https://github.com/kubernetes/cloud-provider-openstack/issues/412,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/412,TCP listener prevents application of HTTP rules,"The hard coded protocol type of TCP makes haproxy inside amphorae to neglect L7 HTTP rules. Here's the line
https://github.com/kubernetes/cloud-provider-openstack/blob/cc82c7f119cd63e857c47c34796a272cc967cf02/pkg/ingress/controller/openstack/octavia.go#L395

My suggestion would be to change it to HTTP as for now Ingress only supports HTTP/HTTPS and discuss TCP loadbalancing capabilities of Octavia can be used through annotations in the Ingress resource itself",closed,False,2018-12-23 14:51:57,2019-01-09 10:15:32
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/413,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/413,Remove dependency from csi-common,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
This PR removes dependency of cinder csi driver from
https://github.com/kubernetes-csi/drivers/tree/master/pkg/csi-common.
This is necessary to have own identity rpc plugin.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #359
partialfixes: #301 

**Special notes for your reviewer**:

**Release note**:
`NONE`
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2018-12-27 12:50:54,2019-01-17 19:37:21
cloud-provider-openstack,chaosaffe,https://github.com/kubernetes/cloud-provider-openstack/pull/414,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/414,Fix Octavia typo,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Fixes a typo in the Octavia logs, specifically replacing ` listenserID` with `listenerID`

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: N/A

**Special notes for your reviewer**:
None

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-12-31 15:26:31,2019-01-01 00:27:43
cloud-provider-openstack,chaosaffe,https://github.com/kubernetes/cloud-provider-openstack/pull/415,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/415,Rename newNodes to readyWorkerNodes,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Variable name now reflects the nodes returned by the list predicate. Prevents confusion in the codebase.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: 
N/A

**Special notes for your reviewer**:
xref: https://github.com/kubernetes/cloud-provider-openstack/pull/392#issuecomment-450656838

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2018-12-31 15:32:55,2019-01-01 12:35:52
cloud-provider-openstack,chaosaffe,https://github.com/kubernetes/cloud-provider-openstack/pull/416,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/416,Use worker node predicate directly,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Removes getNodeConditionPredicate() and passes the predicate func directly to ListWithPredicate()
Improves code readability and reduces code surface

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: 
None

**Special notes for your reviewer**:
None

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-01-01 23:49:20,2019-01-07 02:36:51
cloud-provider-openstack,chaosaffe,https://github.com/kubernetes/cloud-provider-openstack/pull/417,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/417,Sanitation on pkg/ingress/,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
General code cleanup based on some linter runs. Specifically focused on:
- Error checking and logging
- Removing dead code
- Deduplication

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: 
N/A

**Special notes for your reviewer**:
None

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-01-02 02:10:22,2019-01-14 16:51:35
cloud-provider-openstack,chaosaffe,https://github.com/kubernetes/cloud-provider-openstack/pull/418,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/418,Move kms packages to klog,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Changes KMS packages from using glog to using klog

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*:
N/A

**Special notes for your reviewer**:
N/A

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-01-02 06:59:50,2019-01-11 13:19:24
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/419,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/419,[octavia-ingress-controller] Use HTTP for loadbalancing protocol,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Use HTTP load balancing protocol for Ingress implementation.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: 
fixes #412

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-07 01:58:06,2019-01-22 00:48:26
cloud-provider-openstack,UristMcMiner,https://github.com/kubernetes/cloud-provider-openstack/issues/420,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/420,manila-provisioner missing share-network configuration option,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature


**What happened**:

Trying to provision a share via manila-provisioner in kubernetes with the option driver_handles_share_servers = True.
The provisioning failed with 400: BAD REQUEST due to the missing share_network parameter

**What you expected to happen**:

share_network to be configurable

**How to reproduce it (as minimally and precisely as possible)**:

Use manila-provisioner to provision a share with DHSS enabled

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
  N/A
- OS (e.g. from /etc/os-release):
  Ubuntu 16.04
- Kernel (e.g. `uname -a`):
  4.4.0-140-generic
- Install tools:
  N/A
- Others:
Image of manila-provisioner used:
k8scloudprovider/manila-provisioner:1.13.1
",closed,False,2019-01-07 13:28:17,2019-02-19 22:47:31
cloud-provider-openstack,chrigl,https://github.com/kubernetes/cloud-provider-openstack/issues/421,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/421,Enable usage of OpenStack Application Credentials,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:

Currently when configuring components like the `openstack-cloud-controller-manager` with a `cloud.conf`, one is only able to use the owners OpenStack User and Password.

From version 3.10 (included in the Queens release), OpenStack Keystone supports Application Credentials (also see #232 and https://developer.openstack.org/api-ref/identity/v3/index.html#application-credentials).

It would be nice to be able to configure Application Credentials in `cloud.conf` over using real users password. 

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2019-01-08 15:06:30,2019-01-09 09:14:46
cloud-provider-openstack,emonty,https://github.com/kubernetes/cloud-provider-openstack/pull/422,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/422,Add annotations for setting listener timeouts,"**What this PR does / why we need it**:

The octavia loadbalancer supports setting timeouts for client and member
data connections. For some operations, such as pushing git large git
repositories over an ssh connection, it is very important to be able to
increase the timeout.

This adds two annotations that allows a user to create a load balancer
and set its timeout_client_data and timeout_member_data fields.

**Special notes for your reviewer**:

Depends-On: https://github.com/gophercloud/gophercloud/pull/1394",open,True,2019-01-09 23:52:20,2019-02-18 01:29:31
cloud-provider-openstack,woodne,https://github.com/kubernetes/cloud-provider-openstack/issues/423,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/423,csi-cinder plugin manifest examples do not work,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

When installing the cinder-csi plugin, after creating a storage class and a PVC, the provisioner gives errors about no OS_AUTH_URL being defined.

**What you expected to happen**:

It pulls the cloud config settings from the cloud config secret

**How to reproduce it (as minimally and precisely as possible)**:

- Bootstrap a cluster with 1.13.1
- follow the instructions for installing the cinder-csi plugin
- create a storage class 
```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: csi-sc-cinderplugin
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: ""true""
provisioner: csi-cinderplugin```
- create a pvc that uses this storage class
- see the logs for the `kubectl logs csi-provisioner-cinderplugin-0`

example:
$ kubectl logs -n kube-system csi-provisioner-cinderplugin-0 cinder                               
I0109 15:54:37.781015       1 driver.go:49] Driver: cinder.csi.openstack.org version: 0.3.0
I0109 15:54:37.781116       1 driver.go:80] Enabling controller service capability: LIST_VOLUMES
I0109 15:54:37.781125       1 driver.go:80] Enabling controller service capability: CREATE_DELETE_VOLUME
I0109 15:54:37.781129       1 driver.go:80] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
I0109 15:54:37.781132       1 driver.go:80] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
I0109 15:54:37.781136       1 driver.go:80] Enabling controller service capability: LIST_SNAPSHOTS
I0109 15:54:37.781140       1 driver.go:92] Enabling volume access mode: SINGLE_NODE_WRITER
I0109 15:54:37.781275       1 server.go:108] Listening for connections on address: &net.UnixAddr{Name:""/csi/csi.sock"", Net:""unix""}
E0109 15:55:50.355211       1 utils.go:100] GRPC error: Missing environment variable [OS_AUTH_URL]
E0109 15:55:50.359424       1 utils.go:100] GRPC error: Missing environment variable [OS_AUTH_URL]
E0109 15:56:05.365802       1 utils.go:100] GRPC error: Missing environment variable [OS_AUTH_URL]
E0109 15:57:05.376952       1 utils.go:100] GRPC error: Missing environment variable [OS_AUTH_URL]
E0109 15:59:05.387402       1 utils.go:100] GRPC error: Missing environment variable [OS_AUTH_URL]
```

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: 
- OS (e.g. from /etc/os-release):""Ubuntu 18.04.1 LTS""
- Kernel (e.g. `uname -a`): 4.15.0-33-generic
- Install tools: 
- Others:
",closed,False,2019-01-10 01:59:57,2019-01-11 05:57:52
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/424,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/424,Add CRD and deployment for csi snapshot,"<!-- This form is for bug reports and feature requests! -->
Add CRD and Deployment yaml's for CSI Snapshot

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
Snapshot Support Implemented by following PR in Cinder CSI.
 [csi] add snapshot support #344 
- We need to add CRD's to use the snapshot functionality, and deployment file for external-snapshotter
- Update Document and CI",closed,False,2019-01-10 06:48:53,2019-02-18 05:27:07
cloud-provider-openstack,emonty,https://github.com/kubernetes/cloud-provider-openstack/pull/425,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/425,Add support for selecting a specific cloud from clouds.yaml,"**What this PR does / why we need it**:
clouds.yaml files can contain more than one cloud entry. Allow a user
to configure the named cloud from the file explicitly.
",closed,True,2019-01-10 15:18:54,2019-01-15 12:05:48
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/426,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/426,Add application id into ccm,"accordingly to
https://developer.openstack.org/api-ref/identity/v3/index.html#application-credentials
we can add ccm application id into cloud.conf to do validation

**What this PR does / why we need it**:
fix bug 421 to add application cred validation
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
fixes 421
**Special notes for your reviewer**:

**Release note**:
```release-note
to be add here:
```
",open,True,2019-01-11 08:55:09,2019-03-05 07:20:29
cloud-provider-openstack,JoeWrightss,https://github.com/kubernetes/cloud-provider-openstack/pull/427,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/427,Fix some typos in comment,"1. ""unformated"" to ""unformatted"".
2. ""fieds"" to ""fields"".
3. ""Recusively"" to ""Recursively"".
4. ""thet"" to ""that"".",closed,True,2019-01-11 16:30:08,2019-01-14 14:53:27
cloud-provider-openstack,nabheet,https://github.com/kubernetes/cloud-provider-openstack/issues/428,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/428,Pods not getting IP address assigned after adding --cloud-provider=external to kubelet args,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature

**What happened**:
I am not sure that this is a bug and I didn't know where to ask this as a question (maybe on StackOverFlow?). I followed the instructions at https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-controller-manager-with-kubeadm.md to set up the controller. The controller wouldn't start because of the SSL certs mount issue which I saw somewhere (I can't find the link right now. Anyways, one of the suggested solutions was to remove the ca-certs volumeMount from the container spec. After removing ca-certs from the list, the container successfully started.

Anyways, now the issue is that every time I add `--cloud-provider=external` to kubelet args, all the kube-system pods do not get IP addresses. I have added the args multiple ways: adding to /etc/sysconfig/kubelet and via the kubeadm init config file like this: 

```
apiVersion: kubeadm.k8s.io/v1alpha3
kind: ClusterConfiguration
kubernetesVersion: ""1.13.2""
apiServer:
  certSANs:
  - ""MASTER_API_URL""
  - ""IP""
controlPlaneEndpoint: ""MASTER_API_URL""
cloudProvider: external
networking:
  serviceSubnet: ""192.168.0.0/17""
  podSubnet: ""192.168.128.0/17""
apiServerExtraArgs:
  service-node-port-range: 80-32767
  enable-admission-plugins: NodeRestriction,Initializers
  feature-gates: Initializers=true
  runtime-config: admissionregistration.k8s.io/v1alpha1
controllerManagerExtraArgs:
  external-cloud-volume-plugin: openstack
nodeRegistration:
  kubeletExtraArgs:
    cloud-provider: external
```

**What you expected to happen**:
I would think the kube-system pods would still have their IPs assigned and listed in 

kubectl get all -n kube-system -o wide

**How to reproduce it (as minimally and precisely as possible)**:
Follow instructions at https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-controller-manager-with-kubeadm.md

**Anything else we need to know?**:
I am not sure what other information to provide

**Environment**:
- openstack-cloud-controller-manager version: docker.io/k8scloudprovider/openstack-cloud-controller-manager:latest
- OS (e.g. from /etc/os-release): CentOS Linux release 7.6.1810 (Core)
- Kernel (e.g. `uname -a`): Linux master001.domain 3.10.0-957.1.3.el7.x86_64 #1 SMP Thu Nov 29 14:49:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:  kubeadm init --config=/root/kube-adm-config.yaml
- Others:
",closed,False,2019-01-12 23:37:17,2019-01-13 05:01:03
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/429,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/429,Update Docs,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Documentation Update. And also replaces cloud.conf to $CLOUD_CONFIG to maintain consistency across docs. 

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-13 15:55:03,2019-01-16 10:08:30
cloud-provider-openstack,johscheuer,https://github.com/kubernetes/cloud-provider-openstack/pull/430,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/430,Openstack cloud-config as Kubernetes secret,"**What this PR does / why we need it**:

The PR changes the Volume type of the Openstack `cloud-config` used by the Openstack-ccm from `ConfigMap` to `Secret` since the configuration file contains sensible data. I know that secrets are not encrypted but at least they are more restrictive from an RBAC perspective.

**Special notes for your reviewer**:
I already use this ""patch"" in my repo: https://github.com/johscheuer/kubernetes-on-openstack/blob/master/scripts/master.cfg.tpl#L501

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
action required: The volume type for the `cloud-config` used by the `openstack-cloud-controller-mananager` has changed from `ConfigMap` to `Secret`. Ensure that the `Secret exists if you update your Kubernetes manifest.
```
",closed,True,2019-01-14 12:33:12,2019-02-11 12:46:32
cloud-provider-openstack,johscheuer,https://github.com/kubernetes/cloud-provider-openstack/issues/431,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/431,Use docker tags in git tags,"**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug

/kind feature

**What happened**:

Currently all manifest files use the `latest` tags (even on the branches and tags) e.g. https://github.com/kubernetes/cloud-provider-openstack/blob/1.13.1/manifests/controller-manager/openstack-cloud-controller-manager-ds.yaml#L39 which means as a user I get an undefined (and probably not working) state of the manifest file.

**What you expected to happen**:

All manifests should use the according docker tag for the git tags/releases otherwise the manifest in the tag are pretty useless (since flags can be changed)

**How to reproduce it (as minimally and precisely as possible)**:
-

**Anything else we need to know?**:

This should be integrated in the releases process.
",open,False,2019-01-14 13:21:38,2019-03-29 10:31:19
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/432,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/432,Add description for the LoadBalancer type service floating IP,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
In Magnum k8s cluster, after creating floating ip for LoadBalancer type service inside the cluster, it's impossible to delete the floating ip when the cluster is deleted.

**What you expected to happen**:
Add description for the fip similar to the lb description.

**How to reproduce it (as minimally and precisely as possible)**:
- create magnum k8s cluster
- create external service inside the cluster, a load balancer is created in octavia and a floating ip is created in neutron and associated with the load balancer vip.
- delete the cluster
- magnum can delete the load balancer automatically, but the floating ip is left.

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: master
- OS (e.g. from /etc/os-release): Ubuntu 16.04
- Kernel (e.g. `uname -a`): 4.4.0-139-generic
- Install tools: magnum
- Others:
",closed,False,2019-01-14 22:23:04,2019-01-15 21:07:59
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/433,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/433,Add description for the floating ip created for external service,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
fixes #432

**Special notes for your reviewer**:
When creating floating ip for external service, add the cluster and service information in the floating ip description field, so it's easy to query and delete outside of the cluster.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-14 22:25:12,2019-01-16 08:58:23
cloud-provider-openstack,johscheuer,https://github.com/kubernetes/cloud-provider-openstack/pull/434,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/434,Make usage of CCM cloud-config secret in CSI examples,"**What this PR does / why we need it**:

Use the same secret like the `cloud-controller-maanger` does to reduce the management overhead of Openstack secrets.

**Special notes for your reviewer**:

Follow up of: https://github.com/kubernetes/cloud-provider-openstack/pull/430

",closed,True,2019-01-15 10:07:45,2019-02-04 21:55:35
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/issues/435,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/435,"Unable to schedule pods on the host, failing with error Error fetching by NodeName: failed to find object","<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
Pods are unable to get schedules on host , status is always in Pending state
NAME                             READY   STATUS    RESTARTS   AGE
csi-attacher-cinderplugin-0      0/2     Pending   0          19m
csi-provisioner-cinderplugin-0   0/2     Pending   0          19m
ccm log shows This node 127.0.0.1 is still tainted. Will not process.
So initialization of the node got failed with error
I0111 04:33:20.444205   27150 openstack_instances.go:80] NodeAddresses(127.0.0.1) called
E0111 04:33:20.803434   27150 node_controller.go:419] NodeAddress: Error fetching by providerID: ProviderID ""testvm"" didn't match expected format ""openstack:///InstanceID"" Error fetching by NodeName: failed to find object
I0111 04:33:23.070378   27150 openstack_instances.go:46] openstack.Instances() called
**What you expected to happen**:
Pods to get scheduled and to be in Running state
**How to reproduce it (as minimally and precisely as possible)**:
Follow steps in 
https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/getting-started-provider-dev.md
https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-cinder-csi-plugin.md

**Anything else we need to know?**:
The Error got fixed by setting environment variable export HOSTNAME_OVERRIDE=$(hostname) , before starting cluster. Doc needs to be updated for the same.

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2019-01-16 08:11:46,2019-01-17 07:38:11
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/436,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/436,Update getting-started-provider-dev.md,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
updates the doc to set export HOSTNAME_OVERRIDE=$(hostname) before cluster is up, to avoid further errors while scheduling of pods.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #435 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-16 08:16:44,2019-01-16 09:19:52
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/issues/437,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/437,"defaultMetadataVersion used is quite old, update to latest version","<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
defaultMetadataVersion used in code is quite old, need to use the latest version to be in sync with latest developements in metadata service.
https://github.com/kubernetes/cloud-provider-openstack/blob/master/pkg/csi/cinder/openstack/openstack_metadata.go#L11

**What you expected to happen**:
use latest version instead
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-01-16 09:57:22,2019-02-12 04:45:07
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/438,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/438,Improve the cfg options for octavia ingress controller,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Add/change some options for octavia ingress controller configuration, make most of the options consistent with cloud-config, so it's easy to config octavia-ingress-controller together with openstack-cloud-controller-manager.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-17 02:55:54,2019-01-22 00:48:22
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/439,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/439,Add barbican-kms-plugin to gitignore,"make build generates barbican-kms-plugin binary , which comes
under untracked files. Make git to ignore the same.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-17 07:31:53,2019-01-17 12:42:00
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/440,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/440,add a log when ccm start failed,"has following error when start ccm after a unexpected update in
configuration of openstack config file

error building controller context: cloud provider could not be initialized:
could not init cloud provider ""openstack"": Resource not found

'Resource not found' is not really helpful here..



**What this PR does / why we need it**:
helpful for wrong conf debug
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
```release-note
```
",closed,True,2019-01-17 09:42:09,2019-01-21 18:26:52
cloud-provider-openstack,JoeWrightss,https://github.com/kubernetes/cloud-provider-openstack/pull/441,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/441,Fix t.Errorf() error message,"1. ""symblolic"" to ""symbolic"" in line 187.
2. ""volNamne"" to ""volName"" in line 143.",closed,True,2019-01-17 11:58:03,2019-01-21 15:52:54
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/442,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/442,octavia-ingress-controller: Add ingress information to openstack resources description,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
This patch adds the ingress information, e.g. namespace, name and
cluster name into openstack resource description, in order to identify
the corresponding openstack resources created for the ingress and make
it easy to do clean up.

**Which issue this PR fixes** : fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-18 01:13:59,2019-01-21 04:11:45
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/pull/443,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/443,New options validator for manila-provisioner,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Revamp of the previous version of the options validator: cleaner code, easier testing, added a few internal features - this will make https://github.com/kubernetes/cloud-provider-openstack/issues/420 a lot easier to implement with proper input validation

Makes following code no longer necessary as we'll get valid input every time:
https://github.com/kubernetes/cloud-provider-openstack/blob/a63c14dd484381995562dc8c02861b5c557e60db/pkg/share/manila/sharebackends/csicephfs.go#L41-L43

Fixes a bug described here https://github.com/kubernetes/cloud-provider-openstack/pull/370#discussion_r236616224

https://github.com/kubernetes/cloud-provider-openstack/commit/9c066a03ec19d64db7c29729cfc7dc0aa3bc90f8 : unrelated, but the newer version of `external-storage` lib in this repo breaks the deployment because leader election requires access to `endpoints` - this commit fixes it. Hopefully it can be squeezed into this PR.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
manila-provisioner: shareSecretNamespace option now defaults to ""default"" (as opposed to coalescing with osSecretNamespace)
```
",closed,True,2019-01-18 21:57:20,2019-02-19 14:58:03
cloud-provider-openstack,mogaika,https://github.com/kubernetes/cloud-provider-openstack/pull/444,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/444,Add internal-network-name networking option,"**What this PR does / why we need it**:
This will help in case of multi-nic k8s node deployments.
Previously, cloud provider was assigning all addresses in random
order and k8s was selecting only one of them. But usually,
multi-nic scenario requires to specify which network is
""control"" and admins want to bind kubelet listening address only
to that ""control"" net.

This commit will not affect previous logic until
internal-network-name is specified in cloud-config file.

**Which issue this PR fixes**: fixes #407 

**Special notes for your reviewer**:

**Release note**:
```release-note
```
",closed,True,2019-01-19 18:49:10,2019-01-21 20:30:38
cloud-provider-openstack,mogaika,https://github.com/kubernetes/cloud-provider-openstack/issues/445,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/445,"Confusing ""Please use 'external' cloud provider for openstack"" warning","/kind bug

**What happened**:
Log says:
```W0119 22:23:10.778211   10831 plugins.go:118] WARNING: openstack built-in cloud provider is now deprecated. Please use 'external' cloud provider for openstack: https://github.com/kubernetes/cloud-provider-openstack```
what can make administrators confused and think that cloud provider is misconfigured

**Anything else we need to know?**:
cmd that we are using:
`/usr/bin/openstack-cloud-controller-manager --cloud-provider=openstack --cloud-config /etc/kubernetes/cloud-config --cluster-name=kubernetes --kubeconfig /etc/kubernetes/controller-manager.kubeconfig --leader-elect=true --v=4`
",closed,False,2019-01-19 22:30:33,2019-01-24 03:24:22
cloud-provider-openstack,hemantsonawane95,https://github.com/kubernetes/cloud-provider-openstack/issues/446,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/446,openstack-ingress-controller failed ,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:
Failed to create openstack resources for ingress default/test-octavia-ingress: error because no members in pool
**What you expected to happen**:
openstack-ingress-controller should work 
**How to reproduce it (as minimally and precisely as possible)**:

can be fixed by adding worker node to the cluster.

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release): Ubuntu 16.04
- Kernel (e.g. `uname -a`): 4.4.0-134-generic
- Install tools: kubeadm 
- Others: Cloud-provider= Openstack 
",closed,False,2019-01-21 09:14:13,2019-03-22 05:47:10
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/447,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/447,octavia-ingress-controller: Doc improvement,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Improve the octavia-ingress-controller documentation.

**Which issue this PR fixes**: 
fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-21 22:42:04,2019-01-22 00:48:07
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/448,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/448,Update defaultMetadataVersion to latest,"Currently defaultMetadataVersion is set to 2012-08-10 which is
very old. This commit updates the same.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-23 08:12:04,2019-02-15 08:57:57
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/issues/449,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/449,add proxy protocol support for openstack lbaas octavia,"Is this a BUG REPORT or FEATURE REQUEST?: FEATURE
/kind feature

What happened: new lbaas octavia in openstack supports proxy protocol. Currently at least AWS supports elb with proxy protocol https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/aws/aws.go#L107 I am kind of asking to add similar feature to openstack lbaas to enable proxy protocol support in kubernetes as well.

This support should be added first to gophercloud as well

/sig openstack",closed,False,2019-01-24 11:29:27,2019-02-07 10:11:03
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/450,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/450,cinder-csi-plugin: Volume does not contain any information of the cluster and pv,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
The volumes created by cinder-csi-plugin doesn't have any information e.g. the cluster name, the reclaim policy, etc. which are important for the resource management of cloud provider.

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-01-25 02:33:50,2019-02-11 12:38:44
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/451,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/451,cinder-csi-plugin: Add cluster argument to the driver,"**What this PR does / why we need it**:
Add `cluster` parameter for the driver, admin user could specify the CO's identifier, e.g.  in Magnum, the `cluster` could be the magnum cluster id.

When using cinder-csi-plugin together with kubernetes in the cloud environment, we need some information to recognize the volumes created for the PVs from different kubernetes clusters, which is important for the cloud resource management.

This PR is only adding cluster name to the driver, so that the driver could pass the cluster name to the cinder volume metadata.

**Which issue this PR fixes** : fixes #450

**Special notes for your reviewer**:
This PR is only adding cluster name to the driver, so that the driver could pass the cluster name to the cinder volume metadata.

There are several ways to inject the cluster name to the cinder volume property, e.g. kubernetes admin can provide parameters when creating the StorageClass, the cloud admin could config `volume-name-prefix` to include the cluster as part of cinder volume name, etc. However, in order to support a more flexible and extensible way, I prefer to add all those information to the cinder volume medata.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
cinder-csi-driver supports `--cluster` argument to get the the CO's identifier.
```
",closed,True,2019-01-25 03:21:01,2019-02-17 22:51:34
cloud-provider-openstack,imdigitaljim,https://github.com/kubernetes/cloud-provider-openstack/pull/452,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/452,Update OWNERS,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-25 23:26:37,2019-01-25 23:27:41
cloud-provider-openstack,tsmetana,https://github.com/kubernetes/cloud-provider-openstack/issues/453,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/453,[RFE] Manila: add snapshot support,"/kind feature

The Manila provisioner does not offer an API to take snapshot of the volume. This is quilte an essential feature of the storage subsystem.

Please implement API that would allow to take a snapshot of a Manila provisioned volume and enable to create a new volume from the snapshot. There is no standard defined in the core Kubernetes API however the [CSI standard](https://github.com/container-storage-interface/spec/blob/master/spec.md) specifies API for volume snapshots.

Depending on the status of the issue #394 it might make sense to wait for the Manila CSI to land.

I'm opening this ticket because I would like to propose the implementation of this feature as a GSoC project.
",open,False,2019-01-28 09:13:10,2019-01-28 09:13:11
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/issues/454,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/454,Use sig-storage-lib-external-provisioner instead of kubernetes-incubator/external-storage,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
kubernetes-incubator/external-storage lib is deprecated. The library has moved to kubernetes-sigs/sig-storage-lib-external-provisioner . Code needs to be updated accordingly
**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-01-28 10:50:00,2019-02-12 20:53:31
cloud-provider-openstack,johscheuer,https://github.com/kubernetes/cloud-provider-openstack/issues/455,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/455,OpenStack CSI Cinder acceptance tests are failing,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

Currently all runs of `openstack-acceptance-test-csi-cinder` will fail see: https://github.com/kubernetes/cloud-provider-openstack/pull/430#issuecomment-458104445 and https://github.com/theopenlab/openlab-zuul-jobs/pull/420

Another issue is that the images are created inconsistently e.g. https://github.com/kubernetes-csi/external-attacher creates `csi-attacher:latest` but https://github.com/kubernetes-csi/external-provisioner creates `quay.io/k8scsi/csi-provisioner/canary`.

Probably a good idea is to use a pinned version of the CSI tooling to prevent such errors.

**What you expected to happen**:
That the e2e test fro csi passes.

**How to reproduce it (as minimally and precisely as possible)**:
Rerun an e2e csi test

**Anything else we need to know?**:

**Environment**:
-
",closed,False,2019-01-28 12:43:45,2019-01-29 17:26:24
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/456,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/456,Use code from k8s.io/utils repo,"Change-Id: I8423a7919a3e30274fc6e4cc4a165d584c27a8c3

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
We should not have copy of the code, when possible use to the external library

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-01-28 20:02:07,2019-02-12 20:56:20
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/457,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/457,Add support for topology aware dynamic volume provisioning,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
This PR implements following:

1. update metadata struct to return Availability zone information.
2. update NodeGetInfo function to return topology information eg. accessible_topology =
{""topology.cinder.csi.openstack.org/zone"": ""nova""}
2. consumes topology requirement if present in CreateVolume request and creates
volume using the given zone while favoring preferred topologies

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: 
fixes #301 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-29 07:16:29,2019-02-15 11:32:26
cloud-provider-openstack,jim-bach,https://github.com/kubernetes/cloud-provider-openstack/pull/458,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/458,test,"test

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-29 18:18:14,2019-01-29 18:21:08
cloud-provider-openstack,jim-bach,https://github.com/kubernetes/cloud-provider-openstack/pull/459,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/459,test ignore,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-29 18:22:00,2019-01-29 18:24:40
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/460,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/460,octavia-ingress-controller: Incorrect description for load balancer vip floating IP ,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**: 
/kind bug

**What happened**:
After creating an ingress named `test-octavia-ingress` in `default` namespace, the load balancer vip floating ip description shows:
`Floating IP for Kubernetes ingress default in namespace test-octavia-ingress from cluster 5be2a4b2-a10a-4991-b9a7-db3e9cb79d07`

**What you expected to happen**:
Should be ``Floating IP for Kubernetes ingress test-octavia-ingress in namespace default from cluster 5be2a4b2-a10a-4991-b9a7-db3e9cb79d07``

",closed,False,2019-01-31 02:31:30,2019-02-05 11:04:58
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/461,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/461,octavia-ingress-controller: Fix the floating ip description,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Fix the the load balancer vip floating ip description, mainly changed EnsureFloatingIP method parameters order.

**Which issue this PR fixes** : 
fixes #460

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-31 02:32:52,2019-02-05 11:15:25
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/462,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/462,"Fix Snapshot Feature, and Add volume restore from snapshot","<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
While Testing the snapshot feature, VolumeSnapshot does not bind VolumeSnapshotContent, as we are not returning snapshot status from driver, this PR fix this and also adds support for volume restore from snapshot

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
`NONE`",closed,True,2019-01-31 08:10:42,2019-02-19 23:14:13
cloud-provider-openstack,jim-bach,https://github.com/kubernetes/cloud-provider-openstack/pull/463,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/463,test,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-31 23:17:50,2019-01-31 23:20:03
cloud-provider-openstack,cbrumm-blz,https://github.com/kubernetes/cloud-provider-openstack/pull/464,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/464,Update README.md,"JUST TESTING CNCF CLA

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-01-31 23:27:05,2019-01-31 23:27:54
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/465,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/465,Add Snapshot Deployment yamls,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Add Snapshot Deployment yamls

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #424

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
`NONE`
",closed,True,2019-02-01 08:13:16,2019-02-18 05:27:07
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/pull/466,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/466,Implement proxy protocol support for lbaas v2,"**What this PR does / why we need it**: we need implement proxy protocol support. Also this old `extensions/lbaas_v2` is [deprecated](https://github.com/gophercloud/gophercloud/pull/1323#issuecomment-438342506)

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #449 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
openstack loadbalancer now supports PROXY protocol
```

Example:
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-development
spec:
  replicas: 1
  template:
    metadata:
      labels:
        run: web
    spec:
      containers:
      - name: nginx
        image: jesseh/nginx:1.13
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  labels:
    run: web-ext
  name: web-ext
spec:
  selector:
    run: web
  type: LoadBalancer
  ports:
  - port: 80
    name: http
    protocol: TCP
    targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    loadbalancer.openstack.org/proxy-protocol: ""true""
  labels:
    run: web-ext-proxy
  name: web-ext-proxy
spec:
  selector:
    run: web
  type: LoadBalancer
  ports:
  - port: 80
    name: http
    protocol: TCP
    targetPort: 80
```",closed,True,2019-02-03 08:46:34,2019-02-07 10:11:03
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/pull/467,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/467,fix kubernetes hostnames if those contain dot,"
**What this PR does / why we need it**: https://serverfault.com/questions/229331/can-i-have-dots-in-a-hostname the recommendation is that we should not use dots in linux hostnames. If we do not have dots in linux hostnames, we should not have those either in kubernetes nodenames because then `nodename` does not match to `hostname`. This makes huge issues if we are using external cloudprovider plugin and when bootstrapping it.


issue in kops https://github.com/kubernetes/kops/issues/6441",closed,True,2019-02-03 20:39:13,2019-02-05 15:50:45
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/468,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/468,Update to use kubernetes-sigs/sig-storage-lib-external-provisioner,"This commit is to update code to use
kubernetes-sigs/sig-storage-lib-external-provisioner instead of
kubernetes-incubator/external-storage.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #454

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-04 05:21:56,2019-02-15 08:57:02
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/469,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/469,Run CSI Sanity Tests against cinder CSI driver,"<!-- This form is for bug reports and feature requests! -->
**What happened**:
We should run CSI Sanity tests against cinder csi driver
https://github.com/kubernetes-csi/csi-test




",open,False,2019-02-04 06:04:57,2019-02-26 06:55:36
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/470,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/470,Update Cinder CSI Driver to CSI Spec 1.0.0,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Update cinder csi driver to csi specs 1.0.0

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #365

**Special notes for your reviewer**:
CSI v0.3.0 support is deprecated with kubernetes 1.13 and will be removed in v1.15

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
`NONE`
",closed,True,2019-02-04 09:44:34,2019-02-12 20:45:57
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/471,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/471,Add lingxiankong to the OWNERS,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Add myself to the cloud-provider-openstack owners. I've been contributing the repo for almost half a year, the motivation of my contribution comes from the adoption of CPO(as part of Kubernetes as a Service) in our public cloud. I'm also the author of octavia-ingress-controller and maintainer of k8s-keystone-auth service. Being one of the owners doesn't mean more power but more responsibilities.

**Which issue this PR fixes**: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-04 10:45:01,2019-02-17 22:51:32
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/issues/472,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/472,proper error codes should be returned in complaince with CSI spec,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature
Currently , cinder csi driver doesn't return status error codes in conformance to CSI Spec . https://github.com/container-storage-interface/spec/blob/master/spec.md

**What happened**:

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2019-02-06 05:30:02,2019-02-06 05:30:18
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/issues/473,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/473,Add node stage/unstage capability to nodeserver ,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
> /kind bug
/kind feature


**What happened**:
Currently node stage and unstage capability is not implemented in node server.
**What you expected to happen**:
needs to be added 
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-02-07 08:21:56,2019-02-15 09:19:33
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/474,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/474,Add STAGE_UNSTAGE_VOLUME node capability,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
This PR adds NodeStageVolume and NodeUnstageVolume capability to nodeserver.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #301 #473 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-07 11:18:21,2019-02-19 21:44:18
cloud-provider-openstack,johscheuer,https://github.com/kubernetes/cloud-provider-openstack/pull/475,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/475,Fix error output in client-keystone-auth,"**What this PR does / why we need it**:

This PR fixes the error output in `client-keystone-auth`. Before the patch:

```bash
Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]Unable to connect to the server: getting credentials: exec: exit status 1
```

after the patch:

```bash
Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]
Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]
Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]
Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]
Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]
Unable to connect to the server: getting credentials: exec: exit status 1
```

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-08 15:01:00,2019-02-10 23:53:31
cloud-provider-openstack,johscheuer,https://github.com/kubernetes/cloud-provider-openstack/issues/476,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/476,client-keystone-auth: pipe and arguments doesn't work,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug


**What happened**:

I use the `client-keystone-auth` with only arguments passed (see the following snippet):

```yaml
- name: redacted
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --domain-name=redacted
      - --keystone-url=redacted
      - --project-name=redacted
      - --user-name=redacted
      - --password=redacted
      command: ./bin/client-keystone-auth
      env: null
```

everything works as expected when using `kubectl` without a pipe otherwise I get the following error message:

```bash
Failed to read openstack env vars: Missing environment variable [OS_AUTH_URL]
```

**What you expected to happen**:

I expected that kubectl authenticate against Keystone and passes the output to the next command.

**How to reproduce it (as minimally and precisely as possible)**:

Set only arguments in the kubeconfig and run a command like:  `kubectl get po --all-namespaces | grep 'a'`

**Anything else we need to know?**:

I already found the error: This command checks if STDIN is a terminal https://github.com/kubernetes/cloud-provider-openstack/blob/master/cmd/client-keystone-auth/main.go#L166 which returns in the case of `kubectl ...` `false` but in the case of `kubectl ... | do stuff` `true`. In the next step we jump in the [openstack.AuthOptionsFromEnv()](https://github.com/kubernetes/cloud-provider-openstack/blob/master/cmd/client-keystone-auth/main.go#L167) method which ignores the arguments.

There are two possible options to tackle this problem:

1.) Implement a fallback (if env variables are not set) with the specified arguments
2.) Set the required Env Variables for the auth method and unset them afterwards

I also would expect that an argument overwrites the value of an env variable (like it's coded).

**Environment**:
- openstack-cloud-controller-manager version: 1.13.1
- OS (e.g. from /etc/os-release): darwin / linux
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-02-08 17:25:48,2019-02-25 15:11:38
cloud-provider-openstack,johscheuer,https://github.com/kubernetes/cloud-provider-openstack/pull/477,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/477,Implement arguments fallback for client-keystone-auth,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

Without this PR it's not possible to only use arguments in the `client-keystone-auth` and a pipe.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
Fixes: https://github.com/kubernetes/cloud-provider-openstack/issues/476
**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
client-keystone-auth: change the default behavior to use cli arguments if set otherwise use env variables for authentication
```
",closed,True,2019-02-08 17:32:37,2019-02-25 15:11:39
cloud-provider-openstack,XiaYinchang,https://github.com/kubernetes/cloud-provider-openstack/pull/478,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/478,Fix get SyncConfigMapName in config.go bug,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

This PR fix an obvious bug in keystone config.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

when set parameter like this:
`  - ./bin/k8s-keystone-auth
            - --v=10
            - --tls-cert-file
            - /etc/kubernetes/pki/cert-file
            - --tls-private-key-file
            - /etc/kubernetes/pki/key-file
            - --sync-configmap-name
            - k8s-sync-config`

I got eorrors like this:
`I0211 06:17:47.079308       1 keystone.go:544] Kubernetes API client created, server version v1.13`
`E0211 06:17:47.079361       1 sync.go:126] yamlFile get err   #open k8s-sync-config: no such file or directory`
`E0211 06:17:47.079384       1 main.go:60] failed to extract data from sync config file k8s-sync-config: open k8s-sync-config: no such file or directory
`
**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-11 06:46:16,2019-02-19 05:09:34
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/issues/479,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/479,"CSI cinder only use metadata ,not consider config drive","<!-- This form is for bug reports and feature requests! -->
pkg/csi/cinder/openstack/openstack_metadata.go only consider metadata service ,not config drive now

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug
> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:

**What you expected to happen**:
same flow to cloud proivder and CSI
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2019-02-12 02:54:59,2019-02-12 02:55:21
cloud-provider-openstack,dims,https://github.com/kubernetes/cloud-provider-openstack/pull/480,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/480,Switch to Kubernetes v1.14.0-beta.1 and fix stale imports,"Change-Id: Ief4b32f526ad49b2dfe588e2d2eec32f015cb4af

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-02-12 22:46:05,2019-03-08 14:43:28
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/481,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/481,mv metadata to util,"metadata is widely used inside this project such as cloudprovider
and later on CSI or others, so move it to a common base will
be helpful to follow up usage

**What this PR does / why we need it**:
move metadata to util then we can reuse it later
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

```release-note
None
```
",closed,True,2019-02-13 08:17:53,2019-02-18 01:57:34
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/pull/482,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/482,support multiple subnets in lbaas,"**What this PR does / why we need it**: When creating loadbalancer(Lbaas v2) resources to network which do have more than one external subnets attached, we need somekind of way to define from which external floatingip subnet we want the ip address coming from.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
```release-note
```

Example:

```
openstack network list
+--------------------------------------+---------------------+------------------------------------------------------------------------------------------------------------------+
| ID                                   | Name                | Subnets                                                                                                          |
+--------------------------------------+---------------------+------------------------------------------------------------------------------------------------------------------+
| 172f8cef-495e-40ed-9a11-aa900986578a | public              | 8038a37e-e19a-49b8-ab99-5b4f2692649a, bdc08524-bf85-447a-9db2-97e70ffb13bf, e20758b9-e063-40ae-a73f-32297420b309 |

openstack subnet list|grep 172f8cef-495e-40ed-9a11-aa900986578a
| 8038a37e-e19a-49b8-ab99-5b4f2692649a | public-subnet                    | 172f8cef-495e-40ed-9a11-aa900986578a | 192.168.1.128/26    |
| bdc08524-bf85-447a-9db2-97e70ffb13bf | ipv6-public-subnet               | 172f8cef-495e-40ed-9a11-aa900986578a | snip       |
| e20758b9-e063-40ae-a73f-32297420b309 | public-subnet-ext                | 172f8cef-495e-40ed-9a11-aa900986578a | 10.128.0.1/24    |
```

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-development
spec:
  replicas: 1
  template:
    metadata:
      labels:
        run: web
    spec:
      containers:
      - name: nginx
        image: jesseh/nginx:1.13
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    loadbalancer.openstack.org/floating-subnet: public-subnet
  labels:
    run: web1
  name: web1
spec:
  selector:
    run: web
  type: LoadBalancer
  ports:
  - port: 80
    name: http
    protocol: TCP
    targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    loadbalancer.openstack.org/floating-subnet: public-subnet-ext
  labels:
    run: web2
  name: web2
spec:
  selector:
    run: web
  type: LoadBalancer
  ports:
  - port: 80
    name: http
    protocol: TCP
    targetPort: 80
---
---
apiVersion: v1
kind: Service
metadata:
  labels:
    run: web3
  name: web3
spec:
  selector:
    run: web
  type: LoadBalancer
  ports:
  - port: 80
    name: http
    protocol: TCP
    targetPort: 80
```

the result:
```
kubectl get svc -o wide
NAME         TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE       SELECTOR
kubernetes   ClusterIP      100.64.0.1       <none>          443/TCP        11m       <none>
web1         LoadBalancer   100.66.201.184   192.168.1.164   80:30071/TCP   6m        run=web
web2         LoadBalancer   100.64.94.144    10.128.0.5      80:31857/TCP   6m        run=web
web3         LoadBalancer   100.70.25.157    192.168.1.145   80:30040/TCP   6m        run=web
```

If external network do have multiple subnets, the behaviour usually is that openstack will try to take ip address from the first one. If the first one is full, then it will use the second pool. So if annotation is not defined the behaviour is like that (case web3). 
",closed,True,2019-02-17 21:29:20,2019-02-19 21:21:20
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/issues/483,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/483,Cinder CSI should add error probe,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature
> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:
Current CSI plugin always assume cinder is not healthy, we need check it
**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-02-18 03:14:42,2019-03-13 05:09:34
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/issues/484,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/484,Add cluster info into GetPluginInfoResponse manifest output,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature
> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:
we return Name and VendorVersion into GetPluginInfo,
add cluster info would be better 

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-02-18 03:48:32,2019-02-27 05:43:12
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/485,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/485,Add cluster info into GetPluginInfo,"GetPluginInfo can container cluster name for reference
so this patch added the cluster.



**What this PR does / why we need it**:
add cluster info into GetPluginInfo CSI call 
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
fixes #484 
**Special notes for your reviewer**:

**Release note**:
```release-note
Added the cluster info into CSI call GetPluginInfo 
```
",closed,True,2019-02-18 04:36:56,2019-02-19 03:25:36
cloud-provider-openstack,darcyllingyan,https://github.com/kubernetes/cloud-provider-openstack/issues/486,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/486,Question: Deploy the csi plugin for cinder in Kubernetes,"Hi,
I'm new to the Cinder CSI, now I just want to integrate the Cinder CSI to my kubernetes cluster.
I have several questions:

1) If I just want to deploy the cinder CSI, whether it's OK for me to download the pkg/csi/cinder only?
https://github.com/kubernetes/cloud-provider-openstack/tree/master/pkg/csi/cinder
2) If no, whether there is better way fro me to only deploy it?
3) If yes, The pkg/csi/cinder only implements the CSI driver and I need to deploy the CSI sidercars seperately, right?
3) If yes, how to install the Cinder CSI, as the package doesn't have appropriate docs.

Thanks 
 ",closed,False,2019-02-18 05:29:11,2019-02-22 07:35:10
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/487,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/487,Use metadata in CSI,"use util/metadata in CSI 

**What this PR does / why we need it**:
CSI need use util/metadata directly, instead of using its own

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",open,True,2019-02-18 07:50:52,2019-03-25 03:48:24
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/488,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/488,WIP: test gate failure,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-19 06:18:07,2019-02-19 06:49:59
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/489,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/489,WIP: test gate failure,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-19 08:37:56,2019-02-19 11:15:12
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/490,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/490,Security group doesn't work when using Octavia,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
With the configuration:
```
[LoadBalancer]
use-octavia=yes
subnet-id=7e5398da-a77a-4d5a-a2ae-f1273132f869
manage-security-groups=true
```
The service can't be accessed as expected, the security group on the worker node is not correctly configured.

**What you expected to happen**:
The service could be accessed as expected.

**How to reproduce it (as minimally and precisely as possible)**:
- CPO is running with the above configuration
- Create a LoadBalancer type service
- Access the service using external IP address.

**Anything else we need to know?**:
When using Octavia, the security group rule should allow traffic coming from Octavia amphorae.

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-02-19 10:51:16,2019-02-20 21:59:38
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/491,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/491,Improve security group management for service provision,"**What this PR does / why we need it**:
This PR focuses on the scenario of using Octavia, it does several things:
- When creating service, create a new security group and add rules to allow traffic coming from the load balancer subnet IP address range while going to the service nodeport on the nodes. Associate the security group with all the Neutron ports on the nodes, add a tag to the ports.
- When deleting service, disassociate the security group from the Neutron ports on the nodes before removing.

**Which issue this PR fixes**: 
fixes #490

**Special notes for your reviewer**:
Test process for thie PR:
1. Using image `lingxiankong/openstack-cloud-controller-manager:improve-sg`
1. Configuration
    ```
    [Global]
    auth-url=http://192.168.206.8/identity
    user-id=94a665016c3e4d44a315b14d92f9ef49
    password=password
    tenant-id=cd08a539b7c845ddb92c5d08752101d1
    region=RegionOne
    [LoadBalancer]
    use-octavia=yes
    subnet-id=7e5398da-a77a-4d5a-a2ae-f1273132f869
    create-monitor=no
    manage-security-groups=true
    [BlockStorage]
    bs-version=v2
    ```
1. The security group of the Neutron ports on the nodes only allows the basic traffic within the kubernetes cluster such as 10250. The default port range for service ports is not exposed. 
1. Create a service of LoadBalancer type.
    ```
    $ kubectl run hostname-server --image=lingxiankong/alpine-test --port=8080
    $ kubectl expose deployment hostname-server --type=LoadBalancer --target-port=8080 --port=80 --name hostname-server
    $ kubectl get svc
    NAME              TYPE           CLUSTER-IP   EXTERNAL-IP    PORT(S)        AGE
    hostname-server   LoadBalancer   10.96.74.3   172.24.4.243   80:32760/TCP   44s
    ```
1. A load balancer is created and behaves as expected.
    ```
    $ openstack loadbalancer list
    +--------------------------------------+----------------------------------+----------------------------------+-------------+---------------------+----------+
    | id                                   | name                             | project_id                       | vip_address | provisioning_status | provider |
    +--------------------------------------+----------------------------------+----------------------------------+-------------+---------------------+----------+
    | 5ebd3ef7-8069-4735-bc73-b08f33f715e3 | a8c8af25d342711e98f68fa163e8309a | cd08a539b7c845ddb92c5d08752101d1 | 10.0.0.14   | ACTIVE              | amphora  |
    +--------------------------------------+----------------------------------+----------------------------------+-------------+---------------------+----------+
    $ curl 172.24.4.243
    hostname-server-7857c9b75b-b85mx
    ```
1. Check the security groups rules on the worker node Neutron port.
    ```
    $ openstack port list --device-id 3515c2d9-ce37-439e-b3cb-cc759cac444e
    +--------------------------------------+--------------------+-------------------+--------------------------------------------------------------------------+--------+
    | ID                                   | Name               | MAC Address       | Fixed IP Addresses                                                       | Status |
    +--------------------------------------+--------------------+-------------------+--------------------------------------------------------------------------+--------+
    | 87d41ec1-ba9a-44c1-a371-d833d8a4bf94 | lingxian-k8s-node1 | fa:16:3e:19:a1:60 | ip_address='10.0.0.22', subnet_id='7e5398da-a77a-4d5a-a2ae-f1273132f869' | ACTIVE |
    +--------------------------------------+--------------------+-------------------+--------------------------------------------------------------------------+--------+
    $ openstack port show 87d41ec1-ba9a-44c1-a371-d833d8a4bf94 | grep security
    | port_security_enabled   | True                                                                       |
    | security_group_ids      | bf910fe6-e236-49f3-8eb5-1edc263aa02e, c63a2657-300b-4ab0-9201-65b49e3ba815 |
    $ openstack security group rule list bf910fe6-e236-49f3-8eb5-1edc263aa02e
    +--------------------------------------+-------------+-------------+-------------+-----------------------+
    | ID                                   | IP Protocol | IP Range    | Port Range  | Remote Security Group |
    +--------------------------------------+-------------+-------------+-------------+-----------------------+
    | 4120d44e-cb9b-4ac5-a0eb-15b1ac67f01c | tcp         | 10.0.0.0/26 | 32760:32760 | None                  |
    | 9dfee722-95e8-4b20-a58a-6db7f26eaf7b | None        | None        |             | None                  |
    | e7ec4602-21d4-4d9b-9f12-01f4dd360aa8 | None        | None        |             | None                  |
    +--------------------------------------+-------------+-------------+-------------+-----------------------+
    $ openstack port show 87d41ec1-ba9a-44c1-a371-d833d8a4bf94 | grep tag
    | tags                    | bf910fe6-e236-49f3-8eb5-1edc263aa02e
    ```
1. Delete the service and check again. Make sure the security group has been deleted and the port tag has been removed.
    ```
    $ openstack port show 87d41ec1-ba9a-44c1-a371-d833d8a4bf94 | grep tag
    | tags                    |                                                                          |
    ```

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-19 11:12:45,2019-03-04 10:46:46
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/pull/492,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/492,manila: added osShareNetworkID option,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

Adds an optional StorageClass field `osShareNetworkID`

**Which issue this PR fixes**
fixes #420

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
manila-provisioner: added a new StorageClass option osShareNetworkID
```
",closed,True,2019-02-19 16:10:40,2019-02-19 22:47:31
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/493,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/493,merge isNotFound inside cloud provider,"a set of isNotFound function exist in the pkg folder
This PR combine them and remove duplidated function defintion


**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:


```release-note
```
",closed,True,2019-02-20 05:26:11,2019-02-25 05:36:32
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/494,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/494,Add document section about csi version compatibility,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Various user facing issues related to compatibility while deploying cinder csi driver 
#486, https://github.com/kubernetes/kubernetes/issues/74162
This PR adds compatibilty tables between cinder csi driver, sidecar and k8s

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
`NONE`
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-20 09:40:59,2019-03-19 09:22:34
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/issues/495,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/495,example cloud-controller manifests are missing RBAC rules,"
**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**: I deployed ccm using https://github.com/kubernetes/cloud-provider-openstack/blob/master/manifests/controller-manager/openstack-cloud-controller-manager-ds.yaml However, that file is missing all of the clusterrole rbac rules. There are quite many things that needs to be added and in my opinion it is not perfect way to just give cluster-admin for ccm.

**What you expected to happen**: I except that we could have example what kind of rules is needed.

**How to reproduce it (as minimally and precisely as possible)**: Deploy example file


",closed,False,2019-02-20 19:26:12,2019-02-20 22:03:32
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/496,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/496,[octavia-ingress-controller] Support internal/external Ingress,"**What this PR does / why we need it**:
Allow the k8s user choose to create internal or external Ingress by speficying an annotation `octavia.ingress.kubernetes.io/internal`, if it's true, the load balancer created in Octavia won't have floating IP associated. The default value is true.

**Which issue this PR fixes**:

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-20 22:52:52,2019-02-22 11:57:28
cloud-provider-openstack,jianglingxia,https://github.com/kubernetes/cloud-provider-openstack/issues/497,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/497,csi cinder plugin can not dynamic create pv ,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:
the attacher provisioner and node-plugin pod running then create the storageclass and pvc but the pv can not dynamic create 
the pod running :
[root@host-101-101-101-134 csi]# kubectl get po --all-namespaces -w                                                       
NAMESPACE     NAME                                READY     STATUS    RESTARTS   AGE
kube-system   csi-attacher-cinderplugin-0         2/2       Running   0          18h
kube-system   csi-nodeplugin-cinderplugin-6l6mw   2/2       Running   1          18h
kube-system   csi-provisioner-cinderplugin-0      2/2       Running   0          2h
kube-system   kube-flannel-wd54d                  2/2       Running   1          1d

the storageclass and pvc use the web yaml:
https://github.com/kubernetes/cloud-provider-openstack/blob/master/manifests/cinder-csi-plugin/csi-provisioner-cinderplugin.yaml

and the pvc logs:
Events:
  Type     Reason                Age                From                                                                                  Message
  ----     ------                ----               ----                                                                                  -------
  Normal   Provisioning          6m (x11 over 1h)   csi-cinderplugin_csi-provisioner-cinderplugin-0_acde092b-3576-11e9-8591-0a58ac10000a  External provisioner is provisioning volume for claim ""default/csi-pvc-cinderplugin""
  Warning  ProvisioningFailed    6m (x11 over 1h)   csi-cinderplugin_csi-provisioner-cinderplugin-0_acde092b-3576-11e9-8591-0a58ac10000a  failed to provision volume with StorageClass ""csi-sc-cinderplugin"": rpc error: code = Unknown desc = No suitable endpoint could be found in the service catalog.
  Normal   ExternalProvisioning  3m (x343 over 1h)  persistentvolume-controller                                                           waiting for a volume to be created, either by external provisioner ""csi-cinderplugin"" or manually created by system administrator

i use curl to get the instance 
[root@host-101-101-101-134 csi]# curl -X GET  'http://169.254.169.254/openstack/2012-08-10/meta_data.json'
{""availability_zone"": ""nova"", ""hostname"": ""storage.novalocal"", ""launch_index"": 0, ""uuid"": ""a2006d12-99dc-46ef-89bc-5ce3266413b6"", ""name"": ""storage""}

the cinder-plugin container is :
[root@host-101-101-101-134 csi]# kubectl exec -it csi-provisioner-cinderplugin-0 -n kube-system -c cinder /bin/bash       
[root@csi-provisioner-cinderplugin-0 /]# env
CSI_ATTACHER_CINDERPLUGIN_PORT=tcp://10.254.73.13:12345
HOSTNAME=csi-provisioner-cinderplugin-0
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT=tcp://10.254.0.1:443
CSI_PROVISIONER_CINDERPLUGIN_PORT_12345_TCP_ADDR=10.254.248.149
CSI_PROVISIONER_CINDERPLUGIN_PORT=tcp://10.254.248.149:12345
KUBERNETES_SERVICE_PORT=443
CLUSTER_NAME=kubernetes
CLOUD_CONFIG=/etc/config/cloud.conf
KUBERNETES_SERVICE_HOST=10.254.0.1
CSI_PROVISIONER_CINDERPLUGIN_PORT_12345_TCP=tcp://10.254.248.149:12345
CSI_ENDPOINT=unix://csi/csi.sock

and the cloud.conf is 
[root@host-101-101-101-134 csi]# cat cloud.conf 
[Global]
auth-url=http://10.2.11.1:5000/v2.0/
username=storage1
password=storage1
region=RegionOne
tenant-id=d8f6da95565d443e92a80537c554f83b



**What you expected to happen**:
can create pv
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:k8s v1.13
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-02-21 03:44:37,2019-02-21 06:02:19
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/issues/498,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/498,Support ca-file option when using csi,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
Currently, when creating Openstack instance in csi, ca-file option is not considered. This should be supported 
**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-02-21 09:06:08,2019-02-25 14:59:24
cloud-provider-openstack,darcyllingyan,https://github.com/kubernetes/cloud-provider-openstack/issues/499,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/499,x509: certificate signed by unknown authority when I use private certificate,"When I create the PVC, the PVC is Pending status, and after check the log, it shows as below:

Based on the log, it reports the ""x509: certificate signed by unknown authority"", as now the openstack certificate we used isn't signed by x509 and signed private, what can I do to resolve the issue? Thanks!

```
# # kubectl get pod -n kube-system
NAME                                           READY   STATUS    RESTARTS   AGE
csi-attacher-cinderplugin-0                    2/2     Running   0          40m
csi-nodeplugin-cinderplugin-2zq9d              2/2     Running   0          3h42m
csi-provisioner-cinderplugin-0                 2/2     Running   0          3h39m

#cat pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: csi-pvc-cinderplugin
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-sc-cinderplugin
# kubectl apply -f pvc.yaml 
persistentvolumeclaim/csi-pvc-cinderplugin created

# kubectl get pvc
NAME                   STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS          AGE
csi-pvc-cinderplugin   Pending                                      csi-sc-cinderplugin   7s
# kubectl describe pvc csi-pvc-cinderplugin
Events:
  Type       Reason                Age               From                                                                                  Message
  ----       ------                ----              ----                                                                                  -------
  Normal     ExternalProvisioning  4s (x4 over 17s)  persistentvolume-controller                                                           waiting for a volume to be created, either by external provisioner ""csi-cinderplugin"" or manually created by system administrator
  Normal     Provisioning          1s (x3 over 17s)  csi-cinderplugin_csi-provisioner-cinderplugin-0_4bb1a2a3-34cc-11e9-8cc5-b27933f5fb30  External provisioner is provisioning volume for claim ""default/csi-pvc-cinderplugin""
  Warning    ProvisioningFailed    1s (x3 over 16s)  csi-cinderplugin_csi-provisioner-cinderplugin-0_4bb1a2a3-34cc-11e9-8cc5-b27933f5fb30  failed to provision volume with StorageClass ""csi-sc-cinderplugin"": rpc error: code = Unknown desc = Post https://10.75.225.36:13000/v3/auth/tokens: x509: certificate signed by unknown authority
Mounted By:  nginx
```",closed,False,2019-02-21 09:10:42,2019-03-03 08:48:39
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/500,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/500,support ca-file parameter in CSI,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
currently, openstack client created in csi, doesn't support ca-file parameter . This PR is to update the same.
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #498 #499

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-21 10:02:25,2019-02-25 14:59:25
cloud-provider-openstack,jianglingxia,https://github.com/kubernetes/cloud-provider-openstack/issues/501,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/501,standalone-cinder-provisioner with iscsi volume type can not attached the k8s minion host,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:
i use the standalone-cinder-provisioner and the backend storage is iscsi,the pv and volume can dynamic created,but the device can not attached the k8s minion

the standalone-cinder-provisioner pod yaml use the web :https://github.com/kubernetes/cloud-provider-openstack/blob/master/manifests/provisioner/deployment.yaml

and the pod running:
kube-system   standalone-cinder-provisioner-7c4649c886-5x6x6   1/1     Running            7          5d5h

[root@hpa-vm:/home/ubuntu/jlx]$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                         STORAGECLASS   REASON   AGE
pvc-1dff02aa-3140-11e9-bde3-002dc92800b3   1Gi        RWO            Delete           Bound    kube-system/keystone-sc-pvc   standard                6d4h

the pv infomation:
[root@hpa-vm:/home/ubuntu/jlx]$ kubectl get pv pvc-1dff02aa-3140-11e9-bde3-002dc92800b3 -o yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    cinderVolumeId: c6daa97f-0bc7-4e8a-879d-1800ba51264a
    pv.kubernetes.io/provisioned-by: openstack.org/standalone-cinder
    standaloneCinderProvisionerIdentity: openstack.org/standalone-cinder
  creationTimestamp: ""2019-02-15T16:38:33Z""
  finalizers:
  - external-provisioner.volume.kubernetes.io/finalizer
  - kubernetes.io/pv-protection
  name: pvc-1dff02aa-3140-11e9-bde3-002dc92800b3
  resourceVersion: ""5776760""
  selfLink: /api/v1/persistentvolumes/pvc-1dff02aa-3140-11e9-bde3-002dc92800b3
  uid: 225064bd-3140-11e9-bde3-002dc92800b3
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 1Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: keystone-sc-pvc
    namespace: kube-system
    resourceVersion: ""5776739""
    uid: 1dff02aa-3140-11e9-bde3-002dc92800b3
  iscsi:
    initiatorName: iqn.2018-01.io.k8s:a13fc3d1cc22
    iqn: iqn.2099-01.cn.com.zte:usp.spr11-74:4a:a4:01:fd:54
    iscsiInterface: default
    lun: 1
    targetPortal: 19.19.19.252:3260
  persistentVolumeReclaimPolicy: Delete
  storageClassName: standard
  volumeMode: Filesystem
status:
  phase: Bound

cinder list:
| c6daa97f-0bc7-4e8a-879d-1800ba51264a | d8f6da95565d443e92a80537c554f83b |   in-use  | cinder-dynamic-pvc-1e103596-3140-11e9-b70a-0242ac100504 |   1   |    iscsi    |  false   |    False    |                                                      None                                                      |

and the openstack web is:
	
cinder-dynamic-pvc-1e103596-3140-11e9-b70a-0242ac100504
	- 	1GB 	In-use 	iscsi 	Attached to None on /k8s.io/standalone-cinder 	nova 	None
but why attached to None named host?
**What you expected to happen**:
can attach the k8s minion
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:k8sv1.13+
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-02-21 11:19:17,2019-03-06 08:22:13
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/502,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/502,[octavia-ingress-controller] Support security group management,"**What this PR does / why we need it**:
Support security group management in octavia-ingress-controller. In most of the production environments, the security group of the cluster nodes is more strict and the default port range of service ports may not be exposed. As a result, it's neccessary for octavia-ingress-controller to manage the security groups on kubernetes cluster admin's behalf.

This PR introduces a new bool type option `manage_security_groups` in octavia-ingress-controller configuration with the default value `false`, so the ingress controller doesn't manage security groups. The kubernetes cluster admin could change it to `true` so that the security group could be automatically managed.

Requirements to the OpenStack environment:
- Neutron needs to support [Standard Attributes Tag Extension](https://developer.openstack.org/api-ref/network/v2/index.html#standard-attributes-tag-extension)

**Which issue this PR fixes**: 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-22 08:04:16,2019-02-26 20:06:41
cloud-provider-openstack,dricoco,https://github.com/kubernetes/cloud-provider-openstack/issues/503,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/503,"octavia ingress controller, load balancer member creation","<!-- This form is for bug reports and feature requests! -->

/kind bug 

**What happened**:
Member creation failed in the octavia load balancer to route traffic to the pods

**What you expected to happen**:
Member creation achieved

**How to reproduce it (as minimally and precisely as possible)**:
just create an octavia ingress controller, and this will failed because the backup field that is necessary at least in stein release is not present.

you'll get
2019-02-25 10:12:00.927 4952 ERROR oslo_db.sqlalchemy.exc_filters [req-f31bfb62-79bf-4c33-9554-b964ddb3136a - 968b910e7f0b46b586ad967a1082a693 - af449f1c789f49fc885452c78954127e af449f1c789f49fc885452c78954127e] DBAPIError exception wrapped from (pymysql.err.InternalError) (1364, u""Field 'backup' doesn't have a default value"") [SQL: u'INSERT INTO member (created_at, updated_at, id, project_id, name, pool_id, subnet_id, ip_address, protocol_port, weight, monitor_address, monitor_port, provisioning_status, operating_status, enabled) VALUES (%(created_at)s, %(updated_at)s, %(id)s, %(project_id)s, %(name)s, %(pool_id)s, %(subnet_id)s, %(ip_address)s, %(protocol_port)s, %(weight)s, %(monitor_address)s, %(monitor_port)s, %(provisioning_status)s, %(operating_status)s, %(enabled)s)'] [parameters: {'monitor_port': None, 'project_id': u'968b910e7f0b46b586ad967a1082a693', 'name': None, 'weight': 1, 'provisioning_status': 'PENDING_CREATE', 'subnet_id': None, 'monitor_address': None, 'created_at': datetime.datetime(2019, 2, 25, 9, 12, 0, 926066), 'enabled': 1, 'updated_at': None, 'pool_id': u'7ee8e8ff-ab5e-4351-ae8c-5dfdbf520d95', 'protocol_port': 30207, 'ip_address': u'10.0.0.17', 'id': '50a4fc8b-634a-465c-916b-b7752f19fe0d', 'operating_status': 'NO_MONITOR'}]: InternalError: (1364, u""**Field 'backup' doesn't have a default value**"")

in octavia-api.log


",open,False,2019-02-25 09:45:03,2019-02-27 05:42:12
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/504,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/504,[WIP] cleanup mount package,"This commit reuses kubernetes/pkg/util/mount in places
required. And also removes, existing copy of k8s mount
package in repository.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",open,True,2019-02-25 09:51:10,2019-03-09 00:58:10
cloud-provider-openstack,johscheuer,https://github.com/kubernetes/cloud-provider-openstack/pull/505,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/505,Make use of the same API versions for DaemonSet/Deployment/Statefulset,"**What this PR does / why we need it**:

Make use of a the same API version for all objects (since 1.9 all used Kinds are in GA: https://kubernetes.io/blog/2017/12/kubernetes-19-workloads-expanded-ecosystem). I changed all `extensions/v1beta1` to `apps/v1`.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-25 11:00:54,2019-02-25 17:37:13
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/506,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/506,[octavia-ingress-controller] Improve floating IP management,"**What this PR does / why we need it**:
Improve floating IP management in octavia-ingress-controller. Adds the method `EnsureFloatingIP` to handle adding and deleting floating IP for the Neutron port, just like the way security group is managed.

**Which issue this PR fixes**: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-26 08:14:07,2019-02-26 20:19:25
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/507,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/507,octavia-ingress-controller document,"**What this PR does / why we need it**:
Make octavia-ingress-controller up to date.

**Which issue this PR fixes**: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-27 02:04:41,2019-03-04 10:46:39
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/508,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/508,Add probe for CSI,"Add CSI identity CSI so that we can return CSI status back

fixes: #483 


**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
some discussion might be considered , whether we need create a client to auth or use cinder to do some query etc...

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-27 06:30:21,2019-03-13 05:09:34
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/509,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/509,use CSI error code instead of own defined error,"CSI spec has error code defined

NOTE: this should have more included, but create a PR for discussion and see whether something need to be corrected or use additional stuff that I am not aware...

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",open,True,2019-02-27 07:18:45,2019-03-08 08:42:19
cloud-provider-openstack,hikhvar,https://github.com/kubernetes/cloud-provider-openstack/pull/510,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/510,Fix typo in documentation,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Fix two typos in the documentation

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-02-27 11:53:02,2019-03-03 02:46:18
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/issues/511,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/511,e2e reporting to testgrid missing for 1.13 and 1.14,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind feature

**What happened**:
e2e conformance testing for k8s 1.13 and 1.14

**What you expected to happen**:
e2e reporting to testgrid

**How to reproduce it (as minimally and precisely as possible)**:
n/a

**Anything else we need to know?**:
n/a

**Environment**:
- openstack-cloud-controller-manager version: n/a
- OS (e.g. from /etc/os-release): n/a
- Kernel (e.g. `uname -a`): n/a
- Install tools: n/a
- Others: n/a
",closed,False,2019-02-27 14:20:29,2019-03-18 07:47:12
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/512,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/512,Add 1.13 and 1.14 pipelines,"Required to run 1.13 and 1.14 stable e2e conformance tests.



<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Adds pipelines that exist in CI for 1.13 and 1.14 conformance testing.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #511

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```",closed,True,2019-02-27 14:20:44,2019-03-18 13:44:51
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/513,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/513,[WIP] Add Support for Running CSI Sanity Tests,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
This PR adds support for running csi sanity tests, from csi-test/sanity, It adds a package sanity which provides fake cloud provider, fake metadata and fake mount
and instantiate cinder csi driver for running sanity tests.

This PR does not fix sanity test failures, for this a separate PR will be created. 

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #469

**Special notes for your reviewer**:
Using Existing mock package for cloud provider, metadata and mount was not possible as mock function arguments comes csi-test/sanity package, and also it does not support conditional mocking.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
`None.`",open,True,2019-03-01 06:17:48,2019-04-05 07:28:01
cloud-provider-openstack,darcyllingyan,https://github.com/kubernetes/cloud-provider-openstack/issues/514,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/514,The cinder-csi couldn't be deployed when upgrade the kubernetes from v1.12 to v1.13,"Hi,
I have deployed the cinder-csi in kubernetes v1.12 successfully, as the volume mount issue, now I have to upgrade the kubernetes from v1.12 to v1.13. 
But after I upgrade the kubernetes to v1.13, all the related cinder-csi pods couldn't be started.
The only different for the cinder-csi-plugin deployed in v1.12 and v1.13 is the cinder-csi-plugin image:
1. The cinder-csi-plugin image, both versions images are latest, but the size is different.
1) In v1.3:
```
# docker images|grep csi
docker.io/k8scloudprovider/cinder-csi-plugin   latest              2db05ffa4d7c        9 hours ago         300 MB
quay.io/k8scsi/csi-node-driver-registrar                                    v1.0.1              4c7fa144c035        2 months ago        46.9 MB
quay.io/k8scsi/csi-provisioner                                              v1.0.1              2004b031bce2        2 months ago        48 MB
quay.io/k8scsi/csi-attacher                                                 v1.0.1              2ef5329b7139        2 months ago        50.2 MB
quay.io/k8scsi/csi-snapshotter                                              v1.0.1              c70168a8d1de        2 months ago        49.2 MB

```
2) In v1.12
```
# docker images|grep csi
docker.io/k8scloudprovider/cinder-csi-plugin                                latest              ad9688cdab83        4 days ago          326 MB
quay.io/k8scsi/csi-node-driver-registrar                                    v1.0.1              4c7fa144c035        2 months ago        46.9 MB
quay.io/k8scsi/csi-provisioner                                              v1.0.1              2004b031bce2        2 months ago        48 MB
quay.io/k8scsi/csi-attacher                                                 v1.0.1              2ef5329b7139        2 months ago        50.2 MB
quay.io/k8scsi/csi-snapshotter                                              v1.0.1              c70168a8d1de        2 months ago        49.2 MB

```
Other plugin images are all v1.0.1(attacher, provisioner)

The pod status are:
```
# kubectl get pod -n kube-system
NAME                                                               READY   STATUS             RESTARTS   AGE
csi-attacher-cinderplugin-0                                        1/2     CrashLoopBackOff   92         9h
csi-nodeplugin-cinderplugin-cch5f                                  1/2     CrashLoopBackOff   110        9h
csi-nodeplugin-cinderplugin-tz4pm                                  1/2     CrashLoopBackOff   110        9h
csi-nodeplugin-cinderplugin-vp7d7                                  1/2     CrashLoopBackOff   110        9h
csi-provisioner-cinderplugin-0                                     0/2     CrashLoopBackOff   110        9h
```
2. The error logs:
1) The external-attacher log:
```
# kubectl logs -f csi-attacher-cinderplugin-0 -n kube-system csi-attacher
I0301 15:02:04.708336       1 main.go:76] Version: v1.0.1-0-gb7dadac
I0301 15:02:04.712729       1 connection.go:89] Connecting to /var/lib/csi/sockets/pluginproxy/csi.sock
I0301 15:02:04.713213       1 connection.go:116] Still trying, connection is CONNECTING
I0301 15:02:04.713876       1 connection.go:113] Connected
I0301 15:02:04.713902       1 connection.go:242] GRPC call: /csi.v1.Identity/Probe
I0301 15:02:04.713910       1 connection.go:243] GRPC request: {}
I0301 15:02:04.716289       1 connection.go:245] GRPC response: {}
I0301 15:02:04.720946       1 connection.go:246] GRPC error: rpc error: code = Unimplemented desc = unknown service csi.v1.Identity
I0301 15:02:04.720978       1 main.go:214] Probe failed with rpc error: code = Unimplemented desc = unknown service csi.v1.Identity
I0301 15:02:05.721971       1 connection.go:242] GRPC call: /csi.v1.Identity/Probe
I0301 15:02:05.722026       1 connection.go:243] GRPC request: {}
I0301 15:02:05.723187       1 connection.go:245] GRPC response: {}
I0301 15:02:05.723587       1 connection.go:246] GRPC error: rpc error: code = Unimplemented desc = unknown service csi.v1.Identity
I0301 15:02:05.723598       1 main.go:214] Probe failed with rpc error: code = Unimplemented desc = unknown service csi.v1.Identity
I0301 15:02:06.723796       1 connection.go:242] GRPC call: /csi.v1.Identity/Probe
I0301 15:02:06.723843       1 connection.go:243] GRPC request: {}
I0301 15:02:06.724945       1 connection.go:245] GRPC response: {}
I0301 15:02:06.725322       1 connection.go:246] GRPC error: rpc error: code = Unimplemented desc = unknown service csi.v1.Identity
I0301 15:02:06.725333       1 main.go:214] Probe failed with rpc error: code = Unimplemented desc = unknown service csi.v1.Identity
I0301 15:02:07.725491       1 connection.go:242] GRPC call: /csi.v1.Identity/Probe
I0301 15:02:07.725544       1 connection.go:243] GRPC request: {}
I0301 15:02:07.726457       1 connection.go:245] GRPC response: {}
I0301 15:02:07.726835       1 connection.go:246] GRPC error: rpc error: code = Unimplemented desc = unknown service csi.v1.Identity
I0301 15:02:07.726846       1 main.go:214] Probe failed with rpc error: code = Unimplemented desc = unknown service csi.v1.Identity
```

2. The external-provisioner log, it's strange the logs shows the command line usage infor.
```
# kubectl logs -f csi-provisioner-cinderplugin-0 -n kube-system csi-provisioner
flag provided but not defined: -provisioner
Usage of /csi-attacher:
  -alsologtostderr
        log to standard error as well as files
  -connection-timeout duration
        Timeout for waiting for CSI driver socket. (default 1m0s)
  -csi-address string
        Address of the CSI driver socket. (default ""/run/csi/socket"")
  -dummy
        Run in dummy mode, i.e. not connecting to CSI driver and marking everything as attached. Expected CSI driver name is ""csi/dummy"".
  -kubeconfig string
        Absolute path to the kubeconfig file. Required only when running out of cluster.
  -leader-election
        Enable leader election.
  -leader-election-identity string
        Unique idenity of this attcher. Typically name of the pod where the attacher runs.
  -leader-election-namespace string
        Namespace where this attacher runs.
  -log_backtrace_at value
        when logging hits line file:N, emit a stack trace
  -log_dir string
        If non-empty, write log files in this directory
  -logtostderr
        log to standard error instead of files
  -resync duration
        Resync interval of the controller. (default 10m0s)
  -stderrthreshold value
        logs at or above this threshold go to stderr
  -timeout duration
        Timeout for waiting for attaching or detaching the volume. (default 15s)
  -v value
        log level for V logs
```
3. The csi-nodeplugin-cinderplugin log:
```
# kubectl logs -f csi-nodeplugin-cinderplugin-wz5nj -n kube-system node-driver-registrar
I0301 15:10:07.732547       1 main.go:111] Version: v1.0.1-0-g27703026
I0301 15:10:07.732721       1 main.go:118] Attempting to open a gRPC connection with: ""/csi/csi.sock""
I0301 15:10:07.732748       1 connection.go:69] Connecting to /csi/csi.sock
I0301 15:10:07.733131       1 connection.go:96] Still trying, connection is CONNECTING
I0301 15:10:07.734290       1 connection.go:93] Connected
I0301 15:10:07.734310       1 main.go:126] Calling CSI driver to discover driver name.
I0301 15:10:07.734356       1 connection.go:137] GRPC call: /csi.v1.Identity/GetPluginInfo
I0301 15:10:07.734389       1 connection.go:138] GRPC request: {}
I0301 15:10:07.737887       1 connection.go:140] GRPC response: {}
I0301 15:10:07.738372       1 connection.go:141] GRPC error: rpc error: code = Unimplemented desc = unknown service csi.v1.Identity
E0301 15:10:07.738392       1 main.go:131] rpc error: code = Unimplemented desc = unknown service csi.v1.Identity
```
4. The cinder container log:
```
# kubectl logs -f csi-attacher-cinderplugin-0 -n kube-system cinder
I0301 14:44:15.303565       1 driver.go:49] Driver: csi-cinderplugin version: 0.2.0
I0301 14:44:15.304003       1 driver.go:80] Enabling controller service capability: CREATE_DELETE_VOLUME
I0301 14:44:15.304043       1 driver.go:80] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
I0301 14:44:15.304050       1 driver.go:92] Enabling volume access mode: SINGLE_NODE_WRITER
I0301 14:44:15.304360       1 server.go:108] Listening for connections on address: &net.UnixAddr{Name:""/csi/csi.sock"", Net:""unix""}
```

Thanks for any suggestions.
Thanks
Darcy

",closed,False,2019-03-01 15:14:38,2019-03-19 08:38:02
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/515,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/515,Trivial: Remove meaningless charactor in doc,"
**What this PR does / why we need it**:

TBH, I am not sure why there is.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-03-01 23:19:40,2019-03-06 21:33:05
cloud-provider-openstack,oomichi,https://github.com/kubernetes/cloud-provider-openstack/pull/516,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/516,Trivial: Fix typo of CIDR,"
**What this PR does / why we need it**:

This fixes typo of CIDR.

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
NONE
```
",closed,True,2019-03-01 23:49:15,2019-03-06 21:26:48
cloud-provider-openstack,darcyllingyan,https://github.com/kubernetes/cloud-provider-openstack/issues/517,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/517,The csi-nodeplugin-cinderplugin Daemonset can't be started in the the nodes in one cluster,"Hi,
The csi-nodeplugin-cinderplugin is Daemonset and should be started in all nodes in the cluster. 
I created several clusters and deployed the cinder-csi plugin, but in one cluster the csi-nodeplugin-cinderplugin couldn't be started in all nodes but only in part nodes.

1. Below is the node information.
```
# kubectl get node
NAME                 STATUS     ROLES    AGE     VERSION
dzhou-3-control-01   Ready      <none>   2d23h   v1.13.2
dzhou-3-control-02   Ready      <none>   2d23h   v1.13.2
dzhou-3-control-03   Ready      <none>   2d23h   v1.13.2
dzhou-3-storage-01   Ready      <none>   2d23h   v1.13.2
dzhou-3-storage-02   NotReady   <none>   2d23h   v1.13.2
dzhou-3-storage-03   Ready      <none>   2d23h   v1.13.2
dzhou-3-worker-01    Ready      <none>   2d23h   v1.13.2
dzhou-3-worker-02    Ready      <none>   2d23h   v1.13.2
```
2. Below is the cinder-csi plugin pods the ```csi-nodeplugin-cinderplugin``` pods are only deployed in ```dzhou-3-worker-01``` and ```dzhou-3-worker-02```.
```
# kubectl get pod -n kube-system -o wide
NAME                                         READY   STATUS    RESTARTS   AGE     IP              NODE                 NOMINATED NODE   READINESS GATES
csi-attacher-cinderplugin-0                  2/2     Running   0          29h     192.168.1.11    dzhou-3-worker-01    <none>           <none>
csi-nodeplugin-cinderplugin-5phkk            2/2     Running   0          29h     192.168.2.10    dzhou-3-worker-01    <none>           <none>
csi-nodeplugin-cinderplugin-bn64r            2/2     Running   0          29h     192.168.2.18    dzhou-3-worker-02    <none>           <none>
csi-provisioner-cinderplugin-0               2/2     Running   0          29h     192.168.1.146   dzhou-3-worker-02    <none>           <none>
csi-snapshotter-cinder-0                     2/2     Running   0          29h     192.168.1.12    dzhou-3-worker-01    <none>           <none>
```
Thanks for any suggestions about the case.

Thanks


",closed,False,2019-03-03 08:56:56,2019-03-24 03:12:31
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/518,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/518,Support new annotation for Service,"**What this PR does / why we need it**:
Support to specify port ID to create Service of LoadBalancer type.

Use case:
Sometimes, we need to know the service IP address before a Service is actually created, so other applications who replies on the Service external IP could be deployed with no dependencies, e.g. the kube-apiserver certificate could be created before the service is actually created.

In order to specify port ID to create Service, a new annotation for Service is introduced, e.g.:
```yaml
annotations:
  loadbalancer.openstack.org/port-id: ""849ee2ed-72a6-44a4-911e-6a8be46dbe8e""
```

Please be aware that after the Service is created, port ID cannot be changed.

**Which issue this PR fixes**:

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Introduce a new annotation(`loadbalancer.openstack.org/port-id`) for Service of LoadBalancer type to specify a particular Neutron port for creating the Octavia load balancer.
```",closed,True,2019-03-04 01:58:21,2019-03-13 11:00:37
cloud-provider-openstack,johscheuer,https://github.com/kubernetes/cloud-provider-openstack/issues/519,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/519,Inconsistent arguments for octavia ingress,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
I tried the [octavia ingress](https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-octavia-ingress-controller.md) feature and noticed that the arguments follow different schemas e.g. in the ""normal"" [cloud provider tree](https://github.com/kubernetes/cloud-provider-openstack/blob/master/pkg/cloudprovider/providers/openstack/openstack.go#L103) (and all sub kinds) uses a hyphen as word separator but the [octavia ingress controller](https://github.com/kubernetes/cloud-provider-openstack/blob/master/pkg/ingress/config/config.go#L42) uses snake case which can lead to some strange errors e.g. if you copy `floating-network-id` for testing the octavia ingress controller won't recognise the floating network and will print an error.

**What you expected to happen**:
In the cloud-provider-openstack and all sub pkgs (or additional controllers) we should follow the same argument schema? From my perspective we should follow the `floating-network-id` schema since the octavia ingress resource is rather new compared to the cloud provider controller.

**How to reproduce it (as minimally and precisely as possible)**:
-

**Anything else we need to know?**:


**Environment**:
- openstack-cloud-controller-manager version: latest
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-03-04 06:56:46,2019-03-07 10:31:03
cloud-provider-openstack,johscheuer,https://github.com/kubernetes/cloud-provider-openstack/pull/520,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/520,Change octavia ingress arguments from snake case to kebab case,"**What this PR does / why we need it**:
This PR changes the argument format for the octavia ingress controller from snake case to kebab case to match the other argument formats in order to provide a consistent user experience.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: 
fixes #519

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
*Action required* Changed argument format for octavia ingress controller to kebab case
```
",closed,True,2019-03-04 06:59:25,2019-03-07 10:31:03
cloud-provider-openstack,darcyllingyan,https://github.com/kubernetes/cloud-provider-openstack/issues/521,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/521,Integrate the external cloud provider for OpenStack to cluster,"Hi,
Now I have installed the cinder-csi successfully, the cinder-csi is part of the cloud-provider-openstack.
My question 1:
If I have just installed the cinder-CSI, whether I need to install the openstack-cloud-controller-manager? If I didn't install the openstack-cloud-controller-manager or other something necessory(I'm not clear), whether it has the usage impact with the cinder-csi? 

Question2:
If now I have deployed one kubernetes cluster, what shall I do to integrate the external cloud provider for OpenStack to cluster? Which doc is better for me to follow?

Thanks very much for any response.

Thanks",closed,False,2019-03-04 08:10:01,2019-03-04 13:42:38
cloud-provider-openstack,jianglingxia,https://github.com/kubernetes/cloud-provider-openstack/issues/522,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/522,"bulid the cloud-provider-openstack print error with the ""sigs.k8s.io/sig-storage-lib-external-provisioner/controller""","<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
 /kind bug


**What happened**:

i clone the cloud-provider-openstack code and bulid the rbd.go then log error :can't load package: package github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/controller: code in directory /media/B/src/github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/controller expects import ""sigs.k8s.io/sig-storage-lib-external-provisioner/controller""

i see the web https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/blob/master/controller/controller.go code import package :""sigs.k8s.io/sig-storage-lib-external-provisioner/controller/metrics""

and use go get also error:

[root@LIN-FE325E0FEA1 github.com]# go get github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/controller
can't load package: package github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/controller: code in directory /media/B/src/github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/controller expects import ""sigs.k8s.io/sig-storage-lib-external-provisioner/controller""

use go get then my dir dynamic create the sigs.k8s.io  dir 

[root@LIN-FE325E0FEA1 vendor]# ls
bitbucket.org  cloud.google.com  golang.org  google.golang.org  go.uber.org  LICENSE  sigs.k8s.io  vendor.json
BUILD          github.com        gonum.org   gopkg.in           k8s.io       OWNERS   vbom.ml
[root@LIN-FE325E0FEA1 vendor]# pwd
/media/B/src/vendor

@ramineni said @jianglingxia delete the vendor dir and build again i do it but many code dependecy the vendor dir

**What you expected to happen**:
i want the code can build and running, i test is that i modify the github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/controller to ""sigs.k8s.io/sig-storage-lib-external-provisioner/controller""
and build normal 
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: master branch
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-03-06 03:15:27,2019-03-14 11:27:59
cloud-provider-openstack,jichenjc,https://github.com/kubernetes/cloud-provider-openstack/pull/523,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/523,upgrade CSI to 1.1.0,"update CSI to 1.1.0

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",open,True,2019-03-06 06:26:51,2019-04-04 05:39:02
cloud-provider-openstack,jianglingxia,https://github.com/kubernetes/cloud-provider-openstack/issues/524,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/524,standalone-cinder-provisioner with iscsi volume type  attached the k8s minion host but cinder list see attached to NONE,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug



**What happened**:
i use the standalone-cinder-provisioner and the backend storage is iscsi,the pv and volume can dynamic created, the device can  attached the k8s minion but cinder list see attached to NONE

| cd265309-5c86-4bba-80dc-da52a63cbe23 | 90d234769080430d9ae8b0052d8f8cd2 |   in-use  | cinder-dynamic-pvc-91233138-39a5-11e9-9a2c-0a58ac100002 |   1   |    iscsi    |  false   |    False    |                                                      None                   

**What you expected to happen**:
cinder list result can attached to k8s minion host name 
**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:k8sv1.13
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2019-03-06 08:27:40,2019-03-07 00:53:34
cloud-provider-openstack,darcyllingyan,https://github.com/kubernetes/cloud-provider-openstack/issues/525,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/525,"cinder-csi-plugin: rpc error: code = Internal desc = executable file not found in $PATH"".","Hi ,
I have a question about the cinder-csi-plugin image usage. As I want to use my own base image so I built the cinder-csi-plugin image private. 
The source code I use is https://github.com/kubernetes/cloud-provider-openstack.git, it is the master branch.

The process is:
```
1. export GOROOT=/usr/local/go; export GOPATH=$HOME/go; export PATH=$GOROOT/bin:$GOPATH/bin:$PATH
2. mkdir -p $(GOPATH)/src/k8s.io; cd $(GOPATH)/src/k8s.io
3. git clone https://github.com/kubernetes/cloud-provider-openstack.git
4. make image-csi-plugin
5. docker tag k8scloudprovider/cinder-csi-plugin:e525108c-dirty k8scloudprovider/cinder-csi-plugin:latest
6. docker push to my private registrary
```
I have built it successfully and all the pods are running:
```
# kubectl get pod -n kube-system|grep csi
csi-attacher-cinderplugin-0                                        2/2     Running       0          51m
csi-nodeplugin-cinderplugin-cblr9                                  2/2     Running       0          48m
csi-nodeplugin-cinderplugin-ftg6h                                  2/2     Running       0          48m
csi-nodeplugin-cinderplugin-p9zxh                                  2/2     Running       0          48m
csi-provisioner-cinderplugin-0                                     2/2     Running       0          48m
csi-snapshotter-cinder-0                                           2/2     Running       1          47m
```

But when I create a nginx pod to test it, it reports
 ```
# kubectl describe pod nginx
Events:
  Type     Reason                  Age                  From                                               Message
  ----     ------                  ----                 ----                                               -------
  Normal   Scheduled               3m29s                default-scheduler                                  Successfully assigned default/nginx to csi-darcy-bcmt-01-worker-control-edge-01
  Normal   SuccessfulAttachVolume  3m25s                attachdetach-controller                            AttachVolume.Attach succeeded for volume ""pvc-20d0f0c4-4074-11e9-ba04-fa163ec36829""
  Warning  FailedMount             86s                  kubelet, csi-darcy-bcmt-01-worker-control-edge-01  Unable to mount volumes for pod ""nginx_default(2a0eb386-4074-11e9-ba04-fa163ec36829)"": timeout expired waiting for volumes to attach or mount for pod ""default""/""nginx"". list of unmounted volumes=[csi-data-cinderplugin]. list of unattached volumes=[csi-data-cinderplugin default-token-96j2d]
  Warning  FailedMount             60s (x9 over 3m19s)  kubelet, csi-darcy-bcmt-01-worker-control-edge-01  MountVolume.MountDevice failed for volume ""pvc-20d0f0c4-4074-11e9-ba04-fa163ec36829"" : rpc error: code = Internal desc = executable file not found in $PATH
```

The detailed log is:
```

# docker ps|grep cinder-csi-plugin
0c7f6dd71d41        bcmt-registry:5000/cinder-csi-plugin@sha256:233681ba78c120ec8d68e34a890d49d9581e1bbf672d3d9d3e20643f0f901ff9   ""/bin/cinder-csi-p...""   13 minutes ago      Up 13 minutes                                                                                k8s_cinder_csi-nodeplugin-cinderplugin-lmgcb_kube-system_05b77947-4074-11e9-ba04-fa163ec36829_0

# docker logs 0c7f6dd71d41
I0307 00:57:55.497175       1 driver.go:56] Driver: cinder.csi.openstack.org version: 1.0.0
I0307 00:57:55.497428       1 driver.go:88] Enabling controller service capability: LIST_VOLUMES
I0307 00:57:55.497449       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_VOLUME
I0307 00:57:55.497456       1 driver.go:88] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
I0307 00:57:55.497462       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
I0307 00:57:55.497468       1 driver.go:88] Enabling controller service capability: LIST_SNAPSHOTS
I0307 00:57:55.497475       1 driver.go:100] Enabling volume access mode: SINGLE_NODE_WRITER
I0307 00:57:55.497518       1 driver.go:110] Enabling node service capability: STAGE_UNSTAGE_VOLUME
I0307 00:57:55.498089       1 server.go:108] Listening for connections on address: &net.UnixAddr{Name:""/csi/csi.sock"", Net:""unix""}
W0307 00:58:57.502860       1 mount_linux.go:447] 'fsck' not found on system; continuing mount without running 'fsck'.
E0307 00:58:57.503566       1 mount_linux.go:151] Mount failed: exec: ""mount"": executable file not found in $PATH
Mounting command: mount
Mounting arguments: -t ext4 -o defaults /dev/vdf /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-20d0f0c4-4074-11e9-ba04-fa163ec36829/globalmount
Output: 
E0307 00:58:57.503812       1 mount_linux.go:529] Could not determine if disk ""/dev/vdf"" is formatted (executable file not found in $PATH)
E0307 00:58:57.503856       1 utils.go:77] GRPC error: rpc error: code = Internal desc = executable file not found in $PATH
W0307 00:58:59.336652       1 mount_linux.go:447] 'fsck' not found on system; continuing mount without running 'fsck'.
E0307 00:58:59.336757       1 mount_linux.go:151] Mount failed: exec: ""mount"": executable file not found in $PATH
Mounting command: mount
Mounting arguments: -t ext4 -o defaults /dev/vdf /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-20d0f0c4-4074-11e9-ba04-fa163ec36829/globalmount
Output: 
E0307 00:58:59.336855       1 mount_linux.go:529] Could not determine if disk ""/dev/vdf"" is formatted (executable file not found in $PATH)
E0307 00:58:59.336873       1 utils.go:77] GRPC error: rpc error: code = Internal desc = executable file not found in $PATH
W0307 00:59:01.741636       1 mount_linux.go:447] 'fsck' not found on system; continuing mount without running 'fsck'.
E0307 00:59:01.741701       1 mount_linux.go:151] Mount failed: exec: ""mount"": executable file not found in $PATH
Mounting command: mount
Mounting arguments: -t ext4 -o defaults /dev/vdf /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-20d0f0c4-4074-11e9-ba04-fa163ec36829/globalmount
Output: 
E0307 00:59:01.741756       1 mount_linux.go:529] Could not determine if disk ""/dev/vdf"" is formatted (executable file not found in $PATH)
E0307 00:59:01.741774       1 utils.go:77] GRPC error: rpc error: code = Internal desc = executable file not found in $PATH
W0307 00:59:05.382492       1 mount_linux.go:447] 'fsck' not found on system; continuing mount without running 'fsck'.
E0307 00:59:05.382543       1 mount_linux.go:151] Mount failed: exec: ""mount"": executable file not found in $PATH
Mounting command: mount
Mounting arguments: -t ext4 -o defaults /dev/vdf /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-20d0f0c4-4074-11e9-ba04-fa163ec36829/globalmount
Output: 
E0307 00:59:05.382644       1 mount_linux.go:529] Could not determine if disk ""/dev/vdf"" is formatted (executable file not found in $PATH)
E0307 00:59:05.382659       1 utils.go:77] GRPC error: rpc error: code = Internal desc = executable file not found in $PATH
W0307 00:59:10.705432       1 mount_linux.go:447] 'fsck' not found on system; continuing mount without running 'fsck'.
E0307 00:59:10.705493       1 mount_linux.go:151] Mount failed: exec: ""mount"": executable file not found in $PATH
Mounting command: mount
Mounting arguments: -t ext4 -o defaults /dev/vdf /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-20d0f0c4-4074-11e9-ba04-fa163ec36829/globalmount
Output: 
E0307 00:59:10.705531       1 mount_linux.go:529] Could not determine if disk ""/dev/vdf"" is formatted (executable file not found in $PATH)
E0307 00:59:10.705548       1 utils.go:77] GRPC error: rpc error: code = Internal desc = executable file not found in $PATH
W0307 00:59:20.045509       1 mount_linux.go:447] 'fsck' not found on system; continuing mount without running 'fsck'.
E0307 00:59:20.045672       1 mount_linux.go:151] Mount failed: exec: ""mount"": executable file not found in $PATH
Mounting command: mount
Mounting arguments: -t ext4 -o defaults /dev/vdf /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-20d0f0c4-4074-11e9-ba04-fa163ec36829/globalmount
Output: 
E0307 00:59:20.045834       1 mount_linux.go:529] Could not determine if disk ""/dev/vdf"" is formatted (executable file not found in $PATH)
E0307 00:59:20.045918       1 utils.go:77] GRPC error: rpc error: code = Internal desc = executable file not found in $PATH
W0307 00:59:37.415496       1 mount_linux.go:447] 'fsck' not found on system; continuing mount without running 'fsck'.
E0307 00:59:37.415595       1 mount_linux.go:151] Mount failed: exec: ""mount"": executable file not found in $PATH
Mounting command: mount

```

After enter the POD, the mount doesn't exist.
```
# docker ps|grep cinder-csi
f71b195a53a8        bcmt-registry:5000/k8scloudprovider/cinder-csi-plugin@sha256:f06688ff1ea783f0e9f031c74b9660ab9ac05d1aa73b3b6f3d4ff8279244d844   ""/bin/cinder-csi-p...""   6 minutes ago       Up 6 minutes                                                                                 k8s_cinder_csi-nodeplugin-cinderplugin-xzgv8_kube-system_f75e955f-40dc-11e9-80f4-fa163ec36829_0
[root@csi-darcy-bcmt-01-worker-control-edge-01 ~]# docker exec -it f71b195a53a8 bash
bash-4.2# ls bin/mount
ls: cannot access bin/mount: No such file or directory
```

Do you have any suggestions, thanks!
Thanks
Darcy",closed,False,2019-03-07 01:08:33,2019-03-26 06:44:37
cloud-provider-openstack,joelsmith,https://github.com/kubernetes/cloud-provider-openstack/pull/526,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/526,Update embargo doc link in SECURITY_CONTACTS and change PST to PSC,See https://github.com/kubernetes/security/issues/8 for more information,closed,True,2019-03-08 17:52:31,2019-03-18 05:39:11
cloud-provider-openstack,jianglingxia,https://github.com/kubernetes/cloud-provider-openstack/pull/527,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/527,add fc volume type support,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
add fc volume type support!
**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Add FC volume type support
```
",open,True,2019-03-12 06:48:48,2019-03-12 07:28:46
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/528,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/528,Update kubernetes to v1.14.0-beta.2,"This PR updates the k8s version to v1.14.0-beta.2 
and also updates to use vanity url of sig-storage-lib-external-provisioner
Ref: https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/pull/31

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #522

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-03-12 08:31:46,2019-03-19 00:48:22
cloud-provider-openstack,GeetikaBatra,https://github.com/kubernetes/cloud-provider-openstack/issues/529,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/529,Switch to gomodules for dependency management,"With introduction of gomodules in from GO 1.11, we should switch to go modules for dependency management.
",open,False,2019-03-12 10:24:42,2019-03-12 10:24:42
cloud-provider-openstack,GeetikaBatra,https://github.com/kubernetes/cloud-provider-openstack/pull/530,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/530,Use gomodules for dependency management,"Signed off by : Geetika Batra <geetka791@gmail.com>

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
fixes #530 
**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
Use gomodule for dependency management
```
",open,True,2019-03-12 11:09:09,2019-03-18 05:09:50
cloud-provider-openstack,mape90,https://github.com/kubernetes/cloud-provider-openstack/issues/531,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/531,Cinder CSI should not trust openstack device path,"**Is this a BUG REPORT**:
/kind bug

**What happened**:
Cinder CSI do not validate path from the node, but trusts the path given by Openstack. This causes that non trivial environment will eventually fail to mount volumes as device path on node and one given by Openstack are out of sync. 

This causes either that volume can not be mounted or wrong volume is mounted to the pod.

**What you expected to happen**:
Cinder CSI (nodeserver) should check the device path from node and not trust the path given by Openstack. This is implemented correctly in CCM openstack volume implementation. 

**How to reproduce it (as minimally and precisely as possible)**:
Attach and detach multiple volumes simultaneously and eventually Openstack and Nodes will get out of sync. 
Just creating multiple pods with volumes and then deleting them might not trigger this behavior as usually OS will allocated device path with same as given by OS. 

**Anything else we need to know?**:
Simple solution is just copy implementation from CCM. There get GetDevicePath function that is the one what should be called when trying to determine the path at node.

Better solution would be to merge these two openstack API implementations to one. 
Basically as CCM do not use volume implementation at all it could be quite heavily modified during merge as it will not break anything. I.e we could just remove support for cinder V1 and V2 apis as Cinder CSI do not support them now and they are any case deprecated. This would help that there would not be need for wrapper functions that duplicate code, as every api version needs own functions. Also we would need to copy snapshot implementation from CSI to CCM. 

This merge we would eliminate the issue that there are multiple implementations of same functions.

**Effects**:
All Cinder CSI versions.
",open,False,2019-03-12 11:40:00,2019-03-12 11:40:29
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/532,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/532,Fix gophercloud digest in Gopkg.lock,"**What this PR does / why we need it**:
Fix gophercloud digest in Gopkg.lock

**Which issue this PR fixes**: fixes #

**Special notes for your reviewer**:

**Release note**:
-->
```release-note
None
```
",closed,True,2019-03-13 11:03:21,2019-03-27 05:29:54
cloud-provider-openstack,JrCs,https://github.com/kubernetes/cloud-provider-openstack/pull/533,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/533,Allow to specify default value in cloud-config for openstack-internal,"-load-balancer

Signed-off-by: JrCs <90z7oey02@sneakemail.com>

**What this PR does / why we need it**:
Add on option (`internal-lb`) in the cloud-config configuration file so we can set the default value for openstack-internal-load-balancer
",closed,True,2019-03-13 11:40:08,2019-03-21 10:42:48
cloud-provider-openstack,opal-labs,https://github.com/kubernetes/cloud-provider-openstack/issues/534,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/534,Standalone-cinder Missing Secret,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:
Volume created by the standalone cinder provisioner fails to mount a pod due to a missing secret. 

**What you expected to happen**:
The pod to mount the volume.

**How to reproduce it (as minimally and precisely as possible)**:
I may be missing a step or doing something wrong, but on a bare metal kubernetes cluster I created a standalone-cinder-provisioner according to [this manifest](https://github.com/kubernetes/cloud-provider-openstack/blob/master/manifests/provisioner/deployment.yaml) trying to get [the example](https://github.com/kubernetes/cloud-provider-openstack/blob/master/examples/persistent-volume-provisioning/cinder/cinder-full.yaml) to work. The volume is being created in cinder, the pv shows as bound and the pvc shows as bound, but the pod fails to mount with `MountVolume.NewMounter initialization failed for volume ""pvc-ee1eaa6e-45ad-11e9-af69-fa163e32930b"" : Couldn't get secret default/standard-cephx-secret err: secrets ""standard-cephx-secret"" not found`. It looks like it's failing on step 13 of [the documentation](https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-cinder-standalone-provisioner.md).

**Anything else we need to know?**:
Kubernetes cluster is on VMs in OVH, talking to OVH cinder. According to #254 I need to create this secret manually, but I don't know if that's true or what's supposed to be in it since the provisioner has the required information to create the volume in the first place. 

**Environment**:
- k8scloudprovider/cinder-provisioner:latest
- Ubuntu 18.04.2 LTS
",open,False,2019-03-13 16:54:48,2019-03-13 16:54:49
cloud-provider-openstack,darcyllingyan,https://github.com/kubernetes/cloud-provider-openstack/issues/535,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/535,"The PV could be created with a new deployed cinder-csi, more than 1.5hour later, create the pv will report failed to provision volume  with StorageClass ""csi-sc-cinderplugin"": timed out waiting for the condition","Hi,
Initially I installed the cinder-csi driver, the pvc could be created and bound, after several hours or longer when I want to create a POD with the volume provided by CSI again, it always failed and report ```ProvisioningFailed    49s (x4 over 11m)     csi-cinderplugin_csi-provisioner-cinderplugin-0_9498d1df-459b-11e9-8537-36bea033277e  failed to provision volume with StorageClass ""csi-sc-cinderplugin"": timed out waiting for the condition```.

The log is below:
1.
```
# kubectl get pod -n kube-system
NAME                                         READY   STATUS    RESTARTS   AGE
csi-attacher-cinderplugin-0                  2/2     Running   0          3d22h
csi-nodeplugin-cinderplugin-4n6sc            2/2     Running   0          3d22h
csi-nodeplugin-cinderplugin-6c5x4            2/2     Running   0          3d22h
csi-nodeplugin-cinderplugin-rfkz6            2/2     Running   0          3d22h
csi-nodeplugin-cinderplugin-wpb8g            2/2     Running   0          3d22h
csi-provisioner-cinderplugin-0               2/2     Running   0          3d22h

# cat pvc-csi.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: csi-pvc-cinderplugin
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: ""csi-sc-cinderplugin""

# kubectl get pvc
NAME                   STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
csi-pvc-cinderplugin   Pending                                                                        csi-sc-cinderplugin   25m

# kubectl describe pvc csi-pvc-cinderplugin
Events:
  Type       Reason                Age                  From                                                                                  Message
  ----       ------                ----                 ----                                                                                  -------
  Normal     Provisioning          5m15s (x7 over 26m)  csi-cinderplugin_csi-provisioner-cinderplugin-0_9498d1df-459b-11e9-8537-36bea033277e  External provisioner is provisioning volume for claim ""default/csi-pvc-cinderplugin""
  Warning    ProvisioningFailed    111s (x7 over 22m)   csi-cinderplugin_csi-provisioner-cinderplugin-0_9498d1df-459b-11e9-8537-36bea033277e  failed to provision volume with StorageClass ""csi-sc-cinderplugin"": timed out waiting for the condition
  Normal     ExternalProvisioning  60s (x102 over 26m)  persistentvolume-controller                                                           waiting for a volume to be created, either by external provisioner ""csi-cinderplugin"" or manually created by system administrator

# kubectl get sc
NAME                       PROVISIONER                    AGE
cinder                     kubernetes.io/cinder           47h
cinder-az-nova (default)   kubernetes.io/cinder           47h
cinder-az-nova-xfs         kubernetes.io/cinder           47h
cinder-xfs                 kubernetes.io/cinder           47h
csi-sc-cinderplugin        csi-cinderplugin               47h
csi-sc-cinderplugin-nova   csi-cinderplugin               115m
local-storage              kubernetes.io/no-provisioner   47h
```
Below is the external-provisioner.log
[external_provisioner.log](https://github.com/kubernetes/cloud-provider-openstack/files/2975168/external_provisioner.log)

The external-attacher.log
[external-attacher.log](https://github.com/kubernetes/cloud-provider-openstack/files/2975170/external-attacher.log)

The cinder_csi-provisioner log:
```
I0317 14:39:10.909183       1 driver.go:56] Driver: cinder.csi.openstack.org version: 1.0.0
I0317 14:39:10.909330       1 driver.go:88] Enabling controller service capability: LIST_VOLUMES
I0317 14:39:10.909335       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_VOLUME
I0317 14:39:10.909340       1 driver.go:88] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
I0317 14:39:10.909344       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
I0317 14:39:10.909348       1 driver.go:88] Enabling controller service capability: LIST_SNAPSHOTS
I0317 14:39:10.909353       1 driver.go:100] Enabling volume access mode: SINGLE_NODE_WRITER
I0317 14:39:10.909376       1 driver.go:110] Enabling node service capability: STAGE_UNSTAGE_VOLUME
I0317 14:39:10.909628       1 server.go:108] Listening for connections on address: &net.UnixAddr{Name:""/csi/csi.sock"", Net:""unix""}
E0317 15:53:57.461244       1 utils.go:77] GRPC error: Cannot delete the volume ""0176c31c-5689-4d54-bbfa-99dcd72e3231"", it's still attached to a node
E0317 15:56:29.773616       1 utils.go:77] GRPC error: Cannot delete the volume ""6f1b0e00-ce49-4e1d-8780-ef2764a98cbf"", it's still attached to a node
E0318 01:01:11.044757       1 utils.go:77] GRPC error: Cannot delete the volume ""ed5bde86-ef5a-48e9-ba45-00e9f395ce04"", it's still attached to a node
```

The cinder_csi-attacher log.
```
I0317 14:39:10.709975       1 driver.go:56] Driver: cinder.csi.openstack.org version: 1.0.0
I0317 14:39:10.710119       1 driver.go:88] Enabling controller service capability: LIST_VOLUMES
I0317 14:39:10.710125       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_VOLUME
I0317 14:39:10.710130       1 driver.go:88] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
I0317 14:39:10.710134       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
I0317 14:39:10.710138       1 driver.go:88] Enabling controller service capability: LIST_SNAPSHOTS
I0317 14:39:10.710143       1 driver.go:100] Enabling volume access mode: SINGLE_NODE_WRITER
I0317 14:39:10.710149       1 driver.go:110] Enabling node service capability: STAGE_UNSTAGE_VOLUME
I0317 14:39:10.710403       1 server.go:108] Listening for connections on address: &net.UnixAddr{Name:""/csi/csi.sock"", Net:""unix""}
E0318 01:29:54.885628       1 utils.go:77] GRPC error: Volume ""4b9d0bd8-68f9-486c-be9f-b6ed774609fd"" failed to be attached within the alloted time
E0318 01:30:09.710929       1 utils.go:77] GRPC error: Volume ""4b9d0bd8-68f9-486c-be9f-b6ed774609fd"" failed to be attached within the alloted time
E0318 01:30:27.790058       1 utils.go:77] GRPC error: Volume ""4b9d0bd8-68f9-486c-be9f-b6ed774609fd"" failed to be attached within the alloted time
E0318 01:30:39.207716       1 utils.go:77] GRPC error: Volume ""4b9d0bd8-68f9-486c-be9f-b6ed774609fd"" failed to be attached within the alloted time
E0318 01:30:54.239216       1 utils.go:77] GRPC error: Volume ""4b9d0bd8-68f9-486c-be9f-b6ed774609fd"" failed to be attached within the alloted time

```

**note: 1. When the cinder-csi driver was created initially, the pvc could be created successfully, several days(hours) later when I want to create new pvc again, it reports the error. After I delete the pvc which had been created successfully and re-install again, it also couldn't be created.**

**2. If I delete the cinder-csi-plugin and re-install it again, and then create the pvc, the pvc could be created successfully.**

**3. The 1.5hour isn't an exactly time, in different clusters the provision failed time is different**

Can you help to check it about the root cause? I have no idea about it after checking several days, thanks very much!

THanks",closed,False,2019-03-17 13:22:43,2019-03-26 06:48:19
cloud-provider-openstack,gman0,https://github.com/kubernetes/cloud-provider-openstack/pull/536,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/536,[WIP] CSI Manila driver,"**What this PR does / why we need it**:
Adds CSI Manila driver, currently supports only CephFS and NFS. I've only tested with CephFS so far.

**Which issue this PR fixes**: fixes #394 

### Design considerations / a brief description of what does what

#### ""proxy"" architecture

This CSI driver acts as a provisioner of Manila shares, all node-related operations (attachments, mounts) are carried out by another CSI driver, dedicated for that particular filesystem. This is achieved by forwarding CSI RPCs to the other CSI driver.

For example, creating and mounting a CephFS Manila share would go like this:

CO requests a volume provisioning with `CreateVolume`:
```
                        +--------------------+
                        |       MASTER       |
+------+  CreateVolume  |  +--------------+  |
|  CO  +------------------>+  csi-manila  |  |
+------+                |  +--------------+  |
                        +--------------------+

1. Authenticate with Manila v2 client
2. Create a CephFS share
3. Create a cephx access rule
```

And when CO publishes the volume onto a node with `NodePublishVolume`:
```
                             +--------------------------------------+
                             |                 NODE                 |
+------+  NodePublishVolume  |  +--------------+                    |
|  CO  +----------------------->+  csi-manila  |                    |
+------+                     |  +------+-------+                    |
                             |         |                            |
                             |         | FORWARD NodePublishVolume  |
                             |         V                            |
                             |  +------+-------+                    |
                             |  |  csi-cephfs  |                    |
                             |  +--------------+                    |
                             +--------------------------------------+

1. Authenticate with Manila v2 client
2. Retrieve the CephFS share
3. Retrieve the cephx access rule
4. Connect to csi-cephfs socket
5. Call csi-cephfs's NodePublishVolume, return its response
```

So the intention here is to deal with only Manila side of things, and leave all filesystem specifics to another CSI driver that's specialized for that particular fs.

If connection with the proxy'd CSI driver cannot be established, csi-manila will return an error and will let the CO retry.

Downsides:

From the documentation
> A single instance of the driver may serve only a single Manila share protocol. To support multiple share protocols, multiple deployments of the driver need to be made. In order to avoid deployment collisions, each instance of the driver should be named differently, e.g. `csi-manila-cephfs`, `csi-manila-nfs`.

The initial idea was to encompass all Manila share protocols within a single instance of csi-manila. Due to limitations of CSI, this cannot be achieved and there will have to be multiple instances of csi-manila running in order to support multiple share protocols, one for each. There are couple of RPCs in Node Service that don't bring enough context to decide which proxy'd driver that particular RPC should be forwarded to (e.g. `NodeGetCapabilities` would be ambiguous and we can't know which proxy'd driver should answer) => therefore with current design, there's only a single proxy'd driver per csi-manila instance. Such ""multiplexer"" design is out-of-scope for CSI anyway IMHO.

#### Concurrency

`CreateVolume` is so far the most time-expensive operation in the driver because it waits till the share is in _Available_ state, and for CephFS shares this waiting is even more pronounced because it waits till the Ceph key for the share is available. This means that the CO may retry the `CreateVolume` RPC for the same volume many times before it's actually done creating.

In Kubernetes, this means there will be multiple `CreateVolume` RPCs for the same volume in parallel - which, in our case, doesn't help the situation at all and would only create more unnecessary load on Manila itself. To guard against situations where there's a long running operation, some form of locking needs to be employed.

Using a simple mutex per volume ID would result in this:
```
  R req1             req2             req3
T -----------------+----------------+----------------+
01| LOCK           |                |                |
02| op1            |                |                |
03| op2            | LOCK...wait    |                |
04| op3            | wait...        |                |
05| UNLOCK         | wait...        | LOCK...wait    |
06| RETURN success + LOCK           | wait...        |
07|                  op1            | wait...        |
08|                  op2            | wait...        |
09|                  op3            | wait...        |
21|                  UNLOCK         | wait...        |
22|                  RETURN success + LOCK           |
23|                                   op1            |
24|                                   op2            |
25|                                   op3            |
26|                                   UNLOCK         |
27|                                   RETURN SUCCESS +
```
_(I really hope these diagrams make at least a bit of sense)_ On the Y axis, there's `T` for time, and on the X axis there's `R` for a request. The first request `req1` locks the mutex and launches a long running operation `op` (taking up 3 time units). The call times out, so the CO decides to retry with `req2` but needs to wait till `req1` finishes so that `req2` may acquire the lock. This pattern may repeat until the call doesn't time-out. There are two problems here: (a) it may take a long time till CO is satisfied with the answer, (b) the long running operation `op` is being run in all of those retries.

`pkg/csi/manila/responsebroker` from this PR tries to tackle the problem (b), and hopes to mitigate (a):
```
  R req1                                             req2                          req3
T -------------------------------------------------+-----------------------------+-----------------------------+
01| LOCK(acq-or-create)                            |                             |                             |
02| op1                                            |                             |                             |
03| op2                                            | LOCK(ack-or-create)...wait  |                             |
04| op3                                            | wait...                     |                             |
05| UNLOCK, write success, wait till all Rs finish | wait...                     | LOCK(ack-or-create)...wait  |
06| wait...                                        | LOCK, read response:success | wait...                     |
07| wait...                                        | UNLOCK                      | wait...                     |
08| wait...                                        | RETURN SUCCESS              + LOCK, read response:success |
09| RETURN SUCCESS                                 +                               UNLOCK                      |
10|                                                                                RETURN SUCCESS              +
```
The same scenario as above: `req1` locks the mutex and runs the long running operation `op` and times-out. `req2` will wait till it can acquire the lock, but instead of running `op` again, it reads the response from `req1` and if it's successful, it immediately exits with success. The `responsebroker` basically caches the result of an operation and propagates it to multiple readers. 

This mechanism could be also used to implement a form of journaling: if an error occurs in one call, the future retries would pick up the operation in the exact spot where the previous call had failed - again, saving some load from the backend. Maybe in another PR.

### To be done in this PR

* If an operator wishes to use a custom certificate for authentication, they may use the `os-certAuthority` parameter in Secrets to inject the contents of a PEM cert file. This is the way manila-provisioner does it. The problem here is the size limit: the [CSI spec allows up to 128 bytes](https://github.com/container-storage-interface/spec/blob/master/spec.md#size-limits) in `string` fields and a certificate can be obviously larger than that,
* `List*` and `GetCapacity` CSI RPCs need OpenStack credentials, but these RPCs don't carry any Secrets fields. CSI Cinder driver solves this by supplying the driver with a file containing the credentials. It doesn't rely on Secrets supplied by the CO.

**Release note**:
```release-note
added CSI Manila driver
```

/cc @tsmetana @dims @adisky ",open,True,2019-03-17 20:43:16,2019-04-01 18:44:59
cloud-provider-openstack,dricoco,https://github.com/kubernetes/cloud-provider-openstack/issues/537,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/537,latest on docker.io is version 0.2,"<!-- This form is for bug reports and feature requests! -->

/kind bug

**What happened**:
pulled cinder docker.io/k8scloudprovider/cinder-csi-plugin:latest and got version 0.2
**What you expected to happen**:
got the latest version of cinder-csi
**How to reproduce it (as minimally and precisely as possible)**:
this is a followup on closed issue #514 
I think a some moment on docker the latest version is the good one and at some other it's 0.2.
I wanted to exclude any kubernetes config bug so I just ran 

> docker run -i -t docker.io/k8scloudprovider/cinder-csi-plugin:latest  /bin/sh                                                                 [153/4618]
Unable to find image 'k8scloudprovider/cinder-csi-plugin:latest' locally
latest: Pulling from k8scloudprovider/cinder-csi-plugin
840caab23da4: Pull complete
56767d3c5823: Pull complete
707da695f2f7: Pull complete
Digest: sha256:d1f0feac6356b0a52b93ff6889b7b2bd63ae75112ec0a95f41e7027f20dcada7
Status: Downloaded newer image for k8scloudprovider/cinder-csi-plugin:latest

and then inside the container 

> /bin/cinder-csi-plugin  --nodeid=$(NODE_ID) --cloud-config=$(CLOUD_CONFI) --endpoint=zer
sh: NODE_ID: command not found
sh: CLOUD_CONFI: command not found
I0318 19:49:55.657173      39 driver.go:49] Driver: csi-cinderplugin version: 0.2.0

as you can see the csi cinderplugin version is not good.

",closed,False,2019-03-18 19:59:29,2019-03-29 06:39:07
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/538,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/538,Use Alpine for Cinder CSI,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Cinder CSI Driver currently uses centos as the base image, which is heavier
This PR proposes to use Alpine as the base image

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
`NONE`
",closed,True,2019-03-19 09:03:07,2019-03-20 11:28:23
cloud-provider-openstack,dricoco,https://github.com/kubernetes/cloud-provider-openstack/issues/539,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/539,enable log ,"<!-- This form is for bug reports and feature requests! -->

> /kind feature


**What happened**:
I have to debug communication between the pod and my openstack. I cannot pass -v4 or -v5 to the kubernetes manifest

```
# kubectl exec -it csi-provisioner-cinderplugin-0 -n kube-system -c cinder -- /bin/sh

sh-4.2# /bin/cinder-csi-plugin
Error: required flag(s) ""cloud-config"", ""endpoint"", ""nodeid"" not set
Usage:
  Cinder [flags]

Flags:
      --cloud-config string     CSI driver cloud config
      --cluster string          The identifier of the cluster that the plugin is running in.
      --endpoint string         CSI endpoint
  -h, --help                    help for Cinder
      --httptest.serve string   if non-empty, httptest.NewServer serves on this address and blocks
      --nodeid string           node id
```


**What you expected to happen**:
I expected that -v would be available in the flags

**How to reproduce it (as minimally and precisely as possible)**:
Just launch the manifest and check /bin/cinder-csi-plugin inside one of the pods

",closed,False,2019-03-19 09:56:18,2019-03-25 12:01:09
cloud-provider-openstack,joroec,https://github.com/kubernetes/cloud-provider-openstack/issues/540,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/540,Nodes do not get initialized although CCM is running,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

When I setup a Kubernetes 1.13.4 cluster with the external cloud controller manager for OpenStack, I experience the following problem: After following the instructions at https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-controller-manager-with-kubeadm.md the OpenStack cloud controller manager comes up. However, the nodes never get initialized, the taint ""node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule"" is never removed and no internal IP addresses are assigned.

My kubadm config is:
```yaml
apiVersion: kubeadm.k8s.io/v1alpha3
kind: InitConfiguration
bootstrapTokens:
- token: ""xxx.xxxx""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
nodeRegistration:
  kubeletExtraArgs:
    cloud-provider: ""external""
---
apiVersion: kubeadm.k8s.io/v1alpha3
kind: ClusterConfiguration
etcd:
  local:
    # imageRepository: ""k8s.gcr.io""
    # imageTag: ""3.2.24""
    # dataDir: ""/var/lib/etcd""
    extraArgs:
      listen-client-urls: ""https://127.0.0.1:2379,https://xxx:2379""
      advertise-client-urls: ""https://xxx:2379""
      listen-peer-urls: ""https://xxxx:2380""
      initial-advertise-peer-urls: ""https://xxxx:2380""
      initial-cluster: ""master-1=https://xxxx:2380""
      initial-cluster-state: ""new""
    serverCertSANs:
      - master-1
      - xxxx
    peerCertSANs:
      - master-1
      - xxxx
networking:
  # This CIDR is a Calico default. Substitute or remove for your CNI provider.
  podSubnet: 10.233.0.0/16
kubernetesVersion: v1.13.4
controlPlaneEndpoint: ""xxxx:6443""
apiServerExtraArgs:
    authorization-mode: ""Node,RBAC""
    enable-admission-plugins: NodeRestriction,Initializers
    runtime-config: admissionregistration.k8s.io/v1alpha1
apiServerCertSANs:
- ""xxxx""
controllerManagerExtraArgs:
  cloud-provider: ""external""
  cloud-config: /etc/kubernetes/cloud.config
  allocate-node-cidrs: ""true""
  cluster-cidr: 10.233.0.0/16
  external-cloud-volume-plugin: openstack
controllerManagerExtraVolumes:
- name: ""cloud-config""
  hostPath: ""/etc/kubernetes/cloud.config""
  mountPath: ""/etc/kubernetes/cloud.config""
  writable: true
  pathType: File
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
# kubelet specific options here
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
# kube-proxy specific options here
```

I could not find any very useful information or error messages in the log files. Here are several log excerpts that might be useful:


Logs of external cloud controller manager for OpenStack:
```
Flag --address has been deprecated, see --bind-address instead.
I0319 13:42:48.680089       1 flags.go:33] FLAG: --address=""127.0.0.1""
I0319 13:42:48.680228       1 flags.go:33] FLAG: --allocate-node-cidrs=""false""
I0319 13:42:48.680274       1 flags.go:33] FLAG: --allow-untagged-cloud=""false""
I0319 13:42:48.680368       1 flags.go:33] FLAG: --alsologtostderr=""false""
I0319 13:42:48.680412       1 flags.go:33] FLAG: --authentication-kubeconfig=""""
I0319 13:42:48.680472       1 flags.go:33] FLAG: --authentication-skip-lookup=""false""
I0319 13:42:48.680503       1 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl=""10s""
I0319 13:42:48.680548       1 flags.go:33] FLAG: --authentication-tolerate-lookup-failure=""false""
I0319 13:42:48.680604       1 flags.go:33] FLAG: --authorization-always-allow-paths=""[/healthz]""
I0319 13:42:48.680691       1 flags.go:33] FLAG: --authorization-kubeconfig=""""
I0319 13:42:48.680739       1 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl=""10s""
I0319 13:42:48.680766       1 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl=""10s""
I0319 13:42:48.680813       1 flags.go:33] FLAG: --bind-address=""0.0.0.0""
I0319 13:42:48.680860       1 flags.go:33] FLAG: --cert-dir=""""
I0319 13:42:48.680893       1 flags.go:33] FLAG: --cidr-allocator-type=""RangeAllocator""
I0319 13:42:48.680936       1 flags.go:33] FLAG: --client-ca-file=""""
I0319 13:42:48.680983       1 flags.go:33] FLAG: --cloud-config=""/etc/config/cloud.config""
I0319 13:42:48.681030       1 flags.go:33] FLAG: --cloud-provider=""openstack""
I0319 13:42:48.681102       1 flags.go:33] FLAG: --cluster-cidr=""""
I0319 13:42:48.681147       1 flags.go:33] FLAG: --cluster-name=""kubernetes""
I0319 13:42:48.681189       1 flags.go:33] FLAG: --concurrent-service-syncs=""1""
I0319 13:42:48.681227       1 flags.go:33] FLAG: --configure-cloud-routes=""true""
I0319 13:42:48.681261       1 flags.go:33] FLAG: --contention-profiling=""false""
I0319 13:42:48.681285       1 flags.go:33] FLAG: --controller-start-interval=""0s""
I0319 13:42:48.681307       1 flags.go:33] FLAG: --controllers=""[*]""
I0319 13:42:48.681371       1 flags.go:33] FLAG: --external-cloud-volume-plugin=""""
I0319 13:42:48.681403       1 flags.go:33] FLAG: --feature-gates=""""
I0319 13:42:48.681452       1 flags.go:33] FLAG: --help=""false""
I0319 13:42:48.681489       1 flags.go:33] FLAG: --http2-max-streams-per-connection=""0""
I0319 13:42:48.681521       1 flags.go:33] FLAG: --kube-api-burst=""30""
I0319 13:42:48.681578       1 flags.go:33] FLAG: --kube-api-content-type=""application/vnd.kubernetes.protobuf""
I0319 13:42:48.681614       1 flags.go:33] FLAG: --kube-api-qps=""20""
I0319 13:42:48.681664       1 flags.go:33] FLAG: --kubeconfig=""""
I0319 13:42:48.681721       1 flags.go:33] FLAG: --leader-elect=""true""
I0319 13:42:48.681768       1 flags.go:33] FLAG: --leader-elect-lease-duration=""15s""
I0319 13:42:48.681853       1 flags.go:33] FLAG: --leader-elect-renew-deadline=""10s""
I0319 13:42:48.681902       1 flags.go:33] FLAG: --leader-elect-resource-lock=""endpoints""
I0319 13:42:48.681927       1 flags.go:33] FLAG: --leader-elect-retry-period=""2s""
I0319 13:42:48.681951       1 flags.go:33] FLAG: --log-backtrace-at="":0""
I0319 13:42:48.682006       1 flags.go:33] FLAG: --log-dir=""""
I0319 13:42:48.682045       1 flags.go:33] FLAG: --log-file=""""
I0319 13:42:48.682090       1 flags.go:33] FLAG: --log-flush-frequency=""5s""
I0319 13:42:48.682214       1 flags.go:33] FLAG: --logtostderr=""true""
I0319 13:42:48.682245       1 flags.go:33] FLAG: --master=""""
I0319 13:42:48.682515       1 flags.go:33] FLAG: --min-resync-period=""12h0m0s""
I0319 13:42:48.682554       1 flags.go:33] FLAG: --node-monitor-period=""5s""
I0319 13:42:48.682613       1 flags.go:33] FLAG: --node-status-update-frequency=""5m0s""
I0319 13:42:48.682648       1 flags.go:33] FLAG: --node-sync-period=""0s""
I0319 13:42:48.682735       1 flags.go:33] FLAG: --port=""0""
I0319 13:42:48.682768       1 flags.go:33] FLAG: --profiling=""false""
I0319 13:42:48.682865       1 flags.go:33] FLAG: --requestheader-allowed-names=""[]""
I0319 13:42:48.682972       1 flags.go:33] FLAG: --requestheader-client-ca-file=""""
I0319 13:42:48.683106       1 flags.go:33] FLAG: --requestheader-extra-headers-prefix=""[x-remote-extra-]""
I0319 13:42:48.683212       1 flags.go:33] FLAG: --requestheader-group-headers=""[x-remote-group]""
I0319 13:42:48.683309       1 flags.go:33] FLAG: --requestheader-username-headers=""[x-remote-user]""
I0319 13:42:48.683360       1 flags.go:33] FLAG: --route-reconciliation-period=""10s""
I0319 13:42:48.683386       1 flags.go:33] FLAG: --secure-port=""10258""
I0319 13:42:48.683411       1 flags.go:33] FLAG: --skip-headers=""false""
I0319 13:42:48.683552       1 flags.go:33] FLAG: --stderrthreshold=""0""
I0319 13:42:48.683640       1 flags.go:33] FLAG: --tls-cert-file=""""
I0319 13:42:48.683683       1 flags.go:33] FLAG: --tls-cipher-suites=""[]""
I0319 13:42:48.683730       1 flags.go:33] FLAG: --tls-min-version=""""
I0319 13:42:48.683772       1 flags.go:33] FLAG: --tls-private-key-file=""""
I0319 13:42:48.683905       1 flags.go:33] FLAG: --tls-sni-cert-key=""[]""
I0319 13:42:48.683980       1 flags.go:33] FLAG: --use-service-account-credentials=""true""
I0319 13:42:48.684052       1 flags.go:33] FLAG: --v=""5""
I0319 13:42:48.684090       1 flags.go:33] FLAG: --version=""false""
I0319 13:42:48.684116       1 flags.go:33] FLAG: --vmodule=""""
I0319 13:42:49.087733       1 serving.go:319] Generated self-signed cert in-memory
W0319 13:42:49.768546       1 client_config.go:549] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0319 13:42:49.774518       1 controllermanager.go:113] Version: v0.0.0-master+$Format:%h$
W0319 13:42:49.774617       1 plugins.go:118] WARNING: openstack built-in cloud provider is now deprecated. Please use 'external' cloud provider for openstack: https://github.com/kubernetes/cloud-provider-openstack
I0319 13:42:49.774998       1 openstack.go:187] AuthURL: xxx
I0319 13:42:49.775101       1 openstack.go:188] Username: xxx
I0319 13:42:49.775169       1 openstack.go:189] UserID: 
I0319 13:42:49.775274       1 openstack.go:190] TenantID: xxx
I0319 13:42:49.775404       1 openstack.go:191] TenantName: 
I0319 13:42:49.775484       1 openstack.go:192] DomainName: xx
I0319 13:42:49.775555       1 openstack.go:193] DomainID: 
I0319 13:42:49.775673       1 openstack.go:194] TrustID: 
I0319 13:42:49.775756       1 openstack.go:195] Region: xxx
I0319 13:42:49.775810       1 openstack.go:196] CAFile: 
I0319 13:42:55.006525       1 healthz.go:116] Installing healthz checkers:""leaderElection""
I0319 13:42:55.007402       1 secure_serving.go:116] Serving securely on [::]:10258
I0319 13:42:55.007806       1 leaderelection.go:217] attempting to acquire leader lease  kube-system/cloud-controller-manager...
I0319 13:42:55.020407       1 leaderelection.go:227] successfully acquired lease kube-system/cloud-controller-manager
I0319 13:42:55.020899       1 controllermanager.go:230] Starting ""route""
I0319 13:42:55.021057       1 core.go:91] Will not configure cloud provider routes for allocate-node-cidrs: false, configure-cloud-routes: true.
W0319 13:42:55.021159       1 controllermanager.go:237] Skipping ""route""
I0319 13:42:55.021333       1 controllermanager.go:230] Starting ""cloud-node""
I0319 13:42:55.021195       1 event.go:209] Event(v1.ObjectReference{Kind:""Endpoints"", Namespace:""kube-system"", Name:""cloud-controller-manager"", UID:""e61ccf2d-4a4c-11e9-b6ca-fa163ee25bf8"", APIVersion:""v1"", ResourceVersion:""760"", FieldPath:""""}): type: 'Normal' reason: 'LeaderElection' master-1_e61c08b5-4a4c-11e9-b016-fa163ee25bf8 became leader
I0319 13:42:55.026428       1 reflector.go:123] Starting reflector *v1.Secret (0s) from k8s.io/client-go/tools/watch/informerwatcher.go:113
I0319 13:42:55.026460       1 reflector.go:161] Listing and watching *v1.Secret from k8s.io/client-go/tools/watch/informerwatcher.go:113
E0319 13:42:55.033263       1 reflector.go:270] k8s.io/client-go/tools/watch/informerwatcher.go:113: Failed to watch *v1.Secret: unknown (get secrets)
I0319 13:42:55.041004       1 node_controller.go:79] Sending events to api server.
I0319 13:42:55.041352       1 controllermanager.go:240] Started ""cloud-node""
I0319 13:42:55.041531       1 controllermanager.go:230] Starting ""cloud-node-lifecycle""
I0319 13:42:55.041488       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:42:55.041768       1 openstack_instances.go:56] Claiming to support Instances
I0319 13:42:55.042547       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:42:55.044237       1 node_controller.go:141] This node master-1 is still tainted. Will not process.
I0319 13:42:55.045341       1 reflector.go:123] Starting reflector *v1.Secret (0s) from k8s.io/client-go/tools/watch/informerwatcher.go:113
I0319 13:42:55.045611       1 reflector.go:161] Listing and watching *v1.Secret from k8s.io/client-go/tools/watch/informerwatcher.go:113
E0319 13:42:55.050676       1 reflector.go:270] k8s.io/client-go/tools/watch/informerwatcher.go:113: Failed to watch *v1.Secret: unknown (get secrets)
I0319 13:42:55.052216       1 node_lifecycle_controller.go:77] Sending events to api server
I0319 13:42:55.052394       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:42:55.052409       1 openstack_instances.go:56] Claiming to support Instances
I0319 13:42:55.052421       1 controllermanager.go:240] Started ""cloud-node-lifecycle""
I0319 13:42:55.052443       1 controllermanager.go:230] Starting ""service""
I0319 13:42:55.052609       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:42:55.052633       1 openstack_instances.go:56] Claiming to support Instances
I0319 13:42:55.064725       1 reflector.go:123] Starting reflector *v1.Secret (0s) from k8s.io/client-go/tools/watch/informerwatcher.go:113
I0319 13:42:55.064749       1 reflector.go:161] Listing and watching *v1.Secret from k8s.io/client-go/tools/watch/informerwatcher.go:113
E0319 13:42:55.078081       1 reflector.go:270] k8s.io/client-go/tools/watch/informerwatcher.go:113: Failed to watch *v1.Secret: unknown (get secrets)
I0319 13:42:56.078359       1 reflector.go:161] Listing and watching *v1.Secret from k8s.io/client-go/tools/watch/informerwatcher.go:113
E0319 13:42:56.084163       1 reflector.go:270] k8s.io/client-go/tools/watch/informerwatcher.go:113: Failed to watch *v1.Secret: unknown (get secrets)
I0319 13:42:56.095115       1 openstack.go:698] openstack.LoadBalancer() called
I0319 13:42:56.095701       1 openstack.go:728] Claiming to support LoadBalancer
I0319 13:42:56.095807       1 service_controller.go:183] Starting service controller
I0319 13:42:56.095857       1 controller_utils.go:1027] Waiting for caches to sync for service controller
I0319 13:42:56.095939       1 controllermanager.go:240] Started ""service""
I0319 13:42:56.098975       1 reflector.go:123] Starting reflector *v1.Node (15h32m25.242237095s) from k8s.io/client-go/informers/factory.go:133
I0319 13:42:56.099037       1 reflector.go:161] Listing and watching *v1.Node from k8s.io/client-go/informers/factory.go:133
I0319 13:42:56.099173       1 reflector.go:123] Starting reflector *v1.Service (30s) from k8s.io/client-go/informers/factory.go:133
I0319 13:42:56.101152       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:42:56.101184       1 openstack_instances.go:56] Claiming to support Instances
I0319 13:42:56.099194       1 reflector.go:161] Listing and watching *v1.Service from k8s.io/client-go/informers/factory.go:133
I0319 13:42:56.106572       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:42:56.106746       1 openstack_instances.go:56] Claiming to support Instances
I0319 13:42:56.196186       1 shared_informer.go:123] caches populated
I0319 13:42:56.196233       1 controller_utils.go:1034] Caches are synced for service controller
I0319 13:42:57.056194       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:42:59.067397       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:00.052944       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:43:00.053019       1 openstack_instances.go:56] Claiming to support Instances
I0319 13:43:01.084190       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:01.306688       1 service_controller.go:330] Not persisting unchanged LoadBalancerStatus for service kube-system/calico-typha to registry.
I0319 13:43:01.307047       1 service_controller.go:716] Finished syncing service ""kube-system/calico-typha"" (5.110654665s)
I0319 13:43:03.096532       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:05.053667       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:43:05.053913       1 openstack_instances.go:56] Claiming to support Instances
I0319 13:43:05.106780       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:06.364633       1 service_controller.go:330] Not persisting unchanged LoadBalancerStatus for service default/kubernetes to registry.
I0319 13:43:06.364668       1 service_controller.go:716] Finished syncing service ""default/kubernetes"" (5.057587987s)
I0319 13:43:07.115597       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:09.129547       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:10.054294       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:43:10.054362       1 openstack_instances.go:56] Claiming to support Instances
I0319 13:43:11.160284       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:11.425709       1 service_controller.go:330] Not persisting unchanged LoadBalancerStatus for service kube-system/kube-dns to registry.
I0319 13:43:11.426108       1 service_controller.go:716] Finished syncing service ""kube-system/kube-dns"" (5.061407407s)
I0319 13:43:11.756294       1 node_controller.go:287] Adding node label from cloud provider: beta.kubernetes.io/instance-type=302
I0319 13:43:11.756341       1 openstack.go:735] Claiming to support Zones
I0319 13:43:13.172549       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:15.054813       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:43:15.054869       1 openstack_instances.go:56] Claiming to support Instances
I0319 13:43:15.181903       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:17.191313       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:17.247671       1 openstack.go:777] The instance master-1 in zone {xxxx}
I0319 13:43:17.247963       1 node_controller.go:297] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/zone=xxxx
I0319 13:43:17.248117       1 node_controller.go:301] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/region=xxxx
I0319 13:43:17.316504       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:43:17.316546       1 openstack_instances.go:56] Claiming to support Instances
I0319 13:43:19.201419       1 leaderelection.go:258] successfully renewed lease kube-system/cloud-controller-manager
I0319 13:43:20.055203       1 openstack_instances.go:48] openstack.Instances() called
I0319 13:43:20.055279       1 openstack_instances.go:56] Claiming to support Instances
...
```

Logs of kube-controller-manager:
```
Flag --address has been deprecated, see --bind-address instead.
I0319 13:39:44.516837       1 serving.go:318] Generated self-signed cert in-memory
I0319 13:39:45.230961       1 controllermanager.go:151] Version: v1.13.4
I0319 13:39:45.231601       1 secure_serving.go:116] Serving securely on [::]:10257
I0319 13:39:45.232066       1 deprecated_insecure_serving.go:51] Serving insecurely on 127.0.0.1:10252
I0319 13:39:45.232273       1 leaderelection.go:205] attempting to acquire leader lease  kube-system/kube-controller-manager...
I0319 13:39:45.261924       1 leaderelection.go:214] successfully acquired lease kube-system/kube-controller-manager
I0319 13:39:45.263136       1 event.go:221] Event(v1.ObjectReference{Kind:""Endpoints"", Namespace:""kube-system"", Name:""kube-controller-manager"", UID:""75021a2a-4a4c-11e9-b6ca-fa163ee25bf8"", APIVersion:""v1"", ResourceVersion:""257"", FieldPath:""""}): type: 'Normal' reason: 'LeaderElection' master-1_74fe95ba-4a4c-11e9-a188-fa163ee25bf8 became leader
W0319 13:39:45.320978       1 plugins.go:118] WARNING: openstack built-in cloud provider is now deprecated. Please use 'external' cloud provider for openstack: https://github.com/kubernetes/cloud-provider-openstack
I0319 13:39:50.562967       1 controller_utils.go:1027] Waiting for caches to sync for tokens controller
I0319 13:39:50.663993       1 controller_utils.go:1034] Caches are synced for tokens controller
I0319 13:39:50.714862       1 controllermanager.go:516] Started ""serviceaccount""
I0319 13:39:50.715132       1 serviceaccounts_controller.go:115] Starting service account controller
I0319 13:39:50.715230       1 controller_utils.go:1027] Waiting for caches to sync for service account controller
I0319 13:39:50.769228       1 controllermanager.go:516] Started ""statefulset""
I0319 13:39:50.769653       1 stateful_set.go:151] Starting stateful set controller
I0319 13:39:50.769876       1 controller_utils.go:1027] Waiting for caches to sync for stateful set controller
I0319 13:39:50.810560       1 controllermanager.go:516] Started ""tokencleaner""
I0319 13:39:50.811198       1 tokencleaner.go:116] Starting token cleaner controller
I0319 13:39:50.811253       1 controller_utils.go:1027] Waiting for caches to sync for token_cleaner controller
I0319 13:39:50.850631       1 controllermanager.go:516] Started ""podgc""
I0319 13:39:50.850936       1 gc_controller.go:76] Starting GC controller
I0319 13:39:50.851039       1 controller_utils.go:1027] Waiting for caches to sync for GC controller
I0319 13:39:50.888431       1 controllermanager.go:516] Started ""pvc-protection""
I0319 13:39:50.888805       1 pvc_protection_controller.go:99] Starting PVC protection controller
I0319 13:39:50.888836       1 controller_utils.go:1027] Waiting for caches to sync for PVC protection controller
I0319 13:39:50.913031       1 controller_utils.go:1034] Caches are synced for token_cleaner controller
W0319 13:39:50.947606       1 garbagecollector.go:649] failed to discover preferred resources: the cache has not been filled yet
I0319 13:39:50.948370       1 controllermanager.go:516] Started ""garbagecollector""
I0319 13:39:50.949045       1 garbagecollector.go:133] Starting garbage collector controller
I0319 13:39:50.949061       1 controller_utils.go:1027] Waiting for caches to sync for garbage collector controller
I0319 13:39:50.949083       1 graph_builder.go:308] GraphBuilder running
I0319 13:39:51.018727       1 controllermanager.go:516] Started ""namespace""
I0319 13:39:51.018904       1 namespace_controller.go:186] Starting namespace controller
I0319 13:39:51.018921       1 controller_utils.go:1027] Waiting for caches to sync for namespace controller
I0319 13:39:51.255229       1 controllermanager.go:516] Started ""ttl""
I0319 13:39:51.255307       1 ttl_controller.go:116] Starting TTL controller
I0319 13:39:51.255325       1 controller_utils.go:1027] Waiting for caches to sync for TTL controller
I0319 13:39:51.622698       1 controllermanager.go:516] Started ""attachdetach""
I0319 13:39:51.622814       1 attach_detach_controller.go:315] Starting attach detach controller
I0319 13:39:51.623109       1 controller_utils.go:1027] Waiting for caches to sync for attach detach controller
I0319 13:39:51.882308       1 controllermanager.go:516] Started ""replicationcontroller""
I0319 13:39:51.882486       1 replica_set.go:182] Starting replicationcontroller controller
I0319 13:39:51.882619       1 controller_utils.go:1027] Waiting for caches to sync for ReplicationController controller
I0319 13:39:52.133234       1 controllermanager.go:516] Started ""clusterrole-aggregation""
I0319 13:39:52.133486       1 clusterroleaggregation_controller.go:148] Starting ClusterRoleAggregator
I0319 13:39:52.133611       1 controller_utils.go:1027] Waiting for caches to sync for ClusterRoleAggregator controller
I0319 13:39:52.375792       1 controllermanager.go:516] Started ""pv-protection""
W0319 13:39:52.376275       1 controllermanager.go:508] Skipping ""root-ca-cert-publisher""
I0319 13:39:52.376177       1 pv_protection_controller.go:81] Starting PV protection controller
I0319 13:39:52.376499       1 controller_utils.go:1027] Waiting for caches to sync for PV protection controller
I0319 13:39:52.624804       1 controllermanager.go:516] Started ""csrcleaner""
I0319 13:39:52.625164       1 cleaner.go:81] Starting CSR cleaner controller
I0319 13:39:52.875113       1 controllermanager.go:516] Started ""deployment""
I0319 13:39:52.875210       1 deployment_controller.go:152] Starting deployment controller
I0319 13:39:52.875221       1 controller_utils.go:1027] Waiting for caches to sync for deployment controller
I0319 13:39:53.189902       1 controllermanager.go:516] Started ""disruption""
I0319 13:39:53.190049       1 disruption.go:288] Starting disruption controller
I0319 13:39:53.190255       1 controller_utils.go:1027] Waiting for caches to sync for disruption controller
I0319 13:39:53.381438       1 controllermanager.go:516] Started ""job""
I0319 13:39:53.381538       1 job_controller.go:143] Starting job controller
I0319 13:39:53.381643       1 controller_utils.go:1027] Waiting for caches to sync for job controller
I0319 13:39:53.626277       1 controllermanager.go:516] Started ""replicaset""
I0319 13:39:53.626395       1 replica_set.go:182] Starting replicaset controller
I0319 13:39:53.626429       1 controller_utils.go:1027] Waiting for caches to sync for ReplicaSet controller
I0319 13:39:53.767714       1 controllermanager.go:516] Started ""csrsigning""
I0319 13:39:53.767777       1 certificate_controller.go:113] Starting certificate controller
I0319 13:39:53.767787       1 controller_utils.go:1027] Waiting for caches to sync for certificate controller
I0319 13:39:54.025084       1 controllermanager.go:516] Started ""bootstrapsigner""
I0319 13:39:54.025442       1 controller_utils.go:1027] Waiting for caches to sync for bootstrap_signer controller
I0319 13:39:54.272079       1 controllermanager.go:516] Started ""endpoint""
I0319 13:39:54.272453       1 endpoints_controller.go:149] Starting endpoint controller
I0319 13:39:54.272646       1 controller_utils.go:1027] Waiting for caches to sync for endpoint controller
I0319 13:39:54.525220       1 controllermanager.go:516] Started ""daemonset""
I0319 13:39:54.525335       1 daemon_controller.go:269] Starting daemon sets controller
I0319 13:39:54.525536       1 controller_utils.go:1027] Waiting for caches to sync for daemon sets controller
I0319 13:39:54.781006       1 controllermanager.go:516] Started ""cronjob""
I0319 13:39:54.781369       1 cronjob_controller.go:92] Starting CronJob Manager
I0319 13:39:54.918811       1 controllermanager.go:516] Started ""csrapproving""
I0319 13:39:54.919015       1 certificate_controller.go:113] Starting certificate controller
I0319 13:39:54.919139       1 controller_utils.go:1027] Waiting for caches to sync for certificate controller
I0319 13:39:55.174075       1 controllermanager.go:516] Started ""persistentvolume-expander""
W0319 13:39:55.174137       1 controllermanager.go:508] Skipping ""ttl-after-finished""
I0319 13:39:55.174364       1 expand_controller.go:153] Starting expand controller
I0319 13:39:55.174388       1 controller_utils.go:1027] Waiting for caches to sync for expand controller
I0319 13:39:55.479367       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for replicasets.extensions
I0319 13:39:55.479799       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for statefulsets.apps
I0319 13:39:55.480133       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for roles.rbac.authorization.k8s.io
I0319 13:39:55.480514       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for endpoints
I0319 13:39:55.480815       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for podtemplates
I0319 13:39:55.481171       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for events.events.k8s.io
I0319 13:39:55.482126       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for jobs.batch
I0319 13:39:55.482532       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for leases.coordination.k8s.io
I0319 13:39:55.483215       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for controllerrevisions.apps
I0319 13:39:55.483646       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for cronjobs.batch
I0319 13:39:55.484071       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for networkpolicies.networking.k8s.io
I0319 13:39:55.484392       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for rolebindings.rbac.authorization.k8s.io
I0319 13:39:55.485225       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for deployments.extensions
I0319 13:39:55.485596       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for limitranges
I0319 13:39:55.485962       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for daemonsets.apps
I0319 13:39:55.486284       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for deployments.apps
I0319 13:39:55.486572       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for horizontalpodautoscalers.autoscaling
I0319 13:39:55.486924       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for ingresses.extensions
I0319 13:39:55.487199       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for replicasets.apps
I0319 13:39:55.487496       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for poddisruptionbudgets.policy
I0319 13:39:55.488001       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for serviceaccounts
I0319 13:39:55.488336       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for daemonsets.extensions
E0319 13:39:55.488815       1 resource_quota_controller.go:171] initial monitor sync has error: couldn't start monitor for resource ""extensions/v1beta1, Resource=networkpolicies"": unable to monitor quota for resource ""extensions/v1beta1, Resource=networkpolicies""
I0319 13:39:55.489182       1 resource_quota_controller.go:276] Starting resource quota controller
I0319 13:39:55.489350       1 controller_utils.go:1027] Waiting for caches to sync for resource quota controller
I0319 13:39:55.489472       1 resource_quota_monitor.go:301] QuotaMonitor running
I0319 13:39:55.489120       1 controllermanager.go:516] Started ""resourcequota""
I0319 13:39:55.681825       1 node_ipam_controller.go:99] Sending events to api server.
I0319 13:40:05.695992       1 range_allocator.go:78] Sending events to api server.
I0319 13:40:05.696414       1 range_allocator.go:99] No Service CIDR provided. Skipping filtering out service addresses.
I0319 13:40:05.696460       1 range_allocator.go:105] Node master-1 has no CIDR, ignoring
I0319 13:40:05.696779       1 controllermanager.go:516] Started ""nodeipam""
I0319 13:40:05.696923       1 node_ipam_controller.go:168] Starting ipam controller
I0319 13:40:05.696972       1 controller_utils.go:1027] Waiting for caches to sync for node controller
I0319 13:40:05.711333       1 node_lifecycle_controller.go:272] Sending events to api server.
I0319 13:40:05.712169       1 node_lifecycle_controller.go:312] Controller is using taint based evictions.
I0319 13:40:05.712784       1 taint_manager.go:175] Sending events to api server.
I0319 13:40:05.713265       1 node_lifecycle_controller.go:378] Controller will taint node by condition.
I0319 13:40:05.713656       1 controllermanager.go:516] Started ""nodelifecycle""
I0319 13:40:05.713790       1 node_lifecycle_controller.go:423] Starting node controller
I0319 13:40:05.713834       1 controller_utils.go:1027] Waiting for caches to sync for taint controller
I0319 13:40:05.794472       1 controllermanager.go:516] Started ""persistentvolume-binder""
I0319 13:40:05.795001       1 pv_controller_base.go:271] Starting persistent volume controller
I0319 13:40:05.795038       1 controller_utils.go:1027] Waiting for caches to sync for persistent volume controller
I0319 13:40:05.908714       1 controllermanager.go:516] Started ""horizontalpodautoscaling""
I0319 13:40:05.912088       1 controller_utils.go:1027] Waiting for caches to sync for garbage collector controller
I0319 13:40:05.912406       1 horizontal.go:156] Starting HPA controller
I0319 13:40:05.941179       1 controller_utils.go:1027] Waiting for caches to sync for HPA controller
W0319 13:40:05.945062       1 actual_state_of_world.go:491] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=""master-1"" does not exist
E0319 13:40:05.949648       1 resource_quota_controller.go:437] failed to sync resource monitors: couldn't start monitor for resource ""extensions/v1beta1, Resource=networkpolicies"": unable to monitor quota for resource ""extensions/v1beta1, Resource=networkpolicies""
I0319 13:40:05.971012       1 controller_utils.go:1034] Caches are synced for certificate controller
I0319 13:40:05.978376       1 controller_utils.go:1034] Caches are synced for PV protection controller
I0319 13:40:05.984801       1 controller_utils.go:1034] Caches are synced for TTL controller
I0319 13:40:05.988806       1 controller_utils.go:1034] Caches are synced for GC controller
I0319 13:40:05.989402       1 controller_utils.go:1034] Caches are synced for stateful set controller
I0319 13:40:05.991245       1 controller_utils.go:1034] Caches are synced for job controller
I0319 13:40:05.991258       1 controller_utils.go:1034] Caches are synced for ReplicationController controller
I0319 13:40:05.991265       1 controller_utils.go:1034] Caches are synced for PVC protection controller
I0319 13:40:05.992012       1 controller_utils.go:1034] Caches are synced for expand controller
I0319 13:40:05.992022       1 controller_utils.go:1034] Caches are synced for deployment controller
I0319 13:40:05.992382       1 controller_utils.go:1034] Caches are synced for endpoint controller
I0319 13:40:05.999570       1 controller_utils.go:1034] Caches are synced for persistent volume controller
I0319 13:40:06.000446       1 controller_utils.go:1034] Caches are synced for node controller
I0319 13:40:06.000519       1 range_allocator.go:157] Starting range CIDR allocator
I0319 13:40:06.000584       1 controller_utils.go:1027] Waiting for caches to sync for cidrallocator controller
I0319 13:40:06.014299       1 controller_utils.go:1034] Caches are synced for taint controller
I0319 13:40:06.014605       1 node_lifecycle_controller.go:1222] Initializing eviction metric for zone: 
W0319 13:40:06.014753       1 node_lifecycle_controller.go:895] Missing timestamp for Node master-1. Assuming now as a timestamp.
E0319 13:40:06.015026       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
I0319 13:40:06.015793       1 taint_manager.go:198] Starting NoExecuteTaintManager
I0319 13:40:06.016990       1 event.go:221] Event(v1.ObjectReference{Kind:""Node"", Namespace:"""", Name:""master-1"", UID:""53448114-4a4c-11e9-b6ca-fa163ee25bf8"", APIVersion:"""", ResourceVersion:"""", FieldPath:""""}): type: 'Normal' reason: 'RegisteredNode' Node master-1 event: Registered Node master-1- in Controller
I0319 13:40:06.018163       1 controller_utils.go:1034] Caches are synced for service account controller
I0319 13:40:06.019278       1 controller_utils.go:1034] Caches are synced for certificate controller
I0319 13:40:06.020000       1 controller_utils.go:1034] Caches are synced for namespace controller
I0319 13:40:06.025514       1 controller_utils.go:1034] Caches are synced for attach detach controller
I0319 13:40:06.026024       1 event.go:221] Event(v1.ObjectReference{Kind:""Deployment"", Namespace:""kube-system"", Name:""coredns"", UID:""539d96a7-4a4c-11e9-b6ca-fa163ee25bf8"", APIVersion:""apps/v1"", ResourceVersion:""159"", FieldPath:""""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set coredns-86c58d9df4 to 2
I0319 13:40:06.027128       1 controller_utils.go:1034] Caches are synced for ReplicaSet controller
I0319 13:40:06.027314       1 controller_utils.go:1034] Caches are synced for daemon sets controller
I0319 13:40:06.027373       1 controller_utils.go:1034] Caches are synced for bootstrap_signer controller
I0319 13:40:06.033955       1 controller_utils.go:1034] Caches are synced for ClusterRoleAggregator controller
I0319 13:40:06.042809       1 controller_utils.go:1034] Caches are synced for HPA controller
I0319 13:40:06.090360       1 controller_utils.go:1034] Caches are synced for disruption controller
I0319 13:40:06.090866       1 disruption.go:296] Sending events to api server.
I0319 13:40:06.100928       1 controller_utils.go:1034] Caches are synced for cidrallocator controller
I0319 13:40:06.138297       1 range_allocator.go:310] Set node master-1- PodCIDR to 10.233.0.0/24
I0319 13:40:06.148880       1 event.go:221] Event(v1.ObjectReference{Kind:""ReplicaSet"", Namespace:""kube-system"", Name:""coredns-86c58d9df4"", UID:""816134e2-4a4c-11e9-b6ca-fa163ee25bf8"", APIVersion:""apps/v1"", ResourceVersion:""367"", FieldPath:""""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: coredns-86c58d9df4-94bd6
I0319 13:40:06.151743       1 event.go:221] Event(v1.ObjectReference{Kind:""DaemonSet"", Namespace:""kube-system"", Name:""kube-proxy"", UID:""53823eb9-4a4c-11e9-b6ca-fa163ee25bf8"", APIVersion:""apps/v1"", ResourceVersion:""151"", FieldPath:""""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kube-proxy-jqngz
I0319 13:40:06.164274       1 log.go:172] [INFO] signed certificate with serial number 410601984025786171313483247588146518920605603458
I0319 13:40:06.193072       1 controller_utils.go:1034] Caches are synced for resource quota controller
I0319 13:40:06.218429       1 event.go:221] Event(v1.ObjectReference{Kind:""ReplicaSet"", Namespace:""kube-system"", Name:""coredns-86c58d9df4"", UID:""816134e2-4a4c-11e9-b6ca-fa163ee25bf8"", APIVersion:""apps/v1"", ResourceVersion:""367"", FieldPath:""""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: coredns-86c58d9df4-rxgzt
I0319 13:40:06.528537       1 controller_utils.go:1034] Caches are synced for garbage collector controller
I0319 13:40:06.553283       1 controller_utils.go:1034] Caches are synced for garbage collector controller
I0319 13:40:06.553321       1 garbagecollector.go:142] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0319 13:40:11.445798       1 node_lifecycle_controller.go:1072] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
E0319 13:40:16.446344       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
I0319 13:40:19.454360       1 event.go:221] Event(v1.ObjectReference{Kind:""DaemonSet"", Namespace:""kube-system"", Name:""calico-node"", UID:""88c2ef3d-4a4c-11e9-b6ca-fa163ee25bf8"", APIVersion:""apps/v1"", ResourceVersion:""452"", FieldPath:""""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: calico-node-gxctv
E0319 13:40:26.809303       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
E0319 13:40:36.055269       1 resource_quota_controller.go:437] failed to sync resource monitors: [couldn't start monitor for resource ""extensions/v1beta1, Resource=networkpolicies"": unable to monitor quota for resource ""extensions/v1beta1, Resource=networkpolicies"", couldn't start monitor for resource ""crd.projectcalico.org/v1, Resource=networkpolicies"": unable to monitor quota for resource ""crd.projectcalico.org/v1, Resource=networkpolicies""]
I0319 13:40:36.743962       1 controller_utils.go:1027] Waiting for caches to sync for garbage collector controller
I0319 13:40:37.144286       1 controller_utils.go:1034] Caches are synced for garbage collector controller
E0319 13:40:37.186043       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
E0319 13:40:47.561841       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
E0319 13:40:57.934006       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
E0319 13:41:08.306070       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
E0319 13:41:18.653377       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
E0319 13:41:29.258011       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
E0319 13:41:39.608980       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
E0319 13:41:50.226963       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
E0319 13:42:00.570446       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
E0319 13:42:10.961365       1 node_lifecycle_controller.go:803] Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
I0319 13:42:19.175415       1 event.go:221] Event(v1.ObjectReference{Kind:""DaemonSet"", Namespace:""kube-system"", Name:""openstack-cloud-controller-manager"", UID:""899e4912-4a4c-11e9-b6ca-fa163ee25bf8"", APIVersion:""apps/v1"", ResourceVersion:""510"", FieldPath:""""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: openstack-cloud-controller-manager-b7gjc
I0319 13:42:21.343146       1 node_lifecycle_controller.go:1099] Controller detected that some Nodes are Ready. Exiting master disruption mode.

```

Especially the line
```
Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID""
```
seems suspicious, however I am not sure whether this is the origin of the problem.


**What you expected to happen**:

The taint ""node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule"" should get removed when the OpenStack cloud controller manager is initializing my nodes. Apart from this, my nodes should get assigned an internal IP address.

**How to reproduce it (as minimally and precisely as possible)**:

Follow instructions at https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-controller-manager-with-kubeadm.md

**Anything else we need to know?**:

I am not sure what other information to provide.

**Environment**:
- openstack-cloud-controller-manager version: docker.io/k8scloudprovider/openstack-cloud-controller-manager:latest
- OS (e.g. from /etc/os-release): Ubuntu 16.04.6 LTS (Xenial Xerus)
- Kernel (e.g. `uname -a`): 4.4.0-1040-kvm
- Install tools: kubeadm version: &version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.4"", GitCommit:""c27b913fddd1a6c480c229191a087698aa92f0b1"", GitTreeState:""clean"", BuildDate:""2019-02-28T13:35:32Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/amd64""}

- Others:

Thank you for your help!
",closed,False,2019-03-19 14:23:07,2019-03-26 23:02:06
cloud-provider-openstack,oz123,https://github.com/kubernetes/cloud-provider-openstack/issues/541,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/541,missing  pods-cloud-data in openstack-cloud-controller-manager-ds.yaml,"> /kind bug

**What happened**:
I was trying to run the cloud-controller-manager and encountered a lot of the following messages:
> Error determining if node master-1 shutdown in cloud: ProviderID """" didn't match expected format ""openstack:///InstanceID"" 

See also the discussion in #540 

It turns out, that this happens because  [readInstanceID](https://github.com/kubernetes/cloud-provider-openstack/blob/664540459d96aaf10b809352e33a2a3c129241ef/pkg/cloudprovider/providers/openstack/openstack.go#L364) tries to read the file `/var/lib/cloud/data/instance-id`
which does not exist in the container.

This is already fixed in [csi-nodeplugin-cinderplugin](https://github.com/kubernetes/cloud-provider-openstack/blob/888e3c5b16ef992fb51b675693362c4c03aee3b0/manifests/cinder-csi-plugin/csi-nodeplugin-cinderplugin.yaml#L75)

**What you expected to happen**:
The function [readInstanceID](https://github.com/kubernetes/cloud-provider-openstack/blob/664540459d96aaf10b809352e33a2a3c129241ef/pkg/cloudprovider/providers/openstack/openstack.go#L364) should return the instance properly.

**Environment**:
- openstack-cloud-controller-manager version: 1.13.1
- OS : Ubuntu 16.04
",closed,False,2019-03-19 20:13:20,2019-03-22 09:53:32
cloud-provider-openstack,oz123,https://github.com/kubernetes/cloud-provider-openstack/pull/542,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/542,Add missing /var/lib/cloud mount to pod volumes,"This fixes #541.

See also #540.

**What this PR does / why we need it**:

**Which issue this PR fixes** :
fixes #541 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",open,True,2019-03-19 20:22:17,2019-03-22 09:43:23
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/543,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/543,initialize explictly to enable verbose option,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #539 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-03-20 05:45:10,2019-03-25 12:01:09
cloud-provider-openstack,oz123,https://github.com/kubernetes/cloud-provider-openstack/issues/544,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/544,Please document how to setup local development environment," /kind bug

**What happened**:
The documentation in `cloud-provider-openstack/docs/getting-started-provider-dev.md`
refers to a script in `./hack/local-up-cluster.sh`. However, this script does not exist. 

**What you expected to happen**:
I can find the script and setup a development environment.

**How to reproduce it (as minimally and precisely as possible)**:
Checkout the current master branch, see that  `./hack/local-up-cluster.sh` does not exist.
",closed,False,2019-03-20 11:15:27,2019-03-22 05:17:38
cloud-provider-openstack,tghartland,https://github.com/kubernetes/cloud-provider-openstack/issues/545,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/545,cloud-controller-manager should have permission to watch secrets,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

I was consistently seeing 2-3 pod restarts by the time the cluster was ready. The number of restarts does not increase after this.
```
NAME                                       READY   STATUS              RESTARTS   AGE
openstack-cloud-controller-manager-qgp7h   1/1     Running             3          3m41s
```
By SSHing to the master node and patching the cloud-controller-manager daemonset as soon as it is created to increase the logging level, then checking the logs through docker:
```
I0320 12:34:40.376415       1 round_trippers.go:405] GET https://10.254.0.1:443/api/v1/namespaces/kube-system/secrets?fieldSelector=type%3Dkubernetes.io%2Fservice-account-token&resourceVersion=612&watch=true 403 Forbidden in 35 milliseconds
I0320 12:34:40.376444       1 round_trippers.go:411] Response Headers:
I0320 12:34:40.376469       1 round_trippers.go:414]     Content-Type: application/vnd.kubernetes.protobuf
I0320 12:34:40.376474       1 round_trippers.go:414]     X-Content-Type-Options: nosniff
I0320 12:34:40.376477       1 round_trippers.go:414]     Content-Length: 244
I0320 12:34:40.376481       1 round_trippers.go:414]     Date: Wed, 20 Mar 2019 12:34:40 GMT
I0320 12:34:40.376534       1 request.go:895] Response Body:
00000000  6b 38 73 00 0a 0c 0a 02  76 31 12 06 53 74 61 74  |k8s.....v1..Stat|
00000010  75 73 12 db 01 0a 06 0a  00 12 00 1a 00 12 07 46  |us.............F|
00000020  61 69 6c 75 72 65 1a a6  01 73 65 63 72 65 74 73  |ailure...secrets|
00000030  20 69 73 20 66 6f 72 62  69 64 64 65 6e 3a 20 55  | is forbidden: U|
00000040  73 65 72 20 22 73 79 73  74 65 6d 3a 73 65 72 76  |ser ""system:serv|
00000050  69 63 65 61 63 63 6f 75  6e 74 3a 6b 75 62 65 2d  |iceaccount:kube-|
00000060  73 79 73 74 65 6d 3a 63  6c 6f 75 64 2d 63 6f 6e  |system:cloud-con|
00000070  74 72 6f 6c 6c 65 72 2d  6d 61 6e 61 67 65 72 22  |troller-manager""|
00000080  20 63 61 6e 6e 6f 74 20  77 61 74 63 68 20 72 65  | cannot watch re|
00000090  73 6f 75 72 63 65 20 22  73 65 63 72 65 74 73 22  |source ""secrets""|
000000a0  20 69 6e 20 41 50 49 20  67 72 6f 75 70 20 22 22  | in API group """"|
000000b0  20 69 6e 20 74 68 65 20  6e 61 6d 65 73 70 61 63  | in the namespac|
000000c0  65 20 22 6b 75 62 65 2d  73 79 73 74 65 6d 22 22  |e ""kube-system""""|
000000d0  09 46 6f 72 62 69 64 64  65 6e 2a 11 0a 00 12 00  |.Forbidden*.....|
000000e0  1a 07 73 65 63 72 65 74  73 28 00 32 00 30 93 03  |..secrets(.2.0..|
000000f0  1a 00 22 00                                       |.."".|
F0320 12:34:40.376577       1 client_builder.go:273] unable to get token for service account: unknown (get secrets)
```
which comes from here https://github.com/kubernetes/kubernetes/blob/v1.11.0/pkg/controller/client_builder.go#L185 (in v1.11.0 which is used by openstack-cloud-controller-manager v0.2.0). In the [master branch](https://github.com/kubernetes/kubernetes/blob/bdc013ad57df721a148a5e15953c5a8b67efdeec/pkg/controller/client_builder.go#L174) this has changed slightly but it looks like the same problem would still happen.

The error message at the default logging level (`unable to get token for service account: unknown (get secrets)`) is not helpful but the request response shows that the CCM permissions do not allow it to watch secrets. Once the secret exists the watch never needs to be used which is why the restarts stop.

The watch permission should be added here https://github.com/kubernetes/cloud-provider-openstack/blob/8f453273748a2bb047bdd25f19ab0e308774bb93/cluster/addons/rbac/cloud-controller-manager-roles.yaml#L70-L74

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: **v0.2.0**
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2019-03-20 13:45:52,2019-03-21 23:24:19
cloud-provider-openstack,tghartland,https://github.com/kubernetes/cloud-provider-openstack/pull/546,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/546,Give cloud-controller-manager permission to watch secrets,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

In [kubernetes/kubernetes/pkg/controller/client_builder.go](https://github.com/kubernetes/kubernetes/blob/bdc013ad57df721a148a5e15953c5a8b67efdeec/pkg/controller/client_builder.go#L174) it attempts to start a watch for new secrets if the secret containing a token for a service account does not exist the first time it checks. Without the watch permission for secrets this crashes the cloud-controller-manager and the pod restarts until the secret does exist.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #545 

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-03-20 14:04:56,2019-03-21 23:24:19
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/issues/547,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/547,<!-- This form is for bug reports and feature requests! -->,"**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature


**What happened**:

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:",closed,False,2019-03-20 18:43:31,2019-03-20 18:43:40
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/548,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/548,Add 1.13 conformance reporting,"**What this PR does / why we need it**:
Adds 1.13 conformance testing reporting to testgrid for cloud-provider-openstack

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```",closed,True,2019-03-20 18:51:15,2019-03-20 19:28:37
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/549,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/549,Improve load balancer name,"**What this PR does / why we need it**:
- Using a more meaningful name for load balancer created by Service.
- Support backward compatibility.
- Adding doc for LoadBalancer type Service guide.

**Which issue this PR fixes**:

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
The load balancer name created by Service has been changed, now the name includes cluster name, Service namespace and Service name. The services created before this change are not affected.
```
",closed,True,2019-03-20 23:30:10,2019-03-27 05:29:51
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/550,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/550,k8s-keystone-auth: Policy setting improvement,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
The authorization policy setting is too complex and not flexible, especially for the fine-grained controls for the resource.

**What you expected to happen**:
I'd propose the following setting format:
```JSON
[
  {
    ""match"": {
      ""role"": [""member""],
      ""project"": [""demo""]
    },
    ""resource_permissions"": [
      {""*.service"": [""*""]},
      {""*.pod"": [""get"", ""list"", ""watch""]},
      {""!kube-system.*"": [""get"", ""list"", ""watch""]},
      {""*.*"": [""get"", ""list"", ""watch""]},
      {""*.!secret"": [""get"", ""list"", ""watch""]}
    ],
    ""nonresource_permissions"": [
      {""/healthz"": [""get""]}
    ]
  },
  {...}
]
```

- The key in `resource_permissions` is in the format of `<namespace>.<resource>`
- `*` means all the namespaces or resources are allowed.
- `!` means all the namespaces or resources except the following one are allowed.
- The policy is whitelist and the config orders in `resource_permissions` matters.
- resource_permissions and nonresource_permissions can't be both used at the same time(same with the current behavior).

**How to reproduce it (as minimally and precisely as possible)**:

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2019-03-21 02:54:25,2019-03-21 03:02:31
cloud-provider-openstack,dricoco,https://github.com/kubernetes/cloud-provider-openstack/issues/551,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/551,csi-attacher-cinderplugin cannot communicate with openstack,"<!-- This form is for bug reports and feature requests! -->


/kind bug

**What happened**:
The daemonset is running ok 

> kube-system   csi-attacher-cinderplugin-0                                         2/2     Running             0          112s
kube-system   csi-nodeplugin-cinderplugin-bkwx8                                   2/2     Running             1          25h
kube-system   csi-nodeplugin-cinderplugin-f64d9                                   2/2     Running             4          25h
kube-system   csi-provisioner-cinderplugin-0                                      2/2     Running             0          25h
kube-system   csi-snapshotter-cinder-0                                            2/2     Running             0          25h


The csi-provisionner is allocating correctly ressources :


>  kc logs  csi-provisioner-cinderplugin-0  -n kube-system csi-provisioner
I0320 07:27:42.383056       1 controller.go:571] Starting provisioner controller csi-cinderplugin_csi-provisioner-cinderplugin-0_a5f397d9-4ae1-11e9-8eea-0a580a645108!
I0320 07:27:42.484252       1 controller.go:620] Started provisioner controller csi-cinderplugin_csi-provisioner-cinderplugin-0_a5f397d9-4ae1-11e9-8eea-0a580a645108!
I0320 07:33:39.095692       1 controller.go:926] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": started
I0320 07:33:39.205059       1 event.go:221] Event(v1.ObjectReference{Kind:""PersistentVolumeClaim"", Namespace:""default"", Name:""csi-pvc-cinderplugin"", UID:""7a857818-4ae2-11e9-8ad3-fa163e90edbf"", APIVersion:""v1"", ResourceVersion:""261513"", FieldPath:""""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim ""default/csi-pvc-cinderplugin""
I0320 07:33:40.768192       1 controller.go:666] successfully created PV {GCEPersistentDisk:nil AWSElasticBlockStore:nil HostPath:nil Glusterfs:nil NFS:nil RBD:nil ISCSI:nil Cinder:nil CephFS:nil FC:nil Flocker:nil FlexVolume:nil AzureFile:nil VsphereVolume:nil Quobyte:nil AzureDisk:nil PhotonPersistentDisk:nil PortworxVolume:nil ScaleIO:nil Local:nil StorageOS:nil CSI:&CSIPersistentVolumeSource{Driver:cinder.csi.openst
ack.org,VolumeHandle:3ac23adf-7f91-49c0-8fb4-38d90311b415,ReadOnly:false,FSType:ext4,VolumeAttributes:map[string]string{storage.kubernetes.io/csiProvisionerIdentity: 1553066854376-8081-csi-cinderplugin,},ControllerPublishSecretRef:nil,NodeStageSecretRef:nil,NodePublishSecretRef:nil,}}
I0320 07:33:40.768600       1 controller.go:1026] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": volume ""pvc-7a857818-4ae2-11e9-8ad3-fa163e90edbf"" provisioned
I0320 07:33:40.768639       1 controller.go:1040] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": trying to save persistentvolume ""pvc-7a857818-4ae2-11e9-8ad3-fa163e90edbf""
I0320 07:34:10.776244       1 controller.go:1052] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": failed to save persistentvolume ""pvc-7a857818-4ae2-11e9-8ad3-fa163e90edbf"": Timeout: request did not complete within requested timeout 30s
I0320 07:34:20.776528       1 controller.go:1040] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": trying to save persistentvolume ""pvc-7a857818-4ae2-11e9-8ad3-fa163e90edbf""
I0320 07:34:20.892541       1 controller.go:1044] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": persistentvolume ""pvc-7a857818-4ae2-11e9-8ad3-fa163e90edbf"" already exists, reusing
I0320 07:34:20.892748       1 controller.go:1088] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": succeeded
I0320 07:34:20.893274       1 controller.go:926] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": started
I0320 07:34:20.893678       1 event.go:221] Event(v1.ObjectReference{Kind:""PersistentVolumeClaim"", Namespace:""default"", Name:""csi-pvc-cinderplugin"", UID:""7a857818-4ae2-11e9-8ad3-fa163e90edbf"", APIVersion:""v1"", ResourceVersion:""261513"", FieldPath:""""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-7a857818-4ae2-11e9-8ad3-fa163e90edbf
I0320 07:34:20.903921       1 controller.go:935] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": persistentvolume ""pvc-7a857818-4ae2-11e9-8ad3-fa163e90edbf"" already exists, skipping
I0320 07:42:42.394555       1 controller.go:926] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": started
I0320 07:42:42.418033       1 controller.go:935] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": persistentvolume ""pvc-7a857818-4ae2-11e9-8ad3-fa163e90edbf"" already exists, skipping
I0320 07:57:42.394671       1 controller.go:926] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": started
I0320 07:57:42.408542       1 controller.go:935] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": persistentvolume ""pvc-7a857818-4ae2-11e9-8ad3-fa163e90edbf"" already exists, skipping
I0320 08:12:42.394975       1 controller.go:926] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": started
I0320 08:12:42.406775       1 controller.go:935] provision ""default/csi-pvc-cinderplugin"" class ""csi-sc-cinderplugin"": persistentvolume ""pvc-7a857818-4ae2-11e9-8ad3-fa163e90edbf"" already exists, skipping


> kc logs  csi-provisioner-cinderplugin-0  -n kube-system cinder
I0320 07:27:41.179530       1 driver.go:56] Driver: cinder.csi.openstack.org version: 1.0.0
I0320 07:27:41.741358       1 driver.go:88] Enabling controller service capability: LIST_VOLUMES
I0320 07:27:41.741367       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_VOLUME
I0320 07:27:41.741372       1 driver.go:88] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
I0320 07:27:41.741376       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
I0320 07:27:41.741381       1 driver.go:88] Enabling controller service capability: LIST_SNAPSHOTS
I0320 07:27:41.741387       1 driver.go:100] Enabling volume access mode: SINGLE_NODE_WRITER
I0320 07:27:41.741392       1 driver.go:110] Enabling node service capability: STAGE_UNSTAGE_VOLUME
I0320 07:27:41.741761       1 server.go:108] Listening for connections on address: &net.UnixAddr{Name:""/csi/csi.sock"", Net:""unix""}


We have the cinder v3 endpoint exposed correctly

But at the same time the csi attacher cannot communicate with openstack :

> kc logs  csi-attacher-cinderplugin-0 -n kube-system cinder | more                   
I0321 08:40:22.256270       1 driver.go:56] Driver: cinder.csi.openstack.org version: 1.0.0                                            
I0321 08:40:22.256449       1 driver.go:88] Enabling controller service capability: LIST_VOLUMES
I0321 08:40:22.256601       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_VOLUME
I0321 08:40:22.256612       1 driver.go:88] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
I0321 08:40:22.256616       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
I0321 08:40:22.256620       1 driver.go:88] Enabling controller service capability: LIST_SNAPSHOTS
I0321 08:40:22.256626       1 driver.go:100] Enabling volume access mode: SINGLE_NODE_WRITER
I0321 08:40:22.256630       1 driver.go:110] Enabling node service capability: STAGE_UNSTAGE_VOLUME
I0321 08:40:22.257222       1 server.go:108] Listening for connections on address: &net.UnixAddr{Name:""/csi/csi.sock"", Net:""unix""}
E0321 08:41:12.686669       1 utils.go:77] GRPC error: rpc error: code = FailedPrecondition desc = Failed to communicate with openstack
E0321 08:41:13.690246       1 utils.go:77] GRPC error: rpc error: code = FailedPrecondition desc = Failed to communicate with openstack
E0321 08:41:14.694091       1 utils.go:77] GRPC error: rpc error: code = FailedPrecondition desc = Failed to communicate with openstack
E0321 08:41:15.697341       1 utils.go:77] GRPC error: rpc error: code = FailedPrecondition desc = Failed to communicate with openstack

> kc logs  csi-attacher-cinderplugin-0 -n kube-system csi-attacher                          
I0321 08:45:04.725831       1 main.go:76] Version: v1.0.1-0-gb7dadac              
I0321 08:45:04.728032       1 connection.go:89] Connecting to /var/lib/csi/sockets/pluginproxy/csi.sock
I0321 08:45:04.728573       1 connection.go:116] Still trying, connection is CONNECTING
I0321 08:45:04.729227       1 connection.go:113] Connected                                                                                   
I0321 08:45:04.729339       1 connection.go:242] GRPC call: /csi.v1.Identity/Probe                                                           
I0321 08:45:04.729493       1 connection.go:243] GRPC request: {}                 
I0321 08:45:04.733316       1 connection.go:245] GRPC response: {}
I0321 08:45:04.735225       1 connection.go:246] GRPC error: rpc error: code = FailedPrecondition desc = Failed to communicate with openstack
I0321 08:45:04.735367       1 main.go:214] Probe failed with rpc error: code = FailedPrecondition desc = Failed to communicate with openstack
I0321 08:45:05.735763       1 connection.go:242] GRPC call: /csi.v1.Identity/Probe    

The pvc stays on pending : 

> kc get pvc
NAME                   STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS          AGE
csi-pvc-cinderplugin   Pending                                      csi-sc-cinderplugin   25h




**What you expected to happen**:

pvc should be out of pending state I should be available to attach storage to pods

**How to reproduce it (as minimally and precisely as possible)**:
just follow the example manifest provided in the documentation

**Anything else we need to know?**:

I cannot increase log level because of #539 
",closed,False,2019-03-21 08:48:28,2019-03-22 10:25:01
cloud-provider-openstack,darcyllingyan,https://github.com/kubernetes/cloud-provider-openstack/issues/552,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/552,"node-driver-registrar pod can't be started and reports ""Connecting to /csi/csi.sock timed out"" ","Hi,
When I deployed the CSI driver, the node-driver-registrar pod couldn't be started and always reports the ``` Connecting to /csi/csi.sock TRANSIENT_FAILURE Connection timed out``` error, whether there are some issues or restrictions for my cluster environment when using the CSI plugin as I deployed successfully in another cluster.
The difference between the two clusters are node number.

Now the external provisioner, external attacher are all running, only the node-driver-registrar crash.
Based on below log, the csi driver socket is OK but the node-driver-registrar socket couldn't connect.
```
# kubectl get pod -n kube-system
NAME                                          READY   STATUS             RESTARTS   AGE
csi-attacher-cinderplugin-0                   2/2     Running            0          5h53m
csi-nodeplugin-cinderplugin-dj8w4             1/2     CrashLoopBackOff   6          17m
csi-nodeplugin-cinderplugin-sfrqf             1/2     CrashLoopBackOff   6          17m
csi-provisioner-cinderplugin-0                2/2     Running            0          5h53m
csi-snapshotter-cinder-0                      2/2     Running            0          5h53m
```
The node-driver-registrar log is below:
```
# kubectl logs csi-nodeplugin-cinderplugin-gm7sl -n kube-system node-driver-registrar
I0321 08:59:37.981108       1 main.go:111] Version: 
I0321 08:59:37.981219       1 main.go:118] Attempting to open a gRPC connection with: ""/csi/csi.sock""
I0321 08:59:37.981238       1 connection.go:69] Connecting to /csi/csi.sock
I0321 08:59:37.981533       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:37.982349       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:38.981960       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:38.982200       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:40.024028       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:40.024239       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:41.200354       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:41.200952       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:42.266468       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:42.266622       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:43.241629       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:43.241848       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:44.211730       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:44.211804       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:45.286693       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:46.113061       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:46.113212       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:46.975907       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:46.975991       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:47.815025       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:47.815552       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:48.735753       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:49.741918       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:49.742031       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:50.867745       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:51.753623       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:51.753767       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:52.705934       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:52.706004       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:53.633234       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:53.633507       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:54.620910       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:54.621000       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:55.534845       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:55.535080       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:56.451708       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:56.451879       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:57.523498       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:57.523625       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:58.411093       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:58.411253       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 08:59:59.292524       1 connection.go:96] Still trying, connection is CONNECTING
I0321 08:59:59.292616       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:00.237176       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:00.237323       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:01.265661       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:01.265814       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:02.411186       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:03.328529       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:03.328771       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:04.247522       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:04.247714       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:05.348857       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:05.348987       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:06.231515       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:06.231624       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:07.377978       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:07.378151       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:08.456791       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:08.456977       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:09.466570       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:10.278022       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:10.278267       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:11.141699       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:11.141731       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:12.184630       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:13.374929       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:13.375161       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:14.206948       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:15.245086       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:15.245589       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:16.068902       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:16.069127       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:17.146052       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:17.146131       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:18.066639       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:18.067036       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:18.936236       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:18.936701       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:19.952976       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:19.953258       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:20.970847       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:20.970957       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:21.882470       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:21.882509       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:22.851985       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:22.852068       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:23.864330       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:23.864465       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:24.766355       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:24.766704       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:25.679190       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:26.794854       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:26.794984       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:27.739730       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:27.739875       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:28.892300       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:29.811378       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:30.969299       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:30.969543       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:31.808533       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:32.999305       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:32.999488       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:33.829136       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:33.829275       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:34.718244       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:34.718459       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:35.790877       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:35.790983       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:36.687615       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:36.687817       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:37.612463       1 connection.go:96] Still trying, connection is CONNECTING
I0321 09:00:37.612718       1 connection.go:96] Still trying, connection is TRANSIENT_FAILURE
I0321 09:00:37.981411       1 connection.go:89] Connection timed out
I0321 09:00:37.981436       1 main.go:126] Calling CSI driver to discover driver name.
I0321 09:00:37.981453       1 connection.go:137] GRPC call: /csi.v1.Identity/GetPluginInfo
I0321 09:00:37.981458       1 connection.go:138] GRPC request: {}
I0321 09:00:37.982497       1 connection.go:140] GRPC response: {}
I0321 09:00:37.982918       1 connection.go:141] GRPC error: rpc error: code = Unavailable desc = all SubConns are in TransientFailure
E0321 09:00:37.982934       1 main.go:131] rpc error: code = Unavailable desc = all SubConns are in TransientFailure
```
The CSI driver socket is below:
```
# kubectl logs -f csi-nodeplugin-cinderplugin-228sh -n kube-system cinder
I0321 03:19:18.516206       1 driver.go:56] Driver: cinder.csi.openstack.org version: 1.0.0
I0321 03:19:18.516722       1 driver.go:88] Enabling controller service capability: LIST_VOLUMES
I0321 03:19:18.516744       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_VOLUME
I0321 03:19:18.516756       1 driver.go:88] Enabling controller service capability: PUBLISH_UNPUBLISH_VOLUME
I0321 03:19:18.516802       1 driver.go:88] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
I0321 03:19:18.516827       1 driver.go:88] Enabling controller service capability: LIST_SNAPSHOTS
I0321 03:19:18.516840       1 driver.go:100] Enabling volume access mode: SINGLE_NODE_WRITER
I0321 03:19:18.516885       1 driver.go:110] Enabling node service capability: STAGE_UNSTAGE_VOLUME
I0321 03:19:18.518095       1 server.go:108] Listening for connections on address: &net.UnixAddr{Name:""/csi/csi.sock"", Net:""unix""}
```
Thanks",closed,False,2019-03-21 15:07:40,2019-03-24 03:10:31
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/553,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/553,Update CI config,"release-1.11 is available and config should reflect that



<!-- Thanks for sending a pull request! -->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```",closed,True,2019-03-21 16:04:42,2019-03-21 16:05:59
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/554,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/554,Update release-1.13 CI config,".zuul.yaml conformance jobs should only exist for the current
branch which is 1.13 here

**What this PR does / why we need it**:
Updates the CI system to run the appropriate conformance tests based on the release/branch it is targeting.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```",closed,True,2019-03-21 18:39:17,2019-03-21 18:49:14
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/555,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/555,Update CI conformance jobs,"**What this PR does / why we need it**:

Conformance jobs should only be running relevant to the branch they appear on. The periodic job is being disabled due to release-1.10 being EOL and when run it pushes the wrong binaries with the tag latest to dockerhub.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
We need to merge this as soon as possible due to it pushing the wrong binaries to dockerhub using the latest tag. I recommend ignoring OpenLab CI errors and forcing it.

**Release note**:

```release-note
NONE
```
",closed,True,2019-03-21 18:58:36,2019-03-21 20:09:55
cloud-provider-openstack,dricoco,https://github.com/kubernetes/cloud-provider-openstack/issues/556,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/556,cinder csi attacher is missing from Makefile,"<!-- This form is for bug reports and feature requests! -->

/kind bug

**What happened**:
I'm trying to build my own image of CSI cinder and saw that csi-attacher:v1.0.1 wasn't build

watching the line in Makefile we can see that 

build: openstack-cloud-controller-manager cinder-provisioner cinder-flex-volume-driver cinder-csi-plugin k8s-keystone-auth client-keystone-auth octavia-ingress-controller manila-provisioner barbican-kms-plugin

doesn't include cinder-attacher

the docker push line in upload-images also doesn't include cinder-csi-attacher


**What you expected to happen**:

cinder-attacher should be built


**How to reproduce it (as minimally and precisely as possible)**:
Just build the project
",closed,False,2019-03-22 08:14:44,2019-03-22 08:30:32
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/557,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/557,Update CI conformance jobs - 1.11,"**What this PR does / why we need it**:

Conformance jobs should only be running relevant to the branch they appear on. The periodic job is being disabled due to release-1.11 being EOL and when run it pushes the wrong binaries with the tag latest to dockerhub.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
We need to merge this as soon as possible due to it pushing the wrong binaries to dockerhub using the latest tag. I recommend ignoring OpenLab CI errors and forcing it.

**Release note**:

```release-note
NONE
```",open,True,2019-03-22 14:27:18,2019-03-26 07:45:40
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/558,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/558,Update CI conformance jobs - 1.12,"**What this PR does / why we need it**:

Conformance jobs should only be running relevant to the branch they appear on. The periodic jobs for 1.10 and 1.11 are being removed due to being EOL and when run pushing the wrong binaries with the tag latest to dockerhub.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
We need to merge this as soon as possible due to it pushing the wrong binaries to dockerhub using the latest tag. I recommend ignoring OpenLab CI errors and forcing it.

**Release note**:

```release-note
NONE
```",open,True,2019-03-22 14:36:40,2019-03-22 21:03:05
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/559,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/559,Update CI conformance jobs - 1.13,"**What this PR does / why we need it**:

Conformance jobs should only be running relevant to the branch they appear on. The periodic jobs for 1.10 and 1.11 are being removed due to being EOL and when run pushing the wrong binaries with the tag latest to dockerhub.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
We need to merge this as soon as possible due to it pushing the wrong binaries to dockerhub using the latest tag. I recommend ignoring OpenLab CI errors and forcing it.

**Release note**:

```release-note
NONE
```",open,True,2019-03-22 14:38:48,2019-03-22 21:03:10
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/560,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/560,Update CI conformance jobs - 1.14/master,"**What this PR does / why we need it**:

Conformance jobs should only be running relevant to the branch they appear on. The periodic jobs for 1.10 and 1.11 are being removed due to being EOL and when run pushing the wrong binaries with the tag latest to dockerhub.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
We need to merge this as soon as possible due to it pushing the wrong binaries to dockerhub using the latest tag. I recommend ignoring OpenLab CI errors and forcing it.

**Release note**:

```release-note
NONE
```",open,True,2019-03-22 14:43:14,2019-03-31 20:49:39
cloud-provider-openstack,mrhillsman,https://github.com/kubernetes/cloud-provider-openstack/pull/561,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/561,Update CI conformance jobs 1.10,"**What this PR does / why we need it**:
Remove periodic job. It is not required since 1.10 is EOL and it is still running after commenting section.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:
OpenLab CI errors ok to ignore

**Release note**:
```release-note
NONE
```
",open,True,2019-03-22 15:39:52,2019-03-26 06:22:15
cloud-provider-openstack,zdenekjanda,https://github.com/kubernetes/cloud-provider-openstack/issues/562,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/562,octavia-ingress-controller no longer starts with latest image k8scloudprovider/octavia-ingress-controller:latest,"**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:
octavia-ingress-controller pod no longer starts after pulling new image k8scloudprovider/octavia-ingress-controller:latest with ID 987a41db8ada. Worse, existing clusters pulled new image randomly when pods were rescheduled to workers with no image cache and service failed.
Image with ID aad2571eb5ee works ok.

Logs from octavia-ingress-controller pod:
```
time=""2019-03-22T16:35:14Z"" level=info msg=""creating kubernetes API client""
time=""2019-03-22T16:35:14Z"" level=info msg=""kubernetes API client created"" version=v1.13
time=""2019-03-22T16:35:14Z"" level=fatal msg=""failed to initialize openstack client"" error=""Get /: unsupported protocol scheme \""\""""
```
Pod status:
```
kube-system   octavia-ingress-controller-0                                                0/1     CrashLoopBackOff   12         41m
```
My configmap:
```
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: octavia-ingress-controller-config
  namespace: kube-system
data:
  config: |
    cluster-name: 7d987d05-637e-48a7-ada0-f8897a829243
    openstack:
      auth-url: http://10.8.0.5:5000/v3
      trust-id: fb1d513ff0274164a7c7554b0b649abe
      user-id: 233d20b2fc194fa4be20711196f9cd00
      password: <dedacted>
    octavia:
      subnet-id: 36581dfd-ceca-46d8-96e3-01bf42d24565`
```

**What you expected to happen**:
octavia-ingress-controller pod starts

**How to reproduce it (as minimally and precisely as possible)**:
Deploy latest version of octavia-ingress-controller

**Environment**:
- openstack-cloud-controller-manager version: v1.13.4
- OS (e.g. from /etc/os-release): Ubuntu 18.04.1 LTS
- Kernel (e.g. `uname -a`): 4.15.0-43-generic
- Install tools: kubeadm

Also, is there any documentation on how to debug this ? I would appreciate some easy suggestion how to create a new container of octavia-ingress-controller with altered code with debug points so i can see what it does. I am friend with Go and Gophercloud, but debugging stuff running inside kubernetes is kind of new for me. Many thanks in advance - I will then dig in and debug why exactly this happened. Many thanks !
",closed,False,2019-03-22 17:00:17,2019-03-26 20:04:14
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/563,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/563,WIP: Add block volume support,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",open,True,2019-03-25 08:28:28,2019-03-25 11:56:38
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/564,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/564,Support to keep original client IP for the Service,"**What this PR does / why we need it**:
Add `X-Forwarded-For` support for the backend HTTP service. A new Service annotation `loadbalancer.openstack.org/x-forwarded-for` is introduced.

**Which issue this PR fixes**: fixes #

**Special notes for your reviewer**:

Test steps:

1. Create the echoserver service

    ```
    $ kubectl run echoserver --image=gcr.io/google-containers/echoserver:1.10 --port=8080
    $ cat <<EOF | kubectl apply -f -
    kind: Service
    apiVersion: v1
    metadata:
      name: echoserver
      annotations:
        loadbalancer.openstack.org/x-forwarded-for: ""true""
    spec:
      type: LoadBalancer
      selector:
        run: echoserver
      ports:
        - protocol: TCP
          port: 80
          targetPort: 8080
    EOF
    ```

1. Waiting for the service to get an external IP.

    ```
    $ kubectl get svc
    NAME         TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)        AGE
    echoserver   LoadBalancer   10.103.126.110   172.24.4.166   80:31209/TCP   32s
    ```

1. Try to send HTTP request from a host that could access the service external IP.

    ```
    $ ip route get 8.8.8.8 | head -1 | awk '{print $7}'
    192.168.206.8
    $ curl 172.24.4.166
    Hostname: echoserver-74dcfdbd78-fthv9

    Pod Information:
            -no pod information available-

    Server values:
            server_version=nginx: 1.13.3 - lua: 10008

    Request Information:
            client_address=10.0.0.7
            method=GET
            real path=/
            query=
            request_version=1.1
            request_scheme=http
            request_uri=http://172.24.4.166:8080/

    Request Headers:
            accept=*/*
            host=172.24.4.166
            user-agent=curl/7.47.0
            x-forwarded-for=192.168.206.8

    Request Body:
            -no body in request-
    ```
    You can see the host IP address appears in x-forwarded-for header.

**Release note**:
```release-note
Support a new Service annotation 'loadbalancer.openstack.org/x-forwarded-for', if set to ""true"", the backend HTTP service is able to get the real source IP of the request from the HTTP headers(X-Forwarded-For).
```
",closed,True,2019-03-26 06:25:48,2019-03-27 05:29:49
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/565,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/565,Add Support for Cinder CSI Volume Resize,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

 /kind feature


**What happened**:
CSI Volume Resize Support has been added in k/k, we need to add same support in cinder CSI
https://github.com/kubernetes/kubernetes/pull/74863

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2019-03-26 07:30:38,2019-03-26 09:12:01
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/issues/566,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/566,Add Support for Inline Volumes Cinder CSI,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

 /kind feature


**What happened**:
CSI Inline volume support has been added in k/k, same support needs to be added in cinder CSI

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2019-03-26 07:33:05,2019-03-26 08:54:23
cloud-provider-openstack,kiwik,https://github.com/kubernetes/cloud-provider-openstack/pull/567,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/567,Move periodic jobs back to OpenLab,"We hope to move all of outside periodic jobs back to OpenLab
projects.yaml, so that we can plan the periodic testing resource
usage amount for OpenLab and avoid periodic jobs define confusion
in different project branch maintaining policy. Periodic jobs will
be moved to theopenlab/openlab-zuul-jobs#479 and will be triggered
at UTC-0 04:00 and 16:00 of everyday.

Related-Bug: theopenlab/openlab#173",closed,True,2019-03-26 09:12:22,2019-03-27 01:40:07
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/568,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/568,Update to k/k 1.14,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Update kubernetes to latest release v1.14

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-03-26 11:11:02,2019-03-26 17:41:20
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/issues/569,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/569,Remove support for Identity v2 API as its deprecated,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
Identity v2 API support to be removed from provider as it is deprecated as of Queens release of Openstack.
**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2019-03-27 11:55:32,2019-03-27 11:55:54
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/570,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/570,Update docs,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",closed,True,2019-03-27 12:05:36,2019-03-29 09:38:16
cloud-provider-openstack,zetaab,https://github.com/kubernetes/cloud-provider-openstack/issues/571,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/571,standalone provisioner is missing scaleio support,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**: FEATURE

Uncomment only one, leave it on its own line: 
/kind feature


**What happened**: tried to provision cinder volumes (scaleio) using standalone cinder provisioner 

**What you expected to happen**: I except that it should work in similar matter that it works in-tree k8s

",open,False,2019-03-28 09:10:54,2019-03-28 09:10:55
cloud-provider-openstack,mape90,https://github.com/kubernetes/cloud-provider-openstack/issues/572,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/572,Add MaxVolumesPerNode limit to node info in Cinder CSI,"**What happened**:
`NodeGetInfo` in pkg/csi/cinder/nodeserver.go should return also `MaxVolumesPerNode`. As openstack has default limit of 26 Cinder volumes per VM. 

**What you expected to happen**:
There should be a way to set `MaxVolumesPerNode` value and it should default to 26 if nothing is provided. Other safe value for limit would be 25 so it would take a count the situation when you are using Cinder volume as root volume.

**Anything else we need to know?**:

Way to provider configurable value for `MaxVolumesPerNode` could be via openstack cloud config file. i.e
```
type Config struct {
 ...
  BlockStorage struct {
    MaxVolumesPerNode int64 `gcfg:""max-volumes-per-node""`
  }
}
```
Then we would need to introduce global variable (similar to `OsInstance` and `configFile`) for `maxVolumes` as it would be static after initialization. Also function is needed so that we can access it from nodeserver.go.
```
var maxVolumes int64 = 26

func GetMaxVolumes() int64 {
	return maxVolumes
}
```
We would just need to set the defined value to the global variable. If `cfg.BlockStorage.MaxVolumesPerNode` is defined. (int64 should default to 0 so we ignore it and also all non positive values)
```
func CreateOpenStackProvider() (IOpenStack, error) {
...
  cfg, epOpts, err := GetConfigFromFile(configFile)
  if err == nil {
    ...
    if cfg.BlockStorage.MaxVolumesPerNode > 0 {
       maxVolumes = cfg.BlockStorage.MaxVolumesPerNode
    }
...
```
Then last in nodeserver.go we could just call `GetMaxVolumes` 
```
func (ns *nodeServer) NodeGetInfo(ctx context.Context, ...
...
return &csi.NodeGetInfoResponse{
  NodeId:             nodeID,
  AccessibleTopology: topology,
  MaxVolumesPerNode:  openstack.GetMaxVolumes(),
}, nil
```

**Environment**:
- Cinder CSI

/kind feature",open,False,2019-03-28 09:36:42,2019-04-02 07:04:57
cloud-provider-openstack,brtknr,https://github.com/kubernetes/cloud-provider-openstack/issues/573,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/573,Unstable node IP in a multi NIC setup,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

When there is more than one NIC attached to an instance, openstack cloud provider returns a random InternalIP back to the host resulting in instability with API server which only talks to a default interface. Additionally, when a --node-ip arg is provided to kubelet with the intended node IP, this is not respected.

**What you expected to happen**:

Consistently return the default interface IP.

**How to reproduce it (as minimally and precisely as possible)**:

Create a cluster with multiple NICs with --cloud-provider=openstack arg or --cloud-provider=external with OpenStack cloud provider pod running on kube-system.

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: latest
- OS (e.g. from /etc/os-release): Fedora Atomic 27 
- Kernel (e.g. `uname -a`): Linux k8s-demo-n7mwfmzoeg2q-minion-0.novalocal 4.16.12-200.fc27.x86_64
- Install tools: OpenStack Magnum
- Others: NA

Here's what it looks like when I watch my nodes:

```
(venv-openstack)   ~ kubectl get nodes -o wide --watch
NAME                             STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                       KERNEL-VERSION            CONTAINER-RUNTIME
k8s-demo-n7mwfmzoeg2q-minion-0   Ready    <none>   10d   v1.13.4   10.60.253.61   <none>        Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.1.0.16   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.1.0.16   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.1.0.16   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.1.0.16   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.1.0.16   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.1.0.16   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.1.0.16   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.1.0.16   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.60.253.61   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
k8s-demo-n7mwfmzoeg2q-minion-0   Ready   <none>   10d   v1.13.4   10.3.0.22   <none>   Debian GNU/Linux 9 (stretch)   4.16.12-200.fc27.x86_64   docker://18.3.1
```",closed,False,2019-03-29 17:45:05,2019-03-30 07:54:12
cloud-provider-openstack,ramineni,https://github.com/kubernetes/cloud-provider-openstack/pull/574,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/574,Update to use beta in-tree CSINode object,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
In kubernetes v1.14 CSINodeInfo and CSIDriver alpha CRDs are moved to in-tree CSINode and CSIDriver core storage v1beta1 APIs . This PR is to update the apigroup of csinode in rbac rules.

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",open,True,2019-04-01 12:08:08,2019-04-03 13:12:30
cloud-provider-openstack,hyperbolic2346,https://github.com/kubernetes/cloud-provider-openstack/pull/575,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/575,Adding support for CA specfication,"**What this PR does / why we need it**:
This PR adds the ability to specify a CA that isn't installed on the system to use for the client-kubernetes-auth plugin. Handled the same way that client cert and key are handled and keys off the OS_CACERT environment variable.

**Special notes for your reviewer**:

**Release note**:
```release-note
Added support for specifying a certificate authority that isn't installed on the client machine via OS_CACERT environment variable.
```
",open,True,2019-04-01 21:23:36,2019-04-05 15:25:02
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/issues/576,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/576,Service: Load balancer not created for the Service,"**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
Create a LoadBalancer type Service first then delete, and quickly create another one, Octavia load balancer is not created. The OCCM log said the load balancer already exists. Although after for a while, the load balancer is created eventually but that sounds like a bug to the end user.

**What you expected to happen**:
Load balancer should be created for the second Service.

**How to reproduce it (as minimally and precisely as possible)**:
- Create a LoadBalancer type Service1
- Delete Service1
- Quickly create a second one with the same name

**Anything else we need to know?**:

**Environment**:
- openstack-cloud-controller-manager version: master branch as of 2 Apr
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",open,False,2019-04-02 01:29:24,2019-04-03 22:25:38
cloud-provider-openstack,pytimer,https://github.com/kubernetes/cloud-provider-openstack/issues/577,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/577,k8s-keystone-auth: sync keystone project to namespace exclude project name,"<!-- This form is for bug reports and feature requests! -->

**Is this a BUG REPORT or FEATURE REQUEST?**:

FEATURE REQUEST

> Uncomment only one, leave it on its own line: 
>
> /kind bug
> /kind feature

/kind feature

**What happened**:

Now i use `k8s-keystone-auth` to auto sync Keystone projects into Kubernetes namespaces, but i want exclude `admin` project, now i have to get `admin` project id, and setting in ConfigMap. When i install `k8s-keystone-auth` before, Keystone not installed, so i must change the ConfigMap and restart pod when Keystone running.

**What you expected to happen**:

I hope `k8s-keystone-auth` can add a parameter that names `project_name_black_list`, a list of Keystone project names can excluded from synchronization.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

In my case, any other method to do it? Thanks.

**Environment**:
- openstack-cloud-controller-manager version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:


",open,False,2019-04-02 09:48:07,2019-04-02 09:48:08
cloud-provider-openstack,lingxiankong,https://github.com/kubernetes/cloud-provider-openstack/pull/578,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/578,Do not get load balancers in deleted or deleting status,"**What this PR does / why we need it**:
Ignore the load balancers in deleting or deleted status when doing query from load balancing service.

**Which issue this PR fixes**: 
fixes #576

**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",open,True,2019-04-04 01:32:25,2019-04-04 08:56:30
cloud-provider-openstack,adisky,https://github.com/kubernetes/cloud-provider-openstack/pull/579,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/579,Add snapshot example in CSI doc,"Added Snapshot Example in Cinder CSI Docs

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
-->

**What this PR does / why we need it**:
Updates  CSI Documentation for Snapshot Feature

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #

**Special notes for your reviewer**:

**Release note**:
`NONE`
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```

",open,True,2019-04-04 07:25:44,2019-04-04 08:41:14
cloud-provider-openstack,Fedosin,https://github.com/kubernetes/cloud-provider-openstack/pull/580,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/580,Allow to specify a Tenant Domain Name/ID,"This commit adds a possibility to specify a domain name/id for a tenant other than the default one.

/kind feature

```release-note
Allow to specify a domain name/id for a tenant other than the default one for the OpenStack cloud provider.
```",open,True,2019-04-05 11:05:33,2019-04-05 14:24:35
cloud-provider-openstack,dricoco,https://github.com/kubernetes/cloud-provider-openstack/issues/581,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/581,trust-id not used in auth for CSI,"<!-- This form is for bug reports and feature requests! -->

/kind bug



**What happened**:
I'm working on a template with openstack magnum using CSI. Magnum provides a trustee to the template and this is used to have the cloud-config config file done.
The problem is that trust-id parameter is not used in the authentification process between the CSI pod and this leads to the following error : 

> I0405 11:07:46.749363       1 server.go:108] Listening for connections on address: &net.UnixAddr{Name:""/csi/csi.sock"", Net:""unix""}
I0405 11:07:46.810438       1 utils.go:73] GRPC call: /csi.v1.Identity/Probe
I0405 11:07:46.810457       1 utils.go:74] GRPC request:
I0405 11:07:47.517388       1 identityserver.go:52] Failed to CreateOpenStackProvider: No suitable endpoint could be found in the service catalog.

in the pods log.

in the JSON sent from the pod to openstack we can see: 

> {""auth"":{""identity"":{""methods"":[""password""],""password"":{""user"":{""domain"":{""id"":""c5503d1bd8b14141b782cad252c6b7cb""},""name"":""b67ae13d-5c93-4374-8902-712f291de94f_968b910e7f0b46b586ad967a1082a693"",""password"":""CCC""}}}}}

we don't have any trust id.

If I reproduce those parameter using CLI in openstack I get 

> user@stack:~# openstack port list
The service catalog is empty.

If I add the 
OS_TRUST_ID=xxx
in my CLI file, then it works fine





**What you expected to happen**:
CSI should work with a trustee user as it is stated in the docs
**How to reproduce it (as minimally and precisely as possible)**:
create a trustee user, configure the cloud-config, launch a pod
",open,False,2019-04-05 11:26:07,2019-04-05 11:28:32
cloud-provider-openstack,dricoco,https://github.com/kubernetes/cloud-provider-openstack/pull/582,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/582,Adding TrustID to authentification,"**What this PR does / why we need it**:
We need to have Trust-id working with openstack magnum
**Which issue this PR fixes** *
fixes #581
**Special notes for your reviewer**:

**Release note**:
<!--  Steps to write your release note:
1. Use the release-note-* labels to set the release note state (if you have access)
2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.
-->
```release-note
```
",open,True,2019-04-05 13:28:05,2019-04-05 15:41:48
cloud-provider-openstack,gwaines,https://github.com/kubernetes/cloud-provider-openstack/issues/583,https://api.github.com/repos/kubernetes/cloud-provider-openstack/issues/583,client-keystone-auth: can no longer setup KUBECONFIG file with 'kubectl config ...' command,"/kind bug

**What happened**:
With OLD version of client-keystone-auth, one could use 

kubectl --kubeconfig=/etc/kubernetes/admin.conf config set-credentials openstackuser --auth-provider=openstack

to update the KUBECONFIG file with the following lines ... to enable use of client-keystone-auth:
          -name openstackuser
            user:
               auth-provider:
                   config: {}
                   name: openstack


With the NEW version of client-keystone-auth, there does NOT seem to be any 'kubectl config ...' options for setting up the NEW lines required in the KUBECONFIG file.
e.g.
             - name: openstackuser
                user: 
                     exec:
                          command: ""/etc/kubernetes/client-keystone-auth""
                          apiVersion: ""client.authentication.k8s.io/v1beta1""


**What you expected to happen**:

Expected a command option like 

kubectl --kubeconfig=/etc/kubernetes/admin.conf config set-credentials openstackuser --command=""/etc/kubernetes/client-keystone-auth""

... in order to enable client-keystone-auth in the KUBECONFIG file ... writing the appropriate lines.


**How to reproduce it (as minimally and precisely as possible)**:

% kubectl config set-credentials

.... shows the help and valid options.
Only valid options listed are:

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig


**Anything else we need to know?**:

**Environment**:

- openstack-cloud-controller-manager version:
client-keystone-auth being used is 1.13.1 
kubectl @ Kubernetes 1.13.5 

- OS (e.g. from /etc/os-release):
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

- Kernel (e.g. `uname -a`):
Linux controller-1 3.10.0-957.1.3.el7.1.tis.x86_64 #1 SMP PREEMPT Wed Mar 13 09:48:27 EDT 2019 x86_64 x86_64 x86_64 GNU/Linux

- Install tools:
kubeadm

- Others:",open,False,2019-04-05 15:28:15,2019-04-05 16:11:54
