name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/1,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/1,Custom scheduler,"...instead of CPU allocation hack and split compose files
",closed,False,2015-11-07 16:48:37,2016-11-21 09:41:54
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/2,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/2,Need support for host namespaces,,closed,False,2015-11-07 16:49:40,2016-12-05 18:27:32
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/3,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/3,Cloud provider integration,,closed,False,2015-11-07 16:50:11,2016-02-08 23:36:20
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/4,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/4,Support for TLS,,closed,False,2015-11-07 16:50:57,2016-01-07 13:39:07
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/5,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/5,Support for cloud provider options,"- [x] GCE
- [x] AWS (see #25)
",closed,False,2015-11-07 16:51:31,2016-03-02 08:05:13
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/6,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/6,Support for master failover,,closed,False,2015-11-07 16:51:56,2018-01-15 10:31:41
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/7,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/7,Solve restarts,,closed,False,2015-11-24 21:10:58,2016-02-19 12:02:53
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/8,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/8,Figure out what happen with `kube-system` namespace in TLS-enabled mode,,closed,False,2016-01-07 13:39:36,2016-01-19 15:48:37
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/9,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/9,Suport extra args for each of the components,"Figure out whether it's better to pass pass them along in the `*-anywhere.sh` script as is or set actually set `ENTRYPOINT [ ""/*-anywhere.sh"" ]`.
",closed,False,2016-01-07 13:42:06,2018-01-15 10:31:59
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/10,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/10,Use CloudFormation,"https://github.com/weaveworks/integrations/issues/59
",closed,False,2016-01-07 19:41:53,2016-11-21 09:42:31
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/11,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/11,Fix SkyDNS with TLS,"Service account appears to be missing, but even creating it doesn't seem to make it work.
",closed,False,2016-01-08 13:12:53,2016-01-19 17:49:44
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/12,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/12,Invistigate if certs should have apiserver IP address,"@2opremio said it should be determistic, it's probably the first address in the cluster IP range...
",closed,False,2016-01-12 21:29:01,2016-01-19 15:51:47
kubernetes-anywhere,ghadishayban,https://github.com/kubernetes/kubernetes-anywhere/issues/13,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/13,First link broken in README,"Seems to point to some slides with more information
",closed,False,2016-01-15 18:48:55,2016-01-19 15:41:56
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/14,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/14,Find out what causes SkyDNS to error so much,"It keeps logging this:

```
failure to forward request ""read udp 172.17.0.1:53: i/o timeout""`
```

Clearly it tries to send some kind of question to WeaveDNS.
",closed,False,2016-01-19 17:53:37,2016-04-25 15:03:06
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/15,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/15,Avoid nameserver entry for WeaveDNS,"One of the pods gets this in `/etc/resolv.conf`:

```
search default.svc.kube.local svc.kube.local kube.local
nameserver 10.16.0.3
nameserver 172.17.0.1
nameserver 172.17.0.1
options ndots:5
```

What's unclear is why it has a duplicate entry for `172.17.0.1`.
",closed,False,2016-01-19 18:05:49,2016-04-25 15:02:33
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/16,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/16,Admission controls require some token foo in non-TLS setup,"```
I0120 12:43:21.021807       1 event.go:206] Event(api.ObjectReference{Kind:""ReplicationController"", Namespace:""default"", Name:""redis-slave"", UID:""3d4f8c1a-bf73-11e5-8bd6-0242ac110002"", APIVersion:""v1"", ResourceVersion:""282"", FieldPath:""""}): reason: 'FailedCreate' Error creating: Pod ""redis-slave-"" is forbidden: no API token found for service account default/default, retry after the token is automatically created and added to the service account
E0120 12:43:21.022792       1 replication_controller.go:357] unable to create pods: Pod ""redis-slave-"" is forbidden: no API token found for service account default/default, retry after the token is automatically created and added to the service account
```

The service account exists, but there is no token to it.
",closed,False,2016-01-20 12:52:34,2018-01-15 10:31:48
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/17,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/17,Enable daemon sets and add Weave Scope,,closed,False,2016-01-20 13:55:12,2018-01-15 10:31:22
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/18,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/18,Consider making a PR to ECS agent,"...to fix https://github.com/aws/amazon-ecs-agent/issues/185 as well as `--pid=host`.
",closed,False,2016-01-21 01:42:53,2016-11-21 09:42:02
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/19,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/19,Investigate  `aws ecs start-task --container-instances` ,"It [was show](https://aws.amazon.com/blogs/compute/running-an-amazon-ecs-task-on-every-instance/) that

```
aws ecs start-task --cluster $cluster --task-definition cadvisor:1 --container-instances $instance_arn --region $region
```

is a thing, so it'd be possible to run `kube-proxy` and `kubelet` that way, once #18 is resolved.
",closed,False,2016-01-21 01:45:26,2016-11-21 09:42:24
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/20,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/20,Integrate Docker Machine example with weave-osx,,closed,False,2016-01-26 13:39:49,2016-05-06 18:14:02
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/21,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/21,Document GCE example,"Now that #5 is fulfilled for the GCE example, we can document it properly, i.e. provide instructions and not just scripts with _Obvious Naming Convention™_.
",closed,False,2016-01-28 14:33:43,2016-12-05 18:28:31
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/22,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/22,Document Docker Machine example,"_Obvious Naming Convention™_ is great, but people shouldn't have to read the code right away to see whether there are params to be set etc. Also the `kubectl` sequences are slightly different due to TLS being enabled.
",closed,False,2016-01-28 14:37:18,2016-12-05 18:28:25
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/23,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/23,Notes on etcd,"From speaking to @jonboulle, there are a few things that need to be documented.

Firstly, it's enough to pass only one address of a node, thereby the API server doesn't really have to care about having the `ETCD_CLUSTER_SIZE` parameter and we can just pass it `etcd1.weave.local` and drop our [amazing node list generator](https://github.com/weaveworks/weave-kubernetes-anywhere/blob/master/docker-images/apiserver-anywhere.sh#L3).

Secondly, etcd node replacement story won't work and magically just because the DNS name is the same, as all nodes are considered unique, although this could have worked with some very old version. So all user really benefits from is that no API server config change is needed when etcd cluster changes in any way.
",closed,False,2016-02-01 06:52:26,2018-01-15 10:31:30
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/24,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/24,Test Docker Machine example with EC2 driver,"First issue is with access to `/var/run/docker.sock`, it's owned by `root:docker`, so we need to either add `ubuntu` user to group `docker` or just prefix all commands with `sudo`...

Second issue is that the Weave Net ports need to be open, so we probably need to add so `aws`-foo to this.
",closed,False,2016-02-07 15:37:15,2016-02-08 23:26:47
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/25,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/25,Wrap-up EC2/Terraform example,"- [x] base Terraform code (https://github.com/weaveworks/weave-kubernetes-anywhere/commit/e191dc85ae4253144bbedf90d5e17e916dcdf858)
- [x] cleanup
  - [x] purge useless resources
  - [x] flatten what can be flattened
  - [x] add basic sensible parameters
  - [x] refactor as a module
- [x] fix cloud provider expectations (#26)
- [x] use systemd (#33)
- [x] <strike>find and solve cause of weird error on CLI calls in provisioning script (#27)</strike>
- [x] **_write the docs**_
",closed,False,2016-02-08 23:05:53,2016-04-28 11:17:30
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/26,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/26,Fix AWS cloud provider expectations,"```
E0208 22:42:32.342954       1 servicecontroller.go:187] Failed to process service delta. Retrying: Failed to create load balancer for service default/frontend: no instances found for name: ip-172-20-0-179
```

Anything else?
",closed,False,2016-02-08 23:07:34,2016-02-09 13:32:40
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/27,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/27,Weird AWS authentication error during provisioning,"```
A client error (AuthFailure) occurred when calling the DescribeTags operation: AWS was not able to validate the provided access credentials
```
",closed,False,2016-02-08 23:13:13,2016-05-06 18:12:32
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/28,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/28,See if we can override value of `KubernetesCluster` tag,"Ff that's easy enough, it'd be nice enable people to potentially deploy multiple clusters.
",closed,False,2016-02-08 23:32:07,2016-03-02 08:23:38
kubernetes-anywhere,asim,https://github.com/kubernetes/kubernetes-anywhere/pull/29,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/29,Fix cluter typo,,closed,True,2016-02-09 11:11:39,2016-02-10 14:33:38
kubernetes-anywhere,abuehrle,https://github.com/kubernetes/kubernetes-anywhere/pull/30,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/30,edited text added intro,"Here's the working copy. Still could use some tweaking here and there. How about a diagram?

<!-- Reviewable:start -->

[<img src=""https://reviewable.io/review_button.svg"" height=""40"" alt=""Review on Reviewable""/>](https://reviewable.io/reviews/weaveworks/weave-kubernetes-anywhere/30)

<!-- Reviewable:end -->
",closed,True,2016-02-11 15:04:40,2018-01-15 11:24:40
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/31,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/31,Get TLS certs from Amazon Certificate Manager,"It should be pretty doable actually. We should be able to transparently fetch certs from the ACM API when tools config container is ran.
",closed,False,2016-02-16 17:47:39,2016-12-05 18:28:09
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/32,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/32,Document how one can vendor images,"..and show how one can set image parameters in systemd-based deployment.
",closed,False,2016-02-18 16:53:11,2016-04-28 11:17:08
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/33,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/33,Use systemd on Ubuntu with EC2/Terraform,,closed,True,2016-02-18 16:54:05,2016-02-18 16:54:52
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/34,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/34,Image version tags,,closed,False,2016-02-18 16:57:25,2016-03-15 11:50:18
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/35,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/35,Remove kubelet hostname override with v1.2,"```
docker-images/kubelet-anywhere.sh-17-if [ ${CLOUD_PROVIDER} = 'aws' ]
docker-images/kubelet-anywhere.sh:18:then ## TODO: check if not needed with v1.2.0 is out (see kubernetes/kubernetes#11543)
docker-images/kubelet-anywhere.sh-19-  args=""${args} --hostname-override=${AWS_LOCAL_HOSTNAME}""
docker-images/kubelet-anywhere.sh-20-fi
```
",closed,False,2016-02-18 18:30:11,2016-12-05 18:28:18
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/36,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/36,Provide TLS-enabled systemd units,"we have `install-basic-systemd-units`, and should add `install-secure-systemd-units`
",closed,False,2016-02-19 14:38:26,2016-03-02 08:02:14
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/37,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/37,Provide a way to re-create server TLS cert,"to accommondate for external access
",closed,False,2016-02-19 14:39:19,2018-01-15 10:31:11
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/38,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/38,API server needs to listen on public address for external access,"This would be to go with #37. Actually, we can run an extra server on whatever address...
",closed,False,2016-02-20 01:15:25,2018-01-15 10:25:49
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/39,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/39,Rename `secure-config` to `pki`,"As it's really a containerised PKI.
",closed,False,2016-03-01 17:51:32,2016-03-15 11:49:54
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/40,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/40,Put etcd on a different subnet,,closed,False,2016-03-02 08:22:20,2018-01-15 10:25:40
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/41,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/41,Configure CI and publish latest builds on Docker Hub,"Travis builds take quite long when you require Docker, might have to look into other CIs. Should probably adopt parts of [`weaveworks/build-tools`](https://github.com/weaveworks/build-tools).
",closed,False,2016-03-03 13:15:17,2018-01-15 10:31:01
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/42,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/42,Provide a way to run end-to-end tests,,closed,False,2016-03-03 13:39:36,2016-03-03 16:01:13
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/43,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/43,Make end-to-end tests pass on EC2,,closed,False,2016-03-03 16:03:49,2016-12-05 18:29:06
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/44,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/44,ECR tokens do expire,,closed,False,2016-03-04 11:15:17,2016-03-04 13:32:25
kubernetes-anywhere,runningman84,https://github.com/kubernetes/kubernetes-anywhere/issues/45,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/45,"client is newer than server (client API version: 1.22, server API version: 1.21)","I just followed your tutorial ""Get started using a single Docker host"" using a trusty64 ubuntu host:

```
[root@22df29b2be99 resources]# setup-kubelet-volumes
+ KUBERNETES_ANYWHERE_TOOLS_IMAGE=weaveworks/kubernetes-anywhere:tools
++ docker inspect '--format={{.State.Status}}' kubelet-volumes
Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.21)
+ [[ '' = \c\r\e\a\t\e\d ]]
+ def_docker_root=/var/lib/docker
+ def_kubelet_root=/var/lib/kubelet
+ '[' -d /rootfs/etc ']'
+ '[' -f /rootfs/etc/os-release ']'
+ case ""$(eval `cat /rootfs/etc/os-release` ; echo $ID)"" in
+++ cat /rootfs/etc/os-release
++ eval 'NAME=""Ubuntu""' 'VERSION=""14.04.4' LTS, Trusty 'Tahr""' ID=ubuntu ID_LIKE=debian 'PRETTY_NAME=""Ubuntu' 14.04.4 'LTS""' 'VERSION_ID=""14.04""' 'HOME_URL=""http://www.ubuntu.com/""' 'SUPPORT_URL=""http://help.ubuntu.com/""' 'BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""'
+++ NAME=Ubuntu
+++ VERSION='14.04.4 LTS, Trusty Tahr'
+++ ID=ubuntu
+++ ID_LIKE=debian
+++ PRETTY_NAME='Ubuntu 14.04.4 LTS'
+++ VERSION_ID=14.04
+++ HOME_URL=http://www.ubuntu.com/
+++ SUPPORT_URL=http://help.ubuntu.com/
+++ BUG_REPORT_URL=http://bugs.launchpad.net/ubuntu/
++ echo ubuntu
+ docker_root_vol='          --volume=""/var/lib/docker/:/var/lib/docker:rw""         '
+ kubelet_root_vol='           --volume=""/var/lib/kubelet:/var/lib/kubelet:rw,rshared""         '
+ docker run --pid=host --privileged=true weaveworks/kubernetes-anywhere:tools nsenter --mount=/proc/1/ns/mnt -- mount --make-rshared /
docker: Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.21).
See 'docker run --help'.
+ docker create --volume=/:/rootfs:ro --volume=/sys:/sys:ro --volume=/dev:/dev --volume=/var/run:/var/run:rw '--volume=""/var/lib/kubelet:/var/lib/kubelet:rw,rshared""' '--volume=""/var/lib/docker/:/var/lib/docker:rw""' --volume=/var/run/weave/weave.sock:/docker.sock --name=kubelet-volumes weaveworks/kubernetes-anywhere:tools true
Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.21)
```
",closed,False,2016-03-04 22:24:41,2016-03-15 11:49:35
kubernetes-anywhere,squaremo,https://github.com/kubernetes/kubernetes-anywhere/issues/46,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/46,lack of shared mount,"Following the ""single host"" path of the README, in the first bash shell:

```
[root@cd55303fe5b1 resources]# compose up -d
Creating resources_controller-manager_1
Creating resources_scheduler_1
Creating resources_apiserver_1
Creating resources_kubelet_1
ERROR: Cannot start container 86fbeefafb7bce30dd3b6dfbe5bd9c7c1d15ccb4cc02140ba01e3fc8b78def29: Path /var/lib/kubelet is mounted on / but it is not a shared mount.
```
",closed,False,2016-03-08 11:58:08,2016-03-08 12:18:55
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/47,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/47,Find out why proxy doesn't work in iptables mode,,closed,False,2016-03-09 20:53:46,2016-03-23 18:03:04
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/48,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/48,Minor bashisms,"Still have a few non-critical ones like this:

```
+ [ = aws ]
/kubelet-anywhere: 15: [: =: unexpected operator
```
",closed,False,2016-03-10 08:17:31,2016-03-21 11:14:51
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/49,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/49,Toolbox and PKI refactoring,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/weave-kubernetes-anywhere/49)

<!-- Reviewable:end -->
",closed,True,2016-03-15 10:47:26,2016-03-15 11:47:31
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/50,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/50,Add support for unversioned tags for default stable kubernetes release,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/weave-kubernetes-anywhere/50)

<!-- Reviewable:end -->
",closed,True,2016-03-15 12:06:57,2016-03-15 12:30:46
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/51,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/51,Test Weave Net CNI plugin,,closed,False,2016-03-15 16:47:04,2016-03-23 12:45:42
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/52,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/52,Add CNI plugin support,"ping https://github.com/weaveworks/weave/issues/1991, https://github.com/weaveworks/weave/issues/1992 and https://github.com/weaveworks/weave/issues/2030

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/weave-kubernetes-anywhere/52)

<!-- Reviewable:end -->
",closed,True,2016-03-22 13:55:23,2016-03-22 13:57:58
kubernetes-anywhere,paulbellamy,https://github.com/kubernetes/kubernetes-anywhere/issues/53,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/53,Repository is confusingly named,"I expected it to be at https://github.com/weaveworks/kubernetes-anywhere
",closed,False,2016-03-23 16:48:55,2016-03-31 08:09:00
kubernetes-anywhere,paulbellamy,https://github.com/kubernetes/kubernetes-anywhere/issues/54,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/54,Readme should say you need a local docker,"As the instructions don't work on docker-machine unless you are _on_ the vm
",closed,False,2016-03-23 16:56:53,2016-05-05 16:39:40
kubernetes-anywhere,paulbellamy,https://github.com/kubernetes/kubernetes-anywhere/issues/55,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/55,Deployment instructions are unclear,"I _guess_ I run the toolbox, then run some commands inside it?

Could there be a `weaveworks/kubernetes-anywhere:deploy` image that just does all this?
Or just a `weaveworks/kubernetes-anywhere:toolbox deploy` tool, or something?
",closed,False,2016-03-23 16:58:23,2018-01-15 10:30:52
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/56,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/56,Top-level docs rewrite,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/weave-kubernetes-anywhere/56)

<!-- Reviewable:end -->
",closed,True,2016-03-24 00:04:24,2016-04-28 11:04:03
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/57,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/57,Add instructions using Docker for Mac,"We could leverage CNI, or use the proxy, which should still work as from the host's perspective `weave.sock` is still available, yet toolbox we have to be launched via the Docker plugin.
",closed,False,2016-03-30 20:08:51,2016-04-25 15:48:19
kubernetes-anywhere,vpetersson,https://github.com/kubernetes/kubernetes-anywhere/pull/58,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/58,Fixes typo in create-cluster.sh,"Fixes typo in `create-cluster.sh` for Docker Machine.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/58)

<!-- Reviewable:end -->
",closed,True,2016-04-07 10:50:17,2016-04-07 11:00:35
kubernetes-anywhere,paulbellamy,https://github.com/kubernetes/kubernetes-anywhere/issues/59,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/59,Terraform instructions are unclear,"- It's unclear how to access the nodes

`Once instances are up, you can login to any of the 3 worker nodes or the master ...`
Terraform should output the IP of the nodes, so I know where to ssh to after. And what the ssh key is, username?
- I got confused on where to start from, so I started from the aws-ec2 example directory. Apparently I shouldn't have.
- The copy-paste terraform file depends on (and refers to) variables which don't exist.
- Why are `cluster` and `cluster_config_flavour` hardcoded, while everything else you set is a variable?
- What to do (how to run `kubernetes-anywhere-toolbox`) after logging into a box is unclear. Do I `docker run kubernetes-anywhere-toolbox`? but there's no docker?
- `ssh -A` is required for kubernetes-anywhere-toolbox
",closed,False,2016-04-25 14:43:27,2018-01-15 11:18:18
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/60,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/60,Compose example with TLS,,closed,False,2016-04-28 09:41:09,2016-04-28 19:42:05
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/61,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/61,"Make v1.2 the default, as well as USE_CNI=yes","This needs some final testing, but all is working well already.
",closed,False,2016-04-28 17:59:43,2016-05-04 16:27:38
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/62,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/62,Enable TLS PKI in the Docker Compose example,"This PR closes #60

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/62)

<!-- Reviewable:end -->
",closed,True,2016-04-28 18:11:30,2016-04-28 19:42:05
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/63,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/63,Compose example should expose CNI,,closed,False,2016-04-28 19:00:31,2016-04-29 16:52:19
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/64,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/64,Pin version of AWS CLI,,closed,False,2016-05-03 10:41:31,2016-12-05 18:28:49
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/65,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/65,Make API server listen on localhost:8080 in single-node setup,"This would be convenient for single-node setup.
",closed,False,2016-05-03 10:42:59,2016-05-06 15:26:30
kubernetes-anywhere,phagunbaya,https://github.com/kubernetes/kubernetes-anywhere/issues/66,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/66,etcd processes cannot talk with each other,"I'm trying to setup a k8 cluster with following configuration :
1 Master node - (dev-env-00)
3 Worker nodes - (dev-env-01, dev-env-02, dev-env-03)
3 etcd nodes - (dev-env-etcd-00, dev-env-etcd-01, dev-env-etcd-02)

Problem is I see below logs in `weaveworks/kubernetes-anywhere:etcd` container

```
2016-05-03 11:11:00.155783 E | rafthttp: failed to write bd388e7810915853 on pipeline (dial tcp: lookup etcd3 on 172.17.0.1:53: no such host)
2016-05-03 11:11:00.156220 E | rafthttp: failed to write d282ac2ce600c1ce on pipeline (dial tcp: lookup etcd2 on 172.17.0.1:53: no such host)
...
...
2016-05-03 11:48:29.698076 W | rafthttp: the connection to peer bd388e7810915853 is unhealthy
2016-05-03 11:48:30.258878 W | etcdserver: failed to reach the peerURL(http://etcd3:2380) of member bd388e7810915853 (Get http://etcd3:2380/version: dial tcp: lookup etcd3 on 172.17.0.1:53: no such host)
```

Do I need to update the hosts to map `etcd{x}` to `dev-env-ectd-0{x}` or should my nodes be named as `etcd{x}` ?

Following are the setup commands I'm running :
- Starting weave router and proxy on each node

```
# weave launch-router dev-env-etcd-00 dev-env-etcd-01 dev-env-etcd-02 dev-env-00 dev-env-01 dev-env-02 dev-env-03
# weave launch-proxy --rewrite-inspect; weave expose -h ""$(hostname).weave.local""; eval $(weave env)
```
- Starting weave on etcd nodes

```
# docker run -d -e ETCD_CLUSTER_SIZE=3 --name=etcd{x} weaveworks/kubernetes-anywhere:etcd
```
- Start k8 master on dev-env-00

```
# docker run -d -e ETCD_CLUSTER_SIZE=3 --name=kube-apiserver weaveworks/kubernetes-anywhere:apiserver
# docker run -d --name=kube-controller-manager weaveworks/kubernetes-anywhere:controller-manager
# docker run -d --name=kube-scheduler weaveworks/kubernetes-anywhere:scheduler
```
- Starting k8 workers

```
# docker run --volume=""/:/rootfs"" --volume=""/var/run/docker.sock:/docker.sock"" weaveworks/kubernetes-anywhere:toolbox  setup-kubelet-volumes
# docker run -d --name=kubelet --privileged=true --net=host --pid=host --volumes-from=kubelet-volumes weaveworks/kubernetes-anywhere:kubelet
# docker run -d --name=kube-proxy --privileged=true --net=host --pid=host weaveworks/kubernetes-anywhere:proxy
```
",closed,False,2016-05-03 12:00:55,2018-01-15 10:30:42
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/67,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/67,Add API proxy (close #65),"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/67)

<!-- Reviewable:end -->
",closed,True,2016-05-03 12:59:59,2016-05-04 16:27:15
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/68,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/68,Make addons creation less verbose,,closed,False,2016-05-03 14:00:56,2016-05-04 12:36:27
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/69,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/69,Combine SkyDNS bits into one manifest,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/69)

<!-- Reviewable:end -->
",closed,True,2016-05-03 21:15:36,2016-05-04 12:36:27
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/70,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/70,Make `USE_CNI=yes` by default,"Ping #61.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/70)

<!-- Reviewable:end -->
",closed,True,2016-05-03 21:42:20,2016-05-04 13:35:54
kubernetes-anywhere,phagunbaya,https://github.com/kubernetes/kubernetes-anywhere/issues/71,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/71,Accessing k8 apiserver externally,"Right now, we cannot access the kube apiserver outside the docker container. This creates constraint for exposing apiserver to internet. We should expose port 8080 so that if anyone wants to do so, they can start the container by mapping port 8080.

```
# docker run -d -e ETCD_CLUSTER_SIZE=3 --name=kube-apiserver -p 8080:8080 weaveworks/kubernetes-anywhere:apiserver
```
",closed,False,2016-05-04 08:44:56,2018-01-15 10:23:45
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/72,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/72,External script targeting Docker for Mac and local Docker in general,"At the end of the day there is still a bit of a fuff around `docker run` args required for spinning cluster up and down, it'd be more convenient if user could install a simple script that would be as simple as `kubernetes-anywhere [up|down|*]` and handle full vs quick resets as well as upgrades. Updating `~/.kube/config` would be another responsibility of this script.
",closed,False,2016-05-04 10:46:05,2018-01-15 10:26:01
kubernetes-anywhere,paulbellamy,https://github.com/kubernetes/kubernetes-anywhere/issues/73,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/73,"fails to mount pki volumes if pki images exist, but pki containers don't","We should probably re-run the pki containers if they don't exist, regardless of whether the images already exist.
",closed,False,2016-05-05 09:41:19,2016-05-06 16:27:57
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/74,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/74,Make v1.2 the default,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/74)

<!-- Reviewable:end -->
",closed,True,2016-05-05 16:22:05,2016-05-05 16:38:34
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/75,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/75,Ditch apiproxy,"Close #65.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/75)

<!-- Reviewable:end -->
",closed,True,2016-05-06 15:14:36,2016-05-06 15:26:30
kubernetes-anywhere,paulbellamy,https://github.com/kubernetes/kubernetes-anywhere/pull/76,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/76,make setup-single-node more robust when pki images have been deleted,"Fixes #73, I think.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/76)

<!-- Reviewable:end -->
",closed,True,2016-05-06 15:45:06,2016-05-31 17:21:25
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/77,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/77,Enable SSH forwarding of the API server,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/77)

<!-- Reviewable:end -->
",closed,True,2016-05-06 16:21:51,2016-05-06 17:03:00
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/78,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/78,toolbox wrapper should check if shell is interactive,"It would be very handy to do this:

```
ssh user@node kubernetes-anywhere-toolbox print-secure-localhost-config > ~/.kube/config
```
",closed,False,2016-05-06 18:03:17,2016-05-31 10:18:36
kubernetes-anywhere,saruye,https://github.com/kubernetes/kubernetes-anywhere/issues/79,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/79,addons don't install correctly,"hi, 
I followed the instructions to setup a kubernetes cluster on three servers. Everything is working fine until I start installing the addons.yml. 
The command goes through and I get the output: 
`namespace ""kube-system"" created
replicationcontroller ""kube-dns-v11"" created
service ""kube-dns"" created`

When running `kubectl get rc` afterwards I get no result though. When running `kubectl get svc` I get: 

```
[root@10f10d7c3750 resources]# kubectl get svc
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.16.0.1    <none>        443/TCP   29m
```

I think the services are not created properly. It seems like this is related to the sky-dns container not starting correctly. The container `k8s_kube2sky....` is not starting correctly. When running `docker logs` on this container I get (many entries of): 
`I0508 19:41:50.565989       1 kube2sky.go:604] Ignoring error while waiting for service default/kubernetes: yaml: mapping values are not allowed in this context. Sleeping 1s before retrying`

I just started with playing with kubernetes and don't know how to debug this any further. I found that somebody had the same issue here: https://github.com/kubernetes/kubernetes/issues/23111 
Reading this sounds like it is an issue with authorization and secrets being wrong. I didn't do anything in this regard. And running kubectl get secrets returns nothing. 
Am I doing something wrong? I thought with this setup everything should be taken care of automatically. 

I appreciate any help!
arne
",closed,False,2016-05-08 19:49:53,2016-05-09 20:55:22
kubernetes-anywhere,paulbellamy,https://github.com/kubernetes/kubernetes-anywhere/pull/80,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/80,Split build-and-push into 2 separate scripts,"NO_PUSH and NO_BUILD are unintuitive, and didn't seem to work for me (leading me to accidentally push to docker hub. This seems safer.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/80)

<!-- Reviewable:end -->
",closed,True,2016-05-10 16:30:12,2016-05-18 14:30:38
kubernetes-anywhere,peterbourgon,https://github.com/kubernetes/kubernetes-anywhere/issues/81,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/81,"kubelet fails, memory cgroup subsystem not mounted","```
k1 ~ kubectl --server=http://kube-apiserver.weave.local:8080 get nodes
NAME      STATUS     AGE
k1        NotReady   3m
k2        NotReady   1m

k1 ~ docker ps
CONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS               NAMES
c46abae17b6b        weaveworks/kubernetes-anywhere:proxy     ""/proxy-anywhere""        3 minutes ago       Up 3 minutes                            kube-proxy
ab51f8bc9646        weaveworks/kubernetes-anywhere:kubelet   ""/kubelet-anywhere""      3 minutes ago       Up 3 minutes                            kubelet
942c866fd2bd        weaveworks/kubernetes-anywhere:etcd      ""/w/w /etcd-anywhere""    6 minutes ago       Up 6 minutes                            etcd2
b29fc98841e4        weaveworks/weaveexec:1.5.1               ""/home/weave/weavepro""   8 minutes ago       Up 8 minutes                            weaveproxy
c3a2e26e0c5e        weaveworks/weave:1.5.1                   ""/home/weave/weaver -""   8 minutes ago       Up 8 minutes                            weave

k1 ~ docker logs kubelet
+ /fix-nameserver
+ ip -4 addr show dev docker0
+ sed s/inet \([.0-9]*\)/nameserver \1/
+ grep -m1 -o inet [.0-9]*
+ config=/srv/kubernetes/kubelet/kubeconfig
+ master=kube-apiserver.weave.local
+ args=(--cluster-dns=""10.16.0.3"" --cluster-domain=""cluster.local"" --cloud-provider=""${CLOUD_PROVIDER}"" --allow-privileged=""true"" --logtostderr=""true"")
+ '[' -f /srv/kubernetes/kubelet/kubeconfig ']'
+ args+=(--api-servers=""http://${master}:8080"")
+ '[' '' = aws ']'
+ case ""${KUBERNETES_RELEASE}"" in
+ args+=(--docker-endpoint=""unix:///docker.sock"")
+ '[' yes = yes ']'
+ args+=(--network-plugin=""cni"" --network-plugin-dir=""/etc/cni/net.d"")
+ exec /hyperkube kubelet --cluster-dns=10.16.0.3 --cluster-domain=cluster.local --cloud-provider= --allow-privileged=true --logtostderr=true --api-servers=http://kube-apiserver.weave.local:8080 --docker-endpoint=unix:///docker.sock --network-plugin=cni --network-plugin-dir=/etc/cni/net.d
I0518 20:47:50.715926   14111 docker.go:283] Connecting to docker on unix:///docker.sock
W0518 20:47:50.717059   14111 server.go:445] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: stat /var/lib/kubelet/kubeconfig: no such file or directory. Trying auth path instead.
W0518 20:47:50.717140   14111 server.go:406] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat /var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.
I0518 20:47:50.717366   14111 plugins.go:71] No cloud provider specified.
I0518 20:47:50.717565   14111 manager.go:132] cAdvisor running in container: ""/docker/ab51f8bc96467b27e182f1f8d8efe4234b6c8c2826056b6d6367a7d7d981ff49""
I0518 20:47:50.755427   14111 fs.go:109] Filesystem partitions: map[/dev/vda1:{mountpoint:/rootfs major:254 minor:1 fsType: blockSize:0}]
I0518 20:47:50.776531   14111 manager.go:169] Machine: {NumCores:2 CpuFrequency:1999999 MemoryCapacity:4156375040 MachineID:b0943a7971b74bd2aeb09ebd0428b14b SystemUUID:20466056-9B41-4615-B6D1-2EDBB9C46C90 BootID:3ff224fe-4b08-497a-997e-84d75f717569 Filesystems:[{Device:/dev/vda1 Capacity:63278227456 Type:vfs Inodes:3916800}] DiskMap:map[254:0:{Name:vda Major:254 Minor:0 Size:64424509440 Scheduler:none}] NetworkDevices:[{Name:datapath MacAddress:b2:fa:b7:5c:79:29 Speed:0 Mtu:1410} {Name:dummy0 MacAddress:92:e5:0c:87:e6:59 Speed:0 Mtu:1500} {Name:eth0 MacAddress:04:01:ed:2c:f6:01 Speed:0 Mtu:1500} {Name:eth1 MacAddress:04:01:ed:2c:f6:02 Speed:0 Mtu:1500} {Name:weave MacAddress:8a:c9:63:22:d7:5b Speed:0 Mtu:1410}] Topology:[{Id:0 Memory:4156375040 Cores:[{Id:0 Threads:[0] Caches:[]}] Caches:[]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
I0518 20:47:50.780672   14111 manager.go:175] Version: {KernelVersion:3.16.0-4-amd64 ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.11.1 CadvisorVersion: CadvisorRevision:}
I0518 20:47:50.787222   14111 server.go:683] Watching apiserver
W0518 20:47:50.796493   14111 kubelet.go:508] Hairpin mode set to ""promiscuous-bridge"" but configureCBR0 is false, falling back to ""hairpin-veth""
I0518 20:47:50.797067   14111 kubelet.go:380] Hairpin mode set to ""hairpin-veth""
I0518 20:47:50.810553   14111 manager.go:212] Setting dockerRoot to /var/lib/docker
I0518 20:47:50.810605   14111 plugins.go:56] Registering credential provider: .dockercfg
E0518 20:47:50.838505   14111 kubelet.go:956] Image garbage collection failed: unable to find data for container /
I0518 20:47:50.838873   14111 server.go:645] Started kubelet
I0518 20:47:50.839079   14111 server.go:109] Starting to listen on 0.0.0.0:10250
E0518 20:47:50.876764   14111 kubelet.go:1016] Failed to start ContainerManager system validation failed - Following Cgroup subsystem not mounted: [memory]
I0518 20:47:50.884720   14111 manager.go:123] Starting to sync pod status with apiserver
I0518 20:47:50.884860   14111 kubelet.go:2356] Starting kubelet main sync loop.
I0518 20:47:50.884935   14111 kubelet.go:2365] skipping pod synchronization - [Failed to start ContainerManager system validation failed - Following Cgroup subsystem not mounted: [memory] container runtime is down]
E0518 20:47:50.900358   14111 manager.go:212] Docker container factory registration failed: docker found, but not using native exec driver.
I0518 20:47:50.918046   14111 factory.go:97] Registering Raw factory
I0518 20:47:50.939541   14111 manager.go:1003] Started watching for new ooms in manager
I0518 20:47:50.942003   14111 oomparser.go:182] oomparser using systemd
I0518 20:47:50.943649   14111 manager.go:256] Starting recovery of all containers
I0518 20:47:50.962796   14111 manager.go:261] Recovery completed
I0518 20:47:51.053546   14111 kubelet.go:1165] Successfully registered node k1
I0518 20:47:55.885556   14111 kubelet.go:2365] skipping pod synchronization - [Failed to start ContainerManager system validation failed - Following Cgroup subsystem not mounted: [memory]]
I0518 20:48:00.890245   14111 kubelet.go:2365] skipping pod synchronization - [Failed to start ContainerManager system validation failed - Following Cgroup subsystem not mounted: [memory]]
I0518 20:48:05.890698   14111 kubelet.go:2365] skipping pod synchronization - [Failed to start ContainerManager system validation failed - Following Cgroup subsystem not mounted: [memory]]
```
",closed,False,2016-05-18 20:52:20,2019-01-01 04:26:08
kubernetes-anywhere,2opremio,https://github.com/kubernetes/kubernetes-anywhere/issues/82,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/82,Test preventively whether shared mounts work in Docker,"Telling the users they need to take action is way better than simply documenting it and letting it break https://github.com/weaveworks/kubernetes-anywhere/blob/master/FIXES.md#official-docker-engine-packages-and-systemd
",closed,False,2016-05-19 11:40:49,2018-01-15 11:18:47
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/83,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/83,cleanup toolbox image,"The `reset-single-node` script does a good job cleaning things up, but it skips over toolbox containers...
",closed,False,2016-05-19 16:28:25,2016-12-05 18:30:05
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/84,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/84,introduce immutable tags,"Perhaps this could be done from CI, but for the start we can simply create and extra tag with git rev in `./publish`.
",closed,False,2016-05-19 16:30:26,2016-05-27 10:50:11
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/85,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/85,build toolbox from alpine,"It's over 400M right now, can be much smaller.
",closed,False,2016-05-21 12:59:07,2016-12-05 18:29:59
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/86,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/86,Use Alpine for toolbox (2x reduction in size),"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/86)

<!-- Reviewable:end -->
",closed,True,2016-05-21 16:56:35,2016-05-26 07:08:01
kubernetes-anywhere,paulbellamy,https://github.com/kubernetes/kubernetes-anywhere/issues/87,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/87,Resuming on a laptop running docker-machine logs errors,"After sleeping for a while and resuming the terminal on the docker-machine box gets spammed with:

`Error attempting heartbeat call to plugin server: connection is shut down`

I can't really tell which process is logging this, so not sure if it is kubernetes, or kubernetes-anywhere...
",closed,False,2016-05-24 09:10:00,2016-05-26 07:06:14
kubernetes-anywhere,josephwinston,https://github.com/kubernetes/kubernetes-anywhere/issues/88,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/88,ERROR: for kubelet  linux mounts: Path /var/lib/kubelet is mounted on / but it is not a shared mount.,"Just starting on Ubuntu 16.04 LTS with Docker 1.11.1 and I'm receiving the message:

``` bash
ERROR: for kubelet  linux mounts: Path /var/lib/kubelet is mounted on / but it is not a shared mount.
```

What can I provide/help with to chase down and squash this?
",closed,False,2016-05-24 13:50:47,2017-04-07 02:38:07
kubernetes-anywhere,2opremio,https://github.com/kubernetes/kubernetes-anywhere/issues/89,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/89,Incompatibility with former Docker versions,"I am using Docker 1.8.3 in one of my machines (which was released in October last year).

This is what I get when trying to use kubernetes anywhere:

```
[...]
Status: Downloaded newer image for weaveworks/weave:1.5.1
15ee7a1b66f87dbc98e2f28be6f74965de4ca52b953ab099444081d9f4e3761a
f8b4e6b17f2eb979506b301d4d0532eb64a65e1f9b6778d5f892d57fce73bf65
Creating data volumes container for kubelet (`kubelet-volumes`)...
Calling `mount  --make-rshared /` to ensure `kubelet-volumes` is functional
Unable to find image 'weaveworks/kubernetes-anywhere:toolbox-v1.2.4' locally
Warning: failed to get default registry endpoint from daemon (Error response from daemon: client is newer than server (client API
 version: 1.22, server API version: 1.20)). Using system default: https://index.docker.io/v1/
docker: Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.20).
See 'docker run --help'.
Unable to find image 'weaveworks/kubernetes-anywhere:toolbox-v1.2.4' locally
Warning: failed to get default registry endpoint from daemon (Error response from daemon: client is newer than server (client API
 version: 1.22, server API version: 1.20)). Using system default: https://index.docker.io/v1/
Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.20)
Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.20)
Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.20)
Creating single-node Public Key Infrastructure (PKI)...
Creating local container images (`kubernetes-anywhere:*-pki`)...
Creating data volumes containers for each of the commponents...
Unable to find image 'kubernetes-anywhere:apiserver-pki' locally
Warning: failed to get default registry endpoint from daemon (Error response from daemon: client is newer than server (client API
 version: 1.22, server API version: 1.20)). Using system default: https://index.docker.io/v1/
docker: Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.20).
See 'docker run --help'.
Unable to find image 'kubernetes-anywhere:controller-manager-pki' locally
Warning: failed to get default registry endpoint from daemon (Error response from daemon: client is newer than server (client API
 version: 1.22, server API version: 1.20)). Using system default: https://index.docker.io/v1/
docker: Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.20).
See 'docker run --help'.
Unable to find image 'kubernetes-anywhere:scheduler-pki' locally
Warning: failed to get default registry endpoint from daemon (Error response from daemon: client is newer than server (client API
 version: 1.22, server API version: 1.20)). Using system default: https://index.docker.io/v1/
docker: Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.20).
See 'docker run --help'.
Unable to find image 'kubernetes-anywhere:kubelet-pki' locally
Warning: failed to get default registry endpoint from daemon (Error response from daemon: client is newer than server (client API
 version: 1.22, server API version: 1.20)). Using system default: https://index.docker.io/v1/
docker: Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.20).
See 'docker run --help'.
Unable to find image 'kubernetes-anywhere:proxy-pki' locally
Warning: failed to get default registry endpoint from daemon (Error response from daemon: client is newer than server (client API
 version: 1.22, server API version: 1.20)). Using system default: https://index.docker.io/v1/
docker: Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.20).
See 'docker run --help'.
Unable to find image 'kubernetes-anywhere:toolbox-pki' locally
Warning: failed to get default registry endpoint from daemon (Error response from daemon: client is newer than server (client API
 version: 1.22, server API version: 1.20)). Using system default: https://index.docker.io/v1/
docker: Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.20).
See 'docker run --help'.
```
",closed,False,2016-05-25 18:51:07,2018-01-15 10:25:25
kubernetes-anywhere,2opremio,https://github.com/kubernetes/kubernetes-anywhere/issues/90,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/90,"/var/log/containers not present in the host, which breaks the fluentd add ons","The kubernetes fluentd add-ons rely on kubelet creating /var/log/containers in the host in order to mount them it in the container and parse the containing logs. See https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/fluentd-es-image/td-agent.conf#L2-L6

However, the containerized kubelet in k8s-anywhere only creates /var/log/containers locally, not getting exposed to the host and in turn not propagated to the fluentd add-on.
",closed,False,2016-05-25 21:06:59,2016-05-27 15:11:58
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/91,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/91,[WIP] Public API server,"@tomwilkie @2opremio @peterbourgon would be great if some of you could take a look at the general picture here. The idea is to sort of do self-hosting for this and spin up a service of type `LoadBalancer`, then find out what's the external DNS/IP will be, next actually use tools implemented in this PR to create image with certs in it, which will be spun up after that. The intention is to make create internal PKI and make API server available before we know what external address may be, and don't depend on that being a static one.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/91)

<!-- Reviewable:end -->
",closed,True,2016-05-26 09:04:23,2018-01-15 11:24:31
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/92,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/92,`WEAVE_VERSION` is ignored by Compose,,closed,False,2016-05-26 12:17:17,2016-05-27 10:50:11
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/93,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/93,"Bump Weave Net version, close #92 and #84","<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/93)

<!-- Reviewable:end -->
",closed,True,2016-05-26 12:42:24,2016-05-27 10:50:11
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/94,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/94,Turns out `-no-pki` addon flavours never worked,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/94)

<!-- Reviewable:end -->
",closed,True,2016-05-26 13:05:47,2016-05-27 13:09:44
kubernetes-anywhere,devent,https://github.com/kubernetes/kubernetes-anywhere/issues/95,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/95,Unable to remove filesystem shm device or resource busy,"Hello,
I was trying out the guide how to install kubernetes with weaver net from
https://github.com/weaveworks/kubernetes-anywhere
My system is 

```
Linux kubmaster 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-1 (2016-03-06) x86_64 GNU/Linux
Docker version 1.11.1, build 5604cbe
weave script 1.5.2
weave router 1.5.2
weave proxy  1.5.2
Unable to find weaveworks/plugin:1.5.2 image.
```

```
scheduler-v1.2.4: Pulling from weaveworks/kubernetes-anywhere
81cc5f26a6a0: Already exists
...
06fe64dea086: Pull complete
Digest: sha256:6b11823f5bd400d50bc74694452ed87c38d485f57b9390911cd78b0528f8019e
Status: Downloaded newer image for weaveworks/kubernetes-anywhere:scheduler-v1.2.4
Creating kube_etcd_1
Creating kube_kubelet_1
Creating kube_proxy_1
Creating kube_apiserver_1
Creating kube_controller-manager_1
Creating kube_scheduler_1
Error response from daemon: Unable to remove filesystem for f91045865885327b10df0f7694b0e06dfe6012b8fc4e246c12852c5bb7f15e55: remove /var/lib/docker/containers/f91045865885327b10df0f7694b0e06dfe6012b8fc4e246c12852c5bb7f15e55/shm: device or resource busy
```

I'm not sure if it's a serious problem because I think everything works.

```
$ weave status dns
etcd1        10.32.0.3       3b34581e1422 b2:97:07:ac:bb:58
kube-apiserver 10.32.0.4       6f09bec8b3ca b2:97:07:ac:bb:58
kube-controller-manager 10.32.0.5       df7aed0f38ed b2:97:07:ac:bb:58
kube-scheduler 10.32.0.6       73ca5b1784de b2:97:07:ac:bb:58
kubmaster    10.32.0.1       weave:expose b2:97:07:ac:bb:58

```
",closed,False,2016-05-27 08:24:02,2018-01-15 10:23:20
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/96,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/96,Call `mount --make-rshared /` from kubelet to ensure it persists on restart,,closed,False,2016-05-27 10:54:22,2016-05-31 08:47:31
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/97,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/97,CNI plugin should not default to latest,"With bc29dd2 it turns out that now if one calls toolbox with `docker run -e WEAVE_VERSION ...` and `WEAVE_VERSION` is not set, it will propagate as unset and cause kubelet to pull latest tag.
",closed,False,2016-05-27 13:11:30,2016-05-27 13:44:49
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/98,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/98,Ensure Weave Net CNI wrappers do not defailt to latest tag (close #97),"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/98)

<!-- Reviewable:end -->
",closed,True,2016-05-27 13:13:25,2016-05-27 13:44:48
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/99,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/99,Make toolbox use immutable tags for single node,"That that we have them, let's ensure tight coupling of toolbox and components it deploys.
",closed,False,2016-05-27 13:26:15,2016-05-27 14:54:16
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/100,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/100,Stronger revision coupling with immutable tags for single-node setup,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/100)

<!-- Reviewable:end -->
",closed,True,2016-05-27 13:35:17,2016-05-27 14:54:15
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/101,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/101,Wipe `/var/lib/kubelet` as part of `reset-single-node`,"Currently we don't, and that gets kubelet into a pickle in some cases (e.g. Boot2Docker), should clean it up. Supposedly the issue to do with real mount path being under `/mnt/sda1` prefix.
",closed,False,2016-05-27 13:55:50,2018-01-15 10:23:32
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/102,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/102,Improve kubelet volumes,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/102)

<!-- Reviewable:end -->
",closed,True,2016-05-27 14:46:33,2016-05-27 15:11:58
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/103,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/103,"Use `--embed-certs`, few cosmetic tweaks","<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/103)

<!-- Reviewable:end -->
",closed,True,2016-05-27 14:59:38,2016-05-27 16:37:16
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/104,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/104,Take extra SANs as an agrument,"ping #91

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/104)

<!-- Reviewable:end -->
",closed,True,2016-05-31 09:48:12,2016-05-31 10:06:25
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/105,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/105,Few cosmetic cherry-picks,"Here are some cosmetic changes that were implemented in `test-new-toolbox-in-ec2` branch, and could otherwise get lost.

<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/105)

<!-- Reviewable:end -->
",closed,True,2016-05-31 10:04:02,2016-05-31 10:18:35
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/106,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/106,"Update LICENSE and NOTICE files, ensure copyright boilerplate is present","<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/106)

<!-- Reviewable:end -->
",closed,True,2016-05-31 10:52:09,2016-05-31 11:19:11
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/107,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/107,Split phase1 and phase2,"The docs are still to inherit the idea of new layout, but here is the starting point.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/weaveworks/kubernetes-anywhere/107)

<!-- Reviewable:end -->
",closed,True,2016-05-31 17:16:12,2016-05-31 18:07:47
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/108,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/108,move min-turnup into kubernetes-anywhere,,closed,True,2016-05-31 18:09:51,2016-06-06 22:53:21
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/109,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/109,Cleanup branches,"All branches other then master are stale right now and should be deleted.
",closed,False,2016-06-01 10:14:57,2018-01-15 10:23:54
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/110,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/110,Fix Travis CI,"This got missed out in #107.
",closed,True,2016-06-01 10:43:21,2016-06-01 17:05:56
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/111,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/111,Simplified EC2 setup,"### Phase 1

Master:
- single instance
- runs etcd
- has an EIP

Nodes:
- TBC
### Phase 2
- easy to obtain kubeconfig file for local use
- use EIP in PKI image creation (or consider approach which #112 would enable)
",closed,False,2016-06-01 11:05:54,2018-01-15 10:22:33
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/112,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/112,[RFC] Create pki-admin image and tool to generate apiserver images with new certs ,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/kubernetes-anywhere/112)

<!-- Reviewable:end -->
",closed,True,2016-06-01 11:13:25,2016-09-11 10:05:41
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/113,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/113,Move container images from weaveworks org,"Can probably use GCR.

It may be also a good idea to turn the current naming scheme

```
<org>/kubernetes-anywhere:<component>-<version>
```

into

```
kubernetes-anywhere/<component>:<version>
```

or even

```
k8s-anywhere/<component>:<version>
```
",closed,False,2016-06-02 10:30:09,2018-01-15 10:22:43
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/114,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/114,[WIP] Simplify phase 1 in EC2,"ref #111

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/kubernetes-anywhere/114)

<!-- Reviewable:end -->
",closed,True,2016-06-02 14:56:19,2016-09-11 10:08:03
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/115,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/115,[WIP/RFC]  Bootstrap EC2 with Ansible,"The purpose is to make user data (`#cloud-config`) very short and easier to parametrise.

Current version of [`phase1/aws-ec2-terraform/phase2_implementations/simple-weave-single-master.yaml`](https://github.com/errordeveloper/kubernetes-anywhere/blob/0097b50770b71639c776293c3f445d05e9e110b5/phase1/aws-ec2-terraform/phase2_implementations/simple-weave-single-master.yaml) (as shown in #114) has many lines of bash that install Docker and create systemd units etc.

The idea is to turn it into a two-step version:
1. install and start Docker
2. run Ansible through toolbox container

[New `#cloud-config`](https://github.com/kubernetes/kubernetes-anywhere/pull/115/files#diff-7f4d1e0c0ab90e45984666baee7c6baaR1) is included now, and you can see how much simpler it has become.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/kubernetes-anywhere/115)

<!-- Reviewable:end -->
",closed,True,2016-06-02 17:30:48,2016-09-11 10:06:10
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/116,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/116,How to expand a single-node cluster,"It's fairly quick to fire-up a single-node Docker for Mac cluster, and adding extra nodes using Docker Machine is easy and makes a great demo, but it isn't currently documented.
",closed,False,2016-06-03 11:44:54,2018-01-15 10:23:06
kubernetes-anywhere,ekozan,https://github.com/kubernetes/kubernetes-anywhere/issues/117,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/117,How to run with docker-machine ?? ,"I try to run it but I think I havent understand something 

i have follow the mac guid but when i run on my mac 

```
docker run --rm -ti   -v /:/rootfs -v /var/run/weave/weave.sock:/docker.sock   weaveworks/kubernetes-anywhere:toolbox sh -c 'setup-single-node && compose -p kube up -d'
```

I got

```
Creating data volumes container for kubelet (`kubelet-volumes`)...
Calling `mount --make-rshared /` to ensure `kubelet-volumes` is functional
docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.
See 'docker run --help'.
Cannot connect to the Docker daemon. Is the docker daemon running on this host?
Cannot connect to the Docker daemon. Is the docker daemon running on this host?
Cannot connect to the Docker daemon. Is the docker daemon running on this host?
Creating single-node Public Key Infrastructure (PKI)...
Creating local container images (`kubernetes-anywhere:*-pki`)...
Creating data volumes containers for each of the commponents...
docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.
See 'docker run --help'.
docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.
See 'docker run --help'.
docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.
See 'docker run --help'.
docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.
See 'docker run --help'.
docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.
See 'docker run --help'.
docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.
See 'docker run --help'.
```
",closed,False,2016-06-05 18:37:31,2016-06-07 22:23:48
kubernetes-anywhere,netroby,https://github.com/kubernetes/kubernetes-anywhere/issues/118,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/118,lookup kube-apiserver.weave.local: no such host,"kubectl get nodes
Unable to connect to the server: dial tcp: lookup kube-apiserver.weave.local: no such host

But dns work fine

⋊> ~ weave status dns                                                                                         09:30:15
etcd1        10.32.0.3       6aab145bc811 e2:58:9c:6a:47:cf
kube-apiserver 10.32.0.4       157f559b2ee1 e2:58:9c:6a:47:cf
kube-controller-manager 10.32.0.5       1bd0668ae2a1 e2:58:9c:6a:47:cf
kube-scheduler 10.32.0.6       8c5ba92c9736 e2:58:9c:6a:47:cf
",closed,False,2016-06-06 01:32:32,2017-12-16 01:11:12
kubernetes-anywhere,netroby,https://github.com/kubernetes/kubernetes-anywhere/issues/119,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/119,guestbook-example can not execute due to gcr.io was blocked by firewall,"guestbook-example can not execute due to gcr.io was blocked by firewall
we can not reach gcr.io. so we can not success create image from gcr.io images

```
 7m     3m      2   {kubelet w}             Warning     FailedSync  Error syncing pod, skipping: failed to ""StartContainer"" for ""POD"" with ErrImagePull: ""image pull failed for gcr.io/google_containers/pause:2.0, this may be because there are no credentials on this request.  details: (API error (500): Get https://gcr.io/v1/_ping: dial tcp 173.194.72.82:443: i/o timeout\n)""
```
",closed,False,2016-06-06 03:04:52,2018-01-15 10:33:28
kubernetes-anywhere,netroby,https://github.com/kubernetes/kubernetes-anywhere/issues/120,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/120,How to expose service port to host ip:port?,"How to expose service port to host ip:port?

For example i want to expose guestbook example 80 port to host 80 port
",closed,False,2016-06-07 01:47:55,2017-12-16 01:11:30
kubernetes-anywhere,netroby,https://github.com/kubernetes/kubernetes-anywhere/issues/121,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/121,Add /usr/bin/kubecmd for getting kubectl console quick,"```
#!/bin/sh
eval $(weave env)
docker run --rm -ti --volumes-from=kube-toolbox-pki weaveworks/kubernetes-anywhere:toolbox
```

Here is my quick /usr/bin/kubecmd shell scripts. to let me quick get into the toolbox.
can we add a tips to document?

If we want to run kubectl, we can just type kubecmd , then we will get into the toolbox shell
",closed,False,2016-06-07 03:11:09,2018-01-15 10:22:55
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/122,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/122,Update copyright notice,,closed,True,2016-06-07 09:16:12,2016-06-09 23:50:40
kubernetes-anywhere,ashutosh,https://github.com/kubernetes/kubernetes-anywhere/issues/123,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/123,etcd container dies,"I am struggling to get ""kubernetes-anywhere"" (KA) running on a set of ubuntu 16.04 vms. I am not able to keep the etcd containers running. The problem as far as I can debug is because the container is using container-id in the URL instead of the container name (e.g. ""--name=etcd1"") that is passed as the parameter. I am attaching a portion of the docker logs. I do not know enough to see if this is an issue. If anyone can give me some pointers that would be very helpful.

**=== docker run command ===**
`docker run -d -e ETCD_CLUSTER_SIZE=3 --name=etcd1 weaveworks/kubernetes-anywhere:etcd`

**=== docker logs attached ===**

```
ashutosh@control-vm01:~/works-cluster$ docker logs etcd1
+ sed s|\([1-9]*\)|etcd\1=http://etcd\1:2380|g
+ seq -s , 1 3
+ ETCD_INITIAL_CLUSTER=etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380
+ export ETCD_INITIAL_CLUSTER
+ hostname -s
+ n=c7adab14484e c=http://c7adab14484e:2379 p=http://c7adab14484e:2380
+ exec etcd --name=c7adab14484e --listen-peer-urls=http://c7adab14484e:2380 --initial-advertise-peer-urls=http://c7adab14484e:2380 --listen-client-urls=http://c7adab14484e:2379 --advertise-client-urls=http://c7adab14484e:2379
2016-06-08 03:03:18.590694 I | flags: recognized and used environment variable ETCD_INITIAL_CLUSTER=etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380
2016-06-08 03:03:18.590766 W | flags: unrecognized environment variable ETCD_CLUSTER_SIZE=3
2016-06-08 03:03:18.590790 I | etcdmain: etcd Version: 2.2.1
2016-06-08 03:03:18.590796 I | etcdmain: Git SHA: 75f8282
2016-06-08 03:03:18.590800 I | etcdmain: Go Version: go1.5.1
2016-06-08 03:03:18.590804 I | etcdmain: Go OS/Arch: linux/amd64
2016-06-08 03:03:18.590818 I | etcdmain: setting maximum number of CPUs to 2, total number of available CPUs is 2
2016-06-08 03:03:18.590825 W | etcdmain: no data-dir provided, using default data-dir ./c7adab14484e.etcd
2016-06-08 03:03:18.590970 I | etcdmain: listening for peers on http://c7adab14484e:2380
2016-06-08 03:03:18.590997 I | etcdmain: listening for client requests on http://c7adab14484e:2379
2016-06-08 03:03:18.591119 I | etcdmain: stopping listening for client requests on http://c7adab14484e:2379
2016-06-08 03:03:18.591132 I | etcdmain: stopping listening for peers on http://c7adab14484e:2380
2016-06-08 03:03:18.591141 C | etcdmain: couldn't find local name ""c7adab14484e"" in the initial cluster configuration
```
",closed,False,2016-06-08 07:33:47,2016-06-13 08:25:45
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/124,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/124,overlay min-turnup into kubernetes anywhere,,closed,True,2016-06-10 00:44:08,2016-06-13 20:47:07
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/125,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/125,run weave for the pod network in ansible phase2,"until kubenet is ready to go.
",closed,False,2016-06-10 22:01:30,2016-08-12 20:50:12
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/126,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/126,design and implement phase2 and phase3 handoff (a.k.a. deploy addons),"Right now addon configuration can be generated from the cluster config object. We need a way to run it automatically after phase2 on the master completes. kube-up uses the kube-addon-manager.sh which is a mess which we should avoid. Open questions:
- Where are templates expanded? On the local machine or the master?
- Do we support upgrades? If so how?
- Is this a continuously running and reconciling process or does it only run once?
",closed,False,2016-06-10 22:16:35,2018-01-15 10:20:49
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/127,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/127,kubernetes-anywhere v1.4 umbrella,"This is an umbrella issue to track the next work that needs to be done in kubernetes-anywhere.
- [ ] Implement weave net for control plane pods
- [ ] strawman contract for phases #133
- [ ] Move of cloud provider pod network (implement pod network with weave at least until kubenet is stable) #125
- [ ] Implement addon management #126
- [ ] Implement some sort of release process
- [ ] Run comformance tests in jenkins
- [ ] Fixup documentation and polish UX

cc @kubernetes/sig-cluster-lifecycle 
",closed,False,2016-06-14 00:24:20,2018-01-15 11:19:09
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/128,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/128,Usage of Kconfig,"There is a number of nice things about Kconfig.

If we would like to stick with Kconfig, it'd be nicer/easier to make it part of the build process, but it will require compiler installed. We probably could ship a container with all the dependencies.

An alternative could be to use Python, as it's very popular and easy to understand, it also doesn't require user to compile anything etc.

I've had a quick look, and there exists a usable [Kconfig parser](https://github.com/ulfalizer/Kconfiglib). We could potentially consider triggering jsonnet run from Python directly as well.
",closed,False,2016-06-14 14:13:58,2018-01-15 10:32:40
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/129,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/129,Few tweaks to the proposal document,,closed,True,2016-06-14 14:55:15,2016-06-15 18:27:01
kubernetes-anywhere,obi1kenobi,https://github.com/kubernetes/kubernetes-anywhere/issues/130,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/130,"provision.sh downloads files over HTTP, then executes them","See:
https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase1/google-compute-engine/provision.sh#L29
https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase1/google-compute-engine/provision.sh#L62

`git.io` supports HTTPS, and `provision.sh` should use it. Otherwise, it would be trivial for an attacker to launch a man-in-the-middle (MITM) attack against the machine running the script. Since the files being downloaded are executed by the same script promptly thereafter, this is a very easy way for an attacker to achieve remote code execution on the machine running the script.
",closed,False,2016-06-14 18:44:27,2016-06-14 22:19:33
kubernetes-anywhere,obi1kenobi,https://github.com/kubernetes/kubernetes-anywhere/pull/131,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/131,Use HTTPS to download executables in provision.sh,"`git.io` supports HTTPS, and `provision.sh` should use it. Otherwise, it would be trivial for an attacker to launch a man-in-the-middle (MITM) attack against the machine running the script. Since the files being downloaded are executed by the same script promptly thereafter, this is a very easy way for an attacker to achieve remote code execution on the machine running the script.

Fixes #130.
",closed,True,2016-06-14 18:46:03,2016-06-14 22:20:26
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/132,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/132,"Add Kconfig options for pod, service and node cidr",,closed,False,2016-06-14 22:05:42,2018-01-15 11:19:43
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/133,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/133,What the basic contract between phases should be,"I think we need do define the contract between phases a bit better, and hopefully avoid bikeshedding  exact attributes names and object format.

More specifically, I am talking about things like ""what info phase 1 is expected to pass to phase2?"" vs ""phase 2 is expected to do some basic auto-detection of certain artefacts of phase 1"".

It's important to node that in some cases phase 1 doesn't have the info you'd expect it to have in theory, e.g. IPs of all nodes (at least in EC2 it's complicated). And passing some information makes phase 1 a bit more complicated, e.g. passing master IP to nodes introduces a dependency, where phase 1 has to do more synchronisation.
",closed,False,2016-06-16 19:22:18,2018-01-15 10:21:00
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/134,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/134,add a make commands that start phase1,,closed,True,2016-06-17 21:50:03,2016-06-22 20:27:37
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/135,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/135,remove stray ( in proposal,,closed,True,2016-06-17 21:50:11,2016-06-17 22:31:54
kubernetes-anywhere,leecalcote,https://github.com/kubernetes/kubernetes-anywhere/issues/136,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/136,Docker for Mac: ERROR: for kubelet  oci runtime error,"When bootstrapping a single-node local cluster per the instructions on [Docker for Mac](https://github.com/kubernetes/kubernetes-anywhere/blob/master/DOCKER_FOR_MAC.md) page, many kubernetes containers are created:

> Creating kube_proxy_1
> Creating kube_etcd_1
> Creating kube_kubelet_1
> Creating kube_apiserver_1
> Creating kube_scheduler_1
> Creating kube_controller-manager_1

 the following error is returned:

> ERROR: for kubelet  oci runtime error: rootfs_linux.go:89: jailing process inside rootfs caused ""pivot_root invalid argument""
> Error response from daemon: Unable to remove filesystem for a2decf5c3499875d9ba0fe9d7428d8191c021d0d5291b87a003aef9cca13f66c: remove /var/lib/docker/containers/a2decf5c3499875d9ba0fe9d7428d8191c021d0d5291b87a003aef9cca13f66c/shm: device or resource busy

I'm using:
- OS X El Capitan 10.11.5
- Docker for Mac Version 1.12.0-rc2-beta16 (build: 9493)
",closed,False,2016-06-19 19:24:30,2018-01-15 10:32:21
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/137,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/137,Docker version pinning strategy,"We are currently using 1.11, but the versions aren't pinned and we will get 1.12 as soon as it's out.
",closed,False,2016-06-20 09:44:13,2018-01-15 10:21:48
kubernetes-anywhere,the-destro,https://github.com/kubernetes/kubernetes-anywhere/issues/138,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/138,On Docker For Mac does not handle suspend/resume well,,closed,False,2016-06-20 21:17:00,2018-01-15 10:20:30
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/issues/139,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/139,User should be able to run terraform apply from any directory,"Right now gce.tf assumes that it is being invoked from phase1/gce as `terraform apply .tmp` and it hard codes the paths to different files. For example, `san-extras` is assumed to be at `../../phase1b/crypto/san-extras` and hence `terraform apply` doesn't work if we invoke it from any other directory than phase1/gce. 

Users should have the flexibility to run terraform apply from where ever they want.

cc @mikedanese 
",closed,False,2016-06-22 01:43:12,2018-01-15 10:20:20
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/issues/140,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/140,Notify when phase2 succeeds or fails,"Right now terraform apply returns with a success message after phase1 and phase1b are complete. phase2 is run on the nodes and there is nothing that indicates whether that succeeded or failed. Users should be notified about phase2's success or failure.

cc @mikedanese 
",closed,False,2016-06-22 02:16:15,2018-01-15 11:20:45
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/issues/141,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/141,terraform destroy .tmp for phase1/gce fails due to routes,"`routecontroller` installs GCE routes to route cluster traffic to appropriate pods on the cluster nodes. Since these routes aren't being managed by terraform, `terraform destroy` fails on the same cluster it setup with the following error message:

```
Error applying plan:

1 error(s) occurred:

* google_compute_network.network: The network resource 'networkname' is already being used by 'k-1-14679864-381e-11e6-b088-42010a800002'


Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.
```

This isn't the expected behavior. I should be able to destroy the cluster I setup using terraform.

cc @mikedanese 
",closed,False,2016-06-22 03:04:41,2018-01-15 11:19:50
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/pull/142,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/142,Output master IP.,"This change makes terraform apply print the master IP at the end of the output.

cc @mikedanese 
",closed,True,2016-06-22 07:18:03,2016-06-22 18:27:22
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/pull/143,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/143,A bunch of typo fixes.,"cc @mikedanese 
",closed,True,2016-06-22 07:18:51,2016-06-22 16:18:28
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/144,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/144,Update documentation,"It's out of date with all the new developments.
",closed,False,2016-06-22 18:45:41,2018-01-15 11:20:23
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/145,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/145,PKI can be created with Terraform,"@errordeveloper said:

> Just for the record, certs can be generated using Terraform. I haven't tested this feature and only learned about it somewhat recently, I would love to hear if anyone has tried it in Kubernetes context or in general. It does seem appropriate for min-turnup and any other solutions using Terraform.

In https://github.com/kubernetes/kube-deploy/issues/35
",closed,False,2016-06-22 19:52:19,2016-06-27 21:48:02
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/146,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/146,generate pki with terraform,"I think this is an improvement. It reduces moving parts and moves kubeconfig generation out of phase2. This also improves the security of the GCE setup by requiring ComputeReadScope to see the apiserver private key (instead of StorateReadScope) and completely removing the ability to see the root CA private key (instead of requiring StorageReadScope (i know, it's bad)). Please review commits individually.
",closed,True,2016-06-23 01:11:50,2016-06-27 20:54:10
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/147,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/147,Add Dockerfile,,closed,True,2016-06-23 22:19:31,2016-06-24 00:38:15
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/149,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/149,We should only have one bootstrapping phase,"We need to merge the two bootstrapping phases.
",closed,False,2016-06-24 21:18:00,2018-01-15 10:21:10
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/150,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/150,Rename phase folders,"People complain that they don't mean anything. I'm thinking we do something that if lexically sorted would produce the correct order of execution, but with more descriptive names, e.g.:

```
10_provision
20_bootstrap
30_addons
```
",closed,False,2016-06-27 20:57:03,2018-01-15 10:21:17
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/151,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/151,Run weave node agent in a daemonset and use it to configure the pod network,"It should use kubelet cni plugin and be deployed as an addon in phase3.
",closed,False,2016-06-28 02:05:42,2018-01-15 11:20:12
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/152,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/152,How to get a `kubeconfig` file for connecting to the cluster?,"Should we have Terraform output a `kubeconfig` file for the user?

Not sure the best way to accomplish this. Maybe using `local-exec` provisioner with the `kubeconfig` jsonnet function result piped into a file.
",closed,False,2016-06-28 02:10:06,2016-06-28 03:34:18
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/153,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/153,jsonnet `kubeconfig` outputs invalid json/yaml,"It's currently outputting some hybrid of json/yaml which doesn't seem to be valid json. (`kubectl` is complaining when I try to pull the `/srv/kubernetes/kubeconfig.json` from my cluster to check if my bringup works).

example file: https://gist.github.com/d9e9dd1008328583d497219c5a18dd36
",closed,False,2016-06-28 02:13:55,2016-06-28 02:49:03
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/154,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/154,ansible hard codes a (stale) version of kubectl to install,"https://github.com/kubernetes/kubernetes-anywhere/blob/0459fbbc0411318f7e0a07d53be39712483961b2/phase2/ansible/playbooks/roles/master/tasks/main.yml#L4

We know the desired version from the effective config.

Can the checksum be omitted? Or maybe we don't care that much about what version of `kubectl` happens to be a bit stale?
",closed,False,2016-06-28 03:37:33,2018-01-15 11:20:18
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/155,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/155,Azure: Phase1 Implementation + necessary Phase2 Changes,"Of the remaining issues in [the closed min-turnup PR](https://github.com/kubernetes/kube-deploy/pull/106):
1. ""terraform resource names"": I still punted on this. I've started to a couple times, but it's a rather surprisingly large amount of extra changes/text with unclear benefit to me. In terms of enabling side-by-side deployments, it seems there are other problems to sort out. (Comparing to the GCE one, it looks like GCE is going to hit the same ""multiple resources with same name"" error due to the TLS/kubeconfig assets not being named uniquely per-`instance_prefix`.) I still think it'd be better to support multiple clusters by having entirely separate directories per-cluster. With their own separate `tfstate`s, `kubeconfig`s, etc.
2. ""prefer jsonnet"": I've added the Azure cloud config file parameter into the kube-{apiserver,scheduler} jsonnet templates rather than rewriting them to jinja2. (**Jinja2 is still used by `kubelet.service`.** I just added my parameters and left the templating language as I found it)
3. ""pki in terraform"": Mike did this in https://github.com/kubernetes/kubernetes-anywhere/pull/146

Please Note:
1. Workaround 1: Docker start fails after install. https://github.com/colemickens/kubernetes-anywhere/blob/5007ff0daa04ea32d81ea463ed72c50f73539c83/phase1/azure/configure-vm.sh#L19
2. Workaround 2: Terraform bug that causes flakiness in Azure: https://github.com/colemickens/kubernetes-anywhere/blob/5007ff0daa04ea32d81ea463ed72c50f73539c83/phase1/azure/do#L18
3. I'm not sure which is correct for kubelet's volume share mode. In this branch, it's already at `-v /var/lib/kubelet/:/var/lib/kubelet:shared` but I had it `-v /var/lib/kubelet/:/var/lib/kubelet:rw,shared` in `min-turnup` originally. Which is correct, and why?

CC: @mikedanese @errordeveloper 
",closed,True,2016-06-28 04:10:21,2016-06-28 22:34:42
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/156,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/156,switch tls key algorithm from ECDSA to RSA,"To work around:
  kubernetes/kubernetes#28180

I'm currently testing this.

cc @colemickens 
",closed,True,2016-06-28 20:48:46,2016-06-29 03:44:29
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/157,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/157,WIP/Discussion: support multiple clusters based on cloud_provider/instance_prefix,"Something I tried out in the Azure phase1.

It treats `phase1/{{cloud_provider}}` as a template and copies it to `clusters/{{cloud_provider}}/{{instance_prefix}}`.

With this change I can change `.config.json` to modify just the `instance_prefix` and get multiple basically identical clusters. And I can go into one and modify `config.json` and re-run `./do deploy` and get my cluster updated (in particular scaled up).

Problems:
- [ ] not sure if this is what others were thinking
- [ ] makes a bit of a mess of the Makefile rule dependencies. I was struggling to get it to use the `.config.json` that was in place and not re-run the whole config process including re-running the Kconfig steps so I clobbered a bit to make it work
- [ ] It's not a fully isolated template, it still reaches up into `../../../phase1` for the `tf.jsonnet` file.

``` shell
$ pwd

/root/kubernetes-anywhere

$ ls -R clusters/

clusters:
azure

clusters/azure:
colemick-ex

clusters/azure/colemick-ex:
all.jsonnet
azure-colemick-ex.tf
azure.json
azure.jsonnet
config.json
configure-vm.sh
do
Kconfig
kubeconfig.json
terraform.tfstate
terraform.tfstate.backup
```
",closed,True,2016-06-29 08:03:03,2016-09-30 06:31:29
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/158,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/158,Few improvements to Kconfig bits,"For some reason current version has thrown this when I tried running
make in a container:

```
docker build .
...
> docker run 8d0b7ef3b34c
make config
make[1]: Entering directory '/root/kubernetes-anywhere'
mkdir -p .tmp; curl -sSL --fail -o "".tmp/conf"" \
        ""https://storage.googleapis.com/public-mikedanese-k8s/kconfig/4.6/linux/conf""; \
chmod +x "".tmp/conf""
curl: no URL specified!
curl: try 'curl --help' or 'curl --manual' for more information
/bin/sh: 2:     ""https://storage.googleapis.com/public-mikedanese-k8s/kconfig/4.6/linux/conf""; \: not found
/bin/sh: 3: chmod +x "".tmp/conf"": not found
Makefile:26: recipe for target '.tmp/conf' failed
make[1]: *** [.tmp/conf] Error 127
make[1]: Leaving directory '/root/kubernetes-anywhere'
make: *** [default] Error 2
Makefile:23: recipe for target 'default' failed
```

So I looked at it and realised the backslashes weren't needed, so I also ended-up reducing some duplication.

I also look at the JSON generator, and found it a bit hard to read, and while understanding what it does, I found how it can be improved.
",closed,True,2016-06-29 11:20:38,2016-06-29 15:32:52
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/159,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/159,Use `cluster_name` instead of `instance_prefix`,"The `instance_prefix` parameter is too implementation-specific, for example it has no meaning in EC2, where the correct name of what's effectively the same thing wold be `cluster_tag_suffix`, but that would be a mess. I think the `cluster_name` is an obvious generalisation.
",closed,False,2016-06-29 16:29:18,2016-07-05 14:59:31
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/160,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/160,reconsider phase2 ansible dependency,"Ansible has good features:
- It's well known, often used and has a ton of documentation
- It's simple (at least the subset we are using) and declarative

Ansible has bad features:
- Requires python dependency on both server and target hosts
- It's super massive and is making deployment slow

Are there alternatives that have the good features without the massive set of dependencies?
",closed,False,2016-06-29 18:15:21,2018-01-15 10:33:14
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/161,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/161,prototype: add ignition phase2 for comparison with ansible,"It's less than 30mb  and I think we could get it down to 20mb, much faster, and no python dependency on host or master. Requires a slightly modified igntion https://github.com/mikedanese/ignition/tree/vanilla

If I preinstall docker it takes 2.5 minutes to bring up a cluster.

ref #160
",closed,True,2016-06-29 22:43:20,2016-08-10 21:22:21
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/162,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/162,reconcile kubelet args with kube-deploy/docker-multinode,"https://github.com/kubernetes/kube-deploy/blob/master/docker-multinode/common.sh#L41

Looks like /var/log/containers is missing and multinode mounts /sys as `rw`.

I think /var/log/containers mount is needed for fluentd-elasticsearch to grab the logs. Not sure how `/sys` needs to be mounted.
",closed,False,2016-07-04 07:59:04,2018-01-15 10:36:45
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/163,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/163,Replace use of `instance_prefix` with `cluster_name`,,closed,True,2016-07-04 09:57:06,2016-07-05 14:59:30
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/164,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/164,fix kubelet mounts for logging,"Partially addresses #162.

This allows EFK to work. Without this no container logs are picked up.
",closed,True,2016-07-04 11:18:52,2016-07-05 14:58:55
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/165,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/165,azure: restarting kubelet for the third time still causes irreparable hangs,"I first encountered this in the initial effort in kube-deploy.

For some reason after `kubelet` is restarted for the third time, CPU spikes up and the VM becomes completely unusable, including being unable to reboot it from the Azure side (at least I waited 10-15 minutes). Current solution is to redeploy the cluster - it might be possible to redeploy just the host VM which causes Azure to reschedule the entire VM.

Either way, needs further investigation - for now I just avoid rebooting kubelet or reboot the VM instead of rebooting just the kubelet.service.
",closed,False,2016-07-04 12:18:15,2018-01-15 10:36:34
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/166,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/166,"Use of self-hosting in ""phase2/ansible"" vs descrete approach ""phase2/docker-images""","As #149 states, we should consolidate boostrap implementation.

What's in `phase2/docker-images` doesn't use self-hosting, while `phase2/ansible` relies on self-hosting.
",closed,False,2016-07-04 17:01:32,2018-01-15 10:33:51
kubernetes-anywhere,raeesbhatti,https://github.com/kubernetes/kubernetes-anywhere/pull/167,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/167,Add https to 'weave' binary link in DOCKER_FOR_MAC.md,,closed,True,2016-07-04 21:35:31,2016-07-11 09:37:25
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/168,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/168,Find best way of installing Docker,"There are too many ways Docker install can fail.

Today I noticed the apt repo was down (at least briefly), and `apt-get update` timed-out causing everything else to fail. Besides, we have seen https://github.com/docker/docker/issues/22599, which remains unresolved. In GCE we have `curl https://get.docker.com | sh` and in EC2 we install the official Debian package. I think that we should define some kind of standard way, and whatever we do probably should keep retrying until it succeeds.
",closed,False,2016-07-06 16:09:50,2018-01-15 10:21:53
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/pull/169,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/169,"Use the same node suffix everywhere, these are used by service controller.","cc @mikedanese @errordeveloper 
",closed,True,2016-07-06 22:04:39,2016-07-06 22:10:03
kubernetes-anywhere,hrzbrg,https://github.com/kubernetes/kubernetes-anywhere/issues/170,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/170,Being able to start in an existing AWS VPC,"Hi,

I would like to run the terraform scripts in an existing AWS VPC which already has an IGW, subnets and DHCP options. Would it be possible to just add these as variables to the templates?
What changes would be necessary here?

Greetings
hrzbrg
",closed,False,2016-07-11 08:56:41,2018-01-15 10:36:24
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/171,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/171,CI/CD for Kubernetes-Anywhere,"What's the current thinking for this? The stuff in `{kubernetes}/hack/jenkins` seemed like it was fairly tied to: 
1. A full release build of Kubernetes
2. The kube-up infrastructure, even taking dependencies on certain functions being implemented in the `$KUBERNETES_PROVIDER/util.sh`.

Given that `kubernetes-anywhere` only needs the `hyperkube` container (which can be built easily by itself with `hack/dev-push-hyperkube.sh`), and given that this is meant to replace `kube-up`, I was wondering if something else should be put together.

What were others thinking?
",closed,False,2016-07-12 04:53:58,2017-10-31 14:03:04
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/172,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/172,azure: phase1 add routeTableName to azure.json,"I changed a small something in the cloudprovider code I'm about to send a PR for; this is the corresponding change for bring-up.
",closed,True,2016-07-12 04:55:11,2016-07-12 17:27:41
kubernetes-anywhere,obi1kenobi,https://github.com/kubernetes/kubernetes-anywhere/issues/173,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/173,apt-get is configured to install unauthenticated packages,"Excerpt:

```
apt-get install -y --allow-unauthenticated --force-yes  docker-engine
```

Original location:
https://github.com/kubernetes/kubernetes-anywhere/blob/901671a6eea055ef524e50b80e1409cc424ee2c8/phase1/aws-ec2-terraform/secure-v1.2-user-data.yaml#L200

I'm not sure why `--allow-unauthenticated` was originally added, but it definitely shouldn't be there -- it's a shoot-yourself-in-the-foot option from a security standpoint.
",closed,False,2016-07-12 19:46:15,2016-09-19 13:12:49
kubernetes-anywhere,obi1kenobi,https://github.com/kubernetes/kubernetes-anywhere/pull/174,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/174,Remove --allow-unauthenticated from apt-get call.,"Adds the public key matching the docker-engine package, and authenticates the package during installation.

Addresses #173
",closed,True,2016-07-12 19:55:26,2016-09-19 13:12:01
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/175,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/175,azure: ad->aad rename,"Reacting to one more change from the cloudprovider PR.
",closed,True,2016-07-15 11:04:43,2016-07-15 16:29:28
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/pull/176,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/176,Store the kubeconfig on the master as well.,"Some kube components such as kube-proxy that run as daemon set
run on master as well and they need kubeconfig to contact the
API server, so we need to store the kubeconfig on the master
as well.
",closed,True,2016-07-19 17:06:52,2016-07-20 03:36:58
kubernetes-anywhere,theobolo,https://github.com/kubernetes/kubernetes-anywhere/issues/177,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/177,[Azure CP] Bad number of nodes / Missing Nodes,"Hello guys,

I'm testing hardly the azure implementation within kubernetes-anywhere and found something weird.

Everytime i made a cluster the number of the nodes that provide the Terraform script is correctly within the Azure ressource group, but not inside the cluster that seeing the kubernetes master.

For exemple if i decide to deploy a cluster with 3 nodes : Standard_DS3_v2 
Kubernetes is only able to see 2 nodes... There is always one missing 

I tried with a bigger cluster 10 nodes : Standard_DS3_v2
Kubernetes was only able to use 7 nodes : kubectl get nodes gives me 7 nodes and not 10.

This is the last cluster i deployed with 3 nodes in the ressource group : 0 / 1 / 2 

```
ghosty@workstation-coursierprive:~$ ./kubectl --kubeconfig=""kubernetes-anywhere/phase1/azure/.tmp/kubeconfig.json"" get nodes
NAME                     STATUS                     AGE
lolilol-cluster-master   Ready,SchedulingDisabled   18m
lolilol-cluster-node-1   Ready                      11m
lolilol-cluster-node-2   Ready                      12m
```

Screenshot of my deployment : 

![image](https://cloud.githubusercontent.com/assets/7281773/16985853/7a8116c0-4e83-11e6-8ed3-1ae0a29f74be.png)

Another thing is that when i deploy a Service with LoadBalancer type : it's create a Backend rule only for two nodes as Kubernetes is able to see.

Any ideas :) 
",closed,False,2016-07-20 12:02:58,2016-09-07 15:34:41
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/pull/178,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/178,Mount the /dev directory in the kubelet container so that it can detect persistent disks.,"This seems to work.
",closed,True,2016-07-21 00:09:23,2016-07-21 06:24:50
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/issues/179,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/179,Document kubelet flags,"We pass a number of arguments to kubelet and also mount a number of directories. It is difficult for someone to take a look and determine what each mount/flag value does. These should be documented.

Ref #178 #162

cc @colemickens 
",closed,False,2016-07-21 03:11:11,2018-01-15 10:36:14
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/pull/180,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/180,Use hyperkube for kube-proxy.,"It makes it much easier to use kubernetes-anywhere for deployments.
",closed,True,2016-07-21 03:16:27,2016-07-21 03:33:53
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/181,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/181,switch minion to node: minion is deprecated,,closed,True,2016-07-21 18:38:07,2016-07-21 22:56:47
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/182,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/182,azure: get away from password auth,"@errordeveloper / @mikedanese How is auth handled for GCE/AWS?

I don't like using password auth for the VMs. I'm guessing GCE just deploys without auth and expects you to use `gcloud` or Cloud Console to push short lived keys into the VM just-in-time?

I'm very surprised there isn't an SSH provisioner for Terraform...
",closed,False,2016-07-26 22:37:10,2018-01-15 10:33:42
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/183,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/183,terraform: tls_cert_request as a resource is deprecated,"I updated to terraform rc3 locally to test out some stuff.

`tls_cert_request.node: using tls_cert_request as a resource is deprecated; consider using the data source instead`

I don't know what this means yet, but it applies to all of the `tls_cert_requests` resources as well as the `template_file`s I use in the azure phase1.
",closed,False,2016-07-26 22:55:45,2018-01-15 10:35:25
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/184,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/184,azure: add helper to phase1 for AAD Service Principal creation,"Follow the same pattern that I use in [`azkube`](https://github.com/colemickens/azkube):
1. Create the resource group using `phase1.deployment_name`
2. Create a AD App, AD SP for the App. Add a `client_secret`/`password` to the App.
3. Grant `Contributor` RBAC role to the SP, scoped to the resource group.
4. Place these creds into `config.json`.

This will get kubernetes-anywhere usability closer to azkube's, while being more standard and benefitting from the cluster mutability that terraform provides.
",closed,False,2016-07-27 18:37:19,2018-01-15 10:38:00
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/185,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/185,[wip] Azure: Enable SSD and Docker storage on ephemeral drive,"This builds on top of https://github.com/kubernetes/kubernetes-anywhere/pull/157

The diff (the latest commit) enables Premium storage in Azure, and mounts the ephemeral drive to /var/lib/docker. It's wiped on every boot for a few reasons, but it's fine for Kubernetes purposes and speeds things up quite a bit.

CC @theobolo

Theo, this differs from your fork in one way: I do the ephemeral drive configuration in Ansible rather than in `configure-vm.sh`; although I realize that means the intial Ansible container is still a bit slow... I feel it's the more appropriate place to do that sort of change.

Obviously shouldn't be merged until we decide some other things; this will have to change if we move to Ignition, etc.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/kubernetes-anywhere/185)

<!-- Reviewable:end -->
",closed,True,2016-07-29 05:48:46,2016-09-18 00:05:52
kubernetes-anywhere,janetkuo,https://github.com/kubernetes/kubernetes-anywhere/pull/186,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/186,Fix typo in README,,closed,True,2016-07-31 00:28:55,2016-08-23 14:45:33
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/187,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/187,extract tls stuff in azure and gce to a cluster_tls function in tf.jsonnet,,closed,True,2016-08-03 07:25:27,2016-08-04 20:02:54
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/188,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/188,azure: public ip was incorrect in kubeconfig,"@brendandburns reported that the `kubeconfig.json` had the wrong public ip inside.

My suspicion is that this is due to Terraform creating the PIP and attaching it to a VM, the file provisioner kicked in and wrote the `kubeconfig.json`. Then the configuration was modified and `terraform apply` ran again, causing new VMs to be deployed. When the PIP was transferred, it got a new lease, but the kubeconfig wasn’t updatee. The same PKI credentials were used for the new machines, hence why only the IP was incorrect.

Just a theory so far.

I’ve probably avoided this because I use [this PR branch that creates deployment-directories-per-deployment-name allowing for side-by-side deployments of clusters with isolation for the kubeconfig.json/terraform state per-deployment-name](https://github.com/kubernetes/kubernetes-anywhere/pull/157) It also avoids storing files down inside the repo at phase1/azure/.tmp which is a bit awkward currently.

Actions:
1. Look at fixing Terraform dependencies so it will recreate the `kubeconfig.json` if this is truly what's going on.
2. Push the side-by-side cluster PR through.
",closed,False,2016-08-11 21:46:29,2018-01-15 10:38:12
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/189,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/189,don't manage the gce network with tf,"Let the user create and delete the network. Default network usually exists.

Temporarily address https://github.com/kubernetes/kubernetes-anywhere/issues/141

@madhusudancs WDYT?
",closed,True,2016-08-12 21:14:33,2016-09-01 22:47:24
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/190,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/190,bump ignite docker version,"This is a build of what is currently in phase2/ignite
",closed,True,2016-08-12 22:27:36,2016-08-12 22:31:58
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/191,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/191,run make fmt,,closed,True,2016-08-12 22:31:42,2016-08-12 22:44:51
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/192,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/192,implement AWS phase1 that supports ignition,"I'll try to do this next week. cc @zmerlynn 
",closed,False,2016-08-12 22:46:42,2018-01-15 10:38:30
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/issues/193,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/193,Investigate/fix conformance test failures on gce,"```
Summarizing 6 Failures:

[Fail] [k8s.io] Kubectl client [k8s.io] Kubectl run default [AfterEach] should create an rc or deployment from an image [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2023

[Fail] [k8s.io] DNS [It] should provide DNS for services [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/dns.go:204

[Fail] [k8s.io] ClusterDns [Feature:Example] [It] should create pod that uses dns [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/example_cluster_dns.go:130

[Fail] [k8s.io] Secrets [It] should be consumable in multiple volumes in a pod [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:2121

[Fail] [k8s.io] DNS [It] should provide DNS for the cluster [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/dns.go:204

[Fail] [k8s.io] Networking [It] should provide Internet connection for containers [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/networking.go:54

Ran 91 of 342 Specs in 2307.186 seconds
FAIL! -- 85 Passed | 6 Failed | 0 Pending | 251 Skipped --- FAIL: TestE2E (2307.20s)
FAIL
```

Full output:

https://gist.githubusercontent.com/mikedanese/3c7698ddf1a9966150514de23f6cbe76/raw/816e3a8a3f2d98591fc304170bce953bec996577/gistfile1.txt
",closed,False,2016-08-12 23:57:24,2016-08-15 23:35:51
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/194,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/194,fix failing dns e2e tests,"I believe this fixes the failing DNS e2e tests. Note that this value matches the --cluster-dns kubelet parameter https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase2/ignition/vanilla/node.jsonnet#L23

Testing now.
",closed,True,2016-08-13 00:29:27,2016-08-13 02:18:56
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/195,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/195,Jenkinsfile for Azure,"This adds a basic Jenkinsfile for testing kubernetes-anywhere (+kubernetes) on Azure.

Currently it builds HEAD of master of `kubernetes` and tests that along with the ""current"" branch of `kubernetes-anywhere` checked out by the Jenkins job. For now this will be master on a regular interval, with PRs being built on demand and tested as well.

Goal is to provide some basic testing of k-a and kubernetes, and allow me to iterate on some other PRs faster (adding back premium storage, ssd support, isolated deploy dirs).

This actually doesn't work on `master` of Kubernetes due to three issues:
1. (general issue) Problems handling paths with spaces. [PR for this issue.](https://github.com/kubernetes/kubernetes/pull/30583)
2. (recent master regression) [kubelet fails to launch static pods: ""inspected image [] does not match []"" ](https://github.com/kubernetes/kubernetes/issues/30580). [PR for this issue.](https://github.com/kubernetes/kubernetes/pull/30582)
3. (recent master regression) [apiserver doesn't come up anymore: ""can't setup API""]()

Any help on getting the first two merged and the third one investigated is appreciated.

In the future there will be another Jenkinsfile for building master/prs of kubernetes and then testing that against the last release and master of kubernetes-anywhere. This will also be a jenkins-dsl job used for reconstituting all of these jobs on a new Jenkins server, so that this work is easily reproducible and so that others can setup CD for Azure.

Future work:
- Another Jenkinsfile oriented around building `kubernetes` master/pull-requests using the latest stable of `kubernetes-anywhere`
- job-dsl definition for all of the Azure Jenkins job so it can easily be re-setup or so other users can run Azure Kubernetes CD.
- Run conformance tests (need help from @mikedanese on this)

Note, this also includes getting Azure over onto Ignition.

This is currently running at https://jenkins.polykube.io. The following GitHub users have been granted access: @brendandburns @rootfs @mikedanese

<img width=""859"" alt=""screen shot 2016-08-15 at 11 25 24 am"" src=""https://cloud.githubusercontent.com/assets/327028/17676907/5bd4a590-62e5-11e6-9e1a-b0f69e94609d.png"">
",closed,True,2016-08-15 19:40:09,2016-08-17 21:02:52
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/196,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/196,fix dns again,"too hasty with the last one.
",closed,True,2016-08-15 20:00:17,2016-08-15 20:21:43
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/197,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/197,delete non ignition phase1 compatible stuff and redo docs.,"Still quite a bit of work to do on this one but opening now for visibility
",closed,True,2016-08-15 20:28:22,2016-08-17 22:11:08
kubernetes-anywhere,philwinder,https://github.com/kubernetes/kubernetes-anywhere/issues/198,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/198,Slightly misleading modlue example,"This example:
https://github.com/kubernetes/kubernetes-anywhere#full-blown-cluster-example-using-terraform-in-ec2

Will only work if the module is created in the same directory as the source code. This is because the although the variables `${var.aws...}` are set in the module variables file, they are probably not defined in a user's directory. I.e. you will get the error:

```
Error configuring: 2 error(s) occurred:

* module.kubernetes-anywhere-aws-ec2: missing dependency: var.aws_access_key
* module.kubernetes-anywhere-aws-ec2: missing dependency: var.aws_secret_key
```

To workaround, recreate the variables in the module, or in another separate variables.tf file. E.g.

```
variable ""aws_access_key"" {}
variable ""aws_secret_key"" {}

module ""kubernetes-anywhere-aws-ec2"" {
    source         = ""github.com/weaveworks/weave-kubernetes-anywhere/phase1/aws-ec2-terraform""
    aws_access_key = ""${var.aws_access_key}""
    aws_secret_key = ""${var.aws_secret_key}""
    aws_region     = ""us-east-1""
    cluster        = ""weave1""

    # You can also set instance types with node_instance_type/master_instance_type/etcd_instance_type
    # For SSH access, you will need to create a key named kubernetes-anywhere or set ec2_key_name
}
```
",closed,False,2016-08-16 12:35:43,2018-01-15 10:38:40
kubernetes-anywhere,philwinder,https://github.com/kubernetes/kubernetes-anywhere/issues/199,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/199,Transient error: InvalidParameterValue: IAM instance profile,"When deploying with AWS terraform phase 1:

```
* aws_instance.kubernetes-etcd.0: Error launching source instance: InvalidParameterValue: IAM Instance Profile ""arn:aws:iam::477246929820:instance-profile/kubernetes-etcd-staging"" has no associated IAM Roles
        status code: 400, request id:
```

I ran `terraform apply` a second time, and everything worked.

Another time I also saw:

```
* aws_launch_configuration.kubernetes-node-group: Error creating launch configuration: ValidationError: Invalid IamInstanceProfile: kubernetes-node-staging
        status code: 400, request id: 9728fb98-63b3-11e6-b009-618b1b02e625
```
",closed,False,2016-08-16 13:00:04,2018-01-15 10:38:48
kubernetes-anywhere,philwinder,https://github.com/kubernetes/kubernetes-anywhere/issues/200,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/200,"kubernetes-anywhere-pki-images.env only available on master, so can't run toolbox from any machine","```
for m in $public_dnses ; do ssh -i $KEY ubuntu@$m 'ls /etc/kubernetes-anywhere-pki-images.env'; done
ls: cannot access '/etc/kubernetes-anywhere-pki-images.env': No such file or directory
ls: cannot access '/etc/kubernetes-anywhere-pki-images.env': No such file or directory
ls: cannot access '/etc/kubernetes-anywhere-pki-images.env': No such file or directory
/etc/kubernetes-anywhere-pki-images.env
```
",closed,False,2016-08-16 14:12:26,2018-01-15 10:38:56
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/201,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/201,Upgrade phase1 (azure) to terraform 0.7.0,"This shouldn't be merged unless there's a plan for fixing the other phase1 implementation, since this PR modifies the shared tf jsonnet lib.
",closed,True,2016-08-17 01:09:20,2016-08-23 14:44:41
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/202,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/202,Mount the /dev directory in the kubelet container so that it can detect persistent disks.,"Looks like this commit (4bb4510592a701c3319c2fb0b22a7ceeacb3ab5f) from @madhusudancs got dropped when we moved from ansible->ignition.

This adds it back.
",closed,True,2016-08-17 01:51:47,2016-08-17 02:12:53
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/203,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/203,[WIP/Discussion] kickstart.sh for easier user experience,"@mikedanese This is what I've got so far.

I need to write the `kickstart.sh` to align with these docs. Should give a similar experience as is offered in the current ""curl-pipe-into-bash"" strategy with `kube-up`.

Thoughts? I'm going to work on `kickstart.sh` now unless you were thinking something else.

`kickstart.sh` needs to boot the latest k8s-anywhere container and execute `make deploy-all` which will in turn call `make config && make deploy && make validate && make addons`. I still need to reason through how to handle the `.tmp` directories in various places between executions... I think to handle that well we need to revisit the idea I have in https://github.com/kubernetes/kubernetes-anywhere/pull/157 since it would just require mounting `$HOME/kubernetes-anywhere-clusters:/opt/kubernetes-anywhere/clusters` and `.config.json` into the container.

DONT_MERGE
",closed,True,2016-08-18 00:19:02,2016-08-24 05:13:54
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/204,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/204,Terraform update with gce fixup,"cc @colemickens 
",closed,True,2016-08-22 01:40:27,2016-08-23 14:44:40
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/205,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/205,azure: test: add timeout waiting for cluster,"Deployments keep getting stuck. Either something is up with Azure networking or the Google storage bucket where `kubectl` is pulled from. If ignition fails to download `kubectl` it seems to fail out, and `kubelet.service` never gets created.

The pipeline then gets stuck waiting. For now, I'm adding the timeout in the Jenkins pipeline.
",closed,True,2016-08-23 06:10:09,2016-08-23 14:44:21
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/206,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/206,getting started guide for gce,,closed,True,2016-08-24 03:51:00,2016-08-24 05:23:43
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/207,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/207,`make deploy` now waits for cluster and deploys addons,"Note:
- I updated the string that `phase1/<provider>/do` expects to look for `{deploy,destroy}-cluster` to align with the target in the top-level `Makefile`. The functions inside `./do` that they map to are still just `deploy`/`destroy`.
",closed,True,2016-08-24 04:51:43,2016-08-24 05:29:45
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/208,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/208,[WIP] add e2e runner,"not even close to ready. I'm using this to play with https://github.com/kubernetes/test-infra/pull/433/
",closed,True,2016-08-24 05:32:34,2016-11-10 06:08:04
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/209,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/209,Azure Getting Started Guide,"Rendered form is here: https://github.com/colemickens/kubernetes-anywhere/blob/pr-azure-docs/phase1/azure/README.md

I think this is good for now and I can finally deprecate [azure-kubernetes-demo](https://github.com/colemickens/azure-kubernetes-demo).
",closed,True,2016-08-24 05:38:05,2016-08-24 05:41:09
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/210,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/210,azure: test: fix teardown,"I accidentally broke teardown in my Jenkins jobs. This should fix it.
",closed,True,2016-08-24 07:24:43,2016-08-24 20:10:48
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/211,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/211,azure: docs: add AAD SP helper,"- Add a script to help the user create an Azure Active Directory Service Principal
- Add instructions to docs for using this script

CC: @brendandburns
",closed,True,2016-08-24 20:44:49,2016-08-25 05:06:02
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/212,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/212,Update addons,"Will do soon.

Most/all of the addons needs to be updated.
",closed,False,2016-08-25 08:09:55,2016-09-30 17:36:59
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/213,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/213,move kconfig-conf to Dockerfile,,closed,True,2016-08-25 10:44:52,2016-08-26 18:00:58
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/214,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/214,Switch base to Alpine,"This stacks on top of #213. Thoughts? Is it worth switching?
",closed,True,2016-08-25 10:48:51,2016-08-26 18:00:55
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/215,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/215,Upgrade addons,"Note, I picked the kubernetes-dashboard release from today (it's 1.4.0-beta1).
",closed,True,2016-08-26 22:09:40,2016-09-01 00:59:42
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/216,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/216,azure: only tf apply twice if needed,"(I cleaned up the wait message as well)
",closed,True,2016-08-26 22:11:05,2016-09-01 00:59:33
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/217,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/217,Azure: Automatically provision Service Principal for user,"The PR does a few things:
1. Fixes a small bug in `Makefile` introduced when I rearranged the `make deploy` command.
2. Upgrades `terraform`, adds `curl` to the docker environment, removes a defunct comment.
3. Rearranges the Azure phase1 `Kconfig` so the user can input `subscription_id` first off and then spam `<enter>` for all the other questions.
4. Changes the defaults in phase2 to point to an ignition container that I'm updating and to point to the latest `1.4.0-alpha.3` build of Kube 1.4.x.
5. **Automatic** creation of a service principal for Azure. This changes the `create-azure-service-principal.sh` script in a few ways:
   - (`do`)  Will notice that the `tenant_id` field is empty and will automatically infer it based on the `subscription_id`.
   - (`do`)  Will notice that the `client_id`/`client_secret` fields are empty and offer to create creds
   - (`create-azure-service-principal.sh`) Accepts command line arguments instead of being interactive
   - (`create-azure-service-principal.sh`) Will automatically retry if the role assignment fails. (It does not backoff yet)
   - (`do`)  Will backfill both `.config` and `.config.json`.

The SP created will have Subscription level access by default. The user is warned of this in `./do` so they can consent or can refuse and stop the deployment.

**The net result:** The user only needs to know their subscription ID in order to do this deployment. They don't need to pre-provision a ServicePrincipal (unless they don't have permission to create a new SP, in which case they need to get a more privileged user to make them one)
",closed,True,2016-08-29 04:53:30,2016-09-07 18:30:12
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/218,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/218,Use nsenter to launch kubelet,"RE: https://github.com/kubernetes/kube-deploy/pull/214

I don't know what that PR is about, but it seems like it would affect us too.

This is another case where `kube-deploy` and `kubernetes-anywhere` are drifting...
",closed,False,2016-08-29 06:45:09,2017-06-06 06:26:43
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/219,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/219,azure: readme: fix expose,,closed,True,2016-08-29 16:51:58,2016-08-30 05:29:24
kubernetes-anywhere,madhusudancs,https://github.com/kubernetes/kubernetes-anywhere/pull/220,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/220,Improve multi-cluster support by prefixing cluster name in TLS provider names and kubeconfigs.,,closed,True,2016-09-01 22:40:51,2016-09-07 19:37:22
kubernetes-anywhere,theobolo,https://github.com/kubernetes/kubernetes-anywhere/pull/221,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/221,yiugyh,,closed,True,2016-09-07 09:40:28,2016-09-07 09:40:44
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/222,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/222,remove travis,"We can readd later if we feel like it but it's currently very broken
",closed,True,2016-09-07 18:31:36,2016-09-07 18:32:07
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/pull/223,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/223,"Indicate that user is in a new shell, which is in a container",,closed,True,2016-09-08 13:40:09,2016-09-22 15:23:34
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/224,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/224,azure: fix tenant_id auto-detection and back-filling,"1. Had accidentally hard-coded my subscription ID, meaning all users would've gotten the MS tenant ID if they were using the automatic SP creation script. No information risk, but it wouldn't have worked for them.
2. The backfilling of the config files wasn't being done correctly, causing the user to need to run `make deploy` twice.

Both of these issues are resolved with this PR.
",closed,True,2016-09-08 21:52:43,2016-09-22 15:23:09
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/225,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/225,azure: fix another SP propagtion delay,"`./create-azure-service-principal.sh` is meant to fix propagation issues, but it only fixes it for the RBAC Role Assignment.

As it turns out, this same problem can occur if Terraform fires up too quickly after the role assignment. For now, the best I can do is introduce a sleep and hope that's sufficient.

If 10 seconds isn't enough, users can simply re-execute `make deploy` and it should eventually succeed.

This stacks on top of #224 .
",closed,True,2016-09-08 22:14:37,2016-09-22 15:23:09
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/226,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/226,azure: fix kubeconfig mkdir instructions,"The instructions were a bit off and had the user creating `~/.kube/config` as a directory, which resulted in the move placing the edit into a directory such that the file ended up at `~/.kube/config/kubeconfig.json` rather than at `~/.kube/confifg`.

CC: @anhowe This is the issue you mentioned this afternoon.
",closed,True,2016-09-09 04:40:05,2016-09-22 15:21:50
kubernetes-anywhere,theobolo,https://github.com/kubernetes/kubernetes-anywhere/pull/227,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/227,azure : Rebase ephemeral & premium Disks options ,"Hello guys,

@colemickens @mikedanese 

I rebased a little bit the Cole's PR about implementation of ephemeral and premium disks on Azure. I need to work a little bit on it so please DO NOT MERGE for the moment ;)

Some points that are not working for the moment : 
- Implementation of `cfg.phase1.azure.use_premium_storage` and `cfg.phase1.azure.use_ephemeral_drive` was made with a `String` type with choice : `yes` or `no`. Actually the condition :  

>  local storage_type = if cfg.azure.use_premium_storage == ""yes"" then ""Premium_LRS"" else ""Standard_LRS"";

into `azure.jsonnet` is not working with a `bool` type if is setted on ""n"". The line is commented into the `.config` ...
- The condition `if` into the `phase2/ignition/vanilla/node.jsonnet` is not working properly, i'm not an expert in `jsonnet` and actually if i hit the condition by putting `cfg.phase1.azure.use_ephemeral_drive == yes` the Ignition init crashed on the nodes and throw an error. My `if` condition is certainly badly placed ... 

@mikedanese  you did something similar in the phase3, with `bool` type (addons) but i wasn't able to make it works on the phase1 for Azure. Terraform is still dropping an error saying that `use_ephemeral_drive` is not defined or missing. Cole thinked about a specific problem into Kconfig-conf, but since i saw that you handled it into phase3 maybe that you have the solution.

By the way, use_premium_storage is working properly, the rest needs just some fixes.

Waiting for your review guys, 

Cheers, 
Théo
",closed,True,2016-09-09 14:56:29,2017-03-02 06:56:28
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/228,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/228,phase2: kubernetes version: 1.4.0-alpha.3 -> 1.4.0-beta.3,,closed,True,2016-09-12 20:27:09,2016-09-16 22:28:00
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/229,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/229,Choosing 'n' to a bool kconfig option results in it being omitted from the config,"Bools don't seem to work properly. If you choose 'n' to them, they don't wind up in `.config` properly.

This affected the PR that @theobolo hit and it also affects the addons (someone actually tried to turn one off and noticed it doesn't work).
",closed,False,2016-09-12 21:05:10,2018-01-15 10:39:12
kubernetes-anywhere,brunoriscado,https://github.com/kubernetes/kubernetes-anywhere/issues/230,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/230,"Hanging after provisioning cluster - ""Waiting for Cluster: Expected 2 healthy nodes; found 0.""","This seems like it's running the utils validate script and validating whether the kubernetes nodes are running and it's failing.

Any idea?
",closed,False,2016-09-20 13:22:45,2016-09-22 20:57:05
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/231,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/231,addons: dashboard: 1.4.0-beta.2 -> 1.4.0,"~~Do not merge. I haven't tested this yet but want to send the PR before I forget about it again.~~

It works fine.
",closed,True,2016-09-23 00:56:06,2016-09-23 05:00:53
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/232,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/232,kube-proxy trying to talk to 10.0.0.1,"User spun up cluster. Things weren't working because kube-proxy was in a bad state.

`kube-proxy` pods were up and running... but they were in a failure loop:

``` shell
kubectl logs --namespace kube-system kube-proxy-bodsy   
Flag --resource-container has been deprecated, This feature will be removed in a later release.
E0927 02:46:21.189588       1 server.go:369] Can't get Node ""ralphk8s-node-3"", assuming iptables proxy, err: Get https://10.0.0.1:443/api/v1/nodes/ralphk8s-node-3: dial tcp 10.0.0.1:443: i/o timeout
I0927 02:46:21.191739       1 server.go:203] Using iptables Proxier.
W0927 02:46:51.192428       1 server.go:417] Failed to retrieve node info: Get https://10.0.0.1:443/api/v1/nodes/ralphk8s-node-3: dial tcp 10.0.0.1:443: i/o timeout
W0927 02:46:51.192724       1 proxier.go:226] invalid nodeIP, initialize kube-proxy with 127.0.0.1 as nodeIP
I0927 02:46:51.192793       1 server.go:215] Tearing down userspace rules.
I0927 02:46:51.203663       1 conntrack.go:40] Setting nf_conntrack_max to 32768
I0927 02:46:51.204145       1 conntrack.go:57] Setting conntrack hashsize to 8192
I0927 02:46:51.212064       1 conntrack.go:62] Setting nf_conntrack_tcp_timeout_established to 86400
E0927 02:47:21.205309       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://10.0.0.1:443/api/v1/services?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0927 02:47:21.212587       1 event.go:208] Unable to write event: 'Post https://10.0.0.1:443/api/v1/namespaces/default/events: dial tcp 10.0.0.1:443: i/o timeout' (may retry after sleeping)
E0927 02:47:21.212806       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://10.0.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0927 02:47:52.206429       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://10.0.0.1:443/api/v1/services?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0927 02:47:52.215875       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://10.0.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0927 02:47:55.750822       1 event.go:208] Unable to write event: 'Post https://10.0.0.1:443/api/v1/namespaces/default/events: dial tcp 10.0.0.1:443: i/o timeout' (may retry after sleeping)
E0927 02:48:23.207747       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://10.0.0.1:443/api/v1/services?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0927 02:48:23.216786       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://10.0.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0927 02:48:35.751387       1 event.go:208] Unable to write event: 'Post https://10.0.0.1:443/api/v1/namespaces/default/events: dial tcp 10.0.0.1:443: i/o timeout' (may retry after sleeping)
E0927 02:48:54.208682       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://10.0.0.1:443/api/v1/services?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0927 02:48:54.217860       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://10.0.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
```

@mikedanese Have you ever seen this? I can't come up with a single good reason this would happen. I changed the the non-beta hyperkube image for kube-proxy and killed the `kube-proxy` pods and it started working. I then reverted back to the original image and checked and it was still working. I have no idea how it got into that bad state...

The `kubeconfig` had to have been there (Ignition finished before apiserver can possibly get up)... so why would `kube-proxy` be trying to use the service address... it could never work...

CC: @squillace
",closed,False,2016-09-27 04:10:43,2016-09-30 05:25:49
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/233,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/233,"Update kubernetes, heapster, kube-proxy, kube-dns","1. Phase2: Default to `1.4.0` final.
2. Addons: DNS: Mirror upstream changes
3. Addons: Kube-Proxy: Remove deprecated flag
4. Addons: Kube-Proxy: kubeconfig volume should be read-only
5. Addons: Heapster: switch to `1.2.0` final.
",closed,True,2016-09-27 05:00:27,2016-09-28 21:22:34
kubernetes-anywhere,dcieslak19973,https://github.com/kubernetes/kubernetes-anywhere/issues/234,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/234,Documentation change for Azure,"Where it says:

The name chosen for phase1.cluster_name needs to be globally unique.

It should also include the restriction that the name _may not_ include numbers. The name _must only_ contain characters.
",closed,False,2016-09-28 15:44:14,2016-10-04 04:03:09
kubernetes-anywhere,ahmetb,https://github.com/kubernetes/kubernetes-anywhere/issues/235,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/235,make docker-dev with --rm,"Since `make docker-dev` uses volumes already to store data outside, perhaps we can run it with `docker run --rm` so once we're done the container does not stick around?
",closed,False,2016-09-28 16:52:13,2016-09-28 21:23:05
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/236,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/236,TEST TRAVIS,,closed,True,2016-09-28 17:10:55,2016-09-28 21:35:38
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/237,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/237,azure: document naming restrictions,"Addresses #234. 

@dcieslak19973, @ahmetalpbalkan: PTAL

Rendered new page is here: https://github.com/colemickens/kubernetes-anywhere/blob/pr-docs-strg-acct-name/phase1/azure/README.md

New section is specifically here: https://github.com/colemickens/kubernetes-anywhere/blob/pr-docs-strg-acct-name/phase1/azure/README.md#cluster-naming-restrictions
",closed,True,2016-09-28 18:39:46,2016-09-29 22:33:24
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/238,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/238,use --rm when running the deploy environment container,"Addresses #235.
",closed,True,2016-09-28 18:51:43,2016-09-28 21:22:57
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/239,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/239,Discussion: Adopt kubeadm instead of ignition,"Does it make sense to completely remove ignition and just switch to `kubeadm`?

`phase2` could more or less go away and `phase1` implementations would be required to install the `kubeadm` package from their package manager (or otherwise) and then execute `kubeadm` with a pre-seeded token.

CC: @errordeveloper @mikedanese 
",closed,False,2016-09-28 19:03:37,2018-01-15 10:33:00
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/240,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/240,Test travis,,closed,True,2016-09-28 21:36:29,2016-09-28 21:37:37
kubernetes-anywhere,mikedanese,https://github.com/kubernetes/kubernetes-anywhere/pull/241,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/241,Test travis,,closed,True,2016-09-28 21:38:04,2016-09-29 22:42:09
kubernetes-anywhere,dcieslak19973,https://github.com/kubernetes/kubernetes-anywhere/issues/242,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/242,High Availability,"If its within the scope of this project, having an option to create a High Availability cluster (e.g. 3 master nodes) would be good
",closed,False,2016-09-29 19:00:08,2018-01-15 10:39:24
kubernetes-anywhere,ahmetb,https://github.com/kubernetes/kubernetes-anywhere/issues/243,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/243,systemd dropin file typo,"- phase1/azure/configure-vm.sh 
- phase1/gce/configure-vm.sh 

have this systemd unit dropin typo: `clear_mount_propagtion_flags.conf`. It's mostly harmless I suppose, but changing it shouldn't be causing any breaking changes either for existing clusters?

@colemickens 
",closed,False,2016-09-29 20:05:04,2016-09-30 01:50:46
kubernetes-anywhere,ggm-at-apnic,https://github.com/kubernetes/kubernetes-anywhere/issues/244,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/244,"The interactions with Client ID, Service Principal, Application permissions is not clear","I'm a clean skin, I just brought up Azure free edition from scratch Empty state. This script is unable to bootstrap kubernetes because of the interactions through the azure command tool to make the Service Principal phase work. 

I have tried about three different paths to do this, using other peoples documentation on how to give the right permissions, none of which make sense because they assume prior knowledge of AD and Microsoft behaviours: this script is trying to elide over this problem but I think misses a stage.

I think one of your core developers needs to swing up a new, clean Microsoft Azure account and try bootstrap from scratch because the current package can't do it.

A typical error state is:

```
Error refreshing state: 1 error(s) occurred:

* Credentials for acessing the Azure Resource Manager API are likely to be incorrect, or
  the service principal does not have permission to use the Azure Service Management
  API.
Error refreshing state: 1 error(s) occurred:

* Credentials for acessing the Azure Resource Manager API are likely to be incorrect, or
  the service principal does not have permission to use the Azure Service Management
  API.
```
",closed,False,2016-09-30 00:25:53,2016-09-30 01:13:26
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/245,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/245,Support deployment without volume mount,"Reported by: @squillace

If you try to run the deployment with docker client on one machine and daemon on the other, bad things happen because of the volume mount.

It should be possible to remove the volume mount, but then we have a problem of getting the `kubeconfig` back out. We could stash in Azure Storage and let the user retrieve it later...
",closed,False,2016-09-30 00:32:44,2018-01-15 10:39:36
kubernetes-anywhere,ahmetb,https://github.com/kubernetes/kubernetes-anywhere/pull/246,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/246,phase1/azure: typo fix for systemd dropin,"Fixes #243.

Signed-off-by: Ahmet Alp Balkan ahmetalpbalkan@gmail.com
",closed,True,2016-09-30 00:40:54,2016-09-30 01:50:51
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/247,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/247,azure: extend wait for AAD consistency,"1. Wait 30 seconds instead of 10 to workaround AAD propagation delays.
2. Document this failure case and workaround.
",closed,True,2016-09-30 00:49:32,2016-09-30 01:53:01
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/248,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/248,"azure: add storage_name param, autogen if blank","1. Add a `storage_name` parameter to `phase1/azure/Kconfig`.
2. Use this field in the Azure Terraform file.
3. In `phase1/azure/do` check, and auto-fill-if-missing the `storage_account_name` similarly to how the TenantID and SP are auto-generated and filled in.

Not terribly pretty, but functional for now and would enable us to remove the ""global uniqueness"" requirement as it's been a source of confusion.

Please take a look @ahmetalpbalkan, @ggm-at-apnic and let me know what your feedback is.
",closed,True,2016-09-30 02:59:08,2016-09-30 06:18:59
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/249,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/249,azure: always infer the tenant_id,,closed,False,2016-09-30 03:17:52,2016-09-30 05:55:02
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/250,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/250,azure: always infer tenant_id,"Fixes #249.

(This stacks on top of PR #248 and should be rebased and merged only after #248 is merged).

cc: @ahmetalpbalkan
",closed,True,2016-09-30 04:05:58,2016-09-30 05:55:02
kubernetes-anywhere,ggm-at-apnic,https://github.com/kubernetes/kubernetes-anywhere/issues/251,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/251,Service Principal doesn't reference a valid application object,"inside  the docker, after `make docker-dev` a run of `make deploy` dies shortly after the creation of the client ID instance.

The error state ls:

```
2016-09-30T03:58:24.611Z:
{ Error: {""odata.error"":{""code"":""Request_BadRequest"",""message"":{""lang"":""en"",""value"":""The appId of the service principal does not reference a valid application object.""},""values"":[{""item"":""PropertyName"",""value"":""appId""},{""item"":""PropertyErrorCode"",""value"":""NoBackingApplicationObject""}]}}
  <<< async stack >>>
  at __1 (/usr/lib/node_modules/azure-cli/lib/commands/arm/ad/ad.servicePrincipals.js:174:41)
  at __3 (/usr/lib/node_modules/azure-cli/lib/commands/arm/ad/ad.servicePrincipals.js:172:30)
  <<< raw stack >>>
    at /usr/lib/node_modules/azure-cli/node_modules/azure-graph/lib/operations/servicePrincipals.js:145:19
    at retryCallback (/usr/lib/node_modules/azure-cli/node_modules/ms-rest/lib/filters/exponentialRetryPolicyFilter.js:140:9)
    at handleRedirect (/usr/lib/node_modules/azure-cli/node_modules/ms-rest/lib/filters/redirectFilter.js:36:9)
    at /usr/lib/node_modules/azure-cli/lib/util/utils.js:587:7
    at handleRedirect (/usr/lib/node_modules/azure-cli/lib/util/utils.js:564:9)
    at /usr/lib/node_modules/azure-cli/lib/util/logging.js:339:7
    at Request._callback (/usr/lib/node_modules/azure-cli/node_modules/ms-rest/lib/requestPipeline.js:125:16)
    at Request.self.callback (/usr/lib/node_modules/azure-cli/node_modules/request/request.js:187:22)
    at emitTwo (events.js:106:13)
    at Request.emit (events.js:191:7)
  stack: [Getter/Setter],
  statusCode: 400,
  request: 
   { rawResponse: false,
     queryString: {},
     method: 'POST',
     headers: 
      { 'x-ms-client-request-id': '50ab48e0-85d7-4000-83cb-aa9094b67cfe',
        'accept-language': 'en-US',
        'Content-Type': 'application/json; charset=utf-8',
        'user-agent': 'WindowsAzureXplatCLI/0.10.5' },
     url: 'https://graph.windows.net/8134123d-2209-41c1-87f1-c410786b408f/servicePrincipals?api-version=1.6',
     body: '{""appId"":""36c3f8b4-f370-4782-a6ac-a459833329b3"",""accountEnabled"":true}' },
  response: 
   { body: '{""odata.error"":{""code"":""Request_BadRequest"",""message"":{""lang"":""en"",""value"":""The appId of the service principal does not reference a valid application object.""},""values"":[{""item"":""PropertyName"",""value"":""appId""},{""item"":""PropertyErrorCode"",""value"":""NoBackingApplicationObject""}]}}',
     headers: 
      { 'cache-control': 'private',
        'content-type': 'application/json;odata=minimalmetadata;charset=utf-8',
        server: 'Microsoft-IIS/8.5',
        'ocp-aad-diagnostics-server-name': 'HgGHeNSQrM9XpX3oc/01cDMvT65hkzBBO7e+bQY/4x4=',
        'request-id': '2afba286-0eab-453c-9214-48ff0c73ecf4',
        'client-request-id': '83d133c1-61d6-4bfc-9133-b98df79922c5',
        'x-ms-dirapi-data-contract-version': '1.6',
        'ocp-aad-session-key': '<removed>',
        'strict-transport-security': 'max-age=31536000; includeSubDomains',
        'access-control-allow-origin': '*',
        'x-aspnet-version': '4.0.30319',
        'x-powered-by': 'ASP.NET, ASP.NET',
        duration: '2357676',
        date: 'Fri, 30 Sep 2016 03:58:25 GMT',
        connection: 'close',
        'content-length': '278' },
     statusCode: 400 },
  body: 
   { code: 'Request_BadRequest',
     message: 'The appId of the service principal does not reference a valid application object.' },
  __frame: 
   { name: '__1',
     line: 173,
     file: '/usr/lib/node_modules/azure-cli/lib/commands/arm/ad/ad.servicePrincipals.js',
     prev: 
      { name: '__3',
        line: 103,
        file: '/usr/lib/node_modules/azure-cli/lib/commands/arm/ad/ad.servicePrincipals.js',
        prev: undefined,
        calls: 1,
        active: false,
        offset: 69,
        col: 29 },
     calls: 0,
     active: false,
     offset: 1,
     col: 40 },
  rawStack: [Getter] }
Error: {""odata.error"":{""code"":""Request_BadRequest"",""message"":{""lang"":""en"",""value"":""The appId of the service principal does not reference a valid application object.""},""values"":[{""item"":""PropertyName"",""value"":""appId""},{""item"":""PropertyErrorCode"",""value"":""NoBackingApplicationObject""}]}}
  <<< async stack >>>
  at __1 (/usr/lib/node_modules/azure-cli/lib/commands/arm/ad/ad.servicePrincipals.js:174:41)
  at __3 (/usr/lib/node_modules/azure-cli/lib/commands/arm/ad/ad.servicePrincipals.js:172:30)
  <<< raw stack >>>
    at /usr/lib/node_modules/azure-cli/node_modules/azure-graph/lib/operations/servicePrincipals.js:145:19
    at retryCallback (/usr/lib/node_modules/azure-cli/node_modules/ms-rest/lib/filters/exponentialRetryPolicyFilter.js:140:9)
    at handleRedirect (/usr/lib/node_modules/azure-cli/node_modules/ms-rest/lib/filters/redirectFilter.js:36:9)
    at /usr/lib/node_modules/azure-cli/lib/util/utils.js:587:7
    at handleRedirect (/usr/lib/node_modules/azure-cli/lib/util/utils.js:564:9)
    at /usr/lib/node_modules/azure-cli/lib/util/logging.js:339:7
    at Request._callback (/usr/lib/node_modules/azure-cli/node_modules/ms-rest/lib/requestPipeline.js:125:16)
    at Request.self.callback (/usr/lib/node_modules/azure-cli/node_modules/request/request.js:187:22)
    at emitTwo (events.js:106:13)
    at Request.emit (events.js:191:7)
```
",closed,False,2016-09-30 04:12:09,2018-01-15 10:39:59
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/252,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/252,validate: clarify when success is achieved,"CC: @squillace who reported this as being somewhat confusing.
",closed,True,2016-09-30 06:13:06,2016-09-30 06:24:57
kubernetes-anywhere,brunoriscado,https://github.com/kubernetes/kubernetes-anywhere/issues/253,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/253,Kubernetes-anywhere on azure (networking),"I'm facing an issue right now with an azure/kubernetes migration, which is related with the following:

We have a VNET1 (ex: 10.66.0.0/16) and a VNET2 (ex:10.55.0.0/16), within each of the VNETs we have a gateway and we connect both VNETs using these gateways and VPN connections.

VNET1 has an elasticsearch cluster, hosted on subnet 10.66.1.0/24 and VNET2 has a kubernetes cluster hosted on 10.55.1.0/24 (which is provisioned by kubernetes-anywhere).

VNET1 has 3 client boxes for elastic (ex: 10.66.1.4, 10.66.1.5 and 10.66.1.6), and VNET2 has 2 kuberenetes nodes (ex: 10.55.1.4 and 10.55.1.5) as well as a master.

Inside the VNET2 kuberenetes nodes run kubernetes pods (which have a containerized api, that needs a connection to the elasticsearch cluster).

Our kubenetes nodes (ex: VNET2 - 10.55.1.4 and 10.55.1.5), can connect to the elasticsearch nodes (ex: VNET1 - 10.66.1.4, 10.66.1.5 and 10.66.1.6).

However the pods/containers which run on these nodes, and are assigned to their own subnets (ex: 10.244.0.0/24, 10.244.1.0/24), cannot connect to the elasticsearch nodes/VMs or to VM outside of the nodes subnet for that matter. Independent of which VNET this VM might sit on.

Is there something misconfigured in the networking model? Should I be using an overlay network? If I deploy my pods with shared host network, I get connection to elastic, but I can only share one pod per host, otherwise they conflict.

Any help is greatly appreciated.
",closed,False,2016-09-30 17:11:11,2016-10-17 12:51:14
kubernetes-anywhere,ahmetb,https://github.com/kubernetes/kubernetes-anywhere/issues/254,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/254,Azure ssh keys vs passwords,"@colemickens is there a reason why k8s-anywhere uses admin password instead of ssh keys? it appears like terraform supports it.

I also see some openssh public keys in `phase1/azure/.tmp/terraform.tfstate.backup`. It appears like the VMs get assigned some form of ssh key?

```
""os_profile_linux_config.2972667452.ssh_keys.#"": ""0"",
```

(not sure what this refers to) but there are fields like `""public_key_openssh""` under keys like `""tls_locally_signed_cert.ahmetb-k8s-node""` too.

It would be good to either:
- accept a public key string (to be used on all nodes) from the user
- or drop the genereted ssh private keys to some output directory
",closed,False,2016-09-30 17:26:10,2018-01-15 10:40:08
kubernetes-anywhere,ahmetb,https://github.com/kubernetes/kubernetes-anywhere/issues/255,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/255,Generated state is very hard to keep track of,"When I first cloned this repo and created a cluster, it took me quite a while to realize where the script(s) actually store the generated state. It was scattered all over the place. I finally realized there's some stuff in:
- `./.config`
- `./.config.json`
- `./phase1/azure/.tmp/**`
- `./phase3/.tmp/**`

What if there was something like an `./output` directory that stores all the state and anytime we re-clone this repository, we just drop the `./output` there and everything works as expected.

This way, users would only need to backup the `./output` directory as opposed to keeping a copy of the entire repo.

Thoughts? @colemickens 
",closed,False,2016-09-30 17:29:40,2018-01-15 10:40:17
kubernetes-anywhere,edevil,https://github.com/kubernetes/kubernetes-anywhere/issues/256,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/256,Hairpin mode differs between master and nodes,"Azure clusters setup with kubernetes-anywhere use different hairpin modes between the master and the nodes, is this intended? Is there a reason for this?

The kubelet flags on the master are:

```
kubelet --address=0.0.0.0 --allow-privileged=true --cloud-provider=azure --enable-server --enable-debugging-handlers --kubeconfig=/srv/kubernetes/kubeconfig.json --config=/etc/kubernetes/manifests --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --v=2 --api-servers=http://localhost:8080 --register-schedulable=false --cloud-config=/etc/kubernetes/azure.json
```

We can see that neither --configure-cbr0 nor the kubenet plugin are specified. Accordingly, in the log we see that the default hairpin mode (promiscuous-bridge) cannot be used:

```
Sep 28 10:45:06 ubuntu docker[4669]: W0928 10:45:06.996630    4760 kubelet_network.go:71] Hairpin mode set to ""promiscuous-bridge"" but configureCBR0 is false, falling back to ""hairpin-veth""
Sep 28 10:45:06 ubuntu docker[4669]: I0928 10:45:06.996699    4760 kubelet.go:516] Hairpin mode set to ""hairpin-veth""
```

The kubelet flags on the nodes are:

```
kubelet --address=0.0.0.0 --allow-privileged=true --cloud-provider=azure --enable-server --enable-debugging-handlers --kubeconfig=/srv/kubernetes/kubeconfig.json --config=/etc/kubernetes/manifests --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --v=2 --api-servers=https://10.240.1.4 --hairpin-mode=promiscuous-bridge --network-plugin=kubenet --reconcile-cidr --cloud-config=/etc/kubernetes/azure.json
```

So we can see that the promiscuous-bridge mode is specified and the kubenet network plugin is used. Another ""strange"" thing is that we end up with two bridges, the cbr0 setup by kubelet and docker0 setup by the docker daemon:

```
cbr0      Link encap:Ethernet  HWaddr 0a:58:0a:f4:00:01
          inet addr:10.244.0.1  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::d025:a6ff:fe17:ec27/64 Scope:Link
          UP BROADCAST PROMISC MULTICAST  MTU:1500  Metric:1
          RX packets:214723 errors:0 dropped:0 overruns:0 frame:0
          TX packets:252988 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:15618583 (15.6 MB)  TX bytes:75332322 (75.3 MB)

docker0   Link encap:Ethernet  HWaddr 02:42:95:1a:90:b4
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
```

Shouldn't the docker daemon be configured to use cbr0?

cc @colemickens 
",closed,False,2016-10-01 14:07:41,2018-01-15 10:40:27
kubernetes-anywhere,Rushit,https://github.com/kubernetes/kubernetes-anywhere/issues/257,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/257,"Error while running ""make deploy"" on OSX - El Capitan","`kubernetes-anywhere (master) $ make deploy`
`/Applications/Xcode.app/Contents/Developer/usr/bin/make config`
`CONFIG_=""."" kconfig-conf Kconfig`
`/bin/bash: kconfig-conf: command not found`
`make[1]: *** [config] Error 127`
`make: *** [.config] Error 2`

I am doing this very first time. Am I missing Anything? I am following these instructions - https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase1/azure/README.md
",closed,False,2016-10-01 19:55:59,2016-10-03 04:37:54
kubernetes-anywhere,jordanyaker,https://github.com/kubernetes/kubernetes-anywhere/pull/258,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/258,[kubelet.service] Add the mounting of /var/log.,"The mounting of `/var/log` enables necessary log sharing when Fluentd is in play.
",closed,True,2016-10-03 12:46:59,2016-10-05 00:55:54
kubernetes-anywhere,uthark,https://github.com/kubernetes/kubernetes-anywhere/pull/259,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/259,Fixed couple of typos in  README.md,"Fixed couple of typos in  README.md
",closed,True,2016-10-03 19:45:17,2016-11-10 06:07:44
kubernetes-anywhere,Rushit,https://github.com/kubernetes/kubernetes-anywhere/issues/260,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/260,"Azure, `make validate` does not work from localbox"," kubernetes-anywhere (master) $ `make validate` 
KUBECONFIG=""$(pwd)/phase1/$(jq -r '.phase1.cloud_provider' .config.json)/.tmp/kubeconfig.json"" ./util/validate
Validation: Expected 5 healthy nodes; found        0. (10s elapsed)
Validation: Expected 5 healthy nodes; found        0. (20s elapsed)
Validation: Expected 5 healthy nodes; found        0. (30s elapsed)
Validation: Expected 5 healthy nodes; found        0. (40s elapsed)
Validation: Expected 5 healthy nodes; found        0. (50s elapsed)
Validation: Expected 5 healthy nodes; found        0. (60s elapsed)
Validation: Expected 5 healthy nodes; found        0. (70s elapsed)

`make deploy` creates the cluster. I can see into azure console, but I cannot run kubectl from local too. Is there any step I missed? or anything related to opening up port or something on azure side?
",closed,False,2016-10-04 03:35:50,2016-11-02 18:44:36
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/261,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/261,azure: add troubleshooting directions,"Add some basic troubleshooting directions for Azure.
",closed,True,2016-10-04 04:00:11,2016-10-04 04:04:50
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/262,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/262,Update admission-controllers for 1.4,,closed,False,2016-10-06 04:27:27,2018-01-15 10:40:36
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/263,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/263,default to kubernetes 1.4.1,,closed,True,2016-10-11 06:46:40,2016-10-11 06:54:04
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/issues/264,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/264,API server build parameters in phase2,"Shouldn't [api-server](https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase2/ignition/vanilla/manifest/kube-apiserver.jsonnet#L29) bind to ""0.0.0.0"" instead of ""127.0.0.1""?
",closed,False,2016-10-18 20:08:07,2018-01-15 10:40:42
kubernetes-anywhere,brunoriscado,https://github.com/kubernetes/kubernetes-anywhere/issues/265,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/265,Scale sets on azure,"the old kubernetes binaries, used to provision azure with kube.sh created a cluster based on scale sets. Is this going to be a feature with kubernetes-anywhere?

Right now availability sets are provisioned, I reckon this is a bit of work on terraform's side of things. So I was wondering whether this is going to be a feature.
",closed,False,2016-10-27 18:26:11,2016-10-27 18:45:39
kubernetes-anywhere,fw0037,https://github.com/kubernetes/kubernetes-anywhere/issues/266,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/266,unable to execute kubectl exec on pod with nohup command ,"my command is below:
kubectl exec -it c-engine-1613829755-v2bov -- nohup sh /home/webadmin/shell/deploy.sh 01 "" COMMON_REST COMMON_DUBBO COMMERCE_REST COMMERCE_DUBBO"" >/dev/null 2>&1 &

When I press the Enter key, & disappeared, the nohup command stopped and display error, how should I write to make it successful?

> > > [root@szxts10004464 logs]# kubectl exec -it c-service--4-2005015123-c8vsz -- nohup sh /home/webadmin/shell/deploy.sh 01 "" COMMON_REST COMMON_DUBBO COMMERCE_REST COMMERCE_DUBBO""  >/dev/null 2>&1 &
> > > [1] 63616
> > > [root@szxts10004464 logs]# 
> > > 
> > > [1]+  Stopped                 kubectl exec -it c-service--4-2005015123-c8vsz -- nohup sh /home/webadmin/shell/deploy.sh 01 "" COMMON_REST COMMON_DUBBO COMMERCE_REST COMMERCE_DUBBO"" > /dev/null 2>&1
> > > [root@szxts10004464 logs]# 
> > > [root@szxts10004464 logs]# 
> > > [root@szxts10004464 logs]# docker --version
> > > Docker version 1.12.1, build 23cf638
> > > [root@szxts10004464 logs]# 
",closed,False,2016-10-29 15:11:10,2016-10-30 02:10:37
kubernetes-anywhere,brunoriscado,https://github.com/kubernetes/kubernetes-anywhere/issues/267,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/267,"Azure, issue with mounting vhd","Hi,

I tried having a look at the multiple tickets regardind using mounting vhd volumes, but was unable to understand if the issue is fixed.

I can't see to replicate this demo:

I'm using the following yml replication controller:


```
apiVersion: v1
kind: Service
metadata:
  name: sayt
  labels:
    run: sayt
spec:
  type: NodePort
  ports:
  - port: 8081
    targetPort: 8081
    protocol: TCP
    name: http
  selector:
    run: sayt

---
apiVersion: v1
kind: ReplicationController
metadata:
  name: sayt
spec:
  replicas: 3
  template:
    metadata:
      labels:
        run: sayt
        uses: elas
    spec:
      containers:
      - name: sayt
        imagePullPolicy: Always
        image: essearch/sayt:dev1
        env:
        - name: SPIFFY_ENV
          value: ""azuredev1""
        - name: ES_CLIENT
          value: ""10.0.57.114:9200""
        ports:
        - containerPort: 8081
        volumeMounts:
          - name: disk0
            mountPath: ""/mnt/disk0""
          - name: disk1
            mountPath: ""/mnt/disk1""
          - name: disk2
            mountPath: ""/mnt/disk2""
      volumes:
        - name: disk0
          azureDisk:
            diskName: sayt-dev1-node0-osdisk.vhd
            diskURI: https://saytdev1.blob.core.windows.net/strgsayt-dev1/sayt-dev1-node0-osdisk.vhd
        - name: disk1
          azureDisk:
            diskName: sayt-dev1-node1-osdisk.vhd
            diskURI: https://saytdev1.blob.core.windows.net/strgsayt-dev1/sayt-dev1-node1-osdisk.vhd
        - name: disk2
          azureDisk:
            diskName: sayt-dev1-node2-osdisk.vhd
            diskURI: https://saytdev1.blob.core.windows.net/strgsayt-dev1/sayt-dev1-node2-osdisk.vhd
      imagePullSecrets:
      - name: a_key
```


My pods stop in ContainerCreating status, with the following message:

FailedSync	Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod ""sayt-j8uwl""/""default"". list of unattached/unmounted volumes=[disk0 disk1 disk2]



Here are the logs for kubernetes-controller pod:

```
E1102 00:48:45.810937       1 nestedpendingoperations.go:253] Operation for ""\""kubernetes.io/azure-disk/sayt-dev1-node2-osdisk.vhd\"""" failed. No retries permitted until 2016-11-02 00:49:01.810922747 +0000 UTC (durationBeforeRetry 16s). Error: AttachVolume.Attach failed for volume ""kubernetes.io/azure-disk/sayt-dev1-node2-osdisk.vhd"" (spec.Name: ""disk2"") from node ""sayt-dev1-node-2"" with: Attach volume ""sayt-dev1-node2-osdisk.vhd"" to instance ""sayt-dev1-node-2"" failed with compute.VirtualMachinesClient#CreateOrUpdate: Failure sending request: StatusCode=200 -- Original Error: Long running operation terminated with status 'Failed': Code=""AcquireDiskLeaseFailed"" Message=""Failed to acquire lease while creating disk 'sayt-dev1-node2-osdisk.vhd' using blob with URI https://saytdev1.blob.core.windows.net/strgsayt-dev1/sayt-dev1-node2-osdisk.vhd. Blob is already in use.""
I1102 00:49:01.847372       1 reconciler.go:170] Started AttachVolume for volume ""kubernetes.io/azure-disk/sayt-dev1-node2-osdisk.vhd"" to node ""sayt-dev1-node-1""
```

I'm not sure I understand the error as I'm not attaching the volume anywhere else.

By the way I'm using a recent version of kubernetes-anywhere, and kubectl version shows:

```
Client Version: version.Info{Major:""1"", Minor:""3"", GitVersion:""v1.3.4"", GitCommit:""dd6b458ef8dbf24aff55795baa68f83383c9b3a9"", GitTreeState:""clean"", BuildDate:""2016-08-01T16:45:16Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4+"", GitVersion:""v1.4.0-alpha.3"", GitCommit:""b44b716965db2d54c8c7dfcdbcb1d54792ab8559"", GitTreeState:""clean"", BuildDate:""2016-08-25T18:31:09Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```",closed,False,2016-11-02 09:03:08,2016-11-09 12:34:05
kubernetes-anywhere,CSdread,https://github.com/kubernetes/kubernetes-anywhere/issues/268,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/268,Logs,http://termbin.com/t2q6,closed,False,2016-11-04 19:05:16,2016-11-05 02:59:46
kubernetes-anywhere,barrymac,https://github.com/kubernetes/kubernetes-anywhere/issues/269,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/269,does not deploy containers on hosts,"Phase 1 appears to have completed ok and then the script hangs on the validation step without making any further progress. Kubelets are not installed on the hosts. I used it with default target vm image, ubuntu 16.04 and also with CoreOs. When I ssh into the master host on Ubuntu there is no docker installed and in CoreOs there is no containers running. It appears to have an issue communicating with the hosts to complete the automation on the host side. I chose k8s version 1.4.5.  I also noticed that cloud init had failed to start on CoreOs with a command not found error. 


`k8soncore-master kube # systemctl status coreos-cloudinit-691521544
● coreos-cloudinit-691521544.service - Unit generated and executed by coreos-cloudinit on behalf of user
   Loaded: loaded (/run/systemd/transient/coreos-cloudinit-691521544.service; transient; vendor preset: disabled)
Transient: yes
   Active: failed (Result: exit-code) since Tue 2016-11-08 00:18:18 UTC; 10ms ago
  Process: 1805 ExecStart=/bin/bash /var/lib/coreos-cloudinit/scripts/691521544 (code=exited, status=127)
 Main PID: 1805 (code=exited, status=127)

Nov 08 00:18:18 k8soncore-master bash[1805]:   installation instructions:
Nov 08 00:18:18 k8soncore-master bash[1805]:     https://docs.docker.com/engine/installation/
Nov 08 00:18:18 k8soncore-master bash[1805]: + true
Nov 08 00:18:18 k8soncore-master bash[1805]: + systemctl start docker
Nov 08 00:18:18 k8soncore-master bash[1805]: Warning: docker.service changed on disk. Run 'systemctl daemon-reload' to reload units.
Nov 08 00:18:18 k8soncore-master bash[1805]: + apt-get install -y jq
Nov 08 00:18:18 k8soncore-master bash[1805]: /var/lib/coreos-cloudinit/scripts/691521544: line 26: apt-get: command not found
Nov 08 00:18:18 k8soncore-master systemd[1]: coreos-cloudinit-691521544.service: Main process exited, code=exited, status=127/n/a
Nov 08 00:18:18 k8soncore-master systemd[1]: coreos-cloudinit-691521544.service: Unit entered failed state.
Nov 08 00:18:18 k8soncore-master systemd[1]: coreos-cloudinit-691521544.service: Failed with result 'exit-code'.
`

",closed,False,2016-11-08 00:20:34,2018-01-15 10:40:54
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/270,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/270,Add kubeadm support for phase2.,"Currently, this only supports using the most recent .deb packages.

- Exposes port 9898 internally for discovery service
- Nodes use master's internal IP
- Move a lot of configure-vm.sh logic into phase2 provider dirs (which get concatenated by jsonnet)

CC @mikedanese ",closed,True,2016-11-08 23:57:51,2017-03-09 19:36:08
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/issues/271,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/271,GCE validation erroneously fails,"It looks like the GCE terraform configuration for phase1 outputs `kubeconfig.json` in an unexpected place on the local host. The azure implementation (and the references in Makefile) assume this file will be placed into

    phase1/$IMPL/.tmp

but `gce.jsonnet` currently outputs this file to:

    phase1/gce

The end result is that if you do `make deploy` with the GCE provider, the `terraform apply` step succeeds, but `util/validate` sets the `KUBECONFIG` to a non-existent file, so every check to see how many nodes are in the cluster fails. While the validation script is looping, copying the `kubeconfig.json` file into the `.tmp` directory fixes things.",closed,False,2016-11-09 05:03:27,2016-11-10 06:07:15
kubernetes-anywhere,brunoriscado,https://github.com/kubernetes/kubernetes-anywhere/issues/272,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/272,kube-proxy talking to apiserver through public ip,"After my cluster gets created on azure, I usually close the network security rules a bit, as the public ip is wide open for any incoming requests.

However if a node goes down, the proxy pod tries to reach the api server through that public ip address, even though they're on the same vnet/subnet.

Is there a way to change this?

````
2016-11-09T12:28:37.715258087Z E1109 12:28:37.715120       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:28:37.715300488Z E1109 12:28:37.715243       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:28:58.394171741Z E1109 12:28:58.394033       1 event.go:208] Unable to write event: 'Post https://13.81.127.36/api/v1/namespaces/default/events: dial tcp 13.81.127.36:443: i/o timeout' (may retry after sleeping)
2016-11-09T12:29:08.716061345Z E1109 12:29:08.715929       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:29:08.716108347Z E1109 12:29:08.716064       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:29:38.394760254Z E1109 12:29:38.394618       1 event.go:208] Unable to write event: 'Post https://13.81.127.36/api/v1/namespaces/default/events: dial tcp 13.81.127.36:443: i/o timeout' (may retry after sleeping)
2016-11-09T12:29:39.716705150Z E1109 12:29:39.716542       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:29:39.716738450Z E1109 12:29:39.716641       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:30:10.717208482Z E1109 12:30:10.717085       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:30:10.717244983Z E1109 12:30:10.717167       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:30:18.395303328Z E1109 12:30:18.395163       1 event.go:208] Unable to write event: 'Post https://13.81.127.36/api/v1/namespaces/default/events: dial tcp 13.81.127.36:443: i/o timeout' (may retry after sleeping)
2016-11-09T12:30:41.717872642Z E1109 12:30:41.717710       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:30:41.717906842Z E1109 12:30:41.717820       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:30:58.395806804Z E1109 12:30:58.395700       1 event.go:208] Unable to write event: 'Post https://13.81.127.36/api/v1/namespaces/default/events: dial tcp 13.81.127.36:443: i/o timeout' (may retry after sleeping)
2016-11-09T12:31:12.718607967Z E1109 12:31:12.718486       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:31:12.718638967Z E1109 12:31:12.718486       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:31:38.396378873Z E1109 12:31:38.396285       1 event.go:208] Unable to write event: 'Post https://13.81.127.36/api/v1/namespaces/default/events: dial tcp 13.81.127.36:443: i/o timeout' (may retry after sleeping)
2016-11-09T12:31:43.719157532Z E1109 12:31:43.719067       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:31:43.719192733Z E1109 12:31:43.719157       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:32:14.719815561Z E1109 12:32:14.719691       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:32:14.719865063Z E1109 12:32:14.719768       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:32:18.399374002Z E1109 12:32:18.399260       1 event.go:208] Unable to write event: 'Post https://13.81.127.36/api/v1/namespaces/default/events: dial tcp 13.81.127.36:443: i/o timeout' (may retry after sleeping)
2016-11-09T12:32:18.399434103Z E1109 12:32:18.399287       1 event.go:143] Unable to write event '&api.Event{TypeMeta:unversioned.TypeMeta{Kind:"""", APIVersion:""""}, ObjectMeta:api.ObjectMeta{Name:""sayt-ppe1-node-0.1485609ad114a771"", GenerateName:"""", Namespace:""default"", SelfLink:"""", UID:"""", ResourceVersion:"""", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*unversioned.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]api.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:""""}, InvolvedObject:api.ObjectReference{Kind:""Node"", Namespace:"""", Name:""sayt-ppe1-node-0"", UID:""sayt-ppe1-node-0"", APIVersion:"""", ResourceVersion:"""", FieldPath:""""}, Reason:""Starting"", Message:""Starting kube-proxy."", Source:api.EventSource{Component:""kube-proxy"", Host:""sayt-ppe1-node-0""}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63614291070, nsec:710097777, loc:(*time.Location)(0x7590aa0)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63614291070, nsec:710097777, loc:(*time.Location)(0x7590aa0)}}, Count:1, Type:""Normal""}' (retry limit exceeded!)
2016-11-09T12:32:45.720461024Z E1109 12:32:45.720304       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:32:45.720492125Z E1109 12:32:45.720394       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:33:16.721085033Z E1109 12:33:16.720960       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:33:16.721193236Z E1109 12:33:16.721151       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:33:47.721761552Z E1109 12:33:47.721636       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:33:47.721799653Z E1109 12:33:47.721742       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:34:18.722270304Z E1109 12:34:18.722186       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:34:18.722300705Z E1109 12:34:18.722191       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:34:49.722947574Z E1109 12:34:49.722826       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:34:49.722979175Z E1109 12:34:49.722864       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:35:20.723595499Z E1109 12:35:20.723477       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:35:20.723627800Z E1109 12:35:20.723533       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:35:51.724275592Z E1109 12:35:51.724142       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:35:51.724307593Z E1109 12:35:51.724174       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:36:22.724997010Z E1109 12:36:22.724873       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:36:22.725034811Z E1109 12:36:22.724961       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:36:53.725607586Z E1109 12:36:53.725488       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:36:53.725646787Z E1109 12:36:53.725487       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:37:24.726161922Z E1109 12:37:24.726052       1 reflector.go:203] pkg/proxy/config/api.go:33: Failed to list *api.Endpoints: Get https://13.81.127.36/api/v1/endpoints?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
2016-11-09T12:37:24.726198323Z E1109 12:37:24.726175       1 reflector.go:203] pkg/proxy/config/api.go:30: Failed to list *api.Service: Get https://13.81.127.36/api/v1/services?resourceVersion=0: dial tcp 13.81.127.36:443: i/o timeout
Logs from 11/9/16 12:24 PM to 11/9/16 12:37 PM
````",closed,False,2016-11-09 12:38:02,2018-01-15 10:41:11
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/273,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/273,Fixes #271: Output GCE's kubeconfig.json to .tmp,"This matches what azure does, and the expectations of the Makefile.

After this change, doing `make deploy` is fixed for GCE, since `util/validate` is able to find the correct configuration to connect and count the nodes.

See https://github.com/kubernetes/kubernetes-anywhere/issues/271

CC @mikedanese ",closed,True,2016-11-09 20:17:17,2016-11-10 06:07:15
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/274,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/274,Support FORCE_DESTROY option in gce.,"This is already implemented in the azure phase1, and will allow users to automate destruction without interactive prompts:

    $ make FORCE_DESTROY=y destroy

This change is built on top of another pull request (https://github.com/kubernetes/kubernetes-anywhere/pull/270) so please only look at the newest commit.

CC @mikedanese ",closed,True,2016-11-09 21:53:34,2016-11-16 01:16:26
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/275,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/275,New make target: kubeconfig-path,"As a consumer of kubernetes-anywhere, I wanted a supported way to discover the `kubeconfig.json` file path without relying on the internal details of where it's stored per phase1 provider.

The target name feels a little awkward, so I'm open to suggestions of a better one.

CC @mikedanese ",closed,True,2016-11-10 03:24:35,2016-11-10 06:06:49
kubernetes-anywhere,danhawkes,https://github.com/kubernetes/kubernetes-anywhere/issues/276,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/276,"Azure, issue with inter-pod communication","Hi, I've been directed here from an issue with https://github.com/deis/workflow/issues/604.

> This looks like the underlying platform (kubernetes) is failing. I would submit a ticket to kubernetes-anywhere and see if they have any open issues related to this one.
```
Failed to setup network for pod 
\""dockerbuild-another-5d313d57-a3ef8932_deis(4d7a6cc8-aa5b-11e6-b9d9-000d3af76add)\"" 
using network plugins \""kubenet\"": Failed to retrieve pods from runtime: operation timeout: 
context deadline exceeded; Skipping pod""
```

There's some context in the linked issue, but unfortunately I'm new to kubernetes and not sure what typical debugging steps are. Any input would be appreciated. 

As a start, here's the output of `cluster-info dump`: 

[dump.txt](https://github.com/kubernetes/kubernetes-anywhere/files/596005/dump.txt)

(note, the output's truncated because the process seems to hang indefinitely at that point of getting logs)",closed,False,2016-11-16 21:52:26,2017-08-20 17:14:55
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/277,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/277,Force API port 443 to match other implementations.,"`Kubeadm` now defaults to 6443 for the secure API listener, so override it to match the conventions of the rest of `kubernetes-anywhere`.

CC @mikedanese ",closed,True,2016-11-17 19:56:01,2016-11-18 02:35:44
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/278,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/278,Add option to fetch kubeconfig from kubeadm master.,"When using `kubeadm` for phase2, the TLS certs created by `terraform` are ignored, and `kubeadm init` generates these on the master. If the user wants to be able to connect to the cluster remotely, we need to fetch the kubeconfig from the master once it's ready:

    $ make WAIT_FOR_KUBECONFIG=y deploy-cluster

CC @mikedanese ",closed,True,2016-11-17 23:23:05,2016-11-21 19:31:10
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/279,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/279,Specify master's external IP via --api-advertise-addresses.,"This allows `kubeadm init` to include the master's external IP as part of the TLS certificate. Otherwise, remote clients reject the certificate since it doesn't match the IP they're connecting to.

CC @mikedanese ",closed,True,2016-11-17 23:48:18,2016-11-21 19:35:49
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/280,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/280,Skip kubeadm preflight checks.,"A race condition in one of `kubeadm`'s preflight checks has been causing occasional failures of masters/nodes to come up correctly.

Since we provide a completely automated setup, any failed preflight checks don't offer much value since the user isn't interacting with `kubeadm` to see its error messages and address the issues, so we should
disable them anyway.

CC @mikedanese ",closed,True,2016-11-18 00:01:30,2016-11-21 19:30:08
kubernetes-anywhere,kim0,https://github.com/kubernetes/kubernetes-anywhere/issues/281,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/281,Howto Upgrade & Configure k8s cluster,"I have deployed k8s through ka and it's awesome! I have a couple of answered questions though.
* How do I upgrade to the next release of k8s. Say when 1.5 is out ? Or 1.4.5 since it seems I deployed 1.4.1
* It seems I will want to configure the api server to add more users and add an abac authorization policy. Can we document what needs to be done ? (i.e. should I just `docker exec -it xx bash` and edit config files for api server), then `docker restart` it ?
",closed,False,2016-11-20 13:24:10,2018-01-15 10:41:26
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/282,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/282,Add support for vSphere cloudprovider,This PR adds support for vSphere cloudprovider in kubernetes-anywhere. It is internally reviewed [here](https://github.com/abrarshivani/kubernetes-anywhere/pull/1).,closed,True,2016-11-23 22:51:08,2016-11-24 21:48:07
kubernetes-anywhere,evnsio,https://github.com/kubernetes/kubernetes-anywhere/issues/283,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/283,Correct way to configure the API server,"I need to enable basic auth on the API server using `--basic-auth-file [FILE]` option.   I'm guessing the best place to do this is in the phase2 `kube-apiserver.jsonnet` file, but I'm not sure where I'd put my file to ensure it's available on the node. 

Is this the right way to do it, and assuming so, how can I ensure my basic auth file is available on the master?",closed,False,2016-11-30 08:56:25,2017-01-25 18:11:37
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/284,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/284,Fix: Kubelet restarts for vSphere CloudProvider,"In vSphere CloudProvider, when node restarts, kubelet doesn't start. This PR fixes the same.",closed,True,2016-12-02 00:26:54,2016-12-09 19:26:08
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/issues/285,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/285,vSphere Cloud Provider: Destroying cluster is flaky,"```make destroy``` doesn't always succeed for kubernetes cluster launched using vSphere Cloud Provider.  This is due to terraform [issue](https://github.com/hashicorp/terraform/issues/10365).
Output after running ```make destroy```:
```
panic: runtime error: index out of range
2016/11/22 00:15:00 [DEBUG] plugin: terraform:
2016/11/22 00:15:00 [DEBUG] plugin: terraform: goroutine 8 [running]:
2016/11/22 00:15:00 [DEBUG] plugin: terraform: panic(0x27f4e60, 0xc42000e0f0)
2016/11/22 00:15:00 [DEBUG] plugin: terraform: 	/opt/go/src/runtime/panic.go:500 +0x1a1
2016/11/22 00:15:00 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/builtin/providers/vsphere.resourceVSphereVirtualMachineRead(0xc42033fe00, 0x2d2bb40, 0xc4203e3030, 0x1, 0x17)
2016/11/22 00:15:00 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/builtin/providers/vsphere/resource_vsphere_virtual_machine.go:1060 +0x3458
2016/11/22 00:15:00 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/helper/schema.(*Resource).Refresh(0xc4203de0c0, 0xc42021b300, 0x2d2bb40, 0xc4203e3030, 0xc42030b280, 0x1, 0x18)
2016/11/22 00:15:00 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/helper/schema/resource.go:259 +0x131
2016/11/22 00:15:00 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/helper/schema.(*Provider).Refresh(0xc4203da570, 0xc42021b2c0, 0xc42021b300, 0x0, 0x18, 0x18)
2016/11/22 00:15:00 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/helper/schema/provider.go:201 +0x91
2016/11/22 00:15:00 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/plugin.(*ResourceProviderServer).Refresh(0xc42030fbc0, 0xc4203d9100, 0xc4203d9730, 0x0, 0x0)
2016/11/22 00:15:00 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/plugin/resource_provider.go:482 +0x4e
2016/11/22 00:15:00 [DEBUG] plugin: terraform: reflect.Value.call(0xc42033f440, 0xc4203c6150, 0x13, 0x2da4800, 0x4, 0xc42054deb0, 0x3, 0x3, 0xc42034e6b0, 0xc4201bd920, ...)
2016/11/22 00:15:00 [DEBUG] plugin: terraform: 	/opt/go/src/reflect/value.go:434 +0x5c8
2016/11/22 00:15:00 [DEBUG] plugin: terraform: reflect.Value.Call(0xc42033f440, 0xc4203c6150, 0x13, 0xc42054deb0, 0x3, 0x3, 0xc42034e6a8, 0xc4201bd800, 0xda2681)
2016/11/22 00:15:00 [DEBUG] plugin: terraform: 	/opt/go/src/reflect/value.go:302 +0xa4
2016/11/22 00:15:00 [DEBUG] plugin: terraform: net/rpc.(*service).call(0xc4203ad800, 0xc4203ad7c0, 0xc4203e2120, 0xc4203c8b00, 0xc4203ea3e0, 0x255f980, 0xc4203d9100, 0x16, 0x255f9c0, 0xc4203d9730, ...)
2016/11/22 00:15:00 [DEBUG] plugin: terraform: 	/opt/go/src/net/rpc/server.go:383 +0x148
2016/11/22 00:15:00 [DEBUG] plugin: terraform: created by net/rpc.(*Server).ServeCodec
2016/11/22 00:15:00 [DEBUG] plugin: terraform: 	/opt/go/src/net/rpc/server.go:477 +0x421
2016/11/22 00:15:00 [ERROR] root: eval: *terraform.EvalRefresh, err: vsphere_virtual_machine.kubevm2: unexpected EOF
2016/11/22 00:15:00 [ERROR] root: eval: *terraform.EvalSequence, err: vsphere_virtual_machine.kubevm2: unexpected EOF
2016/11/22 00:15:00 [ERROR] root: eval: *terraform.EvalOpFilter, err: vsphere_virtual_machine.kubevm2: unexpected EOF
2016/11/22 00:15:00 [ERROR] root: eval: *terraform.EvalSequence, err: vsphere_virtual_machine.kubevm2: unexpected EOF
2016/11/22 00:15:00 [ERROR] root: eval: *terraform.EvalRefresh, err: vsphere_virtual_machine.kubevm1: unexpected EOF
2016/11/22 00:15:00 [TRACE] [walkRefresh] Exiting eval tree: vsphere_virtual_machine.kubevm2
2016/11/22 00:15:00 [ERROR] root: eval: *terraform.EvalSequence, err: vsphere_virtual_machine.kubevm1: unexpected EOF
2016/11/22 00:15:00 [ERROR] root: eval: *terraform.EvalOpFilter, err: vsphere_virtual_machine.kubevm1: unexpected EOF
2016/11/22 00:15:00 [ERROR] root: eval: *terraform.EvalSequence, err: vsphere_virtual_machine.kubevm1: unexpected EOF
2016/11/22 00:15:00 [TRACE] [walkRefresh] Exiting eval tree: vsphere_virtual_machine.kubevm1
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_node, got dep: vsphere_virtual_machine.kubevm2
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_node, got dep: tls_private_key.k8s-test5-master
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_node, got dep: provider.template
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_node, got dep: tls_self_signed_cert.k8s-test5-root
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_node, got dep: vsphere_virtual_machine.kubevm1
2016/11/22 00:15:00 [DEBUG] vertex null_resource.master, got dep: vsphere_virtual_machine.kubevm1
2016/11/22 00:15:00 [DEBUG] vertex provider.vsphere (close), got dep: vsphere_virtual_machine.kubevm1
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_master, got dep: vsphere_virtual_machine.kubevm1
2016/11/22 00:15:00 [DEBUG] vertex provider.vsphere (close), got dep: vsphere_virtual_machine.kubevm2
2016/11/22 00:15:00 [DEBUG] vertex provider.vsphere (close), got dep: vsphere_folder.cluster_folder
2016/11/22 00:15:00 [DEBUG] vertex data.tls_cert_request.k8s-test5-master, got dep: vsphere_virtual_machine.kubevm1
2016/11/22 00:15:00 [DEBUG] vertex null_resource.master, got dep: vsphere_virtual_machine.kubevm2
2016/11/22 00:15:00 [DEBUG] vertex data.tls_cert_request.k8s-test5-master, got dep: tls_private_key.k8s-test5-master
2016/11/22 00:15:00 [DEBUG] vertex null_resource.node2, got dep: vsphere_virtual_machine.kubevm1
2016/11/22 00:15:00 [DEBUG] vertex null_resource.node2, got dep: vsphere_virtual_machine.kubevm2
2016/11/22 00:15:00 [DEBUG] vertex tls_locally_signed_cert.k8s-test5-master, got dep: data.tls_cert_request.k8s-test5-master
2016/11/22 00:15:00 [DEBUG] vertex tls_locally_signed_cert.k8s-test5-master, got dep: tls_self_signed_cert.k8s-test5-root
2016/11/22 00:15:00 [DEBUG] vertex tls_locally_signed_cert.k8s-test5-master, got dep: provider.tls
2016/11/22 00:15:00 [DEBUG] vertex provider.tls (close), got dep: tls_locally_signed_cert.k8s-test5-master
2016/11/22 00:15:00 [DEBUG] vertex provider.tls (close), got dep: data.tls_cert_request.k8s-test5-node
2016/11/22 00:15:00 [DEBUG] vertex provider.tls (close), got dep: tls_locally_signed_cert.k8s-test5-admin
2016/11/22 00:15:00 [DEBUG] vertex provider.tls (close), got dep: tls_private_key.k8s-test5-master
2016/11/22 00:15:00 [DEBUG] vertex provider.tls (close), got dep: data.tls_cert_request.k8s-test5-master
2016/11/22 00:15:00 [DEBUG] vertex provider.tls (close), got dep: tls_self_signed_cert.k8s-test5-root
2016/11/22 00:15:00 [DEBUG] vertex provider.tls (close), got dep: tls_locally_signed_cert.k8s-test5-node
2016/11/22 00:15:00 [DEBUG] vertex provider.tls (close), got dep: tls_private_key.k8s-test5-node
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_master, got dep: tls_locally_signed_cert.k8s-test5-master
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_master, got dep: tls_locally_signed_cert.k8s-test5-node
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_master, got dep: vsphere_virtual_machine.kubevm2
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_master, got dep: provider.template
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_master, got dep: tls_self_signed_cert.k8s-test5-root
2016/11/22 00:15:00 [DEBUG] vertex null_resource.master, got dep: data.template_file.configure_master
2016/11/22 00:15:00 [DEBUG] vertex null_resource.master, got dep: provider.null
2016/11/22 00:15:00 [DEBUG] vertex null_resource.master, got dep: provisioner.remote-exec
2016/11/22 00:15:00 [DEBUG] vertex null_resource.master, got dep: provisioner.local-exec
2016/11/22 00:15:00 [DEBUG] vertex null_resource.master, got dep: data.template_file.cloudprovider
2016/11/22 00:15:00 [DEBUG] vertex null_resource.master, got dep: tls_locally_signed_cert.k8s-test5-admin
2016/11/22 00:15:00 [DEBUG] vertex null_resource.master, got dep: tls_private_key.k8s-test5-admin
2016/11/22 00:15:00 [DEBUG] vertex null_resource.master, got dep: tls_self_signed_cert.k8s-test5-root
2016/11/22 00:15:00 [DEBUG] vertex provisioner.remote-exec (close), got dep: null_resource.master
2016/11/22 00:15:00 [DEBUG] vertex data.template_file.configure_node, got dep: tls_locally_signed_cert.k8s-test5-master
2016/11/22 00:15:00 [DEBUG] vertex provider.template (close), got dep: data.template_file.configure_node
2016/11/22 00:15:00 [DEBUG] vertex provider.template (close), got dep: provider.template
2016/11/22 00:15:00 [DEBUG] vertex provider.template (close), got dep: data.template_file.cloudprovider
2016/11/22 00:15:00 [DEBUG] vertex provider.template (close), got dep: data.template_file.configure_master
2016/11/22 00:15:00 [DEBUG] vertex provider.null (close), got dep: null_resource.master
2016/11/22 00:15:00 [DEBUG] vertex provisioner.local-exec (close), got dep: null_resource.master
2016/11/22 00:15:00 [DEBUG] vertex null_resource.node2, got dep: data.template_file.configure_node
2016/11/22 00:15:00 [DEBUG] vertex null_resource.node2, got dep: data.template_file.cloudprovider
2016/11/22 00:15:00 [DEBUG] vertex null_resource.node2, got dep: provider.null
2016/11/22 00:15:00 [DEBUG] vertex provisioner.remote-exec (close), got dep: null_resource.node2
2016/11/22 00:15:00 [DEBUG] vertex provider.null (close), got dep: null_resource.node2
2016/11/22 00:15:00 [DEBUG] vertex root, got dep: provider.null (close)
2016/11/22 00:15:00 [DEBUG] vertex root, got dep: provider.template (close)
2016/11/22 00:15:00 [DEBUG] vertex root, got dep: provider.vsphere (close)
2016/11/22 00:15:00 [DEBUG] vertex root, got dep: provider.tls (close)
2016/11/22 00:15:00 [DEBUG] vertex root, got dep: provisioner.local-exec (close)
2016/11/22 00:15:00 [DEBUG] vertex root, got dep: provisioner.remote-exec (close)
2016/11/22 00:15:00 [DEBUG] plugin: waiting for all plugin processes to complete...
2016/11/22 00:15:00 [DEBUG] plugin: /bin/terraform: plugin process exited
2016/11/22 00:15:00 [DEBUG] plugin: /bin/terraform: plugin process exited
2016/11/22 00:15:00 [DEBUG] plugin: /bin/terraform: plugin process exited
2016/11/22 00:15:00 [DEBUG] plugin: /bin/terraform: plugin process exited
2016/11/22 00:15:00 [DEBUG] plugin: /bin/terraform: plugin process exited
2016/11/22 00:15:00 [DEBUG] plugin: /bin/terraform: plugin process exited



!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!

Terraform crashed! This is always indicative of a bug within Terraform.
A crash log has been placed at ""crash.log"" relative to your current
working directory. It would be immensely helpful if you could please
report the crash with Terraform[1] so that we can fix this.

When reporting bugs, please include your terraform version. That
information is available on the first line of crash.log. You can also
get it by running 'terraform --version' on the command line.

[1]: https://github.com/hashicorp/terraform/issues

!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!
Makefile:53: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:41: recipe for target 'destroy-cluster' failed
make: *** [destroy-cluster] Error 2

```",closed,False,2016-12-02 20:31:46,2018-01-15 10:41:37
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/issues/286,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/286,Ignite fails to download kubectl and failed to create kubelet.service,"When I built the docker container using DockerFile in phase2 and use that as ```.phase2.installer_container ``` , ignite fails to download kubectl and failed to create kubelet.service.
Here is output which I got while running kubernetes-anywhere,
```
null_resource.master (remote-exec): INFO     : files: createFilesystemsFiles: createFiles: op(11): op(12): [started]  GET ""https://storage.googleapis.com/kubernetes-release/release/v1.3.4/bin/linux/amd64/kubectl""
null_resource.master (remote-exec): DEBUG    : files: createFilesystemsFiles: createFiles: op(11): op(12): GET result: OK
null_resource.master: Still creating... (59s elapsed)
null_resource.master (remote-exec): CRITICAL : files: createFilesystemsFiles: createFiles: op(11): op(12): [failed]   GET ""https://storage.googleapis.com/kubernetes-release/release/v1.3.4/bin/linux/amd64/kubectl"": net/http: request canceled (Client.Timeout exceeded while reading body)
null_resource.master (remote-exec): CRITICAL : files: createFilesystemsFiles: createFiles: op(11): [failed]   fetching file ""/usr/local/bin/kubectl"": net/http: request canceled (Client.Timeout exceeded while reading body)
null_resource.master (remote-exec): CRITICAL : files: failed to create files: failed to create files: failed to resolve file ""/usr/local/bin/kubectl""
null_resource.master (remote-exec): Failed to execute operation: No such file or directory
null_resource.master (remote-exec): Failed to start kubelet.service: Unit kubelet.service failed to load: No such file or directory.
```",closed,False,2016-12-02 23:26:20,2017-01-24 22:06:54
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/287,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/287,Remove cidr parameters for vsphere,This PR removes cidr parameters for vsphere. Since vSphere supports flannel these parameters are not needed.,closed,True,2016-12-09 00:43:16,2016-12-09 19:26:20
kubernetes-anywhere,notsureifkevin,https://github.com/kubernetes/kubernetes-anywhere/issues/288,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/288,Attempted to create cluster in an ESXi 6 host results in `Failed to create folder` error,"```
Error applying plan:

1 error(s) occurred:

* vsphere_folder.cluster_folder: Failed to create folder at kubernetes; ServerFaultCode: The operation is not supported on the object.
```

```
> govc folder.create /kubernetes
govc: ServerFaultCode: The operation is not supported on the object.
```

This is a standalone ESXi 6 host. Please let me know if there is any other information I can provide.",closed,False,2016-12-10 03:34:15,2018-04-10 07:42:53
kubernetes-anywhere,simon-heinen,https://github.com/kubernetes/kubernetes-anywhere/issues/289,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/289,Building docker-dev on Windows using Cygwin & Docker for Windows,"I'm using Docker 1.12.3 and Cygwin to build the deployment environment but it fails with this error:

$ make docker-dev
docker build -t kubernetes-anywhere:v0.0.1 .
Sending build context to Docker daemon 190.5 kB
Step 1 : FROM mhart/alpine-node:6.4.0
6.4.0: Pulling from mhart/alpine-node
e110a4a17941: Pulling fs layer
d6d25f5b0348: Pulling fs layer
e110a4a17941: Verifying Checksum
e110a4a17941: Download complete
e110a4a17941: Pull complete
d6d25f5b0348: Verifying Checksum
d6d25f5b0348: Download complete
d6d25f5b0348: Pull complete
Digest: sha256:0a7f08961c8dfaf42910a5aa58549c840bb93fdafab5af3746c11f1cd4062bda
Status: Downloaded newer image for mhart/alpine-node:6.4.0
 ---> ecd37ad77c2b
Step 2 : RUN apk add --update bash
 ---> Running in f847384ac948
fetch http://dl-cdn.alpinelinux.org/alpine/v3.4/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.4/community/x86_64/APKINDEX.tar.gz
(1/5) Installing ncurses-terminfo-base (6.0-r7)
(2/5) Installing ncurses-terminfo (6.0-r7)
(3/5) Installing ncurses-libs (6.0-r7)
(4/5) Installing readline (6.3.008-r4)
(5/5) Installing bash (4.3.42-r4)
Executing bash-4.3.42-r4.post-install
Executing busybox-1.24.2-r9.trigger
OK: 14 MiB in 18 packages
 ---> 741fc6b0e9ab
Removing intermediate container f847384ac948
Step 3 : ADD ./util/docker-build.sh /opt/
 ---> 5e7fbaadfd9e
Removing intermediate container af211df4338c
Step 4 : RUN /opt/docker-build.sh
 ---> Running in acbe5da1a2f0
/bin/sh: /opt/docker-build.sh: not found
The command '/bin/sh -c /opt/docker-build.sh' returned a non-zero code: 127
Makefile:66: recipe for target 'docker-build' failed
make: *** [docker-build] Error 127


I checked that the Docker VM has enought memory and I tested to start both the Docker VM and Cygwin in Admin mode. Did anyone ever try to build this on Windows? ",closed,False,2016-12-10 14:15:57,2018-01-15 10:41:47
kubernetes-anywhere,evnsio,https://github.com/kubernetes/kubernetes-anywhere/issues/290,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/290,Issues creating storage container on Azure,"I've been trying to create a cluster in Azure and I'm getting the following error whilst terraform is trying to create the storage container within the account.

```
Error applying plan:

1 error(s) occurred:

* azurerm_storage_container.sc: Error creating container ""strgkubernetes"" in storage account ""l3c2dr2gxa4woz6f6xd5yk8s"": storage: service returned error: StatusCode=403, ErrorCode=AuthenticationFailed, ErrorMessage=Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.
```

I've tried using an existing Service Principal (which has owner privs) as well as letting k8s-anywhere create its own.   

I've confirmed that the storage account `l3c2dr2gxa4woz6f6xd5yk8s` exists in the Azure portal, and I've tested that I can create a container through the UI.

Any ideas what might be up? ",closed,False,2016-12-12 10:10:28,2016-12-12 14:28:37
kubernetes-anywhere,evnsio,https://github.com/kubernetes/kubernetes-anywhere/issues/291,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/291,Kubernetes version options are unclear,"During the `make config` step, one of the options is _kubernetes version_, which defaults to the string _v1.4.1_.   It's not clear what this string is used for or what the alternatives are.   

Is it the name of the releases as listed [here](https://github.com/kubernetes/kubernetes/releases), or a specific subset of them?",closed,False,2016-12-12 19:14:56,2018-01-15 10:41:56
kubernetes-anywhere,BaluDontu,https://github.com/kubernetes/kubernetes-anywhere/issues/292,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/292,kube-anywhere doesn't work with multiple resource pools with vsphere,"I have a VC deployed with multiple clusters and hence multiple resource pools. I deployed the ""KubernetesAnywhereTemplatePhotonOS"" in one of the ESX boxes present in one of the clusters.

when I did make deploy, kube-anywhere fails to deploy and reports the following errors.

Error applying plan:

2 error(s) occurred:

* vsphere_virtual_machine.kubevm2: default resource pool resolves to multiple instances, please specify
* vsphere_virtual_machine.kubevm1: default resource pool resolves to multiple instances, please specify

Right now kube-anywhere doesn't take into consideration the resource pools as a config parameter that must provided to the user to be entered. This is in contrast with kube-up which needs a resource pool to be configured in order for kubernetes cluster to be created.

kube-anywhere right now works with single resource pool.

@abrarshivani @kerneltime @pdhamdhere",closed,False,2016-12-13 02:09:44,2017-02-14 20:57:01
kubernetes-anywhere,BaluDontu,https://github.com/kubernetes/kubernetes-anywhere/pull/293,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/293,kube-anywhere doesn't work with multiple resource pools in vSphere,"Currently kube-anywhere doesn't work with multiple resource pools. Consider the below scenario:

I have a VC deployed with multiple clusters and hence multiple resource pools. I deployed the ""KubernetesAnywhereTemplatePhotonOS"" in one of the ESX boxes present in one of the clusters.

when I did make deploy, kube-anywhere fails to deploy and reports the following errors.

Error applying plan:

2 error(s) occurred:

* vsphere_virtual_machine.kubevm2: default resource pool resolves to multiple instances, please specify
* vsphere_virtual_machine.kubevm1: default resource pool resolves to multiple instances, please specify

Right now kube-anywhere doesn't take into consideration the resource pools as a config parameter that must provided to the user to be entered. This is in contrast with kube-up which needs a resource pool to be configured in order for kubernetes cluster to be created.

kube-anywhere right now works with single resource pool.

This fix will make sure kube-anywhere works with multiple resource pools. The resource pool is provided as a config entry to the user.

@abrarshivani @kerneltime @pdhamdhere",closed,True,2016-12-13 02:10:17,2016-12-17 18:32:08
kubernetes-anywhere,BaluDontu,https://github.com/kubernetes/kubernetes-anywhere/issues/294,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/294,Allow user to change the default password of the VM once the kubernetes cluster is up in vSphere,"Currently, the default password for the nodes is ""kubernetes"" on a kubernetes cluster in vSphere.  After successful deployment the user doesn't have a way to change the default password on first login. 

It is better to make sure the root user is forced to change the default password after the first login so that no other user can login into this node other than the root user.",closed,False,2016-12-15 00:24:44,2018-04-10 12:47:54
kubernetes-anywhere,BaluDontu,https://github.com/kubernetes/kubernetes-anywhere/issues/295,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/295,Change the default names of the nodes after a kubernetes cluster is deployed in vSphere,"Currently, after a successful deployment of kubernetes cluster in vSphere, the master is named as ""master"" and the nodes are named as ""node1"", ""node2"" and so on.

It is better if we prefix the name of the nodes with kubernetes so that the user can identify this is a kubernetes cluster deployed in the vSphere. So the plan is to modify the default name of the nodes to

""kubernetes-master""
""kubernetes-worker-1""
""kubernetes-worker-2"" etc.",closed,False,2016-12-15 00:30:21,2018-01-15 10:42:09
kubernetes-anywhere,BaluDontu,https://github.com/kubernetes/kubernetes-anywhere/issues/296,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/296,Unable to deploy a second instance of kubernetes cluster in the same resource pool in vSphere,"Currently, only a single instance of kubernetes cluster can be deployed in a resource pool inside vSphere. You cannot deploy another instance of kubernetes cluster on the same resource pool. The deployment would eventually if any attempt was made to do so. 

The ideal way would be allow the user to deploy multiple instances of kubernetes cluster inside a single resource pool in vSphere.",closed,False,2016-12-15 00:36:23,2018-04-09 03:02:33
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/pull/297,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/297,azure: mention that ACS/ACS-Engine is preferred method,cc: @brendandburns ,closed,True,2016-12-15 07:09:21,2016-12-15 21:48:37
kubernetes-anywhere,RossWilliams,https://github.com/kubernetes/kubernetes-anywhere/issues/298,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/298,Azure - Elasticsearch discovery mechanism incompatible with default settings,"The blessed image for Elasticsearch seems to be https://github.com/pires/docker-elasticsearch-kubernetes
The instructions for setting up node discovery, and templates are here: https://github.com/pires/kubernetes-elasticsearch-cluster

Unfortunately, the default setup on Azure does not have components setup to run the discovery process correctly. Specifically, it appears that communication with kube-apiserver is not successful from the pods. It is using a service account to authenticate.

I'm not sure if this is a problem with the default configuration values, or if the pod is using non-standard techniques. Let me know if this isn't the right place to file this.",closed,False,2016-12-15 16:00:10,2017-01-10 01:27:30
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/issues/299,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/299,Install fails silently if version of k8s is mistyped,"Ran into this when using k8s-any on vsphere.
I typed in 1.4.7 instead of v1.4.7 and the install hangs forever waiting (https://github.com/kubernetes/kubernetes-anywhere/blob/master/util/validate#L14) for the services to come up instead of failing with an error. ",closed,False,2016-12-15 18:27:08,2018-01-15 10:50:25
kubernetes-anywhere,BaluDontu,https://github.com/kubernetes/kubernetes-anywhere/issues/300,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/300,"kube-anywhere fails with ""unexpected EOF"" or ""connection shut down"" when photon OS template is not within the cluster","When trying to deploy a kubernetes on cluster A and datastore DS1 inside a VC, I observe the following 

1. uploading the photonOS template on ""datastore DS2"" on a different cluster say ""cluster B"" altogether is not working.
2. uploading the photonOS template on ""datastore DS3"" in the same cluster ""clusterA"" is working.

The deployment fails with errors ""unexpected EOF"", ""connection shut down"". Its because terraform crashes for vsphere plugin. However, I see this errors only after the kubernetes nodes are created sometimes (or) after powering on those kubernetes nodes. So this is not something related to that it couldn't find the template.",closed,False,2016-12-17 01:41:11,2017-06-12 21:35:35
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/issues/301,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/301,Document the customization done to Photon OS for kubernetes anywhere,The image used for k-any is based on Photon and includes additional configuration. This configuration either needs to be automated or at a minimum documented.,closed,False,2016-12-20 21:25:24,2018-01-15 10:50:33
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/pull/302,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/302,Minor changes and document known issues,,closed,True,2016-12-20 21:39:40,2016-12-21 00:10:22
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/303,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/303,Add support for gs:// links for kubeadm .debs.,"Instead of only supporting the latest stable release, allow specifying a Cloud Storage link for pulling arbitary builds. This should allow developers to bring up kubeadm clusters using their local development build, as well as help e2e test kubeadm on pull requests or master commits.

CC @mikedanese ",closed,True,2016-12-21 21:23:29,2016-12-21 22:03:37
kubernetes-anywhere,edevil,https://github.com/kubernetes/kubernetes-anywhere/issues/304,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/304,Correct value for MountFlags,"I'm following the CoreOS guide and merging with this one for Azure cloudprovider specific sttuff. However, they differ in the value of the MountFlags variable in the Docker drop-in, so I'm not really sure which value to use.

In the [CoreOS](https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/deploy-master.md) guide no value is given for MountFlags, while in [this](https://github.com/kubernetes/kubernetes-anywhere/blob/55737c3818549f06f3af074f8d71b727568c00f2/phase1/azure/configure-vm.sh) guide the ""shared"" value is used. I think this makes sense since kubelet is run inside a Docker container and we want Azure mounted volumes to be available to other pods, but in the CoreOS setup kubelet is run inside a rkt container, not Docker. Can this be reason for the difference?

The default value used to be slave: https://github.com/docker/docker/commit/2aee081cad72352f8b0c37ba0414ebc925b022e8

Thanks.
",closed,False,2016-12-21 23:57:21,2016-12-28 17:29:55
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/305,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/305,Change ignition provider from vanilla to file,Changes ignition provider from vanilla to file and fixes #286.,closed,True,2016-12-23 13:50:57,2017-02-07 17:26:10
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/pull/306,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/306,Add detailed steps for vSphere,,closed,True,2016-12-24 00:39:20,2016-12-26 22:29:27
kubernetes-anywhere,FilipVozar,https://github.com/kubernetes/kubernetes-anywhere/issues/307,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/307,Azure: support for multi-subscription clusters?,"Hi, any plans to support Kubernetes clusters running on multiple Azure subscriptions (single cluster spanning multiple subscriptions)? There's [Azure BizSpark offer](https://azure.microsoft.com/en-us/pricing/member-offers/bizspark-startups/), which includes 5 subscriptions with free 150$/month, but there's no way to join those resources into one single subscription (many people have asked that).

I've been running a cluster in this mode for some time (one does learn a lot when trying to run Kubernetes in this environment using various hacks :)), my biggest challenge so far has been storage - I was very happy to see Azure Disk support being added to Kubernetes, but then I realized  I can't really use it.",closed,False,2016-12-25 22:39:10,2018-08-18 14:44:36
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/pull/308,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/308,Update TOC,,closed,True,2016-12-29 08:40:34,2016-12-29 19:06:10
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/issues/309,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/309,vSphere: Increase size of image and include instructions for configuring the template OS ova,"The current size of the image used for vSphere is pretty small and there are no instructions available as how can it be customized

- [ ] Include steps for how to generate the image using base Photon OS image

- [ ] Documentation for customizations

- [ ] Increase the size of the default image and make it thin provisioned. ",closed,False,2017-01-04 18:26:16,2018-01-15 10:50:52
kubernetes-anywhere,jcbsmpsn,https://github.com/kubernetes/kubernetes-anywhere/pull/310,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/310,Document GCP development deployment process.,"Describe the steps for using kubernetes-anywhere to deploy a kubernetes
cluster on GCE VMs with a local development build of kubernetes.",closed,True,2017-01-05 22:16:27,2017-01-09 21:22:04
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/311,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/311,"Fix ""make menuconfig"" from within ""make docker-dev""","The `menuconfig` target was broken because `kconfig-mconf` was missing from the Docker image, and `kconfig-mconf` wasn't being compiled because `ncurses-dev` wasn't detected. Adding `ncurses-dev` enables compiling of both `mconf` and `nconf`, but we need to disable `nconf` because it depends on `libgpm`, which is missing from the Alpine repo.

After this change, running `make docker-dev` followed by `make menuconfig` from within the Docker shell works.

CC @jcbsmpsn ",closed,True,2017-01-06 00:21:08,2017-01-06 16:45:10
kubernetes-anywhere,thakala,https://github.com/kubernetes/kubernetes-anywhere/issues/312,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/312,"make deploy fails on ""* data.tls_cert_request.kubernetes-master: unexpected EOF""","I have had one successful Kubernetes deployment on vSphere 6. I removed it with ""make destroy"" and tried to redeploy but it is now failing on TLS error. Configuration is still set to allow self signed certificates.

[container]:/opt/kubernetes-anywhere> make clean
rm -rf .tmp
rm -rf phase3/.tmp
rm -rf phase1/gce/.tmp
rm -rf phase1/azure/.tmp
rm -rf phase1/vsphere/.tmp

[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json .config > .config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/$(jq -r '.phase1.cloud_provider' .config.json)""; ./do deploy-cluster )
.tmp/vSphere-kubernetes.tf
data.template_file.cloudprovider: Refreshing state...
tls_private_key.kubernetes-root: Creating...
  algorithm:          """" => ""RSA""
  ecdsa_curve:        """" => ""P224""
  private_key_pem:    """" => ""<computed>""
  public_key_openssh: """" => ""<computed>""
  public_key_pem:     """" => ""<computed>""
  rsa_bits:           """" => ""2048""
tls_private_key.kubernetes-node: Creating...
  algorithm:          """" => ""RSA""
  ecdsa_curve:        """" => ""P224""
  private_key_pem:    """" => ""<computed>""
  public_key_openssh: """" => ""<computed>""
  public_key_pem:     """" => ""<computed>""
  rsa_bits:           """" => ""2048""
tls_private_key.kubernetes-master: Creating...
  algorithm:          """" => ""RSA""
  ecdsa_curve:        """" => ""P224""
  private_key_pem:    """" => ""<computed>""
  public_key_openssh: """" => ""<computed>""
  public_key_pem:     """" => ""<computed>""
  rsa_bits:           """" => ""2048""
tls_private_key.kubernetes-admin: Creating...
  algorithm:          """" => ""RSA""
  ecdsa_curve:        """" => ""P224""
  private_key_pem:    """" => ""<computed>""
  public_key_openssh: """" => ""<computed>""
  public_key_pem:     """" => ""<computed>""
  rsa_bits:           """" => ""2048""
vsphere_folder.cluster_folder: Creating...
  datacenter:    """" => ""Derby""
  existing_path: """" => ""<computed>""
  path:          """" => ""kubernetes""
vsphere_folder.cluster_folder: Creation complete
vsphere_virtual_machine.kubevm1: Creating...
..
..
vsphere_virtual_machine.kubevm5: Still creating... (2m10s elapsed)
vsphere_virtual_machine.kubevm4: Still creating... (2m10s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (2m10s elapsed)
vsphere_virtual_machine.kubevm5: Still creating... (2m20s elapsed)
vsphere_virtual_machine.kubevm4: Still creating... (2m20s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (2m20s elapsed)
vsphere_virtual_machine.kubevm2: Creation complete
vsphere_virtual_machine.kubevm4: Creation complete
vsphere_virtual_machine.kubevm5: Creation complete
Error applying plan:

1 error(s) occurred:

* data.tls_cert_request.kubernetes-master: unexpected EOF

Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.
Makefile:63: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:41: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
[container]:/opt/kubernetes-anywhere> ",closed,False,2017-01-10 15:09:44,2018-03-23 16:46:53
kubernetes-anywhere,maplelabs,https://github.com/kubernetes/kubernetes-anywhere/issues/313,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/313,"No option to set http_proxy for nodes, network timeout in make deploy","I am trying to use kubernetes-anywhere to create cluster in vSphere.
VMs need to have http_proxy configure in order to pull docker images needed for kubernetes cluster but I dont see any option to set proxy

null_resource.node2 (remote-exec): Failed to start kubelet.service: Unit kubelet.service failed to load: No such file or directory.
null_resource.node4 (remote-exec): Failed to start kubelet.service: Unit kubelet.service failed to load: No such file or directory.
null_resource.node4: Creation complete
null_resource.node2: Creation complete
null_resource.node5 (remote-exec): docker: Network timed out while trying to connect to https://index.docker.io/v1/repositories/ashivani/k8s-ignition/images. You may want to check your internet connection or if you are behind a proxy..
null_resource.node5 (remote-exec): See 'docker run --help'.
null_resource.node5 (remote-exec): Failed to execute operation: No such file or directory
null_resource.node5 (remote-exec): Failed to start kubelet.service: Unit kubelet.service failed to load: No such file or directory.
null_resource.node3 (remote-exec): docker: Network timed out while trying to connect to https://index.docker.io/v1/repositories/ashivani/k8s-ignition/images. You may want to check your internet connection or if you are behind a proxy..
null_resource.node3 (remote-exec): See 'docker run --help'.
null_resource.node5: Creation complete
null_resource.node3 (remote-exec): Failed to execute operation: No such file or directory
null_resource.node3 (remote-exec): Failed to start kubelet.service: Unit kubelet.service failed to load: No such file or directory.
null_resource.node3: Creation complete
",closed,False,2017-01-13 07:12:57,2018-05-11 17:31:50
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/314,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/314,Updated documentation to recommend kubernetes 1.4.8 for vSphere Cloud Provider,"Changes for the issue - https://github.com/vmware/kubernetes/issues/54

@kerneltime @BaluDontu Updated phase1/vsphere/README.md and phase2/Kconfig to recommend kubernetes 1.4.8 for vSphere Cloud Provider",closed,True,2017-01-16 20:01:14,2017-01-18 18:49:31
kubernetes-anywhere,csarora,https://github.com/kubernetes/kubernetes-anywhere/issues/315,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/315,Service with Type: :LoadBalancer on vSphere Client ( CentOS 7) stuck in pending state,"Service with type : LoadBalancer is stuck in pending state and does not get a external IP
Can someone please guide how can i debug whats causing the issue? OR suggest some way to expose a service externally.

 **kubectl run my-nginx --image=nginx --replicas=2 --port=80**
 **kubectl expose deployment my-nginx --port=80 --type=LoadBalancer**

[root@dev0]# **kubectl get services**
NAME                       CLUSTER-IP               EXTERNAL-IP            PORT(S)                      AGE
my-nginx              10.110.113.150                  pending             80:30453/TCP                 24m

[root@dev01]# **kubectl describe svc my-nginx**
Name:                   my-nginx
Namespace:              default
Labels:                 run=my-nginx
Selector:               run=my-nginx
Type:                   LoadBalancer
IP:                     10.110.113.150
Port:                   <unset> 80/TCP
NodePort:               <unset> 30453/TCP
Endpoints:              10.32.0.149:80,10.32.0.150:80
Session Affinity:       None
No events.

**System Details :**
OS Image:                      CentOS Linux 7 (Core)
Operating System:              linux
Architecture:                  amd64
Container Runtime Version:     docker://1.10.3
Kubelet Version:               v1.5.1
Kube-Proxy Version:            v1.5.1

Please help in exposing a service externally . Thanks",closed,False,2017-01-17 14:11:56,2018-02-18 11:44:59
kubernetes-anywhere,upendrasahu,https://github.com/kubernetes/kubernetes-anywhere/issues/316,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/316,VSphere Failed to start Kubernetes Kubelet Server,"root@photon-zv1KbtvMG [ ~ ]# journalctl -u kubelet
-- Logs begin at Tue 2017-01-17 10:31:08 UTC, end at Tue 2017-01-17 11:25:17 UTC. --
Jan 17 10:32:14 photon-zv1KbtvMG systemd[1]: Starting Kubernetes Kubelet Server...
Jan 17 10:32:14 photon-zv1KbtvMG systemd[1]: Started Kubernetes Kubelet Server.
Jan 17 10:32:14 photon-zv1KbtvMG docker[735]: Unable to find image 'gcr.io/google-containers/hyperkube-amd64:v1.4.7' locally
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: v1.4.7: Pulling from google-containers/hyperkube-amd64
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 386a066cd84a: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: c3d846c7ae5a: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 0be0d8e6181f: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: fa2a0296b51e: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 3bed2e6f9b4b: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 88b87718c8bc: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: ad569c29c6d9: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 0b137deb028d: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 1546671cc328: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 37f0af6ff931: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: bceffa791df4: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 7410e97eb86f: Pulling fs layer
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 88b87718c8bc: Waiting
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: ad569c29c6d9: Waiting
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 0b137deb028d: Waiting
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 1546671cc328: Waiting
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 37f0af6ff931: Waiting
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: bceffa791df4: Waiting
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 7410e97eb86f: Waiting
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: fa2a0296b51e: Waiting
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 3bed2e6f9b4b: Waiting
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 0be0d8e6181f: Verifying Checksum
Jan 17 10:32:15 photon-zv1KbtvMG docker[735]: 0be0d8e6181f: Download complete
Jan 17 10:32:28 photon-zv1KbtvMG docker[735]: fa2a0296b51e: Verifying Checksum
Jan 17 10:32:28 photon-zv1KbtvMG docker[735]: fa2a0296b51e: Download complete
Jan 17 10:32:28 photon-zv1KbtvMG docker[735]: 3bed2e6f9b4b: Verifying Checksum
Jan 17 10:32:28 photon-zv1KbtvMG docker[735]: 3bed2e6f9b4b: Download complete
Jan 17 10:32:29 photon-zv1KbtvMG docker[735]: 88b87718c8bc: Verifying Checksum
Jan 17 10:32:29 photon-zv1KbtvMG docker[735]: 88b87718c8bc: Download complete
Jan 17 10:32:29 photon-zv1KbtvMG docker[735]: ad569c29c6d9: Verifying Checksum
Jan 17 10:32:29 photon-zv1KbtvMG docker[735]: ad569c29c6d9: Download complete
Jan 17 10:32:29 photon-zv1KbtvMG docker[735]: 0b137deb028d: Verifying Checksum
Jan 17 10:32:29 photon-zv1KbtvMG docker[735]: 0b137deb028d: Download complete
Jan 17 10:32:30 photon-zv1KbtvMG docker[735]: 1546671cc328: Verifying Checksum
Jan 17 10:32:30 photon-zv1KbtvMG docker[735]: 1546671cc328: Download complete
Jan 17 10:32:33 photon-zv1KbtvMG docker[735]: 386a066cd84a: Verifying Checksum
Jan 17 10:32:33 photon-zv1KbtvMG docker[735]: 386a066cd84a: Download complete
Jan 17 10:32:33 photon-zv1KbtvMG docker[735]: bceffa791df4: Verifying Checksum
Jan 17 10:32:33 photon-zv1KbtvMG docker[735]: bceffa791df4: Download complete
Jan 17 10:32:34 photon-zv1KbtvMG docker[735]: 7410e97eb86f: Verifying Checksum
Jan 17 10:32:34 photon-zv1KbtvMG docker[735]: 7410e97eb86f: Download complete
Jan 17 10:32:36 photon-zv1KbtvMG docker[735]: 37f0af6ff931: Verifying Checksum
Jan 17 10:32:36 photon-zv1KbtvMG docker[735]: 37f0af6ff931: Download complete
Jan 17 10:32:38 photon-zv1KbtvMG docker[735]: 386a066cd84a: Pull complete
Jan 17 10:32:39 photon-zv1KbtvMG docker[735]: c3d846c7ae5a: Verifying Checksum
Jan 17 10:32:39 photon-zv1KbtvMG docker[735]: c3d846c7ae5a: Download complete
Jan 17 10:32:46 photon-zv1KbtvMG docker[735]: c3d846c7ae5a: Pull complete
Jan 17 10:32:47 photon-zv1KbtvMG docker[735]: 0be0d8e6181f: Pull complete
Jan 17 10:32:51 photon-zv1KbtvMG docker[735]: fa2a0296b51e: Pull complete
Jan 17 10:32:52 photon-zv1KbtvMG docker[735]: 3bed2e6f9b4b: Pull complete
Jan 17 10:32:52 photon-zv1KbtvMG docker[735]: 88b87718c8bc: Pull complete
Jan 17 10:32:52 photon-zv1KbtvMG docker[735]: ad569c29c6d9: Pull complete
Jan 17 10:32:53 photon-zv1KbtvMG docker[735]: 0b137deb028d: Pull complete
Jan 17 10:32:54 photon-zv1KbtvMG docker[735]: 1546671cc328: Pull complete
Jan 17 10:32:55 photon-zv1KbtvMG docker[735]: 37f0af6ff931: Pull complete
Jan 17 10:32:56 photon-zv1KbtvMG docker[735]: bceffa791df4: Pull complete
Jan 17 10:32:57 photon-zv1KbtvMG docker[735]: 7410e97eb86f: Pull complete
Jan 17 10:32:57 photon-zv1KbtvMG docker[735]: Digest: sha256:ea917942c2b69cdabb2430e67e06c6b7522f992f4882c971a8af9670284b0190
Jan 17 10:32:57 photon-zv1KbtvMG docker[735]: Status: Downloaded newer image for gcr.io/google-containers/hyperkube-amd64:v1.4.7
Jan 17 10:32:57 photon-zv1KbtvMG docker[735]: Flag --config has been deprecated, Use --pod-manifest-path instead. Will be removed in a future version.
Jan 17 10:32:57 photon-zv1KbtvMG docker[735]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.
Jan 17 10:32:57 photon-zv1KbtvMG docker[735]: F0117 10:32:57.407126     846 plugins.go:80] Couldn't open cloud provider configuration /etc/kubernetes/vsphere.conf: &os.PathError{Op:""open
"", Path:""/etc/kubernetes/vsphere.conf"", Err:0x2}
Jan 17 10:32:57 photon-zv1KbtvMG systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
Jan 17 10:32:57 photon-zv1KbtvMG systemd[1]: kubelet.service: Unit entered failed state.
Jan 17 10:32:57 photon-zv1KbtvMG systemd[1]: kubelet.service: Failed with result 'exit-code'.
Jan 17 10:32:57 photon-zv1KbtvMG systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Jan 17 10:32:57 photon-zv1KbtvMG systemd[1]: Stopped Kubernetes Kubelet Server.
Jan 17 10:32:57 photon-zv1KbtvMG systemd[1]: Starting Kubernetes Kubelet Server...
Jan 17 10:32:57 photon-zv1KbtvMG systemd[1]: Started Kubernetes Kubelet Server.
Jan 17 10:32:57 photon-zv1KbtvMG docker[887]: Flag --config has been deprecated, Use --pod-manifest-path instead. Will be removed in a future version.
Jan 17 10:32:57 photon-zv1KbtvMG docker[887]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.
Jan 17 10:32:57 photon-zv1KbtvMG docker[887]: F0117 10:32:57.880972     914 plugins.go:80] Couldn't open cloud provider configuration /etc/kubernetes/vsphere.conf: &os.PathError{Op:""open
"", Path:""/etc/kubernetes/vsphere.conf"", Err:0x2}
Jan 17 10:32:57 photon-zv1KbtvMG systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
Jan 17 10:32:57 photon-zv1KbtvMG systemd[1]: kubelet.service: Unit entered failed state.
Jan 17 10:32:57 photon-zv1KbtvMG systemd[1]: kubelet.service: Failed with result 'exit-code'.
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: Stopped Kubernetes Kubelet Server.
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: Starting Kubernetes Kubelet Server...
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: Started Kubernetes Kubelet Server.
Jan 17 10:32:58 photon-zv1KbtvMG docker[954]: Flag --config has been deprecated, Use --pod-manifest-path instead. Will be removed in a future version.
Jan 17 10:32:58 photon-zv1KbtvMG docker[954]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.
Jan 17 10:32:58 photon-zv1KbtvMG docker[954]: F0117 10:32:58.372526     980 plugins.go:80] Couldn't open cloud provider configuration /etc/kubernetes/vsphere.conf: &os.PathError{Op:""open
"", Path:""/etc/kubernetes/vsphere.conf"", Err:0x2}
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: kubelet.service: Unit entered failed state.
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: kubelet.service: Failed with result 'exit-code'.
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: Stopped Kubernetes Kubelet Server.
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: Starting Kubernetes Kubelet Server...
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: Started Kubernetes Kubelet Server.
Jan 17 10:32:58 photon-zv1KbtvMG docker[1022]: Flag --config has been deprecated, Use --pod-manifest-path instead. Will be removed in a future version.
Jan 17 10:32:58 photon-zv1KbtvMG docker[1022]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.
Jan 17 10:32:58 photon-zv1KbtvMG docker[1022]: F0117 10:32:58.876802    1047 plugins.go:80] Couldn't open cloud provider configuration /etc/kubernetes/vsphere.conf: &os.PathError{Op:""ope
n"", Path:""/etc/kubernetes/vsphere.conf"", Err:0x2}
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: kubelet.service: Unit entered failed state.
Jan 17 10:32:58 photon-zv1KbtvMG systemd[1]: kubelet.service: Failed with result 'exit-code'.
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: Stopped Kubernetes Kubelet Server.
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: Starting Kubernetes Kubelet Server...
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: Started Kubernetes Kubelet Server.
Jan 17 10:32:59 photon-zv1KbtvMG docker[1091]: Flag --config has been deprecated, Use --pod-manifest-path instead. Will be removed in a future version.
Jan 17 10:32:59 photon-zv1KbtvMG docker[1091]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.
Jan 17 10:32:59 photon-zv1KbtvMG docker[1091]: F0117 10:32:59.396996    1115 plugins.go:80] Couldn't open cloud provider configuration /etc/kubernetes/vsphere.conf: &os.PathError{Op:""ope
n"", Path:""/etc/kubernetes/vsphere.conf"", Err:0x2}
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: kubelet.service: Unit entered failed state.
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: kubelet.service: Failed with result 'exit-code'.
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: Stopped Kubernetes Kubelet Server.
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: Starting Kubernetes Kubelet Server...
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: Started Kubernetes Kubelet Server.
Jan 17 10:32:59 photon-zv1KbtvMG docker[1157]: Flag --config has been deprecated, Use --pod-manifest-path instead. Will be removed in a future version.
Jan 17 10:32:59 photon-zv1KbtvMG docker[1157]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.
Jan 17 10:32:59 photon-zv1KbtvMG docker[1157]: F0117 10:32:59.889406    1180 plugins.go:80] Couldn't open cloud provider configuration /etc/kubernetes/vsphere.conf: &os.PathError{Op:""ope
n"", Path:""/etc/kubernetes/vsphere.conf"", Err:0x2}
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: kubelet.service: Unit entered failed state.
Jan 17 10:32:59 photon-zv1KbtvMG systemd[1]: kubelet.service: Failed with result 'exit-code'.
Jan 17 10:33:00 photon-zv1KbtvMG systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Jan 17 10:33:00 photon-zv1KbtvMG systemd[1]: Stopped Kubernetes Kubelet Server.
Jan 17 10:33:00 photon-zv1KbtvMG systemd[1]: kubelet.service: Start request repeated too quickly.
Jan 17 10:33:00 photon-zv1KbtvMG systemd[1]: Failed to start Kubernetes Kubelet Server.
Jan 17 10:33:00 photon-zv1KbtvMG systemd[1]: kubelet.service: Unit entered failed state.
Jan 17 10:33:00 photon-zv1KbtvMG systemd[1]: kubelet.service: Failed with result 'start-limit'.",closed,False,2017-01-18 00:06:59,2018-01-15 11:23:40
kubernetes-anywhere,BaluDontu,https://github.com/kubernetes/kubernetes-anywhere/issues/317,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/317,kube-anywhere fails to deploy kubernetes cluster on master builds.,"I am using kubernetes master builds to deploy kubernetes cluster. I see the API server fails to start on the kubernetes master. I see the following errors.

```
I0119 22:51:31.881006       5 interface.go:248] Default route transits interface ""eth0""
I0119 22:51:31.881179       5 interface.go:93] Interface eth0 is up
I0119 22:51:31.881437       5 interface.go:138] Interface ""eth0"" has 2 addresses :[172.1.30.2/24 fe80::42:acff:fe01:1e02/64].
I0119 22:51:31.881474       5 interface.go:105] Checking addr  172.1.30.2/24.
I0119 22:51:31.881481       5 interface.go:114] IP found 172.1.30.2
I0119 22:51:31.881627       5 interface.go:144] valid IPv4 address for interface ""eth0"" found as 172.1.30.2.
I0119 22:51:31.881635       5 interface.go:254] Choosing IP 172.1.30.2 
I0119 22:51:31.881660       5 services.go:51] Setting service IP to ""10.0.0.1"" (read-write).
I0119 22:51:31.889220       5 server.go:170] Initializing deserialization cache size based on 0MB limit
W0119 22:51:31.891010       5 config_selfclient.go:44] Failed to create secure local client, falling back to insecure local connection: failed to find certificate which matches ""localhost""
I0119 22:51:31.891656       5 reflector.go:196] Starting reflector *api.ServiceAccount (0s) from k8s.io/kubernetes/plugin/pkg/admission/serviceaccount/admission.go:123
I0119 22:51:31.891708       5 reflector.go:196] Starting reflector *api.Secret (0s) from k8s.io/kubernetes/plugin/pkg/admission/serviceaccount/admission.go:143
I0119 22:51:31.891800       5 reflector.go:234] Listing and watching *api.ServiceAccount from k8s.io/kubernetes/plugin/pkg/admission/serviceaccount/admission.go:123
E0119 22:51:31.895589       5 reflector.go:199] k8s.io/kubernetes/plugin/pkg/admission/serviceaccount/admission.go:123: Failed to list *api.ServiceAccount: Get http://0.0.0.0:8080/api/v1/serviceaccounts?resourceVersion=0: dial tcp 0.0.0.0:8080: getsockopt: connection refused
I0119 22:51:31.895724       5 reflector.go:234] Listing and watching *api.Secret from k8s.io/kubernetes/plugin/pkg/admission/serviceaccount/admission.go:143
I0119 22:51:31.901574       5 reflector.go:196] Starting reflector *api.ResourceQuota (0s) from k8s.io/kubernetes/plugin/pkg/admission/resourcequota/resource_access.go:88
I0119 22:51:31.901870       5 reflector.go:234] Listing and watching *api.ResourceQuota from k8s.io/kubernetes/plugin/pkg/admission/resourcequota/resource_access.go:88
I0119 22:51:31.907498       5 server.go:328] Initializing cache sizes based on 0MB limit
I0119 22:51:31.907613       5 services.go:51] Setting service IP to ""10.0.0.1"" (read-write).
I0119 22:51:31.911287       5 storage_factory.go:242] storing { podtemplates} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
E0119 22:51:31.913428       5 reflector.go:199] k8s.io/kubernetes/plugin/pkg/admission/serviceaccount/admission.go:143: Failed to list *api.Secret: Get http://0.0.0.0:8080/api/v1/secrets?fieldSelector=type%3Dkubernetes.io%2Fservice-account-token&resourceVersion=0: dial tcp 0.0.0.0:8080: getsockopt: connection refused
E0119 22:51:31.913898       5 reflector.go:199] k8s.io/kubernetes/plugin/pkg/admission/resourcequota/resource_access.go:88: Failed to list *api.ResourceQuota: Get http://0.0.0.0:8080/api/v1/resourcequotas?resourceVersion=0: dial tcp 0.0.0.0:8080: getsockopt: connection refused
I0119 22:51:31.916365       5 storage_factory.go:242] storing { events} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.916529       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.916593       5 storage_factory.go:242] storing { limitranges} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.916855       5 reflector.go:234] Listing and watching *api.PodTemplate from pkg/storage/cacher.go:212
I0119 22:51:31.917040       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.917425       5 storage_factory.go:242] storing { resourcequotas} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.917589       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.917607       5 reflector.go:234] Listing and watching *api.LimitRange from pkg/storage/cacher.go:212
I0119 22:51:31.917926       5 storage_factory.go:242] storing { secrets} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.918102       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.918118       5 reflector.go:234] Listing and watching *api.ResourceQuota from pkg/storage/cacher.go:212
I0119 22:51:31.918685       5 storage_factory.go:242] storing { serviceaccounts} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.918844       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.919012       5 reflector.go:234] Listing and watching *api.Secret from pkg/storage/cacher.go:212
I0119 22:51:31.919223       5 storage_factory.go:242] storing { persistentvolumes} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.919366       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.919566       5 reflector.go:234] Listing and watching *api.ServiceAccount from pkg/storage/cacher.go:212
I0119 22:51:31.919798       5 storage_factory.go:242] storing { persistentvolumeclaims} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.919992       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.920256       5 reflector.go:234] Listing and watching *api.PersistentVolume from pkg/storage/cacher.go:212
I0119 22:51:31.920393       5 storage_factory.go:242] storing { configmaps} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.920535       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.920780       5 storage_factory.go:242] storing { namespaces} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.920912       5 reflector.go:234] Listing and watching *api.PersistentVolumeClaim from pkg/storage/cacher.go:212
I0119 22:51:31.920921       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.921301       5 reflector.go:234] Listing and watching *api.ConfigMap from pkg/storage/cacher.go:212
I0119 22:51:31.921364       5 storage_factory.go:242] storing { endpoints} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.921541       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.921821       5 reflector.go:234] Listing and watching *api.Namespace from pkg/storage/cacher.go:212
I0119 22:51:31.921983       5 storage_factory.go:242] storing { nodes} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.922131       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.922357       5 reflector.go:234] Listing and watching *api.Endpoints from pkg/storage/cacher.go:212
I0119 22:51:31.922958       5 storage_factory.go:242] storing { pods} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.923109       5 reflector.go:234] Listing and watching *api.Node from pkg/storage/cacher.go:212
I0119 22:51:31.923228       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.923977       5 storage_factory.go:242] storing { services} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.924099       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.924372       5 reflector.go:234] Listing and watching *api.Pod from pkg/storage/cacher.go:212
I0119 22:51:31.925062       5 storage_factory.go:242] storing { services} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.925224       5 reflector.go:234] Listing and watching *api.Service from pkg/storage/cacher.go:212
I0119 22:51:31.925325       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.925546       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.925619       5 storage_factory.go:242] storing { replicationcontrollers} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.925848       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.926805       5 reflector.go:234] Listing and watching *api.ReplicationController from pkg/storage/cacher.go:212
I0119 22:51:31.948621       5 storage_factory.go:242] storing {extensions thirdpartyresources} in extensions/v1beta1, reading as extensions/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.948823       5 storage_factory.go:242] storing {apps statefulsets} in apps/v1beta1, reading as apps/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.949469       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.950421       5 master.go:317] Enabling API group ""apps"".
I0119 22:51:31.950500       5 master.go:317] Enabling API group ""authentication.k8s.io"".
I0119 22:51:31.950556       5 master.go:317] Enabling API group ""authorization.k8s.io"".
I0119 22:51:31.950675       5 reflector.go:234] Listing and watching *apps.StatefulSet from pkg/storage/cacher.go:212
I0119 22:51:31.950841       5 storage_factory.go:242] storing {autoscaling horizontalpodautoscalers} in autoscaling/v1, reading as autoscaling/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.951142       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.952131       5 master.go:317] Enabling API group ""autoscaling"".
I0119 22:51:31.952259       5 storage_factory.go:242] storing {batch jobs} in batch/v1, reading as batch/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.952306       5 reflector.go:234] Listing and watching *autoscaling.HorizontalPodAutoscaler from pkg/storage/cacher.go:212
I0119 22:51:31.953125       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.954074       5 master.go:317] Enabling API group ""batch"".
I0119 22:51:31.954387       5 reflector.go:234] Listing and watching *batch.Job from pkg/storage/cacher.go:212
I0119 22:51:31.954712       5 storage_factory.go:242] storing {certificates.k8s.io certificatesigningrequests} in certificates.k8s.io/v1alpha1, reading as certificates.k8s.io/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.955240       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.955869       5 master.go:317] Enabling API group ""certificates.k8s.io"".
I0119 22:51:31.956039       5 storage_factory.go:242] storing {autoscaling horizontalpodautoscalers} in autoscaling/v1, reading as autoscaling/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.956144       5 reflector.go:234] Listing and watching *certificates.CertificateSigningRequest from pkg/storage/cacher.go:212
I0119 22:51:31.956351       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.956561       5 storage_factory.go:242] storing { replicationcontrollers} in v1, reading as __internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.956885       5 reflector.go:234] Listing and watching *autoscaling.HorizontalPodAutoscaler from pkg/storage/cacher.go:212
I0119 22:51:31.956888       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.957213       5 storage_factory.go:242] storing {extensions thirdpartyresources} in extensions/v1beta1, reading as extensions/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.957388       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.957745       5 storage_factory.go:242] storing {extensions daemonsets} in extensions/v1beta1, reading as extensions/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.957988       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.958061       5 reflector.go:234] Listing and watching *api.ReplicationController from pkg/storage/cacher.go:212
I0119 22:51:31.958838       5 storage_factory.go:242] storing {extensions deployments} in extensions/v1beta1, reading as extensions/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.959038       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.959433       5 reflector.go:234] Listing and watching *extensions.DaemonSet from pkg/storage/cacher.go:212
I0119 22:51:31.972917       5 storage_factory.go:242] storing {extensions ingresses} in extensions/v1beta1, reading as extensions/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.973446       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.973607       5 reflector.go:234] Listing and watching *extensions.Deployment from pkg/storage/cacher.go:212
I0119 22:51:31.974374       5 storage_factory.go:242] storing {extensions podsecuritypolicies} in extensions/v1beta1, reading as extensions/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.974510       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.974868       5 reflector.go:234] Listing and watching *extensions.Ingress from pkg/storage/cacher.go:212
I0119 22:51:31.974944       5 storage_factory.go:242] storing {extensions replicasets} in extensions/v1beta1, reading as extensions/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.975008       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.975221       5 reflector.go:234] Listing and watching *extensions.PodSecurityPolicy from pkg/storage/cacher.go:212
I0119 22:51:31.975256       5 reflector.go:234] Listing and watching *extensions.ReplicaSet from pkg/storage/cacher.go:212
I0119 22:51:31.975235       5 storage_factory.go:242] storing {extensions networkpolicies} in extensions/v1beta1, reading as extensions/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.975547       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.975893       5 master.go:317] Enabling API group ""extensions"".
I0119 22:51:31.975970       5 storage_factory.go:242] storing {policy poddisruptionbudgets} in policy/v1beta1, reading as policy/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.976088       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.976092       5 reflector.go:234] Listing and watching *extensions.NetworkPolicy from pkg/storage/cacher.go:212
I0119 22:51:31.976615       5 master.go:317] Enabling API group ""policy"".
I0119 22:51:31.976687       5 storage_factory.go:242] storing {rbac.authorization.k8s.io roles} in rbac.authorization.k8s.io/v1alpha1, reading as rbac.authorization.k8s.io/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.976865       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.976726       5 reflector.go:234] Listing and watching *policy.PodDisruptionBudget from pkg/storage/cacher.go:212
I0119 22:51:31.977145       5 storage_factory.go:242] storing {rbac.authorization.k8s.io rolebindings} in rbac.authorization.k8s.io/v1alpha1, reading as rbac.authorization.k8s.io/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.977227       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.977398       5 reflector.go:234] Listing and watching *rbac.Role from pkg/storage/cacher.go:212
I0119 22:51:31.977580       5 storage_factory.go:242] storing {rbac.authorization.k8s.io clusterroles} in rbac.authorization.k8s.io/v1alpha1, reading as rbac.authorization.k8s.io/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.977657       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.977774       5 reflector.go:234] Listing and watching *rbac.RoleBinding from pkg/storage/cacher.go:212
I0119 22:51:31.977936       5 storage_factory.go:242] storing {rbac.authorization.k8s.io clusterrolebindings} in rbac.authorization.k8s.io/v1alpha1, reading as rbac.authorization.k8s.io/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.978022       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.978142       5 reflector.go:234] Listing and watching *rbac.ClusterRole from pkg/storage/cacher.go:212
I0119 22:51:31.978302       5 master.go:317] Enabling API group ""rbac.authorization.k8s.io"".
I0119 22:51:31.978387       5 storage_factory.go:242] storing {storage.k8s.io storageclasses} in storage.k8s.io/v1beta1, reading as storage.k8s.io/__internal from { /registry [http://127.0.0.1:2379]    false 1000 <nil>}
I0119 22:51:31.978465       5 compact.go:55] compactor already exists for endpoints [http://127.0.0.1:2379]
I0119 22:51:31.978658       5 master.go:317] Enabling API group ""storage.k8s.io"".
I0119 22:51:31.978470       5 reflector.go:234] Listing and watching *rbac.ClusterRoleBinding from pkg/storage/cacher.go:212
I0119 22:51:31.978832       5 reflector.go:234] Listing and watching *storage.StorageClass from pkg/storage/cacher.go:212
I0119 22:51:31.998545       5 factory.go:96] Starting informer factory
I0119 22:51:31.998799       5 factory.go:100] Starting informer for *api.Namespace
I0119 22:51:31.999217       5 factory.go:100] Starting informer for *api.LimitRange
I0119 22:51:32.000154       5 reflector.go:196] Starting reflector *api.Namespace (10m0s) from pkg/controller/informers/factory.go:101
I0119 22:51:32.000499       5 reflector.go:234] Listing and watching *api.Namespace from pkg/controller/informers/factory.go:101
E0119 22:51:32.002009       5 reflector.go:199] pkg/controller/informers/factory.go:101: Failed to list *api.Namespace: Get http://0.0.0.0:8080/api/v1/namespaces?resourceVersion=0: dial tcp 0.0.0.0:8080: getsockopt: connection refused
I0119 22:51:32.002602       5 reflector.go:196] Starting reflector *api.LimitRange (10m0s) from pkg/controller/informers/factory.go:101
I0119 22:51:32.002647       5 reflector.go:234] Listing and watching *api.LimitRange from pkg/controller/informers/factory.go:101
E0119 22:51:32.003877       5 reflector.go:199] pkg/controller/informers/factory.go:101: Failed to list *api.LimitRange: Get http://0.0.0.0:8080/api/v1/limitranges?resourceVersion=0: dial tcp 0.0.0.0:8080: getsockopt: connection refused
[restful] 2017/01/19 22:51:32 log.go:30: [restful/swagger] listing is available at https://172.1.30.2:443/swaggerapi/
[restful] 2017/01/19 22:51:32 log.go:30: [restful/swagger] https://172.1.30.2:443/swaggerui/ is mapped to folder /swagger-ui/
I0119 22:51:32.141138       5 serve.go:79] Serving securely on 0.0.0.0:443
I0119 22:51:32.141378       5 serve.go:94] Serving insecurely on 0.0.0.0:8080
I0119 22:51:32.895865       5 reflector.go:234] Listing and watching *api.ServiceAccount from k8s.io/kubernetes/plugin/pkg/admission/serviceaccount/admission.go:123
I0119 22:51:32.913686       5 reflector.go:234] Listing and watching *api.Secret from k8s.io/kubernetes/plugin/pkg/admission/serviceaccount/admission.go:143
I0119 22:51:32.914414       5 reflector.go:234] Listing and watching *api.ResourceQuota from k8s.io/kubernetes/plugin/pkg/admission/resourcequota/resource_access.go:88
I0119 22:51:33.002289       5 reflector.go:234] Listing and watching *api.Namespace from pkg/controller/informers/factory.go:101
I0119 22:51:33.004067       5 reflector.go:234] Listing and watching *api.LimitRange from pkg/controller/informers/factory.go:101
I0119 22:51:52.058088       5 trace.go:61] Trace ""Create /api/v1/namespaces/kube-system/pods"" (started 2017-01-19 22:51:42.04873791 +0000 UTC):
[27.242µs] [27.242µs] About to convert to expected version
[8.857244ms] [8.830002ms] Conversion done
[10.009317768s] [10.000460524s] END
I0119 22:51:52.058315       5 panics.go:75] POST /api/v1/namespaces/kube-system/pods: (10.00980261s) 403 [[hyperkube/v1.6.0 (linux/amd64) kubernetes/8fa2358] 172.1.30.1:46640]
I0119 22:52:03.308605       5 trace.go:61] Trace ""Create /api/v1/nodes"" (started 2017-01-19 22:51:33.291306953 +0000 UTC):
[1.703632ms] [1.703632ms] About to convert to expected version
[16.396782ms] [14.69315ms] Conversion done
[16.468411ms] [71.629µs] About to store object in database
[30.017202969s] [30.000734558s] END
I0119 22:52:03.309663       5 panics.go:75] POST /api/v1/nodes: (30.018576506s) 504
goroutine 571 [running]:
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/httplog.(*respLogger).recordStatus(0xc420dae5b0, 0x1f8)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/httplog/log.go:219 +0xbb
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/httplog.(*respLogger).WriteHeader(0xc420dae5b0, 0x1f8)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/httplog/log.go:198 +0x35
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/metrics.(*responseWriterDelegator).WriteHeader(0xc420f63350, 0x1f8)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/metrics/metrics.go:117 +0x45
k8s.io/kubernetes/pkg/genericapiserver/api/handlers/responsewriters.WriteObjectNegotiated(0x5ec3400, 0xc420efd440, 0x0, 0x0, 0x39e44c8, 0x2, 0x5ebc440, 0xc420deb2c0, 0xc421c5e000, 0x1f8, ...)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/api/handlers/responsewriters/writers.go:89 +0x162
k8s.io/kubernetes/pkg/genericapiserver/api/handlers/responsewriters.ErrorNegotiated(0x5e8f040, 0xc421318f00, 0x5ec3400, 0xc420efd440, 0x0, 0x0, 0x39e44c8, 0x2, 0x5ebc440, 0xc420deb2c0, ...)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/api/handlers/responsewriters/writers.go:106 +0x103
k8s.io/kubernetes/pkg/genericapiserver/api/handlers.(*RequestScope).err(0xc4209ec540, 0x5e8f040, 0xc421318f00, 0x5ebc440, 0xc420deb2c0, 0xc421c5e000)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/api/handlers/rest.go:93 +0xd7
k8s.io/kubernetes/pkg/genericapiserver/api/handlers.createHandler.func1(0xc420f63020, 0xc421c8f260)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/api/handlers/rest.go:426 +0xee2
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/metrics.InstrumentRouteFunc.func1(0xc420f63020, 0xc421c8f260)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/metrics/metrics.go:101 +0x1f2
k8s.io/kubernetes/vendor/github.com/emicklei/go-restful.(*Container).dispatch(0xc4207cf8c0, 0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/container.go:272 +0xba0
k8s.io/kubernetes/vendor/github.com/emicklei/go-restful.(*Container).(k8s.io/kubernetes/vendor/github.com/emicklei/go-restful.dispatch)-fm(0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/container.go:120 +0x48
net/http.HandlerFunc.ServeHTTP(0xc4212347c0, 0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/usr/local/go/src/net/http/server.go:1726 +0x44
net/http.(*ServeMux).ServeHTTP(0xc420d54ab0, 0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/usr/local/go/src/net/http/server.go:2022 +0x7f
k8s.io/kubernetes/pkg/genericapiserver/api/filters.WithAuthorization.func1(0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/api/filters/authorization.go:50 +0x365
net/http.HandlerFunc.ServeHTTP(0xc42013c500, 0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/usr/local/go/src/net/http/server.go:1726 +0x44
k8s.io/kubernetes/pkg/genericapiserver/api/filters.WithImpersonation.func1(0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/api/filters/impersonation.go:47 +0x1e4d
net/http.HandlerFunc.ServeHTTP(0xc42013c540, 0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/usr/local/go/src/net/http/server.go:1726 +0x44
k8s.io/kubernetes/pkg/auth/handlers.WithAuthentication.func1(0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/auth/handlers/handlers.go:73 +0x2c2
net/http.HandlerFunc.ServeHTTP(0xc42083da90, 0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/usr/local/go/src/net/http/server.go:1726 +0x44
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/request.WithRequestContext.func1(0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/request/requestcontext.go:107 +0xef
net/http.HandlerFunc.ServeHTTP(0xc420f26780, 0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/usr/local/go/src/net/http/server.go:1726 +0x44
k8s.io/kubernetes/pkg/genericapiserver/filters.WithPanicRecovery.func1(0x5ebc400, 0xc420dae5b0, 0xc421c5e000)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/filters/panics.go:74 +0x24a
net/http.HandlerFunc.ServeHTTP(0xc420d55980, 0x7f9fda741298, 0xc420deb2b0, 0xc421c5e000)
	/usr/local/go/src/net/http/server.go:1726 +0x44
k8s.io/kubernetes/pkg/genericapiserver/filters.(*timeoutHandler).ServeHTTP.func1(0xc420f26820, 0x5ec4800, 0xc420deb2b0, 0xc421c5e000, 0xc421c8f200)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/filters/timeout.go:89 +0x8d
created by k8s.io/kubernetes/pkg/genericapiserver/filters.(*timeoutHandler).ServeHTTP
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/filters/timeout.go:91 +0x1db

logging error output: ""k8s\x00\n\f\n\x02v1\x12\x06Status\x12`\n\x04\n\x00\x12\x00\x12\aFailure\x1a9Timeout: request did not complete within allowed duration\""\aTimeout*\b\n\x00\x12\x00\x1a\x00(\x000\xf8\x03\x1a\x00\""\x00""
 [[hyperkube/v1.6.0 (linux/amd64) kubernetes/8fa2358] 172.1.30.1:56340]
I0119 22:52:03.395184       5 panics.go:75] PATCH /api/v1/namespaces/default/events/master.149b4d10d6e92d77: (30.002676558s) 504
goroutine 559 [running]:
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/httplog.(*respLogger).recordStatus(0xc420dc0700, 0x1f8)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/httplog/log.go:219 +0xbb
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/httplog.(*respLogger).WriteHeader(0xc420dc0700, 0x1f8)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/httplog/log.go:198 +0x35
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/metrics.(*responseWriterDelegator).WriteHeader(0xc420ece420, 0x1f8)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/metrics/metrics.go:117 +0x45
k8s.io/kubernetes/pkg/genericapiserver/api/handlers/responsewriters.WriteObjectNegotiated(0x5ec3400, 0xc420efd440, 0x0, 0x0, 0x39e44c8, 0x2, 0x5ebc440, 0xc421dff358, 0xc421c5e1e0, 0x1f8, ...)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/api/handlers/responsewriters/writers.go:89 +0x162
k8s.io/kubernetes/pkg/genericapiserver/api/handlers/responsewriters.ErrorNegotiated(0x5e8f040, 0xc42099d400, 0x5ec3400, 0xc420efd440, 0x0, 0x0, 0x39e44c8, 0x2, 0x5ebc440, 0xc421dff358, ...)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/api/handlers/responsewriters/writers.go:106 +0x103
k8s.io/kubernetes/pkg/genericapiserver/api/handlers.(*RequestScope).err(0xc4201476c0, 0x5e8f040, 0xc42099d400, 0x5ebc440, 0xc421dff358, 0xc421c5e1e0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/api/handlers/rest.go:93 +0xd7
k8s.io/kubernetes/pkg/genericapiserver/api/handlers.PatchResource.func1(0xc420d87da0, 0xc421bb6060)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/api/handlers/rest.go:521 +0xaf3
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/metrics.InstrumentRouteFunc.func1(0xc420d87da0, 0xc421bb6060)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/metrics/metrics.go:101 +0x1f2
k8s.io/kubernetes/vendor/github.com/emicklei/go-restful.(*Container).dispatch(0xc4207cf8c0, 0x5ebc400, 0xc420dc0700, 0xc421c5e1e0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/container.go:272 +0xba0
k8s.io/kubernetes/vendor/github.com/emicklei/go-restful.(*Container).(k8s.io/kubernetes/vendor/github.com/emicklei/go-restful.dispatch)-fm(0x5ebc400, 0xc420dc0700, 0xc421c5e1e0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/container.go:120 +0x48
net/http.HandlerFunc.ServeHTTP(0xc4212347c0, 0x5ebc400, 0xc420dc0700, 0xc421c5e1e0)
	/usr/local/go/src/net/http/server.go:1726 +0x44
net/http.(*ServeMux).ServeHTTP(0xc420d54ab0, 0x5ebc400, 0xc420dc0700, 0xc421c5e1e0)
	/usr/local/go/src/net/http/server.go:2022 +0x7f
k8s.io/kubernetes/pkg/genericapiserver/filters.WithPanicRecovery.func1(0x5ebc400, 0xc420dc0700, 0xc421c5e1e0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/filters/panics.go:74 +0x24a
net/http.HandlerFunc.ServeHTTP(0xc420d55a40, 0x7f9fda73fb90, 0xc421dff348, 0xc421c5e1e0)
	/usr/local/go/src/net/http/server.go:1726 +0x44
k8s.io/kubernetes/pkg/genericapiserver/filters.(*timeoutHandler).ServeHTTP.func1(0xc420f26940, 0x5ec47c0, 0xc421dff348, 0xc421c5e1e0, 0xc421ebdf20)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/filters/timeout.go:89 +0x8d
created by k8s.io/kubernetes/pkg/genericapiserver/filters.(*timeoutHandler).ServeHTTP
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/genericapiserver/filters/timeout.go:91 +0x1db
```

I have tried the following steps to deploy the kubernetes cluster on master builds. 
1. Deploy kubernetes cluster using kube-anywhere on v1.5.1 kubernetes with vsphere cloud provider configured.
2. I see all the services are up and running perfectly fine.
3. Replace the gcr.io/google-containers/hyperkube-amd64:v1.5.1 image with my custom hyperkube image built using kubernetes master code base.
4. Update the manifests files in /etc/kubernetes/manifests to use the new hyperkube image based on master build.
5. Also update the kubelet.service to use the new hyperkube image based on master build.
6. Restart all the services.
7. After this, I see the apiserver container is started but goes to an unhealthy state (fails to start succesfully). 

The following command fails as API server fails to start successfully.
-curl http://localhost:8080/api/v1/nodes
-curl http://localhost:8080/api/v1/namespaces/kube-system/pods/

I even tried removing the cloud provider configuration from all manifests files and kubelet service. Now, it is running with the kubernetes core only without any cloud provider configured.  I see the same errors again. 

So, I feel this basically has to do something with the kubernetes core which fails to start the api server and is independent of the cloud provider.

@mikedanese : Can you please take a look at this.
**P.S**: kube-anywhere works perfectly fine on v1.5.1 and v1.5.2 (which was released 8 days ago).
@kerneltime  ",closed,False,2017-01-20 01:53:41,2018-02-19 07:04:01
kubernetes-anywhere,jcbsmpsn,https://github.com/kubernetes/kubernetes-anywhere/pull/318,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/318,Update DEVEL doc to include procedures for .deb packaged components.,,closed,True,2017-01-23 17:04:09,2017-01-24 22:05:08
kubernetes-anywhere,2vcps,https://github.com/kubernetes/kubernetes-anywhere/issues/319,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/319,Kubernetes Anywhere - PhotonOS Template Issue,"I was using the photonOS OVA linked in the readme for vSphere and Kubernetes Anywhere. The deployment was seemingly fine except every VM would get the same IP. This makes networking really hard. 
I had to boot up the Template VM and run this command from the Photon OS Administration guide.

#echo -n > /etc/machine-id

[](https://github.com/vmware/photon/blob/master/docs/photon-admin-guide.md#clearing-the-machine-id-of-a-cloned-instance-for-dhcp)

This make sure each VM has a unique MAC and DHCP works as expected. 
",closed,False,2017-01-23 20:23:03,2017-01-27 16:09:29
kubernetes-anywhere,jcbsmpsn,https://github.com/kubernetes/kubernetes-anywhere/pull/320,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/320,Use lowercase hex token for kubeadm commands.,"The kubeadm command was updated to do input validation on
the token used for initializing and joining clusters. This update
generates lowercase tokens to match the kubeadm input rules.",closed,True,2017-01-23 23:23:16,2017-01-24 22:04:48
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/pull/321,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/321,Add note for why not to power on template vm,fixes https://github.com/kubernetes/kubernetes-anywhere/issues/319,closed,True,2017-01-25 21:03:31,2017-01-27 16:09:29
kubernetes-anywhere,stefanhdao,https://github.com/kubernetes/kubernetes-anywhere/issues/322,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/322,Changes to kube-apiserver.jsonnet don't show up on the generated json file on the master node,I've modified `kubernetes-anywhere/phase2/ignition/vanilla/manifest/kube-apiserver.jsonnet` and added the `--basic-auth-file` flag but the file `/etc/kubernetes/manifests/kube-apiserver.json` on the master node doesn't reflect my changes. I'm running `make destroy` and `make clean` before I do `make deploy` after my changes.,closed,False,2017-01-26 16:53:53,2018-01-15 10:52:22
kubernetes-anywhere,jcbsmpsn,https://github.com/kubernetes/kubernetes-anywhere/pull/323,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/323,Additional documentation for configuring networking.,"Add GCE developer documentation to configure an overlay network and DNS
related pods.",closed,True,2017-01-30 21:46:12,2017-01-31 04:13:37
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/issues/324,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/324,vSphere: VMs land up having the same hostname as reported by kubernetes.,Nodes cloned land up having the same hostname as reported to kubernetes.,closed,False,2017-02-02 06:26:04,2017-02-14 20:35:34
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/issues/325,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/325,Kubernetes-Anywhere fails to launch kubernetes cluster on vSphere,"This occurs because password is expired. Following is an error message while launching cluster,

```
data.tls_cert_request.kubetest1-master: Refreshing state...
tls_locally_signed_cert.kubetest1-master: Creating...
  allowed_uses.#:        """" => ""3""
  allowed_uses.0:        """" => ""digital_signature""
  allowed_uses.1:        """" => ""server_auth""
  allowed_uses.2:        """" => ""client_auth""
  ca_cert_pem:           """" => ""65de0fab25f2256ce3a77949df6da1a80aa8bb93""
  ca_key_algorithm:      """" => ""RSA""
  ca_private_key_pem:    """" => ""f1595e396fec1e9dcc0fda4af4ba5157c4df8d5a""
  cert_pem:              """" => ""<computed>""
  cert_request_pem:      """" => ""7b119eb765ea1d50ebd776b14249d40f63443987""
  early_renewal_hours:   """" => ""0""
  validity_end_time:     """" => ""<computed>""
  validity_period_hours: """" => ""8760""
  validity_start_time:   """" => ""<computed>""
tls_locally_signed_cert.kubetest1-master: Creation complete
vsphere_virtual_machine.kubevm2: Still creating... (1m40s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (1m50s elapsed)
vsphere_virtual_machine.kubevm2: Creation complete
data.template_file.configure_master: Refreshing state...
data.template_file.configure_node: Refreshing state...
null_resource.master: Creating...
null_resource.node2: Creating...
null_resource.master: Provisioning with 'remote-exec'...
null_resource.node2: Provisioning with 'remote-exec'...
null_resource.master (remote-exec): Connecting to remote host via SSH...
null_resource.master (remote-exec):   Host: 10.162.45.8
null_resource.master (remote-exec):   User: root
null_resource.master (remote-exec):   Password: true
null_resource.master (remote-exec):   Private key: false
null_resource.master (remote-exec):   SSH Agent: false
null_resource.node2 (remote-exec): Connecting to remote host via SSH...
null_resource.node2 (remote-exec):   Host: 10.162.36.221
null_resource.node2 (remote-exec):   User: root
null_resource.node2 (remote-exec):   Password: true
null_resource.node2 (remote-exec):   Private key: false
null_resource.node2 (remote-exec):   SSH Agent: false
null_resource.master (remote-exec): Connected!
null_resource.node2 (remote-exec): Connected!
null_resource.master: Still creating... (10s elapsed)
null_resource.node2: Still creating... (10s elapsed)
null_resource.node2: Still creating... (20s elapsed)
null_resource.master: Still creating... (20s elapsed)
null_resource.master: Still creating... (30s elapsed)
null_resource.node2: Still creating... (30s elapsed)
null_resource.node2: Still creating... (40s elapsed)
null_resource.master: Still creating... (40s elapsed)
null_resource.master: Still creating... (50s elapsed)
null_resource.node2: Still creating... (50s elapsed)
null_resource.node2: Still creating... (1m0s elapsed)
null_resource.master: Still creating... (1m0s elapsed)
null_resource.master: Still creating... (1m10s elapsed)
null_resource.node2: Still creating... (1m10s elapsed)
null_resource.node2: Still creating... (1m20s elapsed)
null_resource.master: Still creating... (1m20s elapsed)
null_resource.node2: Still creating... (1m30s elapsed)
null_resource.master: Still creating... (1m30s elapsed)
null_resource.master: Still creating... (1m40s elapsed)
null_resource.node2: Still creating... (1m40s elapsed)
null_resource.node2: Still creating... (1m50s elapsed)
null_resource.master: Still creating... (1m50s elapsed)
null_resource.node2: Still creating... (2m0s elapsed)
null_resource.master: Still creating... (2m0s elapsed)
null_resource.master: Still creating... (2m10s elapsed)
null_resource.node2: Still creating... (2m10s elapsed)
null_resource.node2: Still creating... (2m20s elapsed)
null_resource.master: Still creating... (2m20s elapsed)
null_resource.master: Still creating... (2m30s elapsed)
null_resource.node2: Still creating... (2m30s elapsed)
null_resource.master: Still creating... (2m40s elapsed)
null_resource.node2: Still creating... (2m40s elapsed)
null_resource.master: Still creating... (2m50s elapsed)
null_resource.node2: Still creating... (2m50s elapsed)
null_resource.node2: Still creating... (3m0s elapsed)
null_resource.master: Still creating... (3m0s elapsed)
null_resource.node2: Still creating... (3m10s elapsed)
null_resource.master: Still creating... (3m10s elapsed)
null_resource.master: Still creating... (3m20s elapsed)
null_resource.node2: Still creating... (3m20s elapsed)
null_resource.node2: Still creating... (3m30s elapsed)
null_resource.master: Still creating... (3m30s elapsed)
null_resource.node2: Still creating... (3m40s elapsed)
null_resource.master: Still creating... (3m40s elapsed)
null_resource.master: Still creating... (3m50s elapsed)
null_resource.node2: Still creating... (3m50s elapsed)
null_resource.node2: Still creating... (4m0s elapsed)
null_resource.master: Still creating... (4m0s elapsed)
null_resource.master: Still creating... (4m10s elapsed)
null_resource.node2: Still creating... (4m10s elapsed)
null_resource.node2: Still creating... (4m20s elapsed)
null_resource.master: Still creating... (4m20s elapsed)
null_resource.node2: Still creating... (4m30s elapsed)
null_resource.master: Still creating... (4m30s elapsed)
null_resource.node2: Still creating... (4m40s elapsed)
null_resource.master: Still creating... (4m40s elapsed)
null_resource.master: Still creating... (4m50s elapsed)
null_resource.node2: Still creating... (4m50s elapsed)
null_resource.node2: Still creating... (5m0s elapsed)
null_resource.master: Still creating... (5m0s elapsed)
Error applying plan:

2 error(s) occurred:

* Failed to upload script: Error reading script: EOF
* Failed to upload script: Error reading script: EOF

Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.
Makefile:63: recipe for target 'do' failed

```
",closed,False,2017-02-07 19:16:54,2017-02-07 20:47:21
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/326,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/326,Add default storage backend as etcd3,"This PR adds parameter default storage backend to apiserver as etcd2. While launching k8s-cluster with kubernetes master branch (commit d82e51e) using kubernetes-anywhere. I found that kubernetes api-server fails with an error

```
E0111 03:00:53.829193   29597 status.go:62] apiserver received an error that is not an metav1.Status: rpc error: code = 13 desc = transport is closing
…
logging error output: ""k8s\x00\n\f\n\x02v1\x12\x06Status\x12F\n\x04\n\x00\x12\x00\x12\aFailure\x1a0rpc error: code = 13 desc = transport is closing\""\x000\xf4\x03\x1a\x00\""\x00""
```
Currently, etcd3 is made as default storage backend for api-server and kubernetes-anywhere uses etcd2. Hence, this error occurs. This can be resolved by adding parameter ```storage-backend=etcd2``` to apiserver manifest.

Refer this: https://github.com/kubernetes/kubernetes/issues/39710",closed,True,2017-02-07 23:58:54,2017-02-22 23:08:33
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/issues/327,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/327,Etcd starts after kubernetes apiserver starts,"While launching k8s-cluster with kubernetes master branch (commit d82e51e) using kubernetes-anywhere. I found that kubernetes api-server starts before etcd server starts. As a result, apiserver fails. ",closed,False,2017-02-08 01:16:00,2018-01-15 10:52:28
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/328,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/328,Move kubectl download to phase2 for vSphere CloudProvider,"For vSphere cloud provider kubectl was downloaded in phase1 since downloading in phase2 using ignition gave an [error](https://github.com/kubernetes/kubernetes-anywhere/issues/286). Now, that is [fixed](https://github.com/kubernetes/kubernetes-anywhere/pull/305) this PR removes the downloading of kubectl from phase1 of vSphere Cloud Provider. Also, kubectl downloaded in phase2 was of v1.3.4 even though kubernetes cluster launched was of different version. This PR fixes that.

@mikedanese Can you please build the static binary and update at https://storage.googleapis.com/public-mikedanese-k8s/k8s/ignition? Also, I tested with master branch of ignition it fails therefore can you build binary for commit 6ff90ec7f985d783246fc429ea.",closed,True,2017-02-08 21:12:50,2017-02-27 20:53:16
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/issues/329,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/329,vSphere: Add UUID for vSphere deployment,Once https://github.com/kubernetes/kubernetes/pull/41217 merged we need to add support for UUID to the config file.,closed,False,2017-02-10 22:16:50,2017-03-23 23:50:21
kubernetes-anywhere,ilchebedelovski,https://github.com/kubernetes/kubernetes-anywhere/issues/330,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/330,"make deploy fails with: ""Failed to upload script: Error reading script: EOF""","I am trying to deploy kubernetes-anywhere v.1.4.8 on vSphere where vCenter is configured.

After filling the .config.json file I start the deployment with `make deploy`, the virtual machines are created on the ESXi host but the Kubernetes is not successfully deployed.

The following error is displayed:

`Failed to upload script: Error reading script: EOF`

`Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.
`
",closed,False,2017-02-12 21:18:31,2017-03-22 22:49:25
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/331,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/331,Make output match reality.,"This loop was trying 60 times, but giving the indication it was only going to try 10.",closed,True,2017-02-14 16:57:17,2017-02-27 20:53:43
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/issues/332,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/332,Honor $GOOGLE_APPLICATION_CREDENTIALS if set,"If using GCE, kubernetes-anywhere requires a file be placed in `phase1/gce/account.json` with the user's credentials in order to bring up a cluster. Other GCE tools/libraries support setting the environment variable `GOOGLE_APPLICATION_CREDENTIALS` to point to the file where your default credentials are stored, so it would be nice if kubernetes-anywhere could check if this was set and use it if `account.json` is missing.

For reference: https://developers.google.com/identity/protocols/application-default-credentials#howtheywork",closed,False,2017-02-14 17:15:11,2018-09-20 22:13:48
kubernetes-anywhere,cdornsife,https://github.com/kubernetes/kubernetes-anywhere/pull/333,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/333,Pip kconfiglib,installing kconfiglib with pip now.,closed,True,2017-02-14 17:58:52,2017-02-14 18:04:21
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/issues/334,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/334,vSphere: Add support for networking configuration ,Currently when deploying the cloned VMs there is no way to change the configuration for the networking setup that can lead to failed installs.,closed,False,2017-02-14 20:49:43,2017-02-27 21:12:37
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/335,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/335,Writing vsphere conf before launching ignition container,"Kubernetes-anywhere scripts were writing vsphere.conf after launching of ignition container which created race condition between vSphere.conf and apiserver. This PR fixes that.

Fixes this error,
```
Feb 14 15:05:16 photon-zv1KbtvMG docker[626]: F0214 15:05:16.854753     709 plugins.go:80] Couldn't open cloud provider configuration /etc/kubernetes/vsphere.conf: &os.PathError{
Op:""open"", Path:""/etc/kubernetes/vsphere.conf"", Err:0x2}
```",closed,True,2017-02-14 21:43:25,2017-02-14 22:56:48
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/336,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/336,Updated documentation to recommend kubernetes 1.5.3 for vSphere Cloud Provider,"Updated documentation and `phase2/Kconfig `to recommend kubernetes 1.5.3 for vSphere Cloud Provider

cc: @kerneltime @abrarshivani @BaluDontu @tusharnt @pdhamdhere",closed,True,2017-02-17 23:22:39,2017-02-22 22:42:42
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/issues/337,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/337,can not execute command in a container using kubectl exec,"**Deployed K8S 1.5.3 cluster using kubernetes-anywhere latest code**

```
$ kubectl cluster-info
Kubernetes master is running at https://10.192.36.202
Heapster is running at https://10.192.36.202/api/v1/proxy/namespaces/kube-system/services/heapster
KubeDNS is running at https://10.192.36.202/api/v1/proxy/namespaces/kube-system/services/kube-dns

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

```

$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.3"", GitCommit:""029c3a408176b55c30846f0faedf56aae5992e9b"", GitTreeState:""clean"", BuildDate:""2017-02-15T06:40:50Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.3"", GitCommit:""029c3a408176b55c30846f0faedf56aae5992e9b"", GitTreeState:""clean"", BuildDate:""2017-02-15T06:34:56Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""} 
```

**Created pod using following YAML**

```
$ cat injectpod.yaml 
apiVersion: v1
kind: Pod
metadata:
    name: inject-pod
spec:
    containers:
    - name: test-container
      image: gcr.io/google_containers/busybox:1.24
      command: [""/bin/sh"", ""-c"", ""echo 'hello' > /mnt/volume1/index.html  && chmod o+rX /mnt /mnt/volume1/index.html && while true ; do sleep 2 ; done""]
      volumeMounts:
      - name: test-volume
        mountPath: /mnt/volume1
    securityContext:
      seLinuxOptions:
        level: ""s0:c0,c1""
    restartPolicy: Never
    volumes:
    - name: test-volume
      vsphereVolume:
          volumePath: ""[vsanDatastore] kubevols/my-vmdk.vmdk""
          fsType: ext4
$ 
```

```
$ kubectl create -f injectpod.yaml
pod ""inject-pod"" created
$
```

```
$ kubectl get pods
NAME         READY     STATUS    RESTARTS   AGE
inject-pod   1/1       Running   0          22s
```

```
$ kubectl describe pods inject-pod
Name:		inject-pod
Namespace:	default
Node:		node1/10.192.40.58
Start Time:	Fri, 17 Feb 2017 17:08:50 -0800
Labels:		<none>
Status:		Running
IP:		172.1.53.3
Controllers:	<none>
Containers:
  test-container:
    Container ID:	docker://b0616f1347dc5b02b4fdb81d52b7c75be3a77393ab5f0b1f9c6a75ec85f19232
    Image:		gcr.io/google_containers/busybox:1.24
    Image ID:		docker://sha256:0cb40641836c461bc97c793971d84d758371ed682042457523e4ae701efe7ec9
    Port:		
    Command:
      /bin/sh
      -c
      echo 'hello' > /mnt/volume1/index.html  && chmod o+rX /mnt /mnt/volume1/index.html && while true ; do sleep 2 ; done
    State:		Running
      Started:		Fri, 17 Feb 2017 17:09:02 -0800
    Ready:		True
    Restart Count:	0
    Volume Mounts:
      /mnt/volume1 from test-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mcqgg (ro)
    Environment Variables:	<none>
Conditions:
  Type		Status
  Initialized 	True 
  Ready 	True 
  PodScheduled 	True 
Volumes:
  test-volume:
    Type:	vSphereVolume (a Persistent Disk resource in vSphere)
    VolumePath:	[vsanDatastore] kubevols/my-vmdk.vmdk
    FSType:	ext4
  default-token-mcqgg:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-mcqgg
QoS Class:	BestEffort
Tolerations:	<none>
Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath			Type		Reason		Message
  ---------	--------	-----	----			-------------			--------	------		-------
  5m		5m		1	{default-scheduler }					Normal		Scheduled	Successfully assigned inject-pod to node1
  5m		5m		1	{kubelet node1}		spec.containers{test-container}	Normal		Pulling		pulling image ""gcr.io/google_containers/busybox:1.24""
  5m		5m		1	{kubelet node1}		spec.containers{test-container}	Normal		Pulled		Successfully pulled image ""gcr.io/google_containers/busybox:1.24""
  5m		5m		1	{kubelet node1}		spec.containers{test-container}	Normal		Created		Created container with docker id b0616f1347dc; Security:[seccomp=unconfined]
  5m		5m		1	{kubelet node1}		spec.containers{test-container}	Normal		Started		Started container with docker id b0616f1347dc
$
```

**When tried to execute command on the container of inject-pod, kubectl exec failed with error: unable to upgrade connection: pod does not exist**


> $ kubectl exec -ti inject-pod -- bin/sh
> error: unable to upgrade connection: pod does not exist


> 
> $ kubectl --server=https://10.192.36.202 --kubeconfig=/Users/divyenp/kanywhere1.5.3/kubernetes-anywhere/phase1/vsphere/.tmp/kubeconfig.json exec inject-pod -- /bin/ls -ld /mnt/volume1
> error: unable to upgrade connection: pod does not exist


**When tried to execute command using docker exec it worked.**
```
root@photon-zv1KbtvMG [ ~ ]# docker exec -it b0616f1347dc bin/sh
/ # ls -ld /mnt/volume1
drwxr-xr-x    3 root     root          4096 Feb 18 01:09 /mnt/volume1
/ # 
/ # 
/ # 
/ # cat /mnt/volume1/index.html 
hello
/ # 
```

verified --enable-debugging-handlers is set for kubelet in master and all workder node.
**kubectl config from worker node**

```
# ps auxw | grep kubelet
root       616  0.0  0.6 143828 27880 ?        Ssl  00:44   0:00 /usr/bin/docker run --net=host --pid=host --privileged -v /dev:/dev -v /sys:/sys:ro -v /var/run:/var/run:rw -v /var/lib/docker/:/var/lib/docker:rw -v /var/lib/kubelet/:/var/lib/kubelet:shared -v /var/log:/var/log:shared -v /srv/kubernetes:/srv/kubernetes:ro -v /etc/kubernetes:/etc/kubernetes:ro gcr.io/google-containers/hyperkube-amd64:v1.5.3 /hyperkube kubelet --address=0.0.0.0 --allow-privileged=true --cloud-provider=vsphere --enable-server --enable-debugging-handlers --kubeconfig=/srv/kubernetes/kubeconfig.json --config=/etc/kubernetes/manifests --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --v=2 --api-servers=https://10.192.36.202 --hairpin-mode=promiscuous-bridge --cloud-config=/etc/kubernetes/vsphere.conf
root       738  1.6  2.2 372656 92260 ?        Ssl  00:48   0:42 /hyperkube kubelet --address=0.0.0.0 --allow-privileged=true --cloud-provider=vsphere --enable-server --enable-debugging-handlers --kubeconfig=/srv/kubernetes/kubeconfig.json --config=/etc/kubernetes/manifests --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --v=2 --api-servers=https://10.192.36.202 --hairpin-mode=promiscuous-bridge --cloud-config=/etc/kubernetes/vsphere.conf
root     28476  0.0  0.0   6484   876 pts/0    S+   01:30   0:00 grep --color=auto kubelet
```

**kubectl config from master node**
```
# ps auxw | grep kubelet
root       656  0.0  0.6  87544 27760 ?        Ssl  00:45   0:00 /usr/bin/docker run --net=host --pid=host --privileged -v /dev:/dev -v /sys:/sys:ro -v /var/run:/var/run:rw -v /var/lib/docker/:/var/lib/docker:rw -v /var/lib/kubelet/:/var/lib/kubelet:shared -v /var/log:/var/log:shared -v /srv/kubernetes:/srv/kubernetes:ro -v /etc/kubernetes:/etc/kubernetes:ro gcr.io/google-containers/hyperkube-amd64:v1.5.3 /hyperkube kubelet --address=0.0.0.0 --allow-privileged=true --cloud-provider=vsphere --enable-server --enable-debugging-handlers --kubeconfig=/srv/kubernetes/kubeconfig.json --config=/etc/kubernetes/manifests --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --v=2 --api-servers=http://localhost:8080 --register-schedulable=false --cloud-config=/etc/kubernetes/vsphere.conf
root       770  2.0  2.2 511924 91396 ?        Ssl  00:48   0:53 /hyperkube kubelet --address=0.0.0.0 --allow-privileged=true --cloud-provider=vsphere --enable-server --enable-debugging-handlers --kubeconfig=/srv/kubernetes/kubeconfig.json --config=/etc/kubernetes/manifests --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --v=2 --api-servers=http://localhost:8080 --register-schedulable=false --cloud-config=/etc/kubernetes/vsphere.conf
root     29751  0.0  0.0   6484   888 pts/0    S+   01:31   0:00 grep --color=auto kubelet
```

**Expectation**
After deployment user should be able to execute command in the pod container using `kubectl exec` 

cc: @kerneltime @abrarshivani @BaluDontu @tusharnt @pdhamdhere",closed,False,2017-02-18 01:38:34,2017-03-01 00:17:00
kubernetes-anywhere,UltimateDogg,https://github.com/kubernetes/kubernetes-anywhere/issues/338,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/338,vsphere provider kubelet fails to start on master node,"i started a new cluster following this guide [here](http://cormachogan.com/2017/02/16/kubernetes-vsphere-kubernetes-anywhere/) or [here](https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase1/vsphere/README.md)

and on step 6 
```
Validation: Expected 5 healthy nodes; found 0. (20s elapsed)
....
Validation: Expected 5 healthy nodes; found 0. (393660s elapsed)
```
never finishes even overnight.


here is my config:

```
# Automatically generated file; DO NOT EDIT.
# Kubernetes Minimal Turnup Configuration
#

#
# Phase 1: Cluster Resource Provisioning
#
.phase1.num_nodes=4
.phase1.cluster_name=""kubernetes""
.phase1.cloud_provider=""vsphere""

#
# vSphere configuration
#
.phase1.vSphere.url=""vcenter.lan.local""
.phase1.vSphere.port=""9443""
.phase1.vSphere.username=""Administrator@vsphere.local""
.phase1.vSphere.password=""password""
.phase1.vSphere.insecure=""true""
.phase1.vSphere.datacenter=""datacenter""
.phase1.vSphere.datastore=""datacluster/ISCSI-HDD""
.phase1.vSphere.resourcepool=""/datacenter/host/datacluster/Resources""
.phase1.vSphere.vcpu=""2""
.phase1.vSphere.memory=""4096""
.phase1.vSphere.template=""KubernetesAnywhereTemplatePhotonOS.ova""
.phase1.vSphere.flannel_net=""172.1.0.0/16""

#
# Phase 2: Node Bootstrapping
#
.phase2.installer_container=""docker.io/ashivani/k8s-ignition:v4""
.phase2.docker_registry=""gcr.io/google-containers""
.phase2.kubernetes_version=""v1.4.8""
.phase2.provider=""ignition""

#
# Phase 3: Deploying Addons
#
.phase3.run_addons=y
.phase3.kube_proxy=y
.phase3.dashboard=y
.phase3.heapster=y
.phase3.kube_dns=y
```
i also tried .phase2.installer_container=""docker.io/ashivani/k8s-ignition:v151""

so logged into the master node and did 
```
journalctl -u kubelet
```
the output is
```
Feb 19 05:06:43 photon-zv1KbtvMG systemd[1]: Starting Kubernetes Kubelet Server...
Feb 19 05:06:43 photon-zv1KbtvMG systemd[1]: Started Kubernetes Kubelet Server.
Feb 19 05:06:44 photon-zv1KbtvMG docker[401]: Flag --config has been deprecated, Use --pod-manifest-path instead. Will be removed in a future version.
Feb 19 05:06:44 photon-zv1KbtvMG docker[401]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.
Feb 19 05:06:44 photon-zv1KbtvMG docker[401]: Error: failed to run Kubelet: could not init cloud provider ""vsphere"": 404 Not Found
Feb 19 05:06:44 photon-zv1KbtvMG docker[401]: failed to run Kubelet: could not init cloud provider ""vsphere"": 404 Not Found
Feb 19 05:06:44 photon-zv1KbtvMG systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Feb 19 05:06:44 photon-zv1KbtvMG systemd[1]: kubelet.service: Unit entered failed state.
Feb 19 05:06:44 photon-zv1KbtvMG systemd[1]: kubelet.service: Failed with result 'exit-code'.
Feb 19 05:06:54 photon-zv1KbtvMG systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Feb 19 05:06:54 photon-zv1KbtvMG systemd[1]: Stopped Kubernetes Kubelet Server.
Feb 19 05:06:54 photon-zv1KbtvMG systemd[1]: Starting Kubernetes Kubelet Server...
Feb 19 05:06:54 photon-zv1KbtvMG systemd[1]: Started Kubernetes Kubelet Server.
Feb 19 05:06:54 photon-zv1KbtvMG docker[488]: Flag --config has been deprecated, Use --pod-manifest-path instead. Will be removed in a future version.
Feb 19 05:06:54 photon-zv1KbtvMG docker[488]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.
Feb 19 05:06:54 photon-zv1KbtvMG docker[488]: Error: failed to run Kubelet: could not init cloud provider ""vsphere"": 404 Not Found
Feb 19 05:06:54 photon-zv1KbtvMG docker[488]: failed to run Kubelet: could not init cloud provider ""vsphere"": 404 Not Found
Feb 19 05:06:54 photon-zv1KbtvMG systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Feb 19 05:06:54 photon-zv1KbtvMG systemd[1]: kubelet.service: Unit entered failed state.
Feb 19 05:06:54 photon-zv1KbtvMG systemd[1]: kubelet.service: Failed with result 'exit-code'.
Feb 19 05:06:59 photon-zv1KbtvMG systemd[1]: Stopped Kubernetes Kubelet Server.
```
i get similar output on the nodes too only with kubectl

i gathered it is from [here](https://github.com/kubernetes/kubernetes/blob/ae1a7784af95f13218a94068ae451f19897562ed/pkg/cloudprovider/plugins.go#L115) but i don't really know enough to debug further. Is there a way i can peel the onion and get more information about the ""404 Not Found""? I verified the master has internet access and connectivity to all nodes.",closed,False,2017-02-19 05:49:25,2017-07-26 14:05:28
kubernetes-anywhere,ianberinger,https://github.com/kubernetes/kubernetes-anywhere/pull/339,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/339,vSphere: Make VM network user configurable,this fixes #334,closed,True,2017-02-21 21:22:39,2017-02-22 22:17:30
kubernetes-anywhere,kcl11,https://github.com/kubernetes/kubernetes-anywhere/issues/340,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/340,kubernetes-anywhere vs kubeadm vs kops,"This isn't an issue per se, but unsure where else to ask.

First off, just want to say I'm a big fan of what kubernetes-anywhere is trying to do. I came across this post (http://blog.kubernetes.io/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters.html) and was wondering if there is any plan to collaborate with the on-going effort from kubeadm and kops projects.",closed,False,2017-02-22 09:48:44,2017-02-28 17:00:39
kubernetes-anywhere,BaluDontu,https://github.com/kubernetes/kubernetes-anywhere/pull/341,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/341,Download right version of kubectl binary,"Currently Kube-anywhere installs 1.4.0 kubectl binary inside the container spawned by using ""make docker-dev"" for any kubernetes cluster deployments either it be 1.4.7, 1.4.8, 1.5.2 or 1.5.3.

So there needs to be a better way to download the specific version of kubectl based on the Kubernetes cluster the user wants to deploy. For example, if the user deployed v1.5.3 kubernetes cluster, the container needs to have a kubectl v1.5.3 binary working in place. If the user deployed v1.4.8 kubernetes cluster, the container needs to have a kubectl v1.4.8 binary working in place..

Also, there needs to be a way where the kubectl binary for a specific version can be used if it is already downloaded rather than downloading it again every time.

The fix for this PR will handle the above mentioned 2 cases.

@mikedanese @abrarshivani @pdhamdhere @kerneltime ",closed,True,2017-02-22 21:41:10,2017-02-27 22:05:08
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/342,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/342,vSphere Cloud Provider: Make hostname of VM same as nodename,"Fixes #337. 

cc @pdhamdhere @tusharnt @kerneltime @BaluDontu @divyenpatel",closed,True,2017-02-23 03:13:09,2017-03-01 00:17:00
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/issues/343,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/343,vSphere: Dashboard installed is v1.4.1 when installing 1.5.3,"I followed the steps to install 1.5.3 the dashboard installed is for version 1.4.1.
This impacts being able to use updated features such as stateful sets.

```
menu
Deployments keyboard_arrow_right
kubernetes-dashboard
 edit EDIT
delete DELETE
add CREATE
Admin
Namespaces
Nodes
Persistent Volumes
Namespace
All namespaces
Workloads
Deployments
Replica Sets
Replication Controllers
Daemon Sets
Pet Sets
Jobs
Pods
Services and discovery
Services
Ingresses
Storage
Persistent Volume Claims
Config
Secrets
Config Maps
Details
Name:
kubernetes-dashboard
Namespace:
kube-system
Labels:
app: kubernetes-dashboard
version: v1.4.1
Label selector:
app: kubernetes-dashboard
Strategy:
RollingUpdate
Min ready seconds:
0
Revision history limit:
Not set
Rolling update strategy:
Max surge: 1, Max unavailable: 1
Status:
1 updated, 1 total, 1 available, 0 unavailable
New Replica Set
Name
Namespace
Labels
Pods
Age
Images
check_circle
kubernetes-dashboard-1763797262
kube-system
app: kubernetes-dashboard
pod-template-hash: 1763797262
1 / 1
16 minutes
gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.1
 
more_vert
Old Replica Sets
There is nothing to display here
There are currently no old Replication Controllers on this Deployment
Events
There is nothing to display here
It is possible that all events have expired.
Edit a Deployment


			
	
	Deployment		{5}
		
kind	:	Deployment
		
apiVersion	:	extensions/v1beta1
		
	metadata		{9}
		
name	:	kubernetes-dashboard
		
namespace	:	kube-system
		
selfLink	:	/apis/extensions/v1beta1/namespaces/kube-system/deployments/kubernetes-dashboard
		
uid	:	6456b3a6-fa02-11e6-9330-005056a961fd
		
resourceVersion	:	320
		
generation	:	1
		
creationTimestamp	:	2017-02-23T19:58:06Z
		
	labels		{2}
		
app	:	kubernetes-dashboard
		
version	:	v1.4.1
		
	annotations		{2}
		
deployment.kubernetes.io/revision	:	1
		
kubectl.kubernetes.io/last-applied-configuration	:	{\""kind\"":\""Deployment\"",\""apiVersion\"":\""extensions/v1beta1\"",\""metadata\"":{\""name\"":\""kubernetes-dashboard\"",\""namespace\"":\""kube-system\"",\""creationTimestamp\"":null,\""labels\"":{\""app\"":\""kubernetes-dashboard\"",\""version\"":\""v1.4.1\""}},\""spec\"":{\""replicas\"":1,\""selector\"":{\""matchLabels\"":{\""app\"":\""kubernetes-dashboard\""}},\""template\"":{\""metadata\"":{\""creationTimestamp\"":null,\""labels\"":{\""app\"":\""kubernetes-dashboard\""}},\""spec\"":{\""containers\"":[{\""name\"":\""kubernetes-dashboard\"",\""image\"":\""gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.1\"",\""ports\"":[{\""containerPort\"":9090,\""protocol\"":\""TCP\""}],\""resources\"":{},\""livenessProbe\"":{\""httpGet\"":{\""path\"":\""/\"",\""port\"":9090},\""initialDelaySeconds\"":30,\""timeoutSeconds\"":30},\""imagePullPolicy\"":\""Always\""}]}},\""strategy\"":{}},\""status\"":{}}
		
	spec		{4}
		
replicas	:	1
		
	selector		{1}
		
	matchLabels		{1}
		
app	:	kubernetes-dashboard
		
	template		{2}
		
	metadata		{2}
		
creationTimestamp	:	null
		
	labels		{1}
		
app	:	kubernetes-dashboard
		
	spec		{5}
		
	containers		[1]
		
	0		{7}
		
name	:	kubernetes-dashboard
		
image	:	gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.1
		
	ports		[1]
		
	0		{2}
		
containerPort	:	9090
		
protocol	:	TCP
		
	resources		{0}
	(empty object)
		
	livenessProbe		{6}
		
	httpGet		{3}
		
path	:	/
		
port	:	9090
		
scheme	:	HTTP
		
initialDelaySeconds	:	30
		
timeoutSeconds	:	30
		
periodSeconds	:	10
		
successThreshold	:	1
		
failureThreshold	:	3
		
terminationMessagePath	:	/dev/termination-log
		
imagePullPolicy	:	Always
		
restartPolicy	:	Always
		
terminationGracePeriodSeconds	:	30
		
dnsPolicy	:	ClusterFirst
		
	securityContext		{0}
	(empty object)
		
	strategy		{2}
		
type	:	RollingUpdate
		
	rollingUpdate		{2}
		
maxUnavailable	:	1
		
maxSurge	:	1
		
	status		{5}
		
observedGeneration	:	1
		
replicas	:	1
		
updatedReplicas	:	1
		
availableReplicas	:	1
		
	conditions		[1]
		
	0		{6}
		
type	:	Available
		
status	:	True
		
lastUpdateTime	:	2017-02-23T19:58:06Z
		
lastTransitionTime	:	2017-02-23T19:58:06Z
		
reason	:	MinimumReplicasAvailable
		
message	:	Deployment has minimum availability.

```",closed,False,2017-02-23 20:16:39,2017-02-27 21:12:37
kubernetes-anywhere,PatrykWo,https://github.com/kubernetes/kubernetes-anywhere/issues/344,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/344,Space in config is wrongly intepreted by the deployer,"When variable in config consist space inside is wrongly interpreted during the deployment.
Example for cluster name:
.phase1.vSphere.resourcepool=""VIO Mgmt Cluster""

During deployment receiving error
vsphere_virtual_machine.kubevm1: resource pool 'VIO|Mgmt|Cluster' not found

resource_pool:                          """" => ""VIO|Mgmt|Cluster""",closed,False,2017-02-23 20:23:27,2017-02-28 12:01:47
kubernetes-anywhere,kerneltime,https://github.com/kubernetes/kubernetes-anywhere/pull/345,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/345,"Upgrade dashboard and fix space handling. Fixes #334, Fixes #343",,closed,True,2017-02-23 21:36:02,2017-02-27 21:12:36
kubernetes-anywhere,theofpa,https://github.com/kubernetes/kubernetes-anywhere/pull/346,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/346,vSphere: The proxy to the Kubernetes API server is http,"On vSphere instructions, the dashboard is running in http, not https.",closed,True,2017-02-25 21:19:58,2017-02-27 21:12:59
kubernetes-anywhere,luomiao,https://github.com/kubernetes/kubernetes-anywhere/pull/347,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/347,vSphere: Add error exit in configure script to prevent silent failure,"Now if any commands in the configure script fail, the install hangs since the script doesn't return any error:
https://github.com/kubernetes/kubernetes-anywhere/issues/299

This PR adds error exit handling in configure script for vSphere provider in order to prevent silent failures.",closed,True,2017-03-01 19:13:51,2017-03-09 18:06:18
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/348,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/348,Add latest OVA timestamp in README for vSphere,"Fixes #https://github.com/vmware/kubernetes/issues/100. This PR adds latest OVA timestamp in README for vSphere. Latest OVA add network dependency in flannel systemd unit.

Tested:
Created kubernetes cluster of v1.5.3. Run ```echo """" > /etc/machine-id``` and **restart** node.
Earlier:
flannel initially failed due to network interface not available and started after docker. This is why dashboard didn't worked.
With this change,
flannel don't fail initially. Now, dashboard works.

//cc @kerneltime ",closed,True,2017-03-02 07:00:19,2017-03-02 07:46:39
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/349,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/349,Add validation for vsphere,This PR adds some type checks in Kconfig for vsphere.,closed,True,2017-03-02 20:53:27,2017-03-09 18:06:57
kubernetes-anywhere,armsby,https://github.com/kubernetes/kubernetes-anywhere/issues/350,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/350,OVA link does not work,"The link for the OVA is not working is gives Access denied.
Anonymous users does not have storage.objects.get access to object kubernetes-anywhere-for-vsphere-cna-storage/KubernetesAnywhereTemplatePhotonOS.ova.",closed,False,2017-03-03 10:39:45,2017-03-03 10:49:55
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/351,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/351,skip downloading kubelet if URL is not accessible,"while deploying kubernetes cluster user can specify hyperkube version from docker registry as below.

```
docker registry (phase2.docker_registry) [gcr.io/google-containers] (NEW) gcr.io/google-containers
kubernetes version (phase2.kubernetes_version) [v1.5.3] (NEW) v1.5.3
```

but when user is using custom hyperkube image from private repository - `https://hub.docker.com/r/divyen/hyperkube-amd64/tags/` for example as below

```
docker registry (phase2.docker_registry) [gcr.io/google-containers] (NEW) docker.io/divyen
kubernetes version (phase2.kubernetes_version) [v1.5.3] (NEW) nightly-1.5
```
kubernete-anywhere will try to download kubelet from `https://storage.googleapis.com/kubernetes-release/release/nightly-1.5/bin/linux/amd64/kubectl` which is not the right URL.

to avoid failing deployment, we can check if URL is accessible, and if accessible then only delete existing kubelet binary and replace it with available kubelet binary from URL.

in this case, users can manually download desired version of kubelet in the kubernete anywhere dev container.


cc: @abrarshivani @BaluDontu @tusharnt @pdhamdhere @luomiao @kerneltime 
",closed,True,2017-03-06 20:48:49,2017-03-09 18:09:03
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/352,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/352,Accommodate upstream kubeadm CLI changes.,"- The `--discovery` flag has reverted back to `--token`
- Token strings are back to `<6>.<16>` instead of `<6>:<16>`
- `--api-advertise-addresses` is now `--apiserver-advertise-address`
- `--api-port` is now `--apiserver-bind-port`
- `--use-kubernetes-version` is now `--kubernetes-version`

CC @luxas @mikedanese ",closed,True,2017-03-08 06:08:00,2017-03-10 05:43:39
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/353,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/353,Remove unnecessary quoting of SHELLFLAGS.,"For some reason, this was breaking the multiline `if` used in the kubeconfig-path target under different versions of make/bash. Removing these quotes fixed the kubeconfig-path make target in the docker-dev shell.",closed,True,2017-03-08 06:15:57,2017-03-09 18:10:30
kubernetes-anywhere,tuomassalo,https://github.com/kubernetes/kubernetes-anywhere/pull/354,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/354,Add port 15441 forwarding to apiserver,"My vSphere deployment failed with the error `Validation: Expected 5 healthy nodes; found 0.`.

From master node's journalctl I found that it tried to connect to localhost:15441 but failed:

    Mar 10 11:16:21 master docker[606]: W0310 11:16:21.519288     703 manager.go:151] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused

With this PR, I managed to get the cluster running.

NB: I'm not aware where this problem came from in the first place - or whether this would be the correct way to fix the problem.

",closed,True,2017-03-10 14:18:37,2018-01-15 11:05:40
kubernetes-anywhere,holiman,https://github.com/kubernetes/kubernetes-anywhere/issues/355,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/355,`make docker-dev` fails due to node-uuid,"Version: 46e00da2db3269d4429bd40e86dded8d761b9ea1 . 
OS: Linux

```bash
> make docker-dev

[...]

+ sed -i '/terraform_${TERRAFORM_VERSION}_linux_amd64.zip/!d' /tmp/terraform/terraform_0.7.2_SHA256SUMS
+ sha256sum -cs terraform_0.7.2_SHA256SUMS
+ unzip terraform_0.7.2_linux_amd64.zip -d /bin
Archive:  terraform_0.7.2_linux_amd64.zip
  inflating: terraform
+ rm -rf /tmp/terraform
+ npm install -g azure-cli
npm WARN deprecated node-uuid@1.4.7: use uuid module instead
/usr/bin/azure -> /usr/lib/node_modules/azure-cli/bin/azure

> fibers@1.0.15 install /usr/lib/node_modules/azure-cli/node_modules/fibers
> node build.js || nodejs build.js



The command '/bin/sh -c /opt/docker-build.sh' returned a non-zero code: 1
Makefile:66: recipe for target 'docker-build' failed
make: *** [docker-build] Error 1
```

Seems to be related to this:https://github.com/kelektiv/node-uuid/issues/155 . 

",closed,False,2017-03-13 09:12:41,2017-03-28 07:45:27
kubernetes-anywhere,varsy,https://github.com/kubernetes/kubernetes-anywhere/issues/356,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/356,Kubernetes Kubelet Service can't start on vsphere,"Trying to run kubernetes v1.5.4 on vsphere.
The VMs have been created, but the K8S won't start:
```
Mar 13 18:11:27 master systemd[1]: Starting Kubernetes Kubelet Server...
Mar 13 18:11:27 master systemd[1]: Started Kubernetes Kubelet Server.
Mar 13 18:11:28 master docker[703]: Flag --config has been deprecated, Use --pod-manifest-path instead. Will be removed in a future version.
Mar 13 18:11:28 master docker[703]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.112163     733 feature_gate.go:189] feature gates: map[]
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.212640     733 server.go:369] Successfully initialized cloud provider: ""vsphere"" from the config file: ""/etc/kubernetes/vsphere.conf""
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.923620     733 docker.go:356] Connecting to docker on unix:///var/run/docker.sock
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.923785     733 docker.go:376] Start docker client with request timeout=2m0s
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.930801     733 server.go:511] cloud provider determined current node name to be master
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.931160     733 manager.go:143] cAdvisor running in container: ""/docker/466c5fc7ba36ca648eaa2940f9852009021cbaab71479f2c6663d423d019b413""
Mar 13 18:11:28 master docker[703]: W0313 18:11:28.933657     733 manager.go:151] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt:
 connection refused
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.936470     733 fs.go:117] Filesystem partitions: map[/dev/root:{mountpoint:/var/lib/docker major:8 minor:2 fsType:ext4 blockSize:0}]
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.937395     733 manager.go:198] Machine: {NumCores:1 CpuFrequency:2194917 MemoryCapacity:4145709056 MachineID:efee03ac51c6418889650dfa2a40350
d SystemUUID:4206A812-65D9-2A0D-730C-B3B742B23C91 BootID:024091c0-7b38-4047-b99a-fe25372b309e Filesystems:[{Device:/dev/root Capacity:8317734912 Type:vfs Inodes:524288 HasInodes:true} {Device
:overlay Capacity:8317734912 Type:vfs Inodes:524288 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:16777216000 Scheduler:noop}] NetworkDevices:[{Name:eth0 MacAddress:00:50:56
:86:01:49 Speed:10000 Mtu:1500} {Name:flannel0 MacAddress: Speed:10 Mtu:1472}] Topology:[{Id:0 Memory:0 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instru
ction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:57671680 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.938794     733 manager.go:204] Version: {KernelVersion:4.4.8-esx ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.11.0 Cadviso
rVersion: CadvisorRevision:}
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.942140     733 server.go:511] cloud provider determined current node name to be master
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.942510     733 server.go:700] Using root directory: /var/lib/kubelet
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.944227     733 kubelet.go:307] cloud provider determined current node name to be master
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.944291     733 kubelet.go:242] Adding manifest file: /etc/kubernetes/manifests
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.944832     733 file.go:47] Watching path ""/etc/kubernetes/manifests""
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.944888     733 kubelet.go:252] Watching apiserver
Mar 13 18:11:28 master docker[703]: E0313 18:11:28.949914     733 reflector.go:188] pkg/kubelet/config/apiserver.go:44: Failed to list *api.Pod: Get http://localhost:8080/api/v1/pods?fieldSel
ector=spec.nodeName%3Dmaster&resourceVersion=0: dial tcp [::1]:8080: getsockopt: connection refused
Mar 13 18:11:28 master docker[703]: E0313 18:11:28.950337     733 reflector.go:188] pkg/kubelet/kubelet.go:378: Failed to list *api.Service: Get http://localhost:8080/api/v1/services?resource
Version=0: dial tcp [::1]:8080: getsockopt: connection refused
Mar 13 18:11:28 master docker[703]: E0313 18:11:28.950674     733 reflector.go:188] pkg/kubelet/kubelet.go:386: Failed to list *api.Node: Get http://localhost:8080/api/v1/nodes?fieldSelector=
metadata.name%3Dmaster&resourceVersion=0: dial tcp [::1]:8080: getsockopt: connection refused
Mar 13 18:11:28 master docker[703]: W0313 18:11:28.951005     733 kubelet_network.go:69] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.951053     733 kubelet.go:477] Hairpin mode set to ""hairpin-veth""
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.953571     733 docker_manager.go:256] Setting dockerRoot to /var/lib/docker
Mar 13 18:11:28 master docker[703]: I0313 18:11:28.953624     733 docker_manager.go:259] Setting cgroupDriver to cgroupfs
```

Could you tell me please where I have to look?
Thank you in advance!",closed,False,2017-03-14 09:29:33,2017-06-13 08:29:25
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/357,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/357,Fix race condition when waiting for kubeconfig.,"If `WAIT_FOR_KUBECONFIG` is set and the cluster comes up very quickly so that first ssh call succeeds, then the output from ssh's initialization (generating keys, showing fingerprints/randomart, etc.) can get included at the start of the `kubeconfig.json` file, rendering it invalid.

To alleviate this, echo a marker to indicate the start of the file so we can delete this noise.

CC @mikedanese ",closed,True,2017-03-15 22:49:07,2017-03-21 00:03:43
kubernetes-anywhere,praseodym,https://github.com/kubernetes/kubernetes-anywhere/pull/358,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/358,Fix links in vSphere Getting Started,,closed,True,2017-03-22 13:06:23,2017-03-22 20:02:41
kubernetes-anywhere,prime421,https://github.com/kubernetes/kubernetes-anywhere/issues/359,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/359,*.data.tls_cert_request.K8S-master: unexpected EOF,"Hello all,

I am a self admitted newbie when it comes to Kubernetes and have been tasked with deploying a cluster for our DEV team.  Using some guides and articles, I was able to:

1.  Install the Photon v1.0 VM and configure it per the articles
2.  Deploy the KubernetesAnywhere .ova (v1.5.4)
3.  Make config succeeds
4.  Make deploy looks like it is going to pass until the very last portion and throws the following error:

*.data.tls_cert_request.K8S-master: unexpected EOF
Makefile:63: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:41: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluuster] Error 2

I have tried make clear, make destroy, deleting the VMs deployed and re-running the make deploy and it always seems to bomb out on this step.  When I try saying no on the step asking if vCenter uses a self signed cert, it errors right away saying it is insecure.

Our vCenter server is a 2012 R2 server v6 Update 3 with embedded PSC.

Any ideas?  We are stuck.

Thanks in advance!",closed,False,2017-03-24 01:31:04,2018-04-08 20:07:52
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/360,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/360,Adding minimal privileges required for kubernetes-Anywhere,"Adding minimal privileges required for kubernetes-Anywhere.

Verified privileges on vCenter 6.5 with deploying Kubernetes 1.5.3 cluster using Kubernetes-Anywhere master branch.

Performed following operations
Create Storage Class, Create PVC, Create POD, Delete POD, Delete Volume.
Verified Disk attach/detach volume create/delete is working fine with these set of privileges.

CC: @abrarshivani @BaluDontu @luomiao @tusharnt @pdhamdhere @kerneltime",closed,True,2017-03-24 18:41:47,2017-04-17 19:28:46
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/361,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/361,Fix sed command.,"This was a silly mistake in 02dddef. If the very first line was `STARTFILE`, then the entire file was getting truncated. Using `0` instead of `1` for the range matching allows the match to occur on the first line, giving the intended behavior if nothing else comes before `STARTFILE`.",closed,True,2017-03-27 17:08:58,2017-03-27 18:42:51
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/362,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/362,Add Google Cloud SDK to Docker image.,"This provides the `gcloud` CLI, which is very useful when creating GCE clusters, and is also a requirement if `WAIT_FOR_KUBECONFIG` is set when using `kubeadm` as the phase2 provider.",closed,True,2017-03-27 18:39:52,2017-03-27 18:42:43
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/363,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/363,Add support for weave-net addon.,"Most likely, this addon will only be used by clusters using `kubeadm` for phase2 that require a CNI provider.

The json files were generated by fetching https://git.io/weave-kube-1.6 (which is multidoc yaml), breaking it apart into individual documents, and converting them to json.

Since `ClusterRole` and `ClusterRoleBinding` weren't supported by the older `kubectl` used in the Docker image, I also bumped its version.

CC @mikedanese ",closed,True,2017-03-27 21:15:04,2017-03-27 21:34:20
kubernetes-anywhere,holiman,https://github.com/kubernetes/kubernetes-anywhere/issues/364,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/364,`make deploy` fails to validate,"I'm having much the same problem as this one: https://github.com/kubernetes/kubernetes-anywhere/issues/260

After deployment, it's stuck in
```
Validation: Expected 5 healthy nodes; found 0. (2870s elapsed)
Validation: Expected 5 healthy nodes; found 0. (2880s elapsed)
Validation: Expected 5 healthy nodes; found 0. (2890s elapsed)
```
Logs: 

`kube@fuzzcluster-master:~$ sudo journalctl -u kubelet 2>&1 | nc termbin.com 9999` : http://termbin.com/3c1h

`kube@fuzzcluster-master:~$ sudo cat /var/log/cloud-init-output.log | nc termbin.com 9999` : http://termbin.com/sdzo


",closed,False,2017-03-28 11:45:31,2017-03-29 07:52:58
kubernetes-anywhere,holiman,https://github.com/kubernetes/kubernetes-anywhere/issues/365,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/365,"DNS, scheduler and dashboard stuck in CrashLoopBackOff","Whenever I create a cluster with k8s-anywhere, I wind up with this result: 

```
kube-controller-manager-fuzzcluster-master   1/1       Running            0          6m
kube-dns-v19-4975w                           1/3       CrashLoopBackOff   5          7m
kube-dns-v19-co0uw                           1/3       CrashLoopBackOff   10         7m
kube-proxy-7jxnz                             1/1       Running            0          7m
kube-proxy-d4xi4                             1/1       Running            0          7m
kube-proxy-eewvn                             1/1       Running            0          7m
kube-proxy-p3eho                             1/1       Running            0          7m
kube-proxy-q6doh                             1/1       Running            0          7m
kube-scheduler-fuzzcluster-master            1/1       Running            0          6m
kubernetes-dashboard-1872455951-7lhdk        0/1       CrashLoopBackOff   5          7m
```

I have tried using various versions of kubernetes:
``` 
*
* Phase 2: Node Bootstrapping
*
installer container (phase2.installer_container) [docker.io/cnastorage/k8s-ignition:v1] 
docker registry (phase2.docker_registry) [gcr.io/google-containers] 
kubernetes version (phase2.kubernetes_version) [v1.6.0-beta.4] v1.5.5
```
Have tried `v1.5.1`, `v1.5.5` and `1.4..9` (and `v1.6.0-beta.4` , but that didn't deploy successfully at all with k8s-anywere ). 
I am aware of this: https://github.com/Azure/acs-engine/issues/200 , but I'm not sure what to do to avoid this, or how to fix it after it happens. The issue comments hints at this being fixed. ",closed,False,2017-03-29 09:33:58,2018-07-27 06:59:34
kubernetes-anywhere,colemickens,https://github.com/kubernetes/kubernetes-anywhere/issues/366,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/366,Deprecate azure support?,"I don't have the bandwidth to support `kubernetes-anywhere` alongside `ACS-Engine`.

But I'm not sure what to do about it necessarily...

Thoughts, @mikedanese ? I have no problem with removing it...",closed,False,2017-03-29 12:37:24,2018-01-04 16:06:53
kubernetes-anywhere,TwoSlick,https://github.com/kubernetes/kubernetes-anywhere/issues/367,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/367,vSphere network not found if the name contains a forward slash,"Example input:
`.phase1.vSphere.network=""DHCP_10.1.1.0/24""`

Result:
`Error applying plan:`
`* vsphere_virtual_machine.kubevm2: network '*DHCP_10.1.1.0/24' not found`
",closed,False,2017-03-30 15:25:57,2018-01-15 10:52:55
kubernetes-anywhere,vielmetti,https://github.com/kubernetes/kubernetes-anywhere/issues/368,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/368,Port kubernetes-anywhere to provision on Packet,"With encouragement from @luxas , this item will track the project to get Kubernetes Anywhere to provision and run services on Packet.

For reference

* [Packet developer API](https://www.packet.net/developers/api/)
",closed,False,2017-04-01 21:11:15,2018-07-03 13:44:20
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/issues/369,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/369,Kubernetes-Anywhere Release,"We don't have any release yet, given that the Kubernetes-Anywhere repo is stable. Can we tag a release now?
// cc @kubernetes/kubernetes-anywhere-admins 
// cc @kubernetes/kubernetes-anywhere-maintainers
// cc @pdhamdhere @tusharnt @BaluDontu @divyenpatel @kerneltime ",closed,False,2017-04-03 19:07:07,2017-04-19 21:33:26
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/issues/370,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/370,Can not deploy Kubernetes Cluster on vSphere with admin user password containing char '%',"```
.phase1.vSphere.username=""Administrator@vsphere.local""
.phase1.vSphere.password=""%Admin23""
```

```
[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json .config > .config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/$(jq -r '.phase1.cloud_provider' .config.json)""; ./do deploy-cluster )
RUNTIME ERROR: Unrecognised conversion type: A
	std.jsonnet:313:21-62	function <parse_conv_type>
	std.jsonnet:326:31-59	thunk <ctype>
	std.jsonnet:328:24-28	object <r>
	std.jsonnet:347:38-40	thunk <i>
	std.jsonnet:347:21-66	function <parse_codes>
	std.jsonnet:347:21-66	function <parse_codes>
	std.jsonnet:351:23-49	thunk <codes>
	std.jsonnet:616:30-34	thunk <codes>
	std.jsonnet:517:32-36	
	std.jsonnet:517:21-37	function <format_codes_arr>
	...
	vSphere.jsonnet:132:21-133	thunk <array_element>
	vSphere.jsonnet:(130:27)-(135:19)	object <anonymous>
	vSphere.jsonnet:(129:32)-(136:17)	object <anonymous>
	vSphere.jsonnet:(128:27)-(137:12)	thunk <array_element>
	vSphere.jsonnet:(128:26)-(141:13)	object <anonymous>
	vSphere.jsonnet:(121:17)-(142:9)	object <anonymous>
	std.jsonnet:(943:13)-(952:13)	object <anonymous>
	std.jsonnet:(943:13)-(952:13)	object <anonymous>
	all.jsonnet:3:49-79	object <anonymous>
	During manifestation	
Makefile:63: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
```

cc: @tusharnt, @BaluDontu @abrarshivani ",closed,False,2017-04-03 22:39:57,2017-10-11 18:50:36
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/issues/371,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/371,RUNTIME ERROR: Field does not exist: weave_net,"weave-net Addon is optional. but selecting N results in to error.
```
*
* Phase 3: Deploying Addons
*
Run the addon manager? (phase3.run_addons) [Y/n/?] (NEW) Y
  Run kube-proxy? (phase3.kube_proxy) [Y/n/?] (NEW) Y
  Run the dashboard? (phase3.dashboard) [Y/n/?] (NEW) Y
  Run heapster? (phase3.heapster) [Y/n/?] (NEW) Y
  Run kube-dns? (phase3.kube_dns) [Y/n/?] (NEW) Y
  Run weave-net? (phase3.weave_net) [N/y/?] (NEW) N
#
# configuration written to .config
#
make[1]: Leaving directory '/opt/kubernetes-anywhere'
util/config_to_json .config > .config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/$(jq -r '.phase1.cloud_provider' .config.json)""; ./do deploy-cluster )
RUNTIME ERROR: Field does not exist: weave_net
	../../phase3/all.jsonnet:2:42-58	function <if_enabled>
	../../phase3/all.jsonnet:10:16-83	thunk <array_element>
	std.jsonnet:631:46-53	thunk <b>
	../../phase3/all.jsonnet:3:50	function <func>
	std.jsonnet:631:32-54	thunk <running>
	std.jsonnet:631:17-64	function <aux>
	std.jsonnet:631:17-64	function <aux>
	std.jsonnet:632:9-31	function <anonymous>
	../../phase3/all.jsonnet:3:21-60	function <join>
	../../phase3/all.jsonnet:(5:14)-(11:15)	thunk <manifest>
	...
	vSphere.jsonnet:132:21-133	thunk <array_element>
	vSphere.jsonnet:(130:27)-(135:19)	object <anonymous>
	vSphere.jsonnet:(129:32)-(136:17)	object <anonymous>
	vSphere.jsonnet:(128:27)-(137:12)	thunk <array_element>
	vSphere.jsonnet:(128:26)-(141:13)	object <anonymous>
	vSphere.jsonnet:(121:17)-(142:9)	object <anonymous>
	std.jsonnet:(943:13)-(952:13)	object <anonymous>
	std.jsonnet:(943:13)-(952:13)	object <anonymous>
	all.jsonnet:3:49-79	object <anonymous>
	During manifestation	
Makefile:63: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:41: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
```

@abrarshivani @BaluDontu @tusharnt",closed,False,2017-04-04 04:11:51,2017-04-05 16:30:42
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/372,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/372,Allow disabled addons to be omitted from .config,"When running `make config` or `make menuconfig`, options that are disabled don't necessarily go into .config with values of `n`. In some versions of kconfig-frontends, like the one in this repo's Docker image, comments get stored instead:

    # .phase3.dashboard is not set

Without this change, attempting to run `make addons` or even just `phase3/do gen` chokes on these values, since jsonnet can't find configuration for them. With the change, it treats addons omitted from .config as disabled.

This should also help upgrade-proof clients that automate the creation of .config, so upstream additions of new addons don't render old .config files broken.",closed,True,2017-04-04 19:54:57,2017-04-04 23:34:06
kubernetes-anywhere,alejdg,https://github.com/kubernetes/kubernetes-anywhere/issues/373,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/373,vSphere: Documentation gap,"I believe there is a gap in the steps of  the doc *Getting started on vSphere*. More specifically between [Upload VM Image](https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase1/vsphere/README.md#upload-vm-image-to-be-used-to-vsphere) and [Clone kubernetes-anywhere](https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase1/vsphere/README.md#clone-kubernetes-anywhere).

It is stated in *Upload VM Image* to ""**NOT POWER ON THE IMPORTED VM**"" but right after it indicates to clone the project. I'm in doubt of where should I clone it. I believe that I should clone the VM, but are there  any details that I need to pay attention to?",closed,False,2017-04-06 13:05:23,2017-04-06 21:08:13
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/374,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/374,phase2 - ignition - reverting etcd-container image back to  etcd:2.2.1,"With the ignition image having` etcd:3.0.4`, we have observed issue, volume can not be detached from the node after pod is deleted. 

This issue is producible on Kubernetes Release **v1.5.3**,  which is working fine with `etcd:2.2.1`.

Issue is verified with creating two ignition images, and using same Kubernetes releases - 1.5.3

> divyen/ignition.etcd-3.0.4:v1 - disk never get detached from node VM, after pod is deleted.
> divyen/ignition.etcd-2.2.1:v1 - disk get detached from the node VM, after pod is deleted.

@tusharnt @BaluDontu @abrarshivani 




",closed,True,2017-04-06 14:39:24,2017-04-06 21:28:39
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/issues/375,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/375,not able to deploy cluster when addons not selected,"```
*
* Phase 3: Deploying Addons
*
Run the addon manager? (phase3.run_addons) [Y/n/?] (NEW) n
#
# configuration written to .config
#
make: '.config' is up to date.
[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json .config > .config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/$(jq -r '.phase1.cloud_provider' .config.json)""; ./do deploy-cluster )
RUNTIME ERROR: Field does not exist: phase3
		object <anonymous>
	std.jsonnet:28:53	function <anonymous>
	vSphere.jsonnet:(18:36)-(24:6)	thunk <config_metadata_template>
	vSphere.jsonnet:132:98-121	thunk <a>
	std.jsonnet:145:21	
	std.jsonnet:145:12-22	thunk <a>
	std.jsonnet:145:12-34	function <anonymous>
	std.jsonnet:145:12-34	function <anonymous>
	vSphere.jsonnet:132:98-132	thunk <b>
	std.jsonnet:148:27	thunk <vals>
	...
	vSphere.jsonnet:132:21-133	thunk <array_element>
	vSphere.jsonnet:(130:27)-(135:19)	object <anonymous>
	vSphere.jsonnet:(129:32)-(136:17)	object <anonymous>
	vSphere.jsonnet:(128:27)-(137:12)	thunk <array_element>
	vSphere.jsonnet:(128:26)-(141:13)	object <anonymous>
	vSphere.jsonnet:(121:17)-(142:9)	object <anonymous>
	std.jsonnet:(943:13)-(952:13)	object <anonymous>
	std.jsonnet:(943:13)-(952:13)	object <anonymous>
	all.jsonnet:3:49-79	object <anonymous>
	During manifestation	
Makefile:63: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:41: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
[container]:/opt/kubernetes-anywhere>
```

cc: @tusharnt @BaluDontu @abrarshivani ",closed,False,2017-04-06 20:47:27,2017-08-31 23:56:00
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/issues/376,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/376,"failed to deploy kubernetes cluster on vsphere when datastore name is ""datastore1 (1)""","```
vsphere_virtual_machine.kubevm2: Still creating... (10s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (10s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (20s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (20s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (30s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (30s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (40s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (40s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (50s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (50s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (1m0s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (1m0s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (1m10s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (1m10s elapsed)
Error applying plan:

2 error(s) occurred:

* vsphere_virtual_machine.kubevm2: [ERROR] Failed trying to parse disk path: [datastore1 (1)] node1/node1.vmdk
* vsphere_virtual_machine.kubevm1: [ERROR] Failed trying to parse disk path: [datastore1 (1)] master/master.vmdk

Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.
Makefile:63: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:41: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
[container]:/opt/kubernetes-anywhere>
```

@tusharnt @BaluDontu @abrarshivani ",closed,False,2017-04-06 21:16:02,2018-01-15 10:53:07
kubernetes-anywhere,steveruckdashel,https://github.com/kubernetes/kubernetes-anywhere/pull/377,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/377,minor grammar fix,minor grammar fix.,closed,True,2017-04-11 14:45:35,2017-04-17 02:47:10
kubernetes-anywhere,robotica72,https://github.com/kubernetes/kubernetes-anywhere/issues/378,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/378,Kubernetes 1.6.x support?  (or 1.7 Alpha?),"I believe the current kubernetes-anywhere only support up to 1.5.4 - all 1.6.x deployments fail to validate, and I did see in a closed issue that was known due to repo support.    So two questions, is there a manual method to add 1.6.x  or  when will k8s-anywhere support 1.6.x?

Thanks!",closed,False,2017-04-21 14:38:55,2018-01-15 10:53:17
kubernetes-anywhere,philips,https://github.com/kubernetes/kubernetes-anywhere/issues/379,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/379,Dockerfile for gcr.io/mikedanese-k8s/ignite:v3,I am trying to understand the system and can't find where that comes from. Link?,closed,False,2017-04-22 09:21:09,2018-01-15 10:53:29
kubernetes-anywhere,billhoph,https://github.com/kubernetes/kubernetes-anywhere/issues/380,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/380,Failed to create VM,"Is there any log I can extract more detail? I'm sure the mentioned VM template is there, but i have no idea why I kept hitting error in deploying VM. Actually the clone tasks have not been kicked off at all.

Error applying plan:

5 error(s) occurred:

* vsphere_virtual_machine.kubevm2: vm 'KubernetesAnywhereTemplatePhotonOS.ova' not found
* vsphere_virtual_machine.kubevm5: vm 'KubernetesAnywhereTemplatePhotonOS.ova' not found
* vsphere_virtual_machine.kubevm3: vm 'KubernetesAnywhereTemplatePhotonOS.ova' not found
* vsphere_virtual_machine.kubevm4: vm 'KubernetesAnywhereTemplatePhotonOS.ova' not found
* vsphere_virtual_machine.kubevm1: vm 'KubernetesAnywhereTemplatePhotonOS.ova' not found
",closed,False,2017-04-27 15:12:52,2017-06-07 23:53:03
kubernetes-anywhere,mvasilenko,https://github.com/kubernetes/kubernetes-anywhere/issues/381,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/381,make deploy failed on vSphere,"I've run into such error while running trying to run `make deploy` on vSphere.

Template with this name exist on ESXi, it can be cloned successfuliy in vCenter.
I've deleted this vm, deployed OVF Template again, no difference.

```
Error applying plan:

5 error(s) occurred:

* vsphere_virtual_machine.kubevm1: vm 'KubernetesAnywhereTemplatePhotonOS.ova' not found
* vsphere_virtual_machine.kubevm4: vm 'KubernetesAnywhereTemplatePhotonOS.ova' not found
* vsphere_virtual_machine.kubevm3: vm 'KubernetesAnywhereTemplatePhotonOS.ova' not found
* vsphere_virtual_machine.kubevm2: vm 'KubernetesAnywhereTemplatePhotonOS.ova' not found
* vsphere_virtual_machine.kubevm5: vm 'KubernetesAnywhereTemplatePhotonOS.ova' not found

Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.

```",closed,False,2017-04-28 22:46:11,2017-05-02 17:17:32
kubernetes-anywhere,MohGeek,https://github.com/kubernetes/kubernetes-anywhere/pull/382,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/382,Fix typo in README,,closed,True,2017-05-01 15:36:32,2017-05-01 16:00:15
kubernetes-anywhere,MohGeek,https://github.com/kubernetes/kubernetes-anywhere/pull/383,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/383,Fix typo in README,,closed,True,2017-05-01 16:04:47,2017-05-01 16:10:37
kubernetes-anywhere,MohGeek,https://github.com/kubernetes/kubernetes-anywhere/pull/384,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/384,Fix typo in README,,closed,True,2017-05-01 16:10:58,2017-05-05 23:39:48
kubernetes-anywhere,jamiehannaford,https://github.com/kubernetes/kubernetes-anywhere/pull/385,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/385,Add Flannel network add-on,"This implements the Flannel addon as a first step to adding better CNI coverage to kubeadm's e2e tests, as documented here https://github.com/kubernetes/kubeadm/issues/218. 

Unfortunately I wasn't able to verify this because I don't have a GCE or Azure account, nor a Vsphere license. Is there any way to get around this, or use some kind of community account? Alternatively is there any word on AWS, Vagrant or OpenStack providers? If not, I can see about getting GCE access.",closed,True,2017-05-03 12:59:58,2017-06-06 13:38:07
kubernetes-anywhere,MohGeek,https://github.com/kubernetes/kubernetes-anywhere/pull/386,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/386,Update terraform to 0.9.4,"This updates the Terraform version used by project from 0.7.2 to 0.9.4. It fixes multiple issues related to Terraform (see : https://github.com/hashicorp/terraform/blob/master/CHANGELOG.md). 

- As the cert request flow changed after the 0.7.5 version, the jsonnet had to be updated to reflect this change. 
- The env variable TERRAFORM_SHA256SUM was also deleted from the docker-build script as it wasn't used. ",closed,True,2017-05-03 15:27:27,2017-05-19 17:34:39
kubernetes-anywhere,haukurk,https://github.com/kubernetes/kubernetes-anywhere/issues/387,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/387,Support for Datastore Cluster with vSphere provider?,"Has anyone used SDRS (Datastore cluster) with the vSphere provider?

I'm getting the following error when provisioning:

```
* vsphere_virtual_machine.kubevm3: ServerFaultCode: A specified parameter was not correct: Datastore unspecified for at least one disk in SDRS-disabled VM
```",closed,False,2017-05-04 22:09:35,2018-01-15 10:55:39
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/388,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/388,Updating vsphere/README.md,"Recommending  user to download tagged release instead of `git clone https://github.com/kubernetes/kubernetes-anywhere repository`

Informing user about not to install `weave-net` add on.


cc: @tusharnt @BaluDontu @abrarshivani ",closed,True,2017-05-05 17:15:37,2017-05-05 23:35:29
kubernetes-anywhere,spyworldxp,https://github.com/kubernetes/kubernetes-anywhere/issues/389,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/389,VMware vSphere 6.5 deploy error,"The current ""terraform"" on kubernetes-anywhere doesn't support vSphere 6.5. Please upgrade hashicorp/terraform to the latest version.",closed,False,2017-05-08 03:57:13,2017-05-17 07:43:16
kubernetes-anywhere,klizhentas,https://github.com/kubernetes/kubernetes-anywhere/pull/390,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/390,Add flannel,,closed,True,2017-05-13 01:03:18,2017-09-07 16:23:51
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/391,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/391,Updating vsphere kubernetes cluster deployment flow,"Current Menu is asking user to specify a cluster, host or a resource pool in which he wants to deploy Kubernetes Cluster. 

`Specify a valid Cluster, Host or Resource Pool in which to deploy Kubernetes VMs. (phase1.vSphere.resourcepool) [] (NEW)`

We have documented how the resource pool path looks like, but it is inconvenient to read up documentation, and build the path. We should change the Menu options to build up resource pool path for user. User just need to  enter the resource pool name.

Following is the change we want in the Menu options.

```
Deploy Kubernetes Cluster on 'host' or 'cluster' (phase1.vSphere.placement) [cluster] (NEW) cluster
    vspherecluster (phase1.vSphere.cluster) [] (NEW) cluster-vsan-1
Do you want to use resource pool created on host or cluster? [yes, no] (phase1.vSphere.useresourcepool) [no] (NEW) yes
    name of the resource pool [Provide Just a name] (phase1.vSphere.resourcepool) [] (NEW) dbResourcePool
```
Verified change with deploying cluster on the resource pool created in host and in cluster.

@msterin @BaluDontu @pdhamdhere @tusharnt

",closed,True,2017-05-15 18:55:56,2017-05-16 20:28:58
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/392,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/392,Correcting release file to download and unzip,"Observed issue while downloading and unzipping kubernetes-anywhere release build.
Note: This issue is not observed on Mac OS X, on linux and windows this issue is re-producible.

```
$ curl -sL https://github.com/kubernetes/kubernetes-anywhere/archive/v0.1.0.zip | tar xz
gzip: stdin has more than one entry--rest ignored
tar: Child died with signal 13
tar: Error is not recoverable: exiting now
```

PR is correcting link to download so that It can be unzipped without this issue.
@abrarshivani @msterin @BaluDontu @tusharnt
",closed,True,2017-05-15 21:19:46,2017-05-16 20:29:29
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/393,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/393,Don't add --kubernetes-version if the value is empty.,This is important for some of our e2e tests to be able to exercise the default behavior of omitting this parameter.,closed,True,2017-05-15 23:52:16,2017-05-16 17:18:54
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/394,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/394,Enable/start kubelet in kubeadm phase2.,"When using our bazel-generated .debs, kubelet had previously been silently started by `kubeadm init` itself (even with preflight checks disabled), which https://github.com/kubernetes/kubernetes/pull/45231 fixed.

With that behavior gone, we need to enable/start kubelet explicitly.

https://github.com/kubernetes/kubeadm/issues/268",closed,True,2017-05-16 00:00:28,2017-05-16 17:18:38
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/395,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/395,"In GCE, make sure apt-transport-https is installed.","I found that this step was required when testing with a Debian image (debian-8-jessie-v20170426), and it's also one the steps in the [kubeadm getting started guide](https://kubernetes.io/docs/getting-started-guides/kubeadm/).

This doesn't actually allow the Debian image to be used end-to-end, since there are still dependency conflicts with the Docker package, but it does allow the startup to get a little further.",closed,True,2017-05-16 00:58:03,2017-05-16 20:36:53
kubernetes-anywhere,ozdanborne,https://github.com/kubernetes/kubernetes-anywhere/pull/396,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/396,Turn off verbose output for untar,Untarring with verbose produces hundreds of lines of output which seems a bit unnecessary and makes it harder to process what the scripts are doing for newcomers. ,closed,True,2017-05-17 18:57:20,2017-05-19 19:26:07
kubernetes-anywhere,rperez31,https://github.com/kubernetes/kubernetes-anywhere/issues/397,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/397,make docker-dev error,"anyone seen this before? I am running the photon ISO provided on this link 

g++: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <http://gcc.gnu.org/bugs.html> for instructions.
make: *** [core/vm.o] Error 4
The command '/bin/sh -c /opt/docker-build.sh' returned a non-zero code: 1
Makefile:66: recipe for target 'docker-build' failed
make: *** [docker-build] Error 1
root@sc04kubernetes01 [ /tmp/kubernetes-anywhere-0.1.0 ]# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
<none>              <none>              8da0d4fa2dba        2 minutes ago       52.2 MB
<none>              <none>              c938aeaa6e39        10 minutes ago      52.2 MB
mhart/alpine-node   6.4.0               ecd37ad77c2b        9 months ago        48 MB
root@sc04kubernetes01 [ /tmp/kubernetes-anywhere-0.1.0 ]# ",closed,False,2017-05-20 00:02:46,2018-03-08 23:56:48
kubernetes-anywhere,adamstrawson,https://github.com/kubernetes/kubernetes-anywhere/issues/398,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/398,vSphere vCenter,"This is a question rather than an issue. The guide on vsphere deployment mentions that it requires vCenter, is this vCenter Server Foundation or vCenter Server Standard, or can it be either?

",closed,False,2017-05-22 14:47:41,2017-05-24 13:47:29
kubernetes-anywhere,pathcl,https://github.com/kubernetes/kubernetes-anywhere/issues/399,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/399,etcd flanneld networking,"Hello,

I've followed 'Get started on vSphere' though Im stucked at 'make deploy'.

On master node I can see there's an issue regarding etcd on master node

May 24 23:42:32 node4 flanneld[358]: E0524 23:42:32.316102 00358 network.go:53] Failed to retrieve network config: client: etcd cluster is unavailable or misconfiguredMay 24 23:42:38 node4 flanneld[358]: E0524 23:42:38.316090 00358 network.go:53] Failed to retrieve network config: client: etcd cluster is unavailable or misconfigured

Network chosen is different from your example. Should it be the same? 

> Configure the POD network using flannel
> Flannel Network (phase1.vSphere.flannel_net) [172.1.0.0/16] (NEW) 

Thank you",closed,False,2017-05-24 23:46:38,2017-05-25 14:01:25
kubernetes-anywhere,ozdanborne,https://github.com/kubernetes/kubernetes-anywhere/issues/400,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/400,default kuberenetes version (v1.5.3) incompatible with default kubeadm channel,"When setting `provider=kubeadm`, the install script defaults to the stable kubeadm channel with Kubernetes v1.5.3. This is an invalid combination, as kubeadm stable channel only supports Kubernetes v1.6.x. The script will continue, no kubernetes services will come up, and the script will just forever wait for them to come up: `Validation: Expected 3 healthy nodes; found 0. (140s elapsed)`

From the configuration steps:

```
kubernetes version (phase2.kubernetes_version) [v1.5.3] (NEW)
bootstrap provider (phase2.provider) [ignition] (NEW) kubeadm
  kubeadm version (phase2.kubeadm.version) [stable] (NEW) 
```

It'd be nice if the script did a better job of steering people away from this. A few suggestions:

1. Fail the configuration steps when the user selects an invalid combination
2. Prompt for provider / kubeadm version first and change the default kubernetes version accordingly",closed,False,2017-05-25 04:28:43,2018-01-15 10:55:46
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/pull/401,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/401,Change default nodes to 3 to suit trial size GCE account,Signed-off-by: Christopher M. Luciano <cmluciano@us.ibm.com>,closed,True,2017-05-31 16:15:45,2017-06-27 17:34:19
kubernetes-anywhere,jarduini,https://github.com/kubernetes/kubernetes-anywhere/issues/402,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/402,Error on make docker-dev command,"I´m having an erro when trying to execute make docker-dev command:

[root@lxl1pupkb001 kubernetes-anywhere-0.1.0]# make docker-dev
docker build -t kubernetes-anywhere:v0.0.1 .
Sending build context to Docker daemon 220.7 kB
Step 1 : FROM mhart/alpine-node:6.4.0
 ---> ecd37ad77c2b
Step 2 : RUN apk add --update bash
 ---> Running in 7884e8249a70
fetch http://dl-cdn.alpinelinux.org/alpine/v3.4/main/x86_64/APKINDEX.tar.gz
ERROR: http://dl-cdn.alpinelinux.org/alpine/v3.4/main: operation timed out
WARNING: Ignoring APKINDEX.167438ca.tar.gz: No such file or directory
fetch http://dl-cdn.alpinelinux.org/alpine/v3.4/community/x86_64/APKINDEX.tar.gz

- cat /etc/sysconfig/docker
OPTIONS='-H unix:///var/run/docker.sock --ip-forward=true --iptables=true --ip-masq=true --selinux-enabled=true -G docker --dns 10.8.3.44 --dns-search servicos.com.br --label environment=lab --label name=lxl1pupkb001 --log-driver=json-file --log-opt max-size=50m --mtu=1450'

- docker version:
Docker version 1.12.6, build 1398f24/1.12.6

-kubernetes-anywhere version:
kubernetes-anywhere-0.1.0

I´m behind a proxy so i exported http_proxy.
What i´m missing here?",closed,False,2017-05-31 20:21:29,2017-06-07 12:46:16
kubernetes-anywhere,g1001p,https://github.com/kubernetes/kubernetes-anywhere/issues/403,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/403,vSphere deployment  build failed ,"Dear forum,
I am trying to deploy kubernetes cluster on vsphere.
Actually the target compilation failed with the following error.
I appreciate for any recommendation to handle this issue:
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/$(jq -r '.phase1.cloud_provider' .config.json)""; ./do deploy-cluster )
.tmp/vSphere-k8s.test.tf
Error configuring: 33 error(s) occurred:

* data.template_file.configure_master: missing dependency: tls_self_signed_cert.k8s
* data.template_file.configure_master: missing dependency: tls_locally_signed_cert.k8s
* data.template_file.configure_master: missing dependency: tls_private_key.k8s
* data.template_file.configure_master: missing dependency: tls_locally_signed_cert.k8s
* data.template_file.configure_master: missing dependency: tls_private_key.k8s
* data.template_file.configure_node: missing dependency: tls_private_key.k8s
* data.template_file.configure_node: missing dependency: tls_self_signed_cert.k8s
* data.template_file.configure_node: missing dependency: tls_locally_signed_cert.k8s
* data.template_file.configure_node: missing dependency: tls_locally_signed_cert.k8s
* data.template_file.configure_node: missing dependency: tls_private_key.k8s
* data.tls_cert_request.k8s.test-admin: missing dependency: tls_private_key.k8s
* data.tls_cert_request.k8s.test-admin: missing dependency: tls_private_key.k8s
* data.tls_cert_request.k8s.test-master: missing dependency: tls_private_key.k8s
* data.tls_cert_request.k8s.test-master: missing dependency: tls_private_key.k8s
* data.tls_cert_request.k8s.test-node: missing dependency: tls_private_key.k8s
* data.tls_cert_request.k8s.test-node: missing dependency: tls_private_key.k8s
* null_resource.master: missing dependency: tls_private_key.k8s
* null_resource.master: missing dependency: tls_self_signed_cert.k8s
* null_resource.master: missing dependency: tls_locally_signed_cert.k8s
* tls_locally_signed_cert.k8s.test-admin: missing dependency: tls_private_key.k8s
* tls_locally_signed_cert.k8s.test-admin: missing dependency: tls_private_key.k8s
* tls_locally_signed_cert.k8s.test-admin: missing dependency: data.tls_cert_request.k8s
* tls_locally_signed_cert.k8s.test-admin: missing dependency: tls_self_signed_cert.k8s
* tls_locally_signed_cert.k8s.test-master: missing dependency: tls_private_key.k8s
* tls_locally_signed_cert.k8s.test-master: missing dependency: tls_private_key.k8s
* tls_locally_signed_cert.k8s.test-master: missing dependency: data.tls_cert_request.k8s
* tls_locally_signed_cert.k8s.test-master: missing dependency: tls_self_signed_cert.k8s
* tls_locally_signed_cert.k8s.test-node: missing dependency: tls_self_signed_cert.k8s
* tls_locally_signed_cert.k8s.test-node: missing dependency: tls_private_key.k8s
* tls_locally_signed_cert.k8s.test-node: missing dependency: tls_private_key.k8s
* tls_locally_signed_cert.k8s.test-node: missing dependency: data.tls_cert_request.k8s
* tls_self_signed_cert.k8s.test-root: missing dependency: tls_private_key.k8s
* tls_self_signed_cert.k8s.test-root: missing dependency: tls_private_key.k8s
Makefile:63: recipe for target 'do' failed",closed,False,2017-06-04 17:54:48,2017-12-31 08:05:07
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/pull/404,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/404,Add tmp path to gce usage instructions,I'm not sure if I did something wrong or if this is a bug. I only found a working  kubeconfig within the .tmp folder.,closed,True,2017-06-05 17:31:15,2017-10-23 20:17:50
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/pull/405,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/405,Add calico cni plugin,"This is a rebase on @klizhentas 's work as well as @ozdanborne 's trial commit.

I'm testing this manually at the moment.

Related kubernetes/kubeadm#218",closed,True,2017-06-07 20:21:01,2017-10-20 23:33:03
kubernetes-anywhere,RobbieJ,https://github.com/kubernetes/kubernetes-anywhere/pull/406,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/406,Create README.md,Quick adjustment to reflect the correct naming scheme for a ESXi without vCenter.,closed,True,2017-06-15 15:06:47,2017-07-13 12:29:48
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/407,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/407,Updating `phase1/vsphere/README.md` for Kubernetes v1.6.5 recommendation,"Updating `phase1/vsphere/README.md` for Kubernetes v1.6.5 recommendation.

1.6.5 change log

- vSphere cloud provider: Report same Node IP as both internal and external. (#45201, @abrarshivani)
- vSphere cloud provider: Filter out IPV6 node addresses. (#45181, @BaluDontu)
- vSphere cloud provider: Remove the dependency of login information on worker nodes for vsphere cloud provider. (#43545, @luomiao)
- vSphere cloud provider: Fix volume detach on node failure. (#45569, @divyenpatel)
- vSphere cloud provider: Fix fetching of VM UUID on Ubuntu 16.04 and Fedora. (#45311, @divyenpatel)


README.md Preview is available at https://github.com/divyenpatel/kubernetes-anywhere/blob/30413563ee5c91d2949715ed5e44cf7c041ba7bd/phase1/vsphere/README.md

PR Changes:

- Recommending v1.6.5

- Provided steps to use pre-build kubernetes-anywhere image `cnastorage/kubernetes-anywhere:latest`, instead of asking user to build image. This saves 10 minutes.

- made docker.io/cnastorage/k8s-ignition:v2 default in the wizard prompt. For 1.6 and above this new ignition image should be used.

- provided screen shots for better understanding of vsphere inventory, before and after the cluster deployment.

cc:  @luomiao @rohitjogvmw @BaluDontu @tusharnt @pdhamdhere",closed,True,2017-06-16 23:14:43,2017-06-21 02:46:45
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/issues/408,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/408,Support Openstack in phase 1,I use Openstack for development and it would be great it Openstack would be a supported cloud provider. I am interested in contributing this functionality if others are interested.,closed,False,2017-06-19 20:16:14,2017-11-03 18:00:09
kubernetes-anywhere,TreverW,https://github.com/kubernetes/kubernetes-anywhere/issues/409,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/409,vsphere deployment fails (RUNTIME ERROR: Field does not exist: cluster),"After running `make docker-dev` and `make config` specifying vSphere options, then running `make deploy`, I'm getting this error.

    [container]:/opt/kubernetes-anywhere> make deploy
    util/config_to_json .config > .config.json
    make do WHAT=deploy-cluster
    make[1]: Entering directory '/opt/kubernetes-anywhere'
    ( cd ""phase1/$(jq -r '.phase1.cloud_provider' .config.json)""; ./do deploy-cluster )
    RUNTIME ERROR: Field does not exist: cluster
    	vSphere.jsonnet:109:22-40	object <anonymous>
    	vSphere.jsonnet:(98:32)-(125:7)	object <anonymous>
    	std.jsonnet:(943:13)-(952:13)	object <anonymous>
    	std.jsonnet:(943:13)-(952:13)	object <anonymous>
    	all.jsonnet:3:49-79	object <anonymous>
    	During manifestation
    Makefile:63: recipe for target 'do' failed
    make[1]: *** [do] Error 1
    make[1]: Leaving directory '/opt/kubernetes-anywhere'
    Makefile:41: recipe for target 'deploy-cluster' failed
    make: *** [deploy-cluster] Error 2

When I look at the `.config.json` that was created, I see that the value I entered for **Deploy Kubernetes Cluster on 'host' or 'cluster' (phase1.vSphere.placement)**  was saved as 

          ""placement"": ""vc-vmhost1"",

instead of

          ""cluster"": ""vc-vmhost1"",

When I edited `.config.json` and copied the **""placement""** line and changed it to **""cluster""** it worked.",closed,False,2017-06-21 16:58:28,2017-06-23 15:36:59
kubernetes-anywhere,hoffin,https://github.com/kubernetes/kubernetes-anywhere/issues/410,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/410,Azure failing to bring up nodes with Kubernetes version 1.6+,"In Azure nodes aren't coming up as the kubelet service on the client nodes are using the `--reconcile-cidr` flag in `/etc/systemd/system/kubelet.service`. That flag has been removed since 1.6 and fails to start if it's there.

I've not had time to fix this but I from initial investigation this systemd file seems to being written 
`/phase2/ignition/vanilla/node.jsonnet`

Basicly it should not set `--reconcile-cidr` when kubernetes version > 1.6. ",closed,False,2017-06-25 22:19:42,2018-01-15 10:56:03
kubernetes-anywhere,psyolent,https://github.com/kubernetes/kubernetes-anywhere/issues/411,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/411,K8s -vSphere : failing to complete null_resource.master:,"Hi

Stuck with deploying kubernetes-anywhere @ work. 
Ironically the same deployment in my lab @ home works fine :)

My VMs deploy, master, nodes1,2,3,4. all good. dhcp works. all get IPd
Script connects to nodes and master creates symlink etc on all 5 nodes - good
I get for nodes 1,2,3,4 msgs eg: null_resource.node2: Creation complete (ID: 4710064365696826687)
But i dont' get one for master.
The it bombs out and I end up here

Error applying plan:

1 error(s) occurred:

* null_resource.master: 1 error(s) occurred:

* Script exited with non-zero exit status: 1

Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.
Makefile:63: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:41: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2

my .config
#
# Automatically generated file; DO NOT EDIT.
# Kubernetes Minimal Turnup Configuration
#

#
# Phase 1: Cluster Resource Provisioning
#
.phase1.num_nodes=4
.phase1.cluster_name=""kubernetes""
.phase1.cloud_provider=""vsphere""

#
# vSphere configuration
#
.phase1.vSphere.url=""192.168.0.35""
.phase1.vSphere.port=443
.phase1.vSphere.username=""myuserid
.phase1.vSphere.password=""mypass""
.phase1.vSphere.insecure=y
.phase1.vSphere.datacenter=""DevMgmt""
.phase1.vSphere.datastore=""datastore1
.phase1.vSphere.placement=""cluster""
.phase1.vSphere.cluster=""Dev
.phase1.vSphere.useresourcepool=""no""
.phase1.vSphere.vcpu=1
.phase1.vSphere.memory=2048
.phase1.vSphere.network=""VL2955""
.phase1.vSphere.template=""/Dev_Management/vm/DC1/KUBERNETES""
.phase1.vSphere.flannel_net=""172.1.0.0/16""

#
# Phase 2: Node Bootstrapping
#
.phase2.installer_container=""docker.io/cnastorage/k8s-ignition:v2""
.phase2.docker_registry=""gcr.io/google-containers""
.phase2.kubernetes_version=""v1.6.5""
.phase2.provider=""ignition""

#
# Phase 3: Deploying Addons
#
.phase3.run_addons=y
.phase3.kube_proxy=y
.phase3.dashboard=y
.phase3.heapster=y
.phase3.kube_dns=y
.phase3.weave_net=n

Initially I thought it was .phase3.weave_net being hashed out so changed that to n
i've exported http_proxy and https_proxy inside my docker
but still going nowhere
can someone point me in the direction of logs so i can see more detailed info why this is failing / any suggestions very welcome.
TIA

",closed,False,2017-07-04 04:26:40,2017-07-04 08:49:12
kubernetes-anywhere,fabriziopandini,https://github.com/kubernetes/kubernetes-anywhere/pull/412,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/412,Enable make Docker-Dev behind corporate proxy ,"improve `make docker-dev`making it sensible to proxy.

if env vars (http_proxy, https_proxy, no_proxy) are set, those variables will be forwarded to `docker build`, `docker run` (and `npm` inside docker).

NB. tested only on GCE (also changes for enabling same functionality on Azure, but not tested yet)

",closed,True,2017-07-07 14:09:27,2018-10-28 19:25:08
kubernetes-anywhere,pathcl,https://github.com/kubernetes/kubernetes-anywhere/issues/413,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/413,No documentation for minions/node to join master,Can't find any docs at all. I guess this should be done editing 'deploy.tf',closed,False,2017-07-17 19:55:42,2018-03-02 01:17:17
kubernetes-anywhere,Cipher333,https://github.com/kubernetes/kubernetes-anywhere/issues/414,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/414,DNS issues on all nodes with vSphere deployment,"Hi all

I have provisioned a new Kubernetes cluster using ""kubernetes-anywhere"" and ""make deploy"" into a temporary lab environment (via vCenter Server Appliance 6.5) but I am contiunally running into DNS issues which is preventing me from progressing any further with the build - the errors are following the completion of the ""make deploy"" process:

`kubectl get pods --namespace=kube-system --output=wide
...
heapster-v1.2.0-867844254-wftfv         0/2       ImagePullBackOff   0          29m       172.1.15.3   node2
kube-dns-v19-bd8l5                      0/3       ImagePullBackOff   0          29m       172.1.34.2   node4
kube-dns-v19-f2xsv                      0/3       ImagePullBackOff   0          29m       172.1.15.2   node2
...
kubernetes-dashboard-1019458639-jld5x   0/1       ImagePullBackOff   0          29m       172.1.34.3   node4`

an excerpt from the logs relating to any of the above associated with the ""ImagePullBackOff"" error is:

`26m   26m     1       kubelet, node4          Warning FailedSync      Error syncing pod, skipping: [failed to ""StartContainer"" for ""kubedns"" with ErrImagePull: ""rpc error: code = 2 desc = Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp: lookup gcr.io on 10.0.0.10:53: write udp 10.2.0.60:55957->10.0.0.10:53: write: operation not permitted""
, failed to ""StartContainer"" for ""dnsmasq"" with ErrImagePull: ""rpc error: code = 2 desc = Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp: lookup gcr.io on 10.0.0.10:53: write udp 10.2.0.60:49750->10.0.0.10:53: write: operation not permitted""
, failed to ""StartContainer"" for ""healthz"" with ErrImagePull: ""rpc error: code = 2 desc = Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp: lookup gcr.io on 10.0.0.10:53: write udp 10.2.0.60:42144->10.0.0.10:53: write: operation not permitted""
]`

other errors similar to the above:

`28m   28m     1       kubelet, node4  spec.containers{dnsmasq}        Warning Failed          Failed to pull image ""gcr.io/google_containers/kube-dnsmasq-amd64:1.3"": rpc error: code = 2 desc = Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp: lookup gcr.io on 10.0.0.10:53: write udp 10.2.0.60:45438->10.0.0.10:53: write: operation not permitted`

Can anyone provide any info on what is the issue here? I have tried to change this 10.0.0.10 ip (which is my lab DNS server) to googles DNS (8.8.8.8) but it doesn't matter where I change it - it still reverts to 10.0.0.10, though, I must point out that my lab DNS server @ 10.0.0.10 CAN resolve internet IP addresses when tested from any other VM, however, if I attempt to resolve addresses from names on any Kubernetes deployed host (master / node1 / node2 etc) it cannot resolve the address but I can ping the IP direct.

Any ideas what is going on here?

Thanks",closed,False,2017-07-17 23:14:38,2018-03-02 01:17:17
kubernetes-anywhere,jck-ccl,https://github.com/kubernetes/kubernetes-anywhere/issues/415,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/415,Kubelet fails to start when generated vsphere.conf contains improperly escaped characters,"When using the latest cnastorage/kubernetes-anywhere image
And deploying to vSphere
If the vCenter username and password contain characters that require escaping, for example backslash or double quote
Then the generated vSphere.conf file does not have these values properly escaped and the kubelet will not start.
The following error can be seen in 'journalctl -u kubelet'

```
Jul 19 19:10:56 master docker[767]: Error: failed to run Kubelet: could not init cloud provider ""vsphere""
: 2:21: unknown escape sequence
Jul 19 19:10:56 master docker[767]: Error: failed to run Kubelet: could not init cloud provider ""vsphere""
: 2:21: unknown escape sequence
Jul 19 19:10:56 master systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FA
ILURE
```

Example snippet from /etc/kubernetes/vsphere.conf

```
[Global]
        user = ""domain\user""
        password = ""mypassword""""
       ...
```
Correcting this on each node in the cluster, like this
```
[Global]
        user = ""domain\\user""
        password = ""mypassword\""""
       ...
```
And reloading the kubelet service results in a successful deployment.",closed,False,2017-07-19 20:24:18,2017-08-15 08:52:55
kubernetes-anywhere,jck-ccl,https://github.com/kubernetes/kubernetes-anywhere/pull/416,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/416,"415 - special characters need escaping in vsphere.conf, escapeStringJ…","…son seems to do this nicely

#415 proposed fix for escaping special characters in vsphere.conf.",closed,True,2017-07-25 11:14:01,2017-07-26 20:18:29
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/issues/417,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/417,unable to deploy cluster when vc port and non-default,"```
$cat .config
.
.
#
# vSphere configuration
#
.phase1.vSphere.url=""10.192.58.44""
.phase1.vSphere.port=7443
.
.
```


#
```
# configuration written to .config
#
make[1]: Leaving directory '/opt/kubernetes-anywhere'
util/config_to_json .config > .config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/$(jq -r '.phase1.cloud_provider' .config.json)""; ./do deploy-cluster )
.tmp/vSphere-kubernetes.tf
data.template_file.cloudprovider: Refreshing state...
Error running plan: 1 error(s) occurred:

* provider.vsphere: Error setting up client: Post https://10.192.58.44/sdk: dial tcp 10.192.58.44:443: getsockopt: connection refused
Makefile:63: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:41: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
```

cc: @BaluDontu @luomiao @tusharnt",closed,False,2017-07-26 21:48:28,2018-03-09 07:07:34
kubernetes-anywhere,brettdh,https://github.com/kubernetes/kubernetes-anywhere/issues/418,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/418,"Configure one host, no resource pool?","Is it possible to configure a single-host server with no resource pool? I have a host, not a cluster, on my vSphere, and no resource pool, and [no ability to create one](https://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=1004098). (I cannot change the config on the VM host.) If I say ""no"" to `phase1.vSphere.useresourcepool`, it tries to use one with the same name as the host, which it can't find:
```
* vsphere_virtual_machine.kubevm1: resource pool 'foo.example.com' not found
```",closed,False,2017-07-28 20:49:19,2017-12-04 13:56:38
kubernetes-anywhere,dayglo,https://github.com/kubernetes/kubernetes-anywhere/issues/419,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/419,vmware esxi 5.5 - ova hardware version,"Hi,

We'd like to use k-a on esxi 5.5 which only supports hardware version 10.

Is there any reason you build your OVA template using vmware hardware version 11?

Do you build the OVA with packer or similar? If so please could you share the source?",closed,False,2017-08-03 13:58:03,2018-01-03 02:39:14
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/420,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/420,Deploy multiple clusters from same workspace,"This PR enables deploying multiple clusters from the same workspace without affecting each other.
- .tmp directory is moved to a directory with name <cluster_name>
- kubeconfig.json is stored in <cluster_name> directory instead of .tmp directory.

We could bring-up multiple clusters with the changes in this PR sequentially, bringing them up in parallel needs some more modifications.

This is done as part of effort to migrate federation cluster bring-up method to k8s-anywhere   Ref: https://github.com/kubernetes/test-infra/issues/3858

The environment supposed to be used in test-infra is `gce+kubeadm+weavenet` and this PR has been tested in this environment.
The other 2 environments azure and vsphere has not been tested, but are minor changes and can be reviewed.

/assign @pipejakob 
/assign @luxas 
/cc @madhusudancs ",closed,True,2017-08-07 09:19:00,2017-08-15 06:40:05
kubernetes-anywhere,vmtocloud,https://github.com/kubernetes/kubernetes-anywhere/issues/421,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/421,Add and Remove nodes?,I can't figure out how to add or remove nodes from the cluster. I tried changing make config node count but it just redeploys the whole cluster again and I lose all my pods. Is this supported or feature request?,closed,False,2017-08-09 22:51:59,2018-05-11 17:31:52
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/issues/422,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/422,Allow cluster to be deployed on specific VM Folder on vCenter,"Currently Kubernetes-Anywhere asks user to specify the name of the cluster and then uses this name to create vm folder on the root. Node VMs are then placed on this folder.

Customer requested to prompt menu option to specify VM folder path where cluster needs to be deployed.

@tusharnt
 

",closed,False,2017-08-10 00:13:26,2017-08-12 23:26:20
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/423,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/423,Corrected gce README,"An instruction was missing and another was obsolete and unwanted. so fixed the README.
Also removed couple of redundant files.

/cc @pipejakob 
/assign @madhusudancs ",closed,True,2017-08-11 05:23:37,2017-08-15 05:35:05
kubernetes-anywhere,psyolent,https://github.com/kubernetes/kubernetes-anywhere/issues/424,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/424,Validation: Expected 5 healthy nodes; found 0. (660s elapsed),"hey

so getting thru deploying this in a secured work environment with VCSA 6.5.1, and vSphere in 5.5u3, using vDS in this particular instance. 

i get the cluster and nodes deployed but get to a point where it goes to discover nodes and master ; and ; never finds a thing, not a master, not a node, zilcho, all i get is an infinite supply of Validation: Expected 5 healthy nodes; found 0. (xxxxxxxs elapsed)

what action is occuring when its doing this and can i manually try to invoke it?

thanks heaps",closed,False,2017-08-11 12:09:07,2018-05-12 19:57:50
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/425,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/425,Allow user to specify VM folder Path where Node VMs needs to be placed,"Fixes: https://github.com/kubernetes/kubernetes-anywhere/issues/422

Currently Kubernetes-Anywhere asks user to specify the name of the cluster and then uses this name to create vm folder on the root. Node VMs are then placed on this folder.

Customer requested to prompt menu option to specify VM folder path where cluster needs to be deployed.

This PR addressed above customer request.

**New menu options looks like as below.**


`VM Folder name or Path (e.g kubernetes, VMFolder1/dev-cluster, VMFolder1/Test Group1/test-cluster) (phase1.vSphere.vmfolderpath) [kubernetes] (NEW) divyen/kubernetes`

**Testing:**

`.phase1.vSphere.vmfolderpath=""divyenp/kubernetes/Test Cluster""`
In this case 
VM Folders divyenp and sub folders were not present, folder structure is created by kubernetes-anywhere and cluster deployed successfully


`.phase1.vSphere.vmfolderpath=""divyenp/kubernetes""`
In this case VM Folder: divyenp/kubernetes was already present. Cluster deployed successfully.


` .phase1.vSphere.vmfolderpath=""divyenp/kubernetes/Test Cluster""`
In this case VM Folder :divyenp/kubernetes was present, but folder ""Test Cluster"" (with space) did not exist, this folder is created by kubernetes-anywhere and cluster deployed successfully.


Please review : @rohitjogvmw @BaluDontu @luomiao @tusharnt
",closed,True,2017-08-11 17:16:12,2017-08-14 17:01:44
kubernetes-anywhere,psyolent,https://github.com/kubernetes/kubernetes-anywhere/issues/426,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/426,kubernetes-dashboard - 'Unauthorized',"hey

next issue. trying to get to the dashboard for kubernetes i've deployed, i just get 'unauthorized'
i thought it might have been SSL non trusted cert related ; but no
so 192.168.0.33 is the IP of my master ; i have 4 nodes.

`[root@docau023adh00 ~]# curl --insecure  https://192.168.0.33
Unauthorized
`
`
[root@docau023adh00 ~]# kubectl logs kubernetes-dashboard-2315583659-5pzzv --namespace=kube-system
Using HTTP port: 8443
Using in-cluster config to connect to apiserver
Using service account token for csrf signing
No request provided. Skipping authorization header
Successful initial request to the apiserver, version: v1.6.5
No request provided. Skipping authorization header
Creating in-cluster Heapster client
Successful initial request to heapster
`
everything happily running ; although i do note that in this kubernetes-dashboard has no 'external' IP address ; 
`
[root@docau023adh00 ~]# kubectl get services -a -o wide --all-namespaces
NAMESPACE     NAME                   CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE       SELECTOR
default       kubernetes             10.0.0.1       <none>        443/TCP         18h       <none>
kube-system   heapster               10.0.243.78    <none>        80/TCP          18h       k8s-app=heapster
kube-system   kube-dns               10.0.0.10      <none>        53/UDP,53/TCP   18h       k8s-app=kube-dns
kube-system   kubernetes-dashboard   10.0.190.110   <none>        80/TCP          13m       k8s-app=kubernetes-dashboard
`

thoughts/comments welcome / appreciated heaps!",closed,False,2017-08-12 06:46:19,2018-03-10 01:25:33
kubernetes-anywhere,mbssaiakhil,https://github.com/kubernetes/kubernetes-anywhere/pull/427,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/427,Fix Typo,Fix Typo in Kubernetes Anywhere Original Design Proposal,closed,True,2017-08-13 10:08:14,2017-08-25 21:03:39
kubernetes-anywhere,jgmize,https://github.com/kubernetes/kubernetes-anywhere/pull/428,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/428,Fix util/validate to match Ready without matching NotReady,,closed,True,2017-08-14 16:45:38,2017-09-07 19:04:36
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/429,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/429,Deploy multiple clusters in parallel and few other fixes,"This is done as part of effort to migrate federation cluster bring-up method to k8s-anywhere Ref: kubernetes/test-infra#3858

This PR mainly does 3 things.
- Configurable username for sshing to deployed OS. This was an issue while testing out this PR and this is one way to solve the issue as discussed in https://github.com/kubernetes/kubeadm/issues/219#issuecomment-293110132
- Configurable context names in kubeconfig. This is required since we deploy multiple clusters and then merge them to one kubeconfig.
- Deploy multiple clusters in parallel by specifying different config files like below:
```
# make CONFIG_FILE=.config-c1 deploy &
# make CONFIG_FILE=.config-c2 deploy
```

/assign @pipejakob
/assign @madhusudancs 
/assign @luxas

p.s: After the merging of this pr, we can takeup updating kubernetes-anywhere deployer in `kubetest` to be able to deploy multiple clusters.",closed,True,2017-08-15 07:22:56,2017-08-23 17:16:09
kubernetes-anywhere,avirat28,https://github.com/kubernetes/kubernetes-anywhere/issues/430,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/430,Cannot complete login due to an incorrect user name or password.,"Hi,

I am attempting to deploy a K8 cluster using kubernetes-anywhere however not able to
connect to vCenter due to wrong user/pass. I am using the same user/pass that I use to
login directly to vCenter thru web. What am I missing?

docker run -it --rm --env=""PS1=[container]:\w> "" --net=host cnastorage/kubernetes-anywhere:latest /bin/bash
[container]:/opt/kubernetes-anywhere> make deploy
make config
make[1]: Entering directory '/opt/kubernetes-anywhere'
CONFIG_=""."" kconfig-conf Kconfig
*
* Kubernetes Minimal Turnup Configuration
*
*
* Phase 1: Cluster Resource Provisioning
*
number of nodes (phase1.num_nodes) [4] (NEW)
kubernetes cluster name (phase1.cluster_name) [kubernetes] (NEW)
*
* cloud provider: gce, azure or vsphere
*
cloud provider: gce, azure or vsphere (phase1.cloud_provider) [gce] (NEW) vsphere
  *
  * vSphere configuration
  *
  vCenter URL Ex: 10.192.10.30 or myvcenter.io (without https://) (phase1.vSphere.url) [] (NEW) 100.115.254.45
  vCenter port (phase1.vSphere.port) [443] (NEW)
  vCenter username (phase1.vSphere.username) [] (NEW) administrator@vsphere.local
  vCenter password (phase1.vSphere.password) [] (NEW) password
  Does host use self-signed cert (phase1.vSphere.insecure) [Y/n/?] (NEW)
  Datacenter (phase1.vSphere.datacenter) [datacenter] (NEW) Cloud-DC
  Datastore (phase1.vSphere.datastore) [datastore] (NEW) datastore1
  Deploy Kubernetes Cluster on 'host' or 'cluster' (phase1.vSphere.placement) [cluster] (NEW) host
    host (phase1.vSphere.host) [] (NEW) 100.115.254.44
  Do you want to use the resource pool created on the host or cluster? [yes, no] (phase1.vSphere.useresourcepool) [no] (NEW) yes
    Name of the Resource Pool. If Resource pool is enclosed within another Resource pool, specify pool hierarchy as ParentResourcePool/ChildResourcePool (phase1.vSphere.resourcepool) [] (NEW) dev-resource-pool
  Number of vCPUs for each VM (phase1.vSphere.vcpu) [1] (NEW) 4
  Memory for VM (phase1.vSphere.memory) [2048] (NEW) 4096
  Network for VM (phase1.vSphere.network) [VM Network] (NEW) 100.115.254.0/25
  Name of the template VM imported from OVA. If Template file is not available at the destination location specify vm path (phase1.vSphere.template) [KubernetesAnywhereTemplatePhotonOS.ova] (NEW)
  Flannel Network (phase1.vSphere.flannel_net) [172.1.0.0/16] (NEW)
*
* Phase 2: Node Bootstrapping
*
installer container (phase2.installer_container) [docker.io/cnastorage/k8s-ignition:v2] (NEW)
docker registry (phase2.docker_registry) [gcr.io/google-containers] (NEW)
kubernetes version (phase2.kubernetes_version) [v1.6.5] (NEW)
bootstrap provider (phase2.provider) [ignition] (NEW)
*
* Phase 3: Deploying Addons
*
Run the addon manager? (phase3.run_addons) [Y/n/?] (NEW)
  Run kube-proxy? (phase3.kube_proxy) [Y/n/?] (NEW)
  Run the dashboard? (phase3.dashboard) [Y/n/?] (NEW)
  Run heapster? (phase3.heapster) [Y/n/?] (NEW)
  Run kube-dns? (phase3.kube_dns) [Y/n/?] (NEW)
  Run weave-net? (phase3.weave_net) [N/y/?] (NEW)
#
# configuration written to .config
#
make[1]: Leaving directory '/opt/kubernetes-anywhere'
util/config_to_json .config > .config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/$(jq -r '.phase1.cloud_provider' .config.json)""; ./do deploy-cluster )
.tmp/vSphere-kubernetes.tf
data.template_file.cloudprovider: Refreshing state...
Error running plan: 1 error(s) occurred:

* provider.vsphere: Error setting up client: ServerFaultCode: Cannot complete login due to an incorrect user name or password.
Makefile:63: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:41: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2",closed,False,2017-08-15 22:16:14,2017-08-24 16:31:58
kubernetes-anywhere,jessicaochen,https://github.com/kubernetes/kubernetes-anywhere/issues/431,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/431,Cluster Upgrade,"Since k8s anywhere already covers lifecycle events of deploy and destroy clusters, how about adding a make command of upgrade-cluster that can upgrade clusters? I am probing community interest (and/or objections).

I am specifically interested in getting this working for kubeadm/gce and am working on prototypes/designs. I do plan on a design that should be generic enough to work easily with other phase 1 and phase 2 providers.",closed,False,2017-08-19 00:13:46,2018-01-05 18:00:15
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/432,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/432,Add OpenStack to phase1.providers,"Adding the support for k8s deployments on OpenStack. This has been tested on openstack + kubeadm.
Only last 2 commits are relevant and rest of the commits are from another PR and hoping them to soon get merged.

/cc @cmluciano @luxas @pipejakob @kubernetes/kubernetes-anywhere-maintainers ",closed,True,2017-08-22 13:31:59,2017-10-20 18:50:50
kubernetes-anywhere,prateekgogia,https://github.com/kubernetes/kubernetes-anywhere/issues/433,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/433,Steps for Getting started on GCE unclear,"In the [getting started guide](https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase1/gce/README.md) for GCE, it mentions the following - 

> You will be logged into your dev shell:
> If kubeadm is the phase2.provider, then login to gcloud by calling:
> $ gcloud auth login

I was able to reach to the point, where it takes you to the dev shell. However, in the next step it is not clear _`If kubeadm is the phase2.provider`_ or something else and there is no way to check which provider is it. If I skip this step, it errors out saying - 

```
Error asking for user input: 1 error(s) occurred:

* provider.google: file: open account.json: no such file or directory in:

`${file(""account.json"")}`
```

",closed,False,2017-08-22 18:41:04,2018-03-11 13:00:34
kubernetes-anywhere,prateekgogia,https://github.com/kubernetes/kubernetes-anywhere/issues/434,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/434,make deploy fails for GCE,"Trying to create a new cluster on GCE, when running make deploy. It fails with the following error- 
```

[container]:/opt/kubernetes-anywhere> make deploy
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/gce""; ./do deploy-cluster )
+ cd .
++ jq -r .phase1.cluster_name ../../.config.json
+ CLUSTER_NAME=kubernetes
+ TMP_DIR=kubernetes/.tmp
+ case ""${1:-}"" in
+ deploy
+ gen
++ generate_token
++ perl -e 'printf ""%06x.%08x%08x\n"", rand(0xffffff), rand(0xffffffff), rand(0xffffffff);'
+ TOKEN=1747d9.0043a6173b307df2
+ mkdir -p kubernetes/.tmp
+ jsonnet -J ../../ --multi kubernetes/.tmp all.jsonnet
kubernetes/.tmp/gce-kubernetes.tf
+ echo 'kubeadm_token = ""1747d9.0043a6173b307df2""'
+ terraform apply -var-file=kubernetes/terraform.tfvars -state=kubernetes/terraform.tfstate kubernetes/.tmp
1 error(s) occurred:

* module root: 9 error(s) occurred:

* resource 'google_compute_instance.kubernetes-master' config: unknown resource 'tls_private_key.kubernetes-master' referenced in variable tls_private_key.kubernetes-master.private_key_pem
* resource 'google_compute_instance.kubernetes-master' config: unknown resource 'tls_locally_signed_cert.kubernetes-master' referenced in variable tls_locally_signed_cert.kubernetes-master.cert_pem
* resource 'google_compute_instance.kubernetes-master' config: unknown resource 'tls_self_signed_cert.kubernetes-root' referenced in variable tls_self_signed_cert.kubernetes-root.cert_pem
* resource 'null_resource.kubeconfig' provisioner local-exec (#1): unknown resource 'tls_self_signed_cert.kubernetes-root' referenced in variable tls_self_signed_cert.kubernetes-root.cert_pem
* resource 'null_resource.kubeconfig' provisioner local-exec (#1): unknown resource 'tls_locally_signed_cert.kubernetes-admin' referenced in variable tls_locally_signed_cert.kubernetes-admin.cert_pem
* resource 'null_resource.kubeconfig' provisioner local-exec (#1): unknown resource 'tls_private_key.kubernetes-admin' referenced in variable tls_private_key.kubernetes-admin.private_key_pem
* resource 'google_compute_instance_template.kubernetes-node-instance-template' config: unknown resource 'tls_self_signed_cert.kubernetes-root' referenced in variable tls_self_signed_cert.kubernetes-root.cert_pem
* resource 'google_compute_instance_template.kubernetes-node-instance-template' config: unknown resource 'tls_locally_signed_cert.kubernetes-node' referenced in variable tls_locally_signed_cert.kubernetes-node.cert_pem
* resource 'google_compute_instance_template.kubernetes-node-instance-template' config: unknown resource 'tls_private_key.kubernetes-node' referenced in variable tls_private_key.kubernetes-node.private_key_pem
Makefile:67: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:45: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
```",closed,False,2017-08-22 19:04:11,2017-08-29 21:25:07
kubernetes-anywhere,avirat28,https://github.com/kubernetes/kubernetes-anywhere/pull/435,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/435,Fixed minor typo,,closed,True,2017-08-23 04:04:04,2017-08-23 20:40:19
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/436,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/436,Follow-up pr to fix comments in #429,"This is a simple pr to fix-up the comments left out in #429.

/assign @pipejakob",closed,True,2017-08-24 08:29:52,2017-08-25 18:13:22
kubernetes-anywhere,rjog,https://github.com/kubernetes/kubernetes-anywhere/pull/437,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/437,UX improvements while deploying k8s on vsphere ,"Few UX issues are mentioned @ https://github.com/vmware/kubernetes/issues/249

Below issues addressed:

1. Where ever vCenter Inventory Resource is automatically created by Kubernetes-Anywhere (For example Folder) specify in the prompt, that object will be created if not present in the inventory. And for objects which needs to be present and kubernetes-anywhere can not create, specify in the user prompt with text - ""Existing"", so user knows that this object should be already present in the inventory. (For example Resource Pool, which can not be created by Kubernetes-Anywhere)

2. Put a note that all ESX host should have time in sync, else some of the node can remain in pending state for ever due to certificate expired certificate. Node VM inherits time from the esx host.

3. At present node VMs has sequence node1, node2 ..., instead append some unique char sequence, so that multiple clusters can be deployed in the same working directory folder. with this change <k8s-cluster-name>-master, <k8s-cluster-name>-node1 .... VM names will be used.

4. When user specifies that he wants to deploy 4 node cluster, in the end Kubernetes-Anywhere is waiting for 5 nodes to become healthy. This is because 5th node is the master node, specify this in the message so that user does not get confused.

This also fixes https://github.com/kubernetes/kubernetes-anywhere/issues/375

@divyenpatel, @BaluDontu, @tusharnt, @luomiao 
",closed,True,2017-08-24 22:51:04,2017-08-31 23:56:00
kubernetes-anywhere,divyenpatel,https://github.com/kubernetes/kubernetes-anywhere/pull/438,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/438,Updating Kubernetes-Anywhere getting started guide for vSphere,"Updating K8S-Anywhere Getting Started Guide for vSphere based on the Menu Option changes made in the PR: https://github.com/kubernetes/kubernetes-anywhere/pull/437

Issue: https://github.com/vmware/kubernetes/issues/249

Summary of changes:

- In the instruction for running deployment container mounted local /tmp directory so that kubeconfig.json file can be copied to the local machine easily.
- Updated Menu Options and briefly describe each menu
- Added OVA for vSphere 5.5
- In cnastorage/kubernetes-anywhere image made vsphere as default provider.


New Image is uploaded at `cnastorage/kubernetes-anywhere:08252017`

Once this PR is approved, I will tag `cnastorage/kubernetes-anywhere:08252017` to `cnastorage/kubernetes-anywhere:latest`

Please review: @rohitjogvmw, @BaluDontu, @tusharnt, @luomiao @msterin ",closed,True,2017-08-25 23:50:09,2017-08-31 23:30:49
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/439,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/439,Fix jq handling of missing fields.,"If a field is completely missing, then this invocation prints out the string `null`:

    $ echo $(jq -r '.phase1.ssh_user' .config.json)

The result is that our `if [[ -n ""$VAR"" ]]` checks will actually see the string `null` instead of an empty string.

Piping the query through the ""values"" filter will filter out null values and produce the desired empty output, which fixes the conditionals:

    $ echo $(jq -r '.phase1.ssh_user | values' .config.json)",closed,True,2017-08-29 00:07:06,2017-08-29 21:25:43
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/440,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/440,"If phase1.ssh_user isn't set, also try to use $USER.","If the .config file is missing `phase1.ssh_user`, we should still attempt to explicitly use `$USER@host` to avoid the poorly documented default behavior of gcloud looking up the user from the service account (which is wrong, in prow's case):

https://github.com/kubernetes/kubeadm/issues/219

If `$USER` is unset, like it is in the `make docker-dev` environment, then we can still fall back to omitting the `user@` portion of the ssh command. But, this will make it a little friendlier to use in other environments.",closed,True,2017-08-29 00:45:07,2017-08-30 17:28:00
kubernetes-anywhere,pipejakob,https://github.com/kubernetes/kubernetes-anywhere/pull/441,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/441,Fix binding of TLS resources for ignition.,"`gce.jsonnet` was recently refactored to skip elements that aren't needed based on the `phase2.provider`. However, based on jsonnet's parsing, the second of the two `if` statements was being appending to the `else { }` block of the previous `if`, and so it was being short-circuited entirely instead of independently evaluated.

The intent of the code was to logically do:

    if condition1 {
      append(X)
    }
    if condition2 {
      append(Y)
    }

but based on jsonnet's list append binding, it was actually doing:

    if condition1 {
      append(X)
    }
    else if condition2 {
      append(Y)
    }

Since `condition1` and `condition2` are identical here, rather than two separate `if`s I've merged them into a single one. I've tested to ensure that in the case of ignition, both the `null_resource` and TLS resources are included, and in the case of kubeadm, both are excluded.

This fixes https://github.com/kubernetes/kubernetes-anywhere/issues/434.",closed,True,2017-08-29 02:18:49,2017-08-29 21:23:39
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/442,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/442,Fix deployments on gce ignition,"This pr should fix the issue described in #434

/cc @prateekgogia
/assign @pipejakob ",closed,True,2017-08-29 06:41:26,2017-08-29 06:50:15
kubernetes-anywhere,marrik96,https://github.com/kubernetes/kubernetes-anywhere/issues/443,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/443,k8s master on vsphere storage usage,"Master node continues to use up all drive space.  Is it logs?  what can I delete?

I've had to expand the VMDK and partition on master to get it back and operational due to 0% drive space left.",closed,False,2017-08-31 15:55:14,2018-03-10 19:43:33
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/444,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/444,Remove Azure as phase1 provider,"Following the discussion in https://github.com/kubernetes/kubernetes-anywhere/issues/366. Removing Azure provider as it is already broken and is no longer used or maintained.
After this PR merged, it will be one step closer to move towards `kubeadm` based deployments and only vsphere will be using ignition phase2 provider.

/assign @pipejakob 
/cc @luxas ",closed,True,2017-09-06 01:24:05,2017-09-13 00:17:26
kubernetes-anywhere,luxas,https://github.com/kubernetes/kubernetes-anywhere/pull/445,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/445,"Make validate only allow Ready nodes, not NotReady ones","@shashidharatd @pipejakob @jessicaochen @roberthbailey 
Fixes the incorrect behavior we noted earlier",closed,True,2017-09-07 06:21:34,2017-09-12 14:58:35
kubernetes-anywhere,jamiehannaford,https://github.com/kubernetes/kubernetes-anywhere/pull/446,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/446,Add flannel,Supersedes #390 ,closed,True,2017-09-07 09:47:36,2017-09-21 03:19:05
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/447,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/447,Fix validate for node readiness,"Current implementation fails to check if node is in ready condition after deployment.
To fix the issue, the validate script is updated to check for both cluster-readiness and node-readiness.

/assign @pipejakob 
/cc @luxas ",closed,True,2017-09-08 12:54:08,2017-09-12 05:36:26
kubernetes-anywhere,scundall,https://github.com/kubernetes/kubernetes-anywhere/issues/448,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/448,sphere_virtual_machine.kubevm1: 1 error(s) occurred: vsphere_virtual_machine.kubevm3: unexpected EOF while creating VMs,"5 error(s) occurred:

* vsphere_virtual_machine.kubevm3: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm3: unexpected EOF
* vsphere_virtual_machine.kubevm2: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm2: unexpected EOF
* vsphere_virtual_machine.kubevm5: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm5: unexpected EOF
* vsphere_virtual_machine.kubevm1: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm1: unexpected EOF
* vsphere_virtual_machine.kubevm4: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm4: unexpected EOF

Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.
panic: runtime error: invalid memory address or nil pointer dereference
2017/09/13 07:49:39 [DEBUG] plugin: terraform: [signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0x3ddadbf]
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 
2017/09/13 07:49:39 [DEBUG] plugin: terraform: goroutine 89 [running]:
2017/09/13 07:49:39 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/vendor/github.com/vmware/govmomi/object.DistributedVirtualPortgroup.EthernetCardBackingInfo(0xc4203b9400, 0xc4209cd6c0, 0x1b, 0xc4207d8790, 0x10, 0xc420483460, 0x1e, 0x7f0ab066a000, 0xc4200563b8, 0x490fe80, ...)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/vendor/github.com/vmware/govmomi/object/distributed_virtual_portgroup.go:52 +0x24f
2017/09/13 07:49:39 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/vendor/github.com/vmware/govmomi/object.(*DistributedVirtualPortgroup).EthernetCardBackingInfo(0xc420855500, 0x7f0ab066a000, 0xc4200563b8, 0x7f0ab066a000, 0xc4200563b8, 0x79e0ec0, 0xc420855500)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	<autogenerated>:446 +0x87
2017/09/13 07:49:39 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/builtin/providers/vsphere.buildNetworkDevice(0xc4208a5bc0, 0xc4207d87a4, 0x7, 0x5064801, 0x7, 0x0, 0x0, 0x0, 0x1b, 0x79d03c0)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/builtin/providers/vsphere/resource_vsphere_virtual_machine.go:1472 +0x157
2017/09/13 07:49:39 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/builtin/providers/vsphere.(*virtualMachine).setupVirtualMachine(0xc420010300, 0xc42088ec90, 0x5, 0x42ff360)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/builtin/providers/vsphere/resource_vsphere_virtual_machine.go:1833 +0xde9
2017/09/13 07:49:39 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/builtin/providers/vsphere.resourceVSphereVirtualMachineCreate(0xc420314380, 0x4f70780, 0xc42088ec90, 0x0, 0x0)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/builtin/providers/vsphere/resource_vsphere_virtual_machine.go:900 +0x17f7
2017/09/13 07:49:39 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/helper/schema.(*Resource).Apply(0xc4203c7920, 0xc420280be0, 0xc4207c0de0, 0x4f70780, 0xc42088ec90, 0x1, 0x28, 0xc4208a8a50)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/helper/schema/resource.go:186 +0x48d
2017/09/13 07:49:39 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/helper/schema.(*Provider).Apply(0xc42031eaf0, 0xc420280b90, 0xc420280be0, 0xc4207c0de0, 0x7f0ab0866000, 0x0, 0x0)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/helper/schema/provider.go:242 +0x9b
2017/09/13 07:49:39 [DEBUG] plugin: terraform: github.com/hashicorp/terraform/plugin.(*ResourceProviderServer).Apply(0xc420870fe0, 0xc4207c0d40, 0xc4207d8980, 0x0, 0x0)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/gopath/src/github.com/hashicorp/terraform/plugin/resource_provider.go:488 +0x57
2017/09/13 07:49:39 [DEBUG] plugin: terraform: reflect.Value.call(0xc4203c0120, 0xc420797e98, 0x13, 0x505c04a, 0x4, 0xc4206d4f20, 0x3, 0x3, 0x0, 0x0, ...)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/go/src/reflect/value.go:434 +0x91f
2017/09/13 07:49:39 [DEBUG] plugin: terraform: reflect.Value.Call(0xc4203c0120, 0xc420797e98, 0x13, 0xc4205d7720, 0x3, 0x3, 0x0, 0x0, 0x0)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/go/src/reflect/value.go:302 +0xa4
2017/09/13 07:49:39 [DEBUG] plugin: terraform: net/rpc.(*service).call(0xc420839d80, 0xc420839d40, 0xc420845080, 0xc420573300, 0xc420726440, 0x41f8a80, 0xc4207c0d40, 0x16, 0x41f8ac0, 0xc4207d8980, ...)
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/go/src/net/rpc/server.go:387 +0x144
2017/09/13 07:49:39 [DEBUG] plugin: terraform: created by net/rpc.(*Server).ServeCodec
2017/09/13 07:49:39 [DEBUG] plugin: terraform: 	/opt/go/src/net/rpc/server.go:481 +0x404
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteState
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalApplyProvisioners
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalIf
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteState
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteDiff
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalApplyPost
2017/09/13 07:49:39 [ERROR] root: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm3: unexpected EOF
2017/09/13 07:49:39 [ERROR] root: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm3: unexpected EOF
2017/09/13 07:49:39 [TRACE] [walkApply] Exiting eval tree: vsphere_virtual_machine.kubevm3
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteState
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalApplyProvisioners
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalIf
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteState
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteDiff
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalApplyPost
2017/09/13 07:49:39 [ERROR] root: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm2: unexpected EOF
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteState
2017/09/13 07:49:39 [ERROR] root: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm2: unexpected EOF
2017/09/13 07:49:39 [TRACE] [walkApply] Exiting eval tree: vsphere_virtual_machine.kubevm2
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalApplyProvisioners
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalIf
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteState
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteState
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteDiff
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalApplyProvisioners
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalIf
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteState
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteDiff
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalApplyPost
2017/09/13 07:49:39 [ERROR] root: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm1: unexpected EOF
2017/09/13 07:49:39 [ERROR] root: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm1: unexpected EOF
2017/09/13 07:49:39 [TRACE] [walkApply] Exiting eval tree: vsphere_virtual_machine.kubevm1
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""tls_cert_request.kubernetes-master""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""tls_locally_signed_cert.kubernetes-master""
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalApplyPost
2017/09/13 07:49:39 [ERROR] root: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm5: unexpected EOF
2017/09/13 07:49:39 [ERROR] root: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm5: unexpected EOF
2017/09/13 07:49:39 [TRACE] [walkApply] Exiting eval tree: vsphere_virtual_machine.kubevm5
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""provider.tls (close)""
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteState
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalApplyProvisioners
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalIf
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteState
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalWriteDiff
2017/09/13 07:49:39 [DEBUG] root: eval: *terraform.EvalApplyPost
2017/09/13 07:49:39 [ERROR] root: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm4: unexpected EOF
2017/09/13 07:49:39 [ERROR] root: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* vsphere_virtual_machine.kubevm4: unexpected EOF
2017/09/13 07:49:39 [TRACE] [walkApply] Exiting eval tree: vsphere_virtual_machine.kubevm4
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""data.template_file.configure_master""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""null_resource.kubernetes-master""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""provisioner.local-exec (close)""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""provider.vsphere (close)""
2017/09/13 07:49:39 [DEBUG] plugin: /bin/terraform: plugin process exited
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""data.template_file.configure_node""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""null_resource.kubernetes-node2""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""null_resource.kubernetes-node5""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""null_resource.kubernetes-node3""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""provider.template (close)""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""null_resource.kubernetes-node4""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""provider.null (close)""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""provisioner.remote-exec (close)""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""meta.count-boundary (count boundary fixup)""
2017/09/13 07:49:39 [DEBUG] dag/walk: upstream errored, not walking ""root""
2017/09/13 07:49:39 [TRACE] Preserving existing state lineage ""c23257d2-4bb4-462b-bfcb-e3e0c7b2cb7f""
2017/09/13 07:49:39 [DEBUG] plugin: waiting for all plugin processes to complete...
2017/09/13 07:49:39 [WARN] plugin: error closing client during Kill: connection is shut down
2017/09/13 07:49:39 [DEBUG] plugin: terraform: local-exec-provisioner (internal) 2017/09/13 07:49:39 [ERR] plugin: plugin server: accept unix /tmp/plugin751621487: use of closed network connection
2017/09/13 07:49:39 [DEBUG] plugin: terraform: template-provider (internal) 2017/09/13 07:49:39 [ERR] plugin: plugin server: accept unix /tmp/plugin453119946: use of closed network connection
2017/09/13 07:49:39 [DEBUG] plugin: terraform: template-provider (internal) 2017/09/13 07:49:39 [DEBUG] plugin: waiting for all plugin processes to complete...
2017/09/13 07:49:39 [DEBUG] plugin: terraform: tls-provider (internal) 2017/09/13 07:49:39 [ERR] plugin: plugin server: accept unix /tmp/plugin169909967: use of closed network connection
2017/09/13 07:49:39 [DEBUG] plugin: terraform: null-provider (internal) 2017/09/13 07:49:39 [ERR] plugin: plugin server: accept unix /tmp/plugin179249613: use of closed network connection
2017/09/13 07:49:39 [DEBUG] plugin: terraform: null-provider (internal) 2017/09/13 07:49:39 [DEBUG] plugin: waiting for all plugin processes to complete...
2017/09/13 07:49:39 [DEBUG] plugin: terraform: local-exec-provisioner (internal) 2017/09/13 07:49:39 [DEBUG] plugin: waiting for all plugin processes to complete...
2017/09/13 07:49:39 [DEBUG] plugin: terraform: tls-provider (internal) 2017/09/13 07:49:39 [DEBUG] plugin: waiting for all plugin processes to complete...
2017/09/13 07:49:39 [DEBUG] plugin: /bin/terraform: plugin process exited
2017/09/13 07:49:39 [DEBUG] plugin: /bin/terraform: plugin process exited
2017/09/13 07:49:39 [DEBUG] plugin: /bin/terraform: plugin process exited
2017/09/13 07:49:39 [DEBUG] plugin: /bin/terraform: plugin process exited
2017/09/13 07:49:39 [DEBUG] plugin: terraform: remote-exec-provisioner (internal) 2017/09/13 07:49:39 [DEBUG] plugin: waiting for all plugin processes to complete...
2017/09/13 07:49:39 [DEBUG] plugin: /bin/terraform: plugin process exited



!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!

Terraform crashed! This is always indicative of a bug within Terraform.
A crash log has been placed at ""crash.log"" relative to your current
working directory. It would be immensely helpful if you could please
report the crash with Terraform[1] so that we can fix this.

When reporting bugs, please include your terraform version. That
information is available on the first line of crash.log. You can also
get it by running 'terraform --version' on the command line.

[1]: https://github.com/hashicorp/terraform/issues

!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!
Makefile:70: recipe for target 'do' failed
make[1]: *** [do] Error 1
",closed,False,2017-09-13 07:54:22,2018-03-11 17:04:34
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/449,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/449,Enable cloud-provider in GCE based deployments.,"This PR is in continuation to migrate deployment mechanism used by federation to k8s-anywhere as listed out in https://github.com/kubernetes/test-infra/issues/3858. This is the 5th task in that series.

Federation CI jobs needs k8s clusters deployed with cloud-provider enabled for load-balancer type services and persistent volumes.

/cc @luxas @pipejakob @madhusudancs @kubernetes/sig-federation-pr-reviews ",closed,True,2017-09-13 09:04:43,2017-10-19 20:09:58
kubernetes-anywhere,jessicaochen,https://github.com/kubernetes/kubernetes-anywhere/pull/450,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/450,Add upgrade logic to kubernetes anywhere,"Issue #431 

Adds structure for upgrades and fills in the upgrade implementation for kubeadm on gce.

Design doc: https://docs.google.com/a/google.com/document/d/1PoDeqD8qu1KbAd-fb7xOVB1G7YTyD6VLLlhaVB3QeA4/edit?usp=sharing
",closed,True,2017-09-13 21:35:12,2017-09-27 17:18:34
kubernetes-anywhere,jessicaochen,https://github.com/kubernetes/kubernetes-anywhere/pull/451,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/451,Allow configuration for a kublet version that differs from kubeadm,"Prior to this change, the kubelet on the master and nodes would always be at the same version as kubeadm. Breaking the version of the kubelet away from the version of kubeadm gives the flexibility of starting up consistent clusters that are at an older version than the kubeadm binary.

This helps with e2e testing issues: https://github.com/kubernetes/kubeadm/issues/431

Configuration could be updated to reflect their purpose a bit better eg. kubernetes_version really is the control plane version. Such naming changes are outside the scope of this pull request to focus efforts on fixing things.",closed,True,2017-09-15 23:37:24,2017-09-27 17:17:38
kubernetes-anywhere,luxas,https://github.com/kubernetes/kubernetes-anywhere/pull/452,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/452,Fix small regression in kubeadm gce startup script,"Follows up #451 
Even though the kubelet version was stable, we should download the right kubeadm CLI version
/assign @pipejakob @jessicaochen ",closed,True,2017-09-21 07:31:53,2018-03-16 11:56:44
kubernetes-anywhere,jessicaochen,https://github.com/kubernetes/kubernetes-anywhere/issues/453,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/453,Consistently mktemp to create temporary directories,"Use mktemp -d /tmp/k8s-debsXXXXXX) consistently when creating temp dirs in order to not have conflicts (which would be a mess to debug).

Known locations to fix:
/phase1/gce/configure-vm-kubeadm.sh
/phase2/kubeadm/do",closed,False,2017-09-21 18:02:53,2018-03-11 21:08:33
kubernetes-anywhere,nthaler-mit,https://github.com/kubernetes/kubernetes-anywhere/issues/454,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/454,null_resource.$cluster-master: 1 error(s) occurred,"I'm attempting to deploy a kubernetes cluster against a test vSphere 6.5 U1 endpoint, but fail each time with a null_resource message against the master (potentially during the phase1 cluster deploy phase)

Nothing in the environment appears too usual: hypervisors in sync using strata 1 ntp time sources, shared iSCSI datastore, dvPortgroup backed by a DHCP server, cluster has sufficient resources.    The resulting workers and master DHCP without issue.  

I've tried the deploy from both a PhotonOS and Ubuntu 16.04 system; no difference.

root@XXXX [ ~ ]# docker pull cnastorage/kubernetes-anywhere
Using default tag: latest
latest: Pulling from cnastorage/kubernetes-anywhere
Digest: sha256:e2ee18aa00b3b6f9ea7a6b8d1ee117384cc8bec11f100bc6f9aaca7941c04e45
Status: Image is up to date for cnastorage/kubernetes-anywhere:latest
root@XXXX [ ~ ]# docker run -it -v /tmp:/tmp --rm --env=""PS1=[container]:\w> "" --net=host cnastorage/kubernetes-anywhere:latest /bin/bash


* Phase 1: Cluster Resource Provisioning
*
number of nodes (phase1.num_nodes) [1]
kubernetes cluster name (phase1.cluster_name) [psod]
SSH user to login to OS for provisioning (phase1.ssh_user) []
*
* cloud provider: gce, azure or vsphere
*
cloud provider: gce, azure or vsphere (phase1.cloud_provider) [vsphere]
  *
  * vSphere configuration
  *
  vCenter URL Ex: 10.192.10.30 or myvcenter.io (without https://) (phase1.vSphere.url) [XXXXXX]
  vCenter port (phase1.vSphere.port) [443]
  vCenter username (phase1.vSphere.username) [kub.service@vsphere.local]
  vCenter password (phase1.vSphere.password) [XXXXX]
  Does host use self-signed cert (phase1.vSphere.insecure) [Y/n/?]
  Datacenter (phase1.vSphere.datacenter) [XXXX]
  Datastore (phase1.vSphere.datastore) [iSCSI-LUN0]
  Deploy Kubernetes Cluster on 'host' or 'cluster' (phase1.vSphere.placement) [cluster]
    vsphere cluster name. Please make sure that all the hosts in the cluster are time-synchronized otherwise some of the nodes can remain in pending state for ever due to expired certificate (phase1.vSphere.cluster) [Sandbox]
  Do you want to use the existing resource pool on the host or cluster? [yes, no] (phase1.vSphere.useresourcepool) [no]
  VM Folder name or Path (e.g kubernetes, VMFolder1/dev-cluster, VMFolder1/Test Group1/test-cluster). Folder path will be created if not present (phase1.vSphere.vmfolderpath) [kubernetes]
  Number of vCPUs for each VM (phase1.vSphere.vcpu) [1]
  Memory for VM (phase1.vSphere.memory) [2048]
  Network for VM (phase1.vSphere.network) [dvInternal]
  Name of the template VM imported from OVA. If Template file is not available at the destination location specify vm path (phase1.vSphere.template) [KubernetesAnywhereTemplatePhotonOS.ova]
  Flannel Network (phase1.vSphere.flannel_net) [172.1.0.0/16]
*
* Phase 2: Node Bootstrapping
*
kubernetes version (phase2.kubernetes_version) [v1.6.5]
bootstrap provider (phase2.provider) [ignition]
  installer container (phase2.installer_container) [docker.io/cnastorage/k8s-ignition:v2]
  docker registry (phase2.docker_registry) [gcr.io/google-containers]
*
* Phase 3: Deploying Addons
*
Run the addon manager? (phase3.run_addons) [Y/n/?]
  Run kube-proxy? (phase3.kube_proxy) [Y/n/?]
  Run the dashboard? (phase3.dashboard) [Y/n/?]
  Run heapster? (phase3.heapster) [Y/n/?]
  Run kube-dns? (phase3.kube_dns) [Y/n/?]
  Run weave-net? (phase3.weave_net) [N/y/?]
#
# configuration written to .config

===

make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/vsphere""; ./do deploy-cluster )
psod/.tmp/vSphere-psod.tf
tls_private_key.psod-root: Refreshing state... (ID: 0ba2b3d31bf7a02afc558fe42bd9afd2cad5bc16)
tls_private_key.psod-node: Refreshing state... (ID: 643b42ec6d597289c746ea0efaaa845ab2926573)
tls_private_key.psod-admin: Refreshing state... (ID: b7926dfd99f46c23979c0e7b382664b61db3e2a3)
tls_private_key.psod-master: Refreshing state... (ID: 5ba8fdab299ba466b21763040e2c268c8aae98e5)
tls_self_signed_cert.psod-root: Refreshing state... (ID: 107725442640741129313727620414884837682)
tls_cert_request.psod-node: Refreshing state... (ID: 79530e7f191ad0c96a0481a58b7bdf7a1955e53a)
tls_cert_request.psod-admin: Refreshing state... (ID: d3a4a063528546961cdb0782524e1fe45a5fdd7c)
data.template_file.cloudprovider: Refreshing state...
tls_locally_signed_cert.psod-admin: Refreshing state... (ID: 37005829126929334050474979607585762354)
tls_locally_signed_cert.psod-node: Refreshing state... (ID: 249377147565960010910775338019743846965)
vsphere_folder.cluster_folder: Refreshing state... (ID: XXXXXX/kubernetes)
vsphere_virtual_machine.kubevm1: Creating...
  datacenter:                             """" => ""W92-153""
  detach_unknown_disks_on_delete:         """" => ""false""
  disk.#:                                 """" => ""1""
  disk.3642767325.bootable:               """" => ""true""
  disk.3642767325.controller_type:        """" => ""scsi""
  disk.3642767325.datastore:              """" => ""iSCSI-LUN0""
  disk.3642767325.iops:                   """" => """"
  disk.3642767325.keep_on_remove:         """" => """"
  disk.3642767325.key:                    """" => ""<computed>""
  disk.3642767325.name:                   """" => """"
  disk.3642767325.size:                   """" => """"
  disk.3642767325.template:               """" => ""KubernetesAnywhereTemplatePhotonOS.ova""
  disk.3642767325.type:                   """" => ""thin""
  disk.3642767325.uuid:                   """" => ""<computed>""
  disk.3642767325.vmdk:                   """" => """"
  domain:                                 """" => ""vsphere.local""
  enable_disk_uuid:                       """" => ""true""
  folder:                                 """" => ""kubernetes""
  linked_clone:                           """" => ""false""
  memory:                                 """" => ""2048""
  memory_reservation:                     """" => ""0""
  name:                                   """" => ""XXXXXX-master""
  network_interface.#:                    """" => ""1""
  network_interface.0.ip_address:         """" => ""<computed>""
  network_interface.0.ipv4_address:       """" => ""<computed>""
  network_interface.0.ipv4_gateway:       """" => ""<computed>""
  network_interface.0.ipv4_prefix_length: """" => ""<computed>""
  network_interface.0.ipv6_address:       """" => ""<computed>""
  network_interface.0.ipv6_gateway:       """" => ""<computed>""
  network_interface.0.ipv6_prefix_length: """" => ""<computed>""
  network_interface.0.label:              """" => ""dvInternal""
  network_interface.0.mac_address:        """" => ""<computed>""
  network_interface.0.subnet_mask:        """" => ""<computed>""
  resource_pool:                          """" => ""Sandbox""
  skip_customization:                     """" => ""true""
  time_zone:                              """" => ""Etc/UTC""
  uuid:                                   """" => ""<computed>""
  vcpu:                                   """" => ""1""
vsphere_virtual_machine.kubevm2: Creating...
  datacenter:                             """" => ""XXXXXX""
  detach_unknown_disks_on_delete:         """" => ""false""
  disk.#:                                 """" => ""1""
  disk.3642767325.bootable:               """" => ""true""
  disk.3642767325.controller_type:        """" => ""scsi""
  disk.3642767325.datastore:              """" => ""iSCSI-LUN0""
  disk.3642767325.iops:                   """" => """"
  disk.3642767325.keep_on_remove:         """" => """"
  disk.3642767325.key:                    """" => ""<computed>""
  disk.3642767325.name:                   """" => """"
  disk.3642767325.size:                   """" => """"
  disk.3642767325.template:               """" => ""KubernetesAnywhereTemplatePhotonOS.ova""
  disk.3642767325.type:                   """" => ""thin""
  disk.3642767325.uuid:                   """" => ""<computed>""
  disk.3642767325.vmdk:                   """" => """"
  domain:                                 """" => ""vsphere.local""
  enable_disk_uuid:                       """" => ""true""
  folder:                                 """" => ""kubernetes""
  linked_clone:                           """" => ""false""
  memory:                                 """" => ""2048""
  memory_reservation:                     """" => ""0""
  name:                                   """" => ""psod-node1""
  network_interface.#:                    """" => ""1""
  network_interface.0.ip_address:         """" => ""<computed>""
  network_interface.0.ipv4_address:       """" => ""<computed>""
  network_interface.0.ipv4_gateway:       """" => ""<computed>""
  network_interface.0.ipv4_prefix_length: """" => ""<computed>""
  network_interface.0.ipv6_address:       """" => ""<computed>""
  network_interface.0.ipv6_gateway:       """" => ""<computed>""
  network_interface.0.ipv6_prefix_length: """" => ""<computed>""
  network_interface.0.label:              """" => ""dvInternal""
  network_interface.0.mac_address:        """" => ""<computed>""
  network_interface.0.subnet_mask:        """" => ""<computed>""
  resource_pool:                          """" => ""Sandbox""
  skip_customization:                     """" => ""true""
  time_zone:                              """" => ""Etc/UTC""
  uuid:                                   """" => ""<computed>""
  vcpu:                                   """" => ""1""
vsphere_virtual_machine.kubevm1: Still creating... (10s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (10s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (20s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (20s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (30s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (30s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (40s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (40s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (50s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (50s elapsed)
vsphere_virtual_machine.kubevm1: Still creating... (1m0s elapsed)
vsphere_virtual_machine.kubevm2: Still creating... (1m0s elapsed)
vsphere_virtual_machine.kubevm2: Creation complete (ID: kubernetes/XXXXXX-node1)
vsphere_virtual_machine.kubevm1: Creation complete (ID: kubernetes/XXXXXX-master)
tls_cert_request.psod-master: Creating...
  cert_request_pem:       """" => ""<computed>""
  dns_names.#:            """" => ""6""
  dns_names.0:            """" => ""XXXXXX-master""
  dns_names.1:            """" => ""kubernetes""
  dns_names.2:            """" => ""kubernetes.default""
  dns_names.3:            """" => ""kubernetes.default.svc""
  dns_names.4:            """" => ""kubernetes.default.svc.local""
  dns_names.5:            """" => ""kubernetes.default.svc.local""
  ip_addresses.#:         """" => ""2""
  ip_addresses.0:         """" => ""192.168.1.23""
  ip_addresses.1:         """" => ""10.0.0.1""
  key_algorithm:          """" => ""RSA""
  private_key_pem:        """" => ""e500946bb7e58d9340a3e121a31567ea6352cbb1""
  subject.#:              """" => ""1""
  subject.0.common_name:  """" => ""psod-master_certificate""
  subject.0.organization: """" => ""kubernetes-anywhere""
tls_cert_request.psod-master: Creation complete (ID: 0aad068b5658eea027f3d2817ce5afdfdf922b64)
tls_locally_signed_cert.XXXXXX-master: Creating...
  allowed_uses.#:        """" => ""3""
  allowed_uses.0:        """" => ""digital_signature""
  allowed_uses.1:        """" => ""server_auth""
  allowed_uses.2:        """" => ""client_auth""
  ca_cert_pem:           """" => ""eef9eaeb30f6c98df8e08188f068e33e12edd3de""
  ca_key_algorithm:      """" => ""RSA""
  ca_private_key_pem:    """" => ""fcecff1466dd6719d01fce7c59725365d2cb9f87""
  cert_pem:              """" => ""<computed>""
  cert_request_pem:      """" => ""314c9517c678a3ed8ef110707400d239f45ad53e""
  early_renewal_hours:   """" => ""0""
  validity_end_time:     """" => ""<computed>""
  validity_period_hours: """" => ""8760""
  validity_start_time:   """" => ""<computed>""
tls_locally_signed_cert.psod-master: Creation complete (ID: 66437661694879197104917205874988962678)
data.template_file.configure_node: Refreshing state...
data.template_file.configure_master: Refreshing state...
null_resource.psod-node2: Creating...
null_resource.psod-master: Creating...
null_resource.psod-master: Provisioning with 'remote-exec'...
null_resource.psod-node2: Provisioning with 'remote-exec'...
null_resource.psod-node2 (remote-exec): Connecting to remote host via SSH...
null_resource.psod-node2 (remote-exec):   Host: 192.168.1.30
null_resource.psod-node2 (remote-exec):   User: root
null_resource.psod-node2 (remote-exec):   Password: true
null_resource.psod-node2 (remote-exec):   Private key: false
null_resource.psod-node2 (remote-exec):   SSH Agent: false
null_resource.psod-master (remote-exec): Connecting to remote host via SSH...
null_resource.psod-master (remote-exec):   Host: 192.168.1.23
null_resource.psod-master (remote-exec):   User: root
null_resource.psod-master (remote-exec):   Password: true
null_resource.psod-master (remote-exec):   Private key: false
null_resource.psod-master (remote-exec):   SSH Agent: false
null_resource.psod-node2 (remote-exec): Connected!
null_resource.psod-node2 (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/flannelc.service to /usr/lib/systemd/system/flannelc.service.
null_resource.psod-master: Still creating... (10s elapsed)
null_resource.psod-node2: Still creating... (10s elapsed)
null_resource.psod-master (remote-exec): Connecting to remote host via SSH...
null_resource.psod-master (remote-exec):   Host: 192.168.1.23
null_resource.psod-master (remote-exec):   User: root
null_resource.psod-master (remote-exec):   Password: true
null_resource.psod-master (remote-exec):   Private key: false
null_resource.psod-master (remote-exec):   SSH Agent: false
null_resource.psod-node2: Still creating... (20s elapsed)
null_resource.psod-master: Still creating... (20s elapsed)
null_resource.psod-master: Still creating... (30s elapsed)
null_resource.psod-node2: Still creating... (30s elapsed)
null_resource.psod-master (remote-exec): Connecting to remote host via SSH...
null_resource.psod-master (remote-exec):   Host: 192.168.1.23
null_resource.psod-master (remote-exec):   User: root
null_resource.psod-master (remote-exec):   Password: true
null_resource.psod-master (remote-exec):   Private key: false
null_resource.psod-master (remote-exec):   SSH Agent: false
null_resource.psod-node2: Still creating... (40s elapsed)
null_resource.psod-master: Still creating... (40s elapsed)
null_resource.psod-master (remote-exec): Connecting to remote host via SSH...
null_resource.psod-master (remote-exec):   Host: 192.168.1.23
null_resource.psod-master (remote-exec):   User: root
null_resource.psod-master (remote-exec):   Password: true
null_resource.psod-master (remote-exec):   Private key: false
null_resource.psod-master (remote-exec):   SSH Agent: false
null_resource.psod-master: Still creating... (50s elapsed)
null_resource.psod-node2: Still creating... (50s elapsed)
null_resource.psod-node2: Still creating... (1m0s elapsed)
null_resource.psod-master: Still creating... (1m0s elapsed)
null_resource.psod-master (remote-exec): Connecting to remote host via SSH...
null_resource.psod-master (remote-exec):   Host: 192.168.1.23
null_resource.psod-master (remote-exec):   User: root
null_resource.psod-master (remote-exec):   Password: true
null_resource.psod-master (remote-exec):   Private key: false
null_resource.psod-master (remote-exec):   SSH Agent: false
null_resource.psod-master (remote-exec): Connected!
null_resource.psod-master (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service.
null_resource.psod-master (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/flanneld.service to /usr/lib/systemd/system/flanneld.service.
null_resource.psod-master: Still creating... (1m10s elapsed)
null_resource.psod-node2: Still creating... (1m10s elapsed)
null_resource.psod-node2: Still creating... (1m20s elapsed)
null_resource.psod-master: Still creating... (1m20s elapsed)
null_resource.psod-master: Still creating... (1m30s elapsed)
null_resource.psod-node2: Still creating... (1m30s elapsed)
null_resource.psod-node2: Creation complete (ID: 7034267004627362524)
null_resource.psod-master: Still creating... (1m40s elapsed)
null_resource.psod-master: Still creating... (1m50s elapsed)
null_resource.psod-master: Still creating... (2m0s elapsed)
null_resource.psod-master: Still creating... (2m10s elapsed)
null_resource.psod-master: Still creating... (2m20s elapsed)
null_resource.psod-master: Still creating... (2m30s elapsed)
Error applying plan:

1 error(s) occurred:

* null_resource.XXXXXX-master: 1 error(s) occurred:

* Script exited with non-zero exit status: 1

Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.

Makefile:77: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:49: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2

",closed,False,2017-09-22 16:27:49,2018-03-12 10:21:33
kubernetes-anywhere,jessicaochen,https://github.com/kubernetes/kubernetes-anywhere/pull/455,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/455,Handle a kubeadm gs version with a trailing slash by trimming the slash.,"If a kubeadm gs version is provided with a backslash, the gsutil cp command will try and fail to pull a file path with a double backslash (ie. gs util gs://kubernetes-release-dev/bazel/v1.8.0-beta.1/bin/linux/amd64//kubeadm). Trim trailing backslashes on kubeadm gs versions to handle this problem.",closed,True,2017-09-22 20:34:01,2017-09-27 17:18:21
kubernetes-anywhere,jessicaochen,https://github.com/kubernetes/kubernetes-anywhere/issues/456,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/456,/etc/kubeadm/kubeadm_init_params.txt Not Populated,Due to accidental reuse of variable KUBEADM_DIR,closed,False,2017-09-26 21:21:27,2017-09-27 19:57:51
kubernetes-anywhere,jessicaochen,https://github.com/kubernetes/kubernetes-anywhere/pull/457,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/457,Fix naming clash of variable KUBEADM_DIR,"Modify variable for kubeadm gs directory to not clash with variable for directory of kubeadm init flags

Issue: https://github.com/kubernetes/kubernetes-anywhere/issues/456",closed,True,2017-09-27 18:39:38,2017-09-27 19:56:35
kubernetes-anywhere,SergeySevruk,https://github.com/kubernetes/kubernetes-anywhere/issues/458,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/458,How to setup second master ?,Could you explain how to setup second master ?,closed,False,2017-09-29 18:07:50,2018-03-12 06:17:34
kubernetes-anywhere,orf,https://github.com/kubernetes/kubernetes-anywhere/issues/459,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/459,Document how to add custom CA certificates to the vSphere template,"We run our internal docker registry using a custom CA certificate. There doesn't seem to be an easy way to add a custom CA certificate into the system when running `make config` so that `docker pull` will work.

We are currently having to manually edit the downloaded PhotonOS OVA to add it. This isn't very nice!",closed,False,2017-10-04 11:38:32,2018-03-12 11:22:36
kubernetes-anywhere,jhedman2,https://github.com/kubernetes/kubernetes-anywhere/issues/460,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/460,vsphere - make deploy fails on vSphere 6 (vcenter) deployment,"Wondering if anyone can help, tried multiple times, including starting from scratch, but the deployment process fails every time with the same error:

[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json /opt/kubernetes-anywhere/.config > /opt/kubernetes-anywhere/.config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/vsphere""; ./do deploy-cluster )
phase1.k8slab/.tmp/vSphere-phase1.k8slab.tf
Error asking for user input: Error parsing address 'null_resource.phase1.k8slab-master': Problem parsing address: ""null_resource.phase1.k8slab-master""
Makefile:70: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:48: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2

This is a lab vsphere environment, vCenter 6.0.0 u3, vSphere 6.  Self signed vCenter, tried both IP and FQDN vCenter url, standard port 443.  tried multiple logins as well.  verified l3 connectivity to vCenter and internet/dns.

any help appreciated. ",closed,False,2017-10-06 18:37:24,2018-04-26 03:37:34
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/461,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/461,Fix for vSphere password and make clean,"1. This PR fixes #370 i.e with this fix: kubernetes-anywhere will accept '%' char in password for vSphere.
2. Currently, if user executes ```make clean``` and config file doesn't exist then phase1 folder is deleted. This PR also fixes this issue.",closed,True,2017-10-06 19:15:52,2017-10-11 18:50:36
kubernetes-anywhere,dims,https://github.com/kubernetes/kubernetes-anywhere/pull/462,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/462,Update latest weave net,"Looking to see if 2.0.5 fixes problems in kubeadm tests:
https://github.com/kubernetes/kubernetes/issues/53570#issuecomment-335582560",closed,True,2017-10-11 18:07:00,2017-11-17 14:05:59
kubernetes-anywhere,RoadRunnr,https://github.com/kubernetes/kubernetes-anywhere/issues/463,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/463,kubelet argument api-servers no longer works with k8s >= 1.8,"kubelet --api-servers was depreciated some time ago and finally removed in 1.8.0

kubernetes-anywhere needs to updated to support k8s >= 1.8.0 to use kubeconfig instead",closed,False,2017-10-12 15:09:16,2018-04-13 14:28:55
kubernetes-anywhere,Lion-Wei,https://github.com/kubernetes/kubernetes-anywhere/pull/464,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/464,add kube-proxy mode in kubeadm config file,"added kube-proxy mode in kubeadm config file, so that we can easily chose proxy mode when build cluster, espically if we want use ipvs mode. 
Already added kube-proxy-mode args in kubeadm, in [this pr](https://github.com/kubernetes/kubernetes/pull/53962) : https://github.com/kubernetes/kubernetes/pull/53962
",closed,True,2017-10-20 12:15:54,2017-11-29 01:11:12
kubernetes-anywhere,wshelley,https://github.com/kubernetes/kubernetes-anywhere/issues/465,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/465,"vSphere deploy fails, missing k8s_config.json","I'm struggling to debug this as I'm totally new to this stack, but it looks like the config files aren't being created during ""make deploy""

The file '/etc/kubernetes/k8s_config.json' doesn't seem to be created in phase1 and the resulting docker container fails whilst running jsonnet.

Any help appreciated.

I used the latest docker image available here:
https://hub.docker.com/r/cnastorage/kubernetes-anywhere/

and also built the project from the latest master copy.

```
null_resource.kubernetes-master (remote-exec): 158914f426fa: Pull complete
null_resource.kubernetes-master (remote-exec): Digest: sha256:27dafee7f988c31c0ff6a4992f7595d2b909fd50390cb5e71479a1e5eef245fe
null_resource.kubernetes-master (remote-exec): Status: Downloaded newer image for cnastorage/k8s-ignition:v2
null_resource.kubernetes-master (remote-exec): + export IGNITION_CONFIG_FILE=/usr/share/oem/ignite.json
null_resource.kubernetes-master (remote-exec): + mkdir -p /usr/share/oem
null_resource.kubernetes-master (remote-exec): + jsonnet --output-file /usr/share/oem/ignite.json --tla-code-file cfg=/etc/kubernetes/k8s_config.json --jpath /opt/kubernetes-anywhere /opt/kubernetes-anywhere/ignite.jsonnet
null_resource.kubernetes-master (remote-exec): STATIC ERROR: tla:cfg:1:1: Unexpected end of file.
null_resource.kubernetes-master (remote-exec): Failed to docker run installer container
Error applying plan:

5 error(s) occurred:

* null_resource.kubernetes-node3: 1 error(s) occurred:

* Script exited with non-zero exit status: 127
* null_resource.kubernetes-master: 1 error(s) occurred:

* Script exited with non-zero exit status: 1
* null_resource.kubernetes-node4: 1 error(s) occurred:

* Script exited with non-zero exit status: 127
* null_resource.kubernetes-node2: 1 error(s) occurred:

* Script exited with non-zero exit status: 127
* null_resource.kubernetes-node5: 1 error(s) occurred:

* Script exited with non-zero exit status: 127

Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.
```


config file:

```
Automatically generated file; DO NOT EDIT.
# Kubernetes Minimal Turnup Configuration
#

#
# Phase 1: Cluster Resource Provisioning
#
.phase1.num_nodes=4
.phase1.cluster_name=""kubernetes""
.phase1.ssh_user=""""
.phase1.cloud_provider=""vsphere""

#
# vSphere configuration
#
.phase1.vSphere.url=""my_server_ip""
.phase1.vSphere.port=443
.phase1.vSphere.username=""my_user""
.phase1.vSphere.password=""my_pass""
.phase1.vSphere.insecure=y
.phase1.vSphere.datacenter=""my_datacenter""
.phase1.vSphere.datastore=""my_datastore""
.phase1.vSphere.placement=""cluster""
.phase1.vSphere.cluster=""my_cluster""
.phase1.vSphere.useresourcepool=y
.phase1.vSphere.resourcepool=""pool""
.phase1.vSphere.vmfolderpath=""path/name""
.phase1.vSphere.vcpu=4
.phase1.vSphere.memory=4096
.phase1.vSphere.network=""network_name""
.phase1.vSphere.template=""path/KubernetesAnywhereTemplatePhotonOS""
.phase1.vSphere.flannel_net=""172.1.0.0/16""

#
# Phase 2: Node Bootstrapping
#
.phase2.kubernetes_version=""v1.6.5""
.phase2.provider=""ignition""
.phase2.installer_container=""docker.io/cnastorage/k8s-ignition:v2""
.phase2.docker_registry=""gcr.io/google-containers""

#
# Phase 3: Deploying Addons
#
.phase3.run_addons=y
.phase3.kube_proxy=y
.phase3.dashboard=y
.phase3.heapster=y
.phase3.kube_dns=y
.phase3.weave_net=n
```",closed,False,2017-10-20 13:51:06,2017-12-27 16:33:10
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/issues/466,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/466,Openstack provider reuse existing network and router,"Upon filling in the config options for my open stack network, I noted that the scripts created a new network instead of re-using my existing network. It would be great to have a switch to flip off creation of a new neutron router in favor of reusing an existing configuration.

cc @shashidharatd @dims ",closed,False,2017-10-23 17:02:17,2018-03-19 15:22:41
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/issues/467,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/467,Openstack provider insecure option,Add the openstack.insecure option so that auth works for clusters using self-signed certificates.,closed,False,2017-10-23 17:03:05,2018-06-21 21:40:31
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/pull/468,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/468,Allow insecure /os_cacert option for openstack provider,Fixes #467,closed,True,2017-10-23 17:05:48,2018-06-17 02:47:57
kubernetes-anywhere,ccit-spence,https://github.com/kubernetes/kubernetes-anywhere/issues/469,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/469,Can not deploy Kubernetes Cluster on vSphere with admin user password containing char '$',"Not sure the other information that needs to be provided.  This is the error:

`RUNTIME ERROR: Unrecognised conversion type: $`",closed,False,2017-10-25 07:22:19,2018-04-29 19:51:04
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/pull/470,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/470,Move docker_registry back to required Kconfig vars,"docker_registry is used in phase3 for deploying addons and other
components that do not hardcode a registry.",closed,True,2017-10-25 18:59:40,2017-11-15 00:04:20
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/issues/471,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/471,Calico CNI testing failing on k8s master,Dashboard: https://k8s-testgrid.appspot.com/sig-cluster-lifecycle-all#kubeadm-gce-cni-calico,closed,False,2017-10-25 19:16:08,2018-04-08 17:04:53
kubernetes-anywhere,mrhillsman,https://github.com/kubernetes/kubernetes-anywhere/pull/472,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/472,Update instructions for SSH keypair,"'make docker-dev' mounts current directory to /opt/kubernetes-anywhere when following the provided instructions. Files outside of the host's kubernetes-anywhere directory that is mounted are not accessible; i.e. /root/.ssh/id_rsa

This change updates the instructions to create the keypair in the kubernetes-anywhere directory by default so the files are accessible during 'make deploy'",closed,True,2017-10-26 06:41:02,2018-01-16 17:52:50
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/pull/473,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/473,Skip phase3 addons deployed by kubeadm,"Kube-dns and kube-proxy should be automatically deployed by kubeadm.

Notes for reviewer: I'm not sure if Kconfig supports nested ifs, but that might make this cleaner.",closed,True,2017-10-26 14:35:44,2017-11-15 00:07:27
kubernetes-anywhere,vinayvenkat,https://github.com/kubernetes/kubernetes-anywhere/issues/474,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/474,How do I ssh into the master and the nodes after deployment on vsphere,"I have the username and the client-certificate-data and the client-key-data. How do I use these to ssh into the master node and the minion nodes? 

Thanks,
- Vinay ",closed,False,2017-10-26 20:01:45,2017-10-26 21:28:06
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/pull/475,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/475,Update Calico CIDR blocks for etcd,,closed,True,2017-10-30 15:03:52,2017-10-31 15:54:57
kubernetes-anywhere,cmluciano,https://github.com/kubernetes/kubernetes-anywhere/issues/476,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/476,Add prow tests to k8s-anywhere,"We should add the following tests at minimum

- kubeadm normal
- kubeadm flannel
- kubeadm calico

These tests should be run against every PR in k8s-anywhere since they are included in the mainline k8s/k8s matrix.",closed,False,2017-10-31 14:02:15,2018-03-30 16:31:52
kubernetes-anywhere,kaziislam,https://github.com/kubernetes/kubernetes-anywhere/issues/477,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/477,make deploy fails for vCenter 6,"I followed this documentation https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase1/vsphere/README.md and tried several times with administrator and a different account with admin privilege still same result. Any suggestions is appreciated.

[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json /opt/kubernetes-anywhere/.config > /opt/kubernetes-anywhere/.config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/vsphere""; ./do deploy-cluster )
STATIC ERROR: vSphere.jsonnet:24:8: Expected a comma before next field.
Makefile:70: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:48: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
",closed,False,2017-11-02 19:27:47,2017-11-07 13:25:37
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/478,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/478,Fix a syntax error in startup script,"There seems to be an error introduced recently in the startup script which produces below error
```
-bash: syntax error in conditional expression
-bash: syntax error near `]'
```

/cc @cmluciano 
/assign @pipejakob 
",closed,True,2017-11-03 12:03:42,2017-11-15 01:54:09
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/479,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/479,Refactor startup scripts to be modular and reusable,"Remove duplication of kubeadm startup scripts in every phase1 provider.
Moved the kubeadm startup scripts to phase2/kubeadm.

I have tested the changes in both gce and openstack and the changes do work for both of them.

/cc @jamiehannaford @pipejakob @cmluciano 
/assign @pipejakob ",closed,True,2017-11-03 12:17:18,2017-11-16 16:11:21
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/issues/480,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/480,pod-network-cidr should be configurable by user,"Currently the k8s-anywhere supports multiple CNIs. Each CNI has a different default pod-network-cidr.
If user do not care about it, the defaults are configured as in [here](https://github.com/kubernetes/kubernetes-anywhere/blob/master/phase1/gce/configure-vm-kubeadm.sh#L73)

All the CNIs which are deployed as addons need to have the capability to take `pod-network-cidr` as a configurable parameter and use it to deploy the pod network.

/cc @jamiehannaford @pipejakob @luxas @dims ",closed,False,2017-11-03 17:23:53,2018-04-08 08:56:52
kubernetes-anywhere,ss22ever,https://github.com/kubernetes/kubernetes-anywhere/pull/481,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/481,Create config file for openstack cloud provider,#479,closed,True,2017-11-06 14:08:12,2018-06-07 20:10:19
kubernetes-anywhere,jjasghar,https://github.com/kubernetes/kubernetes-anywhere/pull/482,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/482,Updated the vSphere readme for the new make config,"- Fixed the ordering of the questions
- Fixed some extra whitespace

Signed-off-by: JJ Asghar <jj@chef.io>",closed,True,2017-11-06 18:25:28,2017-12-21 06:39:38
kubernetes-anywhere,xiangpengzhao,https://github.com/kubernetes/kubernetes-anywhere/pull/483,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/483,Set `--discovery-token-unsafe-skip-ca-verification` flag when `--discovery-token-ca-cert-hash` is not set,"ref: https://github.com/kubernetes/kubeadm/issues/534 https://github.com/kubernetes/kubernetes/pull/54982

/cc @luxas ",closed,True,2017-11-10 16:05:09,2017-11-20 17:00:26
kubernetes-anywhere,dims,https://github.com/kubernetes/kubernetes-anywhere/pull/484,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/484,Add --discovery-token-unsafe-skip-ca-verification to prevent kubeadm …,"…join failure

Since https://github.com/kubernetes/kubernetes/pull/55468 merged, kubeadm now requires the user to specify either the `--discovery-token-ca-cert-hash` flag or the `--discovery-token-unsafe-skip-ca-verification` flag",closed,True,2017-11-19 19:46:08,2017-11-20 21:41:30
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/485,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/485,Add top level OWNERS for the repo,"Adding the top level OWNERS file similar to other maintained kubernetes repos.
This is required for CI test jobs.

/assign @pipejakob 
/cc @kubernetes/sig-cluster-lifecycle-pr-reviews ",closed,True,2017-11-20 15:56:23,2017-11-22 04:01:22
kubernetes-anywhere,fturib,https://github.com/kubernetes/kubernetes-anywhere/pull/486,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/486,add featuregate opt for kubeadm,"## What this PR does / why we need it:

allow option 'FeatureGates' when deploying kubeadm on the master's node of the cluster.
this option when set with ""CoreDNS=true"" allow to run CoreDNS as the default DNS server in the cluster instead of usual kube-dns

## Which issue this PR fixes 
Adds part of e2e integration requires for CoreDNS that is an ALPHA feature added to kubeadm 
See: kubernetes/kubeadm#446
it will need a second part in the project ""test-infra""

## Special notes for your reviewer:
_In order to have the deployment of cluster run on GCE, I needed to fix issues on ""terraform"" tool usage (command line and config file). see gce/do and gce/gce.jsonnet_
**rollbacked in last commit. That was tied to the version of ""terraform"" used for testing.**
",closed,True,2017-11-20 18:22:57,2017-12-11 21:22:53
kubernetes-anywhere,f0o,https://github.com/kubernetes/kubernetes-anywhere/issues/487,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/487,OpenStack fails using default phase2 config - Documentation too sparse,"Hi,

When attempting to deploy this on openstack it just errors with:
`RUNTIME ERROR: Unsupported phase2 provider in config`

if I change it from ignition to kubeadm it does continue but there's absolutely no documentation on what a valid option is or not, I had to dig the code to find this alternative.

Even with kubeadm it fails at a later stage (kubelet doesnt start).

Is there an actual working example with OpenStack?

//Edit: Also no DNS is being set for the network that terraform creates, so you always need to fiddle with the subnet before the first VM boots to add a DNS server.",closed,False,2017-11-22 09:14:52,2018-04-23 18:28:50
kubernetes-anywhere,xiangpengzhao,https://github.com/kubernetes/kubernetes-anywhere/pull/488,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/488,Don't pass true for bool type flag --discovery-token-unsafe-skip-ca-verification,"Passing the flag means `true`, right? Seems like `--discovery-token-unsafe-skip-ca-verification true` is invalid.

xref: https://github.com/kubernetes/kubernetes/issues/56091 `[job failure] kubeadm-gce`

```
Nov 23 05:13:24 ubuntu startupscript: kubeadm join --token ""$KUBEADM_TOKEN"" ""$KUBEADM_MASTER_IP:443"" --skip-preflight-checks --discovery-token-unsafe-skip-ca-verification true
Nov 23 05:13:24 ubuntu startupscript: + kubeadm join --token 043479.1a6ec0adafb593d0 10.128.0.2:443 --skip-preflight-checks --discovery-token-unsafe-skip-ca-verification true
Nov 23 05:13:25 ubuntu startupscript: Flag --skip-preflight-checks has been deprecated, it is now equivalent to --ignore-checks-errors=all
Nov 23 05:13:25 ubuntu startupscript: [kubeadm] WARNING: kubeadm is currently in beta
Nov 23 05:13:25 ubuntu startupscript: [preflight] Running pre-flight checks.
Nov 23 05:13:25 ubuntu startupscript: #011[WARNING FileExisting-crictl]: crictl not found in system path
Nov 23 05:13:25 ubuntu startupscript: [validation] WARNING: kubeadm doesn't fully support multiple API Servers yet
Nov 23 05:13:25 ubuntu startupscript: discovery: Invalid value: ""true"": address true: missing port in address
Nov 23 05:13:25 ubuntu startupscript: Finished running startup script /var/run/google.startup.script
```

@pipejakob @luxas @dims @spiffxp ",closed,True,2017-11-23 06:53:57,2017-11-27 13:22:13
kubernetes-anywhere,Lion-Wei,https://github.com/kubernetes/kubernetes-anywhere/issues/489,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/489,kubeadm join command may have some mistake ,"Now we are using `kubeadm join` with flag `--discovery-token-unsafe-skip-ca-verification` which may be should not follow a `true`. 
When I using kubernetes-anywhere build a cluster, all node can't join the cluster with this log:
```
+ kubeadm join --token 1c5edc.f6626d37dd9eecff 10.128.0.2:443 --skip-preflight-checks --discovery-token-unsafe-skip-ca-verification true
[kubeadm] WARNING: kubeadm is in beta. Please do not use it for production clusters!
[preflight] Skipping pre-flight checks.
[validation] WARNING: kubeadm doesn't fully support multiple API Servers yet
discovery: Invalid value: ""true"": address true: missing port in address
```
And `kubeadm join -h` says:
```
Usage:
  kubeadm join [flags]

Flags:
      --config string                                 Path to kubeadm config file.
      --cri-socket string                             Specify the CRI socket to connect to. (default ""/var/run/dockershim.sock"")
      --discovery-file string                         A file or url from which to load cluster information.
      --discovery-token string                        A token used to validate cluster information fetched from the master.
      --discovery-token-ca-cert-hash stringSlice      For token-based discovery, validate that the root CA public key matches this hash (format: ""<type>:<value>"").
      --discovery-token-unsafe-skip-ca-verification   For token-based discovery, allow joining without --discovery-token-ca-cert-hash pinning.
      --node-name string                              Specify the node name.
      --skip-preflight-checks                         Skip preflight checks normally run before modifying the system.
      --tls-bootstrap-token string                    A token used for TLS bootstrapping.
      --token string                                  Use this token for both discovery-token and tls-bootstrap-token.
```

This may be a bug, I saw lots kubeadm related CI failed in `valid` step cause can only found one node: [kubeadm-gce-cni-calico](https://k8s-testgrid.appspot.com/sig-cluster-lifecycle-all#kubeadm-gce-cni-calico). ",closed,False,2017-11-23 13:00:41,2017-11-24 01:11:05
kubernetes-anywhere,Lion-Wei,https://github.com/kubernetes/kubernetes-anywhere/pull/490,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/490,fix kubeadm join bug,#489,closed,True,2017-11-23 13:01:43,2017-11-24 01:10:54
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/491,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/491,Fix passing flag --discovery-token-unsafe-skip-ca-verification to kubeadm join,"Address the comment in https://github.com/kubernetes/kubernetes-anywhere/pull/488/files#r153078767

I have tested the changes in my local gce environment and it has passed for `""$KUBEADM_VERSION"" == ""stable""`

/cc @xiangpengzhao @luxas @pipejakob ",closed,True,2017-11-27 12:45:45,2017-11-28 01:33:57
kubernetes-anywhere,shashidharatd,https://github.com/kubernetes/kubernetes-anywhere/pull/492,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/492,Improve couple of Kconfig tems,"Partly addresses the concerns raised in https://github.com/kubernetes/kubernetes-anywhere/issues/487
Bumped the default kubernetes version to v1.8.4, which is the current stable version supported.

/assign @pipejakob ",closed,True,2017-11-27 16:00:23,2017-11-29 17:21:34
kubernetes-anywhere,mrhillsman,https://github.com/kubernetes/kubernetes-anywhere/issues/493,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/493,Missing .config throws error,"When running 'make deploy' without having an existing .config causes error:

```sh
[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json  > .json
util/config_to_json: <.config-file-path>
Makefile:43: recipe for target '.json' failed
make: *** [.json] Error 1
```

Reviewing the Makefile it is designed to run 'make config' should .config not exist:

```sh
${CONFIG_FILE_ABS}: $(KCONFIG_FILES)
	$(MAKE) config

${CONFIG_JSON_FILE}: ${CONFIG_FILE_ABS}
	util/config_to_json $< > $@

echo-config: ${CONFIG_JSON_FILE}
	cat $<

deploy-cluster destroy-cluster: ${CONFIG_JSON_FILE}
	$(MAKE) do WHAT=$@
```

But this is not happening.",closed,False,2017-11-28 01:09:04,2018-04-27 02:47:07
kubernetes-anywhere,mrhillsman,https://github.com/kubernetes/kubernetes-anywhere/pull/494,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/494,Force run of 'make config',"Closes issue #493 
Makefile as noted in above issue is designed to force 'make config' when .config is missing.
Additionally changed all ${} variable references to $() for consistency.",closed,True,2017-11-28 01:31:35,2018-04-04 13:03:57
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/issues/495,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/495,Fail to deploy k8s cluster 1.8 on vSphere,"Logs:-
```
kubeanywhere- null_resource.kubernetes-node3 (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/system/kubelet.service.
null_resource.kubernetes-node3: Creation complete (ID: 2083685943369692449)
Error applying plan:

1 error(s) occurred:

* null_resource.kubernetes-master: 1 error(s) occurred:

* Script exited with non-zero exit status: 1
```",closed,False,2017-11-28 23:56:33,2019-01-06 11:07:30
kubernetes-anywhere,andyanfieldroad,https://github.com/kubernetes/kubernetes-anywhere/issues/496,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/496,Second network for backend storage,"Hi,

I am using kubernetes anywhere for a POC kubernetes environment developing towards a production ready design.

One of the requirements is that the backend storage is provided on a second network for traffic isolation and more guaranteed bandwidth of I/O.  I have looked at the build instructions for the kubernetes anywhere cluster and it seems to allow configuration of only a single network with the line:

.phase1.vSphere.network=""<VMNET>""

Also, I tried manually adding a second NIC on each cluster node and reconfigurating the networking manually but it did not accept a second file.  

I followed:  https://github.com/vmware/photon/blob/master/docs/photon-admin-guide.md, and created:

/etc/systemd/network/10-dhcp-en.network:
[Match]
Name=eth0

[Network]
DHCP=yes

/etc/systemd/network/50-eth1-en.network:
[Match]
Name=eth1
Address=10.0.0.122/24

Then ran:
# systemctl restart systemd-networkd

But it gave an error that the file was not found.  Is the kubernetes anywhere photon image different from a standard photon image in such a way that networking cannot be configured as per the documentation?

Thanks

Andy
",closed,False,2017-12-02 07:05:44,2018-05-26 02:09:06
kubernetes-anywhere,alandsidel,https://github.com/kubernetes/kubernetes-anywhere/issues/497,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/497,vSphere cluster provider has bad default network address,The default network address for the vSphere client is **172.1.0.0/16**.  This should be **172.16.0.0/16** like the google client.  The former is a real netblock owned by AT&T while the latter is an RFC1918 private netblock.,closed,False,2017-12-04 16:39:20,2018-05-05 23:18:20
kubernetes-anywhere,abrarshivani,https://github.com/kubernetes/kubernetes-anywhere/pull/498,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/498,Fix typos in vSphere README,Fixes https://github.com/vmware/kubernetes/issues/399,closed,True,2017-12-04 22:01:08,2017-12-04 22:08:12
kubernetes-anywhere,agrebin,https://github.com/kubernetes/kubernetes-anywhere/issues/499,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/499,Cant Deploy OVF,"We're trying to deploy kubernetes anywhere on our vCenter cluster, but it is faililng .
The current permissions are:
[https://docs.google.com/spreadsheets/d/1nB9joHzcEw5E1t7FuM5rQNZ43Ht5FFQJwlsEKVtTo0c/edit?usp=sharing](url)

But we get an error when trying to deploy it:

> A vCenter Server resource is missing or invalid

Any ideas if there are futher permissions we need?
Is it a known bug?

Thanks!


",closed,False,2017-12-05 14:11:50,2018-05-25 22:05:05
kubernetes-anywhere,tmbull,https://github.com/kubernetes/kubernetes-anywhere/issues/500,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/500,Nodes going into NotReady status on VSphere,"Hello, I am experience nodes going into NotReady status seemingly randomly after a period of time. I have a 5 node cluster on vSphere 6.5, and I logged in this morning to find that 3 of the 5 nodes were NotReady. 

Here are the logs:

```
root@kubernetes-node5 [ ~ ]# journalctl -n 50 -u kubelet
-- Logs begin at Thu 2017-12-07 23:26:37 UTC, end at Tue 2017-12-12 21:56:50 UTC. --
Dec 12 21:54:30 kubernetes-node5 docker[641]: I1212 21:54:30.057786     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h22m33.844691347s ago; threshold is 3m0s]
Dec 12 21:54:33 kubernetes-node5 docker[641]: E1212 21:54:33.898905     683 remote_runtime.go:168] ListPodSandbox with filter ""nil"" from runtime service failed: rpc error: code = 4 desc = context deadline exceeded
Dec 12 21:54:33 kubernetes-node5 docker[641]: E1212 21:54:33.899005     683 kuberuntime_sandbox.go:197] ListPodSandbox failed: rpc error: code = 4 desc = context deadline exceeded
Dec 12 21:54:33 kubernetes-node5 docker[641]: E1212 21:54:33.899031     683 generic.go:196] GenericPLEG: Unable to retrieve pods: rpc error: code = 4 desc = context deadline exceeded
Dec 12 21:54:35 kubernetes-node5 docker[641]: I1212 21:54:35.058691     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h22m38.845613182s ago; threshold is 3m0s]
Dec 12 21:54:39 kubernetes-node5 docker[641]: E1212 21:54:39.532479     683 file.go:72] unable to read config path ""/etc/kubernetes/manifests"": path does not exist, ignoring
Dec 12 21:54:40 kubernetes-node5 docker[641]: I1212 21:54:40.059049     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h22m43.845963008s ago; threshold is 3m0s]
Dec 12 21:54:45 kubernetes-node5 docker[641]: I1212 21:54:45.059342     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h22m48.846238393s ago; threshold is 3m0s]
Dec 12 21:54:50 kubernetes-node5 docker[641]: I1212 21:54:50.059978     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h22m53.84689874s ago; threshold is 3m0s]
Dec 12 21:54:55 kubernetes-node5 docker[641]: I1212 21:54:55.060460     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h22m58.847296989s ago; threshold is 3m0s]
Dec 12 21:54:59 kubernetes-node5 docker[641]: E1212 21:54:59.532858     683 file.go:72] unable to read config path ""/etc/kubernetes/manifests"": path does not exist, ignoring
Dec 12 21:55:00 kubernetes-node5 docker[641]: I1212 21:55:00.060850     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m3.847764517s ago; threshold is 3m0s]
Dec 12 21:55:05 kubernetes-node5 docker[641]: I1212 21:55:05.061391     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m8.848215446s ago; threshold is 3m0s]
Dec 12 21:55:10 kubernetes-node5 docker[641]: I1212 21:55:10.061713     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m13.848613613s ago; threshold is 3m0s]
Dec 12 21:55:14 kubernetes-node5 docker[641]: E1212 21:55:14.219669     683 remote_runtime.go:261] ListContainers with filter ""&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"" from runtime service failed:
 rpc error: code = 4 desc = context deadline exceeded
Dec 12 21:55:14 kubernetes-node5 docker[641]: E1212 21:55:14.220009     683 kuberuntime_container.go:341] getKubeletContainers failed: rpc error: code = 4 desc = context deadline exceeded
Dec 12 21:55:14 kubernetes-node5 docker[641]: E1212 21:55:14.220823     683 kubelet.go:1207] Container garbage collection failed: rpc error: code = 4 desc = context deadline exceeded
Dec 12 21:55:15 kubernetes-node5 docker[641]: I1212 21:55:15.062089     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m18.848987841s ago; threshold is 3m0s]
Dec 12 21:55:15 kubernetes-node5 docker[641]: I1212 21:55:15.203695     683 qos_container_manager_linux.go:286] [ContainerManager]: Updated QoS cgroup configuration
Dec 12 21:55:19 kubernetes-node5 docker[641]: E1212 21:55:19.533334     683 file.go:72] unable to read config path ""/etc/kubernetes/manifests"": path does not exist, ignoring
Dec 12 21:55:20 kubernetes-node5 docker[641]: I1212 21:55:20.062435     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m23.84935367s ago; threshold is 3m0s]
Dec 12 21:55:25 kubernetes-node5 docker[641]: I1212 21:55:25.062768     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m28.849649555s ago; threshold is 3m0s]
Dec 12 21:55:30 kubernetes-node5 docker[641]: I1212 21:55:30.063153     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m33.8500322s ago; threshold is 3m0s]
Dec 12 21:55:35 kubernetes-node5 docker[641]: I1212 21:55:35.063553     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m38.850468361s ago; threshold is 3m0s]
Dec 12 21:55:39 kubernetes-node5 docker[641]: E1212 21:55:39.534270     683 file.go:72] unable to read config path ""/etc/kubernetes/manifests"": path does not exist, ignoring
Dec 12 21:55:40 kubernetes-node5 docker[641]: I1212 21:55:40.063851     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m43.850772477s ago; threshold is 3m0s]
Dec 12 21:55:45 kubernetes-node5 docker[641]: I1212 21:55:45.064032     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m48.85090857s ago; threshold is 3m0s]
Dec 12 21:55:50 kubernetes-node5 docker[641]: I1212 21:55:50.064398     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m53.851305752s ago; threshold is 3m0s]
Dec 12 21:55:55 kubernetes-node5 docker[641]: I1212 21:55:55.064712     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h23m58.851614956s ago; threshold is 3m0s]
Dec 12 21:55:59 kubernetes-node5 docker[641]: E1212 21:55:59.534747     683 file.go:72] unable to read config path ""/etc/kubernetes/manifests"": path does not exist, ignoring
Dec 12 21:56:00 kubernetes-node5 docker[641]: I1212 21:56:00.065009     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m3.851919617s ago; threshold is 3m0s]
Dec 12 21:56:05 kubernetes-node5 docker[641]: I1212 21:56:05.065282     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m8.85219531s ago; threshold is 3m0s]
Dec 12 21:56:10 kubernetes-node5 docker[641]: I1212 21:56:10.065550     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m13.852465637s ago; threshold is 3m0s]
Dec 12 21:56:11 kubernetes-node5 docker[641]: W1212 21:56:11.281089     683 container_manager_linux.go:747] CPUAccounting not enabled for pid: 402
Dec 12 21:56:11 kubernetes-node5 docker[641]: W1212 21:56:11.281222     683 container_manager_linux.go:750] MemoryAccounting not enabled for pid: 402
Dec 12 21:56:11 kubernetes-node5 docker[641]: I1212 21:56:11.281258     683 container_manager_linux.go:398] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Dec 12 21:56:15 kubernetes-node5 docker[641]: I1212 21:56:15.066067     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m18.852907377s ago; threshold is 3m0s]
Dec 12 21:56:15 kubernetes-node5 docker[641]: I1212 21:56:15.204642     683 qos_container_manager_linux.go:286] [ContainerManager]: Updated QoS cgroup configuration
Dec 12 21:56:19 kubernetes-node5 docker[641]: E1212 21:56:19.535089     683 file.go:72] unable to read config path ""/etc/kubernetes/manifests"": path does not exist, ignoring
Dec 12 21:56:20 kubernetes-node5 docker[641]: I1212 21:56:20.066851     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m23.85376005s ago; threshold is 3m0s]
Dec 12 21:56:25 kubernetes-node5 docker[641]: I1212 21:56:25.067128     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m28.854041938s ago; threshold is 3m0s]
Dec 12 21:56:30 kubernetes-node5 docker[641]: I1212 21:56:30.067435     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m33.854348394s ago; threshold is 3m0s]
Dec 12 21:56:34 kubernetes-node5 docker[641]: E1212 21:56:34.899659     683 remote_runtime.go:168] ListPodSandbox with filter ""nil"" from runtime service failed: rpc error: code = 4 desc = context deadline exceeded
Dec 12 21:56:34 kubernetes-node5 docker[641]: E1212 21:56:34.900268     683 kuberuntime_sandbox.go:197] ListPodSandbox failed: rpc error: code = 4 desc = context deadline exceeded
Dec 12 21:56:34 kubernetes-node5 docker[641]: E1212 21:56:34.900396     683 generic.go:196] GenericPLEG: Unable to retrieve pods: rpc error: code = 4 desc = context deadline exceeded
Dec 12 21:56:35 kubernetes-node5 docker[641]: I1212 21:56:35.069515     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m38.856405541s ago; threshold is 3m0s]
Dec 12 21:56:39 kubernetes-node5 docker[641]: E1212 21:56:39.535473     683 file.go:72] unable to read config path ""/etc/kubernetes/manifests"": path does not exist, ignoring
Dec 12 21:56:40 kubernetes-node5 docker[641]: I1212 21:56:40.069956     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m43.856849063s ago; threshold is 3m0s]
Dec 12 21:56:45 kubernetes-node5 docker[641]: I1212 21:56:45.070379     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m48.857276093s ago; threshold is 3m0s]
Dec 12 21:56:50 kubernetes-node5 docker[641]: I1212 21:56:50.070758     683 kubelet.go:1820] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 17h24m53.857672725s ago; threshold is 3m0s]
```

The issue seems to be that the docker service is becoming non-responsive. For example, `docker ps` hangs indefinitely. There aren't any recent log messages in the `docker.service` journal unfortunately. If I restart docker.service, the node will recover, but this isn't a viable long-term solution obviously.

Version info:

```
root@photon-machine [ ~ ]# docker version
Client:
 Version:      17.06.0-ce
 API version:  1.30
 Go version:   go1.8.1
 Git commit:   02c1d87
 Built:        Thu Oct 26 06:33:23 2017
 OS/Arch:      linux/amd64

Server:
 Version:      17.06.0-ce
 API version:  1.30 (minimum version 1.12)
 Go version:   go1.8.1
 Git commit:   02c1d87
 Built:        Thu Oct 26 06:34:46 2017
 OS/Arch:      linux/amd64
 Experimental: false
root@photon-machine [ ~ ]# kubectl version
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.7"", GitCommit:""8e1552342355496b62754e61ad5f802a0f3f1fa7"", GitTreeState:""clean"", BuildDate:""2017-09-29T00:18:18Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.7"", GitCommit:""8e1552342355496b62754e61ad5f802a0f3f1fa7"", GitTreeState:""clean"", BuildDate:""2017-09-28T23:56:03Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

Let me know if there's any other info I can provide. Thanks.",closed,False,2017-12-12 22:05:40,2017-12-21 03:01:32
kubernetes-anywhere,spiffxp,https://github.com/kubernetes/kubernetes-anywhere/pull/501,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/501,Add code-of-conduct.md,"Refer to kubernetes/community as authoritative source for code of conduct

ref: kubernetes/community#1527",closed,True,2017-12-20 18:39:23,2018-01-04 22:08:24
kubernetes-anywhere,svasseur,https://github.com/kubernetes/kubernetes-anywhere/issues/502,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/502,provision volume failed with kubernetes v1.9.0,"when i add a volume with vsphere-volume, always an error  ""AttachVolume.Attach failed for volume ""xxxx"" : 404 Not Found""  
it's work like a charm in v1.6.5

 i try to create a vmdk volume 
`vmkfstools -c 10G /vmfs/volumes/Datastore-12/volumes/kubeVolume.vmdk`
and after  
```apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: httpd3
  namespace: default
  labels:
    app: httpd3
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: httpd3
    spec:
      containers:
        - name: httpd3
          image: httpd:alpine
          ports:
            - name: http
              containerPort: 80
          volumeMounts:
            - name: httpd3-persistent-storage
              mountPath: /usr/local/apache2/htdocs/
      volumes:
      - name: httpd3-persistent-storage
        vsphereVolume:
         volumePath: ""[Datastore-12] kubeVolume""
         fsType: ext4
```

kubectl describe pod : 

```
Normal   Scheduled              2m               default-scheduler          Successfully assigned httpd3-74754b7fdc-p8j26 to kubernetes-node4
  Normal   SuccessfulMountVolume  2m               kubelet, kubernetes-node4  MountVolume.SetUp succeeded for volume ""default-token-sfr9w""
  Warning  FailedMount            1m (x8 over 2m)  attachdetach-controller    AttachVolume.Attach failed for volume ""httpd3-persistent-storage"" : 404 Not Found
```


I also try with stateful  storage 
````
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: thin-disk
provisioner: kubernetes.io/vsphere-volume
parameters:
    diskformat: thin
`````
````
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: httpd1 # has to match .spec.template.metadata.labels
  serviceName: ""httpd1""
  replicas: 1 # by default is 1
  template:
    metadata:
      labels:
        app: httpd1 # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: httpd1
        image: httpd:alpine
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/local/apache2/htdocs/
  volumeClaimTemplates:
  - metadata:
      name: www
      annotations:
        volume.beta.kubernetes.io/storage-class: thin-disk
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 1Gi
````

kubectl describe  pvc 
```
Events:
  Type     Reason              Age                From                         Message
  ----     ------              ----               ----                         -------
  Warning  ProvisioningFailed  1m (x321 over 1h)  persistentvolume-controller  Failed to provision volume with StorageClass ""thin-disk"": 404 Not Found
```
",closed,False,2017-12-22 10:48:58,2018-10-27 08:16:16
kubernetes-anywhere,thockin,https://github.com/kubernetes/kubernetes-anywhere/pull/503,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/503,Convert registry to k8s.gcr.io,"This PR was auto-generated.  Please apply human expertise to review for correctness.

Followup to https://github.com/kubernetes/kubernetes/pull/54174

xref https://github.com/kubernetes/release/issues/281",closed,True,2017-12-22 18:01:25,2018-01-09 21:50:51
kubernetes-anywhere,Coolfeather2,https://github.com/kubernetes/kubernetes-anywhere/issues/504,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/504,Kubelet Service Failed [v1.9.0] [vSphere],"Installing Kubernetes v1.9.0 on vSphere
```
Dec 23 02:50:34 kubernetes-master systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Dec 23 02:50:34 kubernetes-master systemd[1]: kubelet.service: Unit entered failed state.
Dec 23 02:50:34 kubernetes-master systemd[1]: kubelet.service: Failed with result 'exit-code'.
Dec 23 02:50:44 kubernetes-master systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Dec 23 02:50:44 kubernetes-master systemd[1]: Stopped Kubernetes Kubelet Server.
Dec 23 02:50:44 kubernetes-master systemd[1]: Starting Kubernetes Kubelet Server...
Dec 23 02:52:15 kubernetes-master systemd[1]: Started Kubernetes Kubelet Server.
Dec 23 02:53:10 kubernetes-master docker[1859]: /usr/bin/docker: Error response from daemon: rpc error: code = 2 desc = ""containerd: container did not start before the specified timeout"".
Dec 23 02:54:18 kubernetes-master systemd[1]: kubelet.service: Main process exited, code=exited, status=125/n/a
Dec 23 02:54:18 kubernetes-master systemd[1]: kubelet.service: Unit entered failed state.
Dec 23 02:54:18 kubernetes-master systemd[1]: kubelet.service: Failed with result 'exit-code'.
Dec 23 02:54:28 kubernetes-master systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Dec 23 02:54:28 kubernetes-master systemd[1]: Stopped Kubernetes Kubelet Server.
Dec 23 02:54:28 kubernetes-master systemd[1]: Starting Kubernetes Kubelet Server...
Dec 23 02:57:56 kubernetes-master systemd[1]: Started Kubernetes Kubelet Server.
Dec 23 03:01:10 kubernetes-master docker[1915]: /usr/bin/docker: Error response from daemon: rpc error: code = 2 desc = ""containerd: container did not start before the specified timeout"".
Dec 23 03:07:21 kubernetes-master systemd[1]: kubelet.service: Main process exited, code=exited, status=125/n/a
Dec 23 03:07:21 kubernetes-master systemd[1]: kubelet.service: Unit entered failed state.
Dec 23 03:07:21 kubernetes-master systemd[1]: kubelet.service: Failed with result 'exit-code'.
Dec 23 03:07:32 kubernetes-master systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Dec 23 03:07:32 kubernetes-master systemd[1]: Stopped Kubernetes Kubelet Server.
Dec 23 03:07:32 kubernetes-master systemd[1]: Starting Kubernetes Kubelet Server...
```",closed,False,2017-12-23 03:14:05,2018-05-26 03:10:07
kubernetes-anywhere,Coolfeather2,https://github.com/kubernetes/kubernetes-anywhere/issues/505,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/505,RUNTIME ERROR: Unrecognised conversion type: r,"```
[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json /opt/kubernetes-anywhere/.config > /opt/kubernetes-anywhere/.config.json
make do WHAT=deploy-cluster
util/config_to_json  /opt/kubernetes-anywhere/.config > /opt/kubernetes-anywhere/.config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/vsphere""; ./do deploy-cluster )
RUNTIME ERROR: Unrecognised conversion type: r
        std.jsonnet:313:21-62   function <parse_conv_type>
        std.jsonnet:326:31-59   thunk <ctype>
        std.jsonnet:328:24-28   object <r>
        std.jsonnet:347:38-40   thunk <i>
        std.jsonnet:347:21-66   function <parse_codes>
        std.jsonnet:347:21-66   function <parse_codes>
        std.jsonnet:351:23-49   thunk <codes>
        std.jsonnet:616:30-34   thunk <codes>
        std.jsonnet:517:32-36
        std.jsonnet:517:21-37   function <format_codes_arr>
        ...
        vSphere.jsonnet:138:21-133      thunk <array_element>
        vSphere.jsonnet:(136:27)-(141:19)       object <anonymous>
        vSphere.jsonnet:(135:32)-(142:17)       object <anonymous>
        vSphere.jsonnet:(134:27)-(143:12)       thunk <array_element>
        vSphere.jsonnet:(134:26)-(147:13)       object <anonymous>
        vSphere.jsonnet:(127:39)-(148:9)        object <anonymous>
        std.jsonnet:(943:13)-(952:13)   object <anonymous>
        std.jsonnet:(943:13)-(952:13)   object <anonymous>
        all.jsonnet:3:49-79     object <anonymous>
        During manifestation
Makefile:70: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:48: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
```",closed,False,2017-12-26 02:49:21,2017-12-26 02:58:20
kubernetes-anywhere,Coolfeather2,https://github.com/kubernetes/kubernetes-anywhere/issues/506,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/506,Error asking for user input: plugin exited before we could connect,"```
[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json /opt/kubernetes-anywhere/.config > /opt/kubernetes-anywhere/.config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/vsphere""; ./do deploy-cluster )
kubernetes/.tmp/vSphere-kubernetes.tf
Error asking for user input: 4 error(s) occurred:

* provider.vsphere: plugin exited before we could connect
* provider.tls: plugin exited before we could connect
* provider.template: plugin exited before we could connect
* provider.null: plugin exited before we could connect
Makefile:70: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:48: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
```",closed,False,2017-12-26 03:26:00,2017-12-27 02:10:00
kubernetes-anywhere,tmbull,https://github.com/kubernetes/kubernetes-anywhere/issues/507,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/507,Connection refused messages after several days,"I have encountered this issue a few times now after running a cluster for several days. All `kubectl` commands time out. For example:

```
root@photon-machine [ ~ ]# kubectl cluster-info dump
The connection to the server 10.1.0.104 was refused - did you specify the right host or port?
```

Here's the journalctl output on master:

```
root@kubernetes-master [ ~ ]# journalctl -n 50 -u kubelet
-- Logs begin at Fri 2017-12-29 06:32:12 UTC, end at Fri 2017-12-29 16:25:36 UTC. --
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.592742     762 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/vnd.kubernetes.protobuf, */*"" -H ""User-Agent: hyperkube/v1.8.5 (linux/amd64) kubernetes/cce11c6
"" https://10.1.0.104/api/v1/services?resourceVersion=0
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.593195     762 round_trippers.go:436] GET https://10.1.0.104/api/v1/services?resourceVersion=0  in 0 milliseconds
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.593228     762 round_trippers.go:442] Response Headers:
Dec 29 16:25:34 kubernetes-master docker[720]: E1229 16:25:34.593335     762 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://10.1.0.104/api/v1/services?resourceVersion=0: dial tcp 1
0.1.0.104:443: getsockopt: connection refused
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.597867     762 reflector.go:240] Listing and watching *v1.Node from k8s.io/kubernetes/pkg/kubelet/kubelet.go:422
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.598398     762 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/vnd.kubernetes.protobuf, */*"" -H ""User-Agent: hyperkube/v1.8.5 (linux/amd64) kubernetes/cce11c6
"" https://10.1.0.104/api/v1/nodes?fieldSelector=metadata.name%3Dkubernetes-master&resourceVersion=0
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.598930     762 round_trippers.go:436] GET https://10.1.0.104/api/v1/nodes?fieldSelector=metadata.name%3Dkubernetes-master&resourceVersion=0  in 0 milliseconds
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.598958     762 round_trippers.go:442] Response Headers:
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.599092     762 reflector.go:240] Listing and watching *v1.Pod from k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47
Dec 29 16:25:34 kubernetes-master docker[720]: E1229 16:25:34.599099     762 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://10.1.0.104/api/v1/nodes?fieldSelector=metadata.name%3Dkuber
netes-master&resourceVersion=0: dial tcp 10.1.0.104:443: getsockopt: connection refused
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.599341     762 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/vnd.kubernetes.protobuf, */*"" -H ""User-Agent: hyperkube/v1.8.5 (linux/amd64) kubernetes/cce11c6
"" https://10.1.0.104/api/v1/pods?fieldSelector=spec.nodeName%3Dkubernetes-master&resourceVersion=0
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.599769     762 round_trippers.go:436] GET https://10.1.0.104/api/v1/pods?fieldSelector=spec.nodeName%3Dkubernetes-master&resourceVersion=0  in 0 milliseconds
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.599799     762 round_trippers.go:442] Response Headers:
Dec 29 16:25:34 kubernetes-master docker[720]: E1229 16:25:34.599891     762 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://10.1.0.104/api/v1/pods?fieldSelector=spec.nodeName%3
Dkubernetes-master&resourceVersion=0: dial tcp 10.1.0.104:443: getsockopt: connection refused
Dec 29 16:25:34 kubernetes-master docker[720]: I1229 16:25:34.857501     762 generic.go:182] GenericPLEG: Relisting
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.593569     762 reflector.go:240] Listing and watching *v1.Service from k8s.io/kubernetes/pkg/kubelet/kubelet.go:413
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.593828     762 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/vnd.kubernetes.protobuf, */*"" -H ""User-Agent: hyperkube/v1.8.5 (linux/amd64) kubernetes/cce11c6
"" https://10.1.0.104/api/v1/services?resourceVersion=0
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.594267     762 round_trippers.go:436] GET https://10.1.0.104/api/v1/services?resourceVersion=0  in 0 milliseconds
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.594293     762 round_trippers.go:442] Response Headers:
Dec 29 16:25:35 kubernetes-master docker[720]: E1229 16:25:35.594381     762 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://10.1.0.104/api/v1/services?resourceVersion=0: dial tcp 1
0.1.0.104:443: getsockopt: connection refused
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.599551     762 reflector.go:240] Listing and watching *v1.Node from k8s.io/kubernetes/pkg/kubelet/kubelet.go:422
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.600020     762 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/vnd.kubernetes.protobuf, */*"" -H ""User-Agent: hyperkube/v1.8.5 (linux/amd64) kubernetes/cce11c6
"" https://10.1.0.104/api/v1/nodes?fieldSelector=metadata.name%3Dkubernetes-master&resourceVersion=0
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.600492     762 round_trippers.go:436] GET https://10.1.0.104/api/v1/nodes?fieldSelector=metadata.name%3Dkubernetes-master&resourceVersion=0  in 0 milliseconds
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.600548     762 round_trippers.go:442] Response Headers:
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.600599     762 reflector.go:240] Listing and watching *v1.Pod from k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47
Dec 29 16:25:35 kubernetes-master docker[720]: E1229 16:25:35.600785     762 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://10.1.0.104/api/v1/nodes?fieldSelector=metadata.name%3Dkuber
netes-master&resourceVersion=0: dial tcp 10.1.0.104:443: getsockopt: connection refused
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.600806     762 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/vnd.kubernetes.protobuf, */*"" -H ""User-Agent: hyperkube/v1.8.5 (linux/amd64) kubernetes/cce11c6
"" https://10.1.0.104/api/v1/pods?fieldSelector=spec.nodeName%3Dkubernetes-master&resourceVersion=0
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.601152     762 round_trippers.go:436] GET https://10.1.0.104/api/v1/pods?fieldSelector=spec.nodeName%3Dkubernetes-master&resourceVersion=0  in 0 milliseconds
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.601186     762 round_trippers.go:442] Response Headers:
Dec 29 16:25:35 kubernetes-master docker[720]: E1229 16:25:35.601272     762 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://10.1.0.104/api/v1/pods?fieldSelector=spec.nodeName%3
Dkubernetes-master&resourceVersion=0: dial tcp 10.1.0.104:443: getsockopt: connection refused
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.785099     762 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.785186     762 kubelet.go:1913] SyncLoop (housekeeping)
Dec 29 16:25:35 kubernetes-master docker[720]: I1229 16:25:35.861211     762 generic.go:182] GenericPLEG: Relisting
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.594597     762 reflector.go:240] Listing and watching *v1.Service from k8s.io/kubernetes/pkg/kubelet/kubelet.go:413
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.595003     762 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/vnd.kubernetes.protobuf, */*"" -H ""User-Agent: hyperkube/v1.8.5 (linux/amd64) kubernetes/cce11c6
"" https://10.1.0.104/api/v1/services?resourceVersion=0
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.596646     762 round_trippers.go:436] GET https://10.1.0.104/api/v1/services?resourceVersion=0  in 1 milliseconds
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.596733     762 round_trippers.go:442] Response Headers:
Dec 29 16:25:36 kubernetes-master docker[720]: E1229 16:25:36.597880     762 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://10.1.0.104/api/v1/services?resourceVersion=0: dial tcp 1
0.1.0.104:443: getsockopt: connection refused
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.600983     762 reflector.go:240] Listing and watching *v1.Node from k8s.io/kubernetes/pkg/kubelet/kubelet.go:422
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.601186     762 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/vnd.kubernetes.protobuf, */*"" -H ""User-Agent: hyperkube/v1.8.5 (linux/amd64) kubernetes/cce11c6
"" https://10.1.0.104/api/v1/nodes?fieldSelector=metadata.name%3Dkubernetes-master&resourceVersion=0
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.601523     762 round_trippers.go:436] GET https://10.1.0.104/api/v1/nodes?fieldSelector=metadata.name%3Dkubernetes-master&resourceVersion=0  in 0 milliseconds
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.601550     762 round_trippers.go:442] Response Headers:
Dec 29 16:25:36 kubernetes-master docker[720]: E1229 16:25:36.601626     762 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://10.1.0.104/api/v1/nodes?fieldSelector=metadata.name%3Dkuber
netes-master&resourceVersion=0: dial tcp 10.1.0.104:443: getsockopt: connection refused
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.602124     762 reflector.go:240] Listing and watching *v1.Pod from k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.602329     762 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/vnd.kubernetes.protobuf, */*"" -H ""User-Agent: hyperkube/v1.8.5 (linux/amd64) kubernetes/cce11c6
"" https://10.1.0.104/api/v1/pods?fieldSelector=spec.nodeName%3Dkubernetes-master&resourceVersion=0
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.602657     762 round_trippers.go:436] GET https://10.1.0.104/api/v1/pods?fieldSelector=spec.nodeName%3Dkubernetes-master&resourceVersion=0  in 0 milliseconds
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.602684     762 round_trippers.go:442] Response Headers:
Dec 29 16:25:36 kubernetes-master docker[720]: E1229 16:25:36.602778     762 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://10.1.0.104/api/v1/pods?fieldSelector=spec.nodeName%3
Dkubernetes-master&resourceVersion=0: dial tcp 10.1.0.104:443: getsockopt: connection refused
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.663137     762 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Dec 29 16:25:36 kubernetes-master docker[720]: I1229 16:25:36.864422     762 generic.go:182] GenericPLEG: Relisting
```

It seems to be stuck in this loop, but I don't know why or how to recover from it. I'm still pretty new to kubernetes. Any help is appreciated.",closed,False,2017-12-29 16:27:32,2018-08-02 19:06:31
kubernetes-anywhere,tarunamati,https://github.com/kubernetes/kubernetes-anywhere/issues/508,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/508,"Tried to deploy Kubernetes on vSphere but errors out on ""insecure""","This is what id got 

![image](https://user-images.githubusercontent.com/7960096/34539052-0a595ade-f09d-11e7-99fb-e2fb303bb2ad.png)

This is my vCenter configuration

![image](https://user-images.githubusercontent.com/7960096/34539254-ca6c4228-f09d-11e7-811a-86a64cf5653c.png)

Please advise, I know this is very little information but I am not sure what else I need to give. I just got introduced to Kubernetes. 


",closed,False,2018-01-03 20:52:05,2018-06-14 22:57:25
kubernetes-anywhere,benjaminknox,https://github.com/kubernetes/kubernetes-anywhere/pull/509,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/509,Fix typo under the Phase 3 header,,closed,True,2018-01-09 14:41:01,2018-08-10 07:18:03
kubernetes-anywhere,benjaminknox,https://github.com/kubernetes/kubernetes-anywhere/issues/510,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/510,Cannot start kubernetes master,"Trying to run the `make deploy` and I have this issue:
```
KUBECONFIG=""$(pwd)/phase1/$(jq -r '.phase1.cloud_provider' .config.json)/.tmp/kubeconfig.json"" ./util/validate
Validation: Expected 5 healthy nodes; found 0. (10s elapsed)
Validation: Expected 5 healthy nodes; found 0. (20s elapsed)
...
...
...
...
Validation: Expected 5 healthy nodes; found 0. (390s elapsed)
Validation: Expected 5 healthy nodes; found 0. (400s elapsed)

```
I tried the image suggested in https://github.com/kubernetes/kubernetes-anywhere/issues/424 but it didn't work.

I ssh'd into the master and ran `journalctl -u kubelet`, it gave me this output:
```
Jan 09 22:19:44 master systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
Jan 09 22:19:44 master systemd[1]: Stopped Kubernetes Kubelet Server.
Jan 09 22:19:44 master systemd[1]: Starting Kubernetes Kubelet Server...
Jan 09 22:19:58 master systemd[1]: Started Kubernetes Kubelet Server.
Jan 09 22:20:04 master docker[1672]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.
Jan 09 22:20:04 master docker[1672]: Flag --register-schedulable has been deprecated, will be removed in a future version
Jan 09 22:20:04 master docker[1672]: I0109 22:20:04.031965    1697 feature_gate.go:144] feature gates: map[]
Jan 09 22:20:04 master docker[1672]: Error: failed to run Kubelet: could not init cloud provider ""vsphere"": 3:33: unknown escape sequence
Jan 09 22:20:04 master docker[1672]: Error: failed to run Kubelet: could not init cloud provider ""vsphere"": 3:33: unknown escape sequence
Jan 09 22:20:15 master systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Jan 09 22:20:15 master systemd[1]: kubelet.service: Unit entered failed state.
Jan 09 22:20:15 master systemd[1]: kubelet.service: Failed with result 'exit-code'.
```

Really need some help! Let me know if you need more information.",closed,False,2018-01-09 22:28:58,2018-05-19 00:12:42
kubernetes-anywhere,panggou,https://github.com/kubernetes/kubernetes-anywhere/issues/511,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/511,vsphere5.5 - make deploy failed,"I am getting the following error from ""make deploy"".
null_resource.node3 (remote-exec): Job for flannelc.service failed because a timeout was exceeded. See ""systemctl status flannelc.service"" and ""journalctl -xe"" for details.
null_resource.node3 (remote-exec): Failed to start flannelc
null_resource.node3: Creation complete (ID: 7952397853066288264)
null_resource.node2 (remote-exec): Job for flannelc.service failed because a timeout was exceeded. See ""systemctl status flannelc.service"" and ""journalctl -xe"" for details.
null_resource.node2 (remote-exec): Failed to start flannelc
null_resource.node2: Creation complete (ID: 7478055152772443477)
null_resource.node5 (remote-exec): Job for flannelc.service failed because a timeout was exceeded. See ""systemctl status flannelc.service"" and ""journalctl -xe"" for details.
null_resource.node5 (remote-exec): Failed to start flannelc
null_resource.node5: Creation complete (ID: 91678433144451082)
null_resource.node4 (remote-exec): Job for flannelc.service failed because a timeout was exceeded. See ""systemctl status flannelc.service"" and ""journalctl -xe"" for details.
null_resource.node4 (remote-exec): Failed to start flannelc
null_resource.node4: Creation complete (ID: 2867989311161441499)
-> all 4 nodes gave me the above, while master node just keep showing this:
null_resource.master: Still creating... (5m50s elapsed)
null_resource.master: Still creating... (6m0s elapsed)
null_resource.master: Still creating... (6m10s elapsed)
null_resource.master: Still creating... (6m20s elapsed)
null_resource.master: Still creating... (6m30s elapsed)
null_resource.master: Still creating... (6m40s elapsed)


SSH to any of the nodes, running ""journalctl -xe"" gives me this:
Jan 15 08:30:19 node4 systemd[1]: Started User Manager for UID 0.
Subject: Unit user@0.service has finished start-up
Defined-By: systemd
Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
Unit user@0.service has finished starting up.
The start-up result is done.
**Jan 15 08:30:22 node4 flanneld[362]: E0115 08:30:22.621731 00362 network.go:53] Failed to retrieve network config: client: etcd cluster is unavailable or misconfigured
Jan 15 08:30:26 node4 flanneld[362]: E0115 08:30:26.035830 00362 network.go:53] Failed to retrieve network config: client: etcd cluster is unavailable or misconfigured
Jan 15 08:30:29 node4 flanneld[362]: E0115 08:30:29.042539 00362 network.go:53] Failed to retrieve network config: client: etcd cluster is unavailable or misconfigured**


Thanks in advance for your help.",closed,False,2018-01-15 08:34:03,2018-01-15 13:20:11
kubernetes-anywhere,panggou,https://github.com/kubernetes/kubernetes-anywhere/issues/512,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/512,"Issue with ""cnastorage/kubernetes-anywhere:lastest"" on vsphere 5.5","REPOSITORY                       TAG                 IMAGE ID            CREATED             SIZE
cnastorage/kubernetes-anywhere   latest              f6197e2e3666        4 months ago        713 MB
cnastorage/kubernetes-anywhere   vmc                 e42e2191e6a7        5 months ago        710.9 MB

cnastorage/kubernetes-anywhere:vmc --> works!

cnastorage/kubernetes-anywhere:latest --> doesnt work with the same .config!!!
make deploy reaches the last stage but the following loops indefinitely.
Validation: Expected 5 healthy nodes; found 1. (10s elapsed)
Validation: Expected 5 healthy nodes; found 1. (20s elapsed)
Validation: Expected 5 healthy nodes; found 1. (30s elapsed)
Validation: Expected 5 healthy nodes; found 1. (40s elapsed)
Validation: Expected 5 healthy nodes; found 1. (50s elapsed)
Validation: Expected 5 healthy nodes; found 1. (60s elapsed)
Validation: Expected 5 healthy nodes; found 1. (70s elapsed)
Validation: Expected 5 healthy nodes; found 1. (80s elapsed)
Validation: Expected 5 healthy nodes; found 1. (90s elapsed)
Validation: Expected 5 healthy nodes; found 1. (100s elapsed)
Validation: Expected 5 healthy nodes; found 1. (110s elapsed)
Validation: Expected 5 healthy nodes; found 1. (120s elapsed)
Validation: Expected 5 healthy nodes; found 1. (130s elapsed)
Validation: Expected 5 healthy nodes; found 1. (140s elapsed)

What is the difference between ""cnastorage/kubernetes-anywhere:vmc"" and ""cnastorage/kubernetes-anywhere:latest""?  They are both v1.65.

Where do I get newer version of kubernetes-anywhere that deploys v1.7+?

Thanks in advance.

",closed,False,2018-01-15 13:26:48,2019-01-13 01:41:49
kubernetes-anywhere,Lion-Wei,https://github.com/kubernetes/kubernetes-anywhere/issues/513,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/513,CI jobs that build by kubernetes-anywhere failed,"kubernetes-anywhere have some problem in build cluster in gce environment, result in some CI job failed:

[kubeadm-gce-ipvs](http://k8s-testgrid.appspot.com/sig-network-gce#kubeadm-gce-ipvs)
[kubeadm-gce-cni-calico](http://k8s-testgrid.appspot.com/sig-network-gce#kubeadm-gce-cni-calico)
[kubeadm-gce-cni-flannel](http://k8s-testgrid.appspot.com/sig-network-gce#kubeadm-gce-cni-flannel)

I tested build cluster in my own environment, turnout run startup script failed in this cmd :
```
dpkg -i $TMPDIR/{kubelet,kubeadm,kubectl,kubernetes-cni}.deb || echo Ignoring expected dpkg failure
  apt-get install -f -y
  systemctl enable kubelet
```

`systemctl enable kubelet` result with `Failed to execute operation: No such file or directory`.

I can see `apt-get install -f -y` removed kubelet and kubeadm, lead to this problem, but I'm not sure why,  after all, this script haven't change for a long time.",closed,False,2018-01-23 13:10:44,2018-02-20 17:53:48
kubernetes-anywhere,leblancd,https://github.com/kubernetes/kubernetes-anywhere/pull/514,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/514,Kubeadm installs fail with removal of kubelet and kubeadm,"## Problem Description:
Kubeadm installations are failing on the master node. Looking at
startup logs on the master node:

```
Jan 23 01:56:33 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script: dpkg: error processing archive /tmp/k8s-debs/kubernetes-cni.deb (--install):
Jan 23 01:56:33 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script:  error creating directory '/opt/cni/bin': No such file or directory
Jan 23 01:56:33 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script: dpkg-deb: error: subprocess paste was killed by signal (Broken pipe)
Jan 23 01:56:33 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script: dpkg: dependency problems prevent configuration of kubelet:
Jan 23 01:56:33 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script:  kubelet depends on kubernetes-cni (>= 0.5.1); however:
Jan 23 01:56:33 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script:   Package kubernetes-cni is not installed.
```
followed by:
```
Jan 23 01:56:33 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script:  /tmp/k8s-debs/kubernetes-cni.deb
Jan 23 01:56:33 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script:  kubelet
Jan 23 01:56:33 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script:  kubeadm
```
and:
```
Jan 23 01:56:35 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script: The following packages will be REMOVED:
Jan 23 01:56:35 e2e-9c7300d2ef-master startup-script[1785]: INFO startup-script:   kubeadm kubelet
```

Apparently what's happening is this:
- Startup script tries to install kubernetes-cni, kubelet, and kubeadm from debs packages
- Installation of kubernetes-cni fails because the directory /opt/cni/bin can't be created
- Installation of kubelet fails because it depends upon kubernetes-cni being installed
- Installation of kubeadm fails because it depends upon kubelet being installed
- The 'apt-get install -f -y' cmd detects these dependency failures and remove kubelet and kubeadm
- Because kubeadm is missing, the 'kubeadm init ...' command can't be run, so that the kubeconfig in /etc/kubernetes is never created
- Because the kubeconfig is never created, kubernetes-anywhere fails after 50 (or is it 60, I forget) attempts to read the kubeconfig.

## Fix:
Preemptively adding a creation of a directory /opt/cni/bin before installing
kubernetes-cni fixes the problem.

## Log with Fix Showing Success:
[Successful Log Snippet](https://gist.github.com/leblancd/f596e4b34f4768efa7748d3cca4a062e)

fixes #513 
",closed,True,2018-01-25 23:34:03,2018-02-22 03:18:49
kubernetes-anywhere,csbogdan,https://github.com/kubernetes/kubernetes-anywhere/issues/515,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/515,Can not make deploy,"Trying to run `make deploy` and getting this:

```
[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json /opt/kubernetes-anywhere/.config > /opt/kubernetes-anywhere/.config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/vsphere""; ./do deploy-cluster )
RUNTIME ERROR: expected string but arr[2] was number
	std.jsonnet:200:17-95	function <aux>
	std.jsonnet:204:17-63	function <aux>
	std.jsonnet:208:13-34	function <anonymous>
	vSphere.jsonnet:6:177-224	thunk <array_element>
	std.jsonnet:585:29-36	thunk <val>
	std.jsonnet:592:41-44	thunk <val>
	std.jsonnet:489:30-33	thunk <a>
	std.jsonnet:35:21
	std.jsonnet:35:12-23	thunk <a>
	std.jsonnet:35:12-35	function <anonymous>
	...
	std.jsonnet:197:21-35	function <aux>
	std.jsonnet:202:17-57	function <aux>
	std.jsonnet:208:13-34	function <anonymous>
	vSphere.jsonnet:48:33-64	object <anonymous>
	vSphere.jsonnet:(40:17)-(53:12)	object <anonymous>
	vSphere.jsonnet:(38:27)-(54:10)	object <anonymous>
	vSphere.jsonnet:(37:22)-(86:8)	object <anonymous>
	std.jsonnet:(988:13)-(997:14)	object <anonymous>
	all.jsonnet:3:49-80	object <anonymous>
	During manifestation
Makefile:77: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:49: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
```

My make config:

```[container]:/opt/kubernetes-anywhere> make config
CONFIG_=""."" kconfig-conf Kconfig
*
* Kubernetes Minimal Turnup Configuration
*
*
* Phase 1: Cluster Resource Provisioning
*
number of nodes (phase1.num_nodes) [3]
kubernetes cluster name (phase1.cluster_name) [kubernetes]
SSH user to login to OS for provisioning (phase1.ssh_user) [root]
*
* cloud provider: gce, vsphere or openstack
*
cloud provider: gce, vsphere or openstack (phase1.cloud_provider) [vsphere]
  *
  * vSphere configuration
  *
  vCenter URL Ex: 10.192.10.30 or myvcenter.io (without https://) (phase1.vSphere.url) [192.168.0.132]
  vCenter port (phase1.vSphere.port) [443]
  vCenter username (phase1.vSphere.username) [bogdan@xxxx]
  vCenter password (phase1.vSphere.password) [xxxxxx]
  Does host use self-signed cert (phase1.vSphere.insecure) [Y/n/?]
  Datacenter (phase1.vSphere.datacenter) [AVMs]
  Datastore (phase1.vSphere.datastore) [DATA]
  Deploy Kubernetes Cluster on 'host' or 'cluster' (phase1.vSphere.placement) [host]
    host (phase1.vSphere.host) [192.168.0.108]
  Do you want to use the existing resource pool on the host or cluster? [yes, no] (phase1.vSphere.useresourcepool) [no]
  VM Folder name or Path (e.g kubernetes, VMFolder1/dev-cluster, VMFolder1/Test Group1/test-cluster). Folder path will be created if not present (phase1.vSphere.vmfolderpath) [kubernetes]
  Number of vCPUs for each VM (phase1.vSphere.vcpu) [2]
  Memory for VM (phase1.vSphere.memory) [2048]
  Network for VM (phase1.vSphere.network) [VM Network]
  Name of the template VM imported from OVA. If Template file is not available at the destination location specify vm path (phase1.vSphere.template) [KubernetesAnywhereTemplatePhotonOS.ova]
  Flannel Network (phase1.vSphere.flannel_net) [172.1.0.0/16]
*
* Phase 2: Node Bootstrapping
*
kubernetes version (phase2.kubernetes_version) [v1.8.4]
docker registry (phase2.docker_registry) [gcr.io/google-containers]
bootstrap provider (phase2.provider) [ignition]
kube-proxy mode: iptables or ipvs (phase2.proxy_mode) [iptables]
installer container (phase2.installer_container) [docker.io/ashivani/k8s-ignition:v4] docker.io/cnastorage/etcd3-ignition:k8smaster
*
* Phase 3: Deploying Addons
*
Run the addon manager? (phase3.run_addons) [Y/n/?]
  Run kube-proxy? (phase3.kube_proxy) [Y/n/?]
  Run kube-dns? (phase3.kube_dns) [Y/n/?]
  Run the dashboard? (phase3.dashboard) [Y/n/?]
  Run heapster? (phase3.heapster) [Y/n/?]
  Run weave-net? (phase3.weave_net) [Y/n/?]
#
# configuration written to .config
#
```

Please provide some help. Thanks!",closed,False,2018-01-26 00:15:37,2018-06-28 09:23:55
kubernetes-anywhere,csbogdan,https://github.com/kubernetes/kubernetes-anywhere/pull/516,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/516,Fix Flannel Network default value as per RFC1918,,closed,True,2018-01-26 00:43:28,2018-06-25 02:56:28
kubernetes-anywhere,keir86,https://github.com/kubernetes/kubernetes-anywhere/issues/517,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/517,Containers log limit on VSphere,"I have installed k8s 1..8.6 on VSphere through the kubernetes-anywhere. And seems like this installation doesn`t have neither logrotate nor docker-opts max-size. Containers log files grows till the nodefs eviction of node. Special - api logs. I had a file like 50G after a few days. I tried to change docker options in /etc/default/docker, but it had no effect. This valus is overwritten on reboot.

Any idea how to change docker options? To be honest, I don`t want to install logrotate on all nodes, when docker engine has it`s own logrotate",closed,False,2018-01-31 13:53:29,2018-06-30 16:05:42
kubernetes-anywhere,serkilov,https://github.com/kubernetes/kubernetes-anywhere/issues/518,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/518,IP addressing in vSphere,"1) Is there a way to avoid cluster to be broken after IP is changed? This sometimes happens since IPs are dynamic. What should be done to fix it?

2) Is there a way to use static IPs in configuration so that cluster will use them in a process of deploying?",closed,False,2018-02-12 10:14:21,2018-09-20 18:25:53
kubernetes-anywhere,stuart-stanley,https://github.com/kubernetes/kubernetes-anywhere/issues/519,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/519,Can't find any indication of how vSphere OVA is created,"The various instructions/readme/blog-posts all point to the magic photon vSphere OVA, but I can't find any reference to where/how it is built. 

I wish I could just take the magic OVA and add/modify everything during terraform, but I have some stupid environmental things that are stupid(tm) ;).

Clues please? Happy to walk through scribbled notes and turn into a step-by-step for the repo!

Thanks.",closed,False,2018-02-13 17:13:26,2018-07-13 19:15:13
kubernetes-anywhere,Boran,https://github.com/kubernetes/kubernetes-anywhere/issues/520,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/520,kubectl version,"Question: What kubectl client version should be used?

Bringing up a Kubernetes cluster on vsphere worked fine.
Now I'm wondering what version of kubectl to use for management. The config mentions 1.6., but that is old. When deploying example containers (https://kubernetes.io/docs/tasks/run-application) via kubectl 1.6.5 from , I get 

> Deployment in version ""v1beta2"" cannot be handled as a Deployment
or
> Deployment in version ""v1"" cannot be handled as a Deployment
Depending on where apiVersion: apps/v1 or apps/v1beta2 is in the yml file.

Using a new v1.9.3 kubectl:
> the server could not find the requested resource
which is due to kubectl asking for /swagger-2.0.0.pb-v1, i.e. aversion of swagger not on the server.
",closed,False,2018-02-19 12:16:30,2018-02-22 09:05:04
kubernetes-anywhere,mcdave2k1,https://github.com/kubernetes/kubernetes-anywhere/issues/521,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/521,"deployment kubernetes on master fails , ","Deployment on vSphere running without problems..

tls_locally_signed_cert.kubernetes-master: Creation complete (ID: 170559125117412117697738662694444858326)
data.template_file.configure_master: Refreshing state...
data.template_file.configure_node: Refreshing state...
null_resource.kubernetes-node2: Creating...
null_resource.kubernetes-master: Creating...
null_resource.kubernetes-node2: Provisioning with 'remote-exec'...
null_resource.kubernetes-master: Provisioning with 'remote-exec'...
null_resource.kubernetes-node2 (remote-exec): Connecting to remote host via SSH...
null_resource.kubernetes-node2 (remote-exec):   Host: 10.89.10.117
null_resource.kubernetes-node2 (remote-exec):   User: root
null_resource.kubernetes-node2 (remote-exec):   Password: true
null_resource.kubernetes-node2 (remote-exec):   Private key: false
null_resource.kubernetes-node2 (remote-exec):   SSH Agent: false
null_resource.kubernetes-master (remote-exec): Connecting to remote host via SSH...
null_resource.kubernetes-master (remote-exec):   Host: 10.0.0.118
null_resource.kubernetes-master (remote-exec):   User: root
null_resource.kubernetes-master (remote-exec):   Password: true
null_resource.kubernetes-master (remote-exec):   Private key: false
null_resource.kubernetes-master (remote-exec):   SSH Agent: false
null_resource.kubernetes-master (remote-exec): Connected!
null_resource.kubernetes-node2 (remote-exec): Connected!
null_resource.kubernetes-node2 (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/flannelc.service to /usr/lib/systemd/system/flannelc.service.
null_resource.kubernetes-master (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service.
null_resource.kubernetes-master (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/flanneld.service to /usr/lib/systemd/system/flanneld.service.
null_resource.kubernetes-node2 (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.
null_resource.kubernetes-master (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.
null_resource.kubernetes-node2 (remote-exec): v1.6.5: Pulling from google-containers/hyperkube-amd64

null_resource.kubernetes-node2 (remote-exec): 6edcfa5cfab7: Pulling fs layer
...
null_resource.kubernetes-node2 (remote-exec): 158914f426fa: Pull complete
....
cnull_resource.kubernetes-master: Still creating... (9m10s elapsed)

null_resource.kubernetes-master (remote-exec): DEBUG    : files: createFilesystemsFiles: createFiles: GET error: **Get https://storage.googleapis.com/kubernetes-release/release/v1.6.5/bin/linux/amd64/kubectl: dial tcp 216.58.207.176:443: i/o timeout**
null_resource.kubernetes-master: Still creating... (9m20s elapsed)
null_resource.kubernetes-master (remote-exec): CRITICAL : files: createFilesystemsFiles: createFiles: Error fetching file ""/usr/local/bin/kubectl"": unable to fetch resource (no more attempts available)
null_resource.kubernetes-master (remote-exec): CRITICAL : files: failed to create files: failed to create files: failed to resolve file ""/usr/local/bin/kubectl""
null_resource.kubernetes-master (remote-exec): Failed to docker run installer container
Error applying plan:
c

i configured proxy for docker service  +   system (/etc/sysconfig/proxy) 

/etc/systemd/system/docker.service.d/http-proxy.conf +  https-proxy.conf
[Service]
Environment=""HTTP_PROXY=http://10.17.40.58:8080/""
Environment=""HTTPS_PROXY=http://10.17.40.58:8080/""
Environment=""NO_PROXY=127.0.0.1,10.0.0.0/8,localhost""

kubernetes-anywhere/phase2/ignition/Dockerfile
ENV http_proxy ""http://10.17.40.58:8080/""
ENV https_proxy ""http://10.17.40.58:8080/""


does the proxy server have to be transferred directly in phase2?
i got no chance to deploy the cluster 

have anyone some experience with kubernetes-anywhere without direct access to internet? 
which location in configuration i missed? 

thx and regards

dave
",closed,False,2018-02-27 17:35:45,2018-07-27 19:45:53
kubernetes-anywhere,andyanfieldroad,https://github.com/kubernetes/kubernetes-anywhere/issues/522,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/522,Connection refused after host crash,"I have a kubernetes-anywhere cluster that was running from a local-disk datastore that dropped off the host causing the host and all VMs to freeze.  I power cycled and when it came back up my kubernetes was dead.  I have run an fsck and corrected issues but it seems the etcd database is corrupt as shown in the logs below:

{""log"":""2018-03-01 16:27:33.234738 I | etcdmain: etcd Version: 2.2.1\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.235736085Z""}
{""log"":""2018-03-01 16:27:33.234955 I | etcdmain: Git SHA: 75f8282\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.235803677Z""}
{""log"":""2018-03-01 16:27:33.234974 I | etcdmain: Go Version: go1.5.1\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.235818686Z""}
{""log"":""2018-03-01 16:27:33.235020 I | etcdmain: Go OS/Arch: linux/amd64\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.235831967Z""}
{""log"":""2018-03-01 16:27:33.235109 I | etcdmain: setting maximum number of CPUs to 4, total number of available CPUs is 4\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.235844934Z""}
{""log"":""2018-03-01 16:27:33.235940 N | etcdmain: the server is already initialized as member before, starting as etcd member...\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.236180284Z""}
{""log"":""2018-03-01 16:27:33.236455 I | etcdmain: listening for peers on http://127.0.0.1:2380\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.236574017Z""}
{""log"":""2018-03-01 16:27:33.236532 I | etcdmain: listening for client requests on http://127.0.0.1:2379\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.236718432Z""}
{""log"":""2018-03-01 16:27:33.533719 I | etcdserver: recovered store from snapshot at index 17411741\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.534129193Z""}
{""log"":""2018-03-01 16:27:33.533911 I | etcdserver: name = default\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.534197104Z""}
{""log"":""2018-03-01 16:27:33.533932 I | etcdserver: data dir = /var/etcd/data\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.53421046Z""}
{""log"":""2018-03-01 16:27:33.533966 I | etcdserver: member dir = /var/etcd/data/member\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.534222751Z""}
{""log"":""2018-03-01 16:27:33.533988 I | etcdserver: heartbeat = 100ms\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.534235093Z""}
{""log"":""2018-03-01 16:27:33.534014 I | etcdserver: election = 1000ms\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.534249549Z""}
{""log"":""2018-03-01 16:27:33.534049 I | etcdserver: snapshot count = 10000\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.534262246Z""}
{""log"":""2018-03-01 16:27:33.534121 I | etcdserver: advertise client URLs = http://127.0.0.1:2379\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.534274655Z""}
{""log"":""2018-03-01 16:27:33.534231 I | etcdserver: loaded cluster information from store: \u003cnil\u003e\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:33.534319006Z""}
{""log"":""2018-03-01 16:27:34.074785 C | etcdserver: read wal error (walpb: crc mismatch) and cannot be repaired\n"",""stream"":""stderr"",""time"":""2018-03-01T16:27:34.07510335Z""}

I have a backup taken consisting of a snap and a wal file but i cannot seem to restore them using the kubernetes documentation I have read.

What can I do to recover this cluster?  I really don't want to have to rebuild it as it is very customised from the original kubernetes-anywhere cluster build.
 ",closed,False,2018-03-02 01:00:06,2018-07-30 05:42:22
kubernetes-anywhere,EmiiKhaos,https://github.com/kubernetes/kubernetes-anywhere/pull/523,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/523,Fix Flannel network default value as per RFC1918,"| Q | A
| --- | ---
| Bug fix? | yes
| New feature? | no
| BC breaks? | no
| Deprecations? | no
| Fixed tickets | fixes #497 
| Related issues/PRs | supersedes #516
| License | Apache 2.0
| CLA signed | yes

#### What's in this PR?

Fixes the default value of the vSphere flannel network to 172.16.0.0/16, which is within the private ""20-bit block"" according to RFC1918.

I assume the 172.1.0.0/16 was solely a typo.",closed,True,2018-03-02 10:42:04,2018-05-05 23:18:20
kubernetes-anywhere,panggou,https://github.com/kubernetes/kubernetes-anywhere/issues/524,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/524,KubernetesAnywhereTemplatePhotonOSESX5.5 root password,"I am trying to increase the size of the template VM.
https://storage.googleapis.com/kubernetes-anywhere-for-vsphere-cna-storage/KubernetesAnywhereTemplatePhotonOSESX5.5.ova

However the root password ""changeme"" or ""kubenetes"" does not work.

Can anyone let me know what is the root password for KubernetesAnywhereTemplatePhotonOSESX5.5.ova?

Thanks in advance.
",closed,False,2018-03-05 08:35:58,2018-03-05 08:51:22
kubernetes-anywhere,tickers,https://github.com/kubernetes/kubernetes-anywhere/issues/525,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/525,socat install ,"Trying to install Helm and the main error according to Helm is that socat is not installed and so far I have not found a tdnf that would work and the source install requires a compiler, installed GCC, still fails to build so I am stuck and would appreciate any insight. 

If I find a way I will post my steps. 

Running Kubernetes 1.9.3 on VMWare 6.5. ",closed,False,2018-03-06 19:35:26,2018-08-24 06:31:35
kubernetes-anywhere,nikhilgeo,https://github.com/kubernetes/kubernetes-anywhere/issues/526,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/526, null_resource.kubernetes-master: 1 error(s) occurred:,"I was trying to deploy the kubernetes on VC 6.5, I'm stuck with the error:
```
Error applying plan:
1 error(s) occurred:
* null_resource.kubernetes-master: 1 error(s) occurred:
* Script exited with non-zero exit status: 1
```
Below is the deployment log. It would be very helpful if anyone can point out the work-around.

```
make deploy
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/vsphere""; ./do deploy-cluster )
kubernetes/.tmp/vSphere-kubernetes.tf
tls_private_key.kubernetes-admin: Refreshing state... (ID: 24cf47d4dacf9ab673f32f770c6ba8649a4acff1)
tls_private_key.kubernetes-root: Refreshing state... (ID: 8009658cbf6f04b56da632d6fc9aed8c5ba3c969)
tls_private_key.kubernetes-node: Refreshing state... (ID: 7baa46937726cc6e72573a31b20731755effda55)
tls_private_key.kubernetes-master: Refreshing state... (ID: 26c4edea2c5312303809f4657e73b9dfb3daa9d0)
tls_cert_request.kubernetes-node: Refreshing state... (ID: 843a939ba8c90c1865584b4a72f7ed3c3ee02651)
tls_self_signed_cert.kubernetes-root: Refreshing state... (ID: 95883925964588396172938629057522969551)
tls_cert_request.kubernetes-admin: Refreshing state... (ID: 5d08d1b05077576816fc537a66e9fcfdfa12a118)
data.template_file.cloudprovider: Refreshing state...
tls_locally_signed_cert.kubernetes-node: Refreshing state... (ID: 154459155633387887988179167534111531092)
tls_locally_signed_cert.kubernetes-admin: Refreshing state... (ID: 40212693340808016328956886277578725288)
vsphere_folder.cluster_folder: Refreshing state... (ID: DC1/kubernetes)
vsphere_virtual_machine.kubevm4: Refreshing state... (ID: kubernetes/kubernetes-node3)
vsphere_virtual_machine.kubevm1: Refreshing state... (ID: kubernetes/kubernetes-master)
vsphere_virtual_machine.kubevm5: Refreshing state... (ID: kubernetes/kubernetes-node4)
vsphere_virtual_machine.kubevm2: Refreshing state... (ID: kubernetes/kubernetes-node1)
vsphere_virtual_machine.kubevm3: Refreshing state... (ID: kubernetes/kubernetes-node2)
vsphere_virtual_machine.kubevm5: Creating...
  datacenter:                             """" => ""DC1""
  detach_unknown_disks_on_delete:         """" => ""false""
  disk.#:                                 """" => ""1""
  disk.3306682066.bootable:               """" => ""true""
  disk.3306682066.controller_type:        """" => ""scsi""
  disk.3306682066.datastore:              """" => ""datastore1""
  disk.3306682066.iops:                   """" => """"
  disk.3306682066.keep_on_remove:         """" => """"
  disk.3306682066.key:                    """" => ""<computed>""
  disk.3306682066.name:                   """" => """"
  disk.3306682066.size:                   """" => """"
  disk.3306682066.template:               """" => ""/DC1/vm/KubernetesAnywhereTemplatePhotonOS.ova""
  disk.3306682066.type:                   """" => ""thin""
  disk.3306682066.uuid:                   """" => ""<computed>""
  disk.3306682066.vmdk:                   """" => """"
  domain:                                 """" => ""vsphere.local""
  enable_disk_uuid:                       """" => ""true""
  folder:                                 """" => ""kubernetes""
  linked_clone:                           """" => ""false""
  memory:                                 """" => ""2048""
  memory_reservation:                     """" => ""0""
  name:                                   """" => ""kubernetes-node4""
  network_interface.#:                    """" => ""1""
  network_interface.0.ip_address:         """" => ""<computed>""
  network_interface.0.ipv4_address:       """" => ""<computed>""
  network_interface.0.ipv4_gateway:       """" => ""<computed>""
  network_interface.0.ipv4_prefix_length: """" => ""<computed>""
  network_interface.0.ipv6_address:       """" => ""<computed>""
  network_interface.0.ipv6_gateway:       """" => ""<computed>""
  network_interface.0.ipv6_prefix_length: """" => ""<computed>""
  network_interface.0.label:              """" => ""VM Network""
  network_interface.0.mac_address:        """" => ""<computed>""
  network_interface.0.subnet_mask:        """" => ""<computed>""
  resource_pool:                          """" => ""10.112.185.145""
  skip_customization:                     """" => ""true""
  time_zone:                              """" => ""Etc/UTC""
  uuid:                                   """" => ""<computed>""
  vcpu:                                   """" => ""1""
vsphere_virtual_machine.kubevm1: Creating...
  datacenter:                             """" => ""DC1""
  detach_unknown_disks_on_delete:         """" => ""false""
  disk.#:                                 """" => ""1""
  disk.3306682066.bootable:               """" => ""true""
  disk.3306682066.controller_type:        """" => ""scsi""
  disk.3306682066.datastore:              """" => ""datastore1""
  disk.3306682066.iops:                   """" => """"
  disk.3306682066.keep_on_remove:         """" => """"
  disk.3306682066.key:                    """" => ""<computed>""
  disk.3306682066.name:                   """" => """"
  disk.3306682066.size:                   """" => """"
  disk.3306682066.template:               """" => ""/DC1/vm/KubernetesAnywhereTemplatePhotonOS.ova""
  disk.3306682066.type:                   """" => ""thin""
  disk.3306682066.uuid:                   """" => ""<computed>""
  disk.3306682066.vmdk:                   """" => """"
  domain:                                 """" => ""vsphere.local""
  enable_disk_uuid:                       """" => ""true""
  folder:                                 """" => ""kubernetes""
  linked_clone:                           """" => ""false""
  memory:                                 """" => ""2048""
  memory_reservation:                     """" => ""0""
  name:                                   """" => ""kubernetes-master""
  network_interface.#:                    """" => ""1""
  network_interface.0.ip_address:         """" => ""<computed>""
  network_interface.0.ipv4_address:       """" => ""<computed>""
  network_interface.0.ipv4_gateway:       """" => ""<computed>""
  network_interface.0.ipv4_prefix_length: """" => ""<computed>""
  network_interface.0.ipv6_address:       """" => ""<computed>""
  network_interface.0.ipv6_gateway:       """" => ""<computed>""
  network_interface.0.ipv6_prefix_length: """" => ""<computed>""
  network_interface.0.label:              """" => ""VM Network""
  network_interface.0.mac_address:        """" => ""<computed>""
  network_interface.0.subnet_mask:        """" => ""<computed>""
  resource_pool:                          """" => ""10.112.185.145""
  skip_customization:                     """" => ""true""
  time_zone:                              """" => ""Etc/UTC""
  uuid:                                   """" => ""<computed>""
  vcpu:                                   """" => ""1""
vsphere_virtual_machine.kubevm3: Creating...
  datacenter:                             """" => ""DC1""
  detach_unknown_disks_on_delete:         """" => ""false""
  disk.#:                                 """" => ""1""
  disk.3306682066.bootable:               """" => ""true""
  disk.3306682066.controller_type:        """" => ""scsi""
  disk.3306682066.datastore:              """" => ""datastore1""
  disk.3306682066.iops:                   """" => """"
  disk.3306682066.keep_on_remove:         """" => """"
  disk.3306682066.key:                    """" => ""<computed>""
  disk.3306682066.name:                   """" => """"
  disk.3306682066.size:                   """" => """"
  disk.3306682066.template:               """" => ""/DC1/vm/KubernetesAnywhereTemplatePhotonOS.ova""
  disk.3306682066.type:                   """" => ""thin""
  disk.3306682066.uuid:                   """" => ""<computed>""
  disk.3306682066.vmdk:                   """" => """"
  domain:                                 """" => ""vsphere.local""
  enable_disk_uuid:                       """" => ""true""
  folder:                                 """" => ""kubernetes""
  linked_clone:                           """" => ""false""
  memory:                                 """" => ""2048""
  memory_reservation:                     """" => ""0""
  name:                                   """" => ""kubernetes-node2""
  network_interface.#:                    """" => ""1""
  network_interface.0.ip_address:         """" => ""<computed>""
  network_interface.0.ipv4_address:       """" => ""<computed>""
  network_interface.0.ipv4_gateway:       """" => ""<computed>""
  network_interface.0.ipv4_prefix_length: """" => ""<computed>""
  network_interface.0.ipv6_address:       """" => ""<computed>""
  network_interface.0.ipv6_gateway:       """" => ""<computed>""
  network_interface.0.ipv6_prefix_length: """" => ""<computed>""
  network_interface.0.label:              """" => ""VM Network""
  network_interface.0.mac_address:        """" => ""<computed>""
  network_interface.0.subnet_mask:        """" => ""<computed>""
  resource_pool:                          """" => ""10.112.185.145""
  skip_customization:                     """" => ""true""
  time_zone:                              """" => ""Etc/UTC""
  uuid:                                   """" => ""<computed>""
  vcpu:                                   """" => ""1""
vsphere_virtual_machine.kubevm2: Creating...
  datacenter:                             """" => ""DC1""
  detach_unknown_disks_on_delete:         """" => ""false""
  disk.#:                                 """" => ""1""
  disk.3306682066.bootable:               """" => ""true""
  disk.3306682066.controller_type:        """" => ""scsi""
  disk.3306682066.datastore:              """" => ""datastore1""
  disk.3306682066.iops:                   """" => """"
  disk.3306682066.keep_on_remove:         """" => """"
  disk.3306682066.key:                    """" => ""<computed>""
  disk.3306682066.name:                   """" => """"
  disk.3306682066.size:                   """" => """"
  disk.3306682066.template:               """" => ""/DC1/vm/KubernetesAnywhereTemplatePhotonOS.ova""
  disk.3306682066.type:                   """" => ""thin""
  disk.3306682066.uuid:                   """" => ""<computed>""
  disk.3306682066.vmdk:                   """" => """"
  domain:                                 """" => ""vsphere.local""
  enable_disk_uuid:                       """" => ""true""
  folder:                                 """" => ""kubernetes""
  linked_clone:                           """" => ""false""
  memory:                                 """" => ""2048""
  memory_reservation:                     """" => ""0""
  name:                                   """" => ""kubernetes-node1""
  network_interface.#:                    """" => ""1""
  network_interface.0.ip_address:         """" => ""<computed>""
  network_interface.0.ipv4_address:       """" => ""<computed>""
  network_interface.0.ipv4_gateway:       """" => ""<computed>""
  network_interface.0.ipv4_prefix_length: """" => ""<computed>""
  network_interface.0.ipv6_address:       """" => ""<computed>""
  network_interface.0.ipv6_gateway:       """" => ""<computed>""
  network_interface.0.ipv6_prefix_length: """" => ""<computed>""
  network_interface.0.label:              """" => ""VM Network""
  network_interface.0.mac_address:        """" => ""<computed>""
  network_interface.0.subnet_mask:        """" => ""<computed>""
  resource_pool:                          """" => ""10.112.185.145""
  skip_customization:                     """" => ""true""
  time_zone:                              """" => ""Etc/UTC""
  uuid:                                   """" => ""<computed>""
  vcpu:                                   """" => ""1""
vsphere_virtual_machine.kubevm4: Creating...
  datacenter:                             """" => ""DC1""
  detach_unknown_disks_on_delete:         """" => ""false""
  disk.#:                                 """" => ""1""
  disk.3306682066.bootable:               """" => ""true""
  disk.3306682066.controller_type:        """" => ""scsi""
  disk.3306682066.datastore:              """" => ""datastore1""
  disk.3306682066.iops:                   """" => """"
  disk.3306682066.keep_on_remove:         """" => """"
  disk.3306682066.key:                    """" => ""<computed>""
  disk.3306682066.name:                   """" => """"
  disk.3306682066.size:                   """" => """"
  disk.3306682066.template:               """" => ""/DC1/vm/KubernetesAnywhereTemplatePhotonOS.ova""
  disk.3306682066.type:                   """" => ""thin""
  disk.3306682066.uuid:                   """" => ""<computed>""
  disk.3306682066.vmdk:                   """" => """"
  domain:                                 """" => ""vsphere.local""
  enable_disk_uuid:                       """" => ""true""
  folder:                                 """" => ""kubernetes""
  linked_clone:                           """" => ""false""
  memory:                                 """" => ""2048""
  memory_reservation:                     """" => ""0""
  name:                                   """" => ""kubernetes-node3""
  network_interface.#:                    """" => ""1""
  network_interface.0.ip_address:         """" => ""<computed>""
  network_interface.0.ipv4_address:       """" => ""<computed>""
  network_interface.0.ipv4_gateway:       """" => ""<computed>""
  network_interface.0.ipv4_prefix_length: """" => ""<computed>""
  network_interface.0.ipv6_address:       """" => ""<computed>""
  network_interface.0.ipv6_gateway:       """" => ""<computed>""
  network_interface.0.ipv6_prefix_length: """" => ""<computed>""
  network_interface.0.label:              """" => ""VM Network""
  network_interface.0.mac_address:        """" => ""<computed>""
  network_interface.0.subnet_mask:        """" => ""<computed>""
  resource_pool:                          """" => ""10.112.185.145""
  skip_customization:                     """" => ""true""
  time_zone:                              """" => ""Etc/UTC""
  uuid:                                   """" => ""<computed>""
  vcpu:                                   """" => ""1""
vsphere_virtual_machine.kubevm5: Still creating... (10s elapsed)

vsphere_virtual_machine.kubevm4: Creation complete (ID: kubernetes/kubernetes-node3)
vsphere_virtual_machine.kubevm5: Creation complete (ID: kubernetes/kubernetes-node4)
vsphere_virtual_machine.kubevm1: Creation complete (ID: kubernetes/kubernetes-master)
tls_cert_request.kubernetes-master: Creating...
  cert_request_pem:       """" => ""<computed>""
  dns_names.#:            """" => ""6""
  dns_names.0:            """" => ""kubernetes-master""
  dns_names.1:            """" => ""kubernetes""
  dns_names.2:            """" => ""kubernetes.default""
  dns_names.3:            """" => ""kubernetes.default.svc""
  dns_names.4:            """" => ""kubernetes.default.svc.local""
  dns_names.5:            """" => ""kubernetes.default.svc.local""
  ip_addresses.#:         """" => ""2""
  ip_addresses.0:         """" => ""10.112.187.11""
  ip_addresses.1:         """" => ""10.0.0.1""
  key_algorithm:          """" => ""RSA""
  private_key_pem:        """" => ""XXXX""
  subject.#:              """" => ""1""
  subject.0.common_name:  """" => ""kubernetes-master_certificate""
  subject.0.organization: """" => ""kubernetes-anywhere""
vsphere_virtual_machine.kubevm2: Creation complete (ID: kubernetes/kubernetes-node1)
vsphere_virtual_machine.kubevm3: Creation complete (ID: kubernetes/kubernetes-node2)
tls_locally_signed_cert.kubernetes-master: Creating...
  allowed_uses.#:        """" => ""3""
  allowed_uses.0:        """" => ""digital_signature""
  allowed_uses.1:        """" => ""server_auth""
  allowed_uses.2:        """" => ""client_auth""
  ca_cert_pem:           """" => ""XXX""
  ca_key_algorithm:      """" => ""RSA""
  ca_private_key_pem:    """" => ""XX""
  cert_pem:              """" => ""<computed>""
  cert_request_pem:      """" => ""XXX""
  early_renewal_hours:   """" => ""0""
  validity_end_time:     """" => ""<computed>""
  validity_period_hours: """" => ""8760""
  validity_start_time:   """" => ""<computed>""
tls_locally_signed_cert.kubernetes-master: Creation complete (ID: 189601159081464308253298719712913011035)
data.template_file.configure_master: Refreshing state...
data.template_file.configure_node: Refreshing state...
null_resource.kubernetes-node2: Creating...
null_resource.kubernetes-node4: Creating...
null_resource.kubernetes-master: Creating...
null_resource.kubernetes-node3: Creating...
null_resource.kubernetes-node5: Creating...
null_resource.kubernetes-master: Provisioning with 'remote-exec'...
null_resource.kubernetes-node2: Provisioning with 'remote-exec'...
null_resource.kubernetes-node3: Provisioning with 'remote-exec'...
null_resource.kubernetes-node5: Provisioning with 'remote-exec'...
null_resource.kubernetes-node4: Provisioning with 'remote-exec'...
null_resource.kubernetes-master (remote-exec): Connecting to remote host via SSH...
null_resource.kubernetes-master (remote-exec):   Host: 10.112.187.11
null_resource.kubernetes-master (remote-exec):   User: root
null_resource.kubernetes-master (remote-exec):   Password: true
null_resource.kubernetes-master (remote-exec):   Private key: false
null_resource.kubernetes-master (remote-exec):   SSH Agent: false
null_resource.kubernetes-node3 (remote-exec): Connecting to remote host via SSH...
null_resource.kubernetes-node3 (remote-exec):   Host: 10.112.184.206
null_resource.kubernetes-node3 (remote-exec):   User: root
null_resource.kubernetes-node3 (remote-exec):   Password: true
null_resource.kubernetes-node3 (remote-exec):   Private key: false
null_resource.kubernetes-node3 (remote-exec):   SSH Agent: false
null_resource.kubernetes-node2 (remote-exec): Connecting to remote host via SSH...
null_resource.kubernetes-node2 (remote-exec):   Host: 10.112.185.196
null_resource.kubernetes-node2 (remote-exec):   User: root
null_resource.kubernetes-node2 (remote-exec):   Password: true
null_resource.kubernetes-node2 (remote-exec):   Private key: false
null_resource.kubernetes-node2 (remote-exec):   SSH Agent: false
null_resource.kubernetes-node5 (remote-exec): Connecting to remote host via SSH...
null_resource.kubernetes-node5 (remote-exec):   Host: 10.112.187.136
null_resource.kubernetes-node5 (remote-exec):   User: root
null_resource.kubernetes-node5 (remote-exec):   Password: true
null_resource.kubernetes-node5 (remote-exec):   Private key: false
null_resource.kubernetes-node5 (remote-exec):   SSH Agent: false
null_resource.kubernetes-node4 (remote-exec): Connecting to remote host via SSH...
null_resource.kubernetes-node4 (remote-exec):   Host: 10.112.184.77
null_resource.kubernetes-node4 (remote-exec):   User: root
null_resource.kubernetes-node4 (remote-exec):   Password: true
null_resource.kubernetes-node4 (remote-exec):   Private key: false
null_resource.kubernetes-node4 (remote-exec):   SSH Agent: false
null_resource.kubernetes-node5 (remote-exec): Connected!
null_resource.kubernetes-node3 (remote-exec): Connected!
null_resource.kubernetes-master (remote-exec): Connected!
null_resource.kubernetes-node2 (remote-exec): Connected!
null_resource.kubernetes-node4 (remote-exec): Connected!
null_resource.kubernetes-node5 (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/flannelc.service to /usr/lib/systemd/system/flannelc.service.
null_resource.kubernetes-node3 (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/flannelc.service to /usr/lib/systemd/system/flannelc.service.
null_resource.kubernetes-node2 (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/flannelc.service to /usr/lib/systemd/system/flannelc.service.
null_resource.kubernetes-master (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service.
null_resource.kubernetes-node4 (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/flannelc.service to /usr/lib/systemd/system/flannelc.service.
null_resource.kubernetes-master (remote-exec): Created symlink from /etc/systemd/system/multi-user.target.wants/flanneld.service to /usr/lib/systemd/system/flanneld.service.
null_resource.kubernetes-node2: Still creating... (10s elapsed)
null_resource.kubernetes-node2: Creation complete (ID: 422537898037059153)
null_resource.kubernetes-node3: Creation complete (ID: 6247345747480823583)
null_resource.kubernetes-node5: Creation complete (ID: 6931630961364895859)
null_resource.kubernetes-node4: Creation complete (ID: 1414777359827700037)
Error applying plan:

1 error(s) occurred:

* null_resource.kubernetes-master: 1 error(s) occurred:

* Script exited with non-zero exit status: 1

Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.
Makefile:70: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:48: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2

```",closed,False,2018-03-08 20:01:54,2018-08-05 22:20:27
kubernetes-anywhere,leblancd,https://github.com/kubernetes/kubernetes-anywhere/issues/527,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/527,IPVS FeatureGate config causing Kube IPVS CI failures,"The Kubernetes ci-kubernetes-e2e-kubeadm-gce-ipvs CI tests are failing (see [gubernator results](https://k8s-gubernator.appspot.com/builds/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-ipvs) in the kubernetes-anywhere Phase 2.

Looking at serial logs on the master node (e.g. [sample serial log](https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-ipvs/502/artifacts/e2e-502-master/serial-1.log) ), the YAML decoder is complaining about a missing '{' or newline after ""featureGates"":
```
Mar 23 10:47:44 e2e-502-master startup-script: INFO startup-script: unable to decode config from ""/etc/kubeadm/kubeadm.yaml"" [v1alpha1.MasterConfiguration: KubeProxy: v1alpha1.KubeProxy: Config: v1alpha1.KubeProxyConfiguration: **FeatureGates: ReadMapCB: expect { or n, but found ""**, error found in #10 byte of ...|reGates"":""SupportIPV|..., bigger context ...|iguration"",""kubeProxy"":{""config"":{""featureGates"":""SupportIPVSProxyMode=true"",""mode"":""ipvs""}},""kubern|...]
```
Looks like this config in the startup script needs to be changed from:
```
    featureGates: SupportIPVSProxyMode=true
```
to:
```
    featureGates:
      SupportIPVSProxyMode=true
```

",closed,False,2018-03-23 14:09:56,2018-03-29 09:46:30
kubernetes-anywhere,leblancd,https://github.com/kubernetes/kubernetes-anywhere/pull/528,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/528,Add newline before IPVS featureGates config in startup,"The Kubernetes ci-kubernetes-e2e-kubeadm-gce-ipvs CI tests are failing (see [gubernator results](https://k8s-gubernator.appspot.com/builds/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-ipvs) ) in the kubernetes-anywhere Phase 2.

The kube master serial logs show YAML decode errors for kubeadm config where either a '{' or newline is expected afer ""featureGates:"". The [kubeadm config file documentation](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file) indicates that the expected format for featureGates is:
```
featureGates:
  <feature>: <bool>
  <feature>: <bool>
```
Accordingly, the config in the startup script needs to be changed from:
```
    featureGates: SupportIPVSProxyMode=true
```
to:
```
    featureGates:
      SupportIPVSProxyMode: true
```

Fixes #527",closed,True,2018-03-23 14:17:36,2018-03-29 12:54:03
kubernetes-anywhere,tickers,https://github.com/kubernetes/kubernetes-anywhere/issues/529,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/529,Local Repo for required files ,"not so much an issue but a request or howto.
I want to be able to have a local repo of the the installs required to build out the cluster to speed my testing up. 

I do snapshot the nodes when I first stand them up and do roll back on them to fix issues however the ability to keep the files local would be great. 
",closed,False,2018-04-04 23:16:35,2018-09-02 00:57:54
kubernetes-anywhere,thockin,https://github.com/kubernetes/kubernetes-anywhere/pull/530,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/530,Pass 2: k8s GCR vanity URL,,closed,True,2018-04-06 15:08:20,2018-06-05 22:52:54
kubernetes-anywhere,tickers,https://github.com/kubernetes/kubernetes-anywhere/issues/531,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/531,Enable HA,Has Anyone enabled HA with Kubernetes-Anywhere?  ,closed,False,2018-04-09 15:33:44,2018-09-06 17:48:57
kubernetes-anywhere,tickers,https://github.com/kubernetes/kubernetes-anywhere/issues/532,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/532,Patching minor releases,Looking for examples on patching minor releases of kubernetes. I want to patch 1.10.0 to 1.10.1.,closed,False,2018-04-13 18:58:02,2018-09-10 21:25:45
kubernetes-anywhere,jiopaley,https://github.com/kubernetes/kubernetes-anywhere/issues/533,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/533,Unable to deploy kubernetes anywhere using vsphere behind a firewall,"When deploying kubernetes anywhere using the vsphere provider behind a firewall (ie no access to the internet, but with local docker repositories), the deployment attempts to retrieve containers from storage.googleapis.com:

null_resource.kubernetes-master (remote-exec): DEBUG    : files: createFilesystemsFiles: createFiles: GET error: Get https://storage.googleapis.com/kubernetes-release/release/v1.6.5/bin/linux/amd64/kubectl: dial tcp xxx.xxx.xxx.xxx:443: i/o timeout

It looks like there isn't a parameter to control the location of these containers.  Request to add this as a parameter to the make config script!

",closed,False,2018-04-17 14:15:59,2018-10-27 15:23:14
kubernetes-anywhere,AdamDang,https://github.com/kubernetes/kubernetes-anywhere/pull/534,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/534,Typo fix: Kuberetes->Kubernetes,Kuberetes->Kubernetes,closed,True,2018-04-29 05:16:45,2018-05-05 23:19:11
kubernetes-anywhere,JSteeleIR,https://github.com/kubernetes/kubernetes-anywhere/pull/535,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/535,Fix vsphere vm-node INTs keeping the jsonnet from compiling correctly.,Also trim trailing whitespace.,closed,True,2018-04-29 07:26:55,2018-05-05 23:20:01
kubernetes-anywhere,allenyang0812,https://github.com/kubernetes/kubernetes-anywhere/issues/536,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/536,the OVA's disk storage is not enough for worker node in vsphere ,"I run my application in the kubernetes ckuster, serverl days later, it will appear a problem on worker node.
this is log
"" error mkdir /run/docker/libcontainerd/523faacac605d229e897469fbcb99b1c016dc3aa1a8c5d1ec9e7dad70c3a4432/rootfs/.pivot_root748860524: no space left on device""

The worker node disk storage is not enough. can you update the OVA  to increase the disk storage? 
  
",closed,False,2018-05-02 08:22:52,2018-08-30 10:50:57
kubernetes-anywhere,eliasp,https://github.com/kubernetes/kubernetes-anywhere/issues/537,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/537,Usage of non-public gcr.io/google-containers breaks default vsphere deployment,"As gcr.io/google-containers is not publicly accessible, it breaks non-GCE setups such as vsphere:
```
null_resource.K8S-EP-master: Provisioning with 'remote-exec'...
null_resource.K8S-EP-master (remote-exec): Connecting to remote host via SSH...
null_resource.K8S-EP-master (remote-exec):   Host: 10.6.10.57
null_resource.K8S-EP-master (remote-exec):   User: root
null_resource.K8S-EP-master (remote-exec):   Password: true
null_resource.K8S-EP-master (remote-exec):   Private key: false
null_resource.K8S-EP-master (remote-exec):   SSH Agent: false
null_resource.K8S-EP-master (remote-exec): Connected!
null_resource.K8S-EP-master (remote-exec): Pulling repository gcr.io/google-containers/hyperkube-amd64
Error applying plan:

1 error(s) occurred:

* null_resource.K8S-EP-master: 1 error(s) occurred:

* Script exited with non-zero exit status: 1

Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.
Makefile:70: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:48: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
```

The problem becomes obvious when trying to pull the image manually:
```
Using default tag: latest
Pulling repository gcr.io/google-containers/hyperkube-amd64
unauthorized: authentication required
```
Is there any public mirror of `google-containers` available which could be used instead as default for kubernetes-anywhere?",closed,False,2018-05-03 12:52:02,2018-05-03 13:16:29
kubernetes-anywhere,VarunUmesh,https://github.com/kubernetes/kubernetes-anywhere/issues/538,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/538,"Deploy K8s cluster, with static IP address on vSphere","Is there anyway I could deploy the K8s cluster with static IP address? If so, could you please give me an example?

P.S. I did take a look at the OVA and it does not have any ovf/ova keys to set network details for the virtual machine.

Thank you 

-v",closed,False,2018-05-07 08:18:48,2018-10-04 18:24:08
kubernetes-anywhere,jessfraz,https://github.com/kubernetes/kubernetes-anywhere/issues/539,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/539,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",closed,False,2018-05-24 14:34:42,2018-09-07 20:25:52
kubernetes-anywhere,AdamDang,https://github.com/kubernetes/kubernetes-anywhere/pull/540,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/540,Typo fix: specifed->specified,Line 319: specifed->specified,closed,True,2018-06-02 11:08:45,2018-08-10 07:17:41
kubernetes-anywhere,leberkas,https://github.com/kubernetes/kubernetes-anywhere/issues/541,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/541,RUNTIME ERROR during make deploy,"Hello!

I've answered all questions to deploy to vsphere and then ran make deploy, but i got an error!
i've added ""set -x"" in phase1/do to get more information.

 ```
make deploy
util/config_to_json /opt/kubernetes-anywhere/.config > /opt/kubernetes-anywhere/.config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/vsphere""; ./do deploy-cluster )
+ set -o errexit
+ set -o pipefail
+ set -o nounset
+ cd .
++ jq -r .phase1.cluster_name ../../.config.json
+ CLUSTER_NAME=kubernetes
+ TMP_DIR=kubernetes/.tmp
+ case ""${1:-}"" in
+ deploy
+ gen
+ mkdir -p kubernetes/.tmp
+ jsonnet -J ../../ --multi kubernetes/.tmp all.jsonnet
RUNTIME ERROR: Field does not exist: cluster
        vSphere.jsonnet:109:22-40       object <anonymous>
        vSphere.jsonnet:(98:32)-(125:7) object <anonymous>
        std.jsonnet:(943:13)-(952:13)   object <anonymous>
        std.jsonnet:(943:13)-(952:13)   object <anonymous>
        all.jsonnet:3:49-79     object <anonymous>
        During manifestation
Makefile:70: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:48: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2
```",closed,False,2018-06-13 07:51:07,2018-06-22 05:28:51
kubernetes-anywhere,dcbw,https://github.com/kubernetes/kubernetes-anywhere/pull/542,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/542,kubeadm requires cri-tools,"https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-cni-flannel/3157/artifacts/e2e-3157-098af-master/serial-1.log

INFO startup-script:  kubeadm depends on cri-tools (>= 1.11.0); however:
INFO startup-script:   Package cri-tools is not installed.
INFO startup-script: dpkg: error processing package kubeadm (--install):
INFO startup-script: Removing kubeadm (1.12.0~alpha.0.1584+aa06ec6dd3a010) ...
...
INFO startup-script: kubeadm init --skip-preflight-checks --config $KUBEADM_CONFIG_FILE
INFO startup-script: + kubeadm init --skip-preflight-checks --config /etc/kubeadm/kubeadm.yaml
INFO startup-script: /startup-wsmbtznt/tmpjucy96_3: line 211: kubeadm: command not found
INFO startup-script: Return code 127.
INFO Finished running startup scripts.

@thockin @caseydavenport @leblancd this is the only thing I can think of that's blocking some of the kubeadm-gce tests.",closed,True,2018-06-28 19:25:01,2018-06-28 22:07:03
kubernetes-anywhere,aojea,https://github.com/kubernetes/kubernetes-anywhere/issues/543,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/543,Missing CONTRIBUTING.md file,"All K8s subrepositories should have a CONTRIBUTING.md file, which at the minimum should point to https://github.com/kubernetes/community/blob/master/contributors/guide/README.md. Care should be taken that all information is in sync with the contributor guide.

Subrepositories may also have contributing guidelines specific to that repository. They should be explicitly documented and explained in the CONTRIBUTING.md

Ref:  https://github.com/kubernetes/community/issues/1832",closed,False,2018-07-16 09:23:32,2018-08-10 07:17:04
kubernetes-anywhere,nikhita,https://github.com/kubernetes/kubernetes-anywhere/pull/544,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/544,Add CONTRIBUTING.md,"Fixes #543 
xref https://github.com/kubernetes/community/issues/1832

/assign errordeveloper ",closed,True,2018-07-19 05:32:14,2018-08-10 07:17:04
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/545,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/545,phase2: use the v1alpha2 kubeadm config if needed ,"Refs kubernetes/kubernetes#66338

kubeadm 1.11 needs v1alpha2, thus we should use `config migrate`
to update the configration file. this should also work for future
versions, as long as `config migrate` works.

kubeadm 1.10.x would not trow an error.

https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-selfhosting/537/artifacts/e2e-537-f2b95-master/serial-1.log

```
Jul 19 19:33:35 e2e-537-f2b95-master startup-script: INFO startup-script: your configuration file uses an old API spec: ""kubeadm.k8s.io/v1alpha1"". Please use kubeadm v1.11 instead and run 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
Jul 19 19:33:35 e2e-537-f2b95-master startup-script: INFO startup-script: Return code 1.
```

@timothysc @dims @BenTheElder 
",closed,True,2018-07-19 20:46:47,2018-07-24 18:00:38
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/546,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/546,[WIP] phase2: fix kubeadm install issues,"https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce/10938/artifacts/e2e-10938-31d72-master/serial-1.log

refs kubernetes/kubernetes#66338

there is a dependency weirdness here that i don't understand:

kubeadm is not installed, even if the cri-tools and the kubelet are installed.
this is a WIP PR to gather feedback.

```
Jun 22 13:04:02 e2e-10938-31d72-master startup-script: INFO startup-script: Selecting previously unselected package kubectl.
Jun 22 13:04:02 e2e-10938-31d72-master startup-script: INFO startup-script: Preparing to unpack /tmp/k8s-debs/kubectl.deb ...
Jun 22 13:04:02 e2e-10938-31d72-master startup-script: INFO startup-script: Unpacking kubectl (1.12.0~alpha.0.1408+1ca851baec6a24) ...
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script: Selecting previously unselected package kubernetes-cni.
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script: Preparing to unpack .../k8s-debs/kubernetes-cni.deb ...
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script: Unpacking kubernetes-cni (0.5.1) ...
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script: dpkg: dependency problems prevent configuration of kubelet:
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:  kubelet depends on socat; however:
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:   Package socat is not installed.
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:  kubelet depends on ebtables; however:
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:   Package ebtables is not installed.
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script: dpkg: error processing package kubelet (--install):
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:  dependency problems - leaving unconfigured
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script: dpkg: dependency problems prevent configuration of kubeadm:
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:  kubeadm depends on kubelet (>= 1.8.0); however:
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:   Package kubelet is not configured yet.
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:  kubeadm depends on cri-tools (>= 1.11.0); however:
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:   Package cri-tools is not installed.
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script: dpkg: error processing package kubeadm (--install):
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:  dependency problems - leaving unconfigured
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script: Setting up kubectl (1.12.0~alpha.0.1408+1ca851baec6a24) ...
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script: Setting up kubernetes-cni (0.5.1) ...
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script: Errors were encountered while processing:
Jun 22 13:04:03 e2e-10938-31d72-master startup-script: INFO startup-script:  kubelet
```

please advise on how to fix this correctly.

@timothysc @dims @BenTheElder 
",closed,True,2018-07-19 22:12:34,2018-07-26 23:22:36
kubernetes-anywhere,haooliveira84,https://github.com/kubernetes/kubernetes-anywhere/issues/547,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/547,Fail to deploy k8s cluster 1.8 on vSphere,"
#
# Phase 2: Node Bootstrapping
#
.phase2.kubernetes_version=""v1.8.15""
.phase2.docker_registry=""gcr.io/google-containers""
.phase2.provider=""ignition""
.phase2.proxy_mode=""iptables""
.phase2.installer_container=""docker.io/cnastorage/etcd3-ignition:latest""

Log docker log on ignite 
`root@LNX-DEV-K8S-master [ ~ ]# docker logs a798461cd92b                                                                                                                 + export IGNITION_CONFIG_FILE=/usr/share/oem/ignite.json                                                                                                                + mkdir -p /usr/share/oem                                                                                                                                               + jsonnet --output-file /usr/share/oem/ignite.json --tla-code-file cfg=/etc/kubernetes/k8s_config.json --jpath /opt/kubernetes-anywhere /opt/kubernetes-anywhere/ignite.jsonnet                                                                                                                                                                 + ignition -oem file -stage files --root /mnt/root                                                                                                                      **ERROR    : unable to open syslog: Unix syslog delivery error**                                                                                                            INFO     : Ignition v0.12.1-1-g6ff90ec-dirty                                                                                                                            DEBUG    : parsed url from cmdline: """"                                                                                                                                  INFO     : no config URL provided                                                                                                                                       INFO     : using config file at ""/usr/share/oem/ignite.json""    `",closed,False,2018-07-23 19:26:45,2018-12-14 13:14:36
kubernetes-anywhere,artursmolarek,https://github.com/kubernetes/kubernetes-anywhere/issues/548,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/548,RBAC and missing default roles,"I successfully deployed k8s cluster using `docker.io/cnastorage/k8s-ignition:v1.8-dev-release` (cluster validation is passing). However it turns out that RBAC default roles (like `cluster-admin`) are missing (according documentation: `API servers create a set of default ClusterRole and ClusterRoleBinding objects`).

```
[container]:/opt/kubernetes-anywhere> kubectl get clusterroles
No resources found.
```

Has anyone else had similar problems?

My config:
```
#
# Automatically generated file; DO NOT EDIT.
# Kubernetes Minimal Turnup Configuration
#

#
# Phase 1: Cluster Resource Provisioning
#
.phase1.num_nodes=2
.phase1.cluster_name=""kubernetes""
.phase1.ssh_user=""""
.phase1.cloud_provider=""vsphere""

#
# vSphere configuration
#
.phase1.vSphere.url=""xxx.xxx.xxx.xxx""
.phase1.vSphere.port=443
.phase1.vSphere.username=""user@vsphere.local""
.phase1.vSphere.password=""password""
.phase1.vSphere.insecure=y
.phase1.vSphere.datacenter=""datacenter""
.phase1.vSphere.datastore=""datastore""
.phase1.vSphere.placement=""cluster""
.phase1.vSphere.cluster=""cluster""
.phase1.vSphere.useresourcepool=""yes""
.phase1.vSphere.resourcepool=""resource-pool""
.phase1.vSphere.vmfolderpath=""kubernetes""
.phase1.vSphere.vcpu=4
.phase1.vSphere.memory=10240
.phase1.vSphere.network=""network""
.phase1.vSphere.template=""KubernetesAnywhereTemplatePhotonOS""
.phase1.vSphere.flannel_net=""172.1.0.0/16""

#
# Phase 2: Node Bootstrapping
#
.phase2.kubernetes_version=""v1.9.0""
.phase2.provider=""ignition""
.phase2.installer_container=""docker.io/cnastorage/k8s-ignition:v1.8-dev-release""
.phase2.docker_registry=""gcr.io/google-containers""

#
# Phase 3: Deploying Addons
#
.phase3.run_addons=y
.phase3.kube_proxy=y
.phase3.dashboard=y
.phase3.heapster=y
.phase3.kube_dns=y
# .phase3.weave_net is not set
```",closed,False,2018-07-23 19:43:50,2019-02-08 10:02:21
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/549,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/549,phase2: convert KUBEADM_KUBERNETES_VERSION to semantic version,"The script has to know the MINOR version from a k8s semantic version,
so that it can decide which kubeadm config version to use.
Fetch the semantic version from the server if it's not in semantic
format already.
The raw $KUBEADM_KUBERNETES_VERSION can be passed to the config
as kubeadm can handle that.


---
the fix in https://github.com/kubernetes/kubernetes-anywhere/pull/545 did not handle the case where the version is not in semantic format (e.g. `ci/latest`).

https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-selfhosting/565/artifacts/e2e-565-f2b95-master/serial-1.log

```
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: # break down the version string
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: KUBEADM_KUBERNETES_VERSION_MAJOR=`cut -d'.' -f 1 <<< $KUBEADM_KUBERNETES_VERSION | cut -d'v' -f 2`
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: cut -d'.' -f 1 <<< $KUBEADM_KUBERNETES_VERSION | cut -d'v' -f 2
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: ++ cut -dv -f 2
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: ++ cut -d. -f 1
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: + KUBEADM_KUBERNETES_VERSION_MAJOR=ci-cross/latest
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: KUBEADM_KUBERNETES_VERSION_MINOR=`cut -d'.' -f 2 <<< $KUBEADM_KUBERNETES_VERSION`
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: cut -d'.' -f 2 <<< $KUBEADM_KUBERNETES_VERSION
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: ++ cut -d. -f 2
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: + KUBEADM_KUBERNETES_VERSION_MINOR=ci-cross/latest
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: KUBEADM_KUBERNETES_VERSION_PATCH=`cut -d'.' -f 3 <<< $KUBEADM_KUBERNETES_VERSION`
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: cut -d'.' -f 3 <<< $KUBEADM_KUBERNETES_VERSION
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: ++ cut -d. -f 3
```

```
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: + [[ ci-cross/latest -lt 11 ]]
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: /startup-_8udnnhb/tmpsqtnfryo: line 186: ci: unbound variable
Jul 26 20:01:40 e2e-565-f2b95-master startup-script: INFO startup-script: Return code 1.
```

refs: https://github.com/kubernetes/kubernetes/issues/66338

/assign @luxas 
/cc @BenTheElder @timothysc 
",closed,True,2018-07-27 01:19:44,2018-07-27 16:07:55
kubernetes-anywhere,armontero,https://github.com/kubernetes/kubernetes-anywhere/issues/550,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/550,Cluster unavailable after sitting idle over night,"Hello!

I recently just installed a new cluster running 1.11.1 following the suggestions from another issue thread.  Some folks on that thread mentioned they had this problem as well, but I figured it might make sense to start a new thread since this is technically a different issue.  

Basically, the cluster behaves fine so long as it is being actively used.  However, when I try to get pods first thing in the morning, the response is:

`The connection to the server 192.168.1.168 was refused - did you specify the right host or port?`

If I go to vmware and restart all of the cluster vm's, everything will work fine all day.  Anybody have any thoughts?",closed,False,2018-07-30 16:47:13,2018-07-31 16:15:26
kubernetes-anywhere,armontero,https://github.com/kubernetes/kubernetes-anywhere/issues/551,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/551,Cluster unavailable after being left alone over night,"Hello!

I'm recreating this issue as it is still occurring.  This morning when I try to get pods, I'm getting the same message:
`amontero@apollo:~/code$ kubectl get pods
The connection to the server 192.168.1.168 was refused - did you specify the right host or port?
amontero@apollo:~/code$`

Journalctl is not giving any helpful information either:
`amontero@apollo:~/code$ journalctl -xeu kubelet
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
-- Logs begin at Sat 2018-07-21 18:29:04 PDT, end at Tue 2018-07-31 09:29:38 PDT. --
-- No entries --
amontero@apollo:~/code$`

Any thoughts on next steps to debug?  Thanks!
",closed,False,2018-07-31 16:30:44,2018-12-31 01:36:26
kubernetes-anywhere,jescuderotraining,https://github.com/kubernetes/kubernetes-anywhere/issues/552,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/552,Make Deploy Runtime Error (from previous ticket still not working),"I get the following error when trying to deploy cluster

[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json /opt/kubernetes-anywhere/.config > /opt/kubernetes-anywhere/.config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/vsphere""; ./do deploy-cluster )
RUNTIME ERROR: Format required number at 0, got string
        std.jsonnet:(477:21)-(478:56)   function <format_code>
        std.jsonnet:557:29-73   thunk <s>
        std.jsonnet:562:38      thunk <str>
        std.jsonnet:369:36-38
        std.jsonnet:369:25-39   thunk <w>
        std.jsonnet:365:17      thunk <w>
        std.jsonnet:361:20      function <aux>
        std.jsonnet:365:13-22   function <padding>
        std.jsonnet:369:13-43   function <pad_left>
        std.jsonnet:562:29-52   thunk <s_padded>
        ...
        vSphere.jsonnet:138:21-133      thunk <array_element>
        vSphere.jsonnet:(136:27)-(141:19)       object <anonymous>
        vSphere.jsonnet:(135:32)-(142:17)       object <anonymous>
        vSphere.jsonnet:(134:27)-(143:12)       thunk <array_element>
        vSphere.jsonnet:(134:26)-(147:13)       object <anonymous>
        vSphere.jsonnet:(127:39)-(148:9)        object <anonymous>
        std.jsonnet:(943:13)-(952:13)   object <anonymous>
        std.jsonnet:(943:13)-(952:13)   object <anonymous>
        all.jsonnet:3:49-79     object <anonymous>
        During manifestation
Makefile:70: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:48: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2

Here is my .config file
",closed,False,2018-08-01 21:03:02,2018-08-01 21:29:30
kubernetes-anywhere,jescuderotraining,https://github.com/kubernetes/kubernetes-anywhere/issues/553,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/553,Make Deploy Runtime Error,"I get the following error when trying to deploy cluster

[container]:/opt/kubernetes-anywhere> make deploy
util/config_to_json /opt/kubernetes-anywhere/.config > /opt/kubernetes-anywhere/.config.json
make do WHAT=deploy-cluster
make[1]: Entering directory '/opt/kubernetes-anywhere'
( cd ""phase1/vsphere""; ./do deploy-cluster )
RUNTIME ERROR: Format required number at 0, got string
        std.jsonnet:(477:21)-(478:56)   function <format_code>
        std.jsonnet:557:29-73   thunk <s>
        std.jsonnet:562:38      thunk <str>
        std.jsonnet:369:36-38
        std.jsonnet:369:25-39   thunk <w>
        std.jsonnet:365:17      thunk <w>
        std.jsonnet:361:20      function <aux>
        std.jsonnet:365:13-22   function <padding>
        std.jsonnet:369:13-43   function <pad_left>
        std.jsonnet:562:29-52   thunk <s_padded>
        ...
        vSphere.jsonnet:138:21-133      thunk <array_element>
        vSphere.jsonnet:(136:27)-(141:19)       object <anonymous>
        vSphere.jsonnet:(135:32)-(142:17)       object <anonymous>
        vSphere.jsonnet:(134:27)-(143:12)       thunk <array_element>
        vSphere.jsonnet:(134:26)-(147:13)       object <anonymous>
        vSphere.jsonnet:(127:39)-(148:9)        object <anonymous>
        std.jsonnet:(943:13)-(952:13)   object <anonymous>
        std.jsonnet:(943:13)-(952:13)   object <anonymous>
        all.jsonnet:3:49-79     object <anonymous>
        During manifestation
Makefile:70: recipe for target 'do' failed
make[1]: *** [do] Error 1
make[1]: Leaving directory '/opt/kubernetes-anywhere'
Makefile:48: recipe for target 'deploy-cluster' failed
make: *** [deploy-cluster] Error 2

Here is my .config file
#
# Automatically generated file; DO NOT EDIT.
# Kubernetes Minimal Turnup Configuration
#

#
# Phase 1: Cluster Resource Provisioning
#
.phase1.num_nodes=4
.phase1.cluster_name=""kubernetes""
.phase1.ssh_user=""""
.phase1.cloud_provider=""vsphere""

#
# vSphere configuration
#
.phase1.vSphere.url=""my-vcsa.com""
.phase1.vSphere.port=443
.phase1.vSphere.username=""domain\service_account""
.phase1.vSphere.password=""password""
.phase1.vSphere.insecure=y
.phase1.vSphere.datacenter=""Data-Center""
.phase1.vSphere.datastore=""Studio_DS/1500MTU/esx_ucs_ssd_ds2""
.phase1.vSphere.placement=""cluster""
.phase1.vSphere.cluster=""UCS-Studio""
.phase1.vSphere.useresourcepool=""no""
.phase1.vSphere.vmfolderpath=""kubernetes""
.phase1.vSphere.vcpu=4
.phase1.vSphere.memory=8192
.phase1.vSphere.network=""My_dvSwitch/VLAN-235%2f236-VDI""
.phase1.vSphere.template=""Templates/Linux/Photon/KubernetesAnywhereTemplatePhotonOS.ova""
.phase1.vSphere.flannel_net=""172.1.0.0/16""

#
# Phase 2: Node Bootstrapping
#
.phase2.kubernetes_version=""v1.6.5""
.phase2.provider=""ignition""
.phase2.installer_container=""docker.io/cnastorage/k8s-ignition:v2""
.phase2.docker_registry=""gcr.io/google-containers""

#
# Phase 3: Deploying Addons
#
.phase3.run_addons=y
.phase3.kube_proxy=y
.phase3.dashboard=y
.phase3.heapster=y
.phase3.kube_dns=y
# .phase3.weave_net is not set",closed,False,2018-08-01 21:06:17,2018-08-02 00:51:33
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/554,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/554,Remove the deprecated --skip-preflight-checks for kubeadm,"Use --ignore-preflight-errors=all instead.

removed in PR:
https://github.com/kubernetes/kubernetes/pull/62727

failing test tracking issue:
https://github.com/kubernetes/kubernetes/issues/66338

i don't have a good solution if someone tries an old kubeadm version with k-a....
this is difficult to maintain.

/assign @luxas 
/cc @timothysc @BenTheElder 
",closed,True,2018-08-09 22:52:55,2018-08-10 07:17:17
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/issues/555,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/555,improve the k-a versioning in the test-infra kubeadm runner,"we eventually want to deprecate kubernetes-anywhere in the e2e suite.
but this isn't that easy, so we have to stick to it for a while.

right now we have a fixed k-a (kuberenetes-anywhere) commit in the image for the kubeadm runner.
this imposes the difficulty that we need to PR two changes:
- one that fixes k-a
- one that updates the SHA in `runner`.

https://github.com/neolit123/test-infra/blob/8f0bd23ac99e7c8809ccc6aefb42273dc96ca066/images/kubeadm/runner#L29

my proposal:
- create a k-a branch called `kubeadm-e2e` in this repo
- push e2e related changes and fixes to this branch.
- **always** use the tip of the branch `kubeadm-e2e` in the runner!!
- cherry pick changes from the `kubeadm-e2e` branch to the k-a master branch if needed.

future changes in kubeadm are going to make k-a very hard to maintain for both users and e2e tests.

/assign @timothysc 
/assign @BenTheElder 
/priority critical-urgent
/kind feature

/cc @errordeveloper @luxas @mikedanese 
for super powers on creating a k-a branch called `kubeadm-e2e` if this is approved by @BenTheElder and @timothysc 
",closed,False,2018-08-10 14:39:59,2018-08-20 16:55:24
kubernetes-anywhere,errordeveloper,https://github.com/kubernetes/kubernetes-anywhere/issues/556,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/556,indicate the status of this repo,"I believe this project is no longer in active development, yet it still serves certain purpose. We should indicate whatever is the case in the readme and repo description, so that it's clear to a newcomer what this is.

cc @roberthbailey @timothysc",open,False,2018-08-20 16:49:31,2018-12-19 09:44:30
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/557,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/557,phase1-gce: make it possible to use GOOGLE_APPLICATION_CREDENTIALS,"In case the user has not created the 'account.json' file,
fallback to using the file defined in GOOGLE_APPLICATION_CREDENTIALS.

If GOOGLE_APPLICATION_CREDENTIALS is not defined or the file there
is missing, do nothing.

fixes https://github.com/kubernetes/kubernetes-anywhere/issues/332
refs: https://github.com/kubernetes/test-infra/pull/9117

i need some feedback on this as have no means to test it ATM. does this look right, GCE wise?
the ideas is to move the following away from the kubeadm test-infra runner and have it in phase1-gce instead:
https://github.com/kubernetes/test-infra/blob/master/images/kubeadm/runner#L32-L36

/assign @mikedanese @errordeveloper 
/cc @BenTheElder
",closed,True,2018-08-22 03:40:58,2018-08-27 20:39:47
kubernetes-anywhere,marema31,https://github.com/kubernetes/kubernetes-anywhere/pull/558,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/558,Vsphere: field does not exist: insecure,"I've encountered same kind of problem resolved by #372 

If on ""make config"" for vSphere you answer ""n"" to insecure, the option is not defined in .config file and the make deploy fail on:

( cd ""phase1/vsphere""; ./do deploy-cluster )
RUNTIME ERROR: field does not exist: insecure
        vSphere.jsonnet:80:56-76        thunk <str_>

I propose a small fix for my current issue, but I think the issue will occurs in other cases.",closed,True,2018-08-23 18:15:56,2018-10-03 18:14:09
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/559,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/559,"phase2/kubeadm: support ""stable*"" and ""latest*"" versions","kuberenetes-anywhere already supports ""stable"" and ""latest"" (kubeadm suppots them too), except we now have different
configs for different MINOR versions and we need to parse the SEMVER
to determine what config to use.

Handle these versions in the `-master.sh` script.

kubeadm source code ref:
https://github.com/kubernetes/kubernetes/blob/ed958b7d79ee7eee0d71952e71475fe667c77909/cmd/kubeadm/app/util/version.go#L55-L61

failing e2e job because ""stable"" is used:
https://github.com/kubernetes/test-infra/blob/82f404e32640453021733e9694a6c5463bb08918/config/jobs/kubernetes/sig-cluster-lifecycle/kubeadm-x-on-y.yaml#L81-L105

https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-stable-on-master/4722

@kubernetes/sig-cluster-lifecycle-pr-reviews 
/assign @roberthbailey @timothysc 
",closed,True,2018-09-06 02:24:21,2018-09-06 04:57:00
kubernetes-anywhere,RA489,https://github.com/kubernetes/kubernetes-anywhere/pull/560,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/560,Create SECURITY_CONTACTS,"Adding Security Contacts file.
fixes #539 ",closed,True,2018-09-06 07:46:56,2018-09-07 20:25:52
kubernetes-anywhere,dcbw,https://github.com/kubernetes/kubernetes-anywhere/issues/561,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/561,kubernetes kubeadm-gce-* tests failing due to missing node configuration,"https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-cni-flannel/3436

None of the nodes start because:
`Sep  6 15:44:40 e2e-3436-098af-node-f2tq kubelet[5051]: F0906 15:44:40.442940    5051 server.go:190] failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file ""/var/lib/kubelet/config.yaml"", error: open /var/lib/kubelet/config.yaml: no such file or directory`

And that may be due to failures in the kubernetes-anywhere Makefile?
```
W0906 15:41:11.941] 2018/09/06 15:41:11 process.go:153: Running: make -C /workspace/k8s.io/kubernetes-anywhere kubeconfig-path
I0906 15:41:12.041] make: Entering directory '/workspace/k8s.io/kubernetes-anywhere'
I0906 15:41:12.042] util/config_to_json /workspace/k8s.io/kubernetes-anywhere/.config > /workspace/k8s.io/kubernetes-anywhere/.config.json
I0906 15:41:12.063] Makefile:57: recipe for target 'kubeconfig-path' failed
I0906 15:41:12.063] make: Leaving directory '/workspace/k8s.io/kubernetes-anywhere'
I0906 15:41:12.064] make: Entering directory '/workspace/k8s.io/kubernetes-anywhere'
I0906 15:41:12.071] make do WHAT=deploy-cluster
I0906 15:41:12.076] make[1]: Entering directory '/workspace/k8s.io/kubernetes-anywhere'
I0906 15:41:12.085] ( cd ""phase1/gce""; ./do deploy-cluster )
W0906 15:41:12.186] Cannot find kubeconfig file. Have you started a cluster with ""make deploy"" yet?
W0906 15:41:12.186] make: *** [kubeconfig-path] Error 1
W0906 15:41:12.186] 2018/09/06 15:41:12 process.go:155: Step 'make -C /workspace/k8s.io/kubernetes-anywhere kubeconfig-path' finished in 122.252521ms
```",closed,False,2018-09-06 21:04:52,2018-09-11 01:42:44
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/562,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/562,phase3: update weave CNI plugin to 2.4.0,"1.9.4 is quite outdated and is causing errors for k8s 1.12.
Use weave 2.4.0 and also consolidate the JSON files into a single
List object.

The 2.4.0 manifest is based on:
  curl -s ""https://cloud.weave.works/k8s/net.json?k8s-version=v1.10.0"" | \
  jq . > phase3/weave-net/weave-net-list.json

Also bump the kubectl version to 1.11.2 as 1.6.x is ancient.

ref: https://github.com/kubernetes/kubernetes/issues/68475

notes:
- this is against the `kubeadm-e2e` branch.
- i have no means to tests this because phase1 fails for me on GCE with unrelated terraform errors that i can't figure out.

/kind bug
/assign @errordeveloper
/cc @timothysc @bboreham 
",closed,True,2018-09-11 02:27:14,2018-09-11 19:15:01
kubernetes-anywhere,spiffxp,https://github.com/kubernetes/kubernetes-anywhere/issues/563,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/563,Use tide for PR merging,"This is a [core repository](https://github.com/kubernetes/community/blob/master/github-management/kubernetes-repositories.md#core-repositories).  As such, it [needs to use the same merge automation as the rest of the project](https://github.com/kubernetes/community/blob/master/github-management/kubernetes-repositories.md#rules-1)

I have a PR open that will address this at an org-wide level: https://github.com/kubernetes/test-infra/pull/9342.  It will:
- enable the approve plugin, to allow use of the `/approve` plugin
- enable the blunderbuss plugin, to assign reviews based on OWNERS files
- add all repos in the kubernetes org to tide's query

Can one of the repo maintainers here drop an LGTM (or objections) on the linked PR?  Alternatively, if I hear no objections by Monday 10am PT of next week, I will merge the PR.",closed,False,2018-09-11 22:25:54,2018-09-17 17:11:17
kubernetes-anywhere,RA489,https://github.com/kubernetes/kubernetes-anywhere/pull/564,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/564,Update README.md,"Updated the readme file.
fixes #556 ",closed,True,2018-09-19 07:27:54,2018-12-04 17:17:35
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/565,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/565,update OWNERS file for branch kubeadm-e2e,"add me and @fabriziopandini to the OWNER files for this branch.

the repo now has `/lgtm` and `/approve` commands enabled and we should use them:
https://github.com/kubernetes/kubernetes-anywhere/issues/563
",closed,True,2018-09-19 19:08:30,2018-09-19 19:17:48
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/566,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/566,update OWNERS file for branch master,"add me and @fabriziopandini to the OWNER files for this branch.

the repo now has `/lgtm` and `/approve` commands enabled and we should use them:
https://github.com/kubernetes/kubernetes-anywhere/issues/563
",closed,True,2018-09-19 19:10:53,2018-09-19 19:16:29
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/567,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/567,apply a set of patches to build scripts and Makefile,"from the discussion on slack:
> echo the .config.json at the beginning of the build-log.txt, it will be simpler to reproduce the failing case.

the second commit adds ac check if the kubeconfig is emtpy....
for some reason `gcloud ssh` gets it like that and i have no idea why.
at least that's some indication of something being wrong with the config.

third change is to ignore pre-release packages for `stable` kubelet.

/kind cleanup
/assign @fabriziopandini 
",closed,True,2018-09-21 03:15:00,2018-09-21 17:56:32
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/568,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/568,phase3: update to flannel 1.10,"the flannel version is k-a is outdated, this PR updates to the latest flannel - 0.10.0.

trying to solve:
https://k8s-testgrid.appspot.com/sig-network-gce#kubeadm-gce-cni-flannel

latest flannel doesn't work for me locally at all with 1.12.x and the guide here:
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network
i need to investigate further.

also, putting this PR on hold because i haven't tested it with k-a yet.

/hold
/kind bug
/area 
/assign @fabriziopandini 
@kubernetes/sig-cluster-lifecycle-pr-reviews 
@kubernetes/sig-network-pr-reviews
",closed,True,2018-09-25 02:46:33,2018-10-05 05:06:41
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/569,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/569,phase2: handle v1alpha3 kubeadm config,"fixes failing end-to-end tests due to the removal of v1alpha2 in master.
fixes kubernetes/kubeadm#1151

the diff is hard to read.
PTAL at the whole file.

- move v1alpha1 and v1alpha2 in the same block under version <= 11.
- add v1alpha3.

/priority critical-urgent
/kind failing-test

/assign @fabriziopandini 
/cc @kubernetes/sig-cluster-lifecycle-pr-reviews 
",closed,True,2018-10-01 15:24:04,2018-10-01 20:37:58
kubernetes-anywhere,martinmike2,https://github.com/kubernetes/kubernetes-anywhere/issues/570,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/570,Terraform crashes,"Using the recommended kubernetes version on vSphere 6 I get a terraform crash on tls_cert_request.kubernetes-master create

Makefile:70: recipe for target 'do' failed",closed,False,2018-10-03 03:30:20,2019-03-02 05:04:10
kubernetes-anywhere,fabriziopandini,https://github.com/kubernetes/kubernetes-anywhere/pull/571,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/571,kubeadm v1beta1 config,"fixes failing test kubeadm-gce-master by switching kubeadm >= v1.13 to the new v1beta1 api

In order to work properly with the new local/stacked etcd the public ip of the machine stored in the `KUBEADM_ADVERTISE_ADDRESSES` variable is now used for the `controlPlaneEndpoint` field (instead of using it for the API server advertise address).

/sig cluster-lifecycle
/priority critical-urgent
/kind failing-test

/assign @timothysc 
/assign @neolit123 
/cc @kubernetes/sig-cluster-lifecycle-pr-reviews",closed,True,2018-10-28 19:52:27,2018-12-15 08:31:28
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/572,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/572,kubeadm: decide on config type based on kubeadm client version,"The decision on which kubeadm configuration type to use
should not happen based on the version from a k8s endpoint,
but rather based on the version of the client (kubeadm binary).

xref: https://github.com/kubernetes/kubernetes/issues/70375

/priority critical-urgent
/kind bug
/assign @fabriziopandini 
",closed,True,2018-10-29 16:15:46,2018-10-29 17:51:48
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/573,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/573,phase2: make kubeadm v1beta1 related updates,"rename apiEndpoint -> localAPIEndpoint
apiServer is now a sub-struct of ClusterConfiguration:
",closed,True,2018-11-10 14:16:44,2018-11-10 14:30:43
kubernetes-anywhere,DJcode99,https://github.com/kubernetes/kubernetes-anywhere/issues/574,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/574,Loop Validation: Expected,"I am able to successfully deploy the Master & Worker Nodes with Terrafrom Script; however, the last step, i get the following:
KUBECONFIG=""$(pwd)/phase1/vsphere/clusters/kubernetes/kubeconfig.json"" ./util/validate
Validation: Expected 3 (workers + master) nodes; found 0. (10s elapsed)
Validation: Expected 3 (workers + master) nodes; found 0. (20s elapsed)
Validation: Expected 3 (workers + master) nodes; found 0. (30s elapsed)
Validation: Expected 3 (workers + master) nodes; found 0. (40s elapsed)


I also noticed that the master and worker nodes each get the same IP address which is probably the issue. I have changed my terrafrom Script to include a different Network/DHCP and same issue.  Not sure why each VM keeps pulling same IP address. Sometimes the Master Node hostname gets mixed up as well with the Worker Host name.
",open,False,2018-11-15 02:57:19,2019-03-15 04:08:09
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/575,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/575,phase2: exclude the 'config upload' step for upgrades,"@fabriziopandini as promised, but i haven't experimented if this fixes it.
do we have to still include this for 1.11?",closed,True,2018-11-16 01:22:46,2019-01-14 20:57:24
kubernetes-anywhere,mauilion,https://github.com/kubernetes/kubernetes-anywhere/pull/576,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/576,Fixes broken e2e test after adding conntrack.,"This should resolve the broken e2e test. 

In my testing I found that if you manually install packages with `dpkg` and then try to resolve their dependencies with `apt-get install -f -y` and the apt-cache is empty. The apt resolver will remove the manually added packages. Presumably cause it doesn't have enough information to resolve the deps. 

If we do an apt-get update before the `apt-get install -f -y` then the resolver should have all that's needed to correctly resolve and will install the missing bits. 

There is an `apt-get update` in the previous for loop. This just adds one for when we are using local packages it should not add any more time to the test.
",closed,True,2018-12-01 06:52:01,2018-12-06 18:15:00
kubernetes-anywhere,luxas,https://github.com/kubernetes/kubernetes-anywhere/pull/577,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/577,Officially deprecate kube-anywhere,"@kubernetes/sig-cluster-lifecycle-pr-reviews 
/assign @timothysc @roberthbailey ",closed,True,2018-12-04 13:43:50,2018-12-04 15:13:16
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/579,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/579,phase1: bump docker version to 1.13.1,"this PR bumped the minimum docker version in the kubelet and turned our e2e tests red:
https://github.com/kubernetes/kubernetes/pull/72831
https://gubernator.k8s.io/build/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-master/1519

i have tested the change in a xenial container.

/assign @fabriziopandini 
/cc @timothysc @mauilion
",closed,True,2019-01-12 11:55:50,2019-01-14 20:46:05
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/580,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/580,phase3: download a recent kubectl binary,"phase3: download a recent kubectl binary

when applying the CNI plugin manifest, kubectl from kubekins is used, which can be quite old.
kubectl 1.10.7 causes a the following failure with recent upstream k8s.
https://gubernator.k8s.io/build/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-master/1607
```
W0119 19:45:01.703] error: SchemaError(io.k8s.api.admissionregistration.v1beta1.MutatingWebhookConfiguration): invalid object doesn't have additional properties
```

download a new kubectl that matches the latest.txt endpoint.

@kubernetes/sig-cluster-lifecycle 
/assign @fabriziopandini @BenTheElder
",closed,True,2019-01-19 21:25:57,2019-01-19 21:37:50
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/581,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/581,"add the Makefile ""setup"" rule","Re-organizes the setup steps:
- Call util/setup...sh when building the local container
- Add a new rule ""setup"" to call from e2e jobs
- in Phase3 use the kubectl installed from util/setup..sh

This change allows removing the kubeadm image from test-infra.

test-infra PR:
https://github.com/kubernetes/test-infra/pull/10854
",closed,True,2019-01-20 13:55:33,2019-01-22 23:19:09
kubernetes-anywhere,neolit123,https://github.com/kubernetes/kubernetes-anywhere/pull/582,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/582,util: fix a bug when fetching terraform,"after the recent switch to the kubekins image from kubeadm image in test-infra
a couple of problems started happening:
- sed does not remove all lines expect the correct line in the list
  of SHAs in the terraform SHA file.
- the sha256sum command in the kubekins image does not have an '-s' option.
  '-c' should suffice.

i didn't have time to check why this SED call suddenly stopped working, but the `-s` problem for sha256sum was obvious in kubeadm-gce-master logs:
https://gubernator.k8s.io/build/kubernetes-jenkins/logs/ci-kubernetes-e2e-kubeadm-gce-master/1663

i've tested the change locally.
",closed,True,2019-01-24 16:03:35,2019-01-24 16:14:23
kubernetes-anywhere,joelsmith,https://github.com/kubernetes/kubernetes-anywhere/pull/583,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/583,Update embargo doc link in SECURITY_CONTACTS and change PST to PSC,See https://github.com/kubernetes/security/issues/8 for more information,closed,True,2019-03-08 18:10:02,2019-03-10 02:27:28
kubernetes-anywhere,jalbou,https://github.com/kubernetes/kubernetes-anywhere/issues/584,https://api.github.com/repos/kubernetes/kubernetes-anywhere/issues/584,Issue when defining vsphere cloud provider config,,closed,False,2019-03-24 14:31:49,2019-03-24 14:32:10
