name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/1,https://api.github.com/repos/kubernetes/federation/issues/1,WIP: Federation move out of core,"First PR on kubernetes/federation :)
cc @marun @shashidharatd ",closed,True,2017-09-23 14:02:07,2017-11-03 13:03:42
federation,marun,https://github.com/kubernetes/federation/pull/2,https://api.github.com/repos/kubernetes/federation/issues/2,WIP: Enable build and test,"This PR is based on a [filtered branch](https://github.com/marun/federation/tree/fed-filter-branch/) of [k8s.io/kubernetes](https://github.com/kubernetes/kubernetes).  From that filtered branch, it adds commits to enable build and test of federation artifacts.

See [README](https://github.com/marun/federation/blob/fed-move-out/migration-tools/README.md) for details of what was done and how to use it.

cc: @irfanurrehman @shashidharatd ",closed,True,2017-09-27 04:31:02,2017-10-25 23:10:50
federation,marun,https://github.com/kubernetes/federation/pull/3,https://api.github.com/repos/kubernetes/federation/issues/3,Add OWNERS from k8s.io/kubernetes/federation,This is required to enable the submit queue.,closed,True,2017-10-25 16:24:47,2017-10-25 16:38:12
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/4,https://api.github.com/repos/kubernetes/federation/issues/4,[Federation] Implement federated pod autoscaler,"_From @irfanurrehman on December 19, 2016 16:22_

This issue is created to track the development of a federated pod autoscaler.
The design document was updated [here ](https://docs.google.com/document/d/1Uacp99LskNHkJ6ZFFOVGorDivYKi4ySkI8E3tTFAZJ8/edit?usp=sharing) some days ago.

cc @kubernetes/sig-cluster-federation
@shashidharatd @kshafiee @deepak-vij

_Copied from original issue: kubernetes/kubernetes#38974_",open,False,2017-10-27 09:11:25,2018-02-23 15:36:42
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/5,https://api.github.com/repos/kubernetes/federation/issues/5,Move federation out of core repo,"<a href=""https://github.com/shashidharatd""><img src=""https://avatars3.githubusercontent.com/u/8734900?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [shashidharatd](https://github.com/shashidharatd)**
_Monday Sep 25, 2017 at 14:57 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/52992_

----

This issue is to track all the tasks associated with moving out federation related code out of core `kubernetes` repo into its own repo (`k8s.io/federation`).

Refer to the plan and discussion on moving out federation in this [google doc](https://docs.google.com/document/d/1aMwWigMh7gvihqghIdqqwRRU7-LTH1erKGsHv7rCASo)

/cc @kubernetes/sig-federation-pr-reviews 
/assign @irfanurrehman 

",closed,False,2017-10-27 09:34:13,2018-02-02 05:00:49
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/6,https://api.github.com/repos/kubernetes/federation/issues/6,[Federation] Follow up items on federated pod autoscaling,"<a href=""https://github.com/irfanurrehman""><img src=""https://avatars0.githubusercontent.com/u/10027921?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [irfanurrehman](https://github.com/irfanurrehman)**
_Wednesday Jul 26, 2017 at 16:16 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/49644_

----

The original issuse for feature ***federated pod autoscaler*** is [here](https://github.com/kubernetes/kubernetes/issues/38974), and the feature is being tracked [here](https://github.com/kubernetes/features/issues/257). This issue is to track and follow up pending items from the first phase of implementation especially items chosen to defer in the first phase of implementation [here](https://github.com/kubernetes/kubernetes/pull/45993).

1 - Make the replica nums offered (both min and max) and in turn increased or reduced from an existing cluster local hpa, configurable. Right now the value is kept to a pessimistic low of 1 at a time.
2 - Optimise the algorithm such that if there are clusters which need the replicas, they can first be taken from those clusters that have capacity to offer more.
3 - Implement hard limit based min/max preferences per cluster from the user.
4 - Contemplate usage of weighted distribution of replicas based on the weights applied on per cluster hpas by the user.
",closed,False,2017-10-27 13:16:19,2018-07-23 22:13:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/7,https://api.github.com/repos/kubernetes/federation/issues/7,[Federation] Better usability for kubefed,"<a href=""https://github.com/irfanurrehman""><img src=""https://avatars0.githubusercontent.com/u/10027921?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [irfanurrehman](https://github.com/irfanurrehman)**
_Sunday Feb 19, 2017 at 20:31 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/41725_

----

**kubefed** tool, which is used to deploy a federation control plane:

1 - Does not do a very good job of giving status information to the end user. If the operations take long time it appears to be hung (especially kubefed init).
2 - There is a possibility that the deployed pods of the federation control plane, do not come up successfully, in such a scenario the kubefed init will wait indefinitely. Ideally, there could be a timeout set to override this behavior (Issue #43123).
3 - If some operation fails midway of overall execution, the tool should ideally clean up all of the objects that it creates into host cluster or target cluster before exiting.
4 - There should ideally be a kubefed deinit which can enable removing the federation control plane deployment from the host cluster.
5 - Both kubefed init and kubefed join can create service accounts to access either the underlying cluster or joining clusters; A workaround is needed such that DNS-1123 subdomain naming is not violated while doing this. Refer https://github.com/kubernetes/kubernetes/issues/50888.  
",closed,False,2017-10-27 13:49:48,2018-07-23 18:09:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/8,https://api.github.com/repos/kubernetes/federation/issues/8,Kubefed can't handle relative paths in kubeconfig,"<a href=""https://github.com/Thermi""><img src=""https://avatars2.githubusercontent.com/u/3606849?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [Thermi](https://github.com/Thermi)**
_Wednesday Oct 04, 2017 at 12:09 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/53431_

----

/kind bug

**What happened**:
Kubefed doesn't handle relative paths in kubeconfig correctly, like kubectl does.

**What you expected to happen**:
That kubefed handles it like kubectl.

**How to reproduce it (as minimally and precisely as possible)**:
Use relative paths in kubeconfig for credentials (e.g. CA certificates). Then use kubectl to see that it works from arbitrary directories. Then try to use kubefed and see it fail.

**Anything else we need to know?**:
No.

**Environment**:
- Kubernetes version (use `kubectl version`): 
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0+0b9efaeb34a2f"", GitCommit:""638b0a06d928e7a12fa29bb12610158a235585d7"", GitTreeState:""dirty"", BuildDate:""2017-10-02T09:48:25Z"", GoVersion:""go1.9"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7+"", GitVersion:""v1.7.6-gke.1"", GitCommit:""407dbfe965f3de06b332cc22d2eb1ca07fb4d3fb"", GitTreeState:""clean"", BuildDate:""2017-09-27T21:21:34Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration**:
GCP
- OS (e.g. from /etc/os-release):
Ubuntu Xenial
- Kernel (e.g. `uname -a`): Irrelevant
- Install tools: GKE


",closed,False,2017-10-30 14:39:01,2018-07-02 12:49:42
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/9,https://api.github.com/repos/kubernetes/federation/issues/9,CustomResourceDefinition support for federation,"<a href=""https://github.com/ljmatkins""><img src=""https://avatars3.githubusercontent.com/u/868806?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [ljmatkins](https://github.com/ljmatkins)**
_Wednesday Oct 04, 2017 at 12:58 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/53432_

----

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind feature

**What happened**:
When I attempt to `kubectl apply` a CustomResourceDefinition to a federation, I receive the following error:

```error: unable to recognize ""resourcedefinition.yaml"": no matches for apiextensions.k8s.io/, Kind=CustomResourceDefinition```

**What you expected to happen**:

It would be nice if federations supported this resource type, so that we can run applications expecting it on top of federated clusters.

**How to reproduce it (as minimally and precisely as possible)**:

1) Bring up a federated cluster
2) Create a `resourcedefinition.yaml` file (e.g. https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/)
2) Run `kubectl apply -f resourcedefinition.yaml` against the federated cluster

**Environment**:
- Kubernetes version (use `kubectl version`): 1.7.6
- Cloud provider or hardware configuration: GKE 1.7.6-gke.1
- OS (e.g. from /etc/os-release): debian stretch


",open,False,2017-10-30 14:39:04,2018-01-29 08:44:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/10,https://api.github.com/repos/kubernetes/federation/issues/10,Federation: Google Cloud DNS provider error 404 caused by the ProjectID,"<a href=""https://github.com/walteraa""><img src=""https://avatars1.githubusercontent.com/u/5852041?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [walteraa](https://github.com/walteraa)**
_Tuesday Sep 19, 2017 at 14:25 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/52726_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

 /kind bug


**What happened**: I've tried to use Google DNS as DNS provider in a federation with FCP in an on-premises cluster. I've configured a secret with the Google credential file with correct permissions, I mounted the secret as a volume and assigned the ""volume path"" to the environment variable `GOOGLE_APPLICATION_CREDENTIALS`(before that I was receiving an authorization error, but this configuration fixed it) by editing the `federation-controller-manager` deployment. 

The credential file was generated from the [Google Cloud console credential page](https://console.developers.google.com/apis/credentials?pli=1&project=corc-tutorial) and has the following structure

```json
{
  ""type"": ""service_account"",
  ""project_id"": ""corc-tutorial"",
  ""private_key_id"": ""{THE PRIVATE KEY ID}"",
  ""private_key"": ""-----BEGIN PRIVATE KEY-----\n
{THE PRIVATE KEY CONTENT}
\n-----END PRIVATE KEY-----\n"",
  ""client_email"": ""{ACCOUNT}"",
  ""client_id"": ""{CLIENT_ID}"",
  ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",
  ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",
  ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",
  ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/{GSERVICEACCOUNT_URI}""
}
```

Then I've got the following logs in the `federation-controller-manager` pod
```
I0822 13:17:33.998181       1 clouddns.go:100] Successfully got DNS service: &{0xc420e19b00 https://www.googleapis.com/dns/v1/projects/  0xc42000e840 0xc42000e848 0xc42000e850 0xc42000e858}
E0822 13:17:35.621941       1 dns.go:97] Failed to retrieve DNS zone: error querying for DNS zones: googleapi: got HTTP response code 404 with body: Not Found
F0822 13:17:35.622009       1 controllermanager.go:144] Failed to start service dns controller: error querying for DNS zones: googleapi: got HTTP response code 404 with body: Not Found
```

Analyzing the issue, I figured out that the Google cloud provider isn't getting the `project_id` key/value, which is causing a bug in the `ProjectID` property, thus generating an invalid URL when trying to get information from my DNS zone.


**What you expected to happen**: Once generated the credential file and configure it in the `federation-controller-manager` deployment as previously described, the Google DNS provider should get the DNS information from the REST API and successfully manage it.

**How to reproduce it (as minimally and precisely as possible)**:

1. Create an on-premises cluster
2. Create a DNS zone in your Google cloud console
3. Initialize a federation which the FCP is the on-premises cluster created before and using Google as DNS provider and the DNS zone created before
4. Create a credential file giving permission to manage the project's DNS
5. Create a secret using the credential file created before
6. Edit the  `federation-controller-manager` by creating the volumes using the secret created before and an environment variable named `GOOGLE_APPLICATION_CREDENTIALS` by using secret's mount path as value.


**Anything else we need to know?**:
I did it work, as it should, doing a workaround which sets the `ProjectID` value hardcoded as you can see [here](https://github.com/walteraa/kubernetes/commit/3110b2cad1751eab499717c1f5f029bbb59f349e#diff-9e1208e1d901d951628091e4e80c06dcR58)

**Environment**:
- Kubernetes version (use `kubectl version`): 1.7.4
- Cloud provider or hardware configuration**: *irrelevant*
- OS (e.g. from /etc/os-release): *irrelevant*
- Kernel (e.g. `uname -a`): *irrelevant*
- Install tools: *irrelevant*
- Others: *irrelevant*

",closed,False,2017-10-30 14:47:19,2018-07-02 12:49:42
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/11,https://api.github.com/repos/kubernetes/federation/issues/11,New Federation API Resource for ClusterDaemonSets,"<a href=""https://github.com/nutchalum""><img src=""https://avatars3.githubusercontent.com/u/22613696?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nutchalum](https://github.com/nutchalum)**
_Thursday Sep 28, 2017 at 03:04 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/53178_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug

/kind feature


**What happened**:
I want a to have a new federation api resource _**ClusterDaemonSets**_

**What you expected to happen**:
This works a little like **_DemonSets_** resource.
_**DemonSets**_ makes sure every nodes must have a pod running in it.
_**ClusterDaemonSets**_ makes sure every clusters must have a pod running in it.

For example
Every cluster must has a kubernetes-dashboard.
I want to make sure there's only one (or more) dashboard **per cluster**
I can do that by using federation feature and create a Deployment and then scaling it out.
However, If one cluster is down, It'll try to make a pod in other cluster instead.
I want a resource to make sure there's only **one or more per cluster**
And if the cluster is gone, It won't try to pop up in other cluster

**Anything else we need to know?**:
Thank you.

",closed,False,2017-10-30 14:47:26,2018-07-23 20:11:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/12,https://api.github.com/repos/kubernetes/federation/issues/12,pr:pull-kubernetes-federation-e2e-gce flaked 94 times in the past week,"<a href=""https://github.com/fejta-bot""><img src=""https://avatars1.githubusercontent.com/u/30488982?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [fejta-bot](https://github.com/fejta-bot)**
_Wednesday Sep 13, 2017 at 22:28 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/52455_

----

### Flaky Job: pr:pull-kubernetes-federation-e2e-gce
 Flakes in the past week: **94**
 Consistency: **86.80%**

#### Flakiest tests by flake count:
| Test | Flake Count |
| --- | --- |
| Federation Up | 95 |
| DumpFederationLogs | 89 |
| FederationTest | 5 |


[Flakiest Jobs](https://storage.googleapis.com/k8s-metrics/flakes-latest.json)

",closed,False,2017-10-30 14:59:09,2018-02-02 05:06:51
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/13,https://api.github.com/repos/kubernetes/federation/issues/13,Federation: google cloud dns records aren't properly when create/update/delete endpoints or LB service,"<a href=""https://github.com/mozhuli""><img src=""https://avatars2.githubusercontent.com/u/13694671?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mozhuli](https://github.com/mozhuli)**
_Monday Sep 18, 2017 at 09:06 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/52643_

----

I have a federation with two clusters in a GKE cluster, the version is `v1.7.5` , show below:
```
$ kubectl get clusters --context=kfed
NAME       STATUS    AGE
cluster1   Ready     1h
cluster2   Ready     1h
```
When I create a federated LoadBalancer service, the LoadBalancer Ingress are created in both clusters. 
```
$ kubectl describe svc nginx --context=kfed
Name:			nginx
Namespace:		default
Labels:			app=nginx
Annotations:		federation.kubernetes.io/service-ingresses={}
Selector:		app=nginx
Type:			LoadBalancer
IP:			
LoadBalancer Ingress:	35.189.185.189, 35.200.87.19
Port:			http	80/TCP
Endpoints:		<none>
Session Affinity:	None
Events:			<none>
```

However, when only cluster1 has healthy endpoints (I create deployment use --context=cluster1), There 
 are not  CNAME records in DNS with cluster2.(DNS should create CNAME records with cluster2)
```
$ kubectl describe svc nginx --context=kfed
Name:			nginx
Namespace:		default
Labels:			app=nginx
Annotations:		federation.kubernetes.io/service-ingresses={""items"":[{""cluster"":""cluster1"",""items"":[{""ip"":""35.189.185.189""}]}]}
Selector:		app=nginx
Type:			LoadBalancer
IP:			
LoadBalancer Ingress:	35.189.185.189, 35.200.87.19
Port:			http	80/TCP
Endpoints:		<none>
Session Affinity:	None
Events:			<none>

$ kubectl create -f deployment.yaml --context=cluster1

$ gcloud  dns record-sets list --zone=kfed
NAME                                                                 TYPE  TTL    DATA
infra.mycompany.com.                                                 NS    21600  ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com.
infra.mycompany.com.                                                 SOA   21600  ns-cloud-e1.googledomains.com. cloud-dns-hostmaster.google.com. 1 21600 3600 259200 300
nginx.default.kfed.svc.asia-east1-c.asia-east1.infra.mycompany.com.  A     180    35.189.185.189
nginx.default.kfed.svc.asia-east1.infra.mycompany.com.               A     180    35.189.185.189
nginx.default.kfed.svc.infra.mycompany.com.                          A     180    35.189.185.189
```

when I create healthy endpoints in cluster2,the dns records(correct) show below:
```
$ kubectl create -f deployment.yaml --context=cluster2

$ kubectl describe svc nginx --context=kfed
Name:			nginx
Namespace:		default
Labels:			app=nginx
Annotations:		federation.kubernetes.io/service-ingresses={""items"":[{""cluster"":""cluster1"",""items"":[{""ip"":""35.189.185.189""}]},{""cluster"":""cluster2"",""items"":[{""ip"":""35.200.87.19""}]}]}
Selector:		app=nginx
Type:			LoadBalancer
IP:			
LoadBalancer Ingress:	35.189.185.189, 35.200.87.19
Port:			http	80/TCP
Endpoints:		<none>
Session Affinity:	None
Events:			<none>

$ gcloud  dns record-sets list --zone=kfed
NAME                                                                           TYPE  TTL    DATA
infra.mycompany.com.                                                           NS    21600  ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com.
infra.mycompany.com.                                                           SOA   21600  ns-cloud-e1.googledomains.com. cloud-dns-hostmaster.google.com. 1 21600 3600 259200 300
nginx.default.kfed.svc.asia-east1-c.asia-east1.infra.mycompany.com.            A     180    35.189.185.189
nginx.default.kfed.svc.asia-east1.infra.mycompany.com.                         A     180    35.189.185.189
nginx.default.kfed.svc.asia-northeast1-c.asia-northeast1.infra.mycompany.com.  A     180    35.200.87.19
nginx.default.kfed.svc.asia-northeast1.infra.mycompany.com.                    A     180    35.200.87.19
nginx.default.kfed.svc.infra.mycompany.com.                                    A     180    35.189.185.189,35.200.87.19
```
And when i delete deployment in cluster2, the related records not update properly.
```
$ kubectl delete -f deployment.yaml --context=cluster2
deployment ""nginx-deployment"" deleted

$ kubectl describe svc nginx --context=kfed
Name:			nginx
Namespace:		default
Labels:			app=nginx
Annotations:		federation.kubernetes.io/service-ingresses={""items"":[{""cluster"":""cluster1"",""items"":[{""ip"":""35.189.185.189""}]}]}
Selector:		app=nginx
Type:			LoadBalancer
IP:			
LoadBalancer Ingress:	35.189.185.189, 35.200.87.19
Port:			http	80/TCP
Endpoints:		<none>
Session Affinity:	None
Events:			<none>

$ gcloud  dns record-sets list --zone=kfed
NAME                                                                           TYPE  TTL    DATA
infra.mycompany.com.                                                           NS    21600  ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com.
infra.mycompany.com.                                                           SOA   21600  ns-cloud-e1.googledomains.com. cloud-dns-hostmaster.google.com. 1 21600 3600 259200 300
nginx.default.kfed.svc.asia-east1-c.asia-east1.infra.mycompany.com.            A     180    35.189.185.189
nginx.default.kfed.svc.asia-east1.infra.mycompany.com.                         A     180    35.189.185.189
nginx.default.kfed.svc.asia-northeast1-c.asia-northeast1.infra.mycompany.com.  A     180    35.200.87.19
nginx.default.kfed.svc.asia-northeast1.infra.mycompany.com.                    A     180    35.200.87.19
nginx.default.kfed.svc.infra.mycompany.com.                                    A     180    35.189.185.189
```
The `nginx.default.kfed.svc.infra.mycompany.com. ` record update `35.189.185.189,35.200.87.19` to `35.189.185.189`, but `nginx.default.kfed.svc.asia-northeast1-c.asia-northeast1.infra.mycompany.com.` and `nginx.default.kfed.svc.asia-northeast1.infra.mycompany.com.  ` didn't update to CNAME.

Also when i delete LB service, the dns records show below:
```
$ gcloud  dns record-sets list --zone=kfed
NAME                                                                           TYPE   TTL    DATA
infra.mycompany.com.                                                           NS     21600  ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com.
infra.mycompany.com.                                                           SOA    21600  ns-cloud-e1.googledomains.com. cloud-dns-hostmaster.google.com. 1 21600 3600 259200 300
nginx.default.kfed.svc.asia-east1-c.asia-east1.infra.mycompany.com.            CNAME  180    nginx.default.kfed.svc.asia-east1.infra.mycompany.com.
nginx.default.kfed.svc.asia-east1.infra.mycompany.com.                         CNAME  180    nginx.default.kfed.svc.infra.mycompany.com.
nginx.default.kfed.svc.asia-northeast1-c.asia-northeast1.infra.mycompany.com.  A      180    35.200.87.19
nginx.default.kfed.svc.asia-northeast1.infra.mycompany.com.                    A      180    35.200.87.19
```
which confuse me that record `nginx.default.kfed.svc.asia-east1-c.asia-east1.infra.mycompany.com. ` and `nginx.default.kfed.svc.asia-east1.infra.mycompany.com. ` update to CNAME which desire to be deleted.

The problems same in v1.6.9


/kind bug

/cc @kubernetes/sig-federation-bugs @quinton-hoole 
",closed,False,2017-10-30 14:59:17,2018-07-23 20:11:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/14,https://api.github.com/repos/kubernetes/federation/issues/14,[e2e test]: pull-kubernetes-federation-e2e-gce test always fail,"<a href=""https://github.com/guangxuli""><img src=""https://avatars2.githubusercontent.com/u/17472298?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [guangxuli](https://github.com/guangxuli)**
_Monday Sep 11, 2017 at 12:12 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/52270_

----

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:
Tests about https://github.com/kubernetes/kubernetes/pull/51889 always generate pull-kubernetes-federation-e2e-gce failure. Seems not relevant to the PR https://github.com/kubernetes/kubernetes/pull/51889 after checking the details. 

**log:**
I0911 10:09:03.239] process 22525 exited with code 0 after 0.5m
I0911 10:09:03.239] Call:  git checkout -B test db809c0eb7d33fac8f54d8735211f2f3a8fc4214
W0911 10:09:04.570] Switched to a new branch 'test'
I0911 10:09:04.582] process 22538 exited with code 0 after 0.0m
I0911 10:09:04.583] Call:  git merge --no-ff -m 'Merge +refs/pull/51889/head:refs/pr/51889' 3e3f9990ddfc3a003aad71340dc1294daa6764f6
I0911 10:09:05.164] Merge made by the 'recursive' strategy.
I0911 10:09:05.169]  plugin/pkg/scheduler/algorithm/predicates/error.go |  39 +++----
I0911 10:09:05.169]  .../scheduler/algorithm/predicates/predicates.go   |  44 ++++----
I0911 10:09:05.169]  .../algorithm/predicates/predicates_test.go        | 115 +++++++++++++--------
I0911 10:09:05.169]  3 files changed, 114 insertions(+), 84 deletions(-)
I0911 10:09:05.170] process 22539 exited with code 0 after 0.0m
I0911 10:09:05.170] Checkout: /var/lib/jenkins/workspace/pull-kubernetes-federation-e2e-gce/go/src/k8s.io/release master
I0911 10:09:05.170] Call:  git init k8s.io/release
... skipping 573 lines ...
I0911 10:22:30.580] md5sum(kubernetes-test.tar.gz)=0225e5099a5d437c9f404a5302ae54c7
I0911 10:22:31.161] sha1sum(kubernetes-test.tar.gz)=7528c3aada4254ce0458942a7eb928207940ffb7
I0911 10:22:31.161] 
I0911 10:22:31.162] Extracting kubernetes-test.tar.gz into /workspace/kubernetes
W0911 10:22:38.006] 2017/09/11 10:22:38 util.go:131: Step './get-kube.sh' finished in 11.992213626s
W0911 10:22:38.006] 2017/09/11 10:22:38 util.go:129: Running: ./federation/cluster/federation-down.sh
**`W0911 10:22:38.195] error: context ""e2e-f8n-agent-pr-52-0"" does not exist`**
I0911 10:22:38.296] Cleaning Federation control plane objects
I0911 10:22:38.608] No resources found
I0911 10:22:39.115] No resources found
I0911 10:22:39.263] +++ [0911 10:22:39] Removing namespace ""f8n-system-agent-pr-52-0"", cluster role ""federation-controller-manager:e2e-f8n-agent-pr-52-0-federation-e2e-gce-us-central1-a-federation-e2e-gce-us-central1-f"" and cluster role binding ""federation-controller-manager:e2e-f8n-agent-pr-52-0-federation-e2e-gce-us-central1-a-federation-e2e-gce-us-central1-f"" from ""federation-e2e-gce-us-central1-a""
I0911 10:22:39.263] +++ [0911 10:22:39] Removing namespace ""f8n-system-agent-pr-52-0"", cluster role ""federation-controller-manager:e2e-f8n-agent-pr-52-0-federation-e2e-gce-us-central1-b-federation-e2e-gce-us-central1-f"" and cluster role binding ""federation-controller-manager:e2e-f8n-agent-pr-52-0-federation-e2e-gce-us-central1-b-federation-e2e-gce-us-central1-f"" from ""federation-e2e-gce-us-central1-b""
I0911 10:22:39.263] +++ [0911 10:22:39] Removing namespace ""f8n-system-agent-pr-52-0"", cluster role ""federation-controller-manager:e2e-f8n-agent-pr-52-0-federation-e2e-gce-us-central1-f-federation-e2e-gce-us-central1-f"" and cluster role binding ""federation-controller-manager:e2e-f8n-agent-pr-52-0-federation-e2e-gce-us-central1-f-federation-e2e-gce-us-central1-f"" from ""federation-e2e-gce-us-central1-f""
... skipping 20 lines ...
I0911 10:44:42.118] Creating federation control plane service...................................................................................................................................................................................................................................................Dumping Federation and DNS pod logs to /workspace/_artifacts
W0911 10:44:42.219] I0911 10:24:42.157731    4041 init.go:305] Creating a namespace f8n-system-agent-pr-52-0 for federation system components
W0911 10:44:42.219] I0911 10:24:42.179313    4041 init.go:314] Creating federation control plane service
W0911 10:44:42.219] 2017/09/11 10:44:42 util.go:131: Step './federation/cluster/federation-up.sh' finished in 20m0.402805001s
W0911 10:44:42.219] 2017/09/11 10:44:42 e2e.go:460: Dumping Federation logs to: /workspace/_artifacts
W0911 10:44:42.219] 2017/09/11 10:44:42 util.go:129: Running: ./federation/cluster/log-dump.sh /workspace/_artifacts
**`W0911 10:44:42.620] Error from server (InternalError): Internal error occurred: Authorization error (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy)`**
W0911 10:44:42.623] 2017/09/11 10:44:42 util.go:131: Step './federation/cluster/log-dump.sh /workspace/_artifacts' finished in 553.886159ms
**`W0911 10:44:42.623] 2017/09/11 10:44:42 util.go:129: Running: ./federation/cluster/federation-down.sh`**
**`W0911 10:44:42.812] error: context ""e2e-f8n-agent-pr-52-0"" does not exist`**
I0911 10:44:42.913] Cleaning Federation control plane objects
I0911 10:44:43.028] service ""e2e-f8n-agent-pr-52-0-apiserver"" deleted
I0911 10:44:43.555] secret ""default-token-d678j"" deleted

**What you expected to happen**:

pull-kubernetes-federation-e2e-gce test pass.

**How to reproduce it (as minimally and precisely as possible)**:
Just trigger the test job.

**Anything else we need to know?**:
none

@kubernetes/test-infra-maintainers
",closed,False,2017-10-30 15:13:43,2017-11-07 03:58:05
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/15,https://api.github.com/repos/kubernetes/federation/issues/15,Federation: API server does not allow privileged containers,"<a href=""https://github.com/erikgrinaker""><img src=""https://avatars2.githubusercontent.com/u/644420?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [erikgrinaker](https://github.com/erikgrinaker)**
_Tuesday Sep 12, 2017 at 14:14 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/52341_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

Attempting to deploy a DaemonSet with `securityContext.privileged: true` in a federated cluster gives an error: `Forbidden: disallowed by cluster policy`

**What you expected to happen**:

The DaemonSet to be created.

**How to reproduce it (as minimally and precisely as possible)**:

Create a federated cluster using `kubefed`, and submit the following DaemonSet:

```yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: privileged
  labels:
    app: privileged
spec:
  template:
    metadata:
      labels:
        app: privileged
    spec:
      containers:
      - name: privileged
        image: busybox
        command: ['sleep', '10000']
        securityContext:
          privileged: true
```

This gives the following error:

```sh
$ kubectl apply -f privileged.yaml
The DaemonSet ""privileged"" is invalid: spec.template.spec.containers[0].securityContext.privileged: Forbidden: disallowed by cluster policy
```
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): 1.7.3
- Cloud provider or hardware configuration**: GKE
- OS (e.g. from /etc/os-release): COS 59 build 9460.73.0
- Kernel (e.g. `uname -a`): 4.4.52
- Install tools:
- Others: kubefed 1.7.3

",closed,False,2017-10-30 15:14:14,2018-07-23 19:10:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/16,https://api.github.com/repos/kubernetes/federation/issues/16,[Federation] Requirement & Design of Federated ResourceQuota,"<a href=""https://github.com/weijinxu""><img src=""https://avatars3.githubusercontent.com/u/17317813?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [weijinxu](https://github.com/weijinxu)**
_Thursday May 25, 2017 at 21:21 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/46464_

----


This issue is created to track the development of federated resource quota.
The design document was updated [here](https://docs.google.com/document/d/1F97GRKJyd0yRS4BNTl38304_DmCX1-O2zbWYRm0FUlE).

cc @kubernetes/sig-cluster-federation
@derekwaynecarr 
",closed,False,2017-10-30 15:19:22,2018-07-23 20:11:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/17,https://api.github.com/repos/kubernetes/federation/issues/17,Federated Ingress sends unnecessary cross-region requests due to low maxRPS setting,"<a href=""https://github.com/dgpc""><img src=""https://avatars0.githubusercontent.com/u/101610?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [dgpc](https://github.com/dgpc)**
_Tuesday May 30, 2017 at 20:20 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/46645_

----

**Kubernetes version** (use `kubectl version`):

$ kubectl version --context=dev
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.3"", GitCommit:""0480917b552be33e2dba47386e51decb1a211df6"", GitTreeState:""clean"", BuildDate:""2017-05-10T15:48:59Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4"", GitCommit:""d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae"", GitTreeState:""clean"", BuildDate:""2017-05-19T18:33:17Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- GKE (cluster version 1.6.4)

**What happened**:

I create a Federated Ingress with clusters in two GCE Regions (us-central1 & europe-west1).

Both clusters are created with balancingMode=rate, maxRPS=1

When more than (1 RPS * num_nodes) is being sent from clients in Europe, traffic appears to be round-robined to both the cluster in the US and the cluster in Europe.

**What you expected to happen**:

I expect to be able to send most traffic from European clients to the cluster in Europe, and only spill-over to the US when Europe is overloaded. To accomplish this, maxRPS needs to be set to an appropriately large value (based on our load testing), or we need to be able to use the utilization-based mode.

Right now, our clients unnecessarily have to pay a latency penalty due traffic being sent cross-region, even when we are not close to being overloaded in any region.

Perhaps the Ingress Controller could adopt a maxRPS from an annotation on the deployment, similar to the way it adopts a Health Check from the readinessProbe?

/kind bug
/sig federation
",closed,False,2017-10-30 15:19:27,2018-10-19 17:59:49
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/18,https://api.github.com/repos/kubernetes/federation/issues/18,Federation: kubectl rollout status with Federated Deployment ,"<a href=""https://github.com/bspradling8""><img src=""https://avatars1.githubusercontent.com/u/8116831?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [bspradling8](https://github.com/bspradling8)**
_Monday Jun 05, 2017 at 22:19 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/46994_

----

It looks like currently with Federated Deployments, the federated API server will get stuck at waiting with 0 out of `n`. I can only assume that this occurs because the federationa API server knows about the deployment and how many replicas there should be but isn't the one responsible for running the pods therefore stuck at 0. Is there a plan to support this functionality in Federated Deployments? 
",closed,False,2017-10-30 15:19:54,2018-07-23 22:13:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/19,https://api.github.com/repos/kubernetes/federation/issues/19,kubefed init errors if namespace already exists,"<a href=""https://github.com/derekwaynecarr""><img src=""https://avatars3.githubusercontent.com/u/6233452?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [derekwaynecarr](https://github.com/derekwaynecarr)**
_Thursday Jun 08, 2017 at 06:04 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/47159_

----

**Is this a BUG REPORT or FEATURE REQUEST?**
BUG

**What happened**:

```
$ kubectl create ns federation-system
$ kubefed init my-fed --dns-provider=google-clouddns \
        --federation-system-namespace=""federation-system"" \
        --etcd-persistent-storage=false
Error from server (AlreadyExists): namespaces ""federation-system"" already exists
```

**What you expected to happen**:
I expected the command to not error if the namespace already existed.
kubefed should only create a namespace if it doesnt already exist, otherwise just use the one i gave.

**How to reproduce it** (as minimally and precisely as possible):
see above

**Anything else we need to know**:
nope

",closed,False,2017-10-30 15:20:09,2018-01-17 13:10:27
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/20,https://api.github.com/repos/kubernetes/federation/issues/20,Federated Ingress fails with hybrid (gke/on-prem) environments,"<a href=""https://github.com/clenimar""><img src=""https://avatars3.githubusercontent.com/u/410648?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [clenimar](https://github.com/clenimar)**
_Wednesday Jun 14, 2017 at 15:07 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/47522_

----

**Is this a request for help?**: yes
**What keywords did you search in Kubernetes issues before filing this one?**: hybrid federated ingress, on-premise ingress
**Is this a BUG REPORT or FEATURE REQUEST?**: bug report
**Kubernetes version** (use `kubectl version`): v1.5.2
**What happened**: I've got a hybrid federation running on GKE with one on-premise cluster:
```
c1 (host): GKE/us-east1-b/v1.6.4
c2       : GKE/asia-east1-b/v1.6.4
local    : on-premise/v1.6.3
```
I'm using Google Cloud DNS as my DNS provider. I'm able to create a federated deployment and a federated service, and they are correctly spread across all the clusters. The DNS entries for all services replicas are correctly created in my DNS zone.

When I create a Federated Ingress, though, the Ingress resource is created in all the clusters, but the on-premise cluster replica never gets an IP address, thus not being able to receive load from the global IP address.

This is the local replica:
```shell
clenimar@local:~$ kubectl describe ing 
Name:			nginx
Namespace:		default
Address:		
Default backend:	nginx:80 (192.168.181.52:80,192.168.181.62:80)
Rules:
  Host	Path	Backends
  ----	----	--------
  *	* 	nginx:80 (192.168.181.52:80,192.168.181.62:80)
Annotations:
  first-cluster:	c1
Events:			<none>
```
This is a replica running in one GKE cluster:
```shell
clenimar@gke-us:~$ k describe ing
Name:			nginx
Namespace:		default
Address:		35.186.232.168
Default backend:	nginx:80 (10.32.0.8:80,10.32.1.8:80)
Rules:
  Host	Path	Backends
  ----	----	--------
  *	* 	nginx:80 (10.32.0.8:80,10.32.1.8:80)
Annotations:
  first-cluster:	c1
  backends:		{""k8s-be-30036--699c6201827491b5"":""HEALTHY""}
  forwarding-rule:	k8s-fw-default-nginx--699c6201827491b5
  target-proxy:		k8s-tp-default-nginx--699c6201827491b5
  url-map:		k8s-um-default-nginx--699c6201827491b5
Events:
  FirstSeen	LastSeen	Count	From				SubObjectPath	Type		Reason	Message
  ---------	--------	-----	----				-------------	--------	------	-------
  7m		7m		1	{loadbalancer-controller }			Normal		ADD	default/nginx
  6m		6m		1	{loadbalancer-controller }			Normal		CREATE	ip: 35.186.232.168
  6m		4m		10	{loadbalancer-controller }			Normal		Service	default backend set to nginx:30036
  5m		4m		2	{loadbalancer-controller }			Warning		GCE	googleapi: Error 400: The resource 'projects/angular-sorter-167417/global/healthChecks/k8s-be-31996--699c6201827491b5' is not ready, resourceNotReady
```

There are some clear differences between the two resources. I have tried to manually created them, without success. I'm open to suggestions.
 
**What you expected to happen**: The load to be routed to the on-premise cluster too. I'm aware that this is not officially supported as of today, but I've seen people getting it to work in AWS/Azure with no problem at all. I'm filling this issue in order to chase at least a temporary workaround until the spec for a hybrid Ingress is finished.

**Ho to reproduce it** (as minimally and precisely as possible):
1. create a federation in GKE.
2. join an on-premise cluster. I've followed these instructions: https://github.com/henriquetruta/hybrid-k8s-federation/
3. create a federated deploy, a federated service and a federated ingress.
4. the on-premise ingress will not work properly.

**Anything else we need to know**:
The on-premise cluster has no `/var/log/glbc.log` file.
The l7-default-backend pod has nothing in its logs.
federation-controller-manager logs:
```shell
E0614 14:50:13.426549       1 ingress_controller.go:898] Failed to execute updates for default/nginx: the server has asked for the client to provide credentials (put ingresses.extensions nginx)
E0614 14:50:16.159188       1 cluster_client.go:147] Failed to list nodes while getting zone names: the server has asked for the client to provide credentials (get nodes)
W0614 14:50:16.159206       1 clustercontroller.go:197] Failed to get zones and region for cluster c1: the server has asked for the client to provide credentials (get nodes)
E0614 14:50:21.905797       1 ingress_controller.go:518] Internal error: Cluster ""c1"" queued for configmap reconciliation, but not found.  Will try again later: error = <nil>
E0614 14:50:41.906015       1 ingress_controller.go:518] Internal error: Cluster ""c1"" queued for configmap reconciliation, but not found.  Will try again later: error = <nil>
E0614 14:50:57.012691       1 cluster_client.go:147] Failed to list nodes while getting zone names: the server has asked for the client to provide credentials (get nodes)
W0614 14:50:57.012712       1 clustercontroller.go:197] Failed to get zones and region for cluster c1: the server has asked for the client to provide credentials (get nodes)
E0614 14:51:01.906283       1 ingress_controller.go:518] Internal error: Cluster ""c1"" queued for configmap reconciliation, but not found.  Will try again later: error = <nil>
I0614 14:51:16.168346       1 deploymentcontroller.go:594] Updating nginx in c2
I0614 14:51:16.168410       1 deploymentcontroller.go:594] Updating nginx in local
E0614 14:51:21.906545       1 ingress_controller.go:518] Internal error: Cluster ""c1"" queued for configmap reconciliation, but not found.  Will try again later: error = <nil>
E0614 14:51:37.864862       1 cluster_client.go:147] Failed to list nodes while getting zone names: the server has asked for the client to provide credentials (get nodes)
W0614 14:51:37.864909       1 clustercontroller.go:197] Failed to get zones and region for cluster c1: the server has asked for the client to provide credentials (get nodes)
E0614 14:51:41.906786       1 ingress_controller.go:518] Internal error: Cluster ""c1"" queued for configmap reconciliation, but not found.  Will try again later: error = <nil>
E0614 14:52:01.907232       1 ingress_controller.go:518] Internal error: Cluster ""c1"" queued for configmap reconciliation, but not found.  Will try again later: error = <nil>
E0614 14:52:18.733527       1 cluster_client.go:147] Failed to list nodes while getting zone names: the server has asked for the client to provide credentials (get nodes)
W0614 14:52:18.733561       1 clustercontroller.go:197] Failed to get zones and region for cluster c1: the server has asked for the client to provide credentials (get nodes)
E0614 14:52:21.907710       1 ingress_controller.go:518] Internal error: Cluster ""c1"" queued for configmap reconciliation, but not found.  Will try again later: error = <nil>
E0614 14:52:41.908053       1 ingress_controller.go:518] Internal error: Cluster ""c1"" queued for configmap reconciliation, but not found.  Will try again later: error = <nil>
I0614 14:53:09.787667       1 deploymentcontroller.go:594] Updating nginx in c1
```
events:
```shell
31m        31m         3         nginx     Ingress                  Normal    CreateInCluster       {federated-ingress-controller }      Creating ingress in cluster c1
31m        31m         1         nginx     Ingress                  Normal    CreateInCluster       {federated-ingress-controller }      Creating ingress in cluster c2
31m        31m         5         nginx     Ingress                  Normal    UpdateInCluster       {federated-ingress-controller }      Updating ingress in cluster local
30m        30m         3         nginx     Ingress                  Normal    CreateInCluster       {federated-ingress-controller }      Creating ingress in cluster c1
30m        30m         1         nginx     Ingress                  Normal    CreateInCluster       {federated-ingress-controller }      Creating ingress in cluster c2
30m        30m         1         nginx     Ingress                  Normal    CreateInCluster       {federated-ingress-controller }      Creating ingress in cluster local
4m         29m         152       nginx     Ingress                  Normal    UpdateInCluster       {federated-ingress-controller }      Updating ingress in cluster c2
4m         29m         135       nginx     Ingress                  Normal    UpdateInCluster       {federated-ingress-controller }      Updating ingress in cluster c1
19m        19m         1         nginx     Ingress                  Normal    FailedClusterUpdate   {federated-ingress-controller }      Ingress update in cluster c1 failed: the server has asked for the client to provide credentials (put ingresses.extensions nginx)
```

",closed,False,2017-10-30 15:20:34,2018-10-20 10:33:19
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/21,https://api.github.com/repos/kubernetes/federation/issues/21,federation: ingress does not work on GCP for multiple clusters in the same zone,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Thursday Jun 15, 2017 at 01:36 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/47565_

----

Problem: When we create a federated ingress with 2 clusters in the same zone, ingress controller in both clusters generate the same instance group name and hence fight with each other by removing each others nodes from that instance group.
This is a problem only with clusters in the same zone since get instance group API takes the name and zone and hence clusters in different zones get different instance groups.

Possible solution: We can solve it in the same way we did for firewall rules. Federation adds a provider Uid which is unique to all clusters. We can update ingress controller to use the provider Uid if present to generate the instance group name and hence different clusters will have different instance group names.
This will break existing federation users.

This is not high priority in my mind, since most multi cluster users run clusters in different zones.

cc @nicksardo @madhusudancs 
",closed,False,2017-10-30 15:20:52,2018-07-23 18:09:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/22,https://api.github.com/repos/kubernetes/federation/issues/22,kubefed should emit error when passing invalid dns-provider ,"<a href=""https://github.com/henriquetruta""><img src=""https://avatars2.githubusercontent.com/u/2501482?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [henriquetruta](https://github.com/henriquetruta)**
_Monday Jul 10, 2017 at 21:00 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/48732_

----

/kind bug

**What happened**:
I've misspelled the dns-provider (put googlecloud-dns instead of google-clouddns) when running `kubefed init`
It successfully ran and said everything was ok. However, federation-control-plane pod entered in CrashLoopBackOff and I could only see the error in the pod's logs:
```F0710 20:15:37.946633       1 controllermanager.go:174] Cloud provider could not be initialized: unknown DNS provider ""googlecloud-dns""```


**What you expected to happen**:
If any string different than 'google-clouddns', 'aws-route53' or 'coredns' is provided in `dns-provider`, kubefed should return an error to the client.

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.0"", GitCommit:""d3ada0119e776222f11ec7945e6d860061339aad"", GitTreeState:""clean"", BuildDate:""2017-06-29T23:15:59Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

",closed,False,2017-10-30 15:20:57,2018-07-23 20:11:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/23,https://api.github.com/repos/kubernetes/federation/issues/23,kubefed join breaks with aws cluster,"<a href=""https://github.com/henriquetruta""><img src=""https://avatars2.githubusercontent.com/u/2501482?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [henriquetruta](https://github.com/henriquetruta)**
_Monday Jul 17, 2017 at 18:23 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/49041_

----

Joining a cluster from AWS fails in kubefed 1.7. The AWS cluster is also k8s 1.7
The federation is running in a GKE cluster 1.7.0. 

```
$ kubefed join aws --host-cluster-context=gke --context=fed
Error from server (Forbidden): clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:fed-aws-gke"" is forbidden: attempt to grant extra privileges: [PolicyRule{Resources:[""*""], APIGroups:[""*""], Verbs:[""*""]} PolicyRule{NonResourceURLs:[""/healthz""], Verbs:[""get""]}] user=&{admin admin [system:authenticated] map[]} ownerrules=[] ruleResolutionErrors=[]
```

This same command works in kubefed 1.6.6
I'm able to access the AWS cluster using its context.

/kind bug


**How to reproduce it (as minimally and precisely as possible)**:
1 - Create a GKE cluster
2 - Create an AWS cluster (I used conjure-up)
3 - kubefed (1.7) init on GKE cluster
4 - Join the AWS cluster
5 - It should fail

If kubefed 1.6 is used, it works.
Joining a GKE or On-prem cluster also works as expected

",closed,False,2017-10-30 15:21:08,2018-07-23 18:09:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/24,https://api.github.com/repos/kubernetes/federation/issues/24,Support for CloudFlare DNS provider for Federated clusters,"<a href=""https://github.com/thecodeassassin""><img src=""https://avatars1.githubusercontent.com/u/939775?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [thecodeassassin](https://github.com/thecodeassassin)**
_Wednesday Jul 26, 2017 at 14:27 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/49639_

----

/kind feature

Currently federation only supports Google CloudDNS, Route53 and CoreDNS. I would love if if CloudFlare could also be supported since we use them for our DNS zones. I'm sure this will be a popular addition as well. It's already supported for ExternalDNS. I'm sure most of the work can be driven from existing providers and ExternalDNS:

https://github.com/kubernetes-incubator/external-dns/blob/master/provider/cloudflare.go

Maybe it would be nice to merge them at some point. Is there anybody already working on this or will it be my summer project?

",closed,False,2017-10-30 15:21:22,2018-07-23 22:13:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/25,https://api.github.com/repos/kubernetes/federation/issues/25,[Federation] Follow up items on federated pod autoscaling,"<a href=""https://github.com/irfanurrehman""><img src=""https://avatars0.githubusercontent.com/u/10027921?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [irfanurrehman](https://github.com/irfanurrehman)**
_Wednesday Jul 26, 2017 at 16:16 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/49644_

----

The original issuse for feature ***federated pod autoscaler*** is [here](https://github.com/kubernetes/kubernetes/issues/38974), and the feature is being tracked [here](https://github.com/kubernetes/features/issues/257). This issue is to track and follow up pending items from the first phase of implementation especially items chosen to defer in the first phase of implementation [here](https://github.com/kubernetes/kubernetes/pull/45993).

1 - Make the replica nums offered (both min and max) and in turn increased or reduced from an existing cluster local hpa, configurable. Right now the value is kept to a pessimistic low of 1 at a time.
2 - Optimise the algorithm such that if there are clusters which need the replicas, they can first be taken from those clusters that have capacity to offer more.
3 - Implement hard limit based min/max preferences per cluster from the user.
4 - Contemplate usage of weighted distribution of replicas based on the weights applied on per cluster hpas by the user.
",closed,False,2017-10-30 15:21:35,2018-07-23 22:13:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/26,https://api.github.com/repos/kubernetes/federation/issues/26,Federated service DNS should be created for all zones,"<a href=""https://github.com/zihaoyu""><img src=""https://avatars1.githubusercontent.com/u/464299?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [zihaoyu](https://github.com/zihaoyu)**
_Friday Aug 11, 2017 at 17:06 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/50529_

----

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

If a federated cluster is a multi-zone cluster, then when a federated `LoadBalancer` type service is created, there is only one zone DNS record created. For example:

```
# This cluster spans three zones
➜  kubernetes git:(master) kubectl --context=hurley-beta-federation get cluster beta-us-west-2 -o json | jq '.status'
{
  ""conditions"": [
    {
      ""lastProbeTime"": ""2017-08-11T17:36:25Z"",
      ""lastTransitionTime"": ""2017-08-08T15:15:40Z"",
      ""message"": ""/healthz responded with ok"",
      ""reason"": ""ClusterReady"",
      ""status"": ""True"",
      ""type"": ""Ready""
    }
  ],
  ""region"": ""us-west-2"",
  ""zones"": [
    ""us-west-2a"",
    ""us-west-2b"",
    ""us-west-2c""
  ]
}

```

In Route53 console, I only see DNS records for one of the zones (`us-west-2a`). Same thing for `us-east-1` federated cluster too.

![route53](https://user-images.githubusercontent.com/464299/29223594-23ce6f38-7e95-11e7-96ee-464969e84e9c.png)

I also see the following in `kube-dns` logs when I tried to resolve `nginx.default.k8s-beta-federation` locally:

```
I0810 20:31:17.072874       1 dns.go:636] Federation: skipping record since service has no endpoint: {10.5.35.160 0 10 10  false 30 0  /skydns/local/cluster/svc/default/nginx/6466336132663632}
I0810 20:31:17.147368       1 logs.go:41] skydns: incomplete CNAME chain from ""nginx.default.my-federation.svc.us-west-2b.us-west-2.mycompany.com."": rcode 3 is not equal to success
```

**What you expected to happen**:

DNS records should be created for each zone of the cluster.

**How to reproduce it (as minimally and precisely as possible)**:

1. Launch a multi-zone cluster.
2. Join this cluster to a federation.
3. Create a federated service with `LoadBalancer` type.

**Anything else we need to know?**:

It looks like, as of v1.7.3, this behavior is [by design](https://github.com/kubernetes/kubernetes/blob/v1.7.3/federation/pkg/federation-controller/service/dns/dns.go#L510). I think we should address that `TODO` and add support for multi-zone clusters.

**Environment**:
- Kubernetes version (use `kubectl version`): `Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.2"", GitCommit:""922a86cfcd65915a9b2f69f3f193b8907d741d9c"", GitTreeState:""clean"", BuildDate:""2017-07-21T08:08:00Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration**: AWS
- OS (e.g. from /etc/os-release): `Container Linux by CoreOS 1353.7.0 (Ladybug)`
- Kernel (e.g. `uname -a`): `Linux ip-10-72-230-146.ec2.internal 4.9.24-coreos #1 SMP Wed Apr 26 21:44:23 UTC 2017 x86_64 Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz GenuineIntel GNU/Linux`
- Install tools: `Custom tool`
- Others:

",closed,False,2017-10-30 15:21:43,2018-07-23 18:09:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/27,https://api.github.com/repos/kubernetes/federation/issues/27,kubefed should use an API version that both it and server uses,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Friday Aug 11, 2017 at 23:57 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/50540_

----

Forked from https://github.com/kubernetes/kubernetes/issues/50534#issuecomment-321940562.

kubefed does API version discovery and then uses the preferred API group version that server supports. This breaks when server supports a newer version that the generated clientset that kubefed uses does not know about.
kubefed should choose the version that both server and it knows about.

https://github.com/kubernetes/kubernetes/pull/50537 fixes this for RBAC. We need the same for all API resources that kubefed creates.

cc @kubernetes/sig-federation-bugs @liggitt 
",closed,False,2017-10-30 15:21:55,2018-07-23 18:09:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/28,https://api.github.com/repos/kubernetes/federation/issues/28,Secure etcd for `kubefed init`,"<a href=""https://github.com/gyliu513""><img src=""https://avatars1.githubusercontent.com/u/4461983?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [gyliu513](https://github.com/gyliu513)**
_Wednesday Aug 16, 2017 at 06:08 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/50734_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug

/kind feature


**What happened**:
The `kubefed init` will install etcd, but the etcd is not secured, we should enable secure mode for etcd, checkout here for how to enable tls for etcd https://coreos.com/etcd/docs/latest/op-guide/security.html

```
      --api-server-advertise-address string      Preferred address to advertise api server nodeport service. Valid only if 'api-server-service-type=NodePort'.
      --api-server-service-type string           The type of service to create for federation API server. Options: 'LoadBalancer' (default), 'NodePort'. (default ""LoadBalancer"")
      --apiserver-arg-overrides string           comma separated list of federation-apiserver arguments to override: Example ""--arg1=value1,--arg2=value2...""
      --apiserver-enable-basic-auth              Enables HTTP Basic authentication for the federation-apiserver. Defaults to false.
      --apiserver-enable-token-auth              Enables token authentication for the federation-apiserver. Defaults to false.
      --controllermanager-arg-overrides string   comma separated list of federation-controller-manager arguments to override: Example ""--arg1=value1,--arg2=value2...""
      --dns-provider string                      Dns provider to be used for this deployment.
      --dns-provider-config string               Config file path on local file system for configuring DNS provider.
      --dns-zone-name string                     DNS suffix for this federation. Federated Service DNS names are published with this suffix.
      --dry-run                                  dry run without sending commands to server.
      --etcd-image string                        Image to use for etcd server. (default ""gcr.io/google_containers/etcd:3.0.17"")
      --etcd-persistent-storage                  Use persistent volume for etcd. Defaults to 'true'. (default true)
      --etcd-pv-capacity string                  Size of persistent volume claim to be used for etcd. (default ""10Gi"")
      --etcd-pv-storage-class string             The storage class of the persistent volume claim used for etcd.   Must be provided if a default storage class is not enabled for the host cluster.
      --federation-system-namespace string       Namespace in the host cluster where the federation system components are installed (default ""federation-system"")
      --host-cluster-context string              Host cluster context
      --image string                             Image to use for federation API server and controller manager binaries. (default ""gcr.io/google_containers/hyperkube-amd64:v0.0.0-master+$Format:%h$"")
      --kubeconfig string                        Path to the kubeconfig file to use for CLI requests.
```
/sig federation
/cc @kubernetes/sig-federation-feature-requests 

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration**:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:

",closed,False,2017-10-30 15:22:05,2018-07-23 18:09:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/29,https://api.github.com/repos/kubernetes/federation/issues/29,Update e2e_federation with specific sig prefix,"<a href=""https://github.com/guangxuli""><img src=""https://avatars2.githubusercontent.com/u/17472298?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [guangxuli](https://github.com/guangxuli)**
_Wednesday Aug 16, 2017 at 06:11 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/50735_

----

**Is this a BUG REPORT or FEATURE REQUEST?**:

Ref to https://github.com/kubernetes/kubernetes/issues/49161, e2e_federation still test use `k8s.io` prefix. Not sure if we could use `sig-federation` instead of `k8s.io`  just as https://github.com/kubernetes/kubernetes/issues/49161 described.

 i would be happy to send a PR to update it if we indeed need.

/cc @quinton-hoole @csbell 

",closed,False,2017-10-30 15:22:20,2018-07-24 00:15:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/30,https://api.github.com/repos/kubernetes/federation/issues/30,Enable federation apiserver connect to external etcd,"<a href=""https://github.com/gyliu513""><img src=""https://avatars1.githubusercontent.com/u/4461983?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [gyliu513](https://github.com/gyliu513)**
_Wednesday Aug 16, 2017 at 06:15 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/50737_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug

/kind feature


**What happened**:
If I want to enable the federation control panel, then I need to use `kubefed init` to install etcd and control panel. For etcd, as the kubernetes already have etcd running, so I want to re-use the existing etcd in my kubernetes cluster and do not want to install a new etcd for federation only.

**What you expected to happen**:
I can specify external etcd when using `kubefed init`.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration**:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:

/sig federation

/cc @kubernetes/sig-federation-feature-requests 

",closed,False,2017-10-30 15:22:50,2018-07-23 23:14:23
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/31,https://api.github.com/repos/kubernetes/federation/issues/31,Kubefed init should not fail if resource are already existed,"<a href=""https://github.com/liqlin2015""><img src=""https://avatars3.githubusercontent.com/u/10703772?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [liqlin2015](https://github.com/liqlin2015)**
_Wednesday Aug 16, 2017 at 07:40 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/50746_

----


**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug


**What happened**:
When I run `kubefed init` to deploy federation control plane in host cluster, some error happened. After I fixed the error and run `kubefed init` again, it throws error like ""Service account already exist"" or ""federation-system namespace already exist"".

**What you expected to happen**:
`kubefed init` should continue if related resource is already existed in host cluster. Or we expect a `--force` flag to force overwrite previous setting of federation.

**How to reproduce it (as minimally and precisely as possible)**:
1. Create federation-system namespace in host cluster.
2. Run `kubefed init`.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration**:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:

",closed,False,2017-10-30 15:23:06,2018-07-13 19:15:14
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/32,https://api.github.com/repos/kubernetes/federation/issues/32,Enable coredns etcd secure mode,"<a href=""https://github.com/gyliu513""><img src=""https://avatars1.githubusercontent.com/u/4461983?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [gyliu513](https://github.com/gyliu513)**
_Wednesday Aug 16, 2017 at 09:41 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/50761_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug

/kind feature


**What happened**:
The kubernetes federation has a provider of coreos, we cannot enable secure mode for etcd, it is better support secure mode for etcd.

```
[root@cfc-555 federation]# cat coredns-provider.conf
[Global]
etcd-endpoints = http://etcd-cluster.default:2379 
zones = example.com.
coredns-endpoints = coredns-coredns:53
```

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration**:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:

/sig federation

/cc @kubernetes/sig-federation-feature-requests 

",closed,False,2017-10-30 15:23:17,2018-02-05 14:21:17
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/33,https://api.github.com/repos/kubernetes/federation/issues/33,Enable `kubefed init` support `tolerations`,"<a href=""https://github.com/gyliu513""><img src=""https://avatars1.githubusercontent.com/u/4461983?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [gyliu513](https://github.com/gyliu513)**
_Thursday Aug 17, 2017 at 02:15 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/50815_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug

/kind feature


**What happened**:
The `kubefed init` can not specify `tolerations`, this will cause that I cannot deploy my federation panel on node which has `taint`, it is better to enable the `kubefed init` can specify the `tolerations` so that I can deploy federation panel on node which have `taint` that the federation panel can tolerate.

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration**:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:

/sig federation

/cc @kubernetes/sig-federation-feature-requests 
",closed,False,2017-10-30 15:23:30,2018-07-23 23:14:22
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/34,https://api.github.com/repos/kubernetes/federation/issues/34,Enable `kubefed init` accept a file to set up the control panel,"<a href=""https://github.com/gyliu513""><img src=""https://avatars1.githubusercontent.com/u/4461983?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [gyliu513](https://github.com/gyliu513)**
_Thursday Aug 17, 2017 at 02:28 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/50817_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug

/kind feature


**What happened**:
Now I need to use a complex command to for `kubefed init` to set up the federation control panel as following:

```
kubefed init federation-cluster \
     --host-cluster-context=host-context \
     --dns-provider=""coredns"" \
     --dns-zone-name=""example.com."" \
     --dns-provider-config=""/root/federation/coredns-provider.conf"" \
     --api-server-service-type=""NodePort"" \
     --api-server-advertise-address=""x.x.x.x"" \
     --etcd-image=""library/etcd:v3.1.5"" \
     --etcd-persistent-storage=false \
     --etcd-pv-capacity=""10Gi"" \
     --etcd-pv-storage-class="""" \
     --federation-system-namespace=""federation-system"" \
     --image=""kubernetes:v1.7.3"" \
     --apiserver-arg-overrides="""" \
     --controllermanager-arg-overrides=""""
```

It would be great to enable `kubefed init` can accept a file just like what we do for `kubectl apply -f <filename>`, so that I can put all parameters in a file which is easy to maintain.

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration**:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:

/sig federation

/cc @kubernetes/sig-federation-feature-requests
",closed,False,2017-10-30 15:23:47,2018-07-13 19:15:14
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/35,https://api.github.com/repos/kubernetes/federation/issues/35,Federation: Support federated ingress outside of GCP,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Friday Sep 01, 2017 at 16:38 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/51806_

----

/kind feature

Splitting off the subthread in https://github.com/kubernetes/kubernetes/issues/39989#issuecomment-305356470

This looks like it might have just got easier on AWS and cross-cloud:

[New – Application Load Balancing via IP Address to AWS & On-Premises Resources](https://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/)

I haven't gone through all the details yet but this seems like it might form a good base for cross-cloud load balancing.  As usual, the devil will be in the details.

",closed,False,2017-10-30 15:24:01,2018-07-23 18:09:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/36,https://api.github.com/repos/kubernetes/federation/issues/36,Proposal: Service connection affinity,"<a href=""https://github.com/eliaslevy""><img src=""https://avatars1.githubusercontent.com/u/725218?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [eliaslevy](https://github.com/eliaslevy)**
_Wednesday Oct 14, 2015 at 23:34 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/15675_

----

As of now connections to Services are either handled in a round robin fashion (the default) or can make use of client IP based affinity, where the endpoint for the initial connection to the service by a client is selected in a round robin fashion and subsequent connections will attempt to use the previously selected endpoint.

In some environments there is a need for a topographical or administrative prioritization of the Service endpoint selection.  For instance, a cluster may be comprised of multiple zones with Service endpoints spread across zones.  Latencies across zones may be low, yet still higher than intra-zone latencies.  There may also be financial costs associated to inter-zone traffic.  Thus, it is could be desirable for a number of reasons to prioritize endpoints for a Service within the same zone as that of Pods making connections to the Service.

Note that this proposal is distinct from that of #14484. #14484 proposes priority affinity when scheduling Pods (colocation).  I am proposing priority affinity of network connections to Services.

This could be implemented allowing the admin to specify a tag selector within the session affinity spec.  kube-proxy could then look up the tag on the Pod that initiated a connection and on any Pods that are endpoints for the Service.  It would prioritize endpoint Pods that a tag with a value that matches the value of the tag on the client Pod.  Partial matches be prioritized by the number of tags that match.

If multiple endpoints match at the same priority, then the selection of one of them can be performed in a round robin fashion, or ClientIP affinity could be applied optionally.

",closed,False,2017-10-30 15:42:42,2017-11-27 13:07:43
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/37,https://api.github.com/repos/kubernetes/federation/issues/37,How do admins of non-cloud nodes apply node zone labels for Kubernetes Lite?,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Friday Nov 20, 2015 at 18:31 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/17575_

----

https://github.com/kubernetes/kubernetes/pull/16866#issuecomment-158125456 refers, copied here:

erictune commented:
We are going to want to have kubelet-computed labels, as well as labels that a user puts into a file on the kubelet and the kubelet automatically syncs to the node. We should think about how those two sets of labels interact.

Specifically, if a user is running on bare metal, and the user does not have a cloud provider to get the zone info from, the user are going to want to set the zone label via the file method.

Should the user should also use the failure-domain.kubernetes.io/zone label in this case?

Or should they use example.com/custom-zone, and then adjust the scheduler to somehow spread on this label instead?

I think this depends on whether the domain prefix means who owns the code that generated the label value, or if it means who owns the meaning of the label.

If we allow kubernetes.io/ domain labels in the file, then we need to decide which takes precedence, the file or the cloud provider. If we don't allow kubernetes.io in the file, then (maybe) we do not have to think about precedence.

",closed,False,2017-10-30 15:43:13,2018-07-24 00:15:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/38,https://api.github.com/repos/kubernetes/federation/issues/38,HA Kubernetes etcd configuration,"<a href=""https://github.com/justinsb""><img src=""https://avatars2.githubusercontent.com/u/100893?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [justinsb](https://github.com/justinsb)**
_Saturday Jan 09, 2016 at 16:48 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/19443_

----

I'm looking at setting up k8s with a HA master, and following along with the HA documentation at http://kubernetes.io/v1.1/docs/admin/high-availability.html

The k8s HA doc suggests using a discovery token.  However, the etcd documentation at https://github.com/coreos/etcd/blob/master/Documentation/clustering.md says:

> Moreover, discovery URLs should ONLY be used for the initial bootstrapping of a cluster. To change cluster membership after the cluster is already running, see the runtime reconfiguration guide.

I was thinking of running the masters in an AWS auto-scaling group, but this means that their IP addresses would change (I have some strategies involving grabbing disks to ensure that they have some notion of persistent identity though).

An alternative idea I had was to use static config, but to have a process that queried the cloud API, and located the instances, and then dynamically updated a hosts file with the IP addresses.  So for example etcd would be launched with

```
etcd -name node1 -initial-advertise-peer-urls http://10.0.1.10:2380 \
  -listen-peer-urls http://10.0.1.10:2380 \
  -listen-client-urls http://10.0.1.10:2379 \
  -advertise-client-urls http://10.0.1.10:2379 \
  -initial-cluster-token etcd-cluster-1 \
  -initial-cluster node1=http://node1:2380,node2=http://node2:2380,node3=http://node3:2380 \
  -initial-cluster-state new
```

And then the hosts file would look like

```
10.0.1.10 node1 
10.0.1.11 node2
# node3 not yet running
```

(I guess I could also use skydns rather than hosts files, but I am not sure about the bootstrapping there.  Or even Route53...)

Will discovery actually work and 'self-heal' as IP addresses change?  Is there a better alternative to dynamically updating DNS / host files?  @xiang90 I think you would know best here.

Also cc @quinton-hoole because I think you'll find this interesting...

",closed,False,2017-10-30 15:44:01,2018-07-23 21:12:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/39,https://api.github.com/repos/kubernetes/federation/issues/39,Zone-spread HA master for Ubernetes Lite,"<a href=""https://github.com/davidopp""><img src=""https://avatars2.githubusercontent.com/u/8264113?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [davidopp](https://github.com/davidopp)**
_Friday Feb 12, 2016 at 07:21 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/21124_

----

The current incarnation of Ubernetes Lite makes the application tolerant to zone outages but does not make the Kubernetes control plane tolerant to zone outages since it doesn't use (zone-distributed) HA master. For a short or medium duration zone outage this is totally fine because the containers will continue running even if the master goes away, but if the zone where the master was running goes out for a long time, eventually there will be problems (since pods from failed nodes won't reschedule, users can't update the containers, etc.). 

It would be good to do a zone distributed HA master for Ubernetes Lite, with at least one etcd replica per zone, at least one API server per zone, and the master-elected components like scheduler and controller-manager distributed across the zones.

cc/ @quinton-hoole @justinsb 

ref/ #17059

",closed,False,2017-10-30 15:45:01,2018-07-23 20:11:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/40,https://api.github.com/repos/kubernetes/federation/issues/40,Prioritized pods (ubernetes),"<a href=""https://github.com/juanjoperl""><img src=""https://avatars2.githubusercontent.com/u/17222177?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [juanjoperl](https://github.com/juanjoperl)**
_Monday Feb 22, 2016 at 20:35 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/21705_

----

For a possible workaround to the problem of federation, can kubernetes have this behabour?: the ability the services have several sets of labels to select pods. The first set would be the priority, the second come into play if the priority pods do not respond, and so on.
With this capability, could have pods tagged area, and the services have sorted priority areas

",closed,False,2017-10-30 15:45:20,2018-07-23 20:11:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/41,https://api.github.com/repos/kubernetes/federation/issues/41,ubernetes: How to collect cluster resource metrics,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Wednesday Mar 30, 2016 at 01:49 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/23614_

----

Forked from https://github.com/kubernetes/kubernetes/pull/23430#issuecomment-202768265

The ubernetes scheduler needs to know the available resources in each cluster to be able to divide resources (such as replicasets) appropriately.

As per https://github.com/kubernetes/kubernetes/blob/master/docs/design/federation-phase-1.md#cluster:

> In phase one it only contains available CPU resources and memory resources. The cluster controller will periodically poll the underlying cluster API Server to get cluster capability. In phase one it gets the metrics by simply aggregating metrics from all nodes. In future we will improve this with more efficient ways like leveraging heapster, and also more metrics will be supported.

There is also a proposal to provide cluster level metrics via apiserver: #23376, which we can start using whenever it is available.

cc @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 15:45:50,2018-07-24 00:15:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/42,https://api.github.com/repos/kubernetes/federation/issues/42,Refactor kubernetes scheduler and controller manager to enable code reuse,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Friday Apr 29, 2016 at 20:06 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/24996_

----

Forked from https://github.com/kubernetes/kubernetes/pull/24038#issuecomment-215610094

We are adding federation scheduler and controller manager as per the plan in https://github.com/kubernetes/kubernetes/issues/23653.

There is obviously some common code that is shared with kubernetes controller manager and kubernetes scheduler.
We want to pull out that code into libraries to enable code reuse rather than having to duplicate code.

We need to figure out the right name and directory structure for such libraries.

cc @davidopp @madhusudancs @kubernetes/goog-control-plane @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 15:46:59,2018-02-05 12:31:31
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/43,https://api.github.com/repos/kubernetes/federation/issues/43,Surface kubectl clusters in kubectl help when talking to federation apiserver,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Friday May 13, 2016 at 21:26 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/25592_

----

From https://github.com/kubernetes/kubernetes/pull/24016#issuecomment-218585269:

> We also want kubectl get clusters to come up in kubectl help, but it should only show up when kubectl is talking to a federation apiserver.
> We can do that by configuring the output based on discovery API from apiserver rather than being a hardcoded list in kubectl.

Will be great to have this. Not critical for 1.3

Ref https://github.com/kubernetes/kubernetes/issues/23653

cc @jianhuiz @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 15:47:45,2018-07-23 20:11:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/44,https://api.github.com/repos/kubernetes/federation/issues/44,Solution discussion for rebuilding federation service controller cache on restart,"<a href=""https://github.com/mfanjie""><img src=""https://avatars1.githubusercontent.com/u/16643108?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mfanjie](https://github.com/mfanjie)**
_Thursday May 26, 2016 at 02:51 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/26329_

----

I am working on federation service controller, and there is a limitation on current cache, after had a call with @quinton-hoole, as it's not a common case, we will leave it as is in v1.3, and I open this issue to track the problem and to find out a more robust solution.

**Federation service controller behavior description:**
1. service controller watch federation apiserver for new services, create service map in the form of
`fedServiceMap: [serviceName]cachedService: {service info, endpointMap: [clusterName]int, serviceStatusMap: [clusterName]api.LoadBalancerStatus}`
endpointMap and serviceStatusMap have no element at this moment.
2. service controller create the service on all healthy clusters
3. service controller watches all healthy clusters for services and endpoints.
When new serviceStatus with LB info is returned, add it to serviceStatusMap and service info and update federation apiserver with the new LB info.
4. when new endpoint info returned, add it to endpoint map with flag 1 which indicates there is reachable endpoints

**Problem description:**
From above behavior description, we can see that serviceStatusMap is the only place to link clusterName and LB info, it is not persisted anywhere and it will go on service controller down.

So if during service controller down time, the service on any clusters is deleted, and after federation service controller being starts, the service will be rebuild on that cluster again, with different LB info, and as we lost the link between cluster and LB info, we do not know which old LB info in federation service was returned by from that cluster, as a result, we can add the new IP to federation service, but there is no way to remove the old one.

**One typical example**
Federation service:S1
Clusters: C1, C2
after S1 is created, it will be created in C1 and C2, with LB IP1, and LB IP2 assigned.
So after the a while S1 will changed to S1[LB:{IP1, IP2}]
serviceCache is same S1[LB:{IP1, IP2}]
serviceStatusMap [C1]S1[LB:IP1], [C2]S1[LB:IP2] , most import one, it links Ingress IP and clusters.
endpointMap [C1]1,[C2]1 

For the case that LB IP in C1 changed from IP1 to IP3
If we have the cache info, we know IP1 is from C1 previously, so we can remove it and add IP3.
But If Service Controller is restarted, we will lose the old LB IP info, we do not know which IP we should remove from Federation IP, and the ingress info will be changed to S1[LB:{IP1, IP2, IP3}]

",closed,False,2017-10-30 15:48:09,2018-02-05 12:37:07
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/45,https://api.github.com/repos/kubernetes/federation/issues/45,Integrate federated service DNS record management: Address remaining code review comments.,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Wednesday Jun 01, 2016 at 23:52 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/26669_

----

See unresolved comments in #25991 for details.

In particular: 
- Possibly return ErrNotFound rather than empty slice from getRrsets()

",closed,False,2017-10-30 15:48:21,2018-07-23 18:09:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/46,https://api.github.com/repos/kubernetes/federation/issues/46,include federation e2e in merge bot testing,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Thursday Jun 02, 2016 at 17:57 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/26723_

----

We should include federation e2e in merge bot testing, so that we can catch PRs that break the test.
We need to ensure that it is 100% green before doing this.

@colhom Can you take this up?

cc @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 15:48:51,2018-02-02 05:13:01
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/47,https://api.github.com/repos/kubernetes/federation/issues/47,Federation-e2e flake - federation-apiserver connection refused,"<a href=""https://github.com/colhom""><img src=""https://avatars0.githubusercontent.com/u/1075028?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [colhom](https://github.com/colhom)**
_Thursday Jun 02, 2016 at 18:16 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/26725_

----

The kubernetes-e2e-gce-federation jenkins jobs in consistently flaking with the following output.

```
â€¢ Failure [5.905 seconds]
[k8s.io] Federation apiserver [Feature:Federation]
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:641
  should allow creation of cluster api objects [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/federation-apiserver.go:63

  creating cluster: Post https://146.148.36.68:443/apis/federation/v1alpha1/clusters: dial tcp 146.148.36.68:443: getsockopt: connection refused
  Expected error:
      <*url.Error | 0xc8209cc6f0>: {
          Op: ""Post"",
          URL: ""https://146.148.36.68:443/apis/federation/v1alpha1/clusters"",
          Err: {
              Op: ""dial"",
              Net: ""tcp"",
              Source: nil,
              Addr: {
                  IP: ""\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xff\xff\x92\x94$D"",
                  Port: 443,
                  Zone: """",
              },
              Err: {
                  Syscall: ""getsockopt"",
                  Err: 0x6f,
              },
          },
      }
      Post https://146.148.36.68:443/apis/federation/v1alpha1/clusters: dial tcp 146.148.36.68:443: getsockopt: connection refused
  not to have occurred

  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/federation-apiserver.go:53
------------------------------
```

This is kubectl attempting to open a connection with the federation-apiserver via a loadbalancer service.

There are two possible underlying causes here, which are not mutually exclusive:
- federation-apiserver pod is not yet ready
- GCE loadbalancer has not finished initializing

I plan to remedy this by introducing some more ""is it ready yet?"" logic to `federated-up` procedure:
- wait until the federation-apiserver pod stays ready for 10 seconds. right now we only wait until it's running
- attempt to do `kubectl version` for another 30 seconds before allowing the e2e tests to start, to verify loadbalancer is initialized. right now we assume the loadbalancer is ready to serve traffic after the service ingress metadata becomes available. this is most likely a poor assumption

\cc @quinton-hoole @nikhiljindal @madhusudancs 

",closed,False,2017-10-30 15:49:42,2018-02-05 12:40:35
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/48,https://api.github.com/repos/kubernetes/federation/issues/48,Address outstanding DNS review comments in #26694,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Monday Jun 06, 2016 at 23:42 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/26921_

----

See https://github.com/kubernetes/kubernetes/pull/26694 for details.  Specifically:
1. Don't call ensureDnsRecords() if the DNS provider has not been initialized.
2. Don't discard errors returned by getClusterZoneNames()

cc: @mfanjie FYI

",closed,False,2017-10-30 15:49:59,2018-02-05 12:13:39
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/49,https://api.github.com/repos/kubernetes/federation/issues/49,kubernetes-e2e-gce-ubernetes-lite should verify the cluster created is actually multi-zone,"<a href=""https://github.com/jlowdermilk""><img src=""https://avatars0.githubusercontent.com/u/2101035?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [jlowdermilk](https://github.com/jlowdermilk)**
_Tuesday Jun 14, 2016 at 17:04 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/27372_

----

Filing separate follow up from #27150. The issue should have been caught by http://kubekins.dls.corp.google.com/job/kubernetes-e2e-gce-ubernetes-lite/ failing. The suite needs a specific feature test that e.g. looks at node labels to verify multiple zones are present.

cc @quinton-hoole 

",closed,False,2017-10-30 15:50:27,2018-07-23 20:11:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/50,https://api.github.com/repos/kubernetes/federation/issues/50,Expand petset volume zone spreading ,"<a href=""https://github.com/bprashanth""><img src=""https://avatars2.githubusercontent.com/u/10927820?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [bprashanth](https://github.com/bprashanth)**
_Tuesday Jun 21, 2016 at 22:28 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/27809_

----

We got petset disk zone spreading in and it's really useful. However we left a couple of todos to follow up on:
1. Don't embed a zone scheduler in the pv provisioner (https://github.com/kubernetes/kubernetes/pull/27553/files#diff-b3d75e3586a2c9a5140cd549861da9c0R2094)
2. Write a unitest that protects the zone spreding from petset implementation changes (https://github.com/kubernetes/kubernetes/pull/27553#discussion_r67625074)
3. Maybe a multi-az e2e with petset before it goes beta?

@justinsb 

",closed,False,2017-10-30 15:50:47,2018-07-23 23:14:25
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/51,https://api.github.com/repos/kubernetes/federation/issues/51,Refactor the getClusterZones implementations.,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Thursday Jun 23, 2016 at 19:31 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/27966_

----

There are two implementations to get the cluster zone and regions. There is one here - https://github.com/kubernetes/kubernetes/blob/6dde087f69b2bdd5f9191edbd572985e00f8c808/federation/pkg/federation-controller/cluster/cluster_client.go#L143 and the other here - https://github.com/kubernetes/kubernetes/blob/c87b61341241bae37017db5f76902ea2642ea169/pkg/dns/dns.go#L630. The two implementations must be refactored and merged.

cc @quinton-hoole @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 15:51:30,2018-02-05 13:45:58
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/52,https://api.github.com/repos/kubernetes/federation/issues/52,federation: non-federation DNS lookup can return federation CNAME,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Thursday Jun 23, 2016 at 20:15 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/27969_

----

With the current KubeDNS code, it can happen that a pod requests `mysvc.somens` and resolve.conf adds `myns.svc.cluster.local` as a search path so that KubeDNS gets `mysvc.somns.myns.svc.cluster.local`.
If there is a federation with the name `myns`, isFederationQuery in KubeDNS will think this is a federation query and try to resolve `mysvc.somens.svc.cluster.local`. If there is a local service resolving to that DNS, then user will get that as expected (all is fine). But if there isnt, then KubeDNS will return `mysvc.somens.myns.svc.myzone.myregion.mydomain`. If there is no federation service with that name and in that namespace, then user will still get an NXDOMAIN as expected. But if there is, then user might be pointed to a service in another cluster, which user may or may not have wanted.

We can consider this a feature, if users want it or should fix it, if we consider it a bug.

Filing this to keep track.

@kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 15:51:41,2018-07-23 19:10:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/53,https://api.github.com/repos/kubernetes/federation/issues/53,Don't require a trailing '.' in federation-controller DNS zone,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Friday Jul 01, 2016 at 04:40 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/28335_

----

See https://github.com/kubernetes/kubernetes/issues/28324

",closed,False,2017-10-30 16:19:10,2018-05-14 23:48:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/54,https://api.github.com/repos/kubernetes/federation/issues/54,session affinity services support in federation,"<a href=""https://github.com/mfanjie""><img src=""https://avatars1.githubusercontent.com/u/16643108?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mfanjie](https://github.com/mfanjie)**
_Thursday Jun 30, 2016 at 09:05 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/28276_

----

@quinton-hoole @nikhiljindal 

One thing need to discuss with you about k8s services.
in single k8s cluster when users define service.spec.sessionAffinity=ClientIP, then the service will be session affinity, means if there are multiple calls from single clientIP, those calls will be redirected to the same pod, so the session context could be reused.

Per my understanding, in k8s, for the case that iptables is enabled, when a service being created, the iptable entry will be write to all nodes, so it is easy to find the target pod and redirect to it.

But if this concept is put to federation level, and if the global dns server is round robin, the service might be redirected to different cluster even sessionAffinity is specified. As a result, the service lost the sessionAffinity capability.

Any comments?

",closed,False,2017-10-30 17:53:58,2018-07-23 19:10:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/55,https://api.github.com/repos/kubernetes/federation/issues/55,Federation users shouldn't be required to have go compiler toolchain/libraries installed,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Tuesday Jul 12, 2016 at 02:29 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/28812_

----

It is unfortunate that our federation turn up scripts run the manifests through a `go` program which isn't shipped as a binary in our release, but instead as a source. This means that our users are required to have the `go` toolchain installed and configured to even try out federation. We should fix this.

It is also quite amusing that e2es did not catch this. Do we have `go` toolchain installed on the Jenkins machine that deploys the test clusters?

cc @colhom @ixdy @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 17:58:16,2018-02-05 13:56:33
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/56,https://api.github.com/repos/kubernetes/federation/issues/56,Ubernetes lite in GCE does not allow to change node configuration,"<a href=""https://github.com/fgrzadkowski""><img src=""https://avatars0.githubusercontent.com/u/10820726?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [fgrzadkowski](https://github.com/fgrzadkowski)**
_Wednesday Jul 13, 2016 at 12:11 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/28894_

----

Currently, when using flag `KUBE_USE_EXISTING_MASTER=true` in kube-up.sh, we will simple recreate nodes in a new zone, but we will use the same template. This makes it impossible to use different machine specification, e.g. use SSD or bigger machines. This defeats one of the main reasons why we have introduced ubernetes lite in the first place.

AFAIU it works on AWS.

Am I missing something or is it a serious gap in our multi-zone cluster offering in GCE?

@kubernetes/goog-control-plane @quinton-hoole @davidopp @matchstick 

",closed,False,2017-10-30 17:58:54,2018-07-24 00:15:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/57,https://api.github.com/repos/kubernetes/federation/issues/57,Federation: e2e test missing: Intermittently uncontactable clusters,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Wednesday Jul 20, 2016 at 23:00 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/29331_

----

We need to add an e2e test to cover the case of clusters disconnecting and reconnecting.  Cluster status should be correctly reflected in API, and controllers should respond appropriately.

",closed,False,2017-10-30 17:59:07,2018-07-21 16:20:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/58,https://api.github.com/repos/kubernetes/federation/issues/58,Federation: Cloud resources (DNS records) need to be cleaned up in CI environments.,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Wednesday Jul 20, 2016 at 23:16 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/29335_

----

1. We don't generally want to delete the DNS records created by the Federated Service Controller when it terminates, as this might break the clients of those federated services in real-world scenarios (e.g. upgrades of the federation control plane).
2. But we do need an easy way to delete all of the records created in some cases (e.g. after a CI test run).

Currently there are over 4,000 DNS records in left over in the project that our CI tests run in.  We will presumably eventually hit some sort of quota limit there.

A few options available.
1. write an independent tool which can delete all of the DNS records in a given DNS zone, and invoke that automatically after our CI test runs.
2. systematically delete all of the created federated services as part of our tests, and ensure that the service controller deletes all of the DNS records when that happens.
   ...

",closed,False,2017-10-30 17:59:15,2018-07-23 23:14:26
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/59,https://api.github.com/repos/kubernetes/federation/issues/59,Federation: Write a brief developer guide for adding a new Federated API object,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Wednesday Jul 20, 2016 at 23:23 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/29340_

----


",closed,False,2017-10-30 17:59:28,2018-07-23 19:10:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/60,https://api.github.com/repos/kubernetes/federation/issues/60,Federation: Services: Plan for going GA,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Thursday Jul 21, 2016 at 00:04 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/29348_

----

Enumerate blockers for taking Federated Services to General Availability.

Resolve the highest priority items as soon as is practical.

",closed,False,2017-10-30 17:59:39,2018-07-23 19:10:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/61,https://api.github.com/repos/kubernetes/federation/issues/61,Make federation DNS TTL configurable,"<a href=""https://github.com/saturnism""><img src=""https://avatars1.githubusercontent.com/u/1998883?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [saturnism](https://github.com/saturnism)**
_Thursday Jul 28, 2016 at 06:16 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/29723_

----

According to doc:
http://kubernetes.io/docs/user-guide/federation/federated-services/#handling-failures-of-backend-pods-and-whole-clusters
""Due to the latency inherent in DNS caching (the cache timeout, or TTL for Federated Service DNS records is configured to 3 minutes, by default, but can be adjusted)""

Currently, the federation DNS TTL seems to be fixed at 180 seconds. What's the right way to adjust it?

",closed,False,2017-10-30 17:59:55,2018-07-24 01:16:23
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/62,https://api.github.com/repos/kubernetes/federation/issues/62,Federation e2e tests must clean up dynamically provisioned PDs while tearing down the clusters.,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Monday Aug 01, 2016 at 20:08 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/29875_

----

The `federation-apiserver`'s etcd is backed by a PVC that dynamically provisions the underlying persistent disk for its PV. We are not cleaning this up when we tear down the federation control plane and the corresponding test clusters. This caused us to run out of quota and the tests started failing.

We should delete the dynamically provisioned persistent disks at the end of the e2e tests, while tearing the clusters down.

@colhom are you seeing this behavior in your test clusters in AWS too?

cc @kubernetes/sig-cluster-federation @mml 

",closed,False,2017-10-30 18:00:07,2018-02-05 13:32:10
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/63,https://api.github.com/repos/kubernetes/federation/issues/63,Make the kubeconfig context for federation API server configurable.,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Wednesday Aug 10, 2016 at 20:14 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/30394_

----

The kubeconfig context for the federation API server is now hard coded to `federation-cluster`. We should make this user-configurable with default value set to `federation`.

cc @kubernetes/sig-cluster-federation @quinton-hoole 

",closed,False,2017-10-30 18:00:15,2018-07-23 19:10:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/64,https://api.github.com/repos/kubernetes/federation/issues/64,Move failure domain annotations from Alpha through G.A.,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Wednesday Aug 10, 2016 at 20:49 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/30398_

----

cc @thockin @davidopp @bgrant0607 @justinsb FYI.

=== email thread below explains details ===

On Wed, Aug 10, 2016 at 7:36 AM, Quinton Hoole quinton@google.com wrote:
I believe I'm the guilty party here. Justin SB and I decided to make them alpha in v1.2 because we didn't solve the general topology problem that Tim mentions, but needed the zone and region labels for multi-zone clusters. And then we never went back to finalize it.

Tim, you are absolutely right, we need to think this through properly, and promote whatever the outcome is through beta and GA within a reasonable time. I suggest we add the move to beta to our v1.5 plans. I can sign up for that.

On Aug 9, 2016 10:52 PM, ""Tim Hockin"" thockin@google.com wrote:
I don't want to derail anything.  I was not trying to demand unplanned work ;)

On Tue, Aug 9, 2016 at 10:49 PM, David Oppenheimer davidopp@google.com wrote:

> I wasn't complaining about your question (or disagreeing with your saying
> that it's half-finished) -- just wanted to understand what specifically you
> were suggesting for 1.4 (if anything).
> 
> I'm totally fine with moving them to GA and doing a more flexible topology
> approach later. Moving these labels to GA doesn't prevent us from inserting
> more labels above, below, or in between the ones we have now (e.g. adding
> rack). So we can potentially put off the more flexible thing for quite some
> time.
> 
> On Tue, Aug 9, 2016 at 10:34 PM, Tim Hockin thockin@google.com wrote:
> 
> > I was simply asking what the plan was.  I hate half-done things with
> > no plan.  The topology annotations are in-use and fairly visible.  I
> > wanted to know if we had a plan for finishing.  We don't and that is
> > OK.
> > 
> > On Tue, Aug 9, 2016 at 10:30 PM, David Oppenheimer davidopp@google.com
> > wrote:
> > 
> > > Tim, are you saying we should move these to GA for 1.4? Or beta is
> > > sufficient? In theory it's just a two line change either way, but I want
> > > to
> > > make sure I understand what you're suggesting.
> > > 
> > > On Tue, Aug 9, 2016 at 10:03 PM, Tim Hockin thockin@google.com wrote:
> > > 
> > > > That's fine, it's just one more thing that we have started but not
> > > > finished.  Should be in the backlog for debt relief.
> > > > 
> > > > On Tue, Aug 9, 2016 at 9:52 PM, David Oppenheimer davidopp@google.com
> > > > wrote:
> > > > 
> > > > > We can move them from alpha/beta and later do a parallel set of
> > > > > labels
> > > > > or
> > > > > whatever to express more complicated topologies. I don't think anyone
> > > > > has
> > > > > time to think about the ""right"" way to describe flexible topologies
> > > > > right
> > > > > now...
> > > > > 
> > > > > On Tue, Aug 9, 2016 at 9:48 PM, Tim Hockin thockin@google.com
> > > > > wrote:
> > > > > 
> > > > > > Yeah, they WORK.  The ""alpha"" is ugly.
> > > > > > 
> > > > > > On Tue, Aug 9, 2016 at 9:48 PM, David Oppenheimer
> > > > > > davidopp@google.com
> > > > > > wrote:
> > > > > > 
> > > > > > > I don't think the alpha.\* prefix actually does anything. So if the
> > > > > > > kubelet
> > > > > > > publishes them (I believe that's where they're getting published
> > > > > > > from),
> > > > > > > you
> > > > > > > should be able to use them already on GKE.
> > > > > > > 
> > > > > > > On Tue, Aug 9, 2016 at 9:44 PM, Brian Grant
> > > > > > > briangrant@google.com
> > > > > > > wrote:
> > > > > > > 
> > > > > > > > +davidopp
> > > > > > > > 
> > > > > > > > Bare metal might well have more failure domains. Given potential
> > > > > > > > variation
> > > > > > > > in cluster topologies, I don't know that we ever want these to be
> > > > > > > > fields.
> > > > > > > > The scheduler will deal with them as labels.
> > > > > > > > 
> > > > > > > > We probably want them to be beta, if they aren't already, so they
> > > > > > > > can
> > > > > > > > be
> > > > > > > > used in GKE, but I don't know if this is on anyone's radar.
> > > > > > > > 
> > > > > > > > On Wed, Jul 13, 2016 at 3:24 PM, Tim Hockin thockin@google.com
> > > > > > > > wrote:
> > > > > > > > 
> > > > > > > > > The failure-domain annotations are documented as alpha/beta and
> > > > > > > > > are
> > > > > > > > > named as such.
> > > > > > > > > 
> > > > > > > > > Do we intend to move the annotations to be ""real"" any time soon?
> > > > > > > > > 
> > > > > > > > > Are we codifying the 2-level zone/region model or do we intend
> > > > > > > > > for
> > > > > > > > > it
> > > > > > > > > to be more generic for arbitrary cases (e.g. rack within a zone)
> > > > > > > > > 
> > > > > > > > > Do we intend for topology info to forever be in labels, or will
> > > > > > > > > it
> > > > > > > > > ever have a home as first-class fields?

",closed,False,2017-10-30 18:00:20,2018-07-23 23:14:24
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/65,https://api.github.com/repos/kubernetes/federation/issues/65,Setup federation e2e on AWS and GKE,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Friday Aug 12, 2016 at 21:22 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/30543_

----

Forked from https://github.com/kubernetes/kubernetes/issues/23653.

@colhom  is working on this. Assigning it to him.

cc @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 18:00:25,2018-07-23 23:14:24
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/66,https://api.github.com/repos/kubernetes/federation/issues/66,federated-image.tag not found,"<a href=""https://github.com/icoloma""><img src=""https://avatars0.githubusercontent.com/u/222529?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [icoloma](https://github.com/icoloma)**
_Thursday Aug 18, 2016 at 12:16 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/30869_

----

Following [the documentation](http://kubernetes.io/docs/admin/federation/). With a vanilla kubernetes install, `federation-up.sh` throws an error.:

```
FATAL: tagfile /home/icoloma/code/kubernetes/cluster/../cluster/gke/../../cluster/../cluster/../federation/manifests/federated-image.tag does not exist. Make sure that you have run build/push-federation-images.sh
```

`push-federation-images.sh` works fine, but the error is still there.

**Kubernetes version**: 1.3.3
**Environment**: 
- **Cloud provider or hardware configuration**: GKE
- **OS**: 14.04.5 in my local laptop
- **Kernel**: Linux icoloma 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

",closed,False,2017-10-30 18:00:45,2018-02-05 12:45:46
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/67,https://api.github.com/repos/kubernetes/federation/issues/67,Add more debug info to replicaset controller,"<a href=""https://github.com/mwielgus""><img src=""https://avatars2.githubusercontent.com/u/11994812?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mwielgus](https://github.com/mwielgus)**
_Monday Aug 22, 2016 at 20:29 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/31152_

----

Especially around planer, its input and output.

",closed,False,2017-10-30 18:01:02,2018-07-23 20:11:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/68,https://api.github.com/repos/kubernetes/federation/issues/68,Federated Ingress Needs to Fail Gracefully for non-GCP Clusters,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Monday Aug 22, 2016 at 22:56 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/31184_

----

Federated Ingress only currently works across GCE/GKE/GCP clusters with the ""GLBC"" ingress controller installed.  This needs to be clearly documented, and appropriate error events generated (by the federation control plane) when attempts are made to create Federated Ingresses across other types of clusters. 

",closed,False,2017-10-30 18:01:12,2018-07-23 22:13:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/69,https://api.github.com/repos/kubernetes/federation/issues/69,Federation e2e setup doesn't clean up resources in the face of failures,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Tuesday Aug 23, 2016 at 19:05 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/31283_

----

When `federation-up.sh` fails in an e2e environment, it causes the test to error out and stop. This means, none of the cloud resources, i.e. none of the VMs, IP addresses, Loadbalancers and network rules are cleaned up. If the tests fail enough number of times to exhaust the quota, it leads to cascading failures and none of the future tests can pass until someone manually deletes all the resources. By this time the number of accumulated resources are so high that it is even painful to manually delete them. We should fix this.

If `federation-up.sh` fails, we should still run all the turn down scripts.

cc @kubernetes/sig-cluster-federation @colhom 

",closed,False,2017-10-30 18:01:50,2018-07-23 22:13:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/70,https://api.github.com/repos/kubernetes/federation/issues/70,Use weights when decreasing replicas number in federated replica set with rebalance=false  ,"<a href=""https://github.com/mwielgus""><img src=""https://avatars2.githubusercontent.com/u/11994812?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mwielgus](https://github.com/mwielgus)**
_Wednesday Aug 31, 2016 at 20:55 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/31816_

----

Currently no weights are used and the result may be very unbalanced. For example, if weight is 1 for all clusters and rebalance = false and we have 10 replicas in A and 70 in B and replica count is reduced to 20 the end result may be B=20 (depending on hashing - see #31814), while A=10 B=10 would be more preferred.  

",closed,False,2017-10-30 18:02:15,2018-07-23 18:09:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/71,https://api.github.com/repos/kubernetes/federation/issues/71,Add e2e tests for Federated ReplicaSet cluster weighting/min/max,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Friday Sep 02, 2016 at 22:59 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/32014_

----

cc @jianhuiz As per https://github.com/kubernetes/kubernetes/pull/31904#issuecomment-244505671

Per design below.  Current e2e tests do not cover that functionality.

```
type FederatedReplicaSetPreferences struct {
    // Map from cluster name to preferences for that cluster. It is assumed that if a cluster 
    // doesn’t have a matching entry then it should not have local replica. The cluster matches 
    // to ""*"" if there is no entry with the real cluster name. 
    Clusters map[string]LocalReplicaSetPreferences
}

type LocalReplicaSetPreferences struct {
     // There should be at least this amount of replicas in the clusters. 0 if not specified.
    MinReplicas int
    // There should be at most this number of replicas in the cluster. Unbounded 
    // if not specified.
    MaxReplicas *int
    //  This weight should be used when spreading replicas across clusters
    Weight int
}

```

",closed,False,2017-10-30 18:02:26,2018-07-21 15:19:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/72,https://api.github.com/repos/kubernetes/federation/issues/72,Federation: Configure a soak test job in Jenkins CI ,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Thursday Sep 22, 2016 at 15:43 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/33284_

----

#33186 highlighted the fact that we do not yet have a soak test for Federation configured in Jenkins.

The basic idea is to have a CI test job which repeatedly runs all of our e2e tests, over and over, against a single federation, without tearing it down between runs. Typically we run the clusters for about a week.  This can be modelled on the existing Kubernetes soak tests:

https://k8s-testgrid.appspot.com/google-soak

@colhom @madhusudancs would either of you like to take a shot at this for v1.5?

cc @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 18:02:39,2018-07-24 00:15:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/73,https://api.github.com/repos/kubernetes/federation/issues/73,federation: creating events on federation namespaces is failing,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Thursday Sep 22, 2016 at 17:51 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/33292_

----

https://github.com/kubernetes/kubernetes/pull/31640 added events to federation namespace controller, but creating namespace events is failing on our e2e tests:

```
Server rejected event '&api.Event{TypeMeta:unversioned.TypeMeta{Kind:"""", APIVersion:""""}, ObjectMeta:api.ObjectMeta{Name:""e2e-tests-federation-replicaset-s7xan.147625127063baf8"", GenerateName:"""", Namespace:""default"", SelfLink:"""", UID:"""", ResourceVersion:"""", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*unversioned.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]api.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:""""}, InvolvedObject:api.ObjectReference{Kind:""Namespace"", Namespace:"""", Name:""e2e-tests-federation-replicaset-s7xan"", UID:""80f0b389-7f78-11e6-898f-0a580ab50304"", APIVersion:""v1"", ResourceVersion:""4"", FieldPath:""""}, Reason:""CreateInCluster"", Message:""Creating namespace in cluster federation-e2e-gce-us-central1-f"", Source:api.EventSource{Component:""federated-namespace-controller"", Host:""""}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63610003489, nsec:135639288, loc:(*time.Location)(0x84ad960)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63610003489, nsec:135639288, loc:(*time.Location)(0x84ad960)}}, Count:1, Type:""Normal""}': 'namespaces ""default"" not found' (will not retry!)
```

It is because, namespaces do not have ObjectMeta.Namespace set and the event recorder library uses that to figure out the namespace in which the event should be generated: https://github.com/kubernetes/kubernetes/blob/a8053c7c59024a40bac5debe31857084d79729c2/pkg/client/record/event.go#L299.

We have 2 options to fix this:
- Update the library to use the namespace name if the ObjectReference is for a namespace object.
- Ensure that api.NamespaceDefault namespace exists in federation

cc @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 18:02:49,2018-07-23 21:12:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/74,https://api.github.com/repos/kubernetes/federation/issues/74,federation: verify and add tests for replicaset preferences annotations,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Thursday Sep 22, 2016 at 20:08 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/33312_

----

Found while writing the federation replicaset guide:

Steps to repro:
- Register a cluster C1 for federation.
- Create a federation replicaset with X replicas.
- Observe that a replicaset is created in the cluster C1 with X replicas.
- Now register another cluster C2 for federation.

Expected:
- The replicaset is also created in cluster C2 with X/2 replicas. Replica set in cluster C1 is scaled down from X to X/2 replicas.

Actual:
- Replicaset not created in cluster C2. Replicaset in cluster C1 continues to have X replicas.

cc @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 18:03:18,2018-07-23 21:12:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/75,https://api.github.com/repos/kubernetes/federation/issues/75,federation: Support cascading deletion,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Tuesday Sep 27, 2016 at 22:31 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/33612_

----

Problem: There are 2 related problems:
- When user deletes a federated object, corresponding objects created in underlying objects are not deleted. For ex: if user creates a federated service, we create a corresponding service in each cluster. When the federated service is deleted, these services in underlying clusters does not get deleted automatically.
- When a cluster is removed from federation, objects that the federation control plane created in that cluster are not deleted. For ex: if user creates a federated service, we create a corresponding service in each cluster. When a cluster is removed from federation, the service corresponding to the federated service in that cluster does not get deleted automatically.

Now that we have cascading deletion in kubernetes, we should support cascading deletion in federation as well. As in kubernetes, we will give users the option to disable it.

cc @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 18:04:01,2018-07-23 21:12:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/76,https://api.github.com/repos/kubernetes/federation/issues/76,federation: Allow reading kubernetes resources from federation-apiserver,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Wednesday Sep 28, 2016 at 00:55 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/33622_

----

Right now, federation-apiserver only returns federated resources (resources created in federation control plane). For ex: doing a GET `/api/v1/services` returns federated services. To get the corresponding services from underlying kubernetes clusters, clients need to talk directly to those clusters.
We want to enable clients to be able to get all resources (including the ones from underlying clusters) from federation-apiserver.

cc @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 18:04:39,2018-05-29 05:23:03
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/77,https://api.github.com/repos/kubernetes/federation/issues/77,Refactor federation unit tests so that they don't modify objects,"<a href=""https://github.com/mwielgus""><img src=""https://avatars2.githubusercontent.com/u/11994812?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mwielgus](https://github.com/mwielgus)**
_Wednesday Sep 28, 2016 at 17:05 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/33667_

----

Currently there is a potential race/data sync issue when tests are modifying data that was already passed to the client/cache mocks. 

cc: @gmarek @quinton-hoole @kubernetes/sig-cluster-federation 

",closed,False,2017-10-30 18:05:43,2018-07-24 00:15:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/78,https://api.github.com/repos/kubernetes/federation/issues/78,Use owner ref to calculate pod statistics in federated replicaset controller ,"<a href=""https://github.com/mwielgus""><img src=""https://avatars2.githubusercontent.com/u/11994812?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mwielgus](https://github.com/mwielgus)**
_Wednesday Oct 05, 2016 at 18:19 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/34130_

----

Right now the controller list all pods to calculate the stats.

",closed,False,2017-10-30 18:05:56,2018-07-24 01:16:23
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/79,https://api.github.com/repos/kubernetes/federation/issues/79,Federated ingress creates flapping backends and health checks,"<a href=""https://github.com/craigbox""><img src=""https://avatars3.githubusercontent.com/u/132510?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [craigbox](https://github.com/craigbox)**
_Sunday Nov 06, 2016 at 23:37 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/36327_

----

Similar to #34291 and possibly related to a problem described in #29341.

I have a federation with three clusters (us, asia, europe).  In it I have created a federated Ingress which refers to a federated Service:

Ingress
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: buttonmasher-frontend
spec:
  backend:
    serviceName: buttonmasher-frontend
    servicePort: 80
```

Service:
```
kind: Service
apiVersion: v1
metadata:
  name: buttonmasher-frontend
  labels:
    app: buttonmasher-frontend
spec:
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30061
  selector:
    app: buttonmasher-frontend
  type: LoadBalancer
```

The Deployment has a livenessProbe on /:
```
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 2
          timeoutSeconds: 1
```

Parts of the L7 LB are constantly being updated, created and removed.  

Over time, the number of instances shown as health in the LoadBalancer page in the Cloud console flaps - it should be 2/2 from all three regions, but there is generally one or two regions that say 0/2.

The project activity page is a hive of activity.
```
23:33
Create backend service
1082022110804@cloudservices.gserviceaccount.com created k8s-be-31488--32faf76db8066d21.
23:33
Update firewall rule
1082022110804@cloudservices.gserviceaccount.com updated k8s-fw-l7--32faf76db8066d21.
23:33
Create HTTP health check
1082022110804@cloudservices.gserviceaccount.com created k8s-be-31488--32faf76db8066d21.
23:33
Create backend service
1082022110804@cloudservices.gserviceaccount.com created k8s-be-31164--32faf76db8066d21.
23:33
Delete HTTP health check
1082022110804@cloudservices.gserviceaccount.com deleted k8s-be-30276--32faf76db8066d21.
23:33
Delete backend service
1082022110804@cloudservices.gserviceaccount.com deleted k8s-be-30276--32faf76db8066d21.
23:33
Create HTTP health check
1082022110804@cloudservices.gserviceaccount.com created k8s-be-31164--32faf76db8066d21.
23:33
Create backend service
1082022110804@cloudservices.gserviceaccount.com created k8s-be-30276--32faf76db8066d21.
23:33
Delete HTTP health check
1082022110804@cloudservices.gserviceaccount.com deleted k8s-be-31488--32faf76db8066d21.
23:33
Update firewall rule
1082022110804@cloudservices.gserviceaccount.com updated k8s-fw-l7--32faf76db8066d21.
23:33
Delete backend service
1082022110804@cloudservices.gserviceaccount.com deleted k8s-be-31488--32faf76db8066d21.
23:33
Create HTTP health check
1082022110804@cloudservices.gserviceaccount.com created k8s-be-30276--32faf76db8066d21.
23:32
Delete HTTP health check
1082022110804@cloudservices.gserviceaccount.com deleted k8s-be-30276--32faf76db8066d21.
23:32
Delete backend service
1082022110804@cloudservices.gserviceaccount.com deleted k8s-be-30276--32faf76db8066d21.
```

The backend service for the federated ingress seems to continue to exist, but one each for the consitutent regions come and go:
```
$ gcloud compute backend-services list
NAME                            BACKENDS                                                                                                                                                           PROTOCOL
k8s-be-30061--32faf76db8066d21  us-central1-c/instanceGroups/k8s-ig--32faf76db8066d21,europe-west1-c/instanceGroups/k8s-ig--32faf76db8066d21,asia-east1-c/instanceGroups/k8s-ig--32faf76db8066d21  HTTP
k8s-be-30276--32faf76db8066d21  asia-east1-c/instanceGroups/k8s-ig--32faf76db8066d21                                                                                                               HTTP
k8s-be-31164--32faf76db8066d21  europe-west1-c/instanceGroups/k8s-ig--32faf76db8066d21                                                                                                             HTTP
$ gcloud compute backend-services list
NAME                            BACKENDS                                                                                                                                                           PROTOCOL
k8s-be-30061--32faf76db8066d21  us-central1-c/instanceGroups/k8s-ig--32faf76db8066d21,europe-west1-c/instanceGroups/k8s-ig--32faf76db8066d21,asia-east1-c/instanceGroups/k8s-ig--32faf76db8066d21  HTTP
k8s-be-30276--32faf76db8066d21  asia-east1-c/instanceGroups/k8s-ig--32faf76db8066d21                                                                                                               HTTP
k8s-be-31164--32faf76db8066d21  europe-west1-c/instanceGroups/k8s-ig--32faf76db8066d21                                                                                                             HTTP
k8s-be-31488--32faf76db8066d21  us-central1-c/instanceGroups/k8s-ig--32faf76db8066d21                                                                                                              HTTP
$ gcloud compute backend-services list
NAME                            BACKENDS                                                                                                                                                           PROTOCOL
k8s-be-30061--32faf76db8066d21  us-central1-c/instanceGroups/k8s-ig--32faf76db8066d21,europe-west1-c/instanceGroups/k8s-ig--32faf76db8066d21,asia-east1-c/instanceGroups/k8s-ig--32faf76db8066d21  HTTP
k8s-be-30276--32faf76db8066d21  asia-east1-c/instanceGroups/k8s-ig--32faf76db8066d21                                                                                                               HTTP
k8s-be-31488--32faf76db8066d21  us-central1-c/instanceGroups/k8s-ig--32faf76db8066d21                                                                                                              HTTP
```

Similarly:
```
$ gcloud compute http-health-checks list
NAME                            HOST  PORT   REQUEST_PATH
k8s-be-30061--32faf76db8066d21        30061  /
k8s-be-30276--32faf76db8066d21        30276  /healthz
k8s-be-31164--32faf76db8066d21        31164  /healthz
k8s-be-31488--32faf76db8066d21        31488  /healthz
$ gcloud compute http-health-checks list
NAME                            HOST  PORT   REQUEST_PATH
k8s-be-30061--32faf76db8066d21        30061  /
k8s-be-30276--32faf76db8066d21        30276  /healthz
k8s-be-31488--32faf76db8066d21        31488  /healthz
```
The health check path is /healthz, which I think is the Kubernetes default, if nothing is specified?


",closed,False,2017-10-30 18:06:16,2018-07-24 00:15:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/80,https://api.github.com/repos/kubernetes/federation/issues/80,Deleting a cluster from the federation doesn't clean up DNS entries,"<a href=""https://github.com/saturnism""><img src=""https://avatars1.githubusercontent.com/u/1998883?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [saturnism](https://github.com/saturnism)**
_Friday Nov 11, 2016 at 17:19 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/36657_

----

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.1"", GitCommit:""33cf7b9acbb2cb7c9c72a10d6636321fb180b159"", GitTreeState:""clean"", BuildDate:""2016-10-10T18:19:49Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.5"", GitCommit:""5a0a696437ad35c133c0c8493f7e9d22b0f9b81b"", GitTreeState:""clean"", BuildDate:""2016-10-29T01:32:42Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**: OSX, GKE
- **Cloud provider or hardware configuration**: GKE
- **OS** (e.g. from /etc/os-release): GCI
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:
When deleting a cluster from a federation, it doesn't clean up any of the DNS records associated w/ that cluster.

**What you expected to happen**:
Deleting a cluster from federation should delete related resources.

**How to reproduce it** (as minimally and precisely as possible):
1. Create a federation w/ 2 clusters
1. Provision a federated service w/ healthy backends
1. See that the DNS entries were created in the cloud provider
1. Delete a cluster from the federation

**Anything else do we need to know**:

",closed,False,2017-10-30 18:07:38,2018-07-24 01:16:22
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/81,https://api.github.com/repos/kubernetes/federation/issues/81,[federation] kubectl delete cluster doesn't clean controllermanager,"<a href=""https://github.com/lsztachanski""><img src=""https://avatars2.githubusercontent.com/u/7313446?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [lsztachanski](https://github.com/lsztachanski)**
_Tuesday Nov 15, 2016 at 13:26 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/36815_

----


**Kubernetes version** 

```Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.5"", GitCommit:""5a0a696437ad35c133c0c8493f7e9d22b0f9b81b"", GitTreeState:""clean"", BuildDate:""2016-10-29T01:38:40Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}```

federation server:
```Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.0"", GitCommit:""a16c0a7f71a6f93c7e0f222d961f4675cd97a46b"", GitTreeState:""clean"", BuildDate:""2016-09-26T18:10:32Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}```

cluster1:
```Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.5"", GitCommit:""5a0a696437ad35c133c0c8493f7e9d22b0f9b81b"", GitTreeState:""clean"", BuildDate:""2016-10-29T01:32:42Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}```

cluster2
```Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.5"", GitCommit:""5a0a696437ad35c133c0c8493f7e9d22b0f9b81b"", GitTreeState:""clean"", BuildDate:""2016-10-29T01:32:42Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}```


**Environment**:
- 2 GKE clusters
- federation setup from: https://github.com/kelseyhightower/kubernetes-cluster-federation


**What happened**:
I've added cluster with wrong `serverAddress`. Tried to delete it:

```kubectl --context=$FEDERATIONAPI delete -f clusters/$CLUSTER.yaml```

Which  removes cluster from the list:

```kubectl --context=$FEDERATIONAPI get clusters```

but `federation-controller-manager` produces constant errors:

```1115 13:09:09.297103       1 reflector.go:214] k8s.io/kubernetes/federation/pkg/federation-controller/service/clusterhelper.go:155: Failed to list *v1.Service: Get http://1.2.3.4/api/v1/services?resourceVersion=0: dial tcp 1.2.3.4:80: i/o timeout```




**Anything else do we need to know**:
- no trace of wrong address in etcd (federation-apiserver)
- deleting federation-controller-manager (spawned by deployment) fixes the issue

",closed,False,2017-10-30 18:19:07,2018-05-14 23:48:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/82,https://api.github.com/repos/kubernetes/federation/issues/82,"When a federated cluster endpoint is updated, federation controller manager still uses old endpoint","<a href=""https://github.com/saturnism""><img src=""https://avatars1.githubusercontent.com/u/1998883?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [saturnism](https://github.com/saturnism)**
_Wednesday Nov 16, 2016 at 22:34 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/36921_

----

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.1"", GitCommit:""33cf7b9acbb2cb7c9c72a10d6636321fb180b159"", GitTreeState:""clean"", BuildDate:""2016-10-10T18:19:49Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.5"", GitCommit:""5a0a696437ad35c133c0c8493f7e9d22b0f9b81b"", GitTreeState:""clean"", BuildDate:""2016-10-29T01:32:42Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**: OSX, GKE
- **Cloud provider or hardware configuration**: GKE
- **OS** (e.g. from /etc/os-release): GKE
- **Kernel** (e.g. `uname -a`): GKE
- **Install tools**: GKE
- **Others**:

**What happened**:
I added a cluster into federation, say, ""cluster-1"", w/ an endpoint address, e.g. http://123.123.123.123. I later deleted cluster-1 and also the secret. I then re-added cluster-1 back, but now, with a new endpoint, e.g., http://111.111.111.111. The federation server shows cluster-1 is healthy, but in the logs, there is a repeating error on unable to connect to the old endpoint, http://123.123.123.123.

**What you expected to happen**:
Federation controller manager should use the new endpoint

**How to reproduce it** (as minimally and precisely as possible):
1. Add a cluster to federation, name it cluster-1
1. Delete the cluster from federation
1. Add a new cluster to federation, w/ a different API server, and name it cluster-1 as well
1. Observe the error

**Anything else do we need to know**:


",closed,False,2017-10-30 18:19:27,2018-05-14 23:48:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/83,https://api.github.com/repos/kubernetes/federation/issues/83,"When a federated cluster becomes offline, federation controller manager doesn't u","<a href=""https://github.com/saturnism""><img src=""https://avatars1.githubusercontent.com/u/1998883?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [saturnism](https://github.com/saturnism)**
_Wednesday Nov 16, 2016 at 22:38 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/36923_

----

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.1"", GitCommit:""33cf7b9acbb2cb7c9c72a10d6636321fb180b159"", GitTreeState:""clean"", BuildDate:""2016-10-10T18:19:49Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.5"", GitCommit:""5a0a696437ad35c133c0c8493f7e9d22b0f9b81b"", GitTreeState:""clean"", BuildDate:""2016-10-29T01:32:42Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**: GKE
- **Cloud provider or hardware configuration**: GKE
- **OS** (e.g. from /etc/os-release): GKE
- **Kernel** (e.g. `uname -a`): GKE
- **Install tools**:
- **Others**:

**What happened**:
I federated 3 clusters together. I deliberately killed the API server on one the clusters. Federation status shows that the cluster is Offline. However, the service IP is not removed from DNS, and there is a repeating error in the federation controller manager log, such as:
```
E1116 22:36:03.276899       1 reflector.go:214] k8s.io/kubernetes/federation/pkg/federation-controller/service/cluster_helper.go:155: Failed to list *v1.Endpoints: the server could not find the requested resource (get endpoints)
E1116 22:36:03.277798       1 reflector.go:214] k8s.io/kubernetes/federation/pkg/federation-controller/service/cluster_helper.go:154: Failed to list *v1.Service: the server could not find the requested resource (get services)
```

**What you expected to happen**:
It should clean up and remove service IPs associated w/ the cluster that's now offline

**How to reproduce it** (as minimally and precisely as possible):
1. Federate multiple clusters together
1. Configure kubedns w/ federation
1. Deploy backend to 2 clusters
1. Run consumers on other clusters not running the backend
1. Pull the plug on the API server on one of the cluster running the backend
1. Observe the error in federation controller manager

**Anything else do we need to know**:


",closed,False,2017-10-30 18:19:40,2018-07-24 00:15:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/84,https://api.github.com/repos/kubernetes/federation/issues/84,"Handle updates, rolling update preferences and revisions in federated deployment controller","<a href=""https://github.com/mwielgus""><img src=""https://avatars2.githubusercontent.com/u/11994812?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mwielgus](https://github.com/mwielgus)**
_Monday Nov 28, 2016 at 20:14 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/37570_

----

... in a more consistent way, especially when looking from the cross-cluster perspective. Currently there is no rolling update subresource, sub-deployments may have different revision numbers, max surge/max unavailable are handled separately in each cluster, etc.

cc: @madhusudancs @nikhiljindal 
",closed,False,2017-10-30 18:19:53,2018-07-23 21:12:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/85,https://api.github.com/repos/kubernetes/federation/issues/85,federation: cluster replicasets should not be scaled down when federated rs is deleted using kubectl,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Tuesday Dec 06, 2016 at 22:42 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/38223_

----

Steps to repro:
* fedkubectl create -f myrs.yaml
* fedkubectl delete rs myrs

where fedkubectl is `kubectl --context=federation-cluster`

Expected:
myrs should be deleted from federation control plane. myrs in underlying clusters should be left as is.

Actual:
myrs is deleted from federation control plane fine, but myrs in all underlying clusters are also scaled down to zero. This should not happen. They should be left as is.

Note that this problem exists only when replicaset is deleted using kubectl. Deleting the replicaset directly using the REST API works fine.

cc @kshafiee @kubernetes/sig-cluster-federation 
",closed,False,2017-10-30 18:20:17,2017-10-31 06:01:42
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/86,https://api.github.com/repos/kubernetes/federation/issues/86,federation: cluster replicasets should not be scaled down when federated rs is deleted using kubectl,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Tuesday Dec 06, 2016 at 22:42 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/38223_

----

Steps to repro:
* fedkubectl create -f myrs.yaml
* fedkubectl delete rs myrs

where fedkubectl is `kubectl --context=federation-cluster`

Expected:
myrs should be deleted from federation control plane. myrs in underlying clusters should be left as is.

Actual:
myrs is deleted from federation control plane fine, but myrs in all underlying clusters are also scaled down to zero. This should not happen. They should be left as is.

Note that this problem exists only when replicaset is deleted using kubectl. Deleting the replicaset directly using the REST API works fine.

cc @kshafiee @kubernetes/sig-cluster-federation 
",closed,False,2017-10-31 06:00:31,2018-07-23 19:10:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/87,https://api.github.com/repos/kubernetes/federation/issues/87,Upgrade tests for federation,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Saturday Dec 10, 2016 at 00:02 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/38537_

----

We need automated tests to verify federation upgrades. Here are the list of things we need to do:

* Define what it means to upgrade test federation.
* Define the APIs/features we want to include in the tests.
* Define the exact tests and implement them.
* Setup the infrastructure to run these tests: jenkins jobs, test-grid, etc.

cc @kubernetes/sig-federation @matchstick @nikhiljindal 
",closed,False,2017-10-31 06:05:12,2018-08-04 03:38:28
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/88,https://api.github.com/repos/kubernetes/federation/issues/88,federation: Known issues,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Friday Dec 16, 2016 at 19:52 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/38893_

----

Compiling a list of high level known issues that customers should be aware of while deciding to use federation:

- [x] No way to disable specific APIs (#38593)
- [ ] Alpha APIs are enabled by default (https://github.com/kubernetes/kubernetes/issues/44291)
- [ ] No upgrade testing (#38537)
- [x] No kubectl tests (https://github.com/kubernetes/kubernetes/issues/32826)
- [x] Not all e2e tests are green (https://github.com/kubernetes/kubernetes/issues/37105)
- [ ] Unclear auth story (https://github.com/kubernetes/kubernetes/issues/35254)
- [ ] Not all API resources are supported in federation (http://kubernetes.io/docs/user-guide/federation/ lists the supported resources)
- [ ] Need to design HA federation control plane (https://github.com/kubernetes/features/issues/104)


cc @kubernetes/sig-federation-misc 

",open,False,2017-10-31 06:05:28,2018-12-26 05:33:43
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/89,https://api.github.com/repos/kubernetes/federation/issues/89,federation: kubectl --cascade should be false by default for federation,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Friday Dec 16, 2016 at 20:18 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/38897_

----

Ref https://github.com/kubernetes/kubernetes/issues/33612.

Cascading deletion is true by default for kubectl.

For federation, we want it to be false since deleting a federation resource with cascade=true will delete the resource from all clusters as well which can lead to a global outage. So we want --cascade to be false by default but users can pass --cascade=true if they do want cascading deletion.

cc @kubernetes/sig-federation-misc @kubernetes/sig-cli-misc @quinton-hoole @caesarxuchao 
",closed,False,2017-10-31 06:05:59,2018-07-23 20:11:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/90,https://api.github.com/repos/kubernetes/federation/issues/90,kubefed should allow adding a label to cluster API resource,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Thursday Jan 05, 2017 at 19:58 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39482_

----

Came up while discussing https://github.com/kubernetes/kubernetes/issues/29887 with @emaildanwilson .

We need a way to add labels to a cluster API resource while joining it in federation.
Something like:
`kubefed join mycluster1 --labels=""region=us-central""`

cc @madhusudancs @kubernetes/sig-federation-misc 
",closed,False,2017-10-31 06:07:03,2018-07-23 21:12:57
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/91,https://api.github.com/repos/kubernetes/federation/issues/91,kubefed init needs to create rolebinding to allow cluster controller to fetch secret,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Friday Jan 06, 2017 at 23:20 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39555_

----

With the latest rbac changes, federation cluster controller is not able to fetch secrets from host cluster for registered clusters.  The exact error is 
`error in fetching secret: User ""system:serviceaccount:federation-system:default"" cannot get secrets in the namespace ""federation-system""`
As a result, registered clusters never turn ready.

Running the following fixed the problem for me:
```
kubectl create rolebinding -n federation-system edit --serviceaccount federation-system:default --clusterrole=admin
```
We should define our own clusterrole rather than using admin.

cc @madhusudancs @kubernetes/sig-federation-misc 
cc @liggitt and @deads2k as fyi
",closed,False,2017-10-31 06:07:24,2018-07-24 01:16:23
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/92,https://api.github.com/repos/kubernetes/federation/issues/92,Federation should enable etcd2 -> etcd3 migration for existing deployments. ,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Wednesday Jan 11, 2017 at 17:58 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39746_

----

PR #39646 moved the federation's storage from etcd2 to etcd3. However, it just changed the image and made `etcd3` use `etcd2` as the storage backend. We still need to enable `etcd3` as a storage backend and also enable people with existing federation control plane deployments to migrate from `etcd2` to `etcd3`.

Follow up to issue #39594.

cc @kubernetes/sig-federation-misc @nikhiljindal @shashidharatd 
",closed,False,2017-10-31 06:07:39,2018-07-23 23:14:26
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/93,https://api.github.com/repos/kubernetes/federation/issues/93,docs for federation-controller-manager should indicate that coredns is a valid option,"<a href=""https://github.com/emaildanwilson""><img src=""https://avatars0.githubusercontent.com/u/3026995?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [emaildanwilson](https://github.com/emaildanwilson)**
_Wednesday Jan 11, 2017 at 19:37 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39749_

----

https://kubernetes.io/docs/admin/federation-controller-manager/ should indicate that coredns is a valid option for --dns-provider. 

https://github.com/kubernetes/kubernetes/issues/39271#issuecomment-271783981

",closed,False,2017-10-31 06:08:00,2018-02-05 12:48:19
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/94,https://api.github.com/repos/kubernetes/federation/issues/94,"[Federation] Updating a deployment on federation with a failed RS, unbalance the RS counts on underlying clusters.","<a href=""https://github.com/ualtinok""><img src=""https://avatars2.githubusercontent.com/u/94532?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [ualtinok](https://github.com/ualtinok)**
_Friday Jan 13, 2017 at 05:40 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39856_

----

**Kubernetes version** (use `kubectl version`):
1.5.1

**Environment**:
GKE

**What happened**:
Created a deployment with 6 replicas on federation cluster which has 3 underlying kube clusters. Deployment failed due to crash of container and RS stayed on CrashLoopbackOff status. Tried to re apply federation deployment with updated container version. Federation successfully changed the version and redeployed but with status of:
- cluster1: 6/6
- cluster2: 0/0
- cluster3: 0/0

**What you expected to happen**:
The balance of the RS' should have stayed 2/2 in all 3 of the underlying clusters.

**How to reproduce it** (as minimally and precisely as possible):
Create a federation cluster on top of 3 kubernetes cluster, create a deployment with 6 replicas which has a container that will crash immediately. Try to update to a new version of the container and apply deployment to federation cluster.


",closed,False,2017-10-31 06:08:05,2018-07-23 19:10:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/95,https://api.github.com/repos/kubernetes/federation/issues/95,enable kubectl tests with federation apiserver for replicasets,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Tuesday Jan 17, 2017 at 02:53 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39984_

----

https://github.com/kubernetes/kubernetes/pull/38844 added kubectl tests with federation apiserver, but did not enable them for replicasets since they were failing. We need to debug the issue, fix it and then enable the tests for replicasets.

Here is the corresponding TODO that needs fixing: https://github.com/kubernetes/kubernetes/blob/f74b4bbbad5463aa35b5d4d769b80db19cf12ee0/hack/make-rules/test-federation-cmd.sh#L77

Related issue for deployments: https://github.com/kubernetes/kubernetes/issues/39985

cc @kubernetes/sig-federation-bugs 
",closed,False,2017-10-31 06:08:15,2018-05-14 23:48:51
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/96,https://api.github.com/repos/kubernetes/federation/issues/96,enable kubectl tests with federation apiserver for deployments,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Tuesday Jan 17, 2017 at 02:54 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39985_

----

#38844 added kubectl tests with federation apiserver, but did not enable them for deployments since they were failing. We need to debug the issue, fix it and then enable the tests for deployments.

Here is the corresponding TODO that needs fixing: https://github.com/kubernetes/kubernetes/blob/f74b4bbbad5463aa35b5d4d769b80db19cf12ee0/hack/make-rules/test-federation-cmd.sh#L77

Related issue for replicasets: https://github.com/kubernetes/kubernetes/issues/39984
",closed,False,2017-10-31 06:08:32,2018-05-14 23:48:51
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/97,https://api.github.com/repos/kubernetes/federation/issues/97,federation replicasets: known issues,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Tuesday Jan 17, 2017 at 02:58 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39986_

----

Documenting the list of known issues with federation replicasets:

- [ ] `kubectl delete rs` not working as expected: https://github.com/kubernetes/kubernetes/issues/38493, https://github.com/kubernetes/kubernetes/issues/42594.
- [ ] Bug in `kubectl delete rs --cascade=false`: https://github.com/kubernetes/kubernetes/issues/38223
- [ ] Enable federation-apiserver kubectl tests for replicasets: https://github.com/kubernetes/kubernetes/issues/39984.
- [ ] Figure out a path to GA for FederatedReplicaSetPreferences.
- [ ] Verify that rebalancing works and add a test for that: https://github.com/kubernetes/kubernetes/issues/33312
- [ ] Document how rebalancing works: https://github.com/kubernetes/kubernetes.github.io/issues/1362
",closed,False,2017-10-31 06:08:37,2018-07-23 22:13:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/98,https://api.github.com/repos/kubernetes/federation/issues/98,Federated Deployments: known issues,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Tuesday Jan 17, 2017 at 03:01 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39987_

----

Documenting the list of known issues with federated deployments:

- [ ] Enable federation-apiserver kubectl tests for deployments: https://github.com/kubernetes/kubernetes/issues/39985.
- [ ] Figure out a path to GA for FederatedReplicaSetPreferences.
- [ ] Verify that rebalancing works and add a test for that: https://github.com/kubernetes/kubernetes/issues/33312
- [ ] Document how rebalancing works: https://github.com/kubernetes/kubernetes.github.io/issues/1362
- [ ] Handle rollbacks: https://github.com/kubernetes/kubernetes/issues/37570
- [ ] Update to use apps group version: https://github.com/kubernetes/kubernetes/issues/52123
",closed,False,2017-10-31 06:09:16,2018-07-23 21:12:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/99,https://api.github.com/repos/kubernetes/federation/issues/99,federation ingress: known issues,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Tuesday Jan 17, 2017 at 03:15 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39989_

----

Documenting the list of known issues with federated ingress:

Issues that need to be fixed soon:
- [x] Need to create a firewall rule. Users will have to create it manually pre 1.5. Document it. Create it in the test for now and get the tests to turn green (https://github.com/kubernetes/kubernetes/pull/37571). In 1.6, create it automatically in the controller (https://github.com/kubernetes/kubernetes/issues/37306).
- [ ] Verify (add a test) and document that users need to create a static IP themselves (by running `gcloud compute addresses create kubernetes-ingress --global`) for HTTP load balancer.
- [ ] GCP Ingress does not work for clusters in the same zone (https://github.com/kubernetes/kubernetes/issues/47565)
- [ ] Resource leak due to race condition with service controller (https://github.com/kubernetes/kubernetes/issues/52315)

Longer term:
- [ ] Support federated ingress outside GCP. https://github.com/kubernetes/kubernetes/issues/51806
",closed,False,2017-10-31 06:09:37,2018-07-24 01:16:24
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/100,https://api.github.com/repos/kubernetes/federation/issues/100,Make kubectl rollout work with federated deployments,"<a href=""https://github.com/mwielgus""><img src=""https://avatars2.githubusercontent.com/u/11994812?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mwielgus](https://github.com/mwielgus)**
_Monday Jan 23, 2017 at 22:41 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/40323_

----

Currently no history information is stored in federated deployment so no rollout rollback is possible.

cc: @nikhiljindal 
",closed,False,2017-10-31 06:10:06,2018-07-24 00:15:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/101,https://api.github.com/repos/kubernetes/federation/issues/101,Federation: Support deploying federation control plane on non-cloud environment,"<a href=""https://github.com/shashidharatd""><img src=""https://avatars3.githubusercontent.com/u/8734900?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [shashidharatd](https://github.com/shashidharatd)**
_Thursday Jan 26, 2017 at 18:42 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/40536_

----

There may be scenarios where there is a need to federate k8s clusters on private-cloud (on-premise) environments.
Listed below are some of the tasks to achieve the scenario above

- [x] Federation control plane deployment itself should be able to be deployed in non-cloud environments. #40516
- [x] Should be possible to disable persistent storage for etcd. #40862
- [x] Ability to configure desired DNS provider. Public DNS servers (AWS Route53/ Google CloudDNS) or Private DNS servers (e.g. CoreDNS) #40528 
- [x] Document Federation CoreDNS support. https://github.com/kubernetes/kubernetes.github.io/pull/2810
- [x] coredns federation provider should support etcd with TLS #47049
- [ ] Document using hosted DNS providers (AWS Route53/ Google CloudDNS) in non-cloud environments.
",closed,False,2017-10-31 06:10:18,2018-05-15 14:02:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/102,https://api.github.com/repos/kubernetes/federation/issues/102,[Federation] Eliminate code duplication across controllers,"<a href=""https://github.com/marun""><img src=""https://avatars3.githubusercontent.com/u/451477?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [marun](https://github.com/marun)**
_Monday Feb 06, 2017 at 01:31 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/40989_

----

At present there is lots of code duplication across the controllers that synchronize the member  clusters of a given federation with the desired federated state.  This issue is targeted at refactoring the controllers so they can share as much code as possible with the goal of simplifying testing and maintenance.

The original suggestion was to enable generation of the controllers, but that will be tackled separately if at all.

cc: @kubernetes/sig-federation-misc
",closed,False,2017-10-31 06:10:55,2018-07-23 23:14:26
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/103,https://api.github.com/repos/kubernetes/federation/issues/103,[Federation] Member cluster config should be stored in federation control plane,"<a href=""https://github.com/marun""><img src=""https://avatars3.githubusercontent.com/u/451477?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [marun](https://github.com/marun)**
_Friday Feb 17, 2017 at 23:42 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/41673_

----

Configuration for member clusters that require secure access is currently stored in secrets in the cluster hosting the federation control plane.  According to the following comment, this was originally necessary because the federation control plane didn't support secrets:

https://github.com/kubernetes/kubernetes/blob/master/federation/apis/federation/v1beta1/types.go#L43

Now that secrets are supported, cluster configuration should be stored in the federation control plane.

cc: @kubernetes/sig-federation-misc 
",closed,False,2017-10-31 06:11:11,2018-08-04 02:37:32
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/104,https://api.github.com/repos/kubernetes/federation/issues/104,Automating federation upgrade tests,"<a href=""https://github.com/shashidharatd""><img src=""https://avatars3.githubusercontent.com/u/8734900?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [shashidharatd](https://github.com/shashidharatd)**
_Tuesday Feb 21, 2017 at 12:08 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/41791_

----

This issue is to track all the tasks related to upgrade tests and framework for federation as described in [Kubernetes federation upgrade tests](https://docs.google.com/document/d/1QJg-8I3Ff3ClqLXRHpWXhe4BRaDcdRiuDPPbPHEomsw/edit?usp=sharing)

- [x] Framework for federation upgrade tests
- [ ] CI jobs for federation upgrade
- Individual test cases
  - [ ] Federated Service: @shashidharatd 
  - [ ] Federated ReplicaSets: @madhusudancs 
  - [ ] Federated Deployments: @madhusudancs 
  - [ ] Federated Ingress: @csbell 
  - [x] Federated ConfigMaps: @marun 
  - [x] Federated Secrets: @marun 
  - [x] Federated Daemonsets: @marun 
",closed,False,2017-10-31 06:11:45,2018-07-23 21:12:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/105,https://api.github.com/repos/kubernetes/federation/issues/105,"Federation: ""InvalidChangeBatch: Duplicate Resource Record"" error while configuring Route53","<a href=""https://github.com/crigor""><img src=""https://avatars0.githubusercontent.com/u/191?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [crigor](https://github.com/crigor)**
_Monday Feb 27, 2017 at 01:13 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42131_

----

<!-- Thanks for filing an issue! Before hitting the button, please answer these questions.-->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

No.

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

InvalidChangeBatch

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

Bug report.

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

1.5.1

**Environment**:
- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04.1 LTS
- **Kernel** (e.g. `uname -a`): `Linux ip-172-20-4-64 4.4.0-59-generic #80-Ubuntu SMP Fri Jan 6 17:47:47 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux`
- **Install tools**: Custom
- **Others**:


**What happened**:

I have 2 clusters in aws, 1 in us-east-1 and 1 in us-east-2. I set up cluster federation using https://github.com/kelseyhightower/kubernetes-cluster-federation as my guide.

I created a replica set with 4 replicas of nginx. 2 ran on each of the 2 regions. All good.

I created a service, the federation controller manager created CNAMES for nginx4.default.federation.svc.us-east-1.federation.com and nginx4.default.federation.svc.us-east-2.federation.com. This looks correct.

Eventually nginx4.default.federation.svc.us-east-1.federation.com was changed to an A record when the endpoints became healthy while nginx4.default.federation.svc.us-east-2.federation.com remained a CNAME.

This is the error from federation-controller-manager 1.5.3 (I also tried 1.5.1).

I0226 07:47:30.273951       1 dns.go:237] Recordset &{0xc421552a80 0xc4202da4d8} already exists.  Ensuring that it is correct.
I0226 07:47:30.273981       1 dns.go:268] nginx4.default.federation.svc.us-east-2a.us-east-2.federation.com.: Healthy endpoints [ae0cf9867fbf611e6954b023f939bc0f-805436699.us-east-2.elb.amazonaws.com ae0cf9867fbf611e6954b023f939bc0f-805436699.us-east-2.elb.amazonaws.com] exist.  Recordset &{0xc421552a80 0xc4202da4d8} exists.  Reconciling.
I0226 07:47:30.301127       1 dns.go:274] Have recordset &{0xc421552a80 0xc4202da4d8}. Need recordset {0xc421553500 0xc420ef8340}
I0226 07:47:30.301156       1 dns.go:282] Existing recordset &{0xc421552a80 0xc4202da4d8} is not equivalent to needed recordset {0xc421553500 0xc420ef8340}, removing existing and adding needed.
I0226 07:47:30.362002       1 service_helper.go:120] Error ensuring DNS Records for service default/nginx4 on cluster additionalzonesftw: InvalidChangeBatch: Duplicate Resource Record: 52.14.105.135
	status code: 400, request id: d35e1156-fbf7-11e6-b1d9-afb55cf95a01

Notice the 2 healthy endpoints are for the same ELB `ae0cf9867fbf611e6954b023f939bc0f-805436699...` Could this be the cause of `InvalidChangeBatch: Duplicate Resource Record`?

**What you expected to happen**:

I expected nginx4.default.federation.svc.us-east-2.federation.com to be an A record instead of CNAME.

**How to reproduce it** (as minimally and precisely as possible):

Set up cluster federation on AWS. Create a replica set. Create a service.

**Anything else we need to know**:


",closed,False,2017-10-31 06:12:01,2018-07-23 19:10:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/106,https://api.github.com/repos/kubernetes/federation/issues/106,Generate the secret name in kubefed join,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Wednesday Mar 01, 2017 at 09:17 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42324_

----

`kubefed join` uses the name of the cluster context to create a secret in which it stores the cluster credentials. The secret naming rules are very restrictive. Only valid DNS subdomain names as described in [RFC 1123](https://tools.ietf.org/html/rfc1123) are allowed. 

When a user runs `kubefed join` today with a cluster context that doesn't conform to these rules, we just give up midway leaving federation in a bad state. Users are expected to run `kubefed join` again with a correct value for `--secret-name` flag. But since federation is left in a bad state from the previous run, users can't run the `join` command again without manually cleaning up the artifacts. We have plans to add pre-flight checks to avoid leaving federation in a bad state. See https://github.com/kubernetes/kubernetes/issues/41725#issuecomment-282591166. While this alleviates the situation, it is still a poor user experience and hence isn't sufficient. 

It really doesn't matter to a user what those secret names exactly are as long as they conform to the naming rules. It is like a hidden detail. So we should always generate the secret name instead of deriving it from the cluster context.

cc @kubernetes/sig-federation-feature-requests 
",closed,False,2017-10-31 06:12:21,2018-07-23 19:10:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/107,https://api.github.com/repos/kubernetes/federation/issues/107,[Federation] Decide whether or not to reintroduce support for non-RBAC Federated clusters in kubefed join,"<a href=""https://github.com/perotinus""><img src=""https://avatars1.githubusercontent.com/u/5122391?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [perotinus](https://github.com/perotinus)**
_Wednesday Mar 01, 2017 at 20:20 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42352_

----

This is a follow-up to #41267.

#42042 removed support in `kubefed join` for clusters that don't support RBAC. Based on a discussion between me, @csbell and @madhusudancs, we need to decide Kubernetes cluster versions Federation will support. If we want to support pre-RBAC clusters, we need to reintroduce the old mechanism, with noisy warnings that the user should update for better security; if not, than this issue can be quickly closed. 
",closed,False,2017-10-31 06:12:55,2018-07-23 22:13:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/108,https://api.github.com/repos/kubernetes/federation/issues/108,[Federation] Joining cluster service account created by kubefed-join has a potentially-non-unique name,"<a href=""https://github.com/perotinus""><img src=""https://avatars1.githubusercontent.com/u/5122391?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [perotinus](https://github.com/perotinus)**
_Wednesday Mar 01, 2017 at 20:32 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42356_

----

This is a follow-up to #41267.

Currently, the service account created in a joining cluster by `kubefed join` is named based on the federation name and the host context name. This is not necessarily unique. We should find a more-unique naming scheme for these service accounts.

We may need to store the service account names somewhere where `kubefed unjoin` can find them.
",closed,False,2017-10-31 06:13:14,2018-07-23 22:13:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/109,https://api.github.com/repos/kubernetes/federation/issues/109,GCE Ingress (GLBC) needs improved federation handling at ingress deletion time,"<a href=""https://github.com/csbell""><img src=""https://avatars1.githubusercontent.com/u/23373339?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [csbell](https://github.com/csbell)**
_Saturday Mar 04, 2017 at 00:02 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42511_

----

While GLBC has been coaxed to handle duplicate Ingress objects at creation time, it needs more logic to correctly handle deletion of federated ingresses. Currently, deleting one of the ingress objects outside of  federation will cause the entire ingress to be torn down. Eventually the federated controller re-creates the ingress object but the downtime should be omitted altogether. Possible solutions include some form of adhoc refcounting mechanism or augmenting GLBC to notice that there are more than one relevant forwarding rules attached to an ingress object at deletion time. Details and design to be hashed out still.
",closed,False,2017-10-31 06:13:21,2018-01-29 16:52:57
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/110,https://api.github.com/repos/kubernetes/federation/issues/110,[Federation] kubefed init roles.rbac.authorization error,"<a href=""https://github.com/mawentao02""><img src=""https://avatars2.githubusercontent.com/u/25891133?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mawentao02](https://github.com/mawentao02)**
_Monday Mar 06, 2017 at 07:52 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42559_

----

<!-- Thanks for filing an issue! Before hitting the button, please answer these questions.-->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):


**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
kubernetes-1.6.0-beta.1

**Environment**:
- **Cloud provider or hardware configuration**:hardware configuration
- **OS** (e.g. from /etc/os-release): Ubuntu 14.04.1 LTS
- **Kernel** (e.g. `uname -a`): 4.2.0-27-generic
- **Install tools**: kubefed init
- **Others**:


**What happened**:
### I use kubefed init to init federation in kubernetes-1.6.0-beta.1.
kubefed init k8s-federation --host-cluster-context=cluster-cloud92  \
--api-server-service-type='NodePort'  --api-server-advertise-address='172.31.8.29' \
--dns-provider='coredns'  --dns-provider-config='/var/run/kubernetes/federation-codedns-config.conf'  --dns-zone-name='cluster-cloud92.com' \
--etcd-persistent-storage=false --kubeconfig='/var/run/kubernetes/kubeconfig'

### It show this error:
**Error from server (Forbidden): roles.rbac.authorization.k8s.io ""federation-system:federation-controller-manager"" is forbidden: attempt to grant extra privileges:** [{[get] [] [secrets] [] []} {[list] [] [secrets] [] []} {[watch] [] [secrets] [] []}] user=&{admin admin [system:authenticated] map[]} ownerrules=[{[create] [authorization.k8s.io] [selfsubjectaccessreviews] [] []} {[get] [] [] [] [/api /api/* /apis /apis/* /healthz /swaggerapi

### It created federation-apiserver successful, but failed create  federation-controller-manager.
kubectl get pod --namespace=federation-system 
NAME READY STATUS RESTARTS AGE
k8s-federation-apiserver-2603477136-7srv0 2/2 Running 0 1m
 
kubectl get secret --namespace=federation-system 
NAME TYPE DATA AGE
default-token-vwmcj kubernetes.io/service-account-token 2 2m
federation-controller-manager-token-vgsg8 kubernetes.io/service-account-token 2 2m
k8s-federation-apiserver-credentials Opaque 3 2m
k8s-federation-controller-manager-kubeconfig Opaque 1 2m

### Here is my k8s cluster configuration:
**cat /var/run/kubernetes/kubeconfig**
apiVersion: v1
kind: Config
clusters:
- cluster:
insecure-skip-tls-verify: true
server: https://172.31.8.29:443
name: cluster-cloud92
contexts:
- context:
cluster: cluster-cloud92 
user: cluster-cloud92 
name: cluster-cloud92
current-context: cluster-cloud92
users:
- name: cluster-cloud92
user:
token: tSjgxDGkhDe8iweWU2fWC7a64clc7MR8

**docker@bjkjy-ite-cloud92:~$ kubectl config view**
apiVersion: v1
clusters:
- cluster:
insecure-skip-tls-verify: true
server: https://172.31.8.29:443
name: cluster-cloud92
contexts:
- context:
cluster: cluster-cloud92
user: cluster-cloud92
name: cluster-cloud92
current-context: cluster-cloud92
kind: Config
preferences: {}
users:
- name: cluster-cloud92
user:
token: tSjgxDGkhDe8iweWU2fWC7a64clc7MR8

**cat /var/run/kubernetes/federation-codedns-config.conf**
[Global]
etcd-endpoints=http://172.31.8.29:2379
zones=cluster-cloud92.com


### I have enabled serviceaccount in kube-apiserver and specify --service_account_private_key_file in kibe-controller-manager.
I didn't use ca, just use token for kube-apiserver.--secure-port=443 --token-auth-file=/etc/kubernetes/token_auth_file.


**ps -ef | grep kube**
 /hyperkube federation-apiserver --admission-control=NamespaceLifecycle --advertise-address=172.31.8.29 --bind-address=0.0.0.0 --client-ca-file=/etc/federation/apiserver/ca.crt --etcd-servers=http://localhost:2379 --secure-port=443 --tls-cert-file=/etc/federation/apiserver/server.crt --tls-private-key-file=/etc/federation/apiserver/server.key


/opt/bin/kube-apiserver --insecure-bind-address=0.0.0.0 --insecure-port=8080 **--secure-port=443 --token-auth-file=/etc/kubernetes/token_auth_file** --etcd-servers=http://172.31.8.29:2379 --logtostderr=true --service-cluster-ip-range=172.31.96.0/22 --admission-control=NamespaceLifecycle,LimitRanger,ResourceQuota,DenyEscalatingExec,SecurityContextDeny,**ServiceAccount** --service-node-port-range=30000-32767 --v=4

/opt/bin/kube-controller-manager --master=127.0.0.1:8080 **--service_account_private_key_file=/var/run/kubernetes/apiserver.key** --leader-elect=true --pod-eviction-timeout=1m0s --logtostderr=true --v=2

 /opt/bin/kube-scheduler --master=127.0.0.1:8080 --leader-elect=true --logtostderr=true --v=2

/opt/bin/kubelet --address=0.0.0.0 --port=10250 --pod-infra-container-image=registry-dev.baiwei.baidu.com/google_containers/pause:latest --api-servers=http://172.31.8.29:8080 --logtostderr=true --cluster-dns=172.31.96.100 --cluster-domain=cluster-cloud92.com --hostname-override=bjkjy-ite-cloud92.bjkjy --resolv-conf=

 /opt/bin/kube-proxy --master=http://172.31.8.29:8080 --logtostderr=true

**kubectl get serviceaccount**
NAME      SECRETS   AGE
default   1         3d

### Question:
How to track and fix this issue？
How to config the host cluster for init federation about authorization? 
Do i must use ca for host cluster?


**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:


",closed,False,2017-10-31 06:13:31,2019-03-02 06:05:13
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/111,https://api.github.com/repos/kubernetes/federation/issues/111,[Federation] kubefed join error in fetching secret,"<a href=""https://github.com/mawentao02""><img src=""https://avatars2.githubusercontent.com/u/25891133?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mawentao02](https://github.com/mawentao02)**
_Tuesday Mar 07, 2017 at 03:10 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42618_

----

<!-- Thanks for filing an issue! Before hitting the button, please answer these questions.-->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):


**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
kubernetes-v1.6.0-alpha.1

**Environment**:
- **Cloud provider or hardware configuration**: hardware configuration
- **OS** (e.g. from /etc/os-release): Ubuntu 14.04.1 LTS
- **Kernel** (e.g. `uname -a`): 4.2.0-27-generic
- **Install tools**: federation-apiserver-deplyment.yaml,federation-controller-manager.yaml,kubefed join
- **Others**:


**What happened**:

### 1. I have started  federation apiserver and controller manager service in my host cluster.
**kubectl get pod --namespace=federation**
NAME                                             READY     STATUS    RESTARTS   AGE
federation-apiserver-1812305095-bx2lg            2/2       Running   0          6d
federation-controller-manager-3122363240-flg0l   1/1       Running   21         6d

### 2. Here is my host cluster kubeconfig
**kubectl config view**
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /var/run/kubernetes/ca.crt
    server: https://172.31.19.28:443
  name: cluster-cloud94
- cluster:
    insecure-skip-tls-verify: true
    server: https://172.31.19.28:30443
  name: federation-cluster
contexts:
- context:
    cluster: cluster-cloud94
    user: cluster-cloud94
  name: cluster-cloud94
- context:
    cluster: """"
    user: """"
  name: default
- context:
    cluster: federation-cluster
    user: federation-cluster
  name: federation-cluster
current-context: cluster-cloud94
kind: Config
preferences: {}
users:
- name: cluster-cloud94
  user:
    client-certificate: /var/run/kubernetes/server.crt
    client-key: /var/run/kubernetes/server.key
    token: mmcRicNslI1K2GdFYDneQO4Jy7hBenT4
- name: federation-cluster
  user:
    token: zWpSikg0DA77z6hWk3FVoXEUDnluE3Dp

### 3. Then I use kubefed join to add cluster-cloud94 into federation.
**kubefed join cluster-cloud94 --host-cluster-context=cluster-cloud94  --context=federation-cluster --federation-system-namespace=federation**
cluster ""cluster-cloud94"" created

### 4. It show cluster-cloud94 Unknow status.
**kubectl get cluster --context=federation-cluster**
NAME STATUS AGE
cluster-cloud94 Unknown 21h

**kubectl get secret --namespace=federation** 
NAME TYPE DATA AGE
cluster-cloud94 Opaque 1 6h

### 5. It show this error:
 **kubectl logs pod/federation-controller-manager-3122363240-6lcvj --namespace=federation**
I0307 02:40:36.798860       1 interface.go:138] Interface ""eth0"" has 2 addresses :[192.168.8.163/27 fe80::42:c0ff:fea8:8a3/64].
I0307 02:40:36.798886       1 interface.go:105] Checking addr  192.168.8.163/27.
I0307 02:40:36.798900       1 interface.go:114] IP found 192.168.8.163
I0307 02:40:36.798911       1 interface.go:144] valid IPv4 address for interface ""eth0"" found as 192.168.8.163.
I0307 02:40:36.798921       1 interface.go:254] Choosing IP 192.168.8.163 
E0307 02:40:36.798821       1 federated_informer.go:195] Cluster &Cluster{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:cluster-cloud94,GenerateName:,Namespace:,SelfLink:/apis/federation/v1beta1/clusterscluster-cloud94,UID:71c77f74-02df-11e7-a8fc-0242c0a803ea,ResourceVersion:1958,Generation:0,CreationTimestamp:2017-03-07 02:40:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,},Spec:ClusterSpec{ServerAddressByClientCIDRs:[{0.0.0.0/0 https://172.31.19.28:443}],SecretRef:&k8s_io_kubernetes_pkg_api_v1.LocalObjectReference{Name:cluster-cloud94,},},Status:ClusterStatus{Conditions:[],Zones:[],Region:,},} not added.  Not of correct type, or cluster not ready.
E0307 02:40:36.798913       1 federated_informer.go:195] Cluster &Cluster{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:cluster-cloud94,GenerateName:,Namespace:,SelfLink:/apis/federation/v1beta1/clusterscluster-cloud94,UID:71c77f74-02df-11e7-a8fc-0242c0a803ea,ResourceVersion:1958,Generation:0,CreationTimestamp:2017-03-07 02:40:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,},Spec:ClusterSpec{ServerAddressByClientCIDRs:[{0.0.0.0/0 https://172.31.19.28:443}],SecretRef:&k8s_io_kubernetes_pkg_api_v1.LocalObjectReference{Name:cluster-cloud94,},},Status:ClusterStatus{Conditions:[],Zones:[],Region:,},} not added.  Not of correct type, or cluster not ready.
E0307 02:40:36.798821       1 federated_informer.go:195] Cluster &Cluster{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:cluster-cloud94,GenerateName:,Namespace:,SelfLink:/apis/federation/v1beta1/clusterscluster-cloud94,UID:71c77f74-02df-11e7-a8fc-0242c0a803ea,ResourceVersion:1958,Generation:0,CreationTimestamp:2017-03-07 02:40:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,},Spec:ClusterSpec{ServerAddressByClientCIDRs:[{0.0.0.0/0 **https://172.31.19.28:443}],SecretRef:&k8s_io_kubernetes_pkg_api_v1.LocalObjectReference{Name:cluster-cloud94,},},Status:ClusterStatus{Conditions:[],Zones:[],Region:,},} not added.  Not of correct type, or cluster not ready.**
E0307 02:40:36.799022       1 federated_informer.go:195] Cluster &Cluster{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:cluster-cloud94,GenerateName:,Namespace:,SelfLink:/apis/federation/v1beta1/clusterscluster-cloud94,UID:71c77f74-02df-11e7-a8fc-0242c0a803ea,ResourceVersion:1958,Generation:0,CreationTimestamp:2017-03-07 02:40:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,},Spec:ClusterSpec{ServerAddressByClientCIDRs:[{0.0.0.0/0 https://172.31.19.28:443}],SecretRef:&k8s_io_kubernetes_pkg_api_v1.LocalObjectReference{Name:cluster-cloud94,},},Status:ClusterStatus{Conditions:[],Zones:[],Region:,},} not added.  Not of correct type, or cluster not ready.
W0307 02:40:36.800897       1 cluster_util.go:121] error in fetching secret: the server has asked for the client to provide credentials (get secrets cluster-cloud94)
W0307 02:40:37.802662       1 cluster_util.go:121] error in fetching secret: the server has asked for the client to provide credentials (get secrets cluster-cloud94)
**W0307 02:40:38.802529       1 cluster_util.go:121] error in fetching secret: the server has asked for the client to provide credentials (get secrets cluster-cloud94)**

## Question:
1. How to fixed this issue?
2.It seams caused by my host cluster secrets. I have configureted CA certification for host cluster, and can kubeconfig to access host cluster API, why it error to get secrets cluster-cloud94?
**kubectl --server=https://172.31.19.28:443 \
> --certificate-authority=/var/run/kubernetes/ca.crt \
> --client-certificate=/var/run/kubernetes/server.crt \
> --client-key=/var/run/kubernetes/server.key \
> get nodes** 
NAME                      STATUS    AGE       VERSION
bjkjy-ite-cloud94.bjkjy   Ready     25d       v1.6.0-alpha.1
bjkjy-ite-cloud95.bjkjy   Ready     25d       v1.6.0-alpha.1


**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):
## For Federation Configuration:
## I use federation yaml file to start federation in my bare metal ubuntu system.
## 1. Create federation nodeport service
**cat federation-apiserver-cluster-service.yaml**  
apiVersion: v1
kind: Service
metadata:
  name: federation-apiserver 
  namespace: federation
  labels:
    app: federated-cluster
spec:
  type: NodePort
  selector:
    app: federated-cluster
    module: federation-apiserver
  ports:
    - name: https
      protocol: TCP
      nodePort: 30443
      port: 443

**kubectl get svc --namespace=federation**
NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
federation-apiserver   172.31.99.184   <nodes>       443:30443/TCP   25d

## 2. Create federation-apiserver-deployment
### 2.1 create token and secret for federation apiserver
**cat /srv/kubernetes/known-tokens.csv**
zWpSikg0DA77z6hWk3FVoXEUDnluE3Dp,admin,admin

**kubectl create secret generic federation-apiserver-secrets --from-file=/srv/kubernetes/known-tokens.csv --namespace=federation** 

 **kubectl describe secrets federation-apiserver-secrets --namespace=federation**
Name:           federation-apiserver-secrets
Namespace:      federation
Labels:         <none>
Annotations:    <none>

Type:   Opaque

Data
====
known-tokens.csv:       45 bytes

2.2 Deploy federation apiserver
 **cat federation-apiserver-deployment.yaml**
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: federation-apiserver
  namespace: federation
  labels:
    app: federated-cluster
spec:
  template:
    metadata:
      name: federation-apiserver
      labels:
        app: federated-cluster
        module: federation-apiserver
    spec:
      containers:
      - name: apiserver
        image: registry-dev.baiwei.baidu.com/baidu_containers/federation/hyperkube-amd64:v1.6.0-alpha.1
        command:
        - /hyperkube
        - federation-apiserver
        - --bind-address=0.0.0.0
        - --etcd-servers=http://localhost:2379
        - --secure-port=443 
        - --advertise-address=172.31.99.184
        - --admission-control=AlwaysAdmit
        - --token-auth-file=/srv/kubernetes/known-tokens.csv
        - --anonymous-auth=true
        ports:
        - containerPort: 443
          name: https
        - containerPort: 8080
          name: local
        volumeMounts:
        - name: federation-apiserver-secrets
          mountPath: /srv/kubernetes/
          #readOnly: true
      - name: etcd
        image: registry-dev.baiwei.baidu.com/baidu_containers/federation/etcd:v3.1.0
        command:
          - /usr/local/bin/etcd
          - --data-dir
          - /var/etcd/data
      volumes:
      - name: federation-apiserver-secrets
        secret:
          secretName: federation-apiserver-secrets

### 2.3 federation apiserver can start successful.
**kubectl logs pod/federation-apiserver-1812305095-bx2lg apiserver --namespace=federation**  
I0111 07:48:02.649714       1 config.go:498] Will report 172.31.99.184  as public IP address.
I0111 07:48:03.394698       1 config.go:427] Generated self-signed cert (/var/run/kubernetes/apiserver.crt, /var/run/kubernetes/apiserver.key)
[restful] 2017/01/11 07:48:03 log.go:30: [restful/swagger] listing is available at https:// 172.31.99.195:443/swaggerapi/
[restful] 2017/01/11 07:48:03 log.go:30: [restful/swagger] https://172.31.99.184:443/swaggerui/ is mapped to folder /swagger-ui/
I0111 07:48:03.450400       1 serve.go:93] Serving securely on 0.0.0.0:443
I0111 07:48:03.450481       1 serve.go:107] Serving insecurely on 127.0.0.1:8080

## 3. Deploy the Federated Controller Manager
### 3.1 I use coredns as cloud provider and start coredns on host cluster
**kubectl get svc**
NAME           CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
coredns        172.31.96.10    <none>        53/UDP,53/TCP   19d

 **cat /etc/federation/coredns/federation-coredns-config**
[Global]
etcd-endpoints = http://172.31.19.28:2379
zones = cluster01.com 

### 3.2 create secret with federation kubeconfig
**cat /etc/federation/federation-apiserver/kubeconfig**
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://172.31.19.28:30443
  name: federation-cluster
contexts:
- context:
    cluster: """"
    user: """"
  name: default
- context:
    cluster: federation-cluster
    user: federation-cluster
  name: federation-cluster
current-context: federation-cluster
kind: Config
preferences: {}
users:
- name: federation-cluster
  user:
    token: zWpSikg0DA77z6hWk3FVoXEUDnluE3Dp

3.3 Deploy federation-controller-manager
**cat federation-controller-manager-deployment.yaml** 
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: federation-controller-manager 
  namespace: federation 
  labels:
    app: federated-cluster
spec:
  template:
    metadata:
      name: federation-controller-manager
      labels:
        app: federated-cluster
        module: federation-controller-manager
    spec:
      volumes:
      - name: ssl-certs
        hostPath:
          path: /etc/ssl/certs
      - name: kubeconfig
        secret:
          secretName: federation-apiserver-kubeconfig
      - name: dns-provider-config
        secret:
          secretName: federation-dnsprovider-config
      containers:
      - name: controller-manager
        volumeMounts:
        - name: ssl-certs
          readOnly: true
          mountPath: /etc/ssl/certs
        - name: kubeconfig
          readOnly: true
          mountPath: ""/etc/federation/federation-apiserver""
        - name: dns-provider-config
          readOnly: true
          mountPath: ""/etc/federation/coredns""
        image: registry-dev.baiwei.baidu.com/baidu_containers/federation/hyperkube-amd64:v1.6.0-alpha.1 
        command:
        - /hyperkube
        - federation-controller-manager
        **- --master=https://172.31.19.28:30443**
        - --kubeconfig=/etc/federation/federation-apiserver/kubeconfig
        **- --dns-provider=coredns
        - --dns-provider-config=/etc/federation/coredns/federation-coredns-config**
        - --federation-name=federation
        - --v=4
        - --zone-name=cluster01.com
        ports:
        - containerPort: 443
          name: https
        - containerPort: 8080
          name: local
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace

### 3.4 federation components start successful.
**kubectl get pod --namespace=federation**
NAME                                             READY     STATUS    RESTARTS   AGE
federation-apiserver-1812305095-bx2lg            2/2       Running   0          6d
federation-controller-manager-3122363240-flg0l   1/1       Running   21         6d

**Anything else we need to know**:

",closed,False,2017-10-31 06:14:09,2018-07-24 00:15:53
federation,marun,https://github.com/kubernetes/federation/pull/112,https://api.github.com/repos/kubernetes/federation/issues/112,Enable build and test,Dependencies vendored with [glide](https://glide.readthedocs.io/en/latest/).,closed,True,2017-11-01 00:08:27,2017-11-03 12:15:53
federation,shashidharatd,https://github.com/kubernetes/federation/pull/113,https://api.github.com/repos/kubernetes/federation/issues/113,wip: test pre-submit e2e job,This pr is just to test if the federation pre-submit e2e job works fine.,closed,True,2017-11-03 03:12:53,2017-11-03 13:15:58
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/114,https://api.github.com/repos/kubernetes/federation/issues/114,Updates to make federation e2e CI jobs work ,"Updates to ensure e2e jobs can run ok on this repo.
cc @shashidharatd @marun 
cc @kubernetes/sig-multicluster-pr-reviews ",closed,True,2017-11-03 13:11:32,2017-11-15 06:54:47
federation,shashidharatd,https://github.com/kubernetes/federation/pull/115,https://api.github.com/repos/kubernetes/federation/issues/115,Enable running e2e tests on federation from k/f repo,"This PR adds the get-federation.sh scripts equivalent to get-kube.sh. These are needed by the `kubetest` in test-infra to download and run the federation binaries.
This PR also pushes fcp image to registry before bringing up federation.

/assign @irfanurrehman  ",closed,True,2017-11-06 04:46:50,2017-11-06 06:21:28
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/116,https://api.github.com/repos/kubernetes/federation/issues/116,[Federation] Refactor the parameter lists in kubefed to use parameter-grouping structs,"<a href=""https://github.com/perotinus""><img src=""https://avatars1.githubusercontent.com/u/5122391?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [perotinus](https://github.com/perotinus)**
_Wednesday Mar 08, 2017 at 18:46 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42743_

----

The parameter lists in kubefed are getting unwieldy, and making it harder to update and understand them. We should figure out a good set of parameter objects that make it easer to follow the data flow.

I think this can wait until the outstanding kubefed PRs are merged in, in order to avoid a string of rebases.
",closed,False,2017-11-06 18:13:22,2018-07-13 19:15:13
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/117,https://api.github.com/repos/kubernetes/federation/issues/117,[Federation][kubefed] Not possible to disable a controller via --controllermanager-arg-overrides,"<a href=""https://github.com/marun""><img src=""https://avatars3.githubusercontent.com/u/451477?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [marun](https://github.com/marun)**
_Wednesday Mar 08, 2017 at 23:25 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42761_

----

#42375 promised to support disabling any of the federation controllers.  However, due to the way ``kubefed init`` marshalls overrides (splits on commas), multi-valued overrides (separated by commas) are not supported.  This means that only a single controller can be disabled. 

Support for multi-valued arg overrides was [discussed](https://reviewable.kubernetes.io/reviews/kubernetes/kubernetes/40917#-Kc5FeEWRiVgtRN8Vv9t) on #40917.

cc: @kubernetes/sig-federation-bugs 
",closed,False,2017-11-06 18:13:30,2018-07-13 19:15:12
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/118,https://api.github.com/repos/kubernetes/federation/issues/118,Introduce a kubefed flag to allow time outs,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Wednesday Mar 15, 2017 at 05:14 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/43123_

----

Today `kubefed` waits infinitely for its operations to complete. This is suboptimal. Our tests, for example, take about 15 hours before the overall timeout. Users probably face the same situation. We should introduce a hidden flag to allow time outs in tests for now and then promote it to a real flag and document it.

This is a sub-issue of issue #41725.

cc @kubernetes/sig-federation-feature-requests @csbell 
",closed,False,2017-11-06 18:14:08,2018-07-14 12:32:10
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/119,https://api.github.com/repos/kubernetes/federation/issues/119,[Federation] ClustersSynced(clusters [] *federationapi.Clusters) documentation doesn't seem to match the implementation,"<a href=""https://github.com/perotinus""><img src=""https://avatars1.githubusercontent.com/u/5122391?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [perotinus](https://github.com/perotinus)**
_Wednesday Mar 15, 2017 at 21:58 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/43169_

----

It appears that there is a bit of an impedance mismatch between how the FederatedInformer discusses ClustersSynced(clusters [] *federationapi.Clusters) and the actual implementation of that method. Conceptually, the idea of ClustersSynced(clusters [] *federationapi.Clusters) seems to be to provide an ongoing update of whether the Federated store is synchronized with the underlying clusters' stores. In practice, it does this by checking the HasSynced() method on each of the clusters' informers. HasSynced() only promises to return whether, at some point in the past, the informer was synced, and from the implementation it appears that it does this by checking that the queue has been populated with some Add/Update/Delete operations, or was populated with some Replace operations that completed. Interestingly, if there is an in-flight Add, Update or Delete operation, HasSynced() appears to return true before that operation has completed.

So, ClustersSynced(clusters [] *federationapi.Clusters) doesn't quite provide the depth of information that it implies in its name and its documentation. Note that the ClustersSynced() method on FederationView does note this in the comment.

It may be worth renaming these to HaveClustersBeenSynced() or some such to make it clear that they do not provide information about the current sync state, but only that at some point they were synced (or at some point were slated to be synced, which seems to be the promise that is provided).jk

Someone else may want to look into this code to corroborate these findings.
",closed,False,2017-11-06 18:14:21,2018-07-24 01:16:23
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/120,https://api.github.com/repos/kubernetes/federation/issues/120,install RBAC api in federation-apiserver,"<a href=""https://github.com/zhouhaibing089""><img src=""https://avatars3.githubusercontent.com/u/4516999?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [zhouhaibing089](https://github.com/zhouhaibing089)**
_Tuesday Mar 21, 2017 at 07:47 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/43433_

----

RBAC has been in beta since v1.6, and I think we should enable this apigroup in federation-apiserver, too.

Per this document: https://docs.google.com/document/d/1O2SEr_TDtgXzndh5lHPuAZmT-haylB9NaBRSo4DsUdk/edit#, RBAC was not enabled due to that it was still alpha at that time.

cc @nikhiljindal 
",closed,False,2017-11-06 18:14:29,2018-07-05 03:51:45
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/121,https://api.github.com/repos/kubernetes/federation/issues/121,Cluster IP update is not carried to federation-controller,"<a href=""https://github.com/henriquetruta""><img src=""https://avatars2.githubusercontent.com/u/2501482?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [henriquetruta](https://github.com/henriquetruta)**
_Tuesday Mar 21, 2017 at 14:16 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/43455_

----

<!-- Thanks for filing an issue! Before hitting the button, please answer these questions.-->
**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
Bug report 

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.2"", GitCommit:""08e099554f3c31f6e6f07b448ab3ed78d0520507"", GitTreeState:""clean"", BuildDate:""2017-01-12T04:57:25Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.3"", GitCommit:""029c3a408176b55c30846f0faedf56aae5992e9b"", GitTreeState:""clean"", BuildDate:""2017-02-15T06:34:56Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}


**Environment**:
- **Cloud provider or hardware configuration**: GCE (federation control plane) + Cluser on-prem

**What happened**:
I had a federation running, with the control plane at GCE and a local on-prem cluster, running in a VM. Then, I had to change the port my local api-server was running. I've updated the cluster file and ran `kubetl --context=federation apply -f cluster.yaml`. The object was properly updated but the `federation-controller-manager` started to fail, because it was still trying to reach in the old port.

**What you expected to happen**:
`federation-controller-manager` to be refreshed at some point, updating the cluster port. I had to delete and recreate the local cluster

**How to reproduce it** (as minimally and precisely as possible):
Create a federation on GCE, join another cluster and change some cluster information. In my case I needed to change the port, so I updated the cluster object.

**Anything else we need to know**:
How I made my federation setup: https://github.com/henriquetruta/hybrid-k8s-federation/

",closed,False,2017-11-06 18:14:45,2018-07-05 16:03:44
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/122,https://api.github.com/repos/kubernetes/federation/issues/122,Prevent users from deleting pods serving live traffic,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Wednesday Mar 22, 2017 at 00:01 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/43487_

----

Problem:
Users today can run `kubectl delete rs` that does cascading deletion by default and hence will also end up deleting all the pods, even though they might have been serving live traffic.
The problem is even more severe in the case of federation since the deletion impacts multiple clusters and can cause global outages.

One possible solution:
When user creates an ingress (or service) targeting an rs, we should add a finalizer to the backend pods for that ingress to ensure that they cant be deleted unless the ingress is deleted.
Once user deletes the ingress, they can go ahead and delete the rs.

cc @quinton-hoole @bgrant0607 @kubernetes/sig-federation-feature-requests 
",closed,False,2017-11-06 18:14:53,2018-12-19 12:59:48
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/123,https://api.github.com/repos/kubernetes/federation/issues/123,[Federation] Print out namespace on each line during Federation E2E tests,"<a href=""https://github.com/perotinus""><img src=""https://avatars1.githubusercontent.com/u/5122391?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [perotinus](https://github.com/perotinus)**
_Thursday Mar 23, 2017 at 22:59 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/43596_

----

Now that the Federation E2E tests are running in parallel, the logs from the control manager and the API server are very difficult to understand because of the number of interleaved operations. We should add the namespace (or, more specifically, the five-character unique suffix that we add to the namespace template we use for tests) to each of the log statements so that we can grep through the logs and make it easier to follow what's going on.

In theory, we may be able to do this in the Federation version of the test framework library that does logging.

cc @csbell @nikhiljindal
",closed,False,2017-11-06 18:15:15,2018-07-02 18:55:43
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/124,https://api.github.com/repos/kubernetes/federation/issues/124,[Federation] Make sure that all documentation about Federated services explains that you need to own the relevant domain name,"<a href=""https://github.com/perotinus""><img src=""https://avatars1.githubusercontent.com/u/5122391?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [perotinus](https://github.com/perotinus)**
_Friday Mar 24, 2017 at 01:01 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/43599_

----

We should make sure that all documentation about Federated services/setting up DNS for Federation explains that you must own the domain name. We do mention this in our documentation, but it's an easy thing to miss and it can be very tricky to diagnose if you don't know that you need to do it.

It may be worth noting as well that, if you don't care about access from the public Internet and your provider supports it, you can use private DNS zones.
",closed,False,2017-11-06 18:15:23,2018-07-02 18:55:44
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/125,https://api.github.com/repos/kubernetes/federation/issues/125,[Federation] Service Controller DNS handling is broken,"<a href=""https://github.com/csbell""><img src=""https://avatars1.githubusercontent.com/u/23373339?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [csbell](https://github.com/csbell)**
_Friday Mar 24, 2017 at 21:54 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/43646_

----

This has surfaced in Ingress and Service e2e tests that manipulate DNS entries. The service controller queries the managed dns entry, thinks that the CNAME doesn't exist and adds a new one. However, clouddns says (through a 409) that the entry already exists. Error message aside, the control flow within the service controller's endpoint_helper prevents a service from being deleted.

It's unclear at this point whether there's a race condition. I doubt it given the amount of logging that happens showing that there is no concurrent CNAME entry being created.

cc @shashidharatd Have you seen this in your refactoring?

```
I0324 13:26:29.743] I0324 20:20:47.452653       1 dns.go:191] No recordsets found for DNS name ""federated-service.e2e-tests-federated-service-r4281.e2e-federation.svc.us-central1-b.us-central1.test-f8n.k8s.io."".  Need to add either A records (if we have healthy endpoints), or a CNAME record to ""federated-service.e2e-tests-federated-service-r4281.e2e-federation.svc.us-central1.test-f8n.k8s.io.""
I0324 13:26:29.743] I0324 20:20:47.452694       1 dns.go:193] There are no healthy endpoint addresses at level ""federated-service.e2e-tests-federated-service-r4281.e2e-federation.svc.us-central1-b.us-central1.test-f8n.k8s.io."", so CNAME to ""federated-service.e2e-tests-federated-service-r4281.e2e-federation.svc.us-central1.test-f8n.k8s.io."", if provided
I0324 13:26:29.743] I0324 20:20:47.452713       1 dns.go:195] Creating CNAME to ""federated-service.e2e-tests-federated-service-r4281.e2e-federation.svc.us-central1.test-f8n.k8s.io."" for ""federated-service.e2e-tests-federated-service-r4281.e2e-federation.svc.us-central1-b.us-central1.test-f8n.k8s.io.""
I0324 13:26:29.743] I0324 20:20:47.452735       1 dns.go:197] Adding recordset <(clouddns) ""federated-service.e2e-tests-federated-service-r4281.e2e-federation.svc.us-central1-b.us-central1.test-f8n.k8s.io."" type=CNAME rrdatas=[""federated-service.e2e-tests-federated-service-r4281.e2e-federation.svc.us-central1.test-f8n.k8s.io.""] ttl=180>
I0324 13:26:29.743] I0324 20:20:47.500162       1 endpoint_helper.go:182] Error ensuring DNS Records: googleapi: Error 409: The resource 'entity.change.additions[0]' named 'federated-service.e2e-tests-federated-service-r4281.e2e-federation.svc.us-central1-b.us-central1.test-f8n.k8s.io. (CNAME)' already exists, alreadyExists
```
",closed,False,2017-11-06 18:15:31,2018-07-02 17:54:42
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/126,https://api.github.com/repos/kubernetes/federation/issues/126,federation: alpha APIs should be disabled by default,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Monday Apr 10, 2017 at 19:01 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/44291_

----

Forked from https://github.com/kubernetes/kubernetes/issues/38593.

Now that we have flags to disable specific APIs (https://github.com/kubernetes/kubernetes/issues/38593), we should use them to disable alpha APIs by default.

cc @kubernetes/sig-federation-bugs 
",closed,False,2017-11-06 18:15:40,2018-02-02 16:19:01
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/127,https://api.github.com/repos/kubernetes/federation/issues/127,ci-kubernetes-e2e-gce-federation-serial suite is broken ,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Tuesday Apr 11, 2017 at 18:48 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/44354_

----

This suite is broken just because of one test - `[k8s.io] Federation apiserver [Feature:Federation] Cluster objects [Serial] should be created and deleted successfully`. This is broken because we join the clusters to federation while bringing up e2e federation control plane and we try to repeat the same operation in the test. 

The simplest fix is to use a different name for the cluster object.
",closed,False,2017-11-06 18:15:48,2018-02-12 12:21:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/128,https://api.github.com/repos/kubernetes/federation/issues/128,federation: federation-controller-manager is restarted during init,"<a href=""https://github.com/shashidharatd""><img src=""https://avatars3.githubusercontent.com/u/8734900?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [shashidharatd](https://github.com/shashidharatd)**
_Friday Apr 14, 2017 at 07:46 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/44490_

----

After bringing up federation control plane, most of the time we do see that federation-controller-manager pod is restarted like below:
```
root@s70553-HP-EliteBook-840-G2:~/gopath/src/k8s.io/kubernetes# kubectl --namespace=federation-system get pods
NAME                                            READY     STATUS    RESTARTS   AGE
federation-apiserver-832758761-z1k2p            2/2       Running   0          54s
federation-controller-manager-105127570-15bpq   1/1       Running   1          54s
```
Although there is no issues afterwards, this is just causing few seconds delay in federation-controller-manager becoming operational.

cc @kubernetes/sig-federation-bugs 
",closed,False,2017-11-06 18:15:57,2018-07-05 16:03:43
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/129,https://api.github.com/repos/kubernetes/federation/issues/129,federation: Support creating resources only in federation control plane,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Tuesday Apr 18, 2017 at 19:28 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/44631_

----

Filing this to converge the same discussion from multiple places.

Problem: Today, when a resource in created in federation, it is automatically spread to all underlying clusters.
https://github.com/kubernetes/kubernetes/issues/29887 is for restricting the clusters to which a federation resource is spread.

There is also a use case to not spread the resource to underlying clusters at all (keep it in federation control plane only). Ref https://github.com/kubernetes/kubernetes/issues/41673, https://github.com/kubernetes/kubernetes/issues/44283.

We have 2 options:
* Use the cluster selector proposal (that is intended to restrict the clusters where federation resource is spread) to also restricting the resource to federation control plane only. This can be done by choosing a cluster label selector `federation.kubernetes.io/federation-control-plane=true`. None of the clusters will have this label and hence the resource will remain in federation control plane.
* Use an explicit way (annotion for now, field later) to indicate federation-only.

I think we all agree to use 1 atleast for now. Once we have a more concrete path from annotation to field for our federation specific fields, we can look at this again.

Documenting this here to make sure everyone is on the same page.

cc @kubernetes/sig-federation-feature-requests 
",closed,False,2017-11-06 18:16:10,2018-07-23 22:13:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/130,https://api.github.com/repos/kubernetes/federation/issues/130,Feature Ask: Azure Cloud DNS Provider,"<a href=""https://github.com/jpoon""><img src=""https://avatars3.githubusercontent.com/u/85374?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [jpoon](https://github.com/jpoon)**
_Monday Apr 24, 2017 at 19:31 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/44874_

----

<!-- Thanks for filing an issue! Before hitting the button, please answer these questions.-->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):


**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): Feature Request

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.1"", GitCommit:""b0b7a323cc5a4a2019b2e9520c21c7830b7f708e"", GitTreeState:""clean"", BuildDate:""2017-04-03T23:37:30Z"", GoVersion:""go1.8"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.3"", GitCommit:""029c3a408176b55c30846f0faedf56aae5992e9b"", GitTreeState:""clean"", BuildDate:""2017-02-15T06:34:56Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Azure
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04.2
- **Kernel** (e.g. `uname -a`): Linux k8s-master-30143214-0 4.4.0-66-generic #87-Ubuntu SMP Fri Mar 3 15:29:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**: ACS
- **Others**:

As of 1.6, the current DNS providers are [AWS (Route53), CoreDNS, Google, (CloudDNS)](https://github.com/kubernetes/kubernetes/tree/master/federation/pkg/dnsprovider/providers). The ask is to implement a DNS provider for Azure.

cc @xtophs @colemickens 

",closed,False,2017-11-06 18:16:38,2018-07-03 02:02:43
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/131,https://api.github.com/repos/kubernetes/federation/issues/131,Federated Ingress fails at high loads,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Tuesday Apr 25, 2017 at 07:28 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/44907_

----

I filed an issue with some debugging information and details in the Ingress repo -  https://github.com/kubernetes/ingress/issues/645. I am also filing it to here to track federated ingress flakiness. This is critical and is blocking us from making federation presubmits ""required"" for all PRs.

@csbell assigning it to you for triaging.

/assign @csbell 

/priority failing-test

/sig federation

cc @kubernetes/sig-federation-bugs 
",closed,False,2017-11-06 18:16:48,2018-07-02 16:53:44
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/132,https://api.github.com/repos/kubernetes/federation/issues/132,Feature Request: API should return cluster name,"<a href=""https://github.com/tback""><img src=""https://avatars0.githubusercontent.com/u/83432?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [tback](https://github.com/tback)**
_Wednesday Apr 26, 2017 at 13:41 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/44954_

----

<!-- Thanks for filing an issue! Before hitting the button, please answer these questions.-->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No


**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
api cluster name, api get cluster name, cluster name programatically 

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FEATURE REQUEST

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->
I want to get the name of the current kubernetes cluster from the api so I can make it available in helm. My usecase is, that I want to annotate automatic backups with the clustername so I don't get name conflicts.


",open,False,2017-11-06 18:17:16,2018-10-08 08:27:36
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/133,https://api.github.com/repos/kubernetes/federation/issues/133,Design Proposal: Federated Job: Cross-cluster task rebalancing,"<a href=""https://github.com/quinton-hoole""><img src=""https://avatars0.githubusercontent.com/u/10390785?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [quinton-hoole](https://github.com/quinton-hoole)**
_Monday May 01, 2017 at 21:23 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/45187_

----

Figure out how best to do task rebalancing across clusters for Federated Jobs.

See #36197 for background.

@jianhuiz leads this effort.  Assigning myself as proxy, as he is not part of the K8s org yet. 

@kubernetes/sig-federation-feature-requests 
",closed,False,2017-11-06 18:17:45,2018-10-05 03:33:09
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/134,https://api.github.com/repos/kubernetes/federation/issues/134,Support federation specific fields in API,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Monday May 01, 2017 at 22:29 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/45193_

----

This has come up many times.

Some options we have discussed:
* federation should define its own API rather than using kubernetes's API
* Try to generalize the federation fields so that they make sense to be added to kubernetes API
* Add an extension object that can be binded to a kubernetes object. The extension object will contain federation specific fields.

cc @kubernetes/sig-federation-feature-requests 


",closed,False,2017-11-06 18:17:54,2018-07-02 15:52:42
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/135,https://api.github.com/repos/kubernetes/federation/issues/135,[Federation] Ingress should detect NodePort mistaches,"<a href=""https://github.com/csbell""><img src=""https://avatars1.githubusercontent.com/u/23373339?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [csbell](https://github.com/csbell)**
_Tuesday May 09, 2017 at 01:35 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/45520_

----

The documentation for federated ingress [here](https://kubernetes.io/docs/tasks/administer-federation/ingress/#adding-backend-services-and-pods) says:

> Note that in order for your federated ingress to work correctly on Google Cloud, the node ports of all of the underlying cluster-local services need to be identical. If you’re using a federated service this is easy to do. Simply pick a node port that is not already being used in any of your clusters, and add that to the spec of your federated service. If you do not specify a node port for your federated service, each cluster will choose it’s own node port for its cluster-local shard of the service, and these will probably end up being different, which is not what you want.

For services with non-matching nodeports, we should be generating events for mismatching NodePorts instead of following through with the ingress creation.
",closed,False,2017-11-06 18:18:05,2018-07-23 21:12:57
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/136,https://api.github.com/repos/kubernetes/federation/issues/136,federated deployments can't handle app label properly,"<a href=""https://github.com/henriquetruta""><img src=""https://avatars2.githubusercontent.com/u/2501482?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [henriquetruta](https://github.com/henriquetruta)**
_Thursday May 11, 2017 at 18:08 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/45673_

----

Creating two federated deployments with the same app label (nginx in my case) will cause the deployments to have wrong spreading of replicas. This doesn't happen in the regular k8s api server. 

How to reproduce:
1 - Create a deployment. I've used this template
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment-fed
spec:
  replicas: 6
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```
2 - Check on the 2 underlying clusters if the replicas are spread 50-50 
```
root@henrique-fed-1:~/run_app# k get deploy --context=fed
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment-fed   6         6         0            6           28s
root@henrique-fed-1:~/run_app# k get deploy --context=kubernetes1
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment-fed       3         3         3            3           26s
root@henrique-fed-1:~/run_app# k get deploy --context=kubernetes2
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment-fed   3         3         3            3           1m
```
3 - Create an equal deployment just changing the name, keeping the same `app` label
4 - The second deployment replicas were not correctly spread, being all in the first cluster:
```
root@henrique-fed-1:~/run_app# k get deploy --context=fed
NAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment-fed     6         6         0            6           1m
nginx-deployment-fed-2   6         6         0            6           22s
root@henrique-fed-1:~/run_app# k get deploy --context=kubernetes1
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment-fed       3         3         3            3           1m
nginx-deployment-fed-2     6         6         6            6           19s
root@henrique-fed-1:~/run_app# k get deploy --context=kubernetes2
NAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment-fed     3         3         3            3           2m
nginx-deployment-fed-2   0         0         0            0           1m
``` 

In the end, I've created another deploy, (nginx-deployment-fed-3) changing the name and the app label, and it was correctly spread:
```
root@henrique-fed-1:~/run_app# k get deploy --context=kubernetes1
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment-fed       3         3         3            3           3m
nginx-deployment-fed-2     6         6         6            6           1m
nginx-deployment-fed-3     3         3         3            0           1s
root@henrique-fed-1:~/run_app# k get deploy --context=kubernetes2
NAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment-fed     3         3         3            3           4m
nginx-deployment-fed-2   0         0         0            0           2m
nginx-deployment-fed-3   3         3         3            3           1m
```
Expected:
Replicas spread in all deployments, even with the same app label

*Kubernetes version** (use `kubectl version`): 1.6.2
Running on-prem federation in some VMs, 2 clusters in the federation
Also happened in GKE fed
",closed,False,2017-11-06 18:18:16,2018-07-24 01:16:25
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/137,https://api.github.com/repos/kubernetes/federation/issues/137,[Federation] Running Federation E2E tests locally with multiple clusters fails if the user's context is not set to the E2E test host cluster context,"<a href=""https://github.com/perotinus""><img src=""https://avatars1.githubusercontent.com/u/5122391?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [perotinus](https://github.com/perotinus)**
_Friday May 12, 2017 at 22:06 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/45751_

----

I was running into an issue running the Federation e2e tests with >1 cluster: when starting to run the tests, the test script would fail to list the nodes in the cluster hosting the federation with an ""x509 invalid certificate"" error. It worked fine with one cluster, however. @madhusudancs and I debugged this, and realized that the e2e script was using my local context to try to interact with the host cluster's API server. Because my E2E_ZONES environment variable had the clusters in ""host non-host"" order, the scripts to bring up the clusters would set the context to ""non-host"" on my machine, which meant that the script was using the wrong credentials to access the host cluster. I was able to fix this by setting my context to ""host"".

Ideally, the scripts should not be affected by the local kubecontext state in this way, and should always use the correct credentials regardless of the state of the local kubecontext.

",closed,False,2017-11-06 18:18:31,2018-07-02 14:51:46
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/138,https://api.github.com/repos/kubernetes/federation/issues/138,Federation: Handling deviations from k8s service registry,"<a href=""https://github.com/shashidharatd""><img src=""https://avatars3.githubusercontent.com/u/8734900?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [shashidharatd](https://github.com/shashidharatd)**
_Monday May 15, 2017 at 08:00 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/45812_

----

This issue is to continue the discussion in #45798, The fix in the PR is temporary and would be reverted as soon as the permanent solution is arrived at.

We want to implement the defaulting `ExternalTrafficPolicy` logic for Service in registry, (for both create and update). We at federation are currently not using the same set of Rest functions for Service. This was agreeable as some of the logic implemented in k8s registry for Service(cluster-ip allocator, NodePort allocator,  NodePort vallidity check) may not be applicable for federated services. Now going forward federation registry also should have some logic implemented which is similar to k8s registry.

@nikhiljindal, request to provide some inputs on this about the right approach.

cc @kubernetes/sig-federation-bugs, @MrHohn @madhusudancs
",closed,False,2017-11-06 18:18:41,2018-07-02 14:51:44
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/139,https://api.github.com/repos/kubernetes/federation/issues/139,[Federation] Replacing deployment definition does not evenly distribute replicas across clusters,"<a href=""https://github.com/ajwootto""><img src=""https://avatars2.githubusercontent.com/u/1740731?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [ajwootto](https://github.com/ajwootto)**
_Friday May 19, 2017 at 18:16 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/46119_

----

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): federation, deployment, replace

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):


**Kubernetes version** (use `kubectl version`): 1.6.2


**Environment**:
- **Cloud provider or hardware configuration**: GCP/GKE

**What happened**:
With a three cluster federation and a deployment with 16 replicas, replacing the deployment using
`kubectl replace -f deployment.yaml` 
results in all the updated pods going to one cluster rather than being evenly distributed.

**What you expected to happen**:
The pods should be evenly distributed (especially given the fact that they were correct before the replace operation)

**How to reproduce it** (as minimally and precisely as possible):
1) Create a deployment resource in the federation control plane (which is managing multiple underlying clusters)
2) Scale the deployment (mine is set to 16). Pods should then be evenly distributed, between 4-6 in each cluster
3) Replace the deployment with an updated version (new image, etc.) using 
`kubectl replace -f deployment.yaml`
4) Watching the list of pods in each cluster, I see the pods starting to be replaced correctly, but at some point the newly updated running pods will all get terminated in two of the clusters and everything will be moved to the third cluster.

**Anything else we need to know**:

Full deployment spec:
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: users
spec:
  replicas: 16
  template: # create pods using pod definition in this template
    metadata:
      annotations:
        federation.kubernetes.io/deployment-preferences: |
          {
            ""rebalance"": true,
            ""clusters"": {
              ""us-central"": {
                  ""minReplicas"": 3
              },
              ""eu-west"": {
                  ""minReplicas"": 3
              },
              ""asia-se"": {
                  ""minReplicas"": 3
              }
            }
          }
      labels:
        app: users
    spec:
      containers:
        - name: users
          image: gcr.io/myimage
          ports:
          - containerPort: 8000
          env:
            - name: GO_ENV
              valueFrom:
                configMapKeyRef:
                  name: environment
                  key: GO_ENV
            - name: REDIS_CLUSTER
              valueFrom:
                configMapKeyRef:
                  name: environment
                  key: user_redis 
            - name: DISABLE_REDIS
              value: ""false""
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8000
            periodSeconds: 1
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 10
---
apiVersion: v1
kind: Service
metadata:
  name: users
spec:
  selector:
    app: users
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
  type: NodePort
```

Despite the federation annotation that should require a minimum of 3 pods (and allow rebalancing) the state remains incorrect with no hosted pods in two of the clusters.

If I scale the replicas down to zero using 
`kubectl scale --replicas 0 deployment users`
and wait long enough for all the user pods to be killed, then scale back up to 16, they will be correctly distributed between the three clusters. Thusfar I've had to do this every time I update the image for the deployment.
",closed,False,2017-11-06 18:18:56,2018-09-07 01:56:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/140,https://api.github.com/repos/kubernetes/federation/issues/140,[Federation] Document permissions that the Federation control plane requires in joined clusters,"<a href=""https://github.com/perotinus""><img src=""https://avatars1.githubusercontent.com/u/5122391?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [perotinus](https://github.com/perotinus)**
_Friday May 19, 2017 at 20:21 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/46131_

----

We need to document what permissions the Federation control plane needs in order to correctly function. After #42042 `kubefed` will create a service account that has very broad permissions, but we should document what are the tightest permissions necessary in order for the FCP to run.
",closed,False,2017-11-06 18:19:19,2018-07-02 14:51:45
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/141,https://api.github.com/repos/kubernetes/federation/issues/141,federation services: known issues,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Monday May 22, 2017 at 19:18 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/46240_

----

Documenting the list of known issues with federated services:

- [ ] Should configure CNAME records instead of A records on AWS: https://github.com/kubernetes/kubernetes/issues/35637
- [ ] Duplicate resource record errors with Route53: https://github.com/kubernetes/kubernetes/issues/42131
- [ ] Remove DNS entries when a cluster is removed from federation: https://github.com/kubernetes/kubernetes/issues/36657
- [ ] Fail to create DNS records for multizone clusters: https://github.com/kubernetes/kubernetes/issues/35997

cc @kubernetes/sig-federation-bugs @shashidharatd @madhusudancs 
",closed,False,2017-11-06 18:19:27,2018-07-01 13:26:21
federation,marun,https://github.com/kubernetes/federation/pull/142,https://api.github.com/repos/kubernetes/federation/issues/142,WIP Enable make verify,"Fix ``make verify`` to allow the pre-submit job added by https://github.com/kubernetes/test-infra/pull/5369 to pass.

/cc irfanurrehman shashidharatd",closed,True,2017-11-07 00:21:13,2017-11-15 10:05:33
federation,hmarcelodn,https://github.com/kubernetes/federation/issues/143,https://api.github.com/repos/kubernetes/federation/issues/143,Kubernetes Federation - Issue when joining cluster MS Azure,"I am running a cluster in west-us for federation in MS Azure. I installed coredns and ran kubefed as following:

```
kubefed init productores-fed --host-cluster-context=edge-westus-5a01b24d --dns-provider=""coredns"" --dns-zone-name=""sancristobal.com.ar."" --dns-provider-config=""./coredns-provider.conf""Creating a namespace federation-system for federation system components... done
Creating federation control plane service................................................... done
Creating federation control plane objects (credentials, persistent volume claim)... done
Creating federation component deployments... done
Updating kubeconfig... done
Waiting for federation control plane to come up............... done
Federation API server is running at: xx.xx.xxx.xxx
```

Then when I tried to join my east-us cluster it failed as belows:
```

kubefed join productores-k8s-cluster-59c2dcaa --host-cluster-context=productores-fed
error: could not find the deployment for controller manager in host cluster
```

Any ideas",closed,False,2017-11-08 15:29:16,2018-07-01 13:26:22
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/144,https://api.github.com/repos/kubernetes/federation/issues/144,Fed move out cleanup - part 1,"This pull request is not yet complete.
The PR is to get comprehensive test signals, rather then running everything locally.

cc @marun @shashidharatd 
/assign",closed,True,2017-11-10 19:09:02,2017-11-20 10:09:24
federation,jianhuiz,https://github.com/kubernetes/federation/pull/145,https://api.github.com/repos/kubernetes/federation/issues/145,[auth] add last updated by admission control,"This is part of federation per-user cluster auth (https://github.com/kubernetes/kubernetes/issues/35254)

This PR add a admission control that insert an annotation with the username (service account currently excluded) so later the federation controllers will read the username and find the correct credential accordingly to update objects in the underlying clusters.

@quinton-hoole @deepak-vij ",closed,True,2017-11-10 20:22:41,2018-07-20 22:02:46
federation,jianhuiz,https://github.com/kubernetes/federation/pull/146,https://api.github.com/repos/kubernetes/federation/issues/146,[WIP][auth] Federation auth identity provider,"requires https://github.com/kubernetes/federation/pull/145
This is part of federation per-user cluster auth (kubernetes/kubernetes#35254)

Federation controllers use identity provider to find per-user-cluster credential to update objects in the underlying clusters; they still always use the controller accounts (per-cluster in `cluster.spec.secretRef`) for reading/watching

- [x] define identity provider interface
- [x] update federation informer and updater to use identity provider
- [ ] cache clients in informer?
- [x] single identity provider that always use identity in cluster spec
- [x] CRD for per-user-cluster identity store
- [ ] CRD validation (wait for openapi v3 support in k8s)
- [x] per-user-cluster identity provider
- [x] cache and index per-user identities?
- [ ] handle duplication per-user identities in CRD
- [x] add command line parameters to select identity provider
- [ ] external (webhook) identity provider?
- [x] should FCP do impersonate or not?
- [ ] testing

@quinton-hoole @deepak-vij ",closed,True,2017-11-10 20:30:36,2018-05-08 00:04:30
federation,jianhuiz,https://github.com/kubernetes/federation/pull/147,https://api.github.com/repos/kubernetes/federation/issues/147,[auth] add RBAC support,"This is part of federation per-user cluster auth (kubernetes/kubernetes#35254)

This installs the same RBAC in kubernetes, including the build-in roles and bindings
There will be following PRs for pulling and enforcing clusters' RBAC roles @irfanurrehman @shashidharatd 

@quinton-hoole @deepak-vij ",closed,True,2017-11-10 21:00:15,2018-05-22 02:35:15
federation,miguelcastilho,https://github.com/kubernetes/federation/issues/148,https://api.github.com/repos/kubernetes/federation/issues/148,Missing proxy parameters in the controller-manager,"
I used kubefed to depoy the federation apiserver and controller-manager. I was having communication problems and I checked on the logs that the controller-manager couldn't communicate with other services. Because I'm behind a corporate proxy it seemed like proxy issues. I checked the kubefed documentation and there is no parameter to add a proxy. I had to edit the controller-manager deployment manually to add the proxy environment variables.

The kubfed init command should be able to accept input parameters to configure proxy variables.

Environment:
Kubernetes version: v1.8.3
Cloud provider: Bare-metal (OpenStack)
OS: CentOS 7
Install tools: kubeadm and kubefed",closed,False,2017-11-15 14:46:28,2018-07-01 13:26:20
federation,onyiny-ang,https://github.com/kubernetes/federation/pull/149,https://api.github.com/repos/kubernetes/federation/issues/149,Integration test verifies replicasets are deleted when namespace is deleted,"**What this PR does / why we need it:**
Provides integration tests to test:
- Replicasets created in a namespace are deleted when a namespace is deleted
- Events created in a namespace are deleted when a namespace is deleted

**Which issue(s) this PR fixes (optional, in fixes #<issue number>(, fixes #<issue_number>, ...) format, will close the issue(s) when PR gets merged):**
Not aware of any

Special notes for your reviewer:
I believe this is complete for testing replicasets but I am still working on events. I'm unsure how to create a test event to be created in a namespace for this purpose.",closed,True,2017-11-17 16:20:45,2017-11-29 16:48:45
federation,shashidharatd,https://github.com/kubernetes/federation/pull/150,https://api.github.com/repos/kubernetes/federation/issues/150,Modifications from running hack/update-bazel.sh,"This is simple change produced by running `hack/update-bazel.sh`. I am doing this as a separate PR to trigger the build ci job `https://k8s-testgrid.appspot.com/sig-multicluster-federation-gce#build` which has been fixed yesterday but no commits in k/f repo to trigger the build.
if this pr is accepted, the new commit will trigger the build job.

/assign @irfanurrehman 
/cc @kubernetes/sig-multicluster-pr-reviews 

",closed,True,2017-11-18 02:58:21,2017-11-20 07:19:42
federation,shashidharatd,https://github.com/kubernetes/federation/pull/151,https://api.github.com/repos/kubernetes/federation/issues/151,Revamp dns-controller unit testing,"This PR is reraised to k/f from k/k. The original PR is here https://github.com/kubernetes/kubernetes/pull/46606

This is just test improvement.

/cc @kubernetes/sig-multicluster-pr-reviews ",closed,True,2017-11-18 03:07:19,2018-01-17 11:54:35
federation,shashidharatd,https://github.com/kubernetes/federation/pull/152,https://api.github.com/repos/kubernetes/federation/issues/152,Remove obsolete deploy mechanism,We are no longer maintaining the interim deployment mechanism for federation and hence cleaning it up.,closed,True,2017-11-21 08:54:24,2017-11-21 13:46:24
federation,shashidharatd,https://github.com/kubernetes/federation/pull/153,https://api.github.com/repos/kubernetes/federation/issues/153,Disable running ingress e2e tests,"DIsable running ingress e2e tests for now which have been failing for ever.
https://k8s-testgrid.appspot.com/sig-multicluster-federation-gce#gce-serial

/cc @kubernetes/sig-multicluster-bugs ",closed,True,2017-11-21 18:05:10,2017-11-22 07:06:24
federation,csbell,https://github.com/kubernetes/federation/pull/154,https://api.github.com/repos/kubernetes/federation/issues/154,Update Federation OWNERS,"As discussed during the 11/21/2017 SIG Multicluster meeting,
update OWNERS in the federation repo. Highlights here are pruning
the contributor list to reflect availability of contributors for
reviews. Also, this PR nominates Maru and Shashi as approvers.",closed,True,2017-11-21 21:34:23,2017-11-29 12:14:24
federation,nikox94,https://github.com/kubernetes/federation/pull/155,https://api.github.com/repos/kubernetes/federation/issues/155,ISS-35637: Mock out the internet,This abstracts away the internet through an interface and tests the hostname resolving for AWS LBs,closed,True,2017-11-26 09:48:48,2017-12-15 20:03:29
federation,wynro,https://github.com/kubernetes/federation/issues/156,https://api.github.com/repos/kubernetes/federation/issues/156,Error launching Jobs in federation,"
**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:
I tried to launch a Job in a federation, and instead of accepting the Job and scheduling it, the server answered with the error code:

    error: error validating ""pi.yaml"": error validating data: unknown object type schema.GroupVersionKind{Group:""batch"", Version:""v1"", Kind:""Job""}; if you choose to ignore these errors, turn validation off with --validate=false

If you turn off validation with `--validate=false`, the answer is:

    error: unable to recognize ""pi.yaml"": no matches for batch/, Kind=Job

**What you expected to happen**:

I expected the federation API server to accept the Job and launch it

**How to reproduce it (as minimally and precisely as possible)**:

I used the example job in kubernetes.io documentation:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: [""perl"",  ""-Mbignum=bpi"", ""-wle"", ""print bpi(2000)""]
      restartPolicy: Never
  backoffLimit: 4
```

Saved in a file called `pi.yaml`, and tried to launch the job with the command

    kubectl --context=federation create -f ./pi.yaml

**Anything else we need to know?**:

Initially I thought that the problem was in the apiVersion+Kind combination, but the documentation (https://kubernetes.io/docs/tasks/administer-federation/job) says ""The API for federated jobs is fully compatible with the API for traditional Kubernetes jobs. You can create a job by sending a request to the federation apiserver. "". If this is an error on my part, I would suggest changing that piece of documentation.

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.2"", GitCommit:""bdaeafa71f6c7c04636251031f93464384d54963"", GitTreeState:""clean"", BuildDate:""2017-10-24T19:48:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.2"", GitCommit:""bdaeafa71f6c7c04636251031f93464384d54963"", GitTreeState:""clean"", BuildDate:""2017-10-24T19:38:10Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
Kubernetes installed in a local network of physical machines. Every master and slave is a physical node. Two clusters, both with one master and 2 slaves, and another master for the federation cluster.
- OS (e.g. from /etc/os-release):
```
NAME=""Ubuntu""
VERSION=""16.04.3 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.3 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
```
- Kernel (e.g. `uname -a`):
```
Linux <HOSTNAME REDACTED> 4.4.0-98-generic #121-Ubuntu SMP Tue Oct 10 14:24:03 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
Manual installation of Kubernetes from last Github release (At the time of deployment)
Manual installation of the federation as described in https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed
- Others:
Nothing",closed,False,2017-11-27 21:28:22,2018-02-08 11:58:28
federation,marun,https://github.com/kubernetes/federation/pull/157,https://api.github.com/repos/kubernetes/federation/issues/157,Fix generation of api reference docs,This PR is in response to a request from @steveperry-53 to source api reference docs from the new repo.,closed,True,2017-11-28 05:56:01,2017-11-28 15:29:24
federation,marun,https://github.com/kubernetes/federation/pull/158,https://api.github.com/repos/kubernetes/federation/issues/158,Fix hack/generate-docs.sh,"This PR is in response to a request from @steveperry-53 to source cli docs from the new repo.

/cc @irfanurrehman @shashidharatd ",closed,True,2017-11-28 06:14:53,2017-11-28 13:10:24
federation,onyiny-ang,https://github.com/kubernetes/federation/pull/159,https://api.github.com/repos/kubernetes/federation/issues/159,Integration test verifies replicasets are deleted when namespace is deleted 2,"Accidentally pushed upstream/master to the previous PR #149  which closed it. This includes the squashed commits that @irfanurrehman requested.

For reference:
**What this PR does / why we need it:**
Provides integration tests to test:
- Replicasets created in a namespace are deleted when a namespace is deleted
- Events created in a namespace are deleted when a namespace is deleted

**Which issue(s) this PR fixes (optional, in fixes #<issue number>(, fixes #<issue_number>, ...) format, will close the issue(s) when PR gets merged):**
Not aware of any

Special notes for your reviewer:
I believe this is complete for testing replicasets but I am still working on events. I'm unsure how to create a test event to be created in a namespace for this purpose.",closed,True,2017-11-29 17:04:42,2018-01-09 14:00:31
federation,ggaaooppeenngg,https://github.com/kubernetes/federation/issues/160,https://api.github.com/repos/kubernetes/federation/issues/160,propose: support kubefed nodeSelector label ,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
> /kind bug

/kind feature


**What happened**:
scheduler schedules federated-apiserver to a random availabe node
**What you expected to happen**:
schedule to specific nodes
**How to reproduce it (as minimally and precisely as possible)**:
set up federation service

**Anything else we need to know?**:
I suggest to add a flag to kubefed command, or support a config file to customize deployment

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2017-11-30 07:18:12,2017-12-01 02:46:37
federation,onyiny-ang,https://github.com/kubernetes/federation/pull/161,https://api.github.com/repos/kubernetes/federation/issues/161,[WIP] Agnostic namespace test for integration/e2e,"**What this PR does / why we need it:**
This is a follow-up test to #159 that:
Allows for integration as well as e2e tests to ensure replicasets created in a namespace are deleted when a namespace is deleted

**Which issue(s) this PR fixes (optional, in fixes #<issue number>(, fixes #<issue_number>, ...) format, will close the issue(s) when PR gets merged):**
Not aware of any

Special notes for your reviewer:
make tests are failing when trying to test with integration. I'm not sure why.",closed,True,2017-11-30 16:33:40,2017-12-11 18:34:20
federation,ggaaooppeenngg,https://github.com/kubernetes/federation/pull/162,https://api.github.com/repos/kubernetes/federation/issues/162,Fix make help cmds absence,,closed,True,2017-12-01 05:46:34,2017-12-20 10:03:35
federation,ggaaooppeenngg,https://github.com/kubernetes/federation/issues/163,https://api.github.com/repos/kubernetes/federation/issues/163,No public fcp image can be downloaded,"I found hyperkube is replaced by fcp, and kubefed supports fcp now, but I found no open image in gcr.io, how can I mannually make a fcp image?
Does federation have SIG group?",closed,False,2017-12-01 06:11:26,2018-02-21 14:22:01
federation,ggaaooppeenngg,https://github.com/kubernetes/federation/pull/164,https://api.github.com/repos/kubernetes/federation/issues/164,Set jobs as default resource,Signed-off-by: Peng Gao <peng.gao.dut@gmail.com>,closed,True,2017-12-05 10:22:29,2017-12-19 02:35:28
federation,perotinus,https://github.com/kubernetes/federation/pull/165,https://api.github.com/repos/kubernetes/federation/issues/165,Improve the tests for Federation API validation,"The parts of the tests that checked for errors will always succeed, because there will always be an error related to `ObjectMeta.ResourceVersion`. Adding the scaffolding from the success cases ensures that that error is not returned, and that the check for some errors is triggered by actual errors.",closed,True,2017-12-06 00:41:44,2017-12-22 01:49:36
federation,gyliu513,https://github.com/kubernetes/federation/issues/166,https://api.github.com/repos/kubernetes/federation/issues/166,Failed to delete deployment in federation cluster,"I have a federation cluster with two clusters, after the deployment was created, it was distributed to two clusters equally with default policy. Such as I have 6 replicas in my deployment and the federation will create 3 replicas in each cluster. 

But when I try to delete the federation deployment, it always failed due to timeout error.

```
error: error when stopping ""./lb-nginx.yaml"": time out waiting for the condition.
```

Also checked logs for federation apiserver and controller manager, no error found.

I was using kubernetes 1.8.3.",closed,False,2017-12-06 03:23:56,2018-08-14 17:46:24
federation,onyiny-ang,https://github.com/kubernetes/federation/pull/167,https://api.github.com/repos/kubernetes/federation/issues/167,[WIP] Agnostic namespace test for integration/e2e,"**What this PR does / why we need it:**
This is a follow-up test to #159 that:
Allows for integration as well as e2e tests to ensure replicasets created in a namespace are deleted when a namespace is deleted

**Which issue(s) this PR fixes (optional, in fixes #<issue number>(, fixes #<issue_number>, ...) format, will close the issue(s) when PR gets merged):**
Not aware of any

Special notes for your reviewer:
make tests are failing when trying to test with integration. I'm not sure why.",closed,True,2017-12-11 18:37:53,2017-12-27 04:54:20
federation,font,https://github.com/kubernetes/federation/pull/168,https://api.github.com/repos/kubernetes/federation/issues/168,Convert the Job controller to a sync controller,,closed,True,2017-12-13 06:00:01,2018-07-02 13:50:12
federation,gyliu513,https://github.com/kubernetes/federation/issues/169,https://api.github.com/repos/kubernetes/federation/issues/169,Load-balance cross cluster should support local clusters,"Now the federated service creates a separate service shard in each underlying cluster in the federation. And to create DNS records for cross-cluster service discovery, federation depends on the load balancer addresses allocated by the cluster service shards.But most of the local clusters do not have a load balancer implementation, federation should support load balancer for multiple local clusters.

/cc @quinton-hoole comments for this? Thanks!",closed,False,2017-12-15 09:16:02,2018-06-03 08:57:18
federation,gyliu513,https://github.com/kubernetes/federation/issues/170,https://api.github.com/repos/kubernetes/federation/issues/170,Host cluster down will cause federation does not work,"Now the federation APIServer and Controller manager are both running on host cluster, but if the host cluster is down, then federation will not work. 

Any suggestions for how to make federation still works even if host cluster is down? /cc @quinton-hoole 

/kind feature",closed,False,2017-12-18 05:17:18,2018-07-01 13:26:21
federation,spiffxp,https://github.com/kubernetes/federation/pull/171,https://api.github.com/repos/kubernetes/federation/issues/171,Add code-of-conduct.md,"Refer to kubernetes/community as authoritative source for code of conduct

ref: kubernetes/community#1527",closed,True,2017-12-20 18:32:14,2017-12-26 16:58:23
federation,spiffxp,https://github.com/kubernetes/federation/pull/172,https://api.github.com/repos/kubernetes/federation/issues/172,Workaround gazelle moving to a new repo,"Keep using the same version of gazelle instead of following to
a later version at a new repo.  Same approach as kubernetes/kubernetes#57564

ref: kubernetes/test-infra#6075",closed,True,2017-12-22 20:15:08,2018-01-26 22:14:32
federation,onyiny-ang,https://github.com/kubernetes/federation/pull/173,https://api.github.com/repos/kubernetes/federation/issues/173,[WIP] Agnostic namespace test for integration/e2e,"**What this PR does / why we need it:**
This is a follow-up test to #159 that:
Allows for integration as well as e2e tests to ensure replicasets created in a namespace are deleted when a namespace is deleted

**Which issue(s) this PR fixes (optional, in fixes #<issue number>(, fixes #<issue_number>, ...) format, will close the issue(s) when PR gets merged):**
Not aware of any

Special notes for your reviewer:
namespace tests are failing due to timeout waiting for namespace deletion. Increasing time in ""waitForNamespaceDeletion"" function (test/common/namespace.go) does not seem to help. ",closed,True,2017-12-27 05:02:34,2017-12-27 05:28:05
federation,onyiny-ang,https://github.com/kubernetes/federation/pull/174,https://api.github.com/repos/kubernetes/federation/issues/174,Agnostic namespace test for integration/e2e,"**What this PR does / why we need it:**
This is a follow-up test to #159 that:
Allows for integration as well as e2e tests to ensure replicasets created in a namespace are deleted when a namespace is deleted

**Which issue(s) this PR fixes (optional, in fixes #<issue number>(, fixes #<issue_number>, ...) format, will close the issue(s) when PR gets merged):**
Not aware of any

Special notes for your reviewer:
namespace tests are failing due to timeout waiting for namespace deletion. Increasing time in ""waitForNamespaceDeletion"" function (test/common/namespace.go) does not seem to help. ",closed,True,2017-12-27 05:23:44,2018-01-09 14:00:41
federation,liqlin2015,https://github.com/kubernetes/federation/issues/175,https://api.github.com/repos/kubernetes/federation/issues/175,Cannot create CNAME record for cluster which don't have reachable backend,"I have two clusters in federation.

```
# kubectl get clusters
NAME        STATUS    AGE
cluster11   Ready     2d
cluster22   Ready     2d
```

And I have created a tomcat deployment and service with 1 instance.

```
# kubectl --namespace=demo get deployment
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
tomcat    1         1         1            1           2d

# kubectl --namespace=demo get svc
NAME      TYPE           CLUSTER-IP   EXTERNAL-IP        PORT(S)    AGE
tomcat    LoadBalancer   <none>       156.254.6.1,1...   8080/TCP   45m
```

Check the federation service detail
```
# kubectl --namespace=demo get svc tomcat -o yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    federation.kubernetes.io/service-ingresses: '{""items"":[{""cluster"":""cluster22"",""items"":[{""ip"":""156.254.6.1""}]}]}'
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiVersion"":""v1"",""kind"":""Service"",""metadata"":{""annotations"":{},""labels"":{""app"":""tomcat""},""name"":""tomcat"",""namespace"":""demo""},""spec"":{""ports"":[{""name"":""http"",""port"":8080,""protocol"":""TCP"",""targetPort"":8080}],""selector"":{""app"":""tomcat""},""sessionAffinity"":""ClientIP"",""sessionAffinityConfig"":{""clientIP"":{""timeoutSeconds"":10800}},""type"":""LoadBalancer""}}
  creationTimestamp: 2017-12-28T06:55:04Z
  finalizers:
  - federation.kubernetes.io/delete-from-underlying-clusters
  - orphan
  labels:
    app: tomcat
  name: tomcat
  namespace: demo
  resourceVersion: ""10070""
  selfLink: /api/v1/namespaces/demo/services/tomcat
  uid: 088ca6b2-eb9c-11e7-b5c9-ea466b176ce6
spec:
  externalTrafficPolicy: Cluster
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 156.254.6.1
    - ip: 192.168.38.4
```

The annotation `federation.kubernetes.io/service-ingresses` only has one cluster `cluster22` item.  Because the one tomcat instance is only deployed in `cluster22`. 

Here is the clusters info
```
 kubectl get clusters -o yaml
apiVersion: v1
items:
- apiVersion: federation/v1beta1
  kind: Cluster
  metadata:
    annotations:
      federation.kubernetes.io/cluster-role-name: federation-controller-manager:my-fed-cluster11-cluster11
      federation.kubernetes.io/servive-account-name: cluster11-cluster11
    creationTimestamp: 2017-12-26T02:57:43Z
    name: cluster11
    namespace: """"
    resourceVersion: ""10245""
    selfLink: /apis/federation/v1beta1/clusters/cluster11
    uid: 8af5716b-e9e8-11e7-b5c9-ea466b176ce6
  spec:
    secretRef:
      name: cluster11-pprb4
    serverAddressByClientCIDRs:
    - clientCIDR: 0.0.0.0/0
      serverAddress: https://9.21.58.181:8001
  status:
    conditions:
    - lastProbeTime: 2017-12-28T07:46:15Z
      lastTransitionTime: 2017-12-28T06:54:53Z
      message: /healthz responded with ok
      reason: ClusterReady
      status: ""True""
      type: Ready
    region: ca
    zones:
    - south
- apiVersion: federation/v1beta1
  kind: Cluster
  metadata:
    annotations:
      federation.kubernetes.io/cluster-role-name: federation-controller-manager:my-fed-cluster22-cluster11
      federation.kubernetes.io/servive-account-name: cluster22-cluster11
    creationTimestamp: 2017-12-26T03:00:00Z
    name: cluster22
    namespace: """"
    resourceVersion: ""10246""
    selfLink: /apis/federation/v1beta1/clusters/cluster22
    uid: dcb4d4c9-e9e8-11e7-b5c9-ea466b176ce6
  spec:
    secretRef:
      name: cluster22-m7gpf
    serverAddressByClientCIDRs:
    - clientCIDR: 0.0.0.0/0
      serverAddress: https://9.21.58.184:8001
  status:
    conditions:
    - lastProbeTime: 2017-12-28T07:46:15Z
      lastTransitionTime: 2017-12-28T06:54:53Z
      message: /healthz responded with ok
      reason: ClusterReady
      status: ""True""
      type: Ready
    region: us
    zones:
    - north
kind: List
metadata:
  resourceVersion: """"
  selfLink: """"
```

Check DNS record generated or the tomcat service
```
# dig @9.21.58.181 -p 31163 tomcat.demo.my-fed.svc.north.us.fed.com
;; ANSWER SECTION:
tomcat.demo.my-fed.svc.north.us.fed.com. 180 IN	A 156.254.6.1

# dig @9.21.58.181 -p 31163 tomcat.demo.my-fed.svc.us.fed.com
;; ANSWER SECTION:
tomcat.demo.my-fed.svc.us.fed.com. 180 IN A	156.254.6.1

# dig @9.21.58.181 -p 31163 tomcat.demo.my-fed.svc.fed.com
;; ANSWER SECTION:
tomcat.demo.my-fed.svc.fed.com.	180 IN	A	156.254.6.1

# dig @9.21.58.181 -p 31163 tomcat.demo.my-fed.svc.south.ca.fed.com
;; QUESTION SECTION:
;tomcat.demo.my-fed.svc.ca.fed.com. IN	A

;; AUTHORITY SECTION:
fed.com.		300	IN	SOA	ns.dns.fed.com. hostmaster.fed.com. 1514447432 7200 1800 86400 60

# dig @9.21.58.181 -p 31163 tomcat.demo.my-fed.svc.ca.fed.com
;; QUESTION SECTION:
;tomcat.demo.my-fed.svc.ca.fed.com. IN	A

;; AUTHORITY SECTION:
fed.com.		300	IN	SOA	ns.dns.fed.com. hostmaster.fed.com. 1514447432 7200 1800 86400 60
```

Try to resolve federation service in cluster22.
```
/ # nslookup tomcat.demo.my-fed
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'tomcat.demo.my-fed'
```

And check kube-dns log
```
I1228 07:52:06.259048       1 logs.go:41] skydns: incomplete CNAME chain from ""tomcat.demo.my-fed.svc.south.ca.fed.com."": rcode 3 is not equal to success
```

kube-dns has auto expend the short name `tomcat.demo.my-fed` to `tomcat.demo.my-fed.svc.south.ca.fed.com.`. But there is no related CNAME record in core-dns.

Just checked the code. `cluster11` is supposed to be in the annotation item list too but with empty IP or hostname. Then dns controller can help create CNAME record for `cluster11`. Please correct me if my understanding is wrong.

",closed,False,2017-12-28 07:54:22,2018-07-27 16:42:53
federation,shashidharatd,https://github.com/kubernetes/federation/issues/176,https://api.github.com/repos/kubernetes/federation/issues/176,helm chart for federation control plane deployment,"I am trying to open up discussion about using helm chart mechanism to deploy federation control plane(fcp) components (api-server & controller-manager). Currently kubefed is the supported tool to deploy federation components. Over a period of time it has grown to do multiple things and has become bulky and difficult to support and maintain more customisation. So does it makes sense to explore helm chart as a mechanism to deploy fcp components?

/cc @kubernetes/sig-multicluster-feature-requests @irfanurrehman @marun @madhusudancs ",closed,False,2018-01-02 10:51:26,2018-07-01 13:26:20
federation,gyliu513,https://github.com/kubernetes/federation/issues/177,https://api.github.com/repos/kubernetes/federation/issues/177,How to distinguish images for kubernetes `hyperkube` and federation `hyperkube`,"Now both kubernetes and federation using same image names in the format of `${REGISTRY}/hyperkube-${ARCH}:${VERSION}`, my question is how to distinguish images from kubernetes and federation?

We should make the name different for kubernetes and federation.

",closed,False,2018-01-03 03:13:52,2018-07-01 13:26:20
federation,gyliu513,https://github.com/kubernetes/federation/pull/178,https://api.github.com/repos/kubernetes/federation/issues/178,Used REGISTRY env in hyperkube Makefile.,This can make the local build successes if the local build machine cannot access internet.,closed,True,2018-01-03 03:58:59,2018-06-02 05:58:19
federation,Ronwebb55,https://github.com/kubernetes/federation/issues/179,https://api.github.com/repos/kubernetes/federation/issues/179,Create Kubernetes federation terrafom resource,"Hi, I'm searching for a way to do federation in GCP with k8's using terraform. I would like to know is there a source for this coming?  ",closed,False,2018-01-03 21:11:44,2018-07-06 02:13:13
federation,shashidharatd,https://github.com/kubernetes/federation/pull/180,https://api.github.com/repos/kubernetes/federation/issues/180,Add option to kubefed to use external etcd server,"Currently etcd is deployed in the same pod as federation apiserver and this may not be desirable for some deployments. So adding an option to kubefed wherein user can pass the pre-deployed etcd-server to be used as federation backend.

Note: We need to secure the communication between api-server and etcd using tls, which would be done in follow up pr's.

/cc @kubernetes/sig-multicluster-feature-requests ",closed,True,2018-01-04 09:37:32,2018-01-30 19:38:09
federation,pmorie,https://github.com/kubernetes/federation/issues/181,https://api.github.com/repos/kubernetes/federation/issues/181,Verify-codegen job appears to be busted,"Present in a few PRs, example: #174 ",closed,False,2018-01-05 17:27:20,2018-01-08 14:38:11
federation,onyiny-ang,https://github.com/kubernetes/federation/pull/182,https://api.github.com/repos/kubernetes/federation/issues/182,ran ./hack/update-all to address failing verify tests,"**What this PR does / why we need it:**
This PR addresses the failing verify tests performed by the k8s-ci-robot
./hack/verify-codegen.sh seems to be failing due to Boilerplate date set to 2017 instead of 2018.

**Which issue(s) this PR fixes (optional, in fixes #<issue number>(, fixes #<issue_number>, ...) format, will close the issue(s) when PR gets merged):**
Fixes: #181  
Fixes verify tests for: #174 #178 #180 

**Special notes for your reviewer:**
Happy New Year~
  
  ",closed,True,2018-01-05 17:49:31,2018-01-09 14:01:00
federation,shashidharatd,https://github.com/kubernetes/federation/pull/183,https://api.github.com/repos/kubernetes/federation/issues/183,Bump vendored code to match that of kubernetes-v1.10.0-alpha.3,"- Add versions in glide.yaml file to match the corresponding version from kubernetes-1.10.0-alpha.3 ~~1.9.2-beta.0~~
- Updated vendor code to match kubernetes-1.10.0-alpha.3 ~~1.9.2-beta.0~~ and fix the usage in federation code

/cc @kubernetes/sig-multicluster-pr-reviews @irfanurrehman @marun ",closed,True,2018-01-08 14:57:24,2018-02-20 05:23:35
federation,marun,https://github.com/kubernetes/federation/pull/184,https://api.github.com/repos/kubernetes/federation/issues/184,Fix openshift compat for kubefed join and e2e,"Openshift >= 1.7 needs the included changes to support ``kubefed join`` (since openshift creates an extra secret per service account) and e2e (since secrets no longer contain ``kubeconfig`` as of 1.7).

/assign irfanurrehman shashidharatd 

cc: @onyiny-ang @font   ",closed,True,2018-01-08 16:48:41,2018-01-10 16:51:13
federation,nikox94,https://github.com/kubernetes/federation/issues/185,https://api.github.com/repos/kubernetes/federation/issues/185,Set up e2e for local testing,"Currently running e2e tests locally is not set up and a tested procedure does not yet exist.

We need to do the following:
1. Evaluate options for e2e testing local environment setups
2. Try some PoCs and decide on a viable candidate
3. Write documentation and set up a procedure for easy set-up by new and experienced contributors alike

We would ideally like the procedure to contain a DNS manager (probably coreDNS) and LBs (looking at https://github.com/heptio/contour).",closed,False,2018-01-08 19:31:32,2018-07-01 13:26:19
federation,nikox94,https://github.com/kubernetes/federation/pull/186,https://api.github.com/repos/kubernetes/federation/issues/186,ISSUE-185: Init e2e README.md,https://github.com/kubernetes/federation/issues/185,closed,True,2018-01-08 20:02:27,2018-12-30 15:26:34
federation,rangapv,https://github.com/kubernetes/federation/issues/187,https://api.github.com/repos/kubernetes/federation/issues/187,kubefed not installing right ....,">Is this a BUG REPORT or FEATURE REQUEST?:

Uncomment only one, leave it on its own line:

/kind bug

>Labels:
/ sig federation

>What happened:
kubefed install not working right...

I did the following.....
Step 1:
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/kubernetes-client-linux-amd64.tar.gz
tar -xzvf kubernetes-client-linux-amd64.tar.gz

Step 2:
sudo snap install kubefed --classic
kubefed version

>Error:
/snap/kubefed/260/kubefed: 1: /snap/kubefed/260/kubefed: Syntax error: redirection unexpected

>What you expected to happen:
kubefed version should display the right version number
Like kubefed version #(like 1.7.0)

>How to reproduce it (as minimally and precisely as possible):
Clean Install of K*s 1.9 and then the above Step1 and step 2

>Anything else we need to know?:

>Environment:

*Kubernetes version (use kubectl version): 1.9
*Cloud provider or hardware configuration: GCP
*OS (e.g. from /etc/os-release): Ubuntu 16.04
*Kernel (e.g. uname -a): 4.13.0-1002-gcp #5-Ubuntu SMP Tue Dec 5 13:20:17 UTC 2017 x86_64 x86_64 *x86_64 GNU/Linux
*Install tools: snap
*Others:",closed,False,2018-01-10 02:18:03,2018-07-01 13:26:19
federation,Blasterdick,https://github.com/kubernetes/federation/issues/188,https://api.github.com/repos/kubernetes/federation/issues/188,Can't start apiserver container,"/kind bug

Faced the error after deploying federated host cluster with next command: 
```
kubefed init federated --image=""k8s.gcr.io/hyperkube-amd64:v1.8.5"" \
  --host-cluster-context=""k8s-federated-host"" \
  --dns-provider=""google-clouddns"" \
  --dns-zone-name=""sub.domain.com."" \
  --apiserver-enable-basic-auth=true \
  --apiserver-enable-token-auth=true
```

The error is in the `federated-apiserver` pod:
`Error: failed to start container ""apiserver"": Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused ""exec: \""/fcp\"": stat /fcp: no such file or directory""`

Versions:
```
$ kubefed version
Client Version: version.Info{Major:"""", Minor:"""", GitVersion:""v0.0.0-master+$Format:%h$"", GitCommit:""$Format:%H$"", GitTreeState:""not a git tree"", BuildDate:""1970-01-01T00:00:00Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.5-gke.0"", GitCommit:""2c2a807131fa8708abc92f3513fe167126c8cce5"", GitTreeState:""clean"", BuildDate:""2017-12-19T20:05:45Z"", GoVersion:""go1.8.3b4"", Compiler:""gc"", Platform:""linux/amd64""}

$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.4"", GitCommit:""9befc2b8928a9426501d3bf62f72849d5cbcd5a3"", GitTreeState:""clean"", BuildDate:""2017-11-20T05:28:34Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.5-gke.0"", GitCommit:""2c2a807131fa8708abc92f3513fe167126c8cce5"", GitTreeState:""clean"", BuildDate:""2017-12-19T20:05:45Z"", GoVersion:""go1.8.3b4"", Compiler:""gc"", Platform:""linux/amd64""

```",closed,False,2018-01-11 11:55:18,2018-07-01 13:26:18
federation,kinghrothgar,https://github.com/kubernetes/federation/issues/189,https://api.github.com/repos/kubernetes/federation/issues/189,Creating federated ingress only creates in single cluster,"**Kubernetes version** (use `kubectl version`): 
1.8.5-gke.0

**Environment**:

- **Cloud provider or hardware configuration**: 
GKE
- **OS** (e.g. from /etc/os-release): 
Container-Optimized OS from Google

**What happened**:
I am running a federated cluster in 4 regions. I ran `kubectl apply -f ing-hello-world.yaml --context production` where `production` is the federation context.

Contents of yaml file:
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: hello-world
  namespace: production
  annotations:
    kubernetes.io/ingress.class: ""nginx""
  labels:
    app: hello-world
spec:
  rules:
  - host: hello.levenlabs.com
    http:
      paths:
      - path: /
        backend:
          serviceName: hello-world
          servicePort: 8080
```
An ingress is created in a random single cluster.  Each time I delete and re-run it, it ends up in a different single cluster.

**What you expected to happen**:
An ingress to be created in all federated clusters.

**How to reproduce it** (as minimally and precisely as possible):
Add a federated ingress with annotation `kubernetes.io/ingress.class: ""nginx""`. I can fine no logs that imply anything went wrong.",closed,False,2018-01-12 16:41:00,2018-07-01 12:25:15
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/190,https://api.github.com/repos/kubernetes/federation/issues/190,Script to initialise Release 1.9.0,"I have used the default project name as `kubernetes-federation-release`, because not sure which project will host the fcp image, yet. On the other hand the same name bucket `kubernetes-federation-release` is hosted in `k8s-jkns-e2e-gce-federation`.
We further need to figure out, if this can be triggered from one of the CI jobs.
@shashidharatd @marun 

PS: once this is accepted I will add a v1.9.0 tag on the latest commit, post which if this is run as 
`hack/release.sh v1.9.0`, will build and push the release.",closed,True,2018-01-15 19:41:20,2018-01-20 06:57:28
federation,shashidharatd,https://github.com/kubernetes/federation/pull/191,https://api.github.com/repos/kubernetes/federation/issues/191,Allow fcp deployment in pre-existing federation namespace,"Fixes: https://github.com/kubernetes/federation/issues/19

/assign @irfanurrehman ",closed,True,2018-01-16 10:32:51,2018-01-17 13:10:28
federation,rangapv,https://github.com/kubernetes/federation/issues/192,https://api.github.com/repos/kubernetes/federation/issues/192,Kubefed binary missing from the tar ball in 1.9 v,"Is this a BUG REPORT or FEATURE REQUEST?:

Uncomment only one, leave it on its own line:

/kind bug

/kind feature

Labels:
/ sig federation
What happened:
kubefed install not working right...

I did the following.....
Step 1:
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/kubernetes-client-linux-amd64.tar.gz
tar -xzvf kubernetes-client-linux-amd64.tar.gz

Step 2:
sudo snap install kubefed --classic
kubefed version

Error:
/snap/kubefed/260/kubefed: 1: /snap/kubefed/260/kubefed: Syntax error: redirection unexpected

What you expected to happen:
kubefed version should display the right version number
Like kubefed version #(like 1.7.0)

How to reproduce it (as minimally and precisely as possible):
Clean Install of K*s 1.9 and then the above Step1 and step 2

Anything else we need to know?:

Environment:

Kubernetes version (use kubectl version): 1.9
Cloud provider or hardware configuration: GCP
OS (e.g. from /etc/os-release): Ubuntu 16.04
Kernel (e.g. uname -a): 4.13.0-1002-gcp #5-Ubuntu SMP Tue Dec 5 13:20:17 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
Install tools: snap
Others:",closed,False,2018-01-18 08:52:04,2018-02-23 15:06:35
federation,shashidharatd,https://github.com/kubernetes/federation/pull/193,https://api.github.com/repos/kubernetes/federation/issues/193,Reduce length of apiserver credential secret name,"Recent PR builds started failing with error as follows.
```
W0124 04:32:04.366] F0124 04:32:04.212763    1370 helpers.go:119] The Deployment ""e2e-f8n-d2c00c37-00ba-11e8-b7d5-0a580a6c0311-0-apiserver"" is invalid: 
W0124 04:32:04.366] * spec.template.spec.volumes[0].name: Invalid value: ""e2e-f8n-d2c00c37-00ba-11e8-b7d5-0a580a6c0311-0-apiserver-credentials"": must be no more than 63 characters
W0124 04:32:04.366] * spec.template.spec.containers[0].volumeMounts[0].name: Not found: ""e2e-f8n-d2c00c37-00ba-11e8-b7d5-0a580a6c0311-0-apiserver-credentials""
```
So reduced the name of the apiserver credential secret. the name is still unique as we create it within namespace having unique name.
",closed,True,2018-01-24 07:56:27,2018-01-24 14:46:33
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/194,https://api.github.com/repos/kubernetes/federation/issues/194,Update/Correct bazel release bins to be used in release job,"Update the build scripts to generate sane release tars and image tar for release.
",closed,True,2018-01-31 07:02:53,2018-02-02 10:19:13
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/195,https://api.github.com/repos/kubernetes/federation/issues/195,Fix release version,The PR is on top of https://github.com/kubernetes/federation/pull/194,closed,True,2018-01-31 13:36:26,2018-02-02 10:19:14
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/196,https://api.github.com/repos/kubernetes/federation/issues/196,Update release script to make it work with bazel,The PR is on top of https://github.com/kubernetes/federation/pull/195,closed,True,2018-01-31 18:53:19,2018-02-02 10:19:11
federation,lukaszo,https://github.com/kubernetes/federation/pull/197,https://api.github.com/repos/kubernetes/federation/issues/197,Do not delete ExternalIPs,"**What this PR does / why we need it**: ExternalIPs are not managed by Kubernetes and should not be touched.

This is copy of PR from k8s repo https://github.com/kubernetes/kubernetes/pull/51894
```release-note
NONE
```
/kind bug

cc @quinton-hoole
",closed,True,2018-01-31 22:12:03,2018-02-02 11:44:06
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/198,https://api.github.com/repos/kubernetes/federation/issues/198,Missing information in docs about jobs API,"Jobs API in federation apiserver is not enabled by default and needs to be explicitly enabled (for example using `--runtime-config=api/all=true` while starting the federation apiserver). This information does not reflect correctly in current documentation about jobs [here](https://kubernetes.io/docs/tasks/administer-federation/job/).
This should be updated.",closed,False,2018-02-02 11:27:37,2018-07-07 18:53:12
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/199,https://api.github.com/repos/kubernetes/federation/issues/199,"Federation does not resolve Hostname, need to use IP","<a href=""https://github.com/tommyknows""><img src=""https://avatars2.githubusercontent.com/u/16387735?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [tommyknows](https://github.com/tommyknows)**
_Thursday Dec 21, 2017 at 08:33 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/57505_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug



**What happened**:
When adding a cluster to the federation (with `kubefed join`) the ""server"" field (in kubeconfig) needs to be an IP-Adress. If I use ""server: https://hostname:6443"", the cluster will be offline when executing `kubectl get clusters` due to `Cluster is not reachable`

**What you expected to happen**:
I expect the federation to be able to work with hostnames as well - as it works with kubectl too. Or at least, it should be documented somewhere that you need to use IP-Addresses.

**How to reproduce it (as minimally and precisely as possible)**:
Create a federation cluster and trying to add a cluster that uses hostnames instead of IPs.

**Anything else we need to know?**:
-

**Environment**:
- Kubernetes version (use `kubectl version`): 1.8.5
- Cloud provider or hardware configuration: On-Premise / OpenStack
- OS (e.g. from /etc/os-release): Ubuntu 16.04.3 LTS
- Kernel (e.g. `uname -a`): 4.4.0

",closed,False,2018-02-02 14:28:59,2019-02-02 15:44:47
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/200,https://api.github.com/repos/kubernetes/federation/issues/200,Remove federation-apiserver's dependency on kubernetes APIs,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Thursday Jun 02, 2016 at 01:31 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/26675_

----

Right now `--storage-versions` in federation-apiserver includes all the k8s APIs (see https://github.com/kubernetes/kubernetes/blob/master/docs/admin/federation-apiserver.md), which is wrong.
This is happening because an API group is registered if there is a dependency on it.

Here is how federation-apiserver depends on k8s extensions API group:

```
$ go run godepq.go -from k8s.io/kubernetes/federation/cmd/federation-apiserver -to k8s.io/kubernetes/pkg/apis/extensions
Packages:
  k8s.io/kubernetes/federation/cmd/federation-apiserver
  k8s.io/kubernetes/federation/cmd/federation-apiserver/app
  k8s.io/kubernetes/pkg/admission
  k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset
  k8s.io/kubernetes/pkg/apis/extensions/install
```

This dependency route exists for all k8s apis.

cc @kubernetes/sig-api-machinery @kubernetes/sig-cluster-federation 

",closed,False,2018-02-02 14:32:46,2018-07-24 03:18:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/201,https://api.github.com/repos/kubernetes/federation/issues/201,Decide the domain name infix for the Ingress entries in KubeDNS,"<a href=""https://github.com/madhusudancs""><img src=""https://avatars0.githubusercontent.com/u/10183?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [madhusudancs](https://github.com/madhusudancs)**
_Friday Aug 12, 2016 at 21:23 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/30544_

----

Discovery DNS records for services take the form `mysvc.myns.svc.cluster.local.` with `svc` as the constant infix for services. We also want to make ingresses discoverable by publishing their DNS records. PR #29768 already does that. It uses the infix constant `ing` to indicate that a record belongs to an ingress resource. We need to decide if `ing` is the appropriate constant for ingresses.

cc @kubernetes/sig-cluster-federation @matchstick 

",closed,False,2018-02-02 14:33:02,2018-07-24 02:17:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/202,https://api.github.com/repos/kubernetes/federation/issues/202,Federated api server should validate cluster api server address,"<a href=""https://github.com/mwielgus""><img src=""https://avatars2.githubusercontent.com/u/11994812?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [mwielgus](https://github.com/mwielgus)**
_Sunday Aug 21, 2016 at 10:13 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/31074_

----

Currently it is possible to pass just an IP address without any schema/port information (no https://) which leads to connectivity problems. 

Also http://kubernetes.io/docs/admin/federation/ should give an example of valid cluster object and tell to include the schema . 

Ideally, if FEDERATION=true is passed to kube-up.sh the corresponding cluster.yaml file should be generated automatically.

cc: @quinton-hoole @kubernetes/sig-cluster-federation 

",closed,False,2018-02-02 14:33:42,2018-07-24 01:16:24
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/203,https://api.github.com/repos/kubernetes/federation/issues/203,Federated service IP out of sync between AWS LB and CloudDNS records,"<a href=""https://github.com/juntagabor""><img src=""https://avatars0.githubusercontent.com/u/13035153?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [juntagabor](https://github.com/juntagabor)**
_Wednesday Oct 26, 2016 at 16:09 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/35637_

----

**Is this a request for help?** No

**What keywords did you search in Kubernetes issues before filing this one?** 
federation dns
federation aws
federation aws cname

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT

**Kubernetes version** (use `kubectl version`): 
v1.4.4

**Environment**:
- **Cloud provider or hardware configuration**:
  GCE for federation control plane
  AWS for cluster1 (federated)
  GCE for cluster2 (federated)
- **OS** (e.g. from /etc/os-release):
  NAME=""Debian GNU/Linux""
  VERSION_ID=""7""
  VERSION=""7 (wheezy)""
- **Kernel** (e.g. `uname -a`):
  3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux
- **Install tools**:
- **Others**:

**What happened**:
Federation controller creates the appropriate A entries for all federated services on Google CloudDNS, including resolving AWS load balancer DNS name to IPs. But as IP changes for AWS LBs, the A records get outdated, causing service disruption as they become out of sync.

**What you expected to happen**:
I expected the federation controller to either add a CNAME pointing to AWS LB, or to keep the IPs for AWS in sync overtime.

**How to reproduce it** (as minimally and precisely as possible):
- Deploy a federation control plane at GCE
- Create cluster at AWS
- Add AWS cluster to federation
- create a federated service
- wait 24h (or until AWS Load Balancer IPs change)
- Cloud DNS will be pointing to different IPs than AWS LB.  

**Anything else do we need to know**:
AWS recommends:
- Because the set of IP addresses associated with a LoadBalancer can change over time, you should never create an ""A"" record with any specific IP address.

",closed,False,2018-02-02 14:33:50,2018-07-05 16:03:44
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/204,https://api.github.com/repos/kubernetes/federation/issues/204,Big route53 accounts may cause dnsprovider to hit rate limits,"<a href=""https://github.com/justinsb""><img src=""https://avatars2.githubusercontent.com/u/100893?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [justinsb](https://github.com/justinsb)**
_Tuesday Jan 10, 2017 at 15:00 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39674_

----

The route53 dnsprovider doesn't implement filtering.  On Route53, there is a 5 request per second rate limit (per AWS _account_): http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DNSLimitations.html

With pagination, it is very likely that we will issue more than 5 requests per second just listing resource records for a big zone.

We need to implement filtering.
",closed,False,2018-02-02 14:34:44,2018-07-24 02:17:57
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/205,https://api.github.com/repos/kubernetes/federation/issues/205,dnsprovider: large google cloud dns rrsets could be truncated,"<a href=""https://github.com/justinsb""><img src=""https://avatars2.githubusercontent.com/u/100893?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [justinsb](https://github.com/justinsb)**
_Thursday Jan 12, 2017 at 14:53 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/39804_

----

We don't implement pagination when retrieving rrsets from google cloud dns, so results could in theory be truncated: https://github.com/kubernetes/kubernetes/blob/master/federation/pkg/dnsprovider/providers/google/clouddns/rrsets.go#L34

We should support NextPageToken.
",closed,False,2018-02-02 14:34:59,2018-07-24 01:16:25
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/206,https://api.github.com/repos/kubernetes/federation/issues/206,federation API not in client-go,"<a href=""https://github.com/justinsb""><img src=""https://avatars2.githubusercontent.com/u/100893?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [justinsb](https://github.com/justinsb)**
_Sunday Feb 12, 2017 at 20:27 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/41302_

----

The federation objects are not currently in client-go.  I believe this makes it hard/impossible to use federation with client-go.  The challenges I hit:

* you would have to refer to the main kubernetes/kuberentes repo again for the federation objects
* mixing and matching the federation objects from kubernetes/kubernetes is tricky, because for example ClusterSpec has a SecretRef field which is of type `k8s.io/kubernetes/pkg/api/v1.LocalObjectReference`, but in client-go view that is `k8s.io/client-go/pkg/api/v1...` which is different.

I think we need to include the federation objects into client-go.




",closed,False,2018-02-02 14:35:09,2018-07-24 02:17:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/207,https://api.github.com/repos/kubernetes/federation/issues/207,[Federation][kubefed][aws kops cluster] Cluster stays in unknown state & TLS handshake error,"<a href=""https://github.com/csarora""><img src=""https://avatars2.githubusercontent.com/u/15997233?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [csarora](https://github.com/csarora)**
_Tuesday Feb 28, 2017 at 16:51 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42263_

----

**Kubernetes version** :
Client Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.2"", GitCommit:""08e099554f3c31f6e6f07b448ab3ed78d0520507"", GitTreeState:""clean"", BuildDate:""2017-01-12T04:57:25Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.2"", GitCommit:""08e099554f3c31f6e6f07b448ab3ed78d0520507"", GitTreeState:""clean"", BuildDate:""2017-01-12T04:52:34Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}


**Environment**:
- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release): Ubuntu

```
root@ip-172-31-26-62:/home/ubuntu# kops get clusters
NAME                    CLOUD   ZONES
k8east.spopt.co.uk      aws     us-east-1a
k8s.test                aws     eu-west-1a,eu-west-1b,eu-west-1c

root@ip-172-31-26-62:/home/ubuntu# kubectl get clusters
NAME      STATUS    AGE
k8s       Unknown   32m
k8stest   Unknown   1h
```

```
cat cluster.yaml >>
apiVersion: federation/v1beta1
kind: Cluster
metadata:
  name: k8stest
spec:
  serverAddressByClientCIDRs:
    - clientCIDR: ""172.31.0.0/16""
      serverAddress: ""https://api.k8s.test""
  secretRef:
    name: k8s
```

I created secret k8s using below command :
```
 kubectl config use-context k8s.test
 kubectl --context=k8s.test config view --flatten --minify > /root/.kube/k8s.test/kubeconfig
 kubectl --context=""k8s.test""   --namespace=federation-system   create secret generic k8s   --from-file=/root/.kube/k8s.test/kubeconfig
```
  
Logs from apiserver : fedcluster-apiserver-3532249059-cmp48

```
2017-02-28T16:19:36.163944507Z I0228 16:19:36.163851       1 logs.go:41] http: TLS handshake error from 10.123.45.1:24237: EOF
2017-02-28T16:19:46.164331830Z I0228 16:19:46.164239       1 logs.go:41] http: TLS handshake error from 10.123.45.1:24241: EOF
2017-02-28T16:19:56.164934170Z I0228 16:19:56.164819       1 logs.go:41] http: TLS handshake error from 10.123.45.1:24253: EOF
2017-02-28T16:20:06.164256376Z I0228 16:20:06.164132       1 logs.go:41] http: TLS handshake error from 10.123.45.1:24265: EOF
2017-02-28T16:20:11.064377515Z I0228 16:20:11.064303       1 logs.go:41] http: TLS handshake error from 10.123.45.1:24269: remote error: tls: unknown certificate authority
```
 
Logs from control manager fedcluster-controller-manager-3452268026-ulyz6:
```
2017-02-28T16:19:44.897940307Z E0228 16:19:44.897889       1 reflector.go:199] k8s.io/kubernetes/federation/pkg/federation-controller/service/servicecontroller.go:266: Failed to list *v1.Service: Get https://fedcluster-apiserver/api/v1/services?resourceVersion=0: dial tcp 100.71.86.69:443: i/o timeout
2017-02-28T16:19:57.116456809Z E0228 16:19:57.116345       1 clustercontroller.go:117] Error monitoring cluster status: Get https://fedcluster-apiserver/apis/federation/v1beta1/clusters: dial tcp 100.71.86.69:443: i/o timeout
2017-02-28T16:20:15.521432571Z E0228 16:20:15.521335       1 reflector.go:199] k8s.io/kubernetes/federation/pkg/federation-controller/util/federated_informer.go:294: Failed to list *v1beta1.Cluster: Get https://fedcluster-apiserver/apis/federation/v1beta1/clusters?resourceVersion=0: dial tcp 100.71.86.69:443: i/o timeout
2017-02-28T16:20:15.524803604Z E0228 16:20:15.524762       1 reflector.go:199] k8s.io/kubernetes/federation/pkg/federation-controller/daemonset/daemonset_controller.go:260: Failed to list *v1beta1.DaemonSet: Get https://fedcluster-apiserver/apis/extensions/v1beta1/daemonsets?resourceVersion=0: dial tcp 100.71.86.69:443: i/o timeout
```
",closed,False,2018-02-02 14:35:26,2018-07-24 02:17:55
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/208,https://api.github.com/repos/kubernetes/federation/issues/208,[Federation] Joining cluster service accounts roles created by kubefed join are very broad,"<a href=""https://github.com/perotinus""><img src=""https://avatars1.githubusercontent.com/u/5122391?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [perotinus](https://github.com/perotinus)**
_Wednesday Mar 01, 2017 at 20:25 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/42355_

----

This is a follow-up to #41267.

Currently, the service account created in a joining cluster by `kubefed join` has role-based access to all APIs and all resources in all namespaces. We should determine if there is a tighter reasonable subset, or decide that this is conceptually OK. Making a tighter subset will be a small maintenance burden, and will potentially cause issues if people are trying to use older versions of kubefed with newer versions of the control plane.
",closed,False,2018-02-02 14:35:36,2018-07-24 02:17:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/209,https://api.github.com/repos/kubernetes/federation/issues/209,[Federation] Federation should fully support clusters with a previous API version.,"<a href=""https://github.com/marun""><img src=""https://avatars3.githubusercontent.com/u/451477?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [marun](https://github.com/marun)**
_Thursday Apr 06, 2017 at 14:42 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/44160_

----

As per a [thread](https://groups.google.com/d/msg/kubernetes-sig-federation/AmByUrm1rz0/vNQUdlv7AgAJ) on the mailing list, it appears that federation will only be possible between a cluster whose API version is >= the API version of the federation control plane.  This suggests that the control plane should be explicitly checking the API version indicated for a member cluster and refusing to federate to a cluster with an older API than is in use by the control plane.

cc: @kubernetes/sig-federation-bugs @kubernetes/sig-api-machinery-misc 
",closed,False,2018-02-02 14:35:44,2018-07-24 02:17:54
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/210,https://api.github.com/repos/kubernetes/federation/issues/210,[Federation] How should kubefed handle cluster access,"<a href=""https://github.com/irfanurrehman""><img src=""https://avatars0.githubusercontent.com/u/10027921?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [irfanurrehman](https://github.com/irfanurrehman)**
_Wednesday Apr 12, 2017 at 08:30 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/44384_

----

This issue is a follow up to https://github.com/kubernetes/kubernetes/issues/41267
Perhaps an umbrella issue is needed to group all the related ones.

Prior to release 1.6 kubefed used to upload the credentials provided by the user in kubeconfig for all operations. With issue https://github.com/kubernetes/kubernetes/issues/41267, the intention was to use service accounts (possibly different ones) to access clusters and also for the communication within federation control plane(controller manager to api server).
Now there are multiple questions that need answers (and solutions to possible problems).

1. Who sets up these service accounts?
	We tried setting this up automatically while bootstrapping control place in kubefed init (https://github.com/kubernetes/kubernetes/pull/40392), but ran into problems with older clusters which do not use the authorizer as RBAC.
	We then further tried to guess if the cluster uses the RBAC authorizer by checking if the cluster exposes the RBAC api and tried using the right API version also, but again ran into problems (https://github.com/kubernetes/kubernetes/issues/42559), because the cluster might not be using the RBAC as the authorizer plugin using ```--authorization-mode=RBAC```.
	Apperantly, and for genuine security reasons, there is no direct way of guessing what authorizer is being used by the cluster.
2. Does federation handle the cluster skew and to what extent?
	Apart from the previous questions/problems, there is an added problem of older clusters. RBAC as authorizer was itself introduced in 1.4. Further service account as a feature might itself not be there in even older clusters.
	Do we need to support a vast range of cluster version skew (which might not be a good idea). If that is the case, we might need to choose using credentials as is, or through service accounts (depending on availability of features).
3. As a security scenario, the admin user (who is bootstrapping control plane and registering clusters) might want to use different credentials then the one he is accessing the clusters with currently (mostly admin).
	Kubefed uploaded the same credentials as in the kubeconfig under use to access the clusters, (as of now for joining clusters).

Based on discussion with @liggitt, @madhusudancs and @perotinus, the suggestions to handle some of the above problems are as below.

1. Kubefed cannot guess which authorizer is used. It however can have a default behaviour and some additional paths with users.

- Assumes the clusters to be using RBAC, and if no other detail (as suggested next with  ```--use-service-account```) is specified, it tries to set up the right service account and needed roles as a default. We also suggest RBAC as the preferred usage in documentation.
- Also provides an alternative option to use the pre-setup service account to be used (additional argument --use-service-account) in both init and join, which can override this behaviour. If the user intends to use a different authoriser then RBAC, they will need to use this mechanism.
- As an alternative path, user can override the default behaviour to using just the access credentials (for example by specifying ```--use-credentials-only```) (and ```--use-credential-kubeconfig=``` if a different set of access credentials need to be uploaded).
	
Which option to use as the default path, is open for suggestions.

2. As listed in 1, its best to keep the cluster skew to as less as possible, there are bigger problems to address in this scope and discussed here https://github.com/kubernetes/kubernetes/issues/44160.

3. As listed in the 3rd option in the solution to 1 above.

Some additional pointers are:

- Kubefed can still discover the RBAC api versions when using the path of creating SA and roles.
- In case some advanced features are needed k8s exposes access review apis https://kubernetes.io/docs/admin/authorization/#checking-api-access
",closed,False,2018-02-02 14:36:42,2018-07-23 17:08:52
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/211,https://api.github.com/repos/kubernetes/federation/issues/211,Federation should use namespace scoped service accounts to talk to underlying clusters,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Friday Apr 21, 2017 at 00:43 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/44743_

----

Background: Federation today uses a single identity to send all its requests to a cluster. This requires that single identity to have the authorization to do everything that federation is being used for.
For ex: if 2 teams are using federation to federate resources in 2 namespaces, they need to create a service account for federation that has CRUD access to both namespaces. This single service account becomes more powerful then the service accounts of each teams (which have access to only a single namespace). This is not good.

Solution:
A way to fix this is by updating federation to accept multiple service accounts (one per namespace) instead of only a single one. Federation will then use the corresponding service account while sending requests in that namespace.

Things I need to figure out:
* How will create federated namespace work?
* How to give federation access to service accounts? Pass the token? Does deleting the service account revoke it?
* Are SA's namespaced by default?
* Does editor on namespace include authorization to CRUD SA in that ns?

Filing this early while I figure out the details to get any high level feedback early.


cc @kubernetes/sig-auth-misc @kubernetes/sig-federation-feature-requests @erictune @smarterclayton @liggitt 
",closed,False,2018-02-02 14:36:56,2018-07-24 02:17:56
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/212,https://api.github.com/repos/kubernetes/federation/issues/212,Should be able to restrict federation to specific namespaces,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Monday May 01, 2017 at 21:54 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/45189_

----

Came up while discussing with @marun that federation controllers assume that they have access to all namespaces and watch resources on all namespaces ([example](https://github.com/kubernetes/kubernetes/blob/0df02517254bc59d26086bd350a5e457e1961933/federation/pkg/federation-controller/deployment/deploymentcontroller.go#L131))

It should be possible to restrict federation to only specific namespaces.

One way to do this would be to update the controllers to first list all namespaces in federation control plane and then watch resources in those namespaces. Open to other suggestions.

cc @kubernetes/sig-federation-misc @marun 
",closed,False,2018-02-02 14:37:12,2018-07-24 03:18:53
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/213,https://api.github.com/repos/kubernetes/federation/issues/213,Identify the server in audit webhook,"<a href=""https://github.com/crassirostris""><img src=""https://avatars1.githubusercontent.com/u/1756505?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [crassirostris](https://github.com/crassirostris)**
_Thursday Aug 03, 2017 at 09:14 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/50076_

----

Spinning up a discussion from https://github.com/kubernetes/kubernetes/issues/48561

> It would be nice to identify the server that sent the audit event, i.e. the aggregator vs. an end-user apiserver.

It can be implemented right now by adding a parameter to the webhook address on the deployment stage and process this parameter later in the pipeline. However, it's (relatively) easy to add information to the API and we want to make sure that the information we currently have is in the right state before going to beta / GA.

One thought is that it probably doesn't make sense to have this information in the event object, because events from different sources rarely mix up (until the backend anyway) and it would just mean the same information copied all over the request to the webhook. WDYT about adding this information to the EventList object?

/cc @tallclair @sttts @CaoShuFeng @ericchiang
",open,False,2018-02-02 14:37:28,2018-02-23 15:36:43
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/214,https://api.github.com/repos/kubernetes/federation/issues/214,federation: Creating a Type LoadBalancer federated service and federated ingress can lead to resource leak,"<a href=""https://github.com/nikhiljindal""><img src=""https://avatars0.githubusercontent.com/u/10199099?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [nikhiljindal](https://github.com/nikhiljindal)**
_Monday Sep 11, 2017 at 23:34 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/52315_

----

Steps to repro:
Create a federated service of type LoadBalancer and a federated ingress and then delete them.
Expected:
All GCP resources (health checks, firewall rules, instance groups, backend service, etc) should be deleted when service and ingress are deleted.
Actual:
GCP Health check and firewall rules are leaked sometimes.

Explanation:
In kubernetes release 1.7, we [updated the service controller](https://github.com/kubernetes/kubernetes/pull/45524) to create health check and firewall rules whose names are generated using providerID/clusterID ([providerID if it exists, else clusterID](https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/gce/gce_clusterid.go#L111-L132)) and since providerID is set by federated ingress controller, service controller looks for a different name if ingress controller sets it after service controller created GCP resources. This race condition between the 2 controller leads to service controller leaking the original health check and firewall rule that it had created.

Possible fixes:
* Service controller should wait for providerID/clusterID to by synced before creating any GCP resources (this will not work in environments where ingress controller is disabled).
* Service controller should have the [same logic](https://github.com/kubernetes/kubernetes/blob/db809c0eb7d33fac8f54d8735211f2f3a8fc4214/federation/pkg/federation-controller/ingress/ingress_controller.go#L455) of syncing providerID and clusterID across clusters as ingress controller (need to ensure that the 2 controllers dont fight with each other).
",closed,False,2018-02-02 14:37:50,2018-07-24 01:16:26
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/215,https://api.github.com/repos/kubernetes/federation/issues/215,Federated deployments cannot be deleted,"<a href=""https://github.com/jsgoller1""><img src=""https://avatars3.githubusercontent.com/u/1567977?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [jsgoller1](https://github.com/jsgoller1)**
_Sunday Oct 08, 2017 at 13:20 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/53566_

----

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug
/sig cli
/sig federation
@kubernetes/sig-federation

**What happened**:
I have a federated cluster composed of one host cluster and one joined cluster running in AWS. They were created following the steps in the documentation, and I am able to see via the kubernetes dashboard that all components are running correctly. 

To test the federated setup, I used a simple nginx deployment also from the docs, stored in `deployment.proxy.yml`:
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: proxy
spec:
  replicas: 5 # tells deployment to run 2 pods matching the template
  template: # create pods using pod definition in this template
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```
I then execute `kubectl create --context=federation -f deployment.proxy.yml`. Assuming I don't change `apiVersion`, the deployment runs fine and almost immediately reaches the desired count (if I use `apiVersion: apps/v1beta2` or any other API version than `extensions/` with my federated setup, I get errors and it won't start). I then go to tear down the deployment by executing `kubectl delete deployment --context=federation proxy`.

However, kubectl reports `error: timed out waiting for the condition`. The deployment instances spin down to zero, but the deployment won't disappear: 
```
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
proxy     0         0         0            0           1h
```
I can delete them via the dashboard, or by switching to joined cluster's context and using `kubctl delete deployment proxy`, but even after kubectll reports they are deleted successfully, they reappear and still cannot be deleted from the federation context.

**What you expected to happen**:
The deployment to be deleted. This works fine if I'm not using my federated context.

**How to reproduce it (as minimally and precisely as possible)**:
Create a federated setup via the docs, launch the above job, and then try to delete it.

**Environment**:
- Kubernetes version:
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"",
```
- Cloud provider or hardware configuration: AWS
- OS (e.g. from /etc/os-release): Ubuntu 16.04

",closed,False,2018-02-02 14:38:11,2018-08-20 06:17:44
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/216,https://api.github.com/repos/kubernetes/federation/issues/216,`kubefed join` do not clear garbage data if join failed,"<a href=""https://github.com/gyliu513""><img src=""https://avatars1.githubusercontent.com/u/4461983?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [gyliu513](https://github.com/gyliu513)**
_Saturday Oct 14, 2017 at 02:52 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/53928_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>

 /kind bug

> /kind feature


**What happened**:

When I call `kubefed join`, it failed due to some invalid configuration, and then after update the configuration, call `kubefed join` again, it reports that there is an sa already in use and cannot add the cluster.

**What you expected to happen**:

We should clear the garbage sa, clusterrole, clusterrolebindings etc if `kubefed init` failed.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration**:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:

/sig multicluster

",closed,False,2018-02-02 14:38:40,2018-07-13 18:14:13
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/217,https://api.github.com/repos/kubernetes/federation/issues/217,UP-TO-DATE value of federation deployment is always 0,"<a href=""https://github.com/gyliu513""><img src=""https://avatars1.githubusercontent.com/u/4461983?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [gyliu513](https://github.com/gyliu513)**
_Saturday Oct 14, 2017 at 13:04 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/53930_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>

/kind bug

> /kind feature


**What happened**:

```
[root@ib17b03 ~]# kubectl  version
Client Version: version.Info{Major:""1"", Minor:""7+"", GitVersion:""v1.7.3-11+f747daa02c9ffb"", GitCommit:""f747daa02c9ffba3fabc9f679b5003b3f7fcb2c0"", GitTreeState:""clean"", BuildDate:""2017-08-28T07:04:45Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7+"", GitVersion:""v1.7.3-11+f747daa02c9ffb"", GitCommit:""f747daa02c9ffba3fabc9f679b5003b3f7fcb2c0"", GitTreeState:""clean"", BuildDate:""2017-08-28T07:04:45Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

UP-TO-DATE value of federation deployment is always 0.

This is the federation deployment
```
# kubectl get deployment --all-namespaces
NAMESPACE   NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
db          mariadb     1         1         0            0           1h
dbs         wordpress   1         1         0            0           1h
default     nginx       1         1         0            1           2h
dev         nginx       2         2         0            2           2h
qa          nginx       2         2         0            2           1h

# kubectl get clusters
NAME       STATUS    AGE
cluster1   Ready     3h
cluster2   Ready     2h
```

And there is deployment from child cluster
```
# kubectl get deployment --namespace=qa
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     2         2         2            2           1h

# kubectl get deployment --namespace=dev
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     2         2         2            2           2h
```

The `UP-TO-DATE` value of federation deployment should be consistent with child cluster.

**What you expected to happen**:

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration**:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:

/sig multicluster

",closed,False,2018-02-02 14:57:23,2018-07-24 02:17:57
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/218,https://api.github.com/repos/kubernetes/federation/issues/218,Federated service discovery failed in hybrid cloud: OpenStack + GKE,"<a href=""https://github.com/arthur0""><img src=""https://avatars1.githubusercontent.com/u/7016388?v=4"" align=""left"" width=""96"" height=""96"" hspace=""10""></img></a> **Issue by [arthur0](https://github.com/arthur0)**
_Wednesday Nov 08, 2017 at 16:23 GMT_
_Originally opened as https://github.com/kubernetes/kubernetes/issues/55326_

----

<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:

Following the  [guide](https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/) to set up federation with kubefed and [another one](https://kubernetes.io/docs/tasks/federation/set-up-coredns-provider-federation/) to set up CoreDNS as DNS provider, is not possible to have service discovery based on region and availability zone for the OpenStack cluster.
```
|   Clusters   |   Provider    |  Region      | Zone           | FCP | 
|--------------|---------------|--------------|----------------|-----|
| os-br        |   OpenStack   | RegionOne    | nova           |  *  |
| gke-us       |   GCE         | us-central1  | us-central1-a  |     |
```
The deployment and service are correctly spread in the two clusters. 
The `status.loadBalancer.ingress` for the service in the `os-br` cluster has two IPs, but it's already fixed by this [PR](https://github.com/kubernetes/kubernetes/pull/52609).
```
$ kubectl get svc -n ns --context gke-us
NAME       TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)
nodename   LoadBalancer   10.11.249.249   104.154.142.100   80:31814/TCP
$ kubectl get svc -n ns --context os-br
NAME       TYPE           CLUSTER-IP     EXTERNAL-IP                 PORT(S) 
nodename   LoadBalancer   10.107.93.68   10.11.4.210,150.165.85.10   80:31240/TCP
```
For the following scenarios, I ran the busybox pod in the same namespace as my service:
`kubectl -n ns exec -ti busybox -- sh`

**Scenario 1: Without region and zone.**
```
/ # nslookup nodename.ns.federation.svc.example.com
Address 1: 104.154.142.100 100.142.154.104.bc.googleusercontent.com
Address 2: 150.165.85.10
Address 3: 10.11.4.210
```

**Scenario 2: With region and zone**
```
/ # nslookup nodename.ns.federation.svc.us-central1-a.us-central1.example.com
Address 1: 104.154.142.100 100.142.154.104.bc.googleusercontent.com

/ # nslookup nodename.ns.federation.svc.nova.regionone.example.com
nslookup: can't resolve 'nodename.ns.federation.svc.nova.regionone.example.com'
```
**Scenario 3: Only region**
```
/ # nslookup nodename.ns.federation.svc.us-central1.example.com
Address 1: 104.154.142.100 100.142.154.104.bc.googleusercontent.com

/ # nslookup nodename.ns.federation.svc.regionone.example.com
nslookup: can't resolve 'nodename.ns.federation.svc.regionone.example.com'
```

**What you expected to happen**:
For the three scenarios, I don't understand the `100.142.154.104.bc.googleusercontent.com` register in the service hosted in GCE.
I think the `Address 1` should have only one register `104.154.142.100` and even if the `100.142.154.104.bc.googleusercontent.com` is a valid registry, it should be added in as a new address.

For scenarios 2 and 3 I expect to have service discovery, based on region and zone, for services hosted in OpenStack cluster.

**How to reproduce it (as minimally and precisely as possible)**:
In the OpenStack cluster:
[1](https://kubernetes.io/docs/tasks/federation/set-up-coredns-provider-federation/) Deploy etcd-operator and CoreDNS
[2](https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/) Set up federation using Kubefed

**Anything else we need to know?**:
The etcd-operator seems healthy:
```
kubectl run --rm -i --tty fun --image quay.io/coreos/etcd --restart=Never -- /bin/sh
/ # ETCDCTL_API=3 etcdctl --endpoints http://etcd-cluster-client.default:2379 put foo bar
OK
/ # ETCDCTL_API=3 etcdctl --endpoints http://etcd-cluster-client.default:2379 get foo
foo
bar
/ # ETCDCTL_API=3 etcdctl --endpoints http://etcd-cluster-client.default:2379 del foo
1
```

Initially, I thought the problem might be if the coredns version, so I tried with newest versions, but I got the same result. My current coredns chart config:
```
replicaCount: 3
image:
  repository: coredns/coredns
  tag: ""011""
  pullPolicy: IfNotPresent
isClusterService: false
serviceType: ""NodePort""
middleware:
  prometheus:
    enabled: false
  errors:
    enabled: true
    file: ""stdout""
  kubernetes:
    enabled: false
  etcd:
    enabled: true
    zones:
    - ""example.com.""
    endpoint: ""http://etcd-cluster-client.default:2379""
```

Logs from Federation Controller Manager: [fed-controller-mannager.txt](https://github.com/kubernetes/kubernetes/files/1454774/fed-controller-mannager.txt)


**Environment**:
- Kubernetes version (use `kubectl version`):
```
$ kubectl version --context os-br
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.2"", GitCommit:""bdaeafa71f6c7c04636251031f93464384d54963"", GitTreeState:""clean"", BuildDate:""2017-10-24T19:48:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.2"", GitCommit:""bdaeafa71f6c7c04636251031f93464384d54963"", GitTreeState:""clean"", BuildDate:""2017-10-24T19:38:10Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
$ kubectl version --context gke-us
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.2"", GitCommit:""bdaeafa71f6c7c04636251031f93464384d54963"", GitTreeState:""clean"", BuildDate:""2017-10-24T19:48:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.1-gke.1"", GitCommit:""aba494e68a76583d2d7d1b9c97e4a97a19c3a920"", GitTreeState:""clean"", BuildDate:""2017-10-27T23:54:39Z"", GoVersion:""go1.8.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```
cc @shashidharatd 
/sig multicluster
/sig openstack
",closed,False,2018-02-02 14:57:31,2018-02-02 19:25:47
federation,xtophs,https://github.com/kubernetes/federation/pull/219,https://api.github.com/repos/kubernetes/federation/issues/219,Federation Provider for Azuredns,"**What this PR does / why we need it**:
Adds Azure DNS support to federation

**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #
Fixes Issue #44874 

**Special notes for your reviewer**:

**Release note**:
```release-note

The federation provider for AzureDNS allows for using AzureDNS to manage DNS records for federated kubernetes clusters. The clusters do not have to run in Azure.

Running the provider requires

	• an active Azure subscription with a DNS zone configured. See Azure Docs on how to [configure a DNS Zone](https://docs.microsoft.com/en-us/azure/dns/dns-zones-records).
	• Service Principal credentials with permissions to manage the DNS Zone

To configure federation, you follow the steps at: https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/

Note, that at this time, a cluster created with acs-engine may need kubernetes RBAC (not Azure RBAC) configured first. See detailed instructions link below.

The DNS provider needs a configuration file with the information to access the Azure Subscription. The format of the config file is:

[Global]
subscription-id = [Azure Subscription ID]
tenant-id = [Azure Tenant ID where the Service Principal is created]
client-id = [Service Principal Client ID]
secret = [Service Principal Secret]
resourceGroup = [Azure Resource Group where the DNS Zone is provisioned]

You can get Subscription ID and Tenant ID by running the following Azure CLI 2.0 command:

`az account show`


You can create a Service Principal with write access to the resource group:

`az ad sp create-for-rbac -n ""http://MyApp"" --role contributor --scopes /subscriptions/[subscription id]/resourceGroups/[resource group name]`

With the provider config file, you can initialize a federation:

`kubefed init myfederation --host-cluster-context=[cluster name] --dns-provider=""azure-azuredns"" --dns-zone-name=""[dns Zone]"" --dns-provider-config=[path to provider config file]`

Join a federation:

`kubefed join [your cluster name] --host-cluster-context=[your cluster name] --cluster-context=[your cluster name]`

And verify that the cluster was joined:

kubectl get clusters
NAME STATUS AGE
fed-west Ready 13s

For complete instructions setting up federated clusters in Azure see: https://github.com/xtophs/k8s-setup-federation-cluster
```",closed,True,2018-02-02 17:04:50,2018-03-18 19:59:00
federation,xtophs,https://github.com/kubernetes/federation/issues/220,https://api.github.com/repos/kubernetes/federation/issues/220,Federation dependencies should not be managed by glide,"Azure SDK dependencies in `vendor/GitHub.com/Azure/azure-sdk-for-go` imported the entire SDK instead of only importing dependent files like it's done in kubernetes/kubernetes.

As @quinton-hoole points out

> Based on the following issue, that seems like the wrong direction.  As far as I understand, kubernetes/kubernetes decided to stick with godeps until ""go dep"" is usable, and not, ever, use glide.
> 
> https://github.com/kubernetes/kubernetes/issues/44873
> 

Dependencies should be managed with `godeps` or `go dep`",open,False,2018-02-02 21:59:48,2019-02-17 18:21:39
federation,xtophs,https://github.com/kubernetes/federation/issues/221,https://api.github.com/repos/kubernetes/federation/issues/221,Version mismatch between vendored libraries in vendor/k8s.io/kubernetes and vendor/github.com/azure,"The versions of vendor/github.comAzure/azure-sdk-for-go do not match what's required by vendor/k8s.io/kubernetes/pkg/cloudprovider/providers/azure.

Error on build is:
```
# make
[...]
# k8s.io/federation/vendor/k8s.io/kubernetes/pkg/cloudprovider/providers/azure
vendor/k8s.io/kubernetes/pkg/cloudprovider/providers/azure/azure.go:170:4: cannot use *oauthConfig (type adal.OAuthConfig) as type string in argument to adal.NewServicePrincipalTokenFromMSI
```

This will be fixed with PR #219 
",closed,False,2018-02-06 01:38:54,2018-07-06 19:30:13
federation,xtophs,https://github.com/kubernetes/federation/issues/222,https://api.github.com/repos/kubernetes/federation/issues/222,fcp container does not run ,"Building the fcp container with `make quick-release`

```
+++ [0207 18:15:08] Syncing out of container
+++ [0207 18:15:13] Building tarball: src
+++ [0207 18:15:13] Building tarball: client linux-amd64
+++ [0207 18:15:13] Waiting on tarballs
+++ [0207 18:15:42] Building tarball: server linux-amd64
+++ [0207 18:15:42] Building fcp image for arch: amd64
+++ [0207 18:16:25] Deleting fcp image gcr.io/google_containers/fcp-amd64:v1.9.0-alpha.2.60_430416309f9e58-dirty
+++ [0207 18:17:04] Building tarball: final
+++ [0207 18:17:04] Building tarball: test
+++ [0207 18:17:15] Completed federation release build.
```

trying to run fcp container via `kubefed init` or `docker run` does not work. Container image does not appear to be valid.

federation apiserver fails with: 
```
kubectl logs apiserver-798b567969-6bndl apiserver --namespace=federation-system
container_linux.go:247: starting container process caused ""exec: \""/fcp\"": stat /fcp: no such file or directory""
```

Trying to run the container on the build machine fails as well.

```
$ docker import  _output/release-images/amd64/fcp-amd64.tar 
sha256:a344f4e7764c092783183b14a25d31a32a6b7f8df353e8b426190e7b2fd0a1d6
$ docker run -it a344f ls /
docker: Error response from daemon: invalid header field value ""oci runtime error: container_linux.go:247: starting container process caused \""exec: \\\""ls\\\"": executable file not found in $PATH\""\n"".
```

Trying to import container from file system instead of the tarball results in the same error:
```
$ tar xvf ../_output/release-images/amd64/fcp-amd64.tar 
21097d87418fd37b93df7ed4212b7b8b10222f6feda04f2f16b6f573998ae55d/
21097d87418fd37b93df7ed4212b7b8b10222f6feda04f2f16b6f573998ae55d/VERSION
21097d87418fd37b93df7ed4212b7b8b10222f6feda04f2f16b6f573998ae55d/json
21097d87418fd37b93df7ed4212b7b8b10222f6feda04f2f16b6f573998ae55d/layer.tar
2fee09a140680be14fbb1890aafeb7e65792f4eac370251b8d63ecdcb5151b11/
2fee09a140680be14fbb1890aafeb7e65792f4eac370251b8d63ecdcb5151b11/VERSION
2fee09a140680be14fbb1890aafeb7e65792f4eac370251b8d63ecdcb5151b11/json
2fee09a140680be14fbb1890aafeb7e65792f4eac370251b8d63ecdcb5151b11/layer.tar
53f2f90db4320420f7da8f527e5f14439c15318482c8a112490de4e52f45164f.json
b2f6553d135e910021fb76c8e93059f13024aebf714f0ac537ef09fa02c34b27/
b2f6553d135e910021fb76c8e93059f13024aebf714f0ac537ef09fa02c34b27/VERSION
b2f6553d135e910021fb76c8e93059f13024aebf714f0ac537ef09fa02c34b27/json
b2f6553d135e910021fb76c8e93059f13024aebf714f0ac537ef09fa02c34b27/layer.tar
d0868bc098d76f12f2286a7af5ec1e7c2a78bb3d140a8767e93885a713923ea7/
d0868bc098d76f12f2286a7af5ec1e7c2a78bb3d140a8767e93885a713923ea7/VERSION
d0868bc098d76f12f2286a7af5ec1e7c2a78bb3d140a8767e93885a713923ea7/json
d0868bc098d76f12f2286a7af5ec1e7c2a78bb3d140a8767e93885a713923ea7/layer.tar
e2731de3eb5b9315656612804310de2b2faa76443bd0c03b10c32174ed547cbc/
e2731de3eb5b9315656612804310de2b2faa76443bd0c03b10c32174ed547cbc/VERSION
e2731de3eb5b9315656612804310de2b2faa76443bd0c03b10c32174ed547cbc/json
e2731de3eb5b9315656612804310de2b2faa76443bd0c03b10c32174ed547cbc/layer.tar
manifest.json
repositories

$ sudo tar -c . | docker import -
sha256:e75446b4309bdade844eb5dfc79cb145e720590054844f01ac7b8cab237da586

$ docker run -it e7544 ls /
docker: Error response from daemon: invalid header field value ""oci runtime error: container_linux.go:247: starting container process caused \""exec: \\\""ls\\\"": executable file not found in $PATH\""\n"".

$ docker run -it e7544 /bin/bash
docker: Error response from daemon: invalid header field value ""oci runtime error: container_linux.go:247: starting container process caused \""exec: \\\""/bin/bash\\\"": stat /bin/bash: no such file or directory\""\n"".

$ docker run -it e7544 /bin/sh
ERRO[0001] error getting events from daemon: net/http: request canceled 
docker: Error response from daemon: invalid header field value ""oci runtime error: container_linux.go:247: starting container process caused \""exec: \\\""/bin/sh\\\"": stat /bin/sh: no such file or directory\""\n"".

```

Looking into the folder structure confirms that `fcp` should be in the container:
```
$ tar tvf e2731de3eb5b9315656612804310de2b2faa76443bd0c03b10c32174ed547cbc/layer.tar 
-rwxr-xr-x 0/0       183523998 2018-02-07 18:15 fcp
```

Build box info:
```
$ uname -a
Linux fedbuild 4.13.0-1005-azure #7-Ubuntu SMP Mon Jan 8 21:37:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

$ docker version
Client:
 Version:      1.13.1
 API version:  1.26
 Go version:   go1.6.2
 Git commit:   092cba3
 Built:        Thu Nov  2 20:40:23 2017
 OS/Arch:      linux/amd64

Server:
 Version:      1.13.1
 API version:  1.26 (minimum version 1.12)
 Go version:   go1.6.2
 Git commit:   092cba3
 Built:        Thu Nov  2 20:40:23 2017
 OS/Arch:      linux/amd64
 Experimental: false
```",closed,False,2018-02-07 18:31:24,2018-02-08 00:46:42
federation,xtophs,https://github.com/kubernetes/federation/pull/223,https://api.github.com/repos/kubernetes/federation/issues/223,Clarified how to run container from make quick-release,Added clarification on how to run build output to README.md,closed,True,2018-02-08 00:46:12,2018-03-16 09:01:24
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/224,https://api.github.com/repos/kubernetes/federation/issues/224,Fix for non matching api defaults for deployment api,"Deployment defaults were not updated in extensions deployment as proposed in https://github.com/kubernetes/kubernetes/pull/34339, but updated in https://github.com/kubernetes/kubernetes/pull/39683 as part of new deployment endpoint.

Recently, after merging https://github.com/kubernetes/kubernetes/pull/58832 and https://github.com/kubernetes/kubernetes/pull/58854, federation builds break for example https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/ci-federation-e2e-gce/3080.

A better mechanism to avoid running into this problem is that federation also updates all the apis that it uses to talk to k8s and to store in its registry. This will eventually happen and while we chalk out a plan for that, the current fix is to use an api type with same defaulting values in federation to store the deployment object in federation registry.",closed,True,2018-02-11 13:33:55,2018-02-13 08:43:51
federation,fkorotkov,https://github.com/kubernetes/federation/issues/225,https://api.github.com/repos/kubernetes/federation/issues/225,Upgrade Instructions,"Hello there,

I configured a federation cluster using version 1.8.5 a few months back. I've just upgraded my GKE cluster to version 1.9.2 of k8s and I was wondering how to upgrade `controller-manager` and `apiserver`. It doesn't seem to be that straightforward and I haven't found an instruction beside [`deploy/cluster/upgrade.sh` script](https://github.com/kubernetes/federation/blob/master/deploy/cluster/upgrade.sh) which suggests just to bump version of `gcr.io/google_containers/fcp-amd64` image for both `controller-manager` and `apiserver` deployments.

The confusing part to me here is that `gcr.io/google_containers/fcp-amd64` is not on `gcr.io` and according to `federation-up.sh` I should [push it myself](https://github.com/kubernetes/federation/blob/546b9812a76dd70cf45c48db85c4a343a87ab6f9/deploy/cluster/federation-up.sh#L139-L152).

I would really appreciate if someone with better understanding will confirm my guesses about the upgrade process.
",closed,False,2018-02-12 14:42:00,2018-02-12 18:34:40
federation,fkorotkov,https://github.com/kubernetes/federation/issues/226,https://api.github.com/repos/kubernetes/federation/issues/226,Getting kubefed instructions are not obsolete,"Hello there,

According to k8s federation documentation here: https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/#getting-kubefed

In order to install `kubefed` one should run:

```
# OS X
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/kubernetes-client-darwin-amd64.tar.gz
tar -xzvf kubernetes-client-darwin-amd64.tar.gz
```

Right now the stable version is `1.9.3` and `kubernetes-client-darwin-amd64.tar.gz` for this version doesn't contain `kubefed` executable.",closed,False,2018-02-12 14:45:21,2018-02-23 15:15:43
federation,george-oakling,https://github.com/kubernetes/federation/issues/227,https://api.github.com/repos/kubernetes/federation/issues/227,Current status od federation,"1. As noted by https://github.com/kubernetes/federation/issues/226 the current version is not packaged to standardn K8s distribution.

2. If you install it by snap, you get the redirect error right after running the command (with or without params). 

3. If you download it manually from https://storage.googleapis.com/kubernetes-release/release/v1.8.7/kubernetes-client-linux-amd64.tar.gz and unpack and run it in accordance to the whole process, you end up with Error from server (Forbidden): roles.rbac.authorization.k8s.io ""federation-system:federation-controller-manager"" is forbidden: attempt to grant extra privileges.

My question is therefore: what is the current status of Kubernetes federation? Should we consider trying it? Are there any public plans that could be shared with us? I couldnt find any info on that, and as it is part of the official Kubernetes Github project, I was expecting a little bit more :)

I will send some PRs to documentation to update the download link and issue a bug in snap package.",closed,False,2018-02-12 16:07:07,2018-07-20 09:50:29
federation,mastamark,https://github.com/kubernetes/federation/issues/228,https://api.github.com/repos/kubernetes/federation/issues/228,batch/v1beta1 for 1.9.x?,"I'm curious if we plan to bring batch/v1beta1 into the federation api.

I see this here:  https://github.com/kubernetes/federation/commit/606e2c57e4866bf987221f7677a8ae0e6c749488#diff-a1329dbefeeb924bbbadaa79a76e4ae3R1251 which does indeed seem to identify that there was issues with bringing it in.

I'm essentially looking for this bad boy right here:  https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#cronjob-v1beta1-batch

My federated endpoint (fcp built from master) shows the following even with the api server flag (--apiserver-arg-overrides=""--runtime-config=api/all=true"") to handed along to kubefed:
* autoscaling/v1
* batch/v1
* extensions/v1beta1
* federation/v1beta1
* v1",closed,False,2018-02-12 22:17:09,2018-02-13 17:21:38
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/229,https://api.github.com/repos/kubernetes/federation/issues/229,Update the image url to use the gce ci project.,"Apparantly the test-infra service account also does not have
permissions to push to gcr.io/google_containers.

/assign @shashidharatd ",closed,True,2018-02-13 11:42:03,2018-02-13 15:41:51
federation,irfanurrehman,https://github.com/kubernetes/federation/issues/230,https://api.github.com/repos/kubernetes/federation/issues/230,go get k8s.io/federation fails.,"As listed on slack by @ramonrue 

I'm trying to use the federation clientset in a go application of mine, however, I can't get the imports right.
I tried to use the same imports as the `federation-clustercontroller` uses, however, I get a `metadata not found` error.
Imports:
```federationv1beta1 ""k8s.io/federation/apis/federation/v1beta1""
clustercache ""k8s.io/federation/client/cache""
federationclientset ""k8s.io/federation/client/clientset_generated/federation_clientset""
```
And the error of `dep ensure`
```The following errors occurred while deducing packages:
  * ""k8s.io/federation/apis/federation/v1beta1"": unable to deduce repository and source type for ""k8s.io/federation/apis/federation/v1beta1"": unable to read metadata: go-import metadata not found
  * ""k8s.io/federation/client/clientset_generated/federation_clientset"": unable to deduce repository and source type for ""k8s.io/federation/client/clientset_generated/federation_clientset"": unable to read metadata: go-import metadata not found
  * ""k8s.io/federation/client/cache"": unable to deduce repository and source type for ""k8s.io/federation/client/cache"": unable to read metadata: go-import metadata not found
```

What are the right imports for the federation? ",closed,False,2018-02-13 12:06:09,2018-02-21 14:25:04
federation,jeis2497052,https://github.com/kubernetes/federation/pull/231,https://api.github.com/repos/kubernetes/federation/issues/231,Propose fix some typos,are these changes OK?,closed,True,2018-02-13 16:31:48,2018-06-04 14:49:23
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/232,https://api.github.com/repos/kubernetes/federation/issues/232,Update the default image name to fcp,"As the image now is available at the release location.
/assign @shashidharatd ",closed,True,2018-02-14 10:55:27,2018-02-14 12:44:52
federation,leahnp,https://github.com/kubernetes/federation/pull/233,https://api.github.com/repos/kubernetes/federation/issues/233,Added timeout flag and default 5 minute timeout ,"Added flag `--timeout` for users to control length of federation control plane initialization. Default is 5 minutes but can be overrode by passing `--timeout=<int representing minutes>` with `kubefed init`. 

Leveraged information and code from: https://github.com/kubernetes/kubernetes/pull/42296

Fixes: #118 ",closed,True,2018-02-15 02:54:19,2018-07-19 19:36:45
federation,mengesb,https://github.com/kubernetes/federation/issues/234,https://api.github.com/repos/kubernetes/federation/issues/234,kubefed hang on kubeadm cluster in AWS,"I've explored a number of kubernetes documents on stack and here in the various repos, and much of the documents. The Issue i'm experiencing is most similar to #101 however I'm deploying to a kubeadm generated cluster on AWS. While I know that kubeadm's target isn't AWS this is where we have deployed as a demo location. I'm trying to initalize a federation and I do have aws route53 zones i could be using however it seems to get stuck similar to https://github.com/kubernetes/kubernetes/issues/39271 - where an init seems to wait forever. When we inspected the federation service api ; it was waiting on external IP, which I'm not sure where it would need to go from there.",closed,False,2018-02-16 01:29:58,2018-08-11 15:33:21
federation,shashidharatd,https://github.com/kubernetes/federation/pull/235,https://api.github.com/repos/kubernetes/federation/issues/235,Fix deploy/cluster/log-dump.sh,"Fixes the failure in federation logging script caused due to changes in k/k.

https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/ci-federation-e2e-gce/3298

/assign @irfanurrehman 
/cc @kubernetes/sig-multicluster-bugs ",closed,True,2018-02-19 13:45:14,2018-02-19 18:04:38
federation,grahamhayes,https://github.com/kubernetes/federation/issues/236,https://api.github.com/repos/kubernetes/federation/issues/236,Feature Ask: OpenStack Designate DNS Provider,"As of 1.9.x only AWS, CoreDNS, and Google Cloud DNS are supported as DNS providers for federation.

There is work underway to add Azure [#130 / #219].

The ask is to implement a DNS provider for the DNS project in OpenStack (Designate).",closed,False,2018-02-19 15:48:22,2018-10-07 20:37:06
federation,shashidharatd,https://github.com/kubernetes/federation/pull/237,https://api.github.com/repos/kubernetes/federation/issues/237,Migrate from glide to golang/dep and also bump vendor,"This PR does following things.
- Migrate from glide to golang/dep as dependency management tool
- Bump the k8s dependency to v1.10.0-alpha.3 and adapt the usage in federation
- Refactor the test codes to not use `k8s.io/kubernetes/test/e2e/framework` (as it is one huge package and lot many unnecessary stuff gets imported. So pulled in only necessary funcs into a reusable package within federation).
- Few other miscellaneous stuff.

/cc @kubernetes/sig-multicluster-pr-reviews 
/assign @irfanurrehman 

p.s: Even though the PR is huge, most of it is either generated code or vendored code. So request to review commit by commit which is much easier.",closed,True,2018-02-19 19:28:30,2018-03-01 13:56:36
federation,chrisprobst,https://github.com/kubernetes/federation/issues/238,https://api.github.com/repos/kubernetes/federation/issues/238,Add option to setup kubeconfig for existing fcp,"When there is an existing fcp running, there should be an option to setup kubeconfig accordingly.
For instance: A new user working on the federated cluster has no context ""kfed"".

There is no way of setting the config up for this user without copy-paste an exsting .kube/config.
Not to speak of what happens when there is no existing .kube/config anymore (good luck, federation must be recreated...).

So there should be an option to kube setup-config-for-existing --context... 

Maybe I missed something, but I think this is a severe issue because you could easily lose the .kube/config.",closed,False,2018-02-25 18:40:09,2018-08-11 15:33:21
federation,shashidharatd,https://github.com/kubernetes/federation/pull/239,https://api.github.com/repos/kubernetes/federation/issues/239, Fix broken DNS CNAME chain for cross-cluster service discovery,"Partly addresses the issue in https://github.com/kubernetes/federation/issues/175

CNAME records were missed for the federated cluster without healthy service shards. This pr fixes the missing CNAME records issue.

/cc @kubernetes/sig-multicluster-bugs 
/assign @irfanurrehman ",closed,True,2018-02-27 15:29:12,2018-03-07 04:51:53
federation,fkorotkov,https://github.com/kubernetes/federation/issues/240,https://api.github.com/repos/kubernetes/federation/issues/240,Build kubefed binary for Mac,"It will be nice to have binaries of `kubefed` prebuilt for Mac OS. Right now GitHub release page has only links for linux binaries:

https://storage.cloud.google.com/kubernetes-federation-release/release/v1.9.0-alpha.3/federation-client-linux-amd64.tar.gz

It will be nice to have `federation-client-darwin-amd64.tar.gz` binaries as well.

PS maybe even binaries for Windows?",closed,False,2018-02-27 16:30:31,2018-08-12 22:03:22
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/241,https://api.github.com/repos/kubernetes/federation/issues/241,Federation bazel fix,"Fixes the issues creeping into `pull-federation-verify` because of the updates in latest `bazel tools and rules`.
/assign @shashidharatd ",closed,True,2018-02-28 05:00:34,2018-03-07 03:35:22
federation,quinton-hoole-zz,https://github.com/kubernetes/federation/issues/242,https://api.github.com/repos/kubernetes/federation/issues/242,Failed to create ingress in cluster: Invalid value: []extensions.IngressRule(nil): either `backend` or `rules` must be specified',"andrein [1:33 PM]
Hi, I finally got federation working on GKE (tested deployments and services) but I’m having trouble with federated ingress. I can create the federated ingress in the control plane, but it doesn’t propagate to the clusters. I get the following errors in the controller-manager logs:

```
E0228 21:28:35.246698       1 event.go:260] Could not construct reference to: '&v1beta1.Ingress{TypeMeta:v1.TypeMeta{Kind:"""", APIVersion:""""}, ObjectMeta:v1.ObjectMeta{Name:""hello-server"", GenerateName:"""", Namespac
e:""default"", SelfLink:"""", UID:"""", ResourceVersion:"""", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeco
nds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string{""ingress.federation.kubernetes.io/first-cluster"":""test-eu"", ""kubernetes.io/ingress.global-static-ip-name"":""federated-ingress""}, Owner
References:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""""}, Spec:v1beta1.IngressSpec{Backend:(*v1beta1.IngressBackend)(nil), TLS:[]v1beta1.IngressTLS(nil),
 Rules:[]v1beta1.IngressRule(nil)}, Status:v1beta1.IngressStatus{LoadBalancer:v1.LoadBalancerStatus{Ingress:[]v1.LoadBalancerIngress(nil)}}}' due to: 'selfLink was empty, can't make reference'. Will not report eve
nt: 'Normal' 'CreateInCluster' 'Creating ingress ""default/hello-server"" in cluster test-us'
E0228 21:28:35.250932       1 event.go:260] Could not construct reference to: '&v1beta1.Ingress{TypeMeta:v1.TypeMeta{Kind:"""", APIVersion:""""}, ObjectMeta:v1.ObjectMeta{Name:""hello-server"", GenerateName:"""", Namespac
e:""default"", SelfLink:"""", UID:"""", ResourceVersion:"""", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeco
nds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string{""kubernetes.io/ingress.global-static-ip-name"":""federated-ingress"", ""ingress.federation.kubernetes.io/first-cluster"":""test-eu""}, Owner
References:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""""}, Spec:v1beta1.IngressSpec{Backend:(*v1beta1.IngressBackend)(nil), TLS:[]v1beta1.IngressTLS(nil),
 Rules:[]v1beta1.IngressRule(nil)}, Status:v1beta1.IngressStatus{LoadBalancer:v1.LoadBalancerStatus{Ingress:[]v1.LoadBalancerIngress(nil)}}}' due to: 'selfLink was empty, can't make reference'. Will not report eve
nt: 'Normal' 'CreateInCluster' 'Creating ingress ""default/hello-server"" in cluster test-eu'
E0228 21:28:35.254593       1 ingress_controller.go:240] Error creating ingress ""default/hello-server"": Ingress.extensions ""hello-server"" is invalid: spec: Invalid value: []extensions.IngressRule(nil): either `bac
kend` or `rules` must be specified
E0228 21:28:35.254646       1 event.go:260] Could not construct reference to: '&v1beta1.Ingress{TypeMeta:v1.TypeMeta{Kind:"""", APIVersion:""""}, ObjectMeta:v1.ObjectMeta{Name:""hello-server"", GenerateName:"""", Namespac
e:""default"", SelfLink:"""", UID:"""", ResourceVersion:"""", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeco
nds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string{""kubernetes.io/ingress.global-static-ip-name"":""federated-ingress"", ""ingress.federation.kubernetes.io/first-cluster"":""test-eu""}, Owner
References:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""""}, Spec:v1beta1.IngressSpec{Backend:(*v1beta1.IngressBackend)(nil), TLS:[]v1beta1.IngressTLS(nil),
 Rules:[]v1beta1.IngressRule(nil)}, Status:v1beta1.IngressStatus{LoadBalancer:v1.LoadBalancerStatus{Ingress:[]v1.LoadBalancerIngress(nil)}}}' due to: 'selfLink was empty, can't make reference'. Will not report eve
nt: 'Warning' 'AddInClusterFailed' 'Failed to create ingress ""default/hello-server"" in cluster test-eu: Ingress.extensions ""hello-server"" is invalid: spec: Invalid value: []extensions.IngressRule(nil): either `bac
kend` or `rules` must be specified'
E0228 21:28:35.254808       1 ingress_controller.go:864] Failed to execute updates for default/hello-server: Ingress.extensions ""hello-server"" is invalid: spec: Invalid value: []extensions.IngressRule(nil): either
 `backend` or `rules` must be specified
E0228 21:28:35.357689       1 ingress_controller.go:240] Error creating ingress ""default/hello-server"": Ingress.extensions ""hello-server"" is invalid: spec: Invalid value: []extensions.IngressRule(nil): either `bac
kend` or `rules` must be specified
E0228 21:28:35.357743       1 event.go:260] Could not construct reference to: '&v1beta1.Ingress{TypeMeta:v1.TypeMeta{Kind:"""", APIVersion:""""}, ObjectMeta:v1.ObjectMeta{Name:""hello-server"", GenerateName:"""", Namespac
e:""default"", SelfLink:"""", UID:"""", ResourceVersion:"""", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeco
nds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string{""kubernetes.io/ingress.global-static-ip-name"":""federated-ingress"", ""ingress.federation.kubernetes.io/first-cluster"":""test-eu""}, Owner
References:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""""}, Spec:v1beta1.IngressSpec{Backend:(*v1beta1.IngressBackend)(nil), TLS:[]v1beta1.IngressTLS(nil),
 Rules:[]v1beta1.IngressRule(nil)}, Status:v1beta1.IngressStatus{LoadBalancer:v1.LoadBalancerStatus{Ingress:[]v1.LoadBalancerIngress(nil)}}}' due to: 'selfLink was empty, can't make reference'. Will not report eve
nt: 'Warning' 'AddInClusterFailed' 'Failed to create ingress ""default/hello-server"" in cluster test-us: Ingress.extensions ""hello-server"" is invalid: spec: Invalid value: []extensions.IngressRule(nil): either `backend` or `rules` must be specified'
```

Here is my ingress, and it includes backend.

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: hello-server
  annotations:
    kubernetes.io/ingress.global-static-ip-name: federated-ingress
spec:
  backend:
    serviceName: hello-server
    servicePort: 8080
```",closed,False,2018-03-02 19:42:18,2018-05-28 12:15:30
federation,JoshuaAndrew,https://github.com/kubernetes/federation/pull/243,https://api.github.com/repos/kubernetes/federation/issues/243,Update kubefed.go,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

 move https://github.com/kubernetes/kubernetes repo to https://github.com/kubernetes/federation repo

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:
     
**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
   NONE
```

",closed,True,2018-03-05 08:02:54,2018-03-05 08:14:36
federation,JoshuaAndrew,https://github.com/kubernetes/federation/pull/244,https://api.github.com/repos/kubernetes/federation/issues/244,Update kubefed.go,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

 Change https://github.com/kubernetes/kubernetes repo name to https://github.com/kubernetes/federation in kubefed help.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:
     
**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
   NONE
```

",closed,True,2018-03-05 08:16:01,2018-03-14 07:52:34
federation,shashidharatd,https://github.com/kubernetes/federation/pull/245,https://api.github.com/repos/kubernetes/federation/issues/245,Fix deepcopy invocation of federated ingress spec to cluster ingress …,"…spec

Fixes: https://github.com/kubernetes/federation/issues/242

Surprising that this bug existed for so long time. 
Have updated the testcase to cover this scenario.

/cc @quinton-hoole @kubernetes/sig-multicluster-bugs 
/assign @irfanurrehman ",closed,True,2018-03-06 18:04:46,2018-03-13 17:42:11
federation,fr0stbyte,https://github.com/kubernetes/federation/pull/246,https://api.github.com/repos/kubernetes/federation/issues/246,Fix comparison of golang versions,"Change hack/lib/golang.sh to compare golang
version properly with ""sort -s -t. -k 1,1 -k 2,2n -k 3,3n"",
which sorts key by key and not as strings.",closed,True,2018-03-07 21:35:45,2018-03-08 13:10:57
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/247,https://api.github.com/repos/kubernetes/federation/issues/247,Alternative path (then default usage of RBAC) for kubefed cluster access,"Implements functionality as discussed in https://github.com/kubernetes/federation/issues/210.
kubefed gets a new flag `--use-credentials-kubeconfig`. 
/assign @shashidharatd ",closed,True,2018-03-13 15:16:55,2018-03-26 05:38:01
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/248,https://api.github.com/repos/kubernetes/federation/issues/248,Fix for deployment/replicaset delete problems,"Fixes #215 
/assign @shashidharatd ",closed,True,2018-03-14 12:40:15,2018-03-19 07:47:59
federation,chenpengdev,https://github.com/kubernetes/federation/pull/249,https://api.github.com/repos/kubernetes/federation/issues/249,fix broken link,fix broken link,closed,True,2018-03-15 07:36:30,2018-03-21 10:37:21
federation,shashidharatd,https://github.com/kubernetes/federation/issues/250,https://api.github.com/repos/kubernetes/federation/issues/250,Document using hosted DNS providers in non-cloud environments,"We currently have DNS providers for Google Cloud DNS, Amazon Route53 & CoreDNS. AzureDNS is in progress. Generally they all work in native environment by auto detecting the configuration. But the initialisation is non- trivial in non-native environments say like in an on-premise cluster using Google Cloud DNS. The configuration has to be fed to the DNS Provider library in order to make it work properly. Each of this DNS provider has their own format for the configuration and has to be documented.

Additionally there is more work for AWS Route53 DNS provider. it does not use the user specified config as in [here](https://github.com/kubernetes/federation/blob/master/pkg/dnsprovider/providers/aws/route53/route53.go#L55).

/cc @kubernetes/sig-multicluster-misc ",closed,False,2018-03-16 05:25:58,2018-11-03 13:05:44
federation,wangxinxu411,https://github.com/kubernetes/federation/issues/251,https://api.github.com/repos/kubernetes/federation/issues/251,"delete federation service ,still can get ips from COREDNS","1.set up a federation using dns provider of coredns.
2.create a federation service.
apiVersion: v1
kind: Service
metadata:
  name: bbbbb
  labels:
    name: bbbbb
  annotations:
    federation.kubernetes.io/service-ingresses: '{""items"": [{""cluster"": ""us"", ""items"": [{""ip"": ""192.168.30.101""}]}, {""cluster"": ""europe"", ""items"": [{""ip"": ""192.168.30.102""}]}]}'
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
  selector:
    name: bbbbb
3.using dnstools to check dns.
kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools --context=minikube
dnstools# nslookup bbbbb.default.myfed.svc.myzone coredns-coredns.default
Server:		coredns-coredns.default
Address:	10.108.170.8#53

Name:	bbbbb.default.myfed.svc.myzone
Address: 192.168.30.101
Name:	bbbbb.default.myfed.svc.myzone
Address: 192.168.30.102
Now,we can get the correct ips.
4.here is the question:when i delete the federation service. controller-manager says:
E0316 08:09:48.864695       1 servicecontroller.go:433] Failed to delete default/bbbbb: waiting for object default/bbbbb to be deleted from clusters: us, europe
but when i check cluster us,europe the service already deleted.how ever when i check dns using command:kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools --context=minikube
dnstools# nslookup bbbbb.default.myfed.svc.myzone coredns-coredns.default
Server:		coredns-coredns.default
Address:	10.108.170.8#53

Name:	bbbbb.default.myfed.svc.myzone
Address: 192.168.30.102
Name:	bbbbb.default.myfed.svc.myzone
Address: 192.168.30.101
it can still get the ips.it seems that controller-manager did not delete the key in etcd which used by coredns.      IT Complicated me.HOPE you can help me.

BY the way the version is:
[root@minikube minikube-federation]# kubefed version
Client Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.0-alpha.3"", GitCommit:""85c06145286da663755b140efa2b65f793cce9ec"", GitTreeState:""clean"", BuildDate:""2018-02-14T12:54:40Z"", GoVersion:""go1.9.1"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.0-alpha.3"", GitCommit:""85c06145286da663755b140efa2b65f793cce9ec"", GitTreeState:""clean"", BuildDate:""2018-02-14T12:54:40Z"", GoVersion:""go1.9.1"", Compiler:""gc"", Platform:""linux/amd64""}

",closed,False,2018-03-16 08:29:09,2018-08-13 10:15:21
federation,prasanna12510,https://github.com/kubernetes/federation/issues/252,https://api.github.com/repos/kubernetes/federation/issues/252,deleting the federated deployment does not delete corresponding route53 records,on deleting the deployment from federation it doesnt delete the corresponding records created in route53 console,closed,False,2018-03-17 15:53:46,2018-08-14 17:46:23
federation,irfanurrehman,https://github.com/kubernetes/federation/pull/253,https://api.github.com/repos/kubernetes/federation/issues/253,Enable test to check cross cluster federated service discovery,Enable to check the flakiness of this test in current setup.,closed,True,2018-03-18 16:34:13,2018-08-15 18:10:39
federation,nashasha1,https://github.com/kubernetes/federation/issues/254,https://api.github.com/repos/kubernetes/federation/issues/254,Is it possible to keep federation cross-cluster,"Federation deploys apiserver and controller-manager to one of the kubernetes cluster. If this cluster broken down. How does federation keep working?

Is it possible to make federation support deploy into multi-cluster?",closed,False,2018-03-21 10:11:29,2018-08-18 12:15:38
federation,Knappek,https://github.com/kubernetes/federation/issues/255,https://api.github.com/repos/kubernetes/federation/issues/255,"kubefed init returns: Cannot parse dnsName ""apiserver.federation-system.svc.cluster.local.""","I guess kubefed has a bug. When I execute

```
kubefed init federation-test --host-cluster-context=gke-andy-test --dns-zone-name=""federation.andy.test."" --dns-provider=""google-clouddns""
``` 

I get the error

``` 
Creating a namespace federation-system for federation system components... done
Creating federation control plane service.............. done
Creating federation control plane objects (credentials, persistent volume claim)...error: failed to create federation API server key and certificate: unable to sign the server certificate: x509: cannot parse dnsName ""apiserver.federation-system.svc.cluster.local.""
``` 

This seems to be related to this issue: https://github.com/kubernetes/ingress-nginx/issues/2188.",closed,False,2018-03-23 14:25:39,2018-08-20 16:06:37
federation,nikox94,https://github.com/kubernetes/federation/pull/256,https://api.github.com/repos/kubernetes/federation/issues/256,[WIP]ISSUE-185: Set up DNS tests,Followup to #186 ,closed,True,2018-04-04 01:23:34,2018-09-01 03:36:54
federation,shashidharatd,https://github.com/kubernetes/federation/pull/257,https://api.github.com/repos/kubernetes/federation/issues/257,Update Bazel build files,"Due to recent updates in bazel job, the generated BUILD files have changed. It is causing the PR tests on this repo to fail, hence this fix

/assign @irfanurrehman ",closed,True,2018-04-06 04:11:08,2018-09-17 05:01:59
federation,woshihaoren,https://github.com/kubernetes/federation/issues/258,https://api.github.com/repos/kubernetes/federation/issues/258,Can't add dns record,"kubenetes version: 1.10.1
kubefed version: v1.9.0-alpha.3
refer to https://github.com/ufcg-lsd/k8s-onpremise-federation
```
# kubectl get svc -o wide
NAME      TYPE           CLUSTER-IP   EXTERNAL-IP                 PORT(S)   AGE       SELECTOR
nginx     LoadBalancer   <none>       172.16.1.209,172.16.1.225   80/TCP    5m        run=nginx

```

```
# kubectl logs -n federation-system -f  controller-manager-764c9978f6-2fd5j --context=fcp
E0414 12:31:39.908311       1 event.go:260] Could not construct reference to: '&v1.Service{TypeMeta:v1.TypeMeta{Kind:"""", APIVersion:""""}, ObjectMeta:v1.ObjectMeta{Name:""nginx"", GenerateName:"""", Namespace:""default"", SelfLink:"""", UID:"""", ResourceVersion:"""", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{""run"":""nginx""}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""""}, Spec:v1.ServiceSpec{Ports:[]v1.ServicePort{v1.ServicePort{Name:"""", Protocol:""TCP"", Port:80, TargetPort:intstr.IntOrString{Type:0, IntVal:80, StrVal:""""}, NodePort:0}}, Selector:map[string]string{""run"":""nginx""}, ClusterIP:"""", Type:""LoadBalancer"", ExternalIPs:[]string(nil), SessionAffinity:""None"", LoadBalancerIP:"""", LoadBalancerSourceRanges:[]string(nil), ExternalName:"""", ExternalTrafficPolicy:""Cluster"", HealthCheckNodePort:0, PublishNotReadyAddresses:false, SessionAffinityConfig:(*v1.SessionAffinityConfig)(nil)}, Status:v1.ServiceStatus{LoadBalancer:v1.LoadBalancerStatus{Ingress:[]v1.LoadBalancerIngress(nil)}}}' due to: 'selfLink was empty, can't make reference'. Will not report event: 'Normal' 'CreateInCluster' 'Creating service ""default/nginx"" in cluster usa'
E0414 12:31:39.911771       1 event.go:260] Could not construct reference to: '&v1.Service{TypeMeta:v1.TypeMeta{Kind:"""", APIVersion:""""}, ObjectMeta:v1.ObjectMeta{Name:""nginx"", GenerateName:"""", Namespace:""default"", SelfLink:"""", UID:"""", ResourceVersion:"""", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{""run"":""nginx""}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""""}, Spec:v1.ServiceSpec{Ports:[]v1.ServicePort{v1.ServicePort{Name:"""", Protocol:""TCP"", Port:80, TargetPort:intstr.IntOrString{Type:0, IntVal:80, StrVal:""""}, NodePort:0}}, Selector:map[string]string{""run"":""nginx""}, ClusterIP:"""", Type:""LoadBalancer"", ExternalIPs:[]string(nil), SessionAffinity:""None"", LoadBalancerIP:"""", LoadBalancerSourceRanges:[]string(nil), ExternalName:"""", ExternalTrafficPolicy:""Cluster"", HealthCheckNodePort:0, PublishNotReadyAddresses:false, SessionAffinityConfig:(*v1.SessionAffinityConfig)(nil)}, Status:v1.ServiceStatus{LoadBalancer:v1.LoadBalancerStatus{Ingress:[]v1.LoadBalancerIngress(nil)}}}' due to: 'selfLink was empty, can't make reference'. Will not report event: 'Normal' 'CreateInCluster' 'Creating service ""default/nginx"" in cluster fcp'
```",closed,False,2018-04-14 12:42:04,2018-09-11 14:42:47
federation,d-winter,https://github.com/kubernetes/federation/issues/259,https://api.github.com/repos/kubernetes/federation/issues/259,Building kubefed on mac osx,"Hey,
is there any guide how to build kubefed for mac osx? When I run make and call _output/local/bin/darwin/amd64/kubefed version it prints out Major: """", Minor"""".
I think this causes an error when trying to init a federation cluster, since the docker image which it tries to pull does not exist.
Using the compiled binaries on ubuntu works perfect.
Any help would be great!",closed,False,2018-04-18 08:47:50,2019-01-20 23:09:35
federation,mmeliani,https://github.com/kubernetes/federation/issues/260,https://api.github.com/repos/kubernetes/federation/issues/260,[FEDERATION] how to join a cluster with certificate apiserver access ,"I deployed a federaton control plane to a host cluster this is kubeconfig of the host cluster : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /etc/kubernetes/ssl/ca.pem
    server: https://k8s-apiserver.bcmt.cluster.local:8443
  name: bcmt-kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
contexts:
- context:
    cluster: bcmt-kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: bcmt-kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
current-context: default-context
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate: /etc/kubernetes/ssl/kubectl.pem
    client-key: /etc/kubernetes/ssl/kubectl-key.pem
```


and now i'm trying to join a cluster to the federation, below the configuration of this cluster : 

```
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /etc/kubernetes/ssl/ca.pem
    server: https://k8s-apiserver.bcmt.cluster.local:8443
  name: bcmt-kubernetes
contexts:
- context:
    cluster: bcmt-kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: bcmt-kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
current-context: default-context
kind: Config
preferences: {}
users:
- name: kubectl
  user:
    client-certificate: /etc/kubernetes/ssl/kubectl.pem
    client-key: /etc/kubernetes/ssl/kubectl-key.pem
```


As you can see the acces to the api server is done with certificate how to do in order to join this cluster to federation; I mean should i expose the apiserver of the cluster that i'm joining and how the host cluster will use the certificate in order to reach the apiserver of the joining cluster ? Any hints please how to do that ! 
sig/multicluster
sig/federation 
@kubernetes/sig-multicluster-bugs
",closed,False,2018-04-18 13:42:36,2018-09-15 17:19:18
federation,m1093782566,https://github.com/kubernetes/federation/issues/261,https://api.github.com/repos/kubernetes/federation/issues/261,make-quick release is failing git version information missing; cannot create Docker tag,"```
[root@SHA1000130405 federation]# make quick-release
+++ [0420 20:57:23] Verifying Prerequisites....
+++ [0420 20:57:23] Building Docker image kube-build:build-c317dc6ab9-5-v1.9.3-1
+++ [0420 20:57:35] Creating data container kube-build-data-c317dc6ab9-5-v1.9.3-1
+++ [0420 20:57:38] Syncing sources to container
+++ [0420 20:57:40] Running build command...
+++ [0420 20:57:53] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [0420 20:57:53] Building go targets for linux/amd64:
    ./vendor/k8s.io/code-generator/cmd/deepcopy-gen
+++ [0420 20:57:58] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [0420 20:57:58] Building go targets for linux/amd64:
    ./vendor/k8s.io/code-generator/cmd/defaulter-gen
+++ [0420 20:58:01] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [0420 20:58:01] Building go targets for linux/amd64:
    ./vendor/k8s.io/code-generator/cmd/conversion-gen
+++ [0420 20:58:05] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [0420 20:58:05] Building go targets for linux/amd64:
    ./vendor/k8s.io/code-generator/cmd/openapi-gen
+++ [0420 20:58:14] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [0420 20:58:14] Building go targets for linux/amd64:
    cmd/fcp
 +++ [0420 20:59:57] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [0420 20:59:57] Building go targets for linux/amd64:
    cmd/kubefed
+++ [0420 21:00:33] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [0420 21:00:33] Building go targets for linux/amd64:
    cmd/genfeddocs
    test/e2e/e2e.test
 +++ [0420 21:02:38] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [0420 21:02:38] Building go targets for linux/amd64:
    vendor/github.com/onsi/ginkgo/ginkgo
+++ [0420 21:02:43] Syncing out of container
+++ [0420 21:02:49] Building tarball: src
+++ [0420 21:02:49] Building tarball: client linux-amd64
+++ [0420 21:02:49] Waiting on tarballs
+++ [0420 21:02:52] Building tarball: server linux-amd64
!!! [0420 21:02:52] git version information missing; cannot create Docker tag
!!! [0420 21:02:52] previous tarball phase failed
make: *** [quick-release] Error 1
```

@shashidharatd any comment?",closed,False,2018-04-20 13:07:32,2018-05-16 09:50:13
federation,AdamDang,https://github.com/kubernetes/federation/pull/262,https://api.github.com/repos/kubernetes/federation/issues/262,"some typo fixes: duplicated ""to to""->""to""","""to to""->""to""",closed,True,2018-04-22 15:23:55,2018-08-24 05:16:26
federation,mahuihuang,https://github.com/kubernetes/federation/pull/263,https://api.github.com/repos/kubernetes/federation/issues/263,fix typo,Fix typo.,closed,True,2018-04-23 02:37:07,2018-06-19 01:54:21
federation,day-jeff,https://github.com/kubernetes/federation/issues/264,https://api.github.com/repos/kubernetes/federation/issues/264,Need help with cluster failure behavior,"I’ve been experimenting with cluster federation, and while it works great under normal circumstances, the federation controller does not appear to properly detect and mitigate cluster failures. 

For example, when I disable a remote cluster (via firewall or by stopping the host VMs), the controller does not appear to detect the issue, and the DNS A record is not updated to remove the cluster’s IP. 

I can’t find documentation on how federation should work on these circumstances, and I don’t know if there is anything I can configure (TTLs, probes, etc.) to influence how the federation controller monitors clusters and when it updates DNS. 

(Note: the controller immediately updates DNS as I bind/unbind services to the K8s load balancer.)",closed,False,2018-04-28 06:49:38,2018-09-25 09:12:53
federation,AdamDang,https://github.com/kubernetes/federation/pull/265,https://api.github.com/repos/kubernetes/federation/issues/265,Typo fix: distriubted->distributed/kuberentes->kubernetes,"distriubted->distributed
kuberentes->kubernetes",closed,True,2018-04-29 07:28:29,2018-05-10 13:31:38
federation,AdamDang,https://github.com/kubernetes/federation/pull/266,https://api.github.com/repos/kubernetes/federation/issues/266,Typo fix: commited->committed,Line 117: commited->committed,closed,True,2018-04-30 15:28:34,2018-08-24 05:16:49
federation,AdamDang,https://github.com/kubernetes/federation/pull/267,https://api.github.com/repos/kubernetes/federation/issues/267,Typo fix in echo message : checkpiont->checkpoint,Line 845: checkpiont->checkpoint,closed,True,2018-05-09 15:49:55,2018-08-24 05:17:36
federation,AdamDang,https://github.com/kubernetes/federation/pull/268,https://api.github.com/repos/kubernetes/federation/issues/268,Typo fix: directy->directory,Line 26: directy->directory,closed,True,2018-05-17 13:29:59,2018-08-24 05:18:31
federation,jessfraz,https://github.com/kubernetes/federation/issues/269,https://api.github.com/repos/kubernetes/federation/issues/269,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",closed,False,2018-05-24 14:43:12,2018-12-21 09:36:31
federation,AdamDang,https://github.com/kubernetes/federation/pull/270,https://api.github.com/repos/kubernetes/federation/issues/270,Typo fix: occurence->occurrence,line 5444: occurence->occurrence,closed,True,2018-05-31 14:53:41,2018-08-24 05:18:47
federation,AdamDang,https://github.com/kubernetes/federation/pull/271,https://api.github.com/repos/kubernetes/federation/issues/271,Typo fix contianer->container,Line 17: contianer->container,closed,True,2018-06-02 10:36:40,2018-08-24 05:19:04
federation,mirake,https://github.com/kubernetes/federation/pull/272,https://api.github.com/repos/kubernetes/federation/issues/272,Typo fix: checkpiont -> checkpoint,,closed,True,2018-06-05 08:34:09,2018-09-16 10:49:21
federation,AdamDang,https://github.com/kubernetes/federation/pull/273,https://api.github.com/repos/kubernetes/federation/issues/273,Typo fix: sepcified->specified,Line 43: sepcified->specified,closed,True,2018-06-12 10:02:39,2018-08-24 05:20:19
federation,marekaf,https://github.com/kubernetes/federation/issues/274,https://api.github.com/repos/kubernetes/federation/issues/274,GKE Federation - Cross Cluster Service Discovery with GoogleDNS in one VPC,"Hi,
I'm trying to setup a working demo on GCP.
2 GKE clusters, each in different region, say 
- southamerica-east1-b cluster-BR
- europe-west1-d cluster-EU
I want a service A running in cluster-BR to be able to communicate via standard local dns svc discovery with service B running in cluster-EU in one Google VPC. 

service A has one pod in cluster-BR, other pod in cluster-EU
service B (10.23.243.18) has only one pod (IP 10.5.6.3), in cluster-EU

**What I want to achieve:**
service A in cluster-BR wants to communicate with service B, it queries a local dns service-b.default.federation, kube-dns find out there are no healthy endpoints of service-b in cluster-BR so it checks GoogleDNS (managed by federation api plane) and returns a private IP 10.23.243.18 which is the IP or service-b k8s service running in cluster-EU. 

**Why is this not working?**
I'm able to communicate from one pod in cluster-EU with pods in cluster-BR using their local pod IPs.
I'm not able to hit services' IPs (virtual IPs?, I can't see them from the outside?). Does not matter if it's ClusterIP or NodePort. I don't want to use External Loadbalancers. Internal loadbalancer are regional, so I can't use those either.
GoogleDNS with federation handles public ipv4 only from LoadBalancer service specs, it ignores everything else. 

Am I the only one trying to solve this problem? What am I missing? 

Thanks :)


",closed,False,2018-06-20 13:31:13,2019-03-28 18:29:24
federation,prasanna12510,https://github.com/kubernetes/federation/issues/275,https://api.github.com/repos/kubernetes/federation/issues/275,Custom Metrics Support at Federation and Migration of replicas based on optimal metrics,"Hi Community,
I have added custom metrics support using Prometheus in each cluster individually and designed a controller that will run on the federation layer, the role of the controller is to collect the metrics data for a certain time window based for example Httprequestcount from each cluster based on that controller will compute optimal number of replicas required in each cluster. This ensures easy migration of replicas where it is required more. I want your thoughts and feedback on this.
Thanks",closed,False,2018-06-26 08:13:19,2018-11-23 09:53:09
federation,jazzyarchitects,https://github.com/kubernetes/federation/issues/276,https://api.github.com/repos/kubernetes/federation/issues/276,Kubefed get and describe giving inconsistent data,"` kubectl get clusters ` returns ` No resource found. Error from server (NotAcceptable): unknown (get clusters.federation) `
while
` kubectl describe clusters ` returns the description of all the joined clusters. 

Possible bug in implementation? 

Attaching screenshot
<img width=""843"" alt=""screen shot 2018-07-04 at 1 58 51 am"" src=""https://user-images.githubusercontent.com/11012686/42243332-9867dc90-7f2e-11e8-8ce8-5630a6493e5a.png"">

",closed,False,2018-07-03 20:37:22,2018-07-04 06:56:48
federation,zqdlove,https://github.com/kubernetes/federation/issues/277,https://api.github.com/repos/kubernetes/federation/issues/277,Where can I see the release plan,Hello，Where can I see the release plan，When to support the new version K8S 1.10.*,closed,False,2018-07-04 05:58:34,2018-12-13 08:41:12
federation,aojea,https://github.com/kubernetes/federation/issues/278,https://api.github.com/repos/kubernetes/federation/issues/278,Missing CONTRIBUTING.md file,"All K8s subrepositories should have a CONTRIBUTING.md file, which at the minimum should point to https://github.com/kubernetes/community/blob/master/contributors/guide/README.md. Care should be taken that all information is in sync with the contributor guide.

Subrepositories may also have contributing guidelines specific to that repository. They should be explicitly documented and explained in the CONTRIBUTING.md

Ref:  https://github.com/kubernetes/community/issues/1832",closed,False,2018-07-16 09:23:29,2019-01-27 13:06:06
federation,RA489,https://github.com/kubernetes/federation/pull/279,https://api.github.com/repos/kubernetes/federation/issues/279,Adding Contribution guidelines,"Adding Contribution guidelines for kubernetes federation.
fixes #278",closed,True,2018-07-19 05:18:03,2018-11-16 06:24:08
federation,ChuckyThh,https://github.com/kubernetes/federation/issues/280,https://api.github.com/repos/kubernetes/federation/issues/280,"If I also use coredns in the k8s cluster, how should I associate the coredns of the federated cluster with the coredns in the cluster and use？","I deployed the k8s cluster through kubeadm, the version is 1.10.0, and used coredns for parsing services, but when I deployed the latest k8s cluster federation through kubefed, the cluster federation worked, but the dns in the cluster could not be resolved to the federation. The dns on the service causes the dns of the federated service to be unavailable.
I saw the official website to use kube-dns, I think this may be the problem, what should I do to connect the coredns of the cluster and the federated coredns?",closed,False,2018-07-23 01:44:55,2018-12-20 08:23:42
federation,yutongp,https://github.com/kubernetes/federation/issues/281,https://api.github.com/repos/kubernetes/federation/issues/281,invalid apiserver image name (broken versioning),"I built the project `master` branch(ccbc47b7f65e09a4bfbc27e157120e0396156d67) with `make` on MacOS.
I got these output when I run `kubefed version`:
```
Client Version: version.Info{Major:"""", Minor:"""", GitVersion:""v0.0.0-master+$Format:%h$"", GitCommit:""$Format:%H$"", GitTreeState:"""", BuildDate:""1970-01-01T00:00:00Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10+"", GitVersion:""v1.10.6-gke.1"", GitCommit:""ee18aa41359293b5c9c7e13a35697690491736c9"", GitTreeState:""clean"", BuildDate:""2018-08-08T18:09:40Z"", GoVersion:""go1.9.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```
the `GitVersion` and `GitCommit` version is broken. This issue caused `kubefed` try to fetch wrong `apiserver` image after I ran `kubefed init` for my GCP clusters.
On GCP console I saw this error message:
```
Failed to apply default image tag ""gcr.io/k8s-jkns-e2e-gce-federation/fcp-amd64:v0.0.0-master_$Format:%h$"": couldn't parse image reference ""gcr.io/k8s-jkns-e2e-gce-federation/fcp-amd64:v0.0.0-master_$Format:%h$"": invalid reference format: InvalidImageName
```

P.S. I built the binrary from master branch because I want to use `credentials-kubeconfig` to avoid #110 




",closed,False,2018-08-20 00:17:05,2019-03-20 07:38:41
federation,cruxyoung,https://github.com/kubernetes/federation/issues/282,https://api.github.com/repos/kubernetes/federation/issues/282,Fail to pull image of federation,"When I try to do deploy federation plane with kubefed, the federation controller plane fail to come up.
So I describe the controller pod, it tells me that it fails to pull the image. And the error is:

`desc = Error response from daemon: Get https://gcr.io/v2/k8s-jkns-e2e-gce-federation/fcp-amd64/manifests/v1.9.0-alpha.3: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication`

Anything changed to this repository? This did not happen on my last try.",closed,False,2018-09-12 07:02:54,2018-09-12 10:54:58
federation,mirake,https://github.com/kubernetes/federation/pull/283,https://api.github.com/repos/kubernetes/federation/issues/283,Fix some typos,"continously -> continuously
occurence -> occurrence
identifer -> identifier

Signed-off-by: Rui Cao <ruicao@alauda.io>",closed,True,2018-09-16 10:58:10,2019-01-09 07:16:03
federation,mooncak,https://github.com/kubernetes/federation/pull/284,https://api.github.com/repos/kubernetes/federation/issues/284,Remove duplicated words,"Remove duplicated words in below files to make it more clearly:
1. pkg/dnsprovider/providers/aws/route53/interface.go
2. pkg/federation-controller/cluster/clustercontroller_test.go
3. test/k8s/e2e/framework/framework.go",closed,True,2018-10-04 16:06:39,2018-10-19 18:12:18
federation,mooncak,https://github.com/kubernetes/federation/pull/285,https://api.github.com/repos/kubernetes/federation/issues/285,Fix typos issues in master-helper.sh,"Fix typos issues in below files:
1. build/README.md
2. cluster/gce/container-linux/master-helper.sh",closed,True,2018-10-07 01:17:21,2018-11-11 01:38:44
federation,SataQiu,https://github.com/kubernetes/federation/pull/286,https://api.github.com/repos/kubernetes/federation/issues/286,fix code format,fix code format,closed,True,2018-10-07 03:18:44,2018-10-09 08:20:02
federation,xichengliudui,https://github.com/kubernetes/federation/pull/287,https://api.github.com/repos/kubernetes/federation/issues/287,Delete repeated 'to',Delete repeated 'to',closed,True,2018-10-17 07:29:34,2018-10-17 07:31:27
federation,xichengliudui,https://github.com/kubernetes/federation/pull/288,https://api.github.com/repos/kubernetes/federation/issues/288,Delete repeated 'to',Delete repeated 'to',closed,True,2018-10-17 07:33:04,2018-10-17 07:41:07
federation,xichengliudui,https://github.com/kubernetes/federation/pull/289,https://api.github.com/repos/kubernetes/federation/issues/289,Delete duplicate 'with',Delete duplicate 'with',closed,True,2018-10-18 02:00:52,2018-11-02 14:20:16
federation,xichengliudui,https://github.com/kubernetes/federation/pull/290,https://api.github.com/repos/kubernetes/federation/issues/290,Delete repeated 'of',Delete repeated 'of',closed,True,2018-10-19 02:32:48,2018-11-02 14:33:37
federation,AdamDang,https://github.com/kubernetes/federation/pull/291,https://api.github.com/repos/kubernetes/federation/issues/291,Fix the typo in echo message of Update upgrade.sh,line 43: sepcified->specified,closed,True,2018-10-19 03:41:56,2018-12-20 10:50:14
federation,xichengliudui,https://github.com/kubernetes/federation/pull/292,https://api.github.com/repos/kubernetes/federation/issues/292,Remove duplicate words,Remove duplicate words,closed,True,2018-10-24 12:19:20,2018-11-21 06:02:54
federation,asad26,https://github.com/kubernetes/federation/issues/293,https://api.github.com/repos/kubernetes/federation/issues/293,RBAC roles error while joining Kubernetes cluster to the Federation,"Hi guys,
I am running one Federation cluster and one Kubernetes cluster (version 1.12.1) on two different Ubuntu 16.04 machines. The federation cluster (host cluster) is on the network 130.233.193.0/24 and the second cluster is on 130.233.97.0/23. I am unable to join the second cluster from host cluster using the command:

kubefed join cloud-cluster --host-cluster-context=kubernetes-admin-host --cluster-context=kubernetes-admin-cloud 

The following error appears:

**error:** Couldn't get clientset to create RBAC roles in the host cluster: Get https://130.233.97.82:6443/api: dial tcp 130.233.97.82:6443: i/o timeout

However, RBAC authorization is already enabled in api-server of both the clusters. Does anyone experience this type of error? If yes, please tell me how did you resolve it. To my understanding, I was thinking that it could be because of two different networks and there is a firewall in between them. Looking forward to your solutions.
Thanks ",open,False,2018-10-26 15:32:53,2019-03-21 17:42:41
federation,red5bongo,https://github.com/kubernetes/federation/issues/294,https://api.github.com/repos/kubernetes/federation/issues/294,Unable to pull Federation v1 Docker images,"The project that hosts the docker images is denying access when kubefed tries to pull down the images for API server and Controller Manager:

Events:
  Type     Reason                 Age              From                                       Message
  ----     ------                 ----             ----                                       -------
  Normal   Scheduled              2m               default-scheduler                          Successfully assigned controller-manager-58ddbc4488-66gzd to gke-trident-new-np-1158f0f4-gpw1
  Normal   SuccessfulMountVolume  2m               kubelet, gke-trident-new-np-1158f0f4-gpw1  MountVolume.SetUp succeeded for volume ""controller-manager-kubeconfig""
  Normal   SuccessfulMountVolume  2m               kubelet, gke-trident-new-np-1158f0f4-gpw1  MountVolume.SetUp succeeded for volume ""federation-controller-manager-token-w5np8""
  Normal   SandboxChanged         2m (x2 over 2m)  kubelet, gke-trident-new-np-1158f0f4-gpw1  Pod sandbox changed, it will be killed and re-created.
  Normal   Pulling                2m (x3 over 2m)  kubelet, gke-trident-new-np-1158f0f4-gpw1  pulling image ""gcr.io/k8s-jkns-e2e-gce-federation/fcp-amd64:v1.10.0-alpha.0""
  Warning  Failed                 2m (x3 over 2m)  kubelet, gke-trident-new-np-1158f0f4-gpw1  Failed to pull image ""gcr.io/k8s-jkns-e2e-gce-federation/fcp-amd64:v1.10.0-alpha.0"": rpc error: code = U
nknown desc = Error response from daemon: Get https://gcr.io/v2/k8s-jkns-e2e-gce-federation/fcp-amd64/manifests/v1.10.0-alpha.0: denied: Token exchange failed for project 'k8s-jkns-e2e-gce-federatio
n'. Please enable or contact project owners to enable the Google Container Registry API in Cloud Console at https://console.cloud.google.com/apis/api/containerregistry.googleapis.com/overview?projec
t=k8s-jkns-e2e-gce-federation before performing this operation.
  Warning  Failed                 2m (x3 over 2m)  kubelet, gke-trident-new-np-1158f0f4-gpw1  Error: ErrImagePull
  Normal   BackOff                1m (x6 over 2m)  kubelet, gke-trident-new-np-1158f0f4-gpw1  Back-off pulling image ""gcr.io/k8s-jkns-e2e-gce-federation/fcp-amd64:v1.10.0-alpha.0""
  Warning  Failed                 1m (x6 over 2m)  kubelet, gke-trident-new-np-1158f0f4-gpw1  Error: ImagePullBackOff

I'm unable to pull it from a standalone Docker install as well.  This is happening for 1.9.0-alpha2, 3 and 1.10.0-alpha0.  ",open,False,2018-11-01 19:27:12,2019-03-16 01:28:50
federation,devdemos,https://github.com/kubernetes/federation/issues/296,https://api.github.com/repos/kubernetes/federation/issues/296,invalid reference format and tag: fcp-amd64:v0.0.0-master_$Format:%h$,"I am in the process of trying to setup federation with kubernetes 1.9.3 using an AWS back end and have hit a roadblock due to an issue related to the image tag

```
kubefed init kube-alliance --host-cluster-context=region1-context --dns-provider=""aws-route53"" --dns-zone-name=""ourdomain.com.""
```
and the process kicks off fine until it hangs on 
```
Waiting for federation control plane to come up.....................................................................................
```
Further investigation shows the status of the pods as 

```
federation-system   apiserver-85fb9f758c-dglm9                                                0/2       Pending            0          29m
federation-system   controller-manager-79b9675989-pk89q                                       0/1       InvalidImageName   0          29m
```
Looking into the details of the pod shows an issue with the image tag
**-> **kubectl describe po controller-manager-79b9675989-pk89q -n federation-system****
```
Failed to apply default image tag ""gcr.io/k8s-jkns-e2e-gce-federation/fcp-amd64:v0.0.0-master_$Format:%h$"": couldn't parse image reference ""gcr.io/k8s-jkns-e2e-gce-federation/fcp-amd64:v0.0.0-master_$Format:%h$"": invalid reference format
```

Looking at the version of kubefed shows the same issue as also reported here [281](https://github.com/kubernetes/federation/issues/281)

**-> kubefed version**
```
Client Version: version.Info{Major:"""", Minor:"""", GitVersion:""v0.0.0-master+$Format:%h$"", GitCommit:""$Format:%H$"", GitTreeState:"""", BuildDate:""1970-01-01T00:00:00Z"", GoVersion:""go1.11.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-07T11:55:20Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""
```

Pull the latest version
```
git clone https://github.com/kubernetes/federation.git
```

Building kubefed seems to work fine, with out any issues
```
 make      
+++ [1213 00:31:35] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [1213 00:31:35] Building go targets for linux/amd64:
    ./vendor/k8s.io/code-generator/cmd/deepcopy-gen
+++ [1213 00:31:38] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [1213 00:31:38] Building go targets for linux/amd64:
    ./vendor/k8s.io/code-generator/cmd/defaulter-gen
+++ [1213 00:31:41] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [1213 00:31:42] Building go targets for linux/amd64:
    ./vendor/k8s.io/code-generator/cmd/conversion-gen
+++ [1213 00:31:45] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [1213 00:31:46] Building go targets for linux/amd64:
    ./vendor/k8s.io/code-generator/cmd/openapi-gen
+++ [1213 00:31:52] Building the toolchain targets:
    k8s.io/federation/vendor/k8s.io/kubernetes/hack/cmd/teststale
+++ [1213 00:31:52] Building go targets for linux/amd64:
    cmd/fcp
    cmd/kubefed
    cmd/genfeddocs
    test/e2e/e2e.test
    vendor/github.com/onsi/ginkgo/ginkgo
```
Additional info 
**-> kubectl version**
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:55:54Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-07T11:55:20Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
```

_> go version
```
go version go1.11.2 linux/amd64
```

",open,False,2018-12-13 01:32:23,2019-03-13 02:19:36
federation,alpe,https://github.com/kubernetes/federation/issues/297,https://api.github.com/repos/kubernetes/federation/issues/297,Create PodSecurityPolicy/ Roles on init,"Problem:
When pod-security-policies are activated the api server and controller fail to start as there is no matching `PodSecurityPolicy` deployed/ roles are missing.

```
# kubectl -n federation-system describe rs apiserver-58fffb589c
Error creating: pods ""apiserver-58fffb589c-"" is forbidden: unable to validate against any pod security policy: []
```

",closed,False,2018-12-19 15:19:12,2018-12-19 16:05:09
federation,fejta,https://github.com/kubernetes/federation/issues/298,https://api.github.com/repos/kubernetes/federation/issues/298,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",open,False,2018-12-21 09:36:31,2019-04-03 05:13:52
federation,nikhita,https://github.com/kubernetes/federation/pull/299,https://api.github.com/repos/kubernetes/federation/issues/299,Add CONTRIBUTING.md,"Ref: https://github.com/kubernetes/steering/issues/28

The content is as per https://github.com/kubernetes/kubernetes-template-project.

/assign @shashidharatd  @marun ",closed,True,2018-12-27 09:24:51,2019-01-17 06:45:03
federation,fejta,https://github.com/kubernetes/federation/issues/300,https://api.github.com/repos/kubernetes/federation/issues/300,Intent to archive this repo as read-only,"https://github.com/kubernetes/federation/issues/298 has gone unresolved for more than a quarter, and there does not appear to be activity on this repo: no issues, no PRs.

I plan to mark this repo as archived next week. If we need to continue making changes to this repo, please someone step up and create the security contacts file. Thanks!

/assign @nikhiljindal @csbell @madhusudancs ",open,False,2019-04-03 17:52:20,2019-04-04 04:10:25
