name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/1,https://api.github.com/repos/kubernetes/kube-deploy/issues/1,Move ansible to kube-deploy,,closed,True,2016-03-29 17:30:28,2016-05-26 20:40:55
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/2,https://api.github.com/repos/kubernetes/kube-deploy/issues/2,move kube_addon_manager to new home,,closed,True,2016-03-29 18:04:23,2016-03-29 18:16:57
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/3,https://api.github.com/repos/kubernetes/kube-deploy/issues/3,package the addon manager in a pod,,closed,True,2016-03-29 20:12:08,2016-05-06 14:55:27
kube-deploy,danehans,https://github.com/kubernetes/kube-deploy/issues/4,https://api.github.com/repos/kubernetes/kube-deploy/issues/4,Project Description,"I'm confused by the project description. Will addon-manager be pulled from k8s and live here? Are there bigger plans for this project?
",closed,False,2016-04-01 18:43:35,2017-11-01 22:46:09
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/5,https://api.github.com/repos/kubernetes/kube-deploy/issues/5,WIP: simple configuration & metaconfiguration tool,"@mikedanese here is an example of a much simpler way to specify configuration.  We have a simple go installer program, that walks a directory tree, and interprets that as task (ala Salt or Ansible).  We have tasks that are a little smarter / more specialized for our use case than Salt or Ansible, such that most of the complexity here goes away.

So here we have a zero-dependency installer, with a very readable (IMO) installation specification.  There are obviously still issues to resolve, e.g. the generation of options is funky, but I think we could use jsonnet for that.

The nice thing here is that this ""filesystem tree as specification"" is simple enough that it would be trivial to transform this into salt, ansible, or whatever people want.

But, at the same time, by having a simple golang tool that works without external dependencies we can package this however we want - in a container, running directly on the host, running over SSH/SFTP etc.
",closed,True,2016-04-01 19:31:11,2016-05-17 16:53:12
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/6,https://api.github.com/repos/kubernetes/kube-deploy/issues/6,Proposal for kube-up v1 -> kube-up v2 strategy,"Straw-man proposal for how we should get from kube-up v1 to kube-up v2:
- [ ] Create v2 of the create-instances-on-cloud-side (i.e. the bash side of kube-up).  We have a growing list of requirements for it.  We also have users that would use it today, to do tasks that the current kube-up tool is bad at.  (e.g. adding more nodes of a different instance type to an existing cluster)  My proposal is that we use something like https://github.com/kopeio/kope/tree/gce  We can start getting this tested right away, with 1.2.  Some people have already used the kope tool to migrate from 1.1 -> 1.2
- [ ] Create an integration test for upgrade 1.2 -> 1.3 using the above tool (and the other tools we create here...)
- [ ] We should export a configuration from the above tool, to terraform.  Terraform seems the most popular such tool.  This would demonstrate that while we have our own easy to use tool, we can in fact work with the other tools people want and are not forcing anyone to use our tool.
- [ ] Create the node-side v2 (i.e. the Salt side of kube-up)  I think we should use something like #5.  I suggest we start with it using the v1.2 configuration file format (kube_env.yaml).
- [ ] Export a configuration from the node-side v2 to cloud-init.  Just like we did with the bash-replacement side, this will verify that our salt-replacement is not a lock-in.
- [ ] Define a v2 configuration format, that makes more use of ConfigMap, is more consistent, etc.  Change both the front-end and back-end tool to support it, but we need an upgrade story, so likely we'll need to support both formats in 1.3.
- [ ] Use our new tooling to make HA master work.  Ideally through k8s-on-k8s, but if we can't get there HA master will still be a good test that our new system is powerful enough.
",closed,False,2016-04-05 15:09:54,2017-11-01 22:52:25
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/7,https://api.github.com/repos/kubernetes/kube-deploy/issues/7,Add imagebuilder tool,"(A version of) this tool is used to build the AWS image that is currently the default image used in kube-up.  I previously proposed this into kubernetes/contrib as https://github.com/kubernetes/contrib/pull/486, but this seems a better place for it.

I extended the tool to support GCE images also.  We don't currently have a public Debian Jessie image on GCE that has the cgroups memory controller enabled (this was the root problem that required the custom image in the first place on AWS as well).

Most of this PR is code to bring up an instance, from which we can then run the bootstrap-vz tool.  I would like to replace that code with the code that will be in the cloudup tool to bring up clouds.  But - for now - getting cloudup & nodeup depends on a working image, so I think this sequence makes sense...

cc @zmerlynn 
",closed,True,2016-04-19 16:44:59,2016-04-28 02:57:18
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/8,https://api.github.com/repos/kubernetes/kube-deploy/issues/8,"cloudup & nodeup: direct, terraform, cloud-init","A working cloudup & nodeup (for GCE)!

To try, just edit `state/kubernetes.yaml` and then run

```
go install k8s.io/kube-deploy/fastdeploy... &&  ~/k8s/bin/cloudup --v=8
```

You should have a cluster on GCE.  (do `--dryrun` first to see what it will do); also you will need to build an image using my other PR.

That is ""direct"" mode, where cloudup runs directly against the APIs, and uses nodeup which a golang tool that applies the configuration directly.

But not everyone will want to use our own tools.  So we can also build configurations for other systems.  Currently terraform is implemented:

```
go install k8s.io/kube-deploy/fastdeploy... &&  ~/k8s/bin/cloudup --v=8 --target=terraform > tf/k8s.tf.json
```

Then do a `terraform apply` and the cluster will come up.

We can pull the same trick for nodeup.  The `nodeup` program itself has a `--target` flag, but normally you won't want to do that.  Instead, cloudup embeds nodeup, and you can specify `NodeInit: cloudinit` in the `state/kubernetes.yaml` directory.  I recommend doing this with --target=terraform so you can easily see what happens.  Sadly this is currently untested, because the GCE jessie images don't use cloud-init.
",closed,True,2016-04-23 19:18:07,2016-05-06 23:55:50
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/9,https://api.github.com/repos/kubernetes/kube-deploy/issues/9,a minimal deployment using terraform and ansible,"Explicit goals:
- create a cluster deployment that is portable across many deployment targets 
- create a cluster deployment this is transparent and can be used as a reference when creating deployments to new targets
- allow for an easy and reliable first experience with running multinode kubernetes in the cloud

Concretely this means:
- minimize bash in lieu of declaritive configuration
- minimize host configuration, kubelet only on nodes
- minimize lines of code/config
- gce is first target then extend to aws/azure

Explicit non-goals
- production ready deployment

TODO
- documentation
- well define configuration format
- nodes
",closed,True,2016-04-26 19:46:41,2016-05-13 21:50:42
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/issues/10,https://api.github.com/repos/kubernetes/kube-deploy/issues/10,Move docker-multinode to this project,"Ref: kubernetes/kubernetes#21646

@mikedanese @fgrzadkowski @vishh @cheld @janetkuo @justinsb
",closed,False,2016-05-01 13:43:16,2016-05-24 14:46:08
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/11,https://api.github.com/repos/kubernetes/kube-deploy/issues/11,add to the readme a description of what this project's goal is,,closed,True,2016-05-03 18:15:16,2016-05-03 19:51:51
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/12,https://api.github.com/repos/kubernetes/kube-deploy/issues/12,Initial add of docker-multinode,"Here's kubernetes/kubernetes#21646 for this repo instead.
Ref: #10 

The scripts here are refactored, but this hasn't been merged into Kuberenetes core, so we should review it and test it here.
I'm sure there are bugs around (haven't touched this for about two months), so feel free to comment on it.

The docs around this are just unorganized, mostly outdated and kind of ugly.
We should link to guides in the docs for example instead of having a testing section here
We should discuss how this updates should be made to the `docs` repo when updating something here etc.

It should be much easier to run this guide too, and I'm proposing to have just one script (we may call it kube-up or whatever) instead of many small (master, worker, etc)
This doesn't work with `docker-1.11`
We should use @ArtfulCoder's one-container DNS solution instead of the current one I think

And the long-term goal would probably to be to write this purely in Go instead.

I will update this when I have time, and when we have an initial draft, let's merge it.

@johndmulhausen @mikedanese @vishh @janetkuo @fgrzadkowski @cheld @zreigz @batikanu @jimmyjones2 @jumpkick @yidinghan @brendandburns
",closed,True,2016-05-03 20:44:21,2016-05-25 12:50:25
kube-deploy,cheld,https://github.com/kubernetes/kube-deploy/issues/13,https://api.github.com/repos/kubernetes/kube-deploy/issues/13,Deploy add-ons to docker-multinode,"The most important add-ons (DNS,Heapster,Dashboard) should be deployed easily or automatically when using docker-multinode

We have created an experimental implementation for that. It is an add-on container, similar to Hypercube:
https://github.com/FujitsuEnablingSoftwareTechnologyGmbH/k8s-docker-provisioner/tree/master/addons

One reason is that the descriptors are in yaml which requires kubectl. 

Any other idea?
",closed,False,2016-05-05 05:01:09,2016-05-27 11:13:47
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/14,https://api.github.com/repos/kubernetes/kube-deploy/issues/14,UpUp: AWS support,"Adds AWS support for both cloudup & nodeup.
Also cleaning up things found along the way!

cc @mikedanese

<!-- Reviewable:start -->

---

This change is [<img src=""http://reviewable.k8s.io/review_button.svg"" height=""35"" align=""absmiddle"" alt=""Reviewable""/>](http://reviewable.k8s.io/reviews/kubernetes/kube-deploy/14)

<!-- Reviewable:end -->
",closed,True,2016-05-09 17:10:43,2016-05-11 21:10:05
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/15,https://api.github.com/repos/kubernetes/kube-deploy/issues/15,"upup: Support for persistent disks, users & symlinks","These are required to mount the master PD.  We use the kubernetes
safe-format-and-mount implementation.

A little bit bloated by the lack of a group-id-to-group function in go, but that should be in 1.7: https://github.com/golang/go/commit/42f07ff2679d38a03522db3ccd488f4cc230c8c2
",closed,True,2016-05-13 18:53:21,2016-05-17 16:32:56
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/16,https://api.github.com/repos/kubernetes/kube-deploy/issues/16,upup: Map GCE image-url to string consistently,"We need to reverse our image shortening consistently with how we resolve
the image, so that --dryrun does not report spurious changes.
",closed,True,2016-05-15 21:40:13,2016-05-17 16:33:02
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/17,https://api.github.com/repos/kubernetes/kube-deploy/issues/17,upup: Discover metadata on GCE instance,"We simply weren't mapping it before, which was causing spurious changes.
",closed,True,2016-05-15 21:41:17,2016-05-17 16:33:11
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/18,https://api.github.com/repos/kubernetes/kube-deploy/issues/18,"upup: Make reflective walk more logical, use for dryrun change printing","Remove a bunch of inconsistencies so that the reflective walk is not
suprising, and also rename it to ReflectRecursive.

Then use this for dry-run change printing.

cc @mikedanese 
",closed,True,2016-05-15 21:42:11,2016-05-17 16:33:16
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/19,https://api.github.com/repos/kubernetes/kube-deploy/issues/19,upup: better keypair support,"- Create HasAddress on cloud addresses, so that the keypair task can depend on them and extract the IP
- Allow render to work against a Context, rather than only against a Target (for keypairs, of course!)
- Rework Keypair task so it follows our pattern & mechanism

We now have automatic dryrun and modification of existing keys (e.g. if the ip address changes!)

cc @mikedanese 
",closed,True,2016-05-15 21:45:27,2016-05-17 16:33:24
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/20,https://api.github.com/repos/kubernetes/kube-deploy/issues/20,upup: add command to generate kubecfg,"Add an upup command, using the cobra framework, currently with a single command, which generates a kubecfg file for a cluster.

cc @mikedanese 
",closed,True,2016-05-15 21:51:02,2016-05-22 20:00:34
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/21,https://api.github.com/repos/kubernetes/kube-deploy/issues/21,upup: Executor that performs some retry logic,"If there is an error performing a task, we will reattempt it as long as
forward progress is still being made (i.e. at least one other task
completed successfully)

This makes everything more reliable (though we should still fix these
problems), but it also lays the groundwork for parallel execution.
",closed,True,2016-05-16 02:30:51,2016-05-22 21:26:44
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/22,https://api.github.com/repos/kubernetes/kube-deploy/issues/22,upup: Fix fingerprint calculation in AWS keypair,"Both fix the calculation itself to match AWS's weird fingerprint
algorithm, and also fix the comparison logic by which we infer that if
the fingerprint matches, that the public key matches also.

cc @mikedanese 
",closed,True,2016-05-16 03:28:45,2016-05-17 16:33:39
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/23,https://api.github.com/repos/kubernetes/kube-deploy/issues/23,upup: Discover tags on AWS instances,"We simply weren't mapping them previously.
",closed,True,2016-05-16 03:33:19,2016-05-17 16:33:50
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/24,https://api.github.com/repos/kubernetes/kube-deploy/issues/24,upup: Perform JSON comparison on IAMRole PolicyDocuments,"AWS reformats them (inserting lots of whitespace), making a string
comparison incorrect.  Instead we parse to JSON and do a
reflect.DeepEqual check; if they are the same then we pretend the actual
value was the expected value.
",closed,True,2016-05-16 03:49:43,2016-05-17 16:34:03
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/25,https://api.github.com/repos/kubernetes/kube-deploy/issues/25,upup: fix dryrun spurious printing of interface values,"We missed the case where the interface changed (this only shows up if
other fields change also)
",closed,True,2016-05-16 14:59:55,2016-05-17 16:34:17
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/26,https://api.github.com/repos/kubernetes/kube-deploy/issues/26,upup: Fetch instance userdata on AWS,"We simply weren't doing this until now
",closed,True,2016-05-16 15:13:26,2016-05-17 16:34:30
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/27,https://api.github.com/repos/kubernetes/kube-deploy/issues/27,upup: Map matching image ids to source name,"This avoids spurious changes, and also is more intuitive for the user -
whatever name the user gave it, if it resolves to the same image, that
is the name we will use.
",closed,True,2016-05-16 15:21:05,2016-05-17 16:34:43
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/28,https://api.github.com/repos/kubernetes/kube-deploy/issues/28,upup: tolerate aws eventual-consistency errors on tags,"AWS will sometimes return an error like ""resource not found"" when a
DescribeTags or CreateTags call immediately follows creation of the
resource.  Introduce a retry-loop when we get an error that is of the
appropriate type.
",closed,True,2016-05-16 16:09:41,2016-05-17 16:34:53
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/29,https://api.github.com/repos/kubernetes/kube-deploy/issues/29,nodeup has hard-coded v1.2.2 in some image names,"It should honor the version passed by cloudup
",closed,False,2016-05-16 16:12:42,2016-06-12 23:47:15
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/30,https://api.github.com/repos/kubernetes/kube-deploy/issues/30,reorganize turnup directory structure,"@colemickens is this going to be a hassle for you to rebase on top of?
",closed,True,2016-05-17 16:26:02,2016-05-17 18:25:06
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/31,https://api.github.com/repos/kubernetes/kube-deploy/issues/31,min-turnup: fill out the basic Kconfig structure,,closed,True,2016-05-17 18:55:31,2016-05-17 21:00:08
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/32,https://api.github.com/repos/kubernetes/kube-deploy/issues/32,move the gce specific cfg.jsonnet file into Kconfig,,closed,True,2016-05-17 22:24:44,2016-05-18 23:19:15
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/33,https://api.github.com/repos/kubernetes/kube-deploy/issues/33,min-turnup: move gce specific bits out of phase 2,"It was just the crypto stuff and some templating of the cloudprovider.

cc @colemickens 
",closed,True,2016-05-18 23:34:46,2016-05-19 19:13:17
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/34,https://api.github.com/repos/kubernetes/kube-deploy/issues/34,create a chroot aci for docker and kubelet,"./unpack creates a chroot with docker and kubelet sutible for using as the RootDirectory (chroot env) for a systemd unit.
",closed,True,2016-05-19 21:31:38,2017-01-11 17:59:12
kube-deploy,errordeveloper,https://github.com/kubernetes/kube-deploy/issues/35,https://api.github.com/repos/kubernetes/kube-deploy/issues/35,PKI can be created with Terraform,"Just for the record, certs [can be generated using Terraform](https://www.terraform.io/docs/providers/tls/). I haven't tested this feature and only learned about it somewhat recently, I would love to hear if anyone has tried it in Kubernetes context or in general. It does seem appropriate for min-turnup and any other solutions using Terraform.
",closed,False,2016-05-20 12:25:51,2016-06-22 19:52:28
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/36,https://api.github.com/repos/kubernetes/kube-deploy/issues/36,Add possibility to set interfaces during configuration,"This code allows set up interface which IP address will be used to override hostname address for hypercube instance.
",closed,True,2016-05-23 10:35:27,2016-05-23 12:59:04
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/issues/37,https://api.github.com/repos/kubernetes/kube-deploy/issues/37,min-turnup: implement a phase 1 for GCE using deployment manager,"https://cloud.google.com/deployment-manager/docs/
",closed,False,2016-05-23 22:54:22,2017-11-01 22:47:51
kube-deploy,cheld,https://github.com/kubernetes/kube-deploy/pull/38,https://api.github.com/repos/kubernetes/kube-deploy/issues/38,automatically detect iface in docker multi-node,"automatically selecting network interface by choosing default route - at least better than eth0
",closed,True,2016-05-25 11:57:43,2016-05-27 12:22:18
kube-deploy,cheld,https://github.com/kubernetes/kube-deploy/pull/39,https://api.github.com/repos/kubernetes/kube-deploy/issues/39,fix link in docker multi-node documentation,,closed,True,2016-05-25 12:44:18,2016-05-26 19:07:26
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/40,https://api.github.com/repos/kubernetes/kube-deploy/issues/40,start a development guide for min-turnup,,closed,True,2016-05-26 18:50:26,2017-01-11 17:59:25
kube-deploy,roblalonde,https://github.com/kubernetes/kube-deploy/pull/41,https://api.github.com/repos/kubernetes/kube-deploy/issues/41,Added information for the Navops Launch installer by Univa,"Added a folder with documentation and readme. Includes link for people to download Navops Launch.
",closed,True,2016-05-26 20:11:49,2016-06-06 21:18:25
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/42,https://api.github.com/repos/kubernetes/kube-deploy/issues/42,cloudup: fromPort/toPort on securityGroupIngress is ambiguous,"It means port range, but it is easy to confuse with source & destination ports.
",closed,False,2016-05-28 18:02:07,2016-06-27 03:16:44
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/43,https://api.github.com/repos/kubernetes/kube-deploy/issues/43,Fix mounting volumes problem,"This PR fixes volume problem especially  `Downward API volume`.
I've run conformance e2e tests. 
Before modification:

```
Ran 94 of 293 Specs in 3553.798 seconds
FAIL! -- 57 Passed | 34 Failed | 0 Pending | 199 Skipped 
```

After modification

```
Ran 94 of 293 Specs in 14656.223 seconds
FAIL! -- 65 Passed | 29 Failed | 0 Pending | 199 Skipped 
```
",closed,True,2016-05-30 06:16:25,2016-06-03 10:55:58
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/44,https://api.github.com/repos/kubernetes/kube-deploy/issues/44,Can we reuse componentconfig for our configuration spec?,"I'd like to experiment with using the componentconfig types for our configuration.  (I think it would be awesome if we could converge on a single configuration system for the various phases, and the types in k8s itself seem a logical choice)

Is there are way to instruct e.g. kubelet to read configuration from a JSON file containing a `componentconfig.KubeletConfiguration`?  Or to generate flags from an object of the same?

I've poked around but didn't see anything.

cc @mikedanese 
",closed,False,2016-05-30 13:34:28,2016-06-27 04:43:25
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/45,https://api.github.com/repos/kubernetes/kube-deploy/issues/45,upup: Replace unmarhsal code with per-type unmarshallers,"Instead of reimplementing the unmarshal code, we implement a trick: we implement an alternative JSON representation of our objects: a string.  We do this using a go generator to eliminate needing to type the required boilerplate.

So now we can unmarshal as normal, and then we replace these 'stub' values with the correct values, by walking the unmarshalled tree.
",closed,True,2016-05-30 21:48:55,2016-06-04 19:56:43
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/46,https://api.github.com/repos/kubernetes/kube-deploy/issues/46,upup: Simple route53 (DNS) support,,closed,True,2016-05-30 21:51:11,2016-06-04 19:56:48
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/47,https://api.github.com/repos/kubernetes/kube-deploy/issues/47,upup: simple command to delete (AWS) clusters,"A relatively dumb retry strategy to work around dependencies, but it
works and it is difficult to do _much_ better.
",closed,True,2016-05-30 21:58:23,2016-06-04 19:56:52
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/48,https://api.github.com/repos/kubernetes/kube-deploy/issues/48,upup: simple ELB support,"Add support for ELB, though this doesn't wire it up yet
",closed,True,2016-05-30 21:58:36,2016-06-04 19:56:57
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/49,https://api.github.com/repos/kubernetes/kube-deploy/issues/49,Protokube: prototyping the 'missing' kubelet pieces,"Working towards self-hosting of k8s, we will likely have to add some
features to kubelet, such as independent mounting of disks or copying of
resources from S3.  protokube lets us develop those features prior to
moving them into kubelet.

In particular, today we need to mount an EBS volume on the master prior
to starting kubelet, if we want to run the master in an ASG.

protokube is a service that runs on boot, and it tries to mount the
master volume.  Once it mounts the master volume, it runs kubelet.
Currently it runs kubelet by looking at a directory
/etc/kubernetes/bootstrap; the intention is that we could actually have
multiple versions of kubelet in here (or other services) and then we
could automatically roll-back from a failed update.
",closed,True,2016-05-30 22:06:08,2016-06-04 19:57:03
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/50,https://api.github.com/repos/kubernetes/kube-deploy/issues/50,upup: better secrets support,"Start creating commands to manage secrets, and also stop implicitly
creating them.
",closed,True,2016-05-30 22:48:16,2016-06-04 19:57:08
kube-deploy,uromahn,https://github.com/kubernetes/kube-deploy/pull/51,https://api.github.com/repos/kubernetes/kube-deploy/issues/51,BUG: removed comments at end of continuation line and merged lines,"header says it all.
I also merged the three lines into one for better readability (at least for me) :)
",closed,True,2016-06-02 14:39:34,2016-06-28 20:19:45
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/52,https://api.github.com/repos/kubernetes/kube-deploy/issues/52,upup: tracking issue to reach v1,"Primary tasks before we can declare v1

P0
- [ ] finalize configuration schema (#44)
- [x] reach consensus on protokube
- [x] clean up terraform generation

P1
- [ ] fix tagging edge cases

P2
- [x] bring up master (& etcd) in ha mode
- [ ] decide if we want to use ELB on single-master
- [ ] store state in S3?
- [ ] encrypt state using KMS?
",closed,False,2016-06-02 18:02:10,2016-06-23 15:12:39
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/53,https://api.github.com/repos/kubernetes/kube-deploy/issues/53,"upup: Add gofmt,codegen command to Makefile",,closed,True,2016-06-04 20:10:33,2016-06-06 21:26:09
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/54,https://api.github.com/repos/kubernetes/kube-deploy/issues/54,Use protokube with upup,,closed,True,2016-06-04 20:12:55,2016-06-07 12:56:24
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/55,https://api.github.com/repos/kubernetes/kube-deploy/issues/55,upup: don't hard-code v1.2.2 in image names,,closed,True,2016-06-04 20:13:35,2016-06-06 21:26:41
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/56,https://api.github.com/repos/kubernetes/kube-deploy/issues/56,nodeup: make sure tasks implement HasDependencies,,closed,True,2016-06-04 20:14:06,2016-06-07 12:43:11
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/57,https://api.github.com/repos/kubernetes/kube-deploy/issues/57,cloudup: better retrying around resource tagging,,closed,True,2016-06-04 20:14:46,2016-06-07 12:43:21
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/58,https://api.github.com/repos/kubernetes/kube-deploy/issues/58,upup: better cluster delete,,closed,True,2016-06-04 20:15:25,2016-06-07 12:47:21
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/59,https://api.github.com/repos/kubernetes/kube-deploy/issues/59,Do not hardcode the path to docker.service,"Trivial path change.

Please review some of you:
@zreigz @cheld @mikedanese 
",closed,True,2016-06-05 06:25:29,2016-06-06 12:55:06
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/60,https://api.github.com/repos/kubernetes/kube-deploy/issues/60,Protokube: components through volumes,"We can now look at volume tags and create manifests in /etc/kubernetes/manifests based on those tags./

Also support for running in a container.
",closed,True,2016-06-07 12:48:49,2016-06-07 19:19:44
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/61,https://api.github.com/repos/kubernetes/kube-deploy/issues/61,cloudup: Add support for ELB health checks,,closed,True,2016-06-07 13:02:49,2016-06-07 13:03:29
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/62,https://api.github.com/repos/kubernetes/kube-deploy/issues/62,upup: apply IAM changes,"We now apply changes to IAM policies, and print the diffs.
",closed,True,2016-06-07 19:18:52,2016-06-07 19:19:59
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/63,https://api.github.com/repos/kubernetes/kube-deploy/issues/63,upup: run create & delete in parallel,"It makes logging harder, but we want to start testing it!
",closed,True,2016-06-07 19:24:18,2016-06-07 19:34:06
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/64,https://api.github.com/repos/kubernetes/kube-deploy/issues/64,upup: use protokube,"This wires up protokube, replacing a lot of the nodeup functionality.

The option of not using protokube is retained, via the _not_protokube
tag.
",closed,True,2016-06-07 19:33:49,2016-06-07 19:38:11
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/65,https://api.github.com/repos/kubernetes/kube-deploy/issues/65,upup: Add back in CompareWithID functions,"Removed when we pulled this functionality out of the go generator, but
then the manual code not re-added.

Also a few other small tweaks to Tasks.
",closed,True,2016-06-07 19:37:29,2016-06-07 19:39:51
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/66,https://api.github.com/repos/kubernetes/kube-deploy/issues/66,upup: HA support,"Specifying multiple zones will bring up an HA cluster.
",closed,True,2016-06-07 19:45:15,2016-06-07 19:45:49
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/67,https://api.github.com/repos/kubernetes/kube-deploy/issues/67,"upup: Update makefile, readme, deps",,closed,True,2016-06-07 21:54:24,2016-06-07 21:54:30
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/68,https://api.github.com/repos/kubernetes/kube-deploy/issues/68,upup: issue pulling glog dependency (with glide),"Issue reported to me:

```
# I think strip-vendor is the workaround for 25572
glide install --strip-vendor --strip-vcs
[INFO] Downloading dependencies. Please wait...
[INFO] Fetching updates for github.com/aws/aws-sdk-go.
[INFO] Fetching updates for github.com/BurntSushi/toml.
[INFO] Fetching updates for github.com/cloudfoundry-incubator/candiedyaml.
[INFO] Fetching updates for github.com/davecgh/go-spew.
[INFO] Fetching updates for github.com/ghodss/yaml.
[INFO] Fetching updates for github.com/inconshreveable/mousetrap.
[INFO] Fetching updates for github.com/mitchellh/mapstructure.
[INFO] Fetching updates for github.com/spf13/cobra.
[INFO] Fetching updates for github.com/spf13/cast.
[INFO] Fetching updates for github.com/spf13/pflag.
[INFO] Fetching updates for github.com/spf13/jwalterweatherman.
[INFO] Fetching updates for golang.org/x/crypto.
[INFO] Fetching updates for github.com/golang/protobuf.
[INFO] Fetching updates for github.com/hashicorp/hcl.
[INFO] Fetching updates for github.com/magiconair/properties.
[INFO] Fetching updates for github.com/jmespath/go-jmespath.
[INFO] Fetching updates for github.com/golang/glog.
[INFO] Fetching updates for github.com/spf13/viper.
[INFO] Fetching updates for github.com/fsnotify/fsnotify.
[INFO] Fetching updates for github.com/go-ini/ini.
[INFO] Fetching updates for golang.org/x/net.
[INFO] Fetching updates for golang.org/x/oauth2.
[INFO] Fetching updates for golang.org/x/sys.
[INFO] Fetching updates for google.golang.org/api.
[INFO] Fetching updates for google.golang.org/appengine.
[INFO] Fetching updates for google.golang.org/cloud.
[INFO] Fetching updates for google.golang.org/grpc.
[INFO] Fetching updates for gopkg.in/yaml.v2.
[INFO] Fetching updates for k8s.io/kubernetes.
[WARN] Unable to checkout google.golang.org/api
[ERROR] Update failed for google.golang.org/api: Cloning into 'XXX/go/src/k8s.io/kube-deploy/upup/vendor/google.golang.org/api'...
error: RPC failed; HTTP 502 curl 22 The requested URL returned error: 502 Bad Gateway
fatal: The remote end hung up unexpectedly
: exit status 128
[INFO] Downloading dependencies. Please wait...
[INFO] Setting references.
[INFO] Setting version for golang.org/x/crypto to 77f4136a99ffb5ecdbdd0226bd5cb146cf56bc0e.
[INFO] Setting version for github.com/spf13/jwalterweatherman to 33c24e77fb80341fe7130ee7c594256ff08ccc46.
[INFO] Setting version for github.com/inconshreveable/mousetrap to 76626ae9c91c4f2a10f34cad8ce83ea42c93bb75.
[INFO] Setting version for github.com/BurntSushi/toml to f0aeabca5a127c4078abb8c8d64298b147264b55.
...
[INFO] Setting version for github.com/spf13/pflag to cb88ea77998c3f024757528e3305022ab50b43be.
[INFO] Setting version for github.com/magiconair/properties to c265cfa48dda6474e208715ca93e987829f572f8.
[ERROR] Failed to set version on google.golang.org/api to 63ade871fd3aec1225809d496e81ec91ab76ea29: open XXX/go/src/k8s.io/kube-deploy/upup/vendor/google.golang.org/api: no such file or directory
[INFO] Setting version for github.com/spf13/cobra to 1238ba19d24b0b9ceee2094e1cb31947d45c3e86.
...
An Error has occurred
make: *** [godeps] Error 2
```

Retrying did work.
",closed,False,2016-06-08 00:42:16,2017-11-01 21:30:58
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/69,https://api.github.com/repos/kubernetes/kube-deploy/issues/69,Add experimental-flannel-overlay flag,"This PR is some kind of proof of concept for experimental-flannel-overlay flag. Because it is not well documented yet I was experimenting with this. I've removed docker ""bootstrap"" service and flannel. What I've seen from logs it uses `hairpin` plugin with `hairpin-veth` mode. I've executed e2e test with different hyperkube versions

v1.2.0

```
Ran 94 of 293 Specs in 14982.578 seconds
FAIL! -- 59 Passed | 35 Failed | 0 Pending | 199 Skipped 
```

v1.2.4

```
Ran 94 of 293 Specs in 4962.784 seconds
FAIL! -- 59 Passed | 32 Failed | 0 Pending | 199 Skipped 
```

v1.3.0.alpha5

```
Ran 94 of 293 Specs in 591.524 seconds
FAIL! -- 81 Passed | 13 Failed | 0 Pending | 199 Skipped
```

I hope it will open discussion about networking in docker-multinode project.
",closed,True,2016-06-08 11:09:37,2016-06-17 07:01:35
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/70,https://api.github.com/repos/kubernetes/kube-deploy/issues/70,upup: not all dependencies deleted,"We don't currently delete:
- the SSH keypair
- the IAM role / policy etc (we could maybe use the IAM path to mark these)
- the DNS zone (that is probably correct)
",closed,False,2016-06-08 13:57:57,2017-11-01 21:30:58
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/71,https://api.github.com/repos/kubernetes/kube-deploy/issues/71,upup: support for terraform on AWS,"All seems good except for a bug with volume tagging
",closed,True,2016-06-08 16:14:07,2016-06-08 16:19:55
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/72,https://api.github.com/repos/kubernetes/kube-deploy/issues/72,upup: misc polish marching towards v1,"Less extraneous/debug logging, better logging etc
",closed,True,2016-06-08 16:19:12,2016-06-08 16:19:17
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/73,https://api.github.com/repos/kubernetes/kube-deploy/issues/73,upup: Secret store / CA store not thread safe,"I think I saw an error which suggested that the secret store served an invalid JSON from the secret.  This would happen if we were mid-write during a read, I believe.  We should write files atomically (anyway), and we should probably do some locking, particularly during create-if-not-exists operations.
",closed,False,2016-06-09 05:45:55,2016-06-12 03:05:51
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/74,https://api.github.com/repos/kubernetes/kube-deploy/issues/74,upup: terraform issue #2143 prevents creation of EC2 tags with dots,"Terraform issue 2143 prevents creation of EC2 tags with dots, which we use for `k8s.io/...`  It actually works for autoscaling groups; the only place it causes us trouble is with volumes.

Going to investigate a workaround where we create the volumes outside of terraform anyway, as you might not want terraform blindly deleting the crucial state of your cluster!
",closed,False,2016-06-09 13:37:20,2017-11-01 21:30:58
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/75,https://api.github.com/repos/kubernetes/kube-deploy/issues/75,upup: Add VFS for storing state in S3 or locally,"This also fixes a few concurrency issues, because we're writing in one
place now.
",closed,True,2016-06-10 03:09:08,2016-06-10 03:12:54
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/76,https://api.github.com/repos/kubernetes/kube-deploy/issues/76,upup: reformat terraform output,"Using string manipulation, sadly, but I believe it's safe as we don't
use heredocs
",closed,True,2016-06-10 03:09:11,2016-06-10 03:12:39
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/77,https://api.github.com/repos/kubernetes/kube-deploy/issues/77,upup: default to latest stable release,"Avoids an extra CLI arg
",closed,True,2016-06-10 03:12:45,2016-06-10 03:12:59
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/78,https://api.github.com/repos/kubernetes/kube-deploy/issues/78,upup: split model into two parts,"This is probably a good idea anyway, but it also lets us side-step the
terraform no-dots-in-tags bug.
",closed,True,2016-06-10 03:15:28,2016-06-10 03:16:05
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/79,https://api.github.com/repos/kubernetes/kube-deploy/issues/79,upup: harmonize model with official config types,"This is what I was talking about for using the official type model @mikedanese 
",closed,True,2016-06-10 05:30:26,2016-06-10 15:31:10
kube-deploy,earthIsAPrisonIfYouNotAwareItCanNotOut,https://github.com/kubernetes/kube-deploy/issues/80,https://api.github.com/repos/kubernetes/kube-deploy/issues/80,all config lost when restart the master server by docker-mutinode,"unable to save current status when restart server and use master.sh again.
",closed,False,2016-06-10 06:00:46,2017-11-01 22:50:25
kube-deploy,ghost,https://github.com/kubernetes/kube-deploy/pull/81,https://api.github.com/repos/kubernetes/kube-deploy/issues/81,Fix comments in multilines.,"Fixes the same problem as #51 while keeping the multiline and comments using the approach suggested here:

http://stackoverflow.com/a/12797512
",closed,True,2016-06-10 10:06:39,2016-06-20 20:23:10
kube-deploy,ghost,https://github.com/kubernetes/kube-deploy/pull/82,https://api.github.com/repos/kubernetes/kube-deploy/issues/82,Use double dash cmdline flags for etcd and  kube2sky cmdlines,"""kube2sky"" container fails to start when following the steps described here:
https://github.com/kubernetes/kube-deploy/blob/master/docker-multinode/deployDNS.md

./kube2sky seems to expect --domain instead of -domain on the cmdline. Etcd also seems to require double dash flags. This PR updates the template to reflect that.
",closed,True,2016-06-10 12:36:09,2016-06-28 17:10:09
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/83,https://api.github.com/repos/kubernetes/kube-deploy/issues/83,upup: node size on CLI and more instance type families,"Adds a `--node-size` argument to the CLI; defines the m3,m4,c4 families
",closed,True,2016-06-10 15:32:35,2016-06-10 15:32:49
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/84,https://api.github.com/repos/kubernetes/kube-deploy/issues/84,upup: prevent spurious changes on tags & names,"A few places we missed populating the name, also compute the actual complete set of tags in advance if we allow tag customization
",closed,True,2016-06-10 15:34:59,2016-06-10 15:35:03
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/85,https://api.github.com/repos/kubernetes/kube-deploy/issues/85,upup: fix options processing,"We want to make sure that any values we set are treated both as defaults
and as overrides (i.e. the options shouldn't be able to override a value
the user has explicitly set)
",closed,True,2016-06-10 15:36:19,2016-06-10 15:36:22
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/86,https://api.github.com/repos/kubernetes/kube-deploy/issues/86,upup: split launchconfiguration from ASG,"It is much more logical this way, and mirrors the way GCE & terraform
work.
",closed,True,2016-06-10 15:37:03,2016-06-10 15:37:08
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/87,https://api.github.com/repos/kubernetes/kube-deploy/issues/87,upup: add experimental cluster rolling-update command,"Little more than a hack right now, but a good place to start.
",closed,True,2016-06-10 15:37:54,2016-06-10 15:38:00
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/88,https://api.github.com/repos/kubernetes/kube-deploy/issues/88,upup: use JSON as fallback in --dryrun printing,"Rather than giving up with an error, we can simply use JSON format.
",closed,True,2016-06-10 15:39:06,2016-06-10 15:39:10
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/89,https://api.github.com/repos/kubernetes/kube-deploy/issues/89,upup: apply gofmt,"Missed a few gofmt places
",closed,True,2016-06-10 15:42:09,2016-06-10 15:42:20
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/90,https://api.github.com/repos/kubernetes/kube-deploy/issues/90,upup: more polish,"Terraform round-tripping and a few bugfixes
",closed,True,2016-06-10 17:41:27,2016-06-10 17:41:31
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/91,https://api.github.com/repos/kubernetes/kube-deploy/issues/91,upup: race condition with kube-addons,"Sometimes, typically when something else goes wrong, kube-addons will be stuck in a loop:

```
Jun 10 19:31:59 ip-172-20-22-48 kube-addons.sh[718]: Error from server: serviceaccounts ""default"" not found
Jun 10 19:32:00 ip-172-20-22-48 kube-addons.sh[718]: Error from server: serviceaccounts ""default"" not found
Jun 10 19:32:00 ip-172-20-22-48 kube-addons.sh[718]: Error from server: serviceaccounts ""default"" not found
```

This blocks cluster bring-up entirely.  KCM won't allocate PodCIDRs etc.

Restarting kube-addons causes it to recover and the cluster starts normally.
",closed,False,2016-06-10 19:34:57,2016-12-04 06:43:55
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/92,https://api.github.com/repos/kubernetes/kube-deploy/issues/92,upup: fix typo,"Duplicated zone name
",closed,True,2016-06-10 22:48:07,2016-06-10 22:48:11
kube-deploy,ghost,https://github.com/kubernetes/kube-deploy/pull/93,https://api.github.com/repos/kubernetes/kube-deploy/issues/93,Use -e grep flag to match patterns starting with -.,"Fixes in docker-multinode/{worker.sh/master.sh}

Setting up bridge-utils (1.5-6ubuntu2) ...
grep: unrecognized option '--bip'
Usage: grep [OPTION]... PATTERN [FILE]...
Try 'grep --help' for more information.
docker stop/waiting
",closed,True,2016-06-11 07:17:55,2016-07-11 20:14:38
kube-deploy,colemickens,https://github.com/kubernetes/kube-deploy/issues/94,https://api.github.com/repos/kubernetes/kube-deploy/issues/94,[min-turnup] kubelet.service can hangnode,"edit: see further down for the node hanging issue

---

I have something close to working with `min-turnup`, but I'm running into an issue with the playbook.

Where does `master_ip` get added to the `cfg` object [here](https://github.com/kubernetes/kube-deploy/blob/09c5d3c80c424d15c34bbf36624d15cb85e0369c/min-turnup/phase2/ansible/playbooks/roles/node/templates/kubeconfig.jsonnet#L16)? 

```
RuntimeError: RUNTIME ERROR: No such field: master_ip
    std.jsonnet:584:29-55   thunk <val>
    std.jsonnet:589:41-43   thunk <val>
    std.jsonnet:440:30-32   thunk <a>
    std.jsonnet:28:21   
    std.jsonnet:28:12-22    thunk <a>
    std.jsonnet:28:12-34    function <anonymous>
    std.jsonnet:28:12-34    function <anonymous>
    std.jsonnet:440:17-33   function <format_code>
    std.jsonnet:589:29-63   thunk <s>
    std.jsonnet:594:38  thunk <str>
    ...
    std.jsonnet:595:61-68   thunk <v>
    std.jsonnet:595:21-69   function <format_codes_obj>
    std.jsonnet:595:21-69   function <format_codes_obj>
    std.jsonnet:600:13-48   function <anonymous>
    std.jsonnet:134:13-28   function <anonymous>
    /opt/playbooks/roles/node/templates/kubeconfig.jsonnet:16:17-45 object <anonymous>
    /opt/playbooks/roles/node/templates/kubeconfig.jsonnet:(14:16)-(17:7)   object <anonymous>
    /opt/playbooks/roles/node/templates/kubeconfig.jsonnet:(12:16)-(18:5)   thunk <array_element>
    /opt/playbooks/roles/node/templates/kubeconfig.jsonnet:(12:15)-(18:6)   object <anonymous>
    During manifestation    
```
",closed,False,2016-06-11 12:37:46,2017-11-01 22:48:05
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/95,https://api.github.com/repos/kubernetes/kube-deploy/issues/95,upup: should be able to launch into existing VPC (on AWS),"A consistent request is to make it easy to launch k8s clusters into an existing VPC.

Two parts, I think:
- Specifying an existing VPC
- Choosing a non-conflicting subnet and/or verifying that a provided subnet does not conflict
",closed,False,2016-06-11 22:25:23,2016-06-13 21:36:52
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/96,https://api.github.com/repos/kubernetes/kube-deploy/issues/96,upup: we should verify that the zones are valid,"It is easy to typo a zone, or to type a duplicate zone.  We should do some validation on the user provided list of zones.
",closed,False,2016-06-11 22:26:51,2016-06-12 03:07:58
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/97,https://api.github.com/repos/kubernetes/kube-deploy/issues/97,upup: describe vendoring and 1.5 build options,"We should support people building with golang 1.5 (or document a restriction if we can't do that).  This may be as simple as adding GO15VENDOREXPERIMENT=1 to the instructions / makefile.
",closed,False,2016-06-11 22:27:46,2016-06-12 03:33:05
kube-deploy,janwillies,https://github.com/kubernetes/kube-deploy/issues/98,https://api.github.com/repos/kubernetes/kube-deploy/issues/98,upup: specify hosted zone for aws,"It should be possible to specify the Hosted Zone on AWS. 

Currently it's implicit, e.g `MYZONE=""test.foo.bar.example.com""` will result in `*awstasks.DNSZone     dnsZone/example.com` whereas the Hosted Zone actually is `foo.bar.example.com`
",closed,False,2016-06-11 22:44:23,2016-06-12 03:06:04
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/99,https://api.github.com/repos/kubernetes/kube-deploy/issues/99,upup: unusual bug when deleting ASG,"```
autoscaling-group:kubernetes.master.eu-west-1a.kubernetes-e2e-upup-aws.awsdata.com      error deleting resource, will retry: error deleting autoscaling group ""kubernetes.master.eu-west-1a.kubernetes-e2e-upup-aws.awsdata.com"": ValidationError: AutoScalingGroup name not found - AutoScalingGroup 'kubernetes.master.eu-west-1a.kubernetes-e2e-upup-aws.awsdata.com' not found
```

I guess this is the ""already deleted"" message.
",closed,False,2016-06-11 23:16:49,2016-06-12 03:08:36
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/100,https://api.github.com/repos/kubernetes/kube-deploy/issues/100,upup: separate node & master zone configuration; validate,"We allow --zones & --master-zones to be specified separately now, but we
validate for common errors (using a region where you meant a zone,
duplicating a zone, spanning regions, entering an invalid AZ etc)
",closed,True,2016-06-12 01:07:18,2016-06-12 03:05:56
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/101,https://api.github.com/repos/kubernetes/kube-deploy/issues/101,upup: fixes for ASG deletion,"Issue #99
",closed,True,2016-06-12 01:34:57,2016-06-12 03:06:00
kube-deploy,netroby,https://github.com/kubernetes/kube-deploy/issues/102,https://api.github.com/repos/kubernetes/kube-deploy/issues/102,Run master node failed with docker-multiplenode,"```
root@kube-master:~/kube-deploy/docker-multinode# service docker start
root@kube-master:~/kube-deploy/docker-multinode# ps axu | grep docker
root      6627  0.0  0.5 146936 10576 ?        Ssl  01:59   0:00 docker-containerd -l /var/run/docker-bootstrap/libcontainerd/docker-containerd.sock --runtime docker-runc --start-timeout 2m
root      6937  2.5  1.6 348124 34124 ?        Ssl  02:01   0:00 /usr/bin/docker -s overlay daemon -H fd://
root      6944  0.3  0.4 286204  9868 ?        Ssl  02:01   0:00 docker-containerd -l /var/run/docker/libcontainerd/docker-containerd.sock --runtime docker-runc --start-timeout 2m
root      6995  0.0  0.0  12956   936 pts/0    S+   02:01   0:00 grep --color=auto docker
root@kube-master:~/kube-deploy/docker-multinode# ./master.sh 
+++ [0612 02:01:36] K8S_VERSION is set to: v1.2.4
+++ [0612 02:01:36] ETCD_VERSION is set to: 2.2.5
+++ [0612 02:01:36] FLANNEL_VERSION is set to: 0.5.5
+++ [0612 02:01:36] FLANNEL_IPMASQ is set to: true
+++ [0612 02:01:36] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [0612 02:01:36] FLANNEL_BACKEND is set to: udp
+++ [0612 02:01:36] DNS_DOMAIN is set to: cluster.local
+++ [0612 02:01:36] DNS_SERVER_IP is set to: 10.0.0.10
+++ [0612 02:01:36] RESTART_POLICY is set to: on-failure
+++ [0612 02:01:36] MASTER_IP is set to: 163.172.162.23
+++ [0612 02:01:36] ARCH is set to: amd64
+++ [0612 02:01:36] NET_INTERFACE is set to: eth0
+++ [0612 02:01:36] --------------------------------------------
+++ [0612 02:01:36] Detected OS: ubuntu
+++ [0612 02:01:36] Launching docker bootstrap...
!!! [0612 02:01:55] docker bootstrap failed to start. Exiting...

```
",closed,False,2016-06-12 02:03:10,2017-11-01 22:50:31
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/103,https://api.github.com/repos/kubernetes/kube-deploy/issues/103,upup: allow -dns-zone to be specified,"We default to the last two components of the cluster DNS name.  But a
lot of people will delelgate a subdomain and want to use that.

Fixes #98
",closed,True,2016-06-12 02:38:23,2016-06-12 03:06:04
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/104,https://api.github.com/repos/kubernetes/kube-deploy/issues/104,upup: Retry if race detected in FindOrCreateSecret,"We do a read / create-if-empty.  If the create fails because of a
concurrent creation, we retry the operation.

Fix #73
",closed,True,2016-06-12 02:42:14,2016-06-12 03:05:51
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/105,https://api.github.com/repos/kubernetes/kube-deploy/issues/105,upup: misc polish items found during first real-world tests,,closed,True,2016-06-12 03:11:36,2016-06-12 03:33:04
kube-deploy,colemickens,https://github.com/kubernetes/kube-deploy/pull/106,https://api.github.com/repos/kubernetes/kube-deploy/issues/106,min-turnup: azure,"I changed:
- `--service-cluster-ip-range` to not overlap with the ip range that will be used for the actual nodes.
- changed the manifest templates to jinja2, it was easier to do the syntax I needed for adding the `--cloud-config` flag
- changed the master pod manifests to use `hyperkube-amd64` instead of the `kube-{apiserver,scheduler,controller-manager}` images. `kubelet.service` was already using `hyperkube-amd64` so it's already on box, plus it makes it easier for me to test changes to cloudprovider if I only have to rebuild and upload hyperkube instead of 4+ containers.
- added a `Dockerfile` that has `terraform`, `jq`, `jsonnet`, etc. (especially since this relies of a pre-release version of terraform [https://github.com/hashicorp/terraform/issues/6803]).

This worksaround Azure's lack of metadata service by using terraform's `template_file` functionality. I use terraform's `base64encode` and `file` functions to access the contents of the crypto assets and use `template_file` to interpolate them into a `configure-vm.sh` template that is rendered and passed as `custom_data` to the VM. Similarly it passes through some environment information and Active Directory credentials that are required by the `cloudprovider` implementation. The `template_file` approach was chosen instead of using the `remote-exec` and `file` provisioners in terraform because they don't work easily for Azure currently: https://github.com/hashicorp/terraform/issues/7122.

This relies on the `azure` `cloudprovider`, so it current defaults to my own `hyperkube-amd` repo/image. (I'm blocked on sending the PR for the `cloudprovider` for reasons... but hopefully that will stop being a thing soon)

It should probably include a script to help create the Azure Service Principal that could then be used by Terraform and passed through for `cloudprovider`.

So, it's not ready to merge yet for reasons mentioned above, but I wanted to get this out for any comments. I was able to deploy a 20 cluster on the first try after getting the single master/node working: http://i.imgur.com/boyFZsw.png.

The biggest issue I see is that `addons` seem to be unaddressed. For now I haven't added `kube-proxy` as a manifest pod since I assume we're going to run it via a `DaemonSet` addon....
",closed,True,2016-06-12 03:25:00,2016-06-24 18:14:10
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/107,https://api.github.com/repos/kubernetes/kube-deploy/issues/107,"Don't manually deploy proxy with v1.3.0-alpha.5+, remove outdated docs, and improve README.md","We should get rid of the scripts that are hanging around in `kubernetes.github.io` as well, and update the docs to use this README, and point to this location.

The individual docs are now outdated, and if someone is interested in what docker-multinode is doing, the source code is very easy to follow.

I'd like to get this in ASAP

@mikedanese @cheld @zreigz 
",closed,True,2016-06-12 16:28:05,2016-06-17 22:30:44
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/108,https://api.github.com/repos/kubernetes/kube-deploy/issues/108,upup: switch from skydns to kube-dns,"We hide skydns (for now) in _old_skydns
",closed,True,2016-06-13 15:34:23,2016-06-13 15:37:03
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/109,https://api.github.com/repos/kubernetes/kube-deploy/issues/109,upup: experimental addon management,,closed,True,2016-06-13 15:36:02,2016-06-13 15:37:07
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/110,https://api.github.com/repos/kubernetes/kube-deploy/issues/110,upup: Support for shared VPCs,"A lot of work that had to happen here:
- Better reuse of config
- Ability to mark VPC & InternetGateway as shared
- Find models relative to the executable, to run from a dir-per-cluster

Fixes #95
",closed,True,2016-06-13 15:41:10,2016-06-13 21:36:52
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/111,https://api.github.com/repos/kubernetes/kube-deploy/issues/111,upup: more polish,,closed,True,2016-06-13 21:38:39,2016-06-13 21:38:44
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/112,https://api.github.com/repos/kubernetes/kube-deploy/issues/112,upup: spurious changes on shared for VPC / InternetGateway,"The new Shared flag is giving us spurious changes...

```
  *awstasks.VPC vpc/kubernetes.upgrade.awsdata.com
    Shared <nil> -> false

  *awstasks.InternetGateway     internetGateway/kubernetes.upgrade.awsdata.com
    Shared <nil> -> false
```
",closed,False,2016-06-14 15:52:06,2017-11-01 21:30:58
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/113,https://api.github.com/repos/kubernetes/kube-deploy/issues/113,upup: more polish,"More notably consistent zone -> CIDR assignment
",closed,True,2016-06-15 18:04:34,2016-06-15 18:04:38
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/114,https://api.github.com/repos/kubernetes/kube-deploy/issues/114,Quick first pass at docs,"This lets us link them from deprecated functionality in kube-up /
kube-down etc.
",closed,True,2016-06-16 03:50:02,2016-06-28 16:56:30
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/115,https://api.github.com/repos/kubernetes/kube-deploy/issues/115,Use CNI plugin,"This solution get rid of docker bootstrap service and uses cni plugin instead.
The newest hypercube must be use for this solution.

Tests result:

```
Summarizing 7 Failures:

[Fail] [k8s.io] Kubectl client [k8s.io] Kubectl describe [It] should check if kubectl describe prints relevant information for rc and pods [Conformance] [Flaky] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1291

[Fail] [k8s.io] Secrets [It] should be consumable from pods in volume [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:1670

[Fail] [k8s.io] Pods [It] should have monotonically increasing restart count [Conformance] [Slow] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/pods.go:58

[Fail] [k8s.io] Kubectl client [k8s.io] Kubectl run job [It] should create a job from an image when restart is Never [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1190

[Fail] [k8s.io] DNS [It] should provide DNS for the cluster [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/dns.go:194

[Fail] [k8s.io] ClusterDns [Feature:Example] [It] should create pod that uses dns [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/example_cluster_dns.go:130

[Fail] [k8s.io] Kubectl client [k8s.io] Guestbook application [It] should create and stop a working application [Conformance] 
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:1369

Ran 94 of 293 Specs in 571.681 seconds
FAIL! -- 87 Passed | 7 Failed | 0 Pending | 199 Skipped 

```
",closed,True,2016-06-17 11:21:53,2016-07-29 12:23:40
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/116,https://api.github.com/repos/kubernetes/kube-deploy/issues/116,upup: yet more polish,,closed,True,2016-06-17 13:38:11,2016-06-17 13:38:17
kube-deploy,knarz,https://github.com/kubernetes/kube-deploy/issues/117,https://api.github.com/repos/kubernetes/kube-deploy/issues/117,upup: support running etcd and apiserver on independent hosts,"Hello,

being able to run the etcd cluster independently from the apiservers would allow:
- managing the etcd cluster seperately
- performance isolation/scaling the api layer independently
- integrating with existing etcd setups

All other servers would then run etcd in proxy mode.
",closed,False,2016-06-17 14:15:19,2017-11-01 21:30:58
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/118,https://api.github.com/repos/kubernetes/kube-deploy/issues/118,upup: We should figure out what to do with instance storage / root disks / btrfs,"We've had a number of problems with ephemeral storage on EC2, not least that newer instance types don't include them (e.g. https://github.com/kubernetes/kubernetes/issues/23787).  Also symlinking /mnt/ephemeral seems to confuse the garbage collector.

We should figure out how to ensure that we have a big enough root disk, maybe how to re-enable btrfs, and then if there is anything we can do with the instance storage if we're otherwise not going to use it (maybe hostVolumes?  Or some sort of caching service?)
",closed,False,2016-06-18 03:41:58,2016-09-12 16:51:53
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/119,https://api.github.com/repos/kubernetes/kube-deploy/issues/119,Template for k8s 1.3 optimized image,"The image as needed by https://github.com/kubernetes/kubernetes/pull/27676

Also apply a long overdue gofmt in the second commit!
",closed,True,2016-06-19 00:51:41,2016-06-20 19:44:21
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/120,https://api.github.com/repos/kubernetes/kube-deploy/issues/120,upup: allow ICMP 3:4 in master/minion groups?,"If we're going to accept traffic to the master (for the API server), we should probably allow ICMP 3:4

cf https://github.com/kubernetes/kubernetes/issues/24254
",closed,False,2016-06-19 01:17:10,2017-11-01 21:30:58
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/121,https://api.github.com/repos/kubernetes/kube-deploy/issues/121,imagebuilder: we should set net.ipv4.neigh.default.gc_thresh1 = 0,"This is what AWS recommends and Amazon Linux does:
https://forums.aws.amazon.com/thread.jspa?messageID=572171

(Probably should check for any other ""magic"" settings in the official AWS AMIs)

cf https://github.com/kubernetes/kubernetes/issues/23395
",closed,False,2016-06-19 02:45:27,2018-02-17 03:13:04
kube-deploy,stefanodoni,https://github.com/kubernetes/kube-deploy/issues/122,https://api.github.com/repos/kubernetes/kube-deploy/issues/122,How does docker-multinode differs from 'production' k8s clusters?,"Hi,

We're evaluating k8s internally and, to do so, I've brought up a 2-machines local cluster with kube-deploy docker-multinode scripts.

I've read some k8s developers saying that this deployment option is different (less reliable?) from standard 'production-ready' clusters deployed using (I think) kube-up clusters/ scripts.

Is it so? If yes, what are the main differences among the two deployment options? Is HA covered in kube-up?

Thank you!
",closed,False,2016-06-20 10:46:59,2017-11-01 22:50:43
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/issues/123,https://api.github.com/repos/kubernetes/kube-deploy/issues/123,sharing infrastruture between deployments,"There is a certain amount of tooling that will benefit all deployment automations. We should discuss how to develop this tools in a way that they benefit as many of the maintained deployments as possible. Some work that I can think of that would benefit all deployments:
- Config revamp. It's hard to configure kubernetes components.
  - Finish componentconfig https://github.com/kubernetes/kubernetes/issues/12245
  - Move as much config as possible into the master https://github.com/kubernetes/kubernetes/issues/1627
  - Create a cluster config for cluster wide configuration prameters https://github.com/kubernetes/kubernetes/issues/19831
  - Kubelet dynamic config https://github.com/kubernetes/kubernetes/issues/27980
-  Own the installation. Get kubelet and it's dependencies installed.
  - Standardize on a node container image #34
  - (or) build packagemanager packages (rpms and debs)
  - commit to single binary
- Make pod network easy to deploy. It's hard to setup the pod network.
  - Allow CNI node agents to run in DaemonSets on the host and configure the kubelet
- Revamp the addon manager. There is no standard way to deploy addons.
  - Move addons out of kubernetes core repo
  - Get rid of the bash addon manager, possibly replace with helm
  - Implement kubectl apply --prune https://github.com/kubernetes/kubernetes/issues/19805
- Initial client bootstrap. It's hard to setup secure communication between k8s components.
  - Discovery API https://github.com/kubernetes/kubernetes/issues/5754
  - TLS Bootstrap https://github.com/kubernetes/kubernetes/issues/18112
- Self hosting. Make it easy to run kubernetes on kubernetes.
  - Daemonset upgrades https://github.com/kubernetes/kubernetes/issues/22543#issuecomment-215833295
  - Easier self hosted control plane bootstrapping
  - Kubelet checkpointing https://github.com/kubernetes/kubernetes/issues/489
  - Kubeclient should follow apiservers and do failover
- Kubeconfig v2

Obviously, some of these have higher relative priority than others. Let's use this issue to track the deployment shared infrastructure effort. Let me know if there are items on this list that are missing.

cc @luxas @errordeveloper @justinsb 
",closed,False,2016-06-20 18:32:36,2017-11-02 16:54:23
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/124,https://api.github.com/repos/kubernetes/kube-deploy/issues/124,upup: support IP based firewalling of SSH / HTTPS ports,"cf https://github.com/kubernetes/kubernetes/pull/27061
",closed,False,2016-06-20 21:51:40,2017-11-01 21:30:59
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/125,https://api.github.com/repos/kubernetes/kube-deploy/issues/125,"Updates for 1.3: Docker 1.11.2, 1.3 image","1.3 is coming soon - we're going to target upup to it!
",closed,True,2016-06-23 13:00:29,2016-06-23 14:26:35
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/126,https://api.github.com/repos/kubernetes/kube-deploy/issues/126,upup: use vfs for secretstore/keystore,"This is needed so that we can have encrypted storage and complex keys
(e.g. multiple CA certs).  Multiple CA certs are needed for an in-place
upgrade from kube-up v1.
",closed,True,2016-06-23 13:00:33,2016-06-23 14:26:42
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/127,https://api.github.com/repos/kubernetes/kube-deploy/issues/127,upup: improved delete; create upup export and upup upgrade,"This is the beginning of the tooling we need for in-place upgrade!
",closed,True,2016-06-23 13:00:55,2016-06-23 14:27:32
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/128,https://api.github.com/repos/kubernetes/kube-deploy/issues/128,protokube: synchronize changes automatically from statestore,"upup includes a state store, currently S3 backed, but we can easily add GCS.  We could put e.g. addons into it, so that we can dynamically reconfigure them without any SSH tricks.

This could also be used to dynamically populate the initial manifests (e.g. apiserver, kcm, scheduler etc).  Maybe we could also include kubelet, in which case nodeup becomes even smaller.
",closed,False,2016-06-23 14:24:57,2017-11-01 21:30:59
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/129,https://api.github.com/repos/kubernetes/kube-deploy/issues/129,upup: Mark GCE as probably-not-working,"I'm about to do a demo of this, and GCE is lagging, so I'd like to warn users early that GCE may not be in-sync with the latest
",closed,True,2016-06-23 14:29:54,2016-06-23 14:30:02
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/130,https://api.github.com/repos/kubernetes/kube-deploy/issues/130,"upup: Add VFS context object, centralize usage","Replaces ad-hoc construction of VFS paths
",closed,True,2016-06-23 14:30:06,2016-06-23 14:30:09
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/131,https://api.github.com/repos/kubernetes/kube-deploy/issues/131,Rename README -> README.md,"So github shows markdown
",closed,True,2016-06-23 14:32:16,2016-06-23 14:32:20
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/132,https://api.github.com/repos/kubernetes/kube-deploy/issues/132,upup:  Verify terraform output is git-stable,"We need to check that the terraform output does not change spuriously between generations.

For example, that the blocks are always output in a consistent order.
",closed,False,2016-06-23 14:33:49,2016-06-27 04:38:03
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/133,https://api.github.com/repos/kubernetes/kube-deploy/issues/133,"upup: Create tool to export the statestore, for adding to git","Users like to store their configuration in git.  We should create a command to export a copy of the state store.
",closed,False,2016-06-23 14:46:44,2017-11-01 21:30:59
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/134,https://api.github.com/repos/kubernetes/kube-deploy/issues/134,upup: create DNS hosted zone in 'proto' phase?,"This would likely make for a better terraform experience, because the lifespan of a DNS zone is likely longer than a particular cluster (not least because users need to reconfigure their DNS hosts)

Or we could maybe mark it as ignored by terraform.
",closed,False,2016-06-23 14:51:19,2017-11-01 21:30:59
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/135,https://api.github.com/repos/kubernetes/kube-deploy/issues/135,upup: verify multizone is enabled on GCE,"We've had some problems with it in kube-up, and it seems likely that we aren't setting multizone everywhere for GCE also.
",closed,False,2016-06-23 14:52:23,2017-11-01 21:30:59
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/136,https://api.github.com/repos/kubernetes/kube-deploy/issues/136,upup: error in kube-scheduler log,"```
W0611 00:02:27.020702       7 client_config.go:355] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
W0611 00:02:27.020788       7 client_config.go:360] error creating inClusterConfig, falling back to default config: unable to load in-cluster configuration, KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined
```
",closed,False,2016-06-23 14:53:19,2017-11-01 21:30:59
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/137,https://api.github.com/repos/kubernetes/kube-deploy/issues/137,upup: review generated names for sanity,"In particular:
- Does the extra kubernetes prefix add anything?
- Do we want to set up names to allow multiple nodesets?
",closed,False,2016-06-23 14:54:13,2017-11-01 21:30:59
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/138,https://api.github.com/repos/kubernetes/kube-deploy/issues/138,upup: support multiple nodesets,"We should support multiple nodesets in upup, at least in the configuration schema.
",closed,False,2016-06-23 14:54:37,2016-06-27 04:41:53
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/139,https://api.github.com/repos/kubernetes/kube-deploy/issues/139,upup: prune configuration schema,"It is easy to add things later; hard to remove them.

If we aren't actively using something, remove it / comment it out from the configuration schema.
",closed,False,2016-06-23 14:55:10,2017-11-01 21:31:00
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/140,https://api.github.com/repos/kubernetes/kube-deploy/issues/140,upup AWS: can we get instance names in AWS to match node names in k8s?,"It would be nice if the instance name in the AWS console matched the node name in k8s
",closed,False,2016-06-23 14:57:09,2017-11-01 21:31:00
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/141,https://api.github.com/repos/kubernetes/kube-deploy/issues/141,upup: Add GCS support to vfs/statestore,"This will then let us store the state store on GCS buckets
",closed,False,2016-06-23 14:59:35,2017-11-01 21:31:00
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/142,https://api.github.com/repos/kubernetes/kube-deploy/issues/142,upup: re-enable GCE,"We added a fail-fast for GCE, because upup support is lagging behind AWS

We should get GCE back up to par, and then remove the fail-fast
",closed,False,2016-06-23 15:00:43,2017-11-01 21:31:00
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/143,https://api.github.com/repos/kubernetes/kube-deploy/issues/143,upup: encrypt secrets,"Support encryption for keystore keys and secretstore secrets.

Unclear whether this should be at the VFS level or not.
",closed,False,2016-06-23 15:01:39,2017-11-01 21:31:00
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/144,https://api.github.com/repos/kubernetes/kube-deploy/issues/144,upup: easy editing of config file,"We should create a command that allows easy editing of the statestore config file, like kubectl edit.
",closed,False,2016-06-23 15:02:47,2017-11-01 21:31:00
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/145,https://api.github.com/repos/kubernetes/kube-deploy/issues/145,upup: validate & document upgrade procedure,"We have an early upgrade procedure.  We should validate it for 1.3, including:
- pods with mounted volumes
- services with load balancers

And then document the final procedure.
",closed,False,2016-06-23 15:03:50,2017-11-01 21:31:00
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/146,https://api.github.com/repos/kubernetes/kube-deploy/issues/146,upup: fix tagging edge cases,"There are some edge cases where items in AWS won't be tagged, primarily when there is no change other than tags.

We should split out tagging, and also check if we can somehow avoid failures when we exit in between resource creation & tagging
",closed,False,2016-06-23 15:10:19,2017-11-01 21:31:00
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/147,https://api.github.com/repos/kubernetes/kube-deploy/issues/147,upup: decide if we want to use ELB on single-master & on multi-master,"Should we use ELB for single-master and multi-master configurations?

The DNS approach apparently has problems when not all nodes are available.
",closed,False,2016-06-23 15:12:09,2017-11-01 21:31:00
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/148,https://api.github.com/repos/kubernetes/kube-deploy/issues/148,upup: add docs for statestore concept,,closed,True,2016-06-23 16:06:25,2016-06-23 16:06:29
kube-deploy,philips,https://github.com/kubernetes/kube-deploy/issues/149,https://api.github.com/repos/kubernetes/kube-deploy/issues/149,protokube: what is it?,"I can't find any docs or anything?
",closed,False,2016-06-23 17:53:04,2017-11-01 22:51:07
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/150,https://api.github.com/repos/kubernetes/kube-deploy/issues/150,delete min-turnup from kube-deploy,"We are moving to https://github.com/kubernetes/kubernetes-anywhere

cc @errordeveloper @colemickens @bgrant0607 
",closed,True,2016-06-23 17:59:28,2016-06-24 04:19:56
kube-deploy,mikedanese,https://github.com/kubernetes/kube-deploy/pull/151,https://api.github.com/repos/kubernetes/kube-deploy/issues/151,add Makefile to demonstrate how certs should be generated for kubernetes,"I think makefiles are good at explaning dependencies.

cc @justinsb @roberthbailey @aaronlevy
",closed,True,2016-06-23 18:00:19,2016-10-18 22:29:35
kube-deploy,colhom,https://github.com/kubernetes/kube-deploy/issues/152,https://api.github.com/repos/kubernetes/kube-deploy/issues/152,Contribute kube-aws tool to kube-deploy repository,"[Kube-aws](https://github.com/coreos/coreos-kubernetes/tree/master/multi-node/aws) is what we use at CoreOS to deploy production Kubernetes clusters on AWS. It offers a number of features that make it well-suited for a production environment:
- reproducible, version-controllable deployment artifacts
- end-to-end encryption of TLS assets
- compresses and inlines all build artifacts- no reliance on hosting artifacts in public S3 buckets
- entire deployment is a cloudformation stack, integrates well with new cloudformation update features
  - building on this, we're working towards in-place cluster upgrades

We'd like to start the discussion on moving development of this tool over to the kube-deploy repository.

\cc @bgrant0607 @philips @justinsb  
",closed,False,2016-06-23 19:32:07,2017-11-01 22:57:40
kube-deploy,bbreslauer,https://github.com/kubernetes/kube-deploy/issues/153,https://api.github.com/repos/kubernetes/kube-deploy/issues/153,"On Ubuntu, Docker config file should use DOCKER_OPTS, not OPTIONS","In docker-multinode/common.sh, the docker config file (/etc/default/docker) is modified and an OPTIONS line is added when being configured on Ubuntu:

kube::helpers::file_replace_line ${DOCKER_CONF} `# Replace content in this file` \
          ""--bip"" `# Find a line with this content...` \
          ""OPTIONS=\""\$OPTIONS --mtu=${FLANNEL_MTU} --bip=${FLANNEL_SUBNET}\"""" `# ...and replace the found line with this line`

I'm using Ubuntu Trusty (which uses Upstart, not Systemd), and this is incorrect. The correct variable name is DOCKER_OPTS, not OPTIONS. I've confirmed this in both the docker.io package provided by Trusty, as well as the docker-engine package provided by apt.dockerproject.org (which is needed for docker-multinode, since Trusty only provides v1.6).
",closed,False,2016-06-24 05:19:18,2016-07-11 20:41:02
kube-deploy,bbreslauer,https://github.com/kubernetes/kube-deploy/issues/154,https://api.github.com/repos/kubernetes/kube-deploy/issues/154,file_replace_line function appends line instead of replacing,"I'm running the docker-multinode scripts on Ubuntu Trusty, and the part that edits the /etc/default/docker file always appends a line instead of replacing an existing line. When running master.sh, the following error is outputted, suggesting that the ""--bip"" is being interpreted as an option and not a pattern.

grep: unrecognized option '--bip'
Usage: grep [OPTION]... PATTERN [FILE]...
Try 'grep --help' for more information.

The dashes in the following line should probably be escaped, e.g.

 ""--bip"" `# Find a line with this content...` \

should be (with the dashes single-escaped)

 ""\-\-bip"" `# Find a line with this content...` \
",closed,False,2016-06-24 06:13:47,2016-07-11 20:40:28
kube-deploy,bbreslauer,https://github.com/kubernetes/kube-deploy/issues/155,https://api.github.com/repos/kubernetes/kube-deploy/issues/155,Add init script to OS when installing with docker-multinode,"It would be great to have an init script added to the OS (for example, as an upstart or systemd script) when running master.sh or worker.sh, so that when a host is rebooted, the appropriate stuff is started up automatically.
",closed,False,2016-06-24 06:28:25,2017-11-01 22:51:28
kube-deploy,slack,https://github.com/kubernetes/kube-deploy/pull/156,https://api.github.com/repos/kubernetes/kube-deploy/issues/156,upup/protokube: tell protokube to use --dns-zone-name,"Poking around the project for the first time, looking good!

My test domain was `whatever.aws.slack.io` with the zone `aws.slack.io` hosted
on Route53. With this configuration protokube kept crashing with nil pointer
because it was detecting the wrong zone rather than using the configured zone
from cloudup:

```
${GOPATH}/bin/cloudup --state=s3://busket/hatever --name=${MYZONE} \
    --zones=us-west-2a --cloud=aws --dns-zone aws.slack.io --v=8 --logtostderr

Jun 26 04:15:26 ip-172-20-35-205 docker[4492]: I0626 04:15:26.539800       1 aws_dns.go:33] AWS API Request: route53/ListHostedZonesByName
Jun 26 04:15:26 ip-172-20-35-205 docker[4492]: panic: runtime error: invalid memory address or nil pointer dereference
Jun 26 04:15:26 ip-172-20-35-205 docker[4492]: [signal 0xb code=0x1 addr=0x10 pc=0x4868a0]
Jun 26 04:15:26 ip-172-20-35-205 docker[4492]: goroutine 1 [running]:
Jun 26 04:15:26 ip-172-20-35-205 docker[4492]: panic(0xa09bc0, 0xc820012070)
Jun 26 04:15:26 ip-172-20-35-205 docker[4492]: /usr/local/go/src/runtime/panic.go:481 +0x3e6
Jun 26 04:15:26 ip-172-20-35-205 docker[4492]: k8s.io/kube-deploy/protokube/pkg/protokube.(*Route53DNSProvider).Set(0xc8203d8200, 0xc8203a08a0, 0x29, 0xb0b6b0, 0x1, 0xc8203b0280, 0xd, 0xdf8475800, 0x0, 0x0)
Jun 26 04:15:26 ip-172-20-35-205 docker[4492]: /go/src/k8s.io/kube-deploy/protokube/pkg/protokube/aws_dns.go:103 +0x4d0
Jun 26 04:15:26 ip-172-20-35-205 docker[4492]: k8s.io/kube-deploy/protokube/pkg/protokube.(*KubeBoot).MapInternalDNSName(0xc820197ea0, 0xc8203a08a0, 0x29, 0x0, 0x0)
```

Smooth sailing after adding `--dons-zone-name` to `/etc/sysconfig/protokube`!

I also discovered a small bug when trying to use a custom nodeup location.
`nodeup.sh` wasn't using the download URL passed in via `NODEUP_TAR_URL`.

I'd be happy to split these into two PRs if you'd rather one over the other!
",closed,True,2016-06-26 15:55:23,2016-06-27 04:41:12
kube-deploy,zapman449,https://github.com/kubernetes/kube-deploy/issues/157,https://api.github.com/repos/kubernetes/kube-deploy/issues/157,upup and support for private subnets,"upup needs the capability to create subnets which are both public and private.  This should do three things:

1) Create an ASG spread across the private subnets
2) Create an ASG on the public subnets (with EIPs ideally)
3) Label the instances distinctly between the ASGs (external / internal or similar)
",closed,False,2016-06-26 23:16:11,2016-07-27 15:41:49
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/158,https://api.github.com/repos/kubernetes/kube-deploy/issues/158,upup: refactor in preparation for first stable release,,closed,True,2016-06-27 04:37:54,2016-06-27 04:38:02
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/159,https://api.github.com/repos/kubernetes/kube-deploy/issues/159,upup: yet more polish,"Final (?) preparations for 1.3
",closed,True,2016-06-27 20:25:22,2016-06-27 20:27:01
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/160,https://api.github.com/repos/kubernetes/kube-deploy/issues/160,upup: More glide deps,,closed,True,2016-06-27 20:36:39,2016-06-27 20:36:43
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/161,https://api.github.com/repos/kubernetes/kube-deploy/issues/161,upup: fix terraform docs,,closed,True,2016-06-27 21:09:29,2016-06-27 21:30:32
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/162,https://api.github.com/repos/kubernetes/kube-deploy/issues/162,upup: update same vpc docs,,closed,True,2016-06-28 00:23:58,2016-06-28 00:24:02
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/163,https://api.github.com/repos/kubernetes/kube-deploy/issues/163,upup: upgrade support,,closed,True,2016-06-28 15:51:29,2016-06-28 15:51:35
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/164,https://api.github.com/repos/kubernetes/kube-deploy/issues/164,Make docker multinode ready for the v1.3 release and update docs,"@cheld @zreigz @mikedanese 

Now we should hopefully have a decent, stable and multiarch solution for v1.3 with at least some documentation.

I'm going to remove all scripts from kubernetes.github.io and replace the readme with this one with a link to kube-deploy.

Please LGTM as fast as possible, v1.3 is soon released.
",closed,True,2016-06-28 17:17:09,2016-07-03 09:18:23
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/165,https://api.github.com/repos/kubernetes/kube-deploy/issues/165,upup: final polish for 1.3 (?),,closed,True,2016-06-28 17:19:14,2016-06-28 17:19:17
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/166,https://api.github.com/repos/kubernetes/kube-deploy/issues/166,Move upup & protokube to kops repo,"Our little tool is all grown up!
",closed,True,2016-06-30 14:16:37,2016-06-30 14:17:36
kube-deploy,virtuald,https://github.com/kubernetes/kube-deploy/issues/167,https://api.github.com/repos/kubernetes/kube-deploy/issues/167,docker-multinode offline support,"I'm working on modifying the scripts to make this possible, if anyone else is interested post here and I can push my modifications up. Once I get it working, I'll do a PR. 

At the moment, my approach is to provide an `offline.sh` script that can download all the images from the internet, and optionally push them to a private registry, and then also pull them from a private registry and retag them as appropriate (due to https://github.com/kubernetes/kubernetes/issues/28189). The chief difficulty is enumerating all of the images required -- hyperkube downloads a variety of images when it starts. I'm hoping I can enumerate them all in a low-maintenance way -- but it still needs some work.
",closed,False,2016-06-30 16:58:57,2017-11-01 22:51:32
kube-deploy,virtuald,https://github.com/kubernetes/kube-deploy/pull/168,https://api.github.com/repos/kubernetes/kube-deploy/issues/168,Allow docker-multinode to be used offline,"It's possible that there are better ways to go about this (particularly related to how this figures out what images to download -- that's a nasty one liner) -- but I've tested it successfully using Kubernetes 1.3.0 (and by tested, I mean I'm able to start master + 2 workers and the k8s UI seems to work and various commands... still new to Kubernetes so I haven't deployed any apps yet).
",closed,True,2016-07-04 02:47:13,2017-10-21 04:14:53
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/169,https://api.github.com/repos/kubernetes/kube-deploy/issues/169,imagebuilder: add glide dependencies and vendor,,closed,True,2016-07-04 20:07:43,2016-07-04 20:08:04
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/170,https://api.github.com/repos/kubernetes/kube-deploy/issues/170,Fix imagebuilder readme: go install -> go get,,closed,True,2016-07-04 20:13:06,2016-07-04 20:13:18
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/171,https://api.github.com/repos/kubernetes/kube-deploy/issues/171,imagebuilder: assumes we are running in imagebuilder dir,"Probably need to add this to README:

`cd`${GOPATH}/src/k8s.io/kube-deploy/imagebuilder`
",closed,False,2016-07-04 20:31:30,2016-09-17 18:38:49
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/172,https://api.github.com/repos/kubernetes/kube-deploy/issues/172,Fix changing mtu and bip in DOCKER OPTIONS file,"Modifying  bip and mtu in docker OPTIONS file causes problem when master/worker script is run again. The kube::helpers::replace_mtu_bip() function truncates last punctuation mark and docker crashes after restart. This PR fix this issue.
",closed,True,2016-07-05 11:48:37,2016-07-12 07:27:44
kube-deploy,colachg,https://github.com/kubernetes/kube-deploy/issues/173,https://api.github.com/repos/kubernetes/kube-deploy/issues/173,docker-multinode: etcd failed to start. Exiting...,"Here is my problem:
**First,I ran ./master.sh**

``` shellscript
developer@ubuntu:~/kube-deploy/docker-multinode$ sudo ./master.sh
+++ [0705 22:44:47] K8S_VERSION is set to: v1.2.5
+++ [0705 22:44:47] ETCD_VERSION is set to: 2.2.5
+++ [0705 22:44:47] FLANNEL_VERSION is set to: 0.5.5
+++ [0705 22:44:47] FLANNEL_IPMASQ is set to: true
+++ [0705 22:44:47] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [0705 22:44:47] FLANNEL_BACKEND is set to: udp
+++ [0705 22:44:47] RESTART_POLICY is set to: always
+++ [0705 22:44:47] MASTER_IP is set to: 192.168.3.71
+++ [0705 22:44:47] ARCH is set to: amd64
+++ [0705 22:44:47] NET_INTERFACE is set to: eth0
+++ [0705 22:44:47] --------------------------------------------
+++ [0705 22:44:47] Detected OS: ubuntu
+++ [0705 22:44:48] Launching docker bootstrap...
+++ [0705 22:44:49] Launching etcd...
7942e23ab66d7262315214d0d8d6a108448026d07005cd60b925e5f195e9c01b
Error:  client: etcd cluster is unavailable or misconfigured
error #0: client: endpoint http://192.168.3.71:4001 exceeded header timeout

+++ [0705 22:44:52] Launching flannel...
5f749de9e92de8737da521fb5a6ad308d72fa74f1843082da188788f46b17e6d
!!! [0705 22:45:11] flannel failed to start. Exiting...
```

**Then, I ran it again:**

``` shellscript
developer@ubuntu:~/kube-deploy/docker-multinode$ sudo ./master.sh 
+++ [0705 22:45:22] K8S_VERSION is set to: v1.2.5
+++ [0705 22:45:22] ETCD_VERSION is set to: 2.2.5
+++ [0705 22:45:22] FLANNEL_VERSION is set to: 0.5.5
+++ [0705 22:45:22] FLANNEL_IPMASQ is set to: true
+++ [0705 22:45:22] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [0705 22:45:22] FLANNEL_BACKEND is set to: udp
+++ [0705 22:45:22] RESTART_POLICY is set to: always
+++ [0705 22:45:22] MASTER_IP is set to: 192.168.3.71
+++ [0705 22:45:22] ARCH is set to: amd64
+++ [0705 22:45:22] NET_INTERFACE is set to: eth0
+++ [0705 22:45:22] --------------------------------------------
+++ [0705 22:45:22] Detected OS: ubuntu
+++ [0705 22:45:22] Killing docker bootstrap...
5f749de9e92d
7942e23ab66d
bfd93ac77d45
2fdfe3992a22
+++ [0705 22:45:22] Launching docker bootstrap...
!!! [0705 22:45:42] docker bootstrap failed to start. Exiting...
```

**The third time:**

``` shellscript
developer@ubuntu:~/kube-deploy/docker-multinode$ sudo ./master.sh 
+++ [0705 22:45:51] K8S_VERSION is set to: v1.2.5
+++ [0705 22:45:51] ETCD_VERSION is set to: 2.2.5
+++ [0705 22:45:51] FLANNEL_VERSION is set to: 0.5.5
+++ [0705 22:45:51] FLANNEL_IPMASQ is set to: true
+++ [0705 22:45:51] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [0705 22:45:51] FLANNEL_BACKEND is set to: udp
+++ [0705 22:45:51] RESTART_POLICY is set to: always
+++ [0705 22:45:51] MASTER_IP is set to: 192.168.3.71
+++ [0705 22:45:51] ARCH is set to: amd64
+++ [0705 22:45:51] NET_INTERFACE is set to: eth0
+++ [0705 22:45:51] --------------------------------------------
+++ [0705 22:45:51] Detected OS: ubuntu
+++ [0705 22:45:51] Launching docker bootstrap...
+++ [0705 22:45:53] Launching etcd...
5751ce8bcd6c334d3411daf6df719c6defc3bee84b7d3731c8d9515ff108f967
etcd cluster has no published client endpoints.
Try '--no-sync' if you want to access non-published client endpoints(http://127.0.0.1:4001,http://127.0.0.1:2379).
Error:  client: no endpoints available
+++ [0705 22:45:55] Launching flannel...
ad07b4104382001b057d9f440f6b7933b3b8f38bd0ba1b68b5290304a330f951
!!! [0705 22:46:14] flannel failed to start. Exiting...
```

**after add  '--no-sync'  in common.sh (#start etcd container)**

``` shellscript
developer@ubuntu:~/kube-deploy/docker-multinode$ sudo ./master.sh 
+++ [0705 22:48:12] K8S_VERSION is set to: v1.2.5
+++ [0705 22:48:12] ETCD_VERSION is set to: 2.2.5
+++ [0705 22:48:12] FLANNEL_VERSION is set to: 0.5.5
+++ [0705 22:48:12] FLANNEL_IPMASQ is set to: true
+++ [0705 22:48:12] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [0705 22:48:12] FLANNEL_BACKEND is set to: udp
+++ [0705 22:48:12] RESTART_POLICY is set to: always
+++ [0705 22:48:12] MASTER_IP is set to: 192.168.3.71
+++ [0705 22:48:12] ARCH is set to: amd64
+++ [0705 22:48:12] NET_INTERFACE is set to: eth0
+++ [0705 22:48:12] --------------------------------------------
+++ [0705 22:48:12] Detected OS: ubuntu
+++ [0705 22:48:12] Launching docker bootstrap...
+++ [0705 22:48:13] Launching etcd...
504f930d72bea71c10de695d08b7cb75233ed49397a2a301b5526b294493065d
!!! [0705 22:48:33] etcd failed to start. Exiting...
```

How to debug where etcd failed? My OS is ubuntu 14.04.4. docker version is 1.11.2.
",closed,False,2016-07-05 14:57:13,2016-08-11 08:22:39
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/174,https://api.github.com/repos/kubernetes/kube-deploy/issues/174,imagebuilder: does not work in cn-north-1,"Imagebuilder currently assumes we build the image in us-east-1, and then copies it to other regions.

We should both remove that assumption (e.g. when replicate=false) and upload the fixes for running in cn-north-1 (I have them locally)
",closed,False,2016-07-05 17:00:22,2018-04-01 01:03:50
kube-deploy,larmog,https://github.com/kubernetes/kube-deploy/pull/175,https://api.github.com/repos/kubernetes/kube-deploy/issues/175,Replaces emptyDir in manifest-multi to use hostPath and introduces wo…,"…rk-directory. Currently apiserver uses a emptyDir to store generated certs. When the apiserver dies all certs are re-generated. That makes all pods that depends on apiserver fail (like dns). This PR copies manifests-multinode and changes emtyDir to hostPath so that generated files are reused. It also introduces work-dir to keep persistent data between restarts (etcd).
",closed,True,2016-07-11 09:15:10,2016-07-12 03:47:45
kube-deploy,taimir,https://github.com/kubernetes/kube-deploy/issues/176,https://api.github.com/repos/kubernetes/kube-deploy/issues/176,Flannel daemonset for docker-multinode,"With the introduction of https://github.com/kubernetes/kube-deploy/pull/115 to the `docker-multinode` project, it would be natural to follow up with a daemonset for `flannel`.

I've already created one and tested it in a small 3-node setup and it seems to work fine (addons start properly, I can deploy guestbook and there's connectivity between all pods). In the process I stumbled upon two issues:
- For the container command in the daemonset, one needs to specify `--etcd-endpoints`, which needs to be set to `MASTER_IP:4001`. What would be the best way to get this configuration into the daemon-set manifest / provide it to `flanneld`? Obviously it is not available during the `hyperkube` build.
- `flanneld` also has the `--iface` parameter. There is no guarantee that this will be the same (e.g. `eth0`) on all nodes. However, the `daemonset` definition remains global for the whole cluster. Is there an established way of providing node-specific configuration to a daemonset?

@luxas @cheld @zreigz 
",closed,False,2016-07-11 15:13:51,2017-11-01 22:51:35
kube-deploy,larmog,https://github.com/kubernetes/kube-deploy/pull/177,https://api.github.com/repos/kubernetes/kube-deploy/issues/177,Fixes that etcd data is not preserved on master in docker-multinode.,,closed,True,2016-07-12 03:44:46,2016-07-12 05:36:31
kube-deploy,ghost,https://github.com/kubernetes/kube-deploy/pull/178,https://api.github.com/repos/kubernetes/kube-deploy/issues/178,[docker-multinode] improve raspbian support,"Once lsb_release is installed the scripts don't detect raspbian (Hypriot 0.8.0) correctly.

With this change:
- raspsbian with lsb_release is handled as 'debian'
- service timeout can be overridden from an env variable (flannel took a while to start on my machines)
- if bootstrap/etcd/flanneld fail to start a non-zero return code is used (useful when calling the scripts from ansible to see that they actually failed)
",closed,True,2016-07-18 06:12:08,2016-07-23 08:59:36
kube-deploy,faxioman,https://github.com/kubernetes/kube-deploy/issues/179,https://api.github.com/repos/kubernetes/kube-deploy/issues/179,Unable to start a master node,"I'm trying many versions of docker on many os (Ubuntu, Debian, Centos).
I am having different errors for each combination.
Which precise version of os/docker setup you are using for your tests?
Thanks
",closed,False,2016-07-29 12:51:40,2016-08-16 15:40:23
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/180,https://api.github.com/repos/kubernetes/kube-deploy/issues/180,"Many improvements to docker-multinode, and some cleanup","Please review this quickly and let it pass through.

Mostly cleanup and refactoring
@zreigz @cheld
",closed,True,2016-07-29 13:05:53,2016-08-01 11:14:43
kube-deploy,NamPNQ,https://github.com/kubernetes/kube-deploy/issues/181,https://api.github.com/repos/kubernetes/kube-deploy/issues/181,docker-multinode kubectl?,"Where I can get kubectl in docker-multinode?
",closed,False,2016-07-30 11:42:01,2016-08-01 11:13:27
kube-deploy,eddytruyen,https://github.com/kubernetes/kube-deploy/issues/182,https://api.github.com/repos/kubernetes/kube-deploy/issues/182,docker-multinode: Running cloud-provider specific code,"I am facing an issue that I want to add specific options to the hyperkube command such as  --cloud-provider=openstack.  I modified the worker and master scripts then such that:
- --cloud-provider and --cloud-config options are added to the hyperkube  command
- I mount a host dir with the cloud_config file into the kubelet container (with the -v option) 

```
  <skip> common.sh 
 -v /home/ubuntu/containers_on_openstack/openstack-integration/hyperkube:/openstack \
         ${REGISTRY}/hyperkube:v${K8S_VERSION} \
        /hyperkube kubelet \
             ....  
             --cloud-provider=openstack \
            --cloud-config=/openstack/cloud_config \
             ....
            --v=2
```

However the cinder plugin of kubernetes (for attaching Openstack Cinder volumes) will try to attach a volume and then find the corresponding device. However the cinder plugin can't find this device by disk-id:

```
I0730 18:28:18.370114    2300 openstack.go:984] Successfully attached 06b25a7b-7c99-4eee-9ab0-98a6c78676a1 volume to 7f07a3d7-1fc9-487f-84b7-c044c1dd5382 compute
W0730 18:28:18.370511    2300 cinder_util.go:103] Failed to find device for the diskid: ""06b25a7b-7c99-4eee-9ab0-98a6c78676a1""
E0730 18:28:18.378081    2300 cinder_util.go:169] error running udevadm trigger fork/exec /usr/bin/udevadm: no such file or directory
```

The last log entry shows a failure to run `udevadm trigger fork/exec /usr/bin/udevadm`. I tried to run this command from within a bash session inside the kubelet container and then got as error:  `Extraneous argument: 'fork/exec'`

How can I solve this?

Thanks for any advice
",closed,False,2016-07-30 18:53:38,2017-11-01 22:51:39
kube-deploy,NamPNQ,https://github.com/kubernetes/kube-deploy/pull/183,https://api.github.com/repos/kubernetes/kube-deploy/issues/183,Update README.md,"Fix #181
",closed,True,2016-07-31 06:01:28,2016-08-01 11:13:28
kube-deploy,taimir,https://github.com/kubernetes/kube-deploy/pull/184,https://api.github.com/repos/kubernetes/kube-deploy/issues/184,[WIP] Flannel and kube-proxy daemonsets,"#### [WIP] Please do not merge yet

This builds upon https://github.com/kubernetes/kube-deploy/issues/176
Since CNI has been merged into `docker-multinode` with https://github.com/kubernetes/kube-deploy/pull/115, it would be great to have self hosted flannel and kube-proxy as daemon-sets. That is what this pull request is about.

Most of the work for this is in the hyperkube: see PR https://github.com/kubernetes/kubernetes/pull/29848, which goes hand in hand with this one.

The biggest issue is how to cleanly propagate configuration (such as the MASTER_IP) to the daemonsets.

I've tested this in a two node setup, all addons were running correctly and I managed to deploy the guestbook example without issues.

@luxas @cheld @zreigz 
",closed,True,2016-08-01 12:00:35,2017-10-21 04:15:13
kube-deploy,huangqingcheng,https://github.com/kubernetes/kube-deploy/issues/185,https://api.github.com/repos/kubernetes/kube-deploy/issues/185,docker-multinode: dashboard&kubedns cannot route to api server,"K8S_VERSION is set to: v1.3.4
ETCD_VERSION is set to: 2.2.5
FLANNEL_VERSION is set to: 0.5.5
FLANNEL_IPMASQ is set to: true
FLANNEL_NETWORK is set to: 10.1.0.0/16
FLANNEL_BACKEND is set to: udp
RESTART_POLICY is set to: unless-stopped
MASTER_IP is set to: 10.10.1.70

dashboard and kubedns cannot connect to api server with wrong server address.

```
Using https://10.0.0.1:443 for kubernetes master
Using kubernetes API <nil>
```
",closed,False,2016-08-02 02:05:12,2016-08-02 14:36:32
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/186,https://api.github.com/repos/kubernetes/kube-deploy/issues/186,Remove OS distro dependencies,,closed,True,2016-08-02 09:30:40,2016-08-02 11:33:27
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/187,https://api.github.com/repos/kubernetes/kube-deploy/issues/187,Bugfixes for docker-multinode,"@cheld @zreigz

Trivial bugfixes, just comment LGTM and I'll merge
",closed,True,2016-08-02 09:46:04,2016-08-02 10:37:09
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/issues/188,https://api.github.com/repos/kubernetes/kube-deploy/issues/188,e2e testing of docker-multinode,"@zreigz Can you add a Vagrantfile for a two-machine cluster with Ubuntu 16.04 that runs conformance test?

Last time I ran conformance tests, 92 of 96 passed :)
We should make it automatic
",closed,False,2016-08-02 12:13:37,2017-11-01 22:51:44
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/189,https://api.github.com/repos/kubernetes/kube-deploy/issues/189,Add Vagrant enviroment to execute e2e tests,"The purpose of this PR is to easily set up a VMs with a running Kubernetes cluster and test machine to run e2e conformance tests.

Start master and node to create k8s cluster

```
vagrant up master
vagrant up node
```

and then start test VM

```
vagrant up tests
```

When it is up and running get e2e tests result

```
vagrant ssh -c 'tail -f /home/vagrant/e2e.txt' tests
```

The final test result for official release v1.3.4:

```
Summarizing 12 Failures:

[Fail] [k8s.io] ResourceQuota [It] should create a ResourceQuota and capture the life of a pod. 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/resource_quota.go:140

[Fail] [k8s.io] ResourceQuota [It] should create a ResourceQuota and capture the life of a service. 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/resource_quota.go:61

[Fail] [k8s.io] Networking [It] should function for intra-pod communication [Conformance] 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/networking.go:154

[Fail] [k8s.io] Kubectl client [k8s.io] Simple pod [It] should support exec through an HTTP proxy 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:346

[Fail] [k8s.io] ResourceQuota [It] should create a ResourceQuota and capture the life of a persistent volume claim. 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/resource_quota.go:275

[Fail] [k8s.io] ResourceQuota [It] should create a ResourceQuota and ensure its status is promptly calculated. 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/resource_quota.go:47

[Fail] [k8s.io] ResourceQuota [It] should create a ResourceQuota and capture the life of a configMap. 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/resource_quota.go:205

[Fail] [k8s.io] ResourceQuota [It] should create a ResourceQuota and capture the life of a replication controller. 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/resource_quota.go:240

[Fail] [k8s.io] ResourceQuota [It] should create a ResourceQuota and capture the life of a secret. 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/resource_quota.go:103

[Fail] [k8s.io] Deployment [It] paused deployment should be able to scale 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/deployment.go:1012

[Fail] [k8s.io] [HPA] Horizontal pod autoscaling (scale resource: CPU) [k8s.io] ReplicationController light [It] Should scale from 1 pod to 2 pods 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/autoscaling_utils.go:284

[Fail] [k8s.io] [HPA] Horizontal pod autoscaling (scale resource: CPU) [k8s.io] ReplicationController light [It] Should scale from 2 pods to 1 pod 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/autoscaling_utils.go:284

Ran 185 of 340 Specs in 1812.087 seconds
FAIL! -- 173 Passed | 12 Failed | 0 Pending | 155 Skipped 

```

TODO 
Extract common variable to group_vars to be able setup K8S_VERSION and USE_CNI in one place because currently it uses LATEST_STABLE_K8S_VERSION and USE_CNI=false.
",closed,True,2016-08-04 09:32:29,2016-08-09 12:38:07
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/190,https://api.github.com/repos/kubernetes/kube-deploy/issues/190,Use the ip command instead of ifconfig and brctl,"@cheld @zreigz 
",closed,True,2016-08-04 10:09:04,2016-08-04 11:32:27
kube-deploy,fjzhuozl,https://github.com/kubernetes/kube-deploy/pull/191,https://api.github.com/repos/kubernetes/kube-deploy/issues/191,"etcd start success,curl return 2",,closed,True,2016-08-04 11:08:28,2016-08-09 04:37:41
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/issues/192,https://api.github.com/repos/kubernetes/kube-deploy/issues/192,docker-multinode: what next,"I would like open discussion about future plans for docker-multinode. I assume the CNI is still a hot topic. Regarding this some PRs are still waiting to be merged, so we have to wait.
Is there anything I can help with?
",closed,False,2016-08-08 11:58:11,2017-11-01 22:51:51
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/193,https://api.github.com/repos/kubernetes/kube-deploy/issues/193,Refactor/cleanup common.sh,"even more tuning/cleanup
- use curl for posting flannel data
- require ip and curl 
- remove NET_INTERFACE

@cheld @zreigz 
",closed,True,2016-08-08 14:21:36,2016-08-09 12:21:35
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/194,https://api.github.com/repos/kubernetes/kube-deploy/issues/194,Get rid off ansible from tests,"The goal for this PR is to simplify execution of e2e tests.
",closed,True,2016-08-11 12:40:44,2016-08-19 06:09:57
kube-deploy,mlbiam,https://github.com/kubernetes/kube-deploy/issues/195,https://api.github.com/repos/kubernetes/kube-deploy/issues/195,docker-multinode: how to access master and how to change api server config,"I have the master up and running, but I'm very confused about how to access it.  None of the containers are exposing ports and neither 8080 or 8443 is available.  Also, how can I change the API server config?  I want to get OpenIDConnect integration working.
",closed,False,2016-08-12 16:44:56,2017-01-16 03:10:44
kube-deploy,huangfushun,https://github.com/kubernetes/kube-deploy/issues/196,https://api.github.com/repos/kubernetes/kube-deploy/issues/196,docker-multinode:containerized is required for volume mount,"> I0815 03:39:58.374660   27689 glusterfs_util.go:37] glusterfs: failure, now attempting to read the gluster log for pod redis-8zme8
> E0815 03:39:58.374837   27689 nestedpendingoperations.go:233] Operation for ""\""kubernetes.io/glusterfs/59ec8069-6299-11e6-8f62-e28917ac5d46-redis1-storage-data\"" (\""59ec8069-6299-11e6-8f62-e28917ac5d46\"")"" failed. No retries permitted until 2016-08-15 03:41:58.374802526 +0000 UTC (durationBeforeRetry 2m0s). Error: MountVolume.SetUp failed for volume ""kubernetes.io/glusterfs/59ec8069-6299-11e6-8f62-e28917ac5d46-redis1-storage-data"" (spec.Name: ""redis1-storage-data"") pod ""59ec8069-6299-11e6-8f62-e28917ac5d46"" (UID: ""59ec8069-6299-11e6-8f62-e28917ac5d46"") with: glusterfs: mount failed: mount failed: exit status 32
> Mounting arguments: localhost:redis /var/lib/kubelet/pods/59ec8069-6299-11e6-8f62-e28917ac5d46/volumes/kubernetes.io~glusterfs/redis1-storage-data glusterfs [log-level=ERROR log-file=/var/lib/kubelet/plugins/kubernetes.io/glusterfs/redis1-storage-data/redis-8zme8-glusterfs.log]

when add --containerized on kubelet it solved.
",closed,False,2016-08-15 04:06:53,2017-11-01 22:51:55
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/197,https://api.github.com/repos/kubernetes/kube-deploy/issues/197,Bump version of etcd and flannel,"@cheld @zreigz
",closed,True,2016-08-16 20:08:37,2016-08-17 11:23:28
kube-deploy,corentin-larose,https://github.com/kubernetes/kube-deploy/issues/198,https://api.github.com/repos/kubernetes/kube-deploy/issues/198,Docker-multinode error while pulling not existing repo in master.sh,"Hello,

I just tried to run master.sh from Docker-multinode (452cf54), it seems that it tries pulling a repo that doesn't exist (at least publicly): `quay.io/coreos/flannel-amd64`

```
+++ [0817 10:03:09] Launching flannel...
{""action"":""set"",""node"":{""key"":""/coreos.com/network/config"",""value"":""{ \""Network\"": \""10.1.0.0/16\"", \""Backend\"": {\""Type\"": \""udp\""}}"",""modifiedIndex"":4,""createdIndex"":4}}
Unable to find image 'quay.io/coreos/flannel-amd64:0.6.0' locally
Pulling repository quay.io/coreos/flannel-amd64
docker: Error: Status 403 trying to pull repository coreos/flannel-amd64: ""{\""error\"": \""Permission Denied\""}"".
See 'docker run --help'.
!!! [0817 10:03:31] flannel failed to start. Exiting...
```

In `common.sh:43`:
Changing

```
FLANNEL_VERSION=${FLANNEL_VERSION:-""0.6.0""}
```

for

```
FLANNEL_VERSION=${FLANNEL_VERSION:-""v0.6.0""}
```

In `common.sh:152`:
changing

```
quay.io/coreos/flannel-${ARCH}:${FLANNEL_VERSION}
```

for

```
quay.io/coreos/flannel:${FLANNEL_VERSION}-${ARCH}
```

Didn't make a PR since naming convention changed in flannel repo, not sure it would work with all versions.

Still have an issue:

```
+++ [0817 10:38:36] Launching flannel...
{""action"":""set"",""node"":{""key"":""/coreos.com/network/config"",""value"":""{ \""Network\"": \""10.1.0.0/16\"", \""Backend\"": {\""Type\"": \""udp\""}}"",""modifiedIndex"":4,""createdIndex"":4}}
fa3f19cb4890220df87454b66737b1774cd57ecdb3c91d134f00c38774dfcd5a
!!! [0817 10:38:55] flannel failed to start. Exiting...
```

Hope my 2 cents could help.
",closed,False,2016-08-17 08:16:10,2016-08-19 16:41:53
kube-deploy,ivanilves,https://github.com/kubernetes/kube-deploy/pull/199,https://api.github.com/repos/kubernetes/kube-deploy/issues/199,Small enhancements for docker-multinode,"![](https://media.giphy.com/media/Xdd1r5uRyd9jG/giphy.gif)

Just two no-brainers here:
- do not trigger etcd Flannel-related keys update, if not deploying master node (makes no sense!).
- correct paths for `cd`, more ""copy-paste friendliness"". :wink: 
",closed,True,2016-08-17 18:32:04,2016-08-19 14:49:33
kube-deploy,daniel-yavorovich,https://github.com/kubernetes/kube-deploy/pull/200,https://api.github.com/repos/kubernetes/kube-deploy/issues/200,fix flannel image name and default version,"Image quay.io/coreos/flannel-${ARCH} and version 0.6.0 not found in registry:

docker: Error: Status 403 trying to pull repository coreos/flannel-amd64: ""{\""error\"": \""Permission Denied\""}"".

Correct image and tag: quay.io/coreos/flannel:v0.6.0
",closed,True,2016-08-18 08:26:44,2016-08-19 16:36:16
kube-deploy,sekka1,https://github.com/kubernetes/kube-deploy/pull/201,https://api.github.com/repos/kubernetes/kube-deploy/issues/201,docker-multinode - fixing flannel image used,"- The repo it is referring to no longer exist.  Changing the repo it moved to
- Moving the version back b/c it looks like the latest 0.6.0 version does not work as expected.  Version 0.5.0 works.
",closed,True,2016-08-18 17:59:30,2016-08-19 16:35:15
kube-deploy,sekka1,https://github.com/kubernetes/kube-deploy/pull/202,https://api.github.com/repos/kubernetes/kube-deploy/issues/202,changes to work on CoreOS,"- This adds functionality to support running on CoreOS
",closed,True,2016-08-18 18:06:17,2016-08-18 18:06:23
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/203,https://api.github.com/repos/kubernetes/kube-deploy/issues/203,"Revert ""Bump version of etcd and flannel""","Sorry for the mess everyone, seems like 
a) the image name was wrong (I recalled incorrectly) and 
b) the flannel team didn't build a working v0.6.0 image. We have to wait for newer versions.

Reverting to make it working again.
Reverts kubernetes/kube-deploy#197
",closed,True,2016-08-18 21:02:56,2016-08-18 21:03:09
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/204,https://api.github.com/repos/kubernetes/kube-deploy/issues/204,Bump version of flannel,,closed,True,2016-08-22 12:12:33,2016-08-23 07:25:19
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/issues/205,https://api.github.com/repos/kubernetes/kube-deploy/issues/205,Merge kube-deploy with the ubuntu deployment,"ref: https://github.com/kubernetes/kubernetes/pull/29180#issuecomment-241063229

We should merge the deployments step-by-step, and eventually end up with one or two.
Also in the future, we should merge kube-deploy with the new deployment process we're designing in @kubernetes/sig-cluster-lifecycle, but this is an intermediate step

@mikedanese @wangzhezhe
",closed,False,2016-08-23 07:07:00,2017-05-28 08:50:00
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/206,https://api.github.com/repos/kubernetes/kube-deploy/issues/206,Disable etcd3 on arm since it's segfaulting,"@zreigz @cheld

Should be a trivial PR, etcd3 isn't working on arm unfortunately :/
I have to debug it more
",closed,True,2016-08-24 06:07:33,2016-08-24 06:41:06
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/207,https://api.github.com/repos/kubernetes/kube-deploy/issues/207,Add libvirt provider support,"Flannel doesn't work well with Vagrant + Virtualbox. There is problem with services using https connection. This is the main reason of failing test:

```
[Fail] [k8s.io] Networking [It] should function for intra-pod communication [Conformance] 
/home/vagrant/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/networking.go:154
```

This PR adds support for libvirt provider.
",closed,True,2016-08-24 11:46:02,2016-08-24 13:55:15
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/208,https://api.github.com/repos/kubernetes/kube-deploy/issues/208,Run kube-proxy in a daemonset with version v1.4.0-alpha.3 and higher,"depends on: https://github.com/kubernetes/kubernetes/pull/30299

@zreigz @cheld @mikedanese
",closed,True,2016-08-24 13:53:58,2016-11-03 08:49:12
kube-deploy,bialad,https://github.com/kubernetes/kube-deploy/issues/209,https://api.github.com/repos/kubernetes/kube-deploy/issues/209,Updating kubernetes,"Hi

I've using the docker-multinode project to spin up a Kubernetes cluster using three Raspberry Pi, and everything seems to be working as it should. And I think I have a good grasp on how the pieces of the installation fit together.

I'm looking into setting up a production-grade Kubernetes cluster using a mixed architecture setup in the long run, and also being able to contribute more once I have it up and running with my applications.

I just don't understand how I'm supposed to managed the cluster after installation, if this project is supposed to be for more than development that is. Is this a ""seft-hosted"" implementation, like the coreos solution, and can I handle the version of the part like so?

I'm sorry if I've misunderstood something basic, I'm rather new to Kubernetes even though I have some experience with docker.

Thank you
",closed,False,2016-08-24 21:12:12,2016-09-06 08:48:47
kube-deploy,sekka1,https://github.com/kubernetes/kube-deploy/issues/210,https://api.github.com/repos/kubernetes/kube-deploy/issues/210,docker-multinode: rebooting a node,"When i reboot the master node the next time the cluster comes up there are problems with it

Looks like the DNS pod cant talk to the kube api b/c of the certs

```
E0824 20:10:35.261649       1 reflector.go:216] pkg/dns/dns.go:155: Failed to list *api.Service: the server has asked for the client to provide credentials (get services)
E0824 20:10:35.360472       1 reflector.go:216] pkg/dns/dns.go:154: Failed to list *api.Endpoints: the server has asked for the client to provide credentials (get endpoints)
```

Is there a way to make this recover after a reboot?
",closed,False,2016-08-24 22:10:23,2016-09-10 06:14:03
kube-deploy,sekka1,https://github.com/kubernetes/kube-deploy/issues/211,https://api.github.com/repos/kubernetes/kube-deploy/issues/211,docker-multinode: Problems mounting EBS volumes,"Using Hyperkube v1.3.5 on CoreOS systems

https://github.com/kubernetes/kube-deploy/tree/master/docker-multinode

It seems to mount the EBS volume ok

controller

```
I0824 19:53:13.637396       1 nodecontroller.go:663] Recording Registered Node ip-172-16-61-10.ec2.internal in NodeController event message for node ip-172-16-61-10.ec2.internal
W0824 19:53:13.637418       1 nodecontroller.go:730] Missing timestamp for Node ip-172-16-61-10.ec2.internal. Assuming now as a timestamp.
I0824 19:53:13.637429       1 nodecontroller.go:613] NodeController exited network segmentation mode.
I0824 19:53:13.637569       1 event.go:216] Event(api.ObjectReference{Kind:""Node"", Namespace:"""", Name:""ip-172-16-61-10.ec2.internal"", UID:""ip-172-16-61-10.ec2.internal"", APIVersion:"""", ResourceVersion:"""", FieldPath:""""}): type: 'Normal' reason: 'RegisteredNode' Node ip-172-16-61-10.ec2.internal event: Registered Node ip-172-16-61-10.ec2.internal in NodeController
I0824 19:54:27.514631       1 replication_controller.go:480] Too few ""kube-system""/""aws-ebs-claim"" replicas, need 1, creating 1
I0824 19:54:27.526837       1 event.go:216] Event(api.ObjectReference{Kind:""ReplicationController"", Namespace:""kube-system"", Name:""aws-ebs-claim"", UID:""90693f43-6a34-11e6-9d88-0e44810298d7"", APIVersion:""v1"", ResourceVersion:""85176"", FieldPath:""""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: aws-ebs-claim-p641q
I0824 19:54:27.635114       1 reconciler.go:170] Started AttachVolume for volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" to node ""ip-172-16-61-10.ec2.internal""
I0824 19:54:27.898936       1 aws.go:1140] Assigned mount device ba -> volume vol-08d26b0824db7bbd2
I0824 19:54:28.308413       1 aws.go:1385] AttachVolume request returned {
  AttachTime: 2016-08-24 19:54:28.262 +0000 UTC,
  Device: ""/dev/xvdba"",
  InstanceId: ""i-07cdd12c8176414ee"",
  State: ""attaching"",
  VolumeId: ""vol-08d26b0824db7bbd2""
}
I0824 19:54:28.403189       1 aws.go:1263] Waiting for volume state: actual=attaching, desired=attached
I0824 19:54:29.468796       1 aws.go:1263] Waiting for volume state: actual=attaching, desired=attached
I0824 19:54:30.530062       1 aws.go:1263] Waiting for volume state: actual=attaching, desired=attached
I0824 19:54:31.675299       1 aws.go:1158] Releasing mount device mapping: ba -> volume vol-08d26b0824db7bbd2
I0824 19:54:31.675326       1 operation_executor.go:474] AttachVolume.Attach succeeded for volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") from node ""ip-172-16-61-10.ec2.internal"".
```

But it doesnt seem to be available for use:

kubelet

```
2016-08-24T20:48:32.065178843Z I0824 20:48:32.065023   19269 aws.go:791] Could not determine public IP from AWS metadata.
2016-08-24T20:48:39.293862757Z I0824 20:48:39.293287   19269 kubelet.go:2536] SyncLoop (ADD, ""api""): ""aws-ebs-claim-k9rr1_kube-system(229cbdd1-6a3c-11e6-83c4-0e44810298d7)""
2016-08-24T20:48:39.446634367Z I0824 20:48:39.446553   19269 reconciler.go:180] VerifyControllerAttachedVolume operation started for volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"")
2016-08-24T20:48:39.446668086Z I0824 20:48:39.446589   19269 reconciler.go:180] VerifyControllerAttachedVolume operation started for volume ""kubernetes.io/secret/229cbdd1-6a3c-11e6-83c4-0e44810298d7-default-token-9mcoi"" (spec.Name: ""default-token-9mcoi"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"")
2016-08-24T20:48:39.446837443Z E0824 20:48:39.446761   19269 nestedpendingoperations.go:233] Operation for ""\""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2\"""" failed. No retries permitted until 2016-08-24 20:48:39.946736769 +0000 UTC (durationBeforeRetry 500ms). Error: Volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"") has not yet been added to the list of VolumesInUse in the node's volume status.
2016-08-24T20:48:39.546930350Z I0824 20:48:39.546828   19269 reconciler.go:254] MountVolume operation started for volume ""kubernetes.io/secret/229cbdd1-6a3c-11e6-83c4-0e44810298d7-default-token-9mcoi"" (spec.Name: ""default-token-9mcoi"") to pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7""). 
2016-08-24T20:48:39.552902040Z I0824 20:48:39.552860   19269 operation_executor.go:740] MountVolume.SetUp succeeded for volume ""kubernetes.io/secret/229cbdd1-6a3c-11e6-83c4-0e44810298d7-default-token-9mcoi"" (spec.Name: ""default-token-9mcoi"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"").
2016-08-24T20:48:39.947921997Z I0824 20:48:39.947829   19269 reconciler.go:180] VerifyControllerAttachedVolume operation started for volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"")
2016-08-24T20:48:39.948010083Z E0824 20:48:39.947915   19269 nestedpendingoperations.go:233] Operation for ""\""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2\"""" failed. No retries permitted until 2016-08-24 20:48:40.947896437 +0000 UTC (durationBeforeRetry 1s). Error: Volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"") has not yet been added to the list of VolumesInUse in the node's volume status.
2016-08-24T20:48:40.951618425Z I0824 20:48:40.951510   19269 reconciler.go:180] VerifyControllerAttachedVolume operation started for volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"")
2016-08-24T20:48:40.951707267Z E0824 20:48:40.951603   19269 nestedpendingoperations.go:233] Operation for ""\""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2\"""" failed. No retries permitted until 2016-08-24 20:48:42.951583685 +0000 UTC (durationBeforeRetry 2s). Error: Volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"") has not yet been added to the list of VolumesInUse in the node's volume status.
2016-08-24T20:48:42.118009314Z I0824 20:48:42.117955   19269 aws.go:791] Could not determine public IP from AWS metadata.
2016-08-24T20:48:42.956769927Z I0824 20:48:42.956646   19269 reconciler.go:180] VerifyControllerAttachedVolume operation started for volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"")
2016-08-24T20:48:42.972669414Z I0824 20:48:42.972622   19269 operation_executor.go:980] Controller successfully attached volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"") devicePath: ""/dev/xvdba""
2016-08-24T20:48:43.056967232Z I0824 20:48:43.056888   19269 reconciler.go:254] MountVolume operation started for volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") to pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7""). 
2016-08-24T20:48:43.057010572Z I0824 20:48:43.056941   19269 operation_executor.go:649] Entering MountVolume.WaitForAttach for volume ""kubernetes.io/aws-ebs/vol-08d26b0824db7bbd2"" (spec.Name: ""test-volume"") pod ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"" (UID: ""229cbdd1-6a3c-11e6-83c4-0e44810298d7"") DevicePath: ""/dev/xvdba""
2016-08-24T20:50:32.361848505Z I0824 20:50:32.361777   19269 aws.go:791] Could not determine public IP from AWS metadata.
2016-08-24T20:50:39.294048096Z E0824 20:50:39.293931   19269 kubelet.go:1932] Unable to mount volumes for pod ""aws-ebs-claim-k9rr1_kube-system(229cbdd1-6a3c-11e6-83c4-0e44810298d7)"": timeout expired waiting for volumes to attach/mount for pod ""aws-ebs-claim-k9rr1""/""kube-system"". list of unattached/unmounted volumes=[test-volume]; skipping pod
2016-08-24T20:50:39.294085688Z E0824 20:50:39.293992   19269 pod_workers.go:183] Error syncing pod 229cbdd1-6a3c-11e6-83c4-0e44810298d7, skipping: timeout expired waiting for volumes to attach/mount for pod ""aws-ebs-claim-k9rr1""/""kube-system"". list of unattached/unmounted volumes=[test-volume]
```

@justinsb Thanks for initially helping me out on the slack channel.  I dont know where else to look for more information on what is wrong here.
",closed,False,2016-08-24 22:12:36,2017-11-01 22:51:59
kube-deploy,zhjwpku,https://github.com/kubernetes/kube-deploy/issues/212,https://api.github.com/repos/kubernetes/kube-deploy/issues/212,worker.sh flannel failed to start,"I have run the master.sh on master host and seems it's ok, but when I try to adding a worker node,
I got:
[root@c3po docker-multinode]# export MASTER_IP=10.0.63.202
[root@c3po docker-multinode]# ./worker.sh 
+++ [0825 06:55:02] K8S_VERSION is set to: v1.3.5
+++ [0825 06:55:02] ETCD_VERSION is set to: 3.0.4
+++ [0825 06:55:02] FLANNEL_VERSION is set to: v0.6.1
+++ [0825 06:55:02] FLANNEL_IPMASQ is set to: true
+++ [0825 06:55:02] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [0825 06:55:02] FLANNEL_BACKEND is set to: udp
+++ [0825 06:55:02] RESTART_POLICY is set to: unless-stopped
+++ [0825 06:55:02] MASTER_IP is set to: 10.0.63.202
+++ [0825 06:55:02] ARCH is set to: amd64
+++ [0825 06:55:02] IP_ADDRESS is set to: 10.0.63.204
+++ [0825 06:55:02] USE_CNI is set to: false
+++ [0825 06:55:02] --------------------------------------------
+++ [0825 06:55:02] Killing all kubernetes containers...
+++ [0825 06:55:02] Launching docker bootstrap...
+++ [0825 06:55:04] Launching flannel...
Unable to find image 'quay.io/coreos/flannel:v0.6.1-amd64' locally
v0.6.1-amd64: Pulling from coreos/flannel
9495713bb019: Pull complete 
81f43540ca0e: Pull complete 
58cc1310d43b: Pull complete 
9da20de5be35: Pull complete 
510e8e84c723: Pull complete 
Digest: sha256:6d2f6054910979239b0486ee84c2890e635ebed0666d0c2f431b31a005d02c02
Status: Downloaded newer image for quay.io/coreos/flannel:v0.6.1-amd64
afe6d5f82a9e99382cde8bbc4471da66903a4c242dd48b0d1f720c791b6b2365
!!! [0825 06:56:38] flannel failed to start. Exiting...

OS is centos 7.1
docker version is:

Client:
 Version:      1.12.1
 API version:  1.24
 Go version:   go1.6.3
 Git commit:   23cf638
 Built:  
 OS/Arch:      linux/amd64

Server:
 Version:      1.12.1
 API version:  1.24
 Go version:   go1.6.3
 Git commit:   23cf638
 Built:  
 OS/Arch:      linux/amd64
",closed,False,2016-08-25 11:00:02,2016-09-24 18:33:26
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/213,https://api.github.com/repos/kubernetes/kube-deploy/issues/213,Bug fix for git checkout,"- It was creating local branch from HEAD instead checkout correct branch. It fixes all `[Fail] [k8s.io] ResourceQuota [It]` tests
",closed,True,2016-08-26 05:13:23,2016-08-29 14:28:45
kube-deploy,cheld,https://github.com/kubernetes/kube-deploy/pull/214,https://api.github.com/repos/kubernetes/kube-deploy/issues/214,[WIP] use nsenter to lauch kubelet,"EXPERIMENTAL HACK: DON'T MERGE

PR could be base for fixing mounting issues like #196, #211, #182 

A lot of different network storage products exist on the market. In my understanding some have proprietary drivers and also kernel modules. 

One possible option to solve the problem seems not to containerize the drivers, but assume they are installed on host. With this PR I mount the host filesytem as root directory of the hyperkube container

I was curious a far I can get without much trouble. Actually, it works quite well. Everything seems to work. Even Dashboard is running.

@zreigz could you run e2e tests?

@huangfushun could you make a quick test if this hack maybe works out of the box?
",closed,True,2016-08-26 09:49:23,2016-09-09 14:38:02
kube-deploy,wenerme,https://github.com/kubernetes/kube-deploy/issues/215,https://api.github.com/repos/kubernetes/kube-deploy/issues/215,docker-multinode allow to change cluster ip range,"FLANNEL_NETWORK is config-able, but that's not enough, No way to change dns ip(hard coded) and cluster ip range. So I suggest 
- Put config outside the container.
- Allow use env var to change ip range.

My cloud provider (AliYun) use `10` ip range as internal network, so the default docker-multinode doesn't works with this ip(dns problem, networking communication problem), I have to change the ip range used by docker-multinode(default is 10/8).This took my days to find out the problem.

This is what I do:
- Copy /etc/kubernetes outside the container
- Replace `10.0.0.1` to `172.16.0.1` in `/etc/kubernetes/master-multi/master-multi.json`
- Change `addon` volume in `/etc/kubernetes/master-multi/addon-manager.json` to 

``` json
{
        ""name"": ""addons"",
        ""hostPath"": {""path"":""/path/to/you/own/etc/kubernetes/""}
}
```
- Change `clusterIP` in `/etc/kubernetes/addon/skydns-svc.yaml` to `172.16.0.10`
- Change all `--cluster-dns=10.0.0.10` in `common.sh` to `--cluster-dns=172.16.0.10`
- Add `-v /path/to/you/own/etc/kubernetes/:/etc/kubernetes/ \` to KUBECTL_MOUNT in `common.sh`
- Then `FLANNEL_IPMASQ=false FLANNEL_NETWORK=172.16.0.0/16 ./master.sh`
- Done
",closed,False,2016-08-29 09:04:52,2017-11-01 22:52:34
kube-deploy,farmdawgnation,https://github.com/kubernetes/kube-deploy/issues/216,https://api.github.com/repos/kubernetes/kube-deploy/issues/216,Support custom hyperkube image for docker-multinode,"Presently, there's no way to override the default hyperkube image path. This creates a bit of an awkward situation if you need to install additional things into the image to make Kubernetes do what you want - as I had to do today when I installed NFS in one of our hyperkube containers locally on the box.

Would there be support for the addition of an environment variable that allows the user to override the default image that's calculated?
",closed,False,2016-08-31 00:35:48,2017-11-01 22:52:43
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/217,https://api.github.com/repos/kubernetes/kube-deploy/issues/217,Make it possible to use containerized mode.,"Will probably fix all volume plugins if they exist on host
@zreigz @cheld 
",closed,True,2016-08-31 21:06:40,2016-09-02 14:31:07
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/218,https://api.github.com/repos/kubernetes/kube-deploy/issues/218,Small stability improvements,"@zreigz @cheld

Merging now because I'm releasing Kubernetes on ARM now.
",closed,True,2016-09-04 12:32:10,2016-09-04 12:32:31
kube-deploy,rusher81572,https://github.com/kubernetes/kube-deploy/issues/219,https://api.github.com/repos/kubernetes/kube-deploy/issues/219,Raspberry Pi: Containers can not resolve services when on different nodes,"Hello. I just setup a 4 node cluster and  noticed that I can not ping/resolve services when they are running on other nodes.

Docker version 1.12.1, build 23cf638, experimental
Latest Kube-Deploy as of 9/4/2016 @ 7:23 PM PST
Debian 8.0 Jessie

Steps to reproduce:

```
kubectl run mysql --image=registry:5000/mysql
kubectl expose deployment mysql --port 3306 --target-port 3307 --type=""LoadBalancer""
kubectl run nodechat --image=registry:5000/nodechat
kubectl expose deployment nodechat --port 80 --target-port 999 --type=""LoadBalancer""
kubectl exec -it nodechat bash
```

When I try to ping the service ""mysql"", it never replies. Same behavior with curl on the port 3306. All the machines have the vxlan kernel module and latest firmware. Everything works fine when running on the same node.
",closed,False,2016-09-05 02:24:31,2017-11-01 22:58:30
kube-deploy,rusher81572,https://github.com/kubernetes/kube-deploy/issues/220,https://api.github.com/repos/kubernetes/kube-deploy/issues/220,External IP for services never gets an IP,"Hello, I have a two node Raspberry Pi cluster and am using the docker-multinode images  provided. I created two services but they never get an external IP? Is there some limitation with this version?

```
$ kubectl get services
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
kubernetes   10.0.0.1     <none>        443/TCP    51m
mysql        10.0.0.211   <pending>     3306/TCP   11m
nodechat     10.0.0.2     <pending>     80/TCP     5m
```

After a few minutes it looks like this:

```
root@rpi-4:~# kubectl get svc
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
kubernetes   10.0.0.1     <none>        443/TCP    55m
mysql        10.0.0.211                 3306/TCP   15m
nodechat     10.0.0.2                   80/TCP     10m
```

When trying to access it from a browser I get:
http://192.168.0.104:8080/api/v1/proxy/namespaces/default/services/nodechat/

```
Error: 'dial tcp 172.17.0.4:999: i/o timeout'
Trying to reach: 'http://172.17.0.4:999/'
```

However, kubectl get svc shows a different IP

```
root@rpi-4:~# kubectl get svc
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
kubernetes   10.0.0.1     <none>        443/TCP    57m
mysql        10.0.0.211                 3306/TCP   17m
nodechat     10.0.0.2                   80/TCP     12m

```
",closed,False,2016-09-05 03:04:54,2018-10-29 14:02:41
kube-deploy,stefanodoni,https://github.com/kubernetes/kube-deploy/issues/221,https://api.github.com/repos/kubernetes/kube-deploy/issues/221,Container command '/usr/local/bin/etcd' not found or does not exist..,"Hi,

Just cloned the kube-deploy repo and I'm not able to setup the master node anymore:

$ sudo ./master.sh 
+++ [0906 16:28:31] K8S_VERSION is set to: v1.3.6
+++ [0906 16:28:31] ETCD_VERSION is set to: 3.0.4
+++ [0906 16:28:31] FLANNEL_VERSION is set to: v0.6.1
+++ [0906 16:28:31] FLANNEL_IPMASQ is set to: true
+++ [0906 16:28:31] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [0906 16:28:31] FLANNEL_BACKEND is set to: udp
+++ [0906 16:28:31] RESTART_POLICY is set to: unless-stopped
+++ [0906 16:28:31] MASTER_IP is set to: localhost
+++ [0906 16:28:31] ARCH is set to: amd64
+++ [0906 16:28:31] IP_ADDRESS is set to: 192.168.200.177
+++ [0906 16:28:31] USE_CNI is set to: false
+++ [0906 16:28:31] USE_CONTAINERIZED is set to: false
+++ [0906 16:28:31] --------------------------------------------
+++ [0906 16:28:31] Killing all kubernetes containers...
Do you want to clean /var/lib/kubelet? [Y/n] 
+++ [0906 16:28:32] Launching docker bootstrap...
+++ [0906 16:28:34] Launching etcd...
5922273a0d7eeb6ed0818cea6b90233c680551d42d147936434afdfa08cb3119
docker: Error response from daemon: Container command '/usr/local/bin/etcd' not found or does not exist..
!!! [0906 16:28:54] etcd failed to start. Exiting...

OS: Ubuntu 16.04
$ docker -v
Docker version 1.11.2, build b9f10c9
",closed,False,2016-09-06 14:33:54,2016-09-07 07:48:19
kube-deploy,zreigz,https://github.com/kubernetes/kube-deploy/pull/222,https://api.github.com/repos/kubernetes/kube-deploy/issues/222,Fix killing docker bootstrap containers,"This should resolve issues reported for flanneld and etcd which are started from docker-bootstrap
",closed,True,2016-09-07 08:01:51,2017-10-21 04:15:44
kube-deploy,stefanodoni,https://github.com/kubernetes/kube-deploy/issues/223,https://api.github.com/repos/kubernetes/kube-deploy/issues/223,Enable ABAC authorization mode in docker-multinode,"Hi,

I would like to test the ABAC authorization mode as per instructions outlined here:

http://kubernetes.io/docs/admin/authorization/

Is it possible to enable the ABAC plugin on the docker-multinode deployment?

It seems that currently the apiserver is not being configured with the ABAC mode:

$ sudo docker exec 46a ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  5.4  2.5 424844 103120 ?       Ssl  08:03   0:01 /hyperkube apiserver --service-cluster-ip-range=10.0.0.1/24 --insecure-bind-address=0.0.0.0 --etcd-servers=http://127.0.0.1:4001 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota --client-ca-file=/srv/kubernetes/ca.crt --basic-auth-file=/srv/kubernetes/basic_auth.csv --min-request-timeout=300 --tls-cert-file=/srv/kubernetes/server.cert --tls-private-key-file=/srv/kubernetes/server.key --token-auth-file=/srv/kubernetes/known_tokens.csv --allow-privileged=true --v=2

I guess I should edit the apiserver command line and add the relevant options (i.e. --authorization-mode), however in the common.sh script there is no apiserver startup command line.

If this is not possible, what is the easiest way to test ABAC auth mode? I've also setup a cluster on GKE, but it doesn't seem to support ABAC either. 

Thanks a lot!
",closed,False,2016-09-07 08:10:54,2017-11-01 22:53:26
kube-deploy,linericyang,https://github.com/kubernetes/kube-deploy/pull/224,https://api.github.com/repos/kubernetes/kube-deploy/issues/224,Fix truncate punctuation mark by mistake,"Previously, if a whole 'mtu' or 'bip' expression ends with a punctuation mark
on the end of line, user have to manually add additional space to prevent
truncate the mark by mistake. So use sed to add space before 'mtu' and 'bip'
replacement.
",closed,True,2016-09-08 05:27:09,2017-10-21 04:15:51
kube-deploy,garrettheaver,https://github.com/kubernetes/kube-deploy/pull/225,https://api.github.com/repos/kubernetes/kube-deploy/issues/225,Install docker-bootstrap as a systemd service where available,"I'm hoping for feedback on this pull request first rather than asking for it to be merged. I've read that CNI is going to be the preferred way for etcd and flannel to be setup in the future but in the short term docker-bootstrap does not survive restarts. This pull request represents my initial idea on how to fix that.

Essentially it just installs docker-bootstrap as a systemd service (where systemd is running), enables the service and starts it. Where systemd is not available it defers to the existing method of just starting a docker daemon.
",closed,True,2016-09-10 22:41:22,2016-09-13 19:25:40
kube-deploy,cheld,https://github.com/kubernetes/kube-deploy/pull/226,https://api.github.com/repos/kubernetes/kube-deploy/issues/226,Enable end-user customization of manifest files in docker-multinode,"I added a simple way to customize the docker-multinode provisioning.

First, a new script copies the manifest files from container to host. The user can customize them as needed. 
Then these files are mounted when kubelet is started. The addon manager must be adapted as well to read from host. Separate PR https://github.com/kubernetes/kubernetes/pull/32578

I tried to make the UX simple. It seemed best to always read from host instead of providing a switch. 

Would fix

https://github.com/kubernetes/kube-deploy/issues/215

and beneficial to more issues like

 https://github.com/kubernetes/kube-deploy/issues/216
https://github.com/kubernetes/kube-deploy/issues/195
",closed,True,2016-09-13 15:18:28,2016-11-15 07:19:13
kube-deploy,jonboulle,https://github.com/kubernetes/kube-deploy/pull/227,https://api.github.com/repos/kubernetes/kube-deploy/issues/227,README: correct minor typo (it's/its),,closed,True,2016-09-13 16:17:44,2016-09-15 16:53:04
kube-deploy,LaurentDumont,https://github.com/kubernetes/kube-deploy/issues/228,https://api.github.com/repos/kubernetes/kube-deploy/issues/228,Ansible and kube-deploy,"Hi,

I'm trying to integrate Ansible into kube-deploy to allow for a faster and smoother deployment. I've ran into a strange issue where if I'm running the script for the ansible playbook, I cant mount gluster volumes in pods. But, if I run the worker.sh/master.sh locally after sudoing myself (or with sudo) everything seems to work fine. 

Here is an example of the Ansible playbook for a master node. In theory, everything should run under the root user with the correct ENV variables.

```
- name: Download the kube-deploy files
  git: repo=https://github.com/kubernetes/kube-deploy.git dest=/opt/kube-deploy version=master

- name: Run the master deploy script
  shell: echo Y | ./master.sh
  args:
    chdir: /opt/kube-deploy/docker-multinode/
  environment:
    USE_CNI: true
    USE_CONTAINERIZED: true
    K8S_VERSION: v1.4.0-alpha.2
```

And here is the actual task definition

```

---
- hosts: k8-master
  become: yes
  become_method: sudo
  gather_facts: yes
  roles:
      #- common
    - master
```
",closed,False,2016-09-14 05:49:03,2017-11-01 22:59:51
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/229,https://api.github.com/repos/kubernetes/kube-deploy/issues/229,Improved README for imagebuilder,"More complete AWS instructions; document need to be in particular
directory.

Fix #171
",closed,True,2016-09-16 14:20:29,2016-09-17 18:38:49
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/230,https://api.github.com/repos/kubernetes/kube-deploy/issues/230,Add gofmt target to imagebuilder makefile,,closed,True,2016-09-16 14:21:18,2016-09-17 18:39:05
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/231,https://api.github.com/repos/kubernetes/kube-deploy/issues/231,imagebuilder: tag images,"By adding some image metadata, an enlightened tool can prompt to update
images or know the capabilities of an image (e.g. aufs vs overlay)
",closed,True,2016-09-16 14:23:54,2016-09-17 18:38:55
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/232,https://api.github.com/repos/kubernetes/kube-deploy/issues/232,imagebuilder: support building in non-us-east regions,"We can support building in regions other than us-east.  This is
important both for cn-north & govcloud, where we can't copy between
regions, but also for people that may not want their images to touch
us-east.
",closed,True,2016-09-16 14:41:01,2016-09-17 18:39:10
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/233,https://api.github.com/repos/kubernetes/kube-deploy/issues/233,imagebuilder: ignore terminated builder instance,"We reuse the instance if it already exists, but we weren't ignoring
terminated instances.
",closed,True,2016-09-16 14:42:40,2016-09-17 18:39:13
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/234,https://api.github.com/repos/kubernetes/kube-deploy/issues/234,imagebuilder: create an image for k8s 1.4,"Highlights:
- 4.4 kernel
- More inodes
",closed,True,2016-09-16 14:44:34,2016-10-05 23:51:07
kube-deploy,kingspp,https://github.com/kubernetes/kube-deploy/issues/235,https://api.github.com/repos/kubernetes/kube-deploy/issues/235,docker-multinode: Unable to install and use docker-multinode offline,"I am currently using kube-deploy/docker-multinode kubernetes deployment solution in a VPN. The VPN is strictly offline and hence I am transferring and loading the required docker images using scp. Among 9 images, master and worker scripts fails to recognise etcd and flannel images. 

We can see that the machine already has etcd-amd64 loaded in docker. But unfortunately for the initial master startup, it fails to find the image and pulls the image from docker hub.

``` bash
#Command
sudo -E ./master.sh
```

<img width=""1100"" alt=""screen shot 2016-09-18 at 6 45 23 pm"" src=""https://cloud.githubusercontent.com/assets/6322488/18617072/d0dd5ba0-7de5-11e6-874d-5249f7e8a0aa.png"">

The same with flannel-amd64

<img width=""1384"" alt=""screen shot 2016-09-18 at 6 46 57 pm"" src=""https://cloud.githubusercontent.com/assets/6322488/18617075/def5f04e-7de5-11e6-84a3-cc288293a0e0.png"">

**Issues:**
1. How to run master and worker scripts in offline mode?
2. What is the location used to store  etcd and flannel docker images ?

**Initial findings**
Master script execution:
master.sh --> common.sh --> kube::multinode::start_etcd() [common.sh] 
",closed,False,2016-09-18 16:05:01,2017-11-01 22:53:33
kube-deploy,Mistobaan,https://github.com/kubernetes/kube-deploy/pull/236,https://api.github.com/repos/kubernetes/kube-deploy/issues/236,fix code formatting,,closed,True,2016-09-22 02:11:23,2017-11-20 22:22:02
kube-deploy,sails,https://github.com/kubernetes/kube-deploy/issues/237,https://api.github.com/repos/kubernetes/kube-deploy/issues/237,docker-multinode: dashboard&kubedns unable start when I set K8S_VERSION:v1.4.0-beta.8 and USE_CNI:true,"when I set K8S_VERSION:v1.4.0-beta.8 and USE_CNI:true:
![image](https://cloud.githubusercontent.com/assets/1535004/18743970/3b94b64a-80ee-11e6-8346-fd06f276af48.png)
",closed,False,2016-09-22 09:59:06,2016-09-23 01:14:11
kube-deploy,vpaul,https://github.com/kubernetes/kube-deploy/issues/238,https://api.github.com/repos/kubernetes/kube-deploy/issues/238,SCSI mount,"Trying to set up 1.3.7 cluster using docker-multinode deployment on Ubuntu 14.04. USE_CONTAINERIZED variable is set to true. I'm able to mount successfully an NFS mount point without installing nfs-common to inside kubelet container. 
However, iSCSI mount doesn't work in spite I'm able to mount it manually on the node.

~/kube-deploy/docker-multinode# iscsiadm -m session -o show
tcp: [1] 192.168.0.12:3260,257 iqn.2005-10.org.freenas.ctl:kube-mysql

~/kube-deploy/docker-multinode# ls -la /dev/disk/by-path/ip-192.168.0.12:3260-iscsi-iqn.2005-10.org.freenas.ctl:kube-mysql-lun-0 
lrwxrwxrwx 1 root root 9 Sep 22 15:40 /dev/disk/by-path/ip-192.168.0.12:3260-iscsi-iqn.2005-10.org.freenas.ctl:kube-mysql-lun-0 -> ../../sdb

pod yaml is as follows. Getting the same result with persistent volume claim.

apiVersion: v1
kind: Pod
metadata:
  name: iscsi-busybox
spec:
  containers:
- image: busybox
  command:
  - sh
  - -c
  - 'tail -f /dev/null'
    imagePullPolicy: IfNotPresent
    name: busybox
    volumeMounts:
    # name must match the volume name below
  - name: iscsi-freenas
    mountPath: ""/mnt""
    volumes:
- name: iscsi-freenas
  iscsi:
    targetPortal: 192.168.0.12:3260
    iqn: ""iqn.2005-10.org.freenas.ctl:kube-mysql""
    fsType: ext4
    lun: 0
    readOnly: false
  
  6m        1m      3   {kubelet 192.168.0.33}          Warning     FailedMount Unable to mount volumes for pod ""iscsi-busybox_default(3e71caa4-8105-11e6-aa7a-0050569f23a5)"": timeout expired waiting for volumes to attach/mount for pod ""iscsi-busybox""/""default"". list of unattached/unmounted volumes=[iscsi-freenas]
  6m        1m      3   {kubelet 192.168.0.33}          Warning     FailedSync  Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod ""iscsi-busybox""/""default"". list of unattached/unmounted volumes=[iscsi-freenas]

~/kube-deploy/docker-multinode# iscsiadm -m session -o show
iscsiadm: No active sessions.

Adding open-iscsi package into kubelet helps a bit. I can see that previously deleted session and target are back but still getting the same error even though /dev/disk/by-path/ip-192.168.0.12:3260-iscsi-iqn.2005-10.org.freenas.ctl:kube-mysql-lun-0 -> ../../sdb is created with new session. 

Any help or advise would be greatly appreciated.
",closed,False,2016-09-22 21:03:39,2017-11-01 23:00:06
kube-deploy,owlab-exp,https://github.com/kubernetes/kube-deploy/issues/239,https://api.github.com/repos/kubernetes/kube-deploy/issues/239,Kernel problem on arm64,"Today I tried the portable multi-node cluster on my arm64 devices.
After executing **docker-multinode/master.sh**, system has stucked. Following is kernel messages caused by it.

```
Sep 25 01:33:18 linaro-developer kernel: [  845.193079] ------------[ cut here ]------------
Sep 25 01:33:18 linaro-developer kernel: [  845.197830] WARNING: CPU: 5 PID: 3210 at kernel/sched/core.c:2603 preempt_count_add+0x10c/0x110()
Sep 25 01:33:18 linaro-developer kernel: [  845.206915] DEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK - 10)
Sep 25 01:33:18 linaro-developer kernel: [  845.214850] Modules linked in: ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 xt_addrtype nf_nat br_netfilter bridge stp llc btwilink bluetooth wl18xx(O) wlcore(O) mac80211(O) cfg80211(O) rfkill mali wlcore_sdio(O) st_drv hisi_powerkey compat(O)
Sep 25 01:33:18 linaro-developer kernel: [  845.238759] CPU: 5 PID: 3210 Comm: hyperkube Tainted: G           O   3.18.0-linaro-hikey #1
Sep 25 01:33:18 linaro-developer kernel: [  845.247376] Call trace:
Sep 25 01:33:18 linaro-developer kernel: [  845.249906] [<ffffffc000088534>] dump_backtrace+0x0/0x11c
Sep 25 01:33:19 linaro-developer kernel: [  845.256835] [<ffffffc000088660>] show_stack+0x10/0x1c
Sep 25 01:33:19 linaro-developer kernel: [  845.263276] [<ffffffc0007ec18c>] dump_stack+0x7c/0xc0
Sep 25 01:33:19 linaro-developer kernel: [  845.269705] [<ffffffc0000afb24>] warn_slowpath_common+0x8c/0xb8
Sep 25 01:33:19 linaro-developer kernel: [  845.276996] [<ffffffc0000afb9c>] warn_slowpath_fmt+0x4c/0x58
Sep 25 01:33:19 linaro-developer kernel: [  845.284050] [<ffffffc0000d5478>] preempt_count_add+0x108/0x110
Sep 25 01:33:19 linaro-developer kernel: [  845.291268] [<ffffffc0007f0904>] _raw_spin_lock+0x14/0x54
Sep 25 01:33:19 linaro-developer kernel: [  845.297232] [<ffffffc0006cd918>] nf_conntrack_set_hashsize+0x9c/0x228
Sep 25 01:33:19 linaro-developer kernel: [  845.303806] [<ffffffc0000cb51c>] param_attr_store+0x58/0xb4
Sep 25 01:33:19 linaro-developer kernel: [  845.309498] [<ffffffc0000caa1c>] module_attr_store+0x1c/0x34
Sep 25 01:33:19 linaro-developer kernel: [  845.315262] [<ffffffc00023c278>] sysfs_kf_write+0x3c/0x50
Sep 25 01:33:19 linaro-developer kernel: [  845.320774] [<ffffffc00023b7c4>] kernfs_fop_write+0x10c/0x178
Sep 25 01:33:19 linaro-developer kernel: [  845.326641] [<ffffffc0001c87b0>] vfs_write+0x98/0x1d8
Sep 25 01:33:19 linaro-developer kernel: [  845.331820] [<ffffffc0001c9070>] SyS_write+0x40/0xa0
Sep 25 01:33:19 linaro-developer kernel: [  845.336880] ---[ end trace fcc0fa22ce02156e ]---
Sep 25 01:33:19 linaro-developer kernel: [  845.342383] Unable to handle kernel paging request at virtual address 4820886000
Sep 25 01:33:19 linaro-developer kernel: [  845.349986] pgd = ffffffc061854000
```

Does anyone have any idea about this?
",closed,False,2016-09-25 06:51:31,2017-11-01 23:00:13
kube-deploy,bjoernbusch,https://github.com/kubernetes/kube-deploy/issues/240,https://api.github.com/repos/kubernetes/kube-deploy/issues/240,Crash of kubernetes 1.4.0 on arm,"I was trying to run docker-multinode with the released v1.4.0 of kubernetes, but the apiserver is unable to start, due to a NullPointer or InvalidMemoryAddress. Anybody else seeing this?

Thanks!

`goroutine 1 [running]:
panic(0x3057c18, 0x14e20008)
        /usr/local/go_k8s_patched/src/runtime/panic.go:500 +0x33c
encoding/gob.newTypeObject(0x2cfc548, 0x8, 0x14e9d420, 0x499d5b0, 0x3272ae0, 0x3272ae0, 0x499d5b0, 0x3272ae0, 0x252fd50)
        /usr/local/go_k8s_patched/src/encoding/gob/type.go:448 +0x24
encoding/gob.getType(0x2cfc548, 0x8, 0x14e9d420, 0x499d5b0, 0x3272ae0, 0x0, 0x0, 0x0, 0x0)
        /usr/local/go_k8s_patched/src/encoding/gob/type.go:608 +0xc0
encoding/gob.getBaseType(0x2cfc548, 0x8, 0x499d5b0, 0x3272ae0, 0x0, 0x0, 0x0, 0x0)
        /usr/local/go_k8s_patched/src/encoding/gob/type.go:595 +0x78
encoding/gob.buildTypeInfo(0x14e9d420, 0x499d5b0, 0x3272ae0, 0x0, 0x0, 0x0)
        /usr/local/go_k8s_patched/src/encoding/gob/type.go:725 +0xf0
encoding/gob.getTypeInfo(0x14e9d420, 0x3272ae0, 0x0, 0x0)
        /usr/local/go_k8s_patched/src/encoding/gob/type.go:712 +0xa4
encoding/gob.mustGetTypeInfo(0x499d5b0, 0x3272ae0, 0x499d5b0)
        /usr/local/go_k8s_patched/src/encoding/gob/type.go:776 +0x34
encoding/gob.init()
        /usr/local/go_k8s_patched/src/encoding/gob/type.go:269 +0x1be0
net/rpc.init()
        /usr/local/go_k8s_patched/src/net/rpc/server.go:713 +0x5c
k8s.io/kubernetes/vendor/github.com/ugorji/go/codec.init()
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/ugorji/go/codec/time.go:223 +0xe4
k8s.io/kubernetes/pkg/runtime/serializer/json.init()
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/runtime/serializer/json/meta.go:62 +0x6c
k8s.io/kubernetes/pkg/runtime/serializer.init()
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/runtime/serializer/protobuf_extension.go:53 +0x64
k8s.io/kubernetes/pkg/api.init()
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/api/zz_generated.deepcopy.go:3750 +0x11c
k8s.io/kubernetes/pkg/api/install.init()
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/api/install/install.go:160 +0x64
k8s.io/kubernetes/federation/cmd/federation-apiserver/app.init()
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/federation/cmd/federation-apiserver/app/server.go:220 +0x5c
main.init()
        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/hyperkube/server.go:78 +0x54
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x2cc4b4c]
`
",closed,False,2016-09-28 05:56:49,2016-09-30 17:37:19
kube-deploy,farmdawgnation,https://github.com/kubernetes/kube-deploy/pull/241,https://api.github.com/repos/kubernetes/kube-deploy/issues/241,Add the ability to specify a custom hyperkube base URL,"While using the docker-multinode deployment strategy it became evident
that it would be beneficial for us to periodically roll our own
hyperkube images with a few extras installed to make things like NFS
work properly for our kubelets. Therefore, this commit introduces the
new HYPERKUBE_BASE_URL setting that allows someone invoking the script
to specify a custom base URL for a hyperkube image.

Arch and Kubernetes version will still be added onto the end to form the
final URL for the image and tag.

As reported in kubernetes/kube-deploy#216
",closed,True,2016-10-01 02:20:47,2017-10-21 04:17:38
kube-deploy,mhas01,https://github.com/kubernetes/kube-deploy/issues/242,https://api.github.com/repos/kubernetes/kube-deploy/issues/242,Docker-multinode: etcd failed to start ... exiting,"Hi I'm using kube-deploy to install a kubernetes cluster on my company VMs. All looks promising until it started launching etcd... 

+++ [1004 14:21:22] K8S_VERSION is set to: v1.4.0
+++ [1004 14:21:22] ETCD_VERSION is set to: 3.0.4
+++ [1004 14:21:22] FLANNEL_VERSION is set to: v0.6.1
+++ [1004 14:21:22] FLANNEL_IPMASQ is set to: true
+++ [1004 14:21:22] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [1004 14:21:22] FLANNEL_BACKEND is set to: udp
+++ [1004 14:21:22] RESTART_POLICY is set to: unless-stopped
+++ [1004 14:21:22] MASTER_IP is set to: localhost
+++ [1004 14:21:22] ARCH is set to: amd64
+++ [1004 14:21:22] IP_ADDRESS is set to: 10.110.10.110
+++ [1004 14:21:22] USE_CNI is set to: false
+++ [1004 14:21:22] USE_CONTAINERIZED is set to: false
+++ [1004 14:21:22] --------------------------------------------
+++ [1004 14:21:22] Killing docker bootstrap...
+++ [1004 14:21:22] Killing all kubernetes containers...
Do you want to clean /var/lib/kubelet? [Y/n] n
+++ [1004 14:21:24] Launching docker bootstrap...
+++ [1004 14:21:25] Launching etcd...
21210dbe7f5d177e535e070233e59c49dc6f6df76f0c3246353f9491c021d149
**!!! [1004 14:21:45] etcd failed to start. Exiting...**

I wonder how can I start troubleshooting this. Is there a log file that I can review in order to see what is preventing etcd to start? 

Thank you in advance for your help. 
",closed,False,2016-10-04 18:23:03,2017-11-01 22:53:38
kube-deploy,colemickens,https://github.com/kubernetes/kube-deploy/issues/243,https://api.github.com/repos/kubernetes/kube-deploy/issues/243,Should kube-deploy mount /var/log instead of /var/log/containers?,"See: https://github.com/deis/logger/issues/50#issuecomment-219871168

In kube-deploy, you only mount `/var/log/containers` where as @chancez's comment implies that we need to mount all of `/var/log`.

Thoughts? Related discussion for `kubernetes-anywhere`: https://github.com/kubernetes/kubernetes-anywhere/pull/258#discussion_r81869888
",closed,False,2016-10-04 22:45:37,2017-11-01 23:10:35
kube-deploy,TDeprez,https://github.com/kubernetes/kube-deploy/issues/244,https://api.github.com/repos/kubernetes/kube-deploy/issues/244,Docker-multinode: Cluster IP's being reassigned whilst in use,"Having setup a cluster twice, first with 2 nodes, then 4,  using kube-deploy/docker-multinode on Centos 7, when creating pods they are occasionally being assigned IP's that are already in use. 

![image](https://cloud.githubusercontent.com/assets/7631438/19120567/1344ed40-8b1b-11e6-946d-0b3640eec7ac.png)

Note: Ingress controller health check is failing as it's trying to access /ingress-controller-healthz on the default backend due to them having being assigned the same IP address.

Ingress controller and default backend have been created using this - https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/rc.yaml

This is using Kubernetes 1.3.6
Docker 1.10.3
",closed,False,2016-10-05 15:49:54,2017-11-01 22:53:43
kube-deploy,WillPlatnick,https://github.com/kubernetes/kube-deploy/issues/245,https://api.github.com/repos/kubernetes/kube-deploy/issues/245,Install kernel headers,"For sysdig to do magic, it builds and inserts a kernel module that intercepts all the things. To build the kernel module, we need the headers installed.

sudo apt-get update ; sudo apt-get -y install linux-headers-$(uname -r)
",closed,False,2016-10-10 19:36:14,2016-12-05 02:11:53
kube-deploy,rusher81572,https://github.com/kubernetes/kube-deploy/issues/246,https://api.github.com/repos/kubernetes/kube-deploy/issues/246,failed to retrieve network config: 100: Key not found (/coreos.com) ,"Hi, I pulled the latest code and wanted to see if docker-multinode worked on Kube 1.4.1. 

Flannel is failing to start:

root@rpi-1:~/kube-deploy/docker-multinode# ./master.sh
+++ [1012 10:56:42] K8S_VERSION is set to: v1.4.1
+++ [1012 10:56:42] ETCD_VERSION is set to: 2.2.5
+++ [1012 10:56:42] FLANNEL_VERSION is set to: v0.6.1
+++ [1012 10:56:42] FLANNEL_IPMASQ is set to: true
+++ [1012 10:56:42] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [1012 10:56:42] FLANNEL_BACKEND is set to: udp
+++ [1012 10:56:42] RESTART_POLICY is set to: unless-stopped
+++ [1012 10:56:42] MASTER_IP is set to: 192.168.0.101
+++ [1012 10:56:42] ARCH is set to: arm
+++ [1012 10:56:42] IP_ADDRESS is set to: 192.168.0.101
+++ [1012 10:56:42] USE_CNI is set to: true
+++ [1012 10:56:42] USE_CONTAINERIZED is set to: false
+++ [1012 10:56:42] --------------------------------------------
+++ [1012 10:56:42] Killing all kubernetes containers...
a1f31faf2724
0281d0cfc2a9
Do you want to clean /var/lib/kubelet? [Y/n] y
+++ [1012 10:56:44] Launching etcd...
edd7e6894b109a586a50612661fe14096f8eec22586f3d02f8298d4da7a0721d
+++ [1012 10:56:49] Launching flannel...
07bff38d3997f7328ecb3fe4a5c79e64286d366a98dc552fdb13205d556f0f5a
!!! [1012 10:57:09] flannel failed to start. Exiting...

E1012 17:57:30.532429 00001 network.go:106] failed to retrieve network config: 100: Key not found (/coreos.com) [3]
",closed,False,2016-10-12 17:59:56,2017-11-01 23:01:13
kube-deploy,AncientRemember,https://github.com/kubernetes/kube-deploy/pull/247,https://api.github.com/repos/kubernetes/kube-deploy/issues/247,adapte to centos7,,closed,True,2016-10-13 01:54:32,2017-10-21 04:17:18
kube-deploy,bassco,https://github.com/kubernetes/kube-deploy/pull/248,https://api.github.com/repos/kubernetes/kube-deploy/issues/248,Add kernel header installation,"Install the linux headers on the AMI for the current k8s kernel.

Fixes #kubernetes/kops/646 and #245 

@justinsb this is dependent on kopeio/kubernetes-kernel/pull/2
@chrislovecnm FYI
",closed,True,2016-10-14 23:14:26,2016-12-05 02:11:50
kube-deploy,taylorsmcclure,https://github.com/kubernetes/kube-deploy/issues/249,https://api.github.com/repos/kubernetes/kube-deploy/issues/249,hyperkube failing to start api server,"I am following the [docker-multinode](https://github.com/kubernetes/kube-deploy/blob/master/docker-multinode/README.md) deployment guide, but I see the hyperkube container appears to not start the api server successfully.

From the hyperkube container after running `master.sh`:

```
I1020 22:18:16.365802   19315 docker.go:327] Start docker client with request timeout=2m0s
W1020 22:18:16.458039   19315 server.go:487] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: Error loading c
onfig file ""/var/lib/kubelet/kubeconfig"": read /var/lib/kubelet/kubeconfig: is a directory. Trying auth path instead.
W1020 22:18:16.458341   19315 server.go:448] Could not load kubernetes auth path /var/lib/kubelet/kubernetes_auth: stat
/var/lib/kubelet/kubernetes_auth: no such file or directory. Continuing with defaults.
I1020 22:18:16.463434   19315 manager.go:138] cAdvisor running in container: ""/docker/7fb8ff1c98d23b308034aecc48571661dd
27a27ff66452382634bd615d035c78""
W1020 22:18:16.769319   19315 manager.go:146] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service
: dial tcp [::1]:15441: getsockopt: connection refused
I1020 22:18:16.982599   19315 fs.go:139] Filesystem partitions: map[/dev/root:{mountpoint:/var/lib/docker/overlay major:
179 minor:2 fsType:ext4 blockSize:0}]
E1020 22:18:16.994914   19315 machine.go:193] failed to get cache information for node 0: open /sys/devices/system/cpu/c
pu0/cache: no such file or directory
```

Then just repeated errors of the following:

```
E1020 22:18:17.023057   19315 reflector.go:205] pkg/kubelet/config/apiserver.go:43: Failed to list *api.Pod: Get http://localhost:8080/api/v1/pods?fieldSelector=spec.nodeName%3D192.168.1.97&resourceVersion=0: dial tcp [::1]:8080: getsockopt: connection refused
E1020 22:18:17.023984   19315 reflector.go:205] pkg/kubelet/kubelet.go:281: Failed to list *api.Node: Get http://localhost:8080/api/v1/nodes?fieldSelector=metadata.name%3D192.168.1.97&resourceVersion=0: dial tcp [::1]:8080: getsockopt: connection refused
E1020 22:18:17.024636   19315 reflector.go:205] pkg/kubelet/kubelet.go:262: Failed to list *api.Service: Get http://localhost:8080/api/v1/services?resourceVersion=0: dial tcp [::1]:8080: getsockopt: connection refused
```

The issue may be associated with:

```
W1020 22:18:16.458039   19315 server.go:487] Could not load kubeconfig file /var/lib/kubelet/kubeconfig: Error loading c
onfig file ""/var/lib/kubelet/kubeconfig"": read /var/lib/kubelet/kubeconfig: is a directory. Trying auth path instead.
```

Where the error is correct, `/var/lib/kubelet/kubeconfig` is a directory. The configuration file it is looking for is located in `/var/lib/kubelete/kubeconfig/kubeconfig.yaml`. I noticed at some point during the docker bootstrap process the directory and YAML file are populated. I'm not really sure where I should look to try and fix this. This is all assuming the error is related to the api server not starting... I honestly am not sure.

Details of my setup:

```
PRETTY_NAME=""Raspbian GNU/Linux 8 (jessie)""
NAME=""Raspbian GNU/Linux""
VERSION_ID=""8""
VERSION=""8 (jessie)""
ID=raspbian
ID_LIKE=debian
HOME_URL=""http://www.raspbian.org/""
SUPPORT_URL=""http://www.raspbian.org/RaspbianForums""
BUG_REPORT_URL=""http://www.raspbian.org/RaspbianBugs""
HYPRIOT_OS=""HypriotOS/armhf""
HYPRIOT_OS_VERSION=""v1.0.0""
HYPRIOT_DEVICE=""Raspberry Pi""
HYPRIOT_IMAGE_VERSION=""v1.0.0""
```

```
Linux ... 4.4.15-hypriotos-v7+ #1 SMP PREEMPT Mon Jul 25 08:46:52 UTC 2016 armv7l GNU/Linux
```

```
+++ [1021 02:03:26] K8S_VERSION is set to: v1.3.2
+++ [1021 02:03:26] ETCD_VERSION is set to: 2.2.5
+++ [1021 02:03:26] FLANNEL_VERSION is set to: v0.6.1
+++ [1021 02:03:26] FLANNEL_IPMASQ is set to: true
+++ [1021 02:03:26] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [1021 02:03:26] FLANNEL_BACKEND is set to: udp
+++ [1021 02:03:26] RESTART_POLICY is set to: unless-stopped
+++ [1021 02:03:26] MASTER_IP is set to: localhost
+++ [1021 02:03:26] ARCH is set to: arm
+++ [1021 02:03:26] IP_ADDRESS is set to: 192.168.1.97
+++ [1021 02:03:26] USE_CNI is set to: false
+++ [1021 02:03:26] USE_CONTAINERIZED is set to: false
```

Any help would be appreciated!
",closed,False,2016-10-21 02:05:32,2016-11-04 01:40:34
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/250,https://api.github.com/repos/kubernetes/kube-deploy/issues/250,Add linux-headers to image,"The headers are needed to enable kernel modules to be built, such as
sysdig
",closed,True,2016-10-21 06:58:14,2016-10-21 06:58:41
kube-deploy,AGrzes,https://github.com/kubernetes/kube-deploy/issues/251,https://api.github.com/repos/kubernetes/kube-deploy/issues/251,How to clear docker-multinode configuration entirely?,"I'm experimenting with kubernetes/docker-multinode and after some time I wanted to start from clear slate. I killed all docker containers, removed all the volumes, and cleared /var/lib/kubelet/ directory

    docker rm --force $(docker ps -a -q)
    docker volume rm $(docker volume ls -q)
    sudo rm -r /var/lib/kubelet/

Bur after docker restart and starting master node I see that my configured pods are starting - so it looks like it stores persistent configuration somewhere else. What else I can do to clear all docker-multinode configuration from my machine (Apart of system reinstall)?",closed,False,2016-11-07 20:35:37,2016-11-11 17:02:31
kube-deploy,xmik,https://github.com/kubernetes/kube-deploy/issues/252,https://api.github.com/repos/kubernetes/kube-deploy/issues/252,"docker-multinode: apiserver container exited, no basic_auth.csv","### Error
I am following this guide: http://kubernetes.io/docs/getting-started-guides/docker-multinode/ . I created 2 virtual machines in openstack, treating one as master and one as worker. The problem is on master vm. I ran:
```
$ git clone https://github.com/kubernetes/kube-deploy
$ cd kube-deploy/docker-multinode
$ ./master.sh
```
It created many docker containers, but the container with `/hyperkube apiserver` command exited (with status 255). Its logs say:
```
$ sudo docker logs 9b9563d3ee96
I1112 14:49:44.765945       1 genericapiserver.go:629] Will report 10.9.0.14 as public IP address.
I1112 14:49:44.766618       1 server.go:146] Initalizing deserialization cache size based on 0MB limit
W1112 14:49:44.767501       1 server.go:205] No RSA key provided, service account token authentication disabled
F1112 14:49:44.767748       1 server.go:238] Invalid Authentication Config: open /srv/kubernetes/basic_auth.csv: no such file or directory
```

### Workaround
I discovered that the missing file is present in the controller container, so I copied it onto the docker host (openstack vm):
```
$ sudo docker cp 18111ec7b38a:/srv/kubernetes/basic_auth.csv /tmp/
```
and then copied it into the apiserver container:
```
$ sudo docker cp /tmp/basic_auth.csv 9b9563d3ee96:/srv/kubernetes/
```
Then, I started the apiserver container, but it exited again, with no new log messages. So I redeployed it. Now docker logs contained: `/srv/kubernetes/ca.crt: no such file or directory`. Thus, I copied the whole kubernetes directory from the controller container to the docker host:
```
$ sudo docker cp 18111ec7b38a:/srv/kubernetes/ /tmp/
$ ls /tmp/kubernetes/
basic_auth.csv  ca.crt  known_tokens.csv  kubecfg.crt  kubecfg.key  server.cert  server.key
```
and redeployed apiserver container again using command:
```
$ sudo docker run -d -p 443:443 -p 8080:8080 --name k8s_apiserver -v /tmp/kubernetes:/srv/kubernetes gcr.io/google_containers/hyperkube-amd64:v1.4.6 /hyperkube apiserver --service-cluster-ip-range=10.0.0.1/24 --insecure-bind-address=0.0.0.0 --etcd-servers=http://127.0.0.1:2379 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota --client-ca-file=/srv/kubernetes/ca.crt --basic-auth-file=/srv/kubernetes/basic_auth.csv --min-request-timeout=300 --tls-cert-file=/srv/kubernetes/server.cert --tls-private-key-file=/srv/kubernetes/server.key --token-auth-file=/srv/kubernetes/known_tokens.csv --allow-privileged=true --v=2
```
I am not sure if the `docker run` command is fully ok, because I got it based on `docker inspect <failed_apiserver_container>`.

This workaround worked, the apiserver is now up and running. I don't know why the `/srv/kubernetes` directory inside the apiserver container was missing files.

### Environment
* Virtual machines with Ubuntu 16.04
* kube-deploy commit: [5f6a8c3861c490d2cc52f280383ef0ed2978f0a3](https://github.com/kubernetes/kube-deploy/commit/5f6a8c3861c490d2cc52f280383ef0ed2978f0a3)
* docker image for apiserver container: `gcr.io/google_containers/hyperkube-amd64:v1.4.6`
* kernel:
```
$ uname -r
4.4.0-45-generic
```
* docker:
```
$ sudo docker version
sudo: unable to resolve host my-cluster-k8s-master-1
Client:
 Version:      1.12.3
 API version:  1.24
 Go version:   go1.6.3
 Git commit:   6b644ec
 Built:        Wed Oct 26 22:01:48 2016
 OS/Arch:      linux/amd64

Server:
 Version:      1.12.3
 API version:  1.24
 Go version:   go1.6.3
 Git commit:   6b644ec
 Built:        Wed Oct 26 22:01:48 2016
 OS/Arch:      linux/amd64
```
",closed,False,2016-11-12 18:59:05,2017-06-21 14:27:24
kube-deploy,farmdawgnation,https://github.com/kubernetes/kube-deploy/pull/253,https://api.github.com/repos/kubernetes/kube-deploy/issues/253,Permit custom Docker roots.,"We ran into issues at @dominodatalab with custom docker roots and kube-deploy.

Net-net, we use a custom docker root in some of our deployments and doing do caused Kubelet to behave badly - specifically it would nuke logfile symlinks super aggressively. This fixes that issue by allowing us to specify a custom docker root.",closed,True,2016-11-12 22:13:57,2016-11-12 22:20:22
kube-deploy,farmdawgnation,https://github.com/kubernetes/kube-deploy/pull/254,https://api.github.com/repos/kubernetes/kube-deploy/issues/254,Permit custom Docker roots,"We ran into issues at @dominodatalab with custom docker roots and kube-deploy.

Net-net, we use a custom docker root in some of our deployments and doing do caused Kubelet to behave badly - specifically it would nuke logfile symlinks super aggressively. This fixes that issue by allowing us to specify a custom docker root.",closed,True,2016-11-12 22:21:03,2017-10-21 04:16:52
kube-deploy,chrislovecnm,https://github.com/kubernetes/kube-deploy/issues/255,https://api.github.com/repos/kubernetes/kube-deploy/issues/255,Recommended stuff for the aws base image to make ops devops,"- sysstats
- sar
- iostat
- htop

We should have these toys installed",closed,False,2016-11-12 22:29:18,2017-12-19 03:44:25
kube-deploy,nicka101,https://github.com/kubernetes/kube-deploy/pull/256,https://api.github.com/repos/kubernetes/kube-deploy/issues/256,Add OpenRC support to docker-multinode's docker-bootstrap,This commit should enable support for using the docker-multinode configuration on OpenRC-based systems such as Alpine,closed,True,2016-11-15 04:42:44,2017-12-27 00:43:28
kube-deploy,chrislovecnm,https://github.com/kubernetes/kube-deploy/issues/257,https://api.github.com/repos/kubernetes/kube-deploy/issues/257,image builder is failing error creating instance: SubnetID must be specified,"Walking through the code now but:

1.  Followed the aws create vpc instructions
2. updated the 1.4 yaml
3. ran the imagebuild command

```
I1116 15:49:53.079399    4695 main.go:154] Parsed template ""templates/1.4.yml""; will build image with name k8s-1.4-debian-jessie-amd64-hvm-ebs-2016-11-16
I1116 15:49:53.079920    4695 aws.go:203] AWS DescribeInstances Filter:tag-key=k8s.io/role/imagebuilder
I1116 15:49:53.358620    4695 aws.go:255] AWS DescribeSubnets Filter:tag-key=k8s.io/role/imagebuilder
F1116 15:49:53.508855    4695 main.go:165] error creating instance: SubnetID must be specified, or a subnet must be tagged with ""k8s.io/role/imagebuilder""
goroutine 1 [running]:
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.stacks(0xed6600, 0xc400000000, 0x9b, 0xee)
	/home/clove/workspace/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:769 +0xa5
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.(*loggingT).output(0xeba300, 0xc400000003, 0xc4200ea3c0, 0xe8af86, 0x7, 0xa5, 0x0)
	/home/clove/workspace/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:720 +0x357
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.(*loggingT).printf(0xeba300, 0x3, 0xb53ec7, 0x1b, 0xc42005bd68, 0x1, 0x1)
	/home/clove/workspace/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:655 +0x14c
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.Fatalf(0xb53ec7, 0x1b, 0xc42005bd68, 0x1, 0x1)
	/home/clove/workspace/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:1148 +0x67
main.main()
	/home/clove/workspace/src/k8s.io/kube-deploy/imagebuilder/main.go:165 +0x2184
```",closed,False,2016-11-16 21:00:34,2018-02-18 09:43:02
kube-deploy,chrislovecnm,https://github.com/kubernetes/kube-deploy/pull/258,https://api.github.com/repos/kubernetes/kube-deploy/issues/258,Ethtool and admin tools,Adding cool stuff ... `ethtool` is required!,closed,True,2016-11-17 04:16:01,2016-12-05 02:22:34
kube-deploy,razic,https://github.com/kubernetes/kube-deploy/issues/259,https://api.github.com/repos/kubernetes/kube-deploy/issues/259,cant run image builder,"no matter what i do i can't seem to run the image builder successfully. constantly getting this output:

```bash
I1120 21:25:16.418807   10738 main.go:154] Parsed template ""templates/1.4.yml""; will build image with name k8s-1.4-debian-jessie-amd64-hvm-ebs-2016-11-20
I1120 21:25:16.419190   10738 aws.go:203] AWS DescribeInstances Filter:tag-key=k8s.io/role/imagebuilder
I1120 21:25:16.746629   10738 aws.go:223] Ignoring instance ""i-5a9c1ecf"" in state ""terminated""
I1120 21:25:16.746730   10738 aws.go:223] Ignoring instance ""i-54a022c1"" in state ""terminated""
I1120 21:25:16.856763   10738 aws.go:255] AWS DescribeSubnets Filter:tag-key=k8s.io/role/imagebuilder
I1120 21:25:17.066388   10738 aws.go:300] AWS DescribeSubnetsInput ID:""subnet-9b6684fc""
I1120 21:25:17.198073   10738 aws.go:282] AWS DescribeSecurityGroups Filter:tag-key=k8s.io/role/imagebuilder
I1120 21:25:17.326255   10738 aws.go:466] AWS RunInstances InstanceType=""m3.medium"" ImageId=""ami-98e114f8"" KeyName=""imagebuilder-d81a08a2ab4b12c9bd843074dd560622""
I1120 21:25:17.920410   10738 aws.go:319] AWS CreateTags Resource=""i-7ea022eb""
I1120 21:25:18.067322   10738 aws.go:532] AWS DescribeImages Filter:Name=""k8s-1.4-debian-jessie-amd64-hvm-ebs-2016-11-20"", Owner=self
I1120 21:25:18.170868   10738 aws.go:156] AWS DescribeInstances InstanceId=""i-7ea022eb""
I1120 21:25:18.337647   10738 aws.go:87] Instance public IP is ""35.162.233.53""
W1120 21:25:54.469750   10738 aws.go:67] error connecting to SSH on server ""35.162.233.53"": dial tcp 35.162.233.53:22: getsockopt: connection refused
W1120 21:25:59.521868   10738 aws.go:67] error connecting to SSH on server ""35.162.233.53"": dial tcp 35.162.233.53:22: getsockopt: connection refused
W1120 21:26:04.579487   10738 aws.go:67] error connecting to SSH on server ""35.162.233.53"": dial tcp 35.162.233.53:22: getsockopt: connection refused
W1120 21:26:09.625901   10738 aws.go:67] error connecting to SSH on server ""35.162.233.53"": dial tcp 35.162.233.53:22: getsockopt: connection refused
I1120 21:26:15.345818   10738 interface.go:75] Executing command: [""sudo"" ""apt-get"" ""update""]
I1120 21:26:29.053704   10738 interface.go:83] Output was: sudo: unable to resolve host ip-172-20-1-212
Ign http://cloudfront.debian.net jessie InRelease
Get:1 http://cloudfront.debian.net jessie-updates InRelease [145 kB]
Get:2 http://cloudfront.debian.net jessie-backports InRelease [166 kB]
Get:3 http://cloudfront.debian.net jessie Release.gpg [2,373 B]
Get:4 http://cloudfront.debian.net jessie Release [148 kB]
Get:5 http://security.debian.org jessie/updates InRelease [63.1 kB]
Get:6 http://cloudfront.debian.net jessie-updates/contrib amd64 Packages [32 B]
Get:7 http://cloudfront.debian.net jessie-updates/contrib Translation-en [14 B]
Get:8 http://cloudfront.debian.net jessie-updates/main Translation-en [12.6 kB]
Get:9 http://security.debian.org jessie/updates/main amd64 Packages [321 kB]
Get:10 http://security.debian.org jessie/updates/contrib amd64 Packages [2,506 B]
Get:11 http://cloudfront.debian.net jessie-updates/non-free Translation-en [496 B]
Get:12 http://security.debian.org jessie/updates/non-free amd64 Packages [14 B]
Get:13 http://security.debian.org jessie/updates/contrib Translation-en [1,211 B]
Get:14 http://security.debian.org jessie/updates/main Translation-en [172 kB]
Get:15 http://security.debian.org jessie/updates/non-free Translation-en [14 B]
Get:16 http://cloudfront.debian.net jessie-backports/main Translation-en [512 kB]
Get:17 http://cloudfront.debian.net jessie/main amd64 Packages [6,787 kB]
Get:18 http://cloudfront.debian.net jessie/contrib amd64 Packages [50.2 kB]
Get:19 http://cloudfront.debian.net jessie/non-free amd64 Packages [83.6 kB]
Get:20 http://cloudfront.debian.net jessie/contrib Translation-en [38.5 kB]
Get:21 http://cloudfront.debian.net jessie/main Translation-en [4,583 kB]
Get:22 http://cloudfront.debian.net jessie/non-free Translation-en [72.3 kB]
Get:23 http://cloudfront.debian.net jessie-updates/main amd64 Packages [15.5 kB]
Get:24 http://cloudfront.debian.net jessie-updates/non-free amd64 Packages [516 B]
Get:25 http://cloudfront.debian.net jessie-backports/main Sources [667 kB]
Get:26 http://cloudfront.debian.net jessie-backports/main amd64 Packages [763 kB]
Fetched 14.6 MB in 9s (1,510 kB/s)
Reading package lists...
I1120 21:26:29.084684   10738 interface.go:75] Executing command: [""sudo"" ""apt-get"" ""install"" ""--yes"" ""git"" ""python"" ""debootstrap"" ""python-pip"" ""kpartx"" ""parted""]
I1120 21:26:30.180742   10738 interface.go:78] Error from SSH command [""sudo"" ""apt-get"" ""install"" ""--yes"" ""git"" ""python"" ""debootstrap"" ""python-pip"" ""kpartx"" ""parted""]: Process exited with status 100
I1120 21:26:30.180839   10738 interface.go:79] Output was: sudo: unable to resolve host ip-172-20-1-212
Reading package lists...
Building dependency tree...
Reading state information...
Package kpartx is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another source

E: Unable to locate package python-pip
E: Package 'kpartx' has no installation candidate
F1120 21:26:30.180867   10738 main.go:221] error setting up instance: error executing SSH command [""sudo"" ""apt-get"" ""install"" ""--yes"" ""git"" ""python"" ""debootstrap"" ""python-pip"" ""kpartx"" ""parted""]: Process exited with status 100
goroutine 1 [running]:
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.stacks(0xed6700, 0xc400000000, 0xe3, 0x137)
        /Users/razic/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:769 +0xa5
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.(*loggingT).output(0xeba320, 0xc400000003, 0xc4200d8480, 0xe8ab66, 0x7, 0xdd, 0x0)
        /Users/razic/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:720 +0x357
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.(*loggingT).printf(0xeba320, 0x3, 0xb55da1, 0x1d, 0xc420045d08, 0x1, 0x1)
        /Users/razic/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:655 +0x14c
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.Fatalf(0xb55da1, 0x1d, 0xc420045d08, 0x1, 0x1)
        /Users/razic/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:1148 +0x67
main.main()
        /Users/razic/src/k8s.io/kube-deploy/imagebuilder/main.go:221 +0x1a24
```

any ideas on what i might be doing wrong?",closed,False,2016-11-20 21:34:21,2018-02-17 03:13:04
kube-deploy,honnix,https://github.com/kubernetes/kube-deploy/issues/260,https://api.github.com/repos/kubernetes/kube-deploy/issues/260,docker-multinode: master node registered as schedulable,"When starting `kubelet` on master node, by default the node will be registered as schedulable, so user pod will later be scheduled to start on master node. This doesn't comply with the drawing in the documentation that user pod will be launched only on worker node. On the other hand, is there some way to pass in the option?",closed,False,2016-11-23 15:01:58,2017-11-01 22:53:49
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/issues/261,https://api.github.com/repos/kubernetes/kube-deploy/issues/261,Image: We should tweak our sysctls,"```
sysctl -w net.core.somaxconn=32768
```

Thanks @aledbf ",closed,False,2016-12-02 18:48:14,2018-02-17 03:13:03
kube-deploy,toliaqat,https://github.com/kubernetes/kube-deploy/pull/262,https://api.github.com/repos/kubernetes/kube-deploy/issues/262,Small stability improvement in docker-bootstrap.sh,,closed,True,2016-12-13 06:48:31,2017-10-21 04:17:56
kube-deploy,huangfushun,https://github.com/kubernetes/kube-deploy/issues/263,https://api.github.com/repos/kubernetes/kube-deploy/issues/263,restart  kubelet container cause mount volume  disconnected,"`root@h2g6s:/# cd data
bash: cd: data: Transport endpoint is not connected
`
here is kubelet container:
docker ps --no-trunc | grep kubelet
c84d894c5a33755401b354527444866f5188eb32698d556d0eb2e1ecafc2da68   docker.gf.com.cn/mot/hyperkube-amd64:v1.3.5    ""/hyperkube kubelet --allow-privileged --api-servers=http://10.2.122.1:8080 --cluster-dns=11.0.0.10 --cluster-domain=cluster.local --hostname-override=10.2.122.11 --containerized --v=2""   

 can kubelet container watch and restore existing mount volume when startup?

possible relate to #196",closed,False,2016-12-28 03:35:05,2017-11-01 23:01:26
kube-deploy,ktraff,https://github.com/kubernetes/kube-deploy/issues/264,https://api.github.com/repos/kubernetes/kube-deploy/issues/264,CephFS Volume Mount Fails,"I've successfully deployed a Ceph cluster and am able to mount a CephFS device manually using the following:

`sudo mount -t ceph monitor1:6789:/ /ceph -o name=admin,secretfile=/etc/ceph/cephfs.secret`

I'm now attempting to launch a pod using the kubernetes example [here](https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/cephfs):

```
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: my-ceph-secret-key
---
apiVersion: v1
kind: Pod
metadata:
  name: cephfs2
spec:
  containers:
  - name: cephfs-rw
    image: kubernetes/pause
    volumeMounts:
    - mountPath: ""/mnt/cephfs""
      name: cephfs
  volumes:
  - name: cephfs
    cephfs:
      monitors:
      - ""monitor1:6789""
      - ""monitor2:6789""
      - ""monitor3:6789""
      user: admin
      secretRef:
        name: ceph-secret
      readOnly: false
```

When I run:

`sudo kubectl create -f cephfs.yml`

I am receiving the following error:

```
Normal		Scheduled	Successfully assigned cephfs2 to monitor2
Warning		FailedMount	MountVolume.SetUp failed for volume ""kubernetes.io/cephfs/445ee063-d1f1-11e6-a3e3-1418776a29a6-cephfs"" (spec.Name: ""cephfs"") pod ""445ee063-d1f1-11e6-a3e3-1418776a29a6"" (UID: ""445ee063-d1f1-11e6-a3e3-1418776a29a6"") with: CephFS: mount failed: mount failed: fork/exec /bin/mount: invalid argument
Mounting arguments: monitor1:6789,monitor2:6789,monitor3:6789:/data /var/lib/kubelet/pods/445ee063-d1f1-11e6-a3e3-1418776a29a6/volumes/kubernetes.io~cephfs/cephfs ceph [name=admin,secret=secret]
```

Do the manager containers need to have the ceph-fs-common package installed in order to perform a successful mount?  I cannot find any further debugging information to determine the cause of the error.",closed,False,2017-01-03 20:59:38,2018-11-07 10:45:10
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/265,https://api.github.com/repos/kubernetes/kube-deploy/issues/265,AWS 1.5 image,"Most notably, a jump to docker 1.12.3",closed,True,2017-01-09 16:27:51,2017-02-07 18:02:10
kube-deploy,mayask,https://github.com/kubernetes/kube-deploy/pull/266,https://api.github.com/repos/kubernetes/kube-deploy/issues/266,Updated the list of possible ENV options,,closed,True,2017-01-11 11:53:23,2017-10-21 04:18:05
kube-deploy,charleschen,https://github.com/kubernetes/kube-deploy/pull/267,https://api.github.com/repos/kubernetes/kube-deploy/issues/267,allow BOOTSTRAP_DOCKER_DAEMON_PARAM to be passed to bootstrap daemon,"Let users pass parameters to docker bootstrap daemon.  For example, in my use case i need to specify the containerd flag (--containerd /var/run/containerd/containerd.sock).",closed,True,2017-01-14 01:19:37,2017-06-27 09:22:21
kube-deploy,fate-grand-order,https://github.com/kubernetes/kube-deploy/pull/268,https://api.github.com/repos/kubernetes/kube-deploy/issues/268,fix err information in local.go,,closed,True,2017-01-17 06:55:46,2017-11-23 02:01:38
kube-deploy,codeasone,https://github.com/kubernetes/kube-deploy/pull/269,https://api.github.com/repos/kubernetes/kube-deploy/issues/269,Add missing apt-get update,"Without the additional `apt-get update` provisioning of VMs fails for me with: 

```
==> master: ca-certificates is already the newest version (20160104ubuntu1).
==> master: The following packages will be upgraded:
==> master:   apt-transport-https
==> master: 1 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.
==> master: Need to get 25.7 kB of archives.
==> master: After this operation, 0 B of additional disk space will be used.
==> master: Err:1 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 apt-transport-https amd64 1.2.12~ubuntu16.04.1
==> master:   404  Not Found [IP: 91.189.91.26 80]
==> master: E: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/a/apt/apt-transport-https_1.2.12~ubuntu16.04.1_amd64.deb  404  Not Found [IP: 91.189.91.26 80]
==> master:
==> master: E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
```",closed,True,2017-01-18 17:25:08,2017-06-27 09:21:14
kube-deploy,cameronbrunner,https://github.com/kubernetes/kube-deploy/pull/270,https://api.github.com/repos/kubernetes/kube-deploy/issues/270,Update Navops Launch documentation,"This commit updates the Navops Launch documentation to match
the current state of the project.",closed,True,2017-01-24 18:23:24,2017-06-27 09:21:47
kube-deploy,morallo,https://github.com/kubernetes/kube-deploy/issues/271,https://api.github.com/repos/kubernetes/kube-deploy/issues/271,docker-multinode: unreliable check for the use of systemd ,"This one is mostly for the record, as I found the solution and I don't consider it a real bug in kube-deploy.

Environment: Debian 8 (jessie) with sysvinit (instead of the default systemd).

When trying to deploy kubernetes multinode with ./master.sh, I got several errors when the script tries to run containers in the docker daemon:

> +++ [0125 14:56:24] Restarting main docker daemon...
> Failed to get D-Bus connection: Unknown error -1
> cp: missing destination file operand after '.backup'
> Try 'cp --help' for more information.
> sed: -e expression #1, char 0: no previous regular expression
> sed: no input files
> Failed to get D-Bus connection: Unknown error -1
> Failed to get D-Bus connection: Unknown error -1
> Failed to get D-Bus connection: Unknown error -1
> +++ [0125 14:56:25] Restarted docker with the new flannel settings
> +++ [0125 14:56:25] Launching Kubernetes master components...
> 00698c3ff2131b60bdc4b5b4d332910f7a906f74fc1d3116c6ad1a90b677487b
> docker: Error response from daemon: linux mounts: Path /var/lib/kubelet is mounted on / but it is not a shared mount..

After reading some code in common.sh, I found out that it [checks for the *existence* of the `systemctl` command](https://github.com/kubernetes/kube-deploy/blob/master/docker-multinode/common.sh#L328) to assess if systemd is in use. In that case, it bypasses some steps necessary for other init systems.

When systemd was replaced by sysvinit in my environment, the systemd package was not removed and thus the command existed, but it obviously didn't work as intended. The solution was to remove the now unused systemd package.

Would it be possible to implement a more reliable check for the use of systemd?
",closed,False,2017-01-25 15:47:42,2017-11-01 22:53:53
kube-deploy,morallo,https://github.com/kubernetes/kube-deploy/issues/272,https://api.github.com/repos/kubernetes/kube-deploy/issues/272,docker-multinode: deployment fails in Debian 8 (memory cgroup disabled by default),"Memory cgroup is a hard requirement for Kubernetes.
It's not enabled in Debian by default.

When trying to use this deployment, `master.sh` and `worker.sh` run without errors. The docker containers for etcd, flannel and kubelet are correctly started.

However, the apiserver is never started inside the kubelet, presumably because of the missing memory cgroup.

> E0125 23:42:05.582421   47570 kubelet.go:1228] Failed to start ContainerManager system validation failed - Following Cgroup subsystem not mounted: [memory]

The error message is present inside the container logs (accessible with `docker logs <container_name>`), but it's not obvious to the user.

There is no *current* mention of this requirement in Kubernetes documentation, as the [old docker guide](https://github.com/kubernetes/community/blob/master/contributors/devel/local-cluster/docker.md#node-is-in-notready-state) has been [superseded by minikube](https://kubernetes.io/docs/getting-started-guides/docker/).

An explicit check for the required cgroups in the shell scripts would be desirable, or at least a mention in the documentation.",closed,False,2017-01-26 00:19:49,2017-11-01 22:53:58
kube-deploy,sstarcher,https://github.com/kubernetes/kube-deploy/pull/273,https://api.github.com/repos/kubernetes/kube-deploy/issues/273,fix directory path and ensure region is set,"* IMGBUILDER_DIRECTORY is an absolute path the .. needs to be at the end to go up a directory
* If the user is setting a region in ~/.aws/config the imagebuilder will break as it is only looking for the region in AWS_REGION and AWS_DEFAULT_REGION
",closed,True,2017-01-31 15:33:16,2017-01-31 15:37:55
kube-deploy,polizinha,https://github.com/kubernetes/kube-deploy/issues/274,https://api.github.com/repos/kubernetes/kube-deploy/issues/274,docker-multinode: pods at workers can't resolve dns,"Hello, any idea?

```
$ kubectl cluster-info
Kubernetes master is running at http://localhost:8080
KubeDNS is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubernetes-dashboard is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
```

```
$ ./worker.sh 
+++ [0201 16:50:47] K8S_VERSION is set to: v1.5.2
+++ [0201 16:50:47] ETCD_VERSION is set to: 3.0.4
+++ [0201 16:50:47] FLANNEL_VERSION is set to: v0.6.1
+++ [0201 16:50:47] FLANNEL_IPMASQ is set to: true
+++ [0201 16:50:47] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [0201 16:50:47] FLANNEL_BACKEND is set to: udp
+++ [0201 16:50:47] RESTART_POLICY is set to: unless-stopped
+++ [0201 16:50:47] MASTER_IP is set to: 192.168.xxx.xxx
+++ [0201 16:50:47] ARCH is set to: amd64
+++ [0201 16:50:47] IP_ADDRESS is set to: 45.33.xx.xxx
+++ [0201 16:50:47] USE_CNI is set to: false
+++ [0201 16:50:47] USE_CONTAINERIZED is set to: false
```

```
/ # cat /etc/resolv.conf 
search default.svc.cluster.local svc.cluster.local cluster.local members.linode.com
nameserver 10.0.0.10
options rotate
options ndots:5
/ # nslookup kubernetes
Server:    10.0.0.10
Address 1: 10.0.0.10

nslookup: can't resolve 'kubernetes'
```",closed,False,2017-02-01 17:23:14,2017-11-01 22:54:20
kube-deploy,apenney,https://github.com/kubernetes/kube-deploy/pull/275,https://api.github.com/repos/kubernetes/kube-deploy/issues/275,Add glusterfs support,"In order to use the glusterfs volume plugin we need the gluster
client available on the instances we run Kubernetes on.",closed,True,2017-02-03 19:03:07,2017-10-17 15:21:37
kube-deploy,mellotron,https://github.com/kubernetes/kube-deploy/issues/276,https://api.github.com/repos/kubernetes/kube-deploy/issues/276,"docker-multinode/arm64: kube-dns fails with ""CrashLoopBackOff""","Hello,

I've attempted to use the docker-multinode setup on my pine64 which is a arm64 machine. I did a git clone and then:

$ USE_CNI=true ./master.sh

I then get on the kubelet node:

$ ./kubectl get pods --all-namespaces                                     
NAMESPACE     NAME                                    READY     STATUS             RESTARTS   AGE
kube-system   k8s-master-172.17.0.10                  4/4       Running            2          35m
kube-system   k8s-proxy-v1-1m940                      1/1       Running            0          36m
kube-system   kube-addon-manager-172.17.0.10          2/2       Running            0          35m
kube-system   kube-dns-3365905565-5rjm3               2/4       CrashLoopBackOff   33         36m
kube-system   kubernetes-dashboard-1416335539-dwstr   1/1       Running            0          36m

If I do:

$ ./kubectl exec --namespace=kube-system -it kube-dns-3365905565-5rjm3 sh

I can see that kube-dns is PID 1:

/ # ps aux
PID   USER     TIME   COMMAND
    1 root       0:01 /kube-dns --domain=cluster.local. --dns-port=10053 --conf

If I try and run it, I get this error:

/ # ./kube-dns 
I0204 22:29:45.233211      22 dns.go:42] version: v1.6.0-alpha.0.876+545f749a0deb85-dirty
I0204 22:29:45.236124      22 server.go:107] Using https://10.0.0.1:443 for kubernetes master, kubernetes API: <nil>
I0204 22:29:45.262404      22 server.go:63] ConfigMap not configured, using values from command line flags
I0204 22:29:45.263377      22 server.go:113] FLAG: --alsologtostderr=""false""
I0204 22:29:45.264063      22 server.go:113] FLAG: --config-map=""""
I0204 22:29:45.264408      22 server.go:113] FLAG: --config-map-namespace=""kube-system""
I0204 22:29:45.264616      22 server.go:113] FLAG: --dns-bind-address=""0.0.0.0""
I0204 22:29:45.264791      22 server.go:113] FLAG: --dns-port=""53""
I0204 22:29:45.265049      22 server.go:113] FLAG: --domain=""cluster.local.""
I0204 22:29:45.265257      22 server.go:113] FLAG: --federations=""""
I0204 22:29:45.265479      22 server.go:113] FLAG: --healthz-port=""8081""
I0204 22:29:45.265656      22 server.go:113] FLAG: --kube-master-url=""""
I0204 22:29:45.265843      22 server.go:113] FLAG: --kubecfg-file=""""
I0204 22:29:45.266010      22 server.go:113] FLAG: --log-backtrace-at="":0""
I0204 22:29:45.266222      22 server.go:113] FLAG: --log-dir=""""
I0204 22:29:45.266455      22 server.go:113] FLAG: --log-flush-frequency=""5s""
I0204 22:29:45.266659      22 server.go:113] FLAG: --logtostderr=""true""
I0204 22:29:45.266836      22 server.go:113] FLAG: --stderrthreshold=""2""
I0204 22:29:45.266992      22 server.go:113] FLAG: --v=""0""
I0204 22:29:45.267104      22 server.go:113] FLAG: --version=""false""
I0204 22:29:45.267294      22 server.go:113] FLAG: --vmodule=""""
I0204 22:29:45.267759      22 server.go:155] Starting SkyDNS server (0.0.0.0:53)
I0204 22:29:45.271882      22 server.go:165] Skydns metrics enabled (/metrics:10055)
I0204 22:29:45.276679      22 logs.go:41] skydns: ready for queries on cluster.local. for tcp://0.0.0.0:53 [rcache 0]
I0204 22:29:45.277730      22 logs.go:41] skydns: ready for queries on cluster.local. for udp://0.0.0.0:53 [rcache 0]
I0204 22:29:45.545696      22 server.go:126] Setting up Healthz Handler (/readiness)
I0204 22:29:45.545944      22 server.go:131] Setting up cache handler (/cache)
I0204 22:29:45.546045      22 server.go:120] Status HTTP port 8081
F0204 22:29:45.546818      22 server.go:121] listen tcp :8081: bind: address already in use

This command:

$ ./kubectl cluster-info

...reports:

KubeDNS is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kube-dns

But if go to the IP and URI I get:

""message"": ""no endpoints available for service \""kube-dns\"""",",closed,False,2017-02-04 22:42:20,2017-11-01 22:54:26
kube-deploy,darrenmok,https://github.com/kubernetes/kube-deploy/issues/277,https://api.github.com/repos/kubernetes/kube-deploy/issues/277,docker-multinode: USE_CNI=true; all pods unable to resolve via DNS,"K8S_VERSION is set to: v1.5.2
ETCD_VERSION is set to: 3.0.14
FLANNEL_VERSION is set to: v0.6.1
FLANNEL_IPMASQ is set to: true
FLANNEL_NETWORK is set to: 10.1.0.0/16
FLANNEL_BACKEND is set to: udp
RESTART_POLICY is set to: unless-stopped
MASTER_IP is set to: localhost
ARCH is set to: amd64
IP_ADDRESS is set to: 172.18.12.100
USE_CNI is set to: true
USE_CONTAINERIZED is set to: false

All my pods services are unable to resolve anything via DNS if USE_CNI is set to true.

If USE_CNI set to false, master can't survive a reboot!
",closed,False,2017-02-10 02:50:14,2017-02-14 03:20:20
kube-deploy,roninby,https://github.com/kubernetes/kube-deploy/issues/278,https://api.github.com/repos/kubernetes/kube-deploy/issues/278,How to change the version of kubernetes-dashboard deployed by kube-addon-manager?,"Hello,

I'd like to change the kubernetes-dashboard from v1.4.0 to v1.4.1 in a deployed kubernetes cluster, the version of kubernetes-dashboard was originally set to v1.4.0 by kube-addon-manager.

It looks like the definition in the file /etc/kubenetes/addons/kubernetes-dashboard.yaml within the docker image gcr.io/google_containers/hyperkube-amd64:v1.4.3 should be changed, but I'm not sure if the kubernetes cluster have to be redeployed after commit the docker image.

Is there a smooth way to avoid cluster rebuilding?

Thanks.
 


",closed,False,2017-02-21 06:24:18,2017-11-02 19:02:46
kube-deploy,guillelb,https://github.com/kubernetes/kube-deploy/issues/279,https://api.github.com/repos/kubernetes/kube-deploy/issues/279,Can not reach the exposed services from another machine in the LAN,"I have installed Kubernetes in a private virtual enviroment with vmware following this simple guide (docker multinode): https://kubernetes.io/docs/getting-started-guides/docker-multinode/

I have deployed kubernetes in two **ubuntu16.04LTS** machines on the same LAN, one master (192.168.215.6/24) and one worker (192.168.215.7/24).

```
root@kubemaster6:~# ./kube-deploy/docker-multinode/master.sh 
+++ [0302 11:51:54] K8S_VERSION is set to: v1.5.3
+++ [0302 11:51:54] ETCD_VERSION is set to: 3.0.4
+++ [0302 11:51:54] FLANNEL_VERSION is set to: v0.6.1
+++ [0302 11:51:54] FLANNEL_IPMASQ is set to: true
+++ [0302 11:51:54] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [0302 11:51:54] FLANNEL_BACKEND is set to: udp
+++ [0302 11:51:54] RESTART_POLICY is set to: unless-stopped
+++ [0302 11:51:54] MASTER_IP is set to: localhost
+++ [0302 11:51:54] ARCH is set to: amd64
+++ [0302 11:51:54] IP_ADDRESS is set to: 192.168.215.6
+++ [0302 11:51:54] USE_CNI is set to: false
+++ [0302 11:51:54] USE_CONTAINERIZED is set to: false
+++ [0302 11:51:54] --------------------------------------------
+++ [0302 11:51:54] Killing all kubernetes containers...
+++ [0302 11:51:54] Launching docker bootstrap...
+++ [0302 11:51:55] Launching etcd...
36c6e984c791c1207471d56e6f5d6c0b6cbbb032ef001942878c08e381833d1a
+++ [0302 11:51:59] Launching flannel...
{""action"":""set"",""node"":{""key"":""/coreos.com/network/config"",""value"":""{ \""Network\"": \""10.1.0.0/16\"", \""Backend\"": {\""Type\"": \""udp\""}}"",""modifiedIndex"":4,""createdIndex"":4}}
b347243a70cebe752fc77689401eec790fc8a6a44ed90d1296153ae44027c632
+++ [0302 11:52:00] FLANNEL_SUBNET is set to: 10.1.12.1/24
+++ [0302 11:52:00] FLANNEL_MTU is set to: 1472
+++ [0302 11:52:00] Restarting main docker daemon...
+++ [0302 11:52:02] Restarted docker with the new flannel settings
+++ [0302 11:52:02] Launching Kubernetes master components...
7627853e95ace53612f8e03f2e8ebf61c6c74308ab0ee462e54e9d3631835013
+++ [0302 11:52:02] Done. It may take about a minute before apiserver is up.

root@minion7:~# export MASTER_IP=192.168.215.6
root@minion7:~# ./kube-deploy/docker-multinode/worker.sh 
+++ [0302 11:53:54] K8S_VERSION is set to: v1.5.3
+++ [0302 11:53:54] ETCD_VERSION is set to: 3.0.4
+++ [0302 11:53:54] FLANNEL_VERSION is set to: v0.6.1
+++ [0302 11:53:54] FLANNEL_IPMASQ is set to: true
+++ [0302 11:53:54] FLANNEL_NETWORK is set to: 10.1.0.0/16
+++ [0302 11:53:54] FLANNEL_BACKEND is set to: udp
+++ [0302 11:53:54] RESTART_POLICY is set to: unless-stopped
+++ [0302 11:53:54] MASTER_IP is set to: 192.168.215.6
+++ [0302 11:53:54] ARCH is set to: amd64
+++ [0302 11:53:54] IP_ADDRESS is set to: 192.168.215.7
+++ [0302 11:53:54] USE_CNI is set to: false
+++ [0302 11:53:54] USE_CONTAINERIZED is set to: false
+++ [0302 11:53:54] --------------------------------------------
+++ [0302 11:53:54] Killing all kubernetes containers...
+++ [0302 11:53:54] Launching docker bootstrap...
+++ [0302 11:53:55] Launching flannel...
8a5c412010fd4b6fd4e5510b2fc2a8092893deba5caa2e23c84046112512147f
+++ [0302 11:53:56] FLANNEL_SUBNET is set to: 10.1.18.1/24
+++ [0302 11:53:56] FLANNEL_MTU is set to: 1472
+++ [0302 11:53:56] Restarting main docker daemon...
+++ [0302 11:53:58] Restarted docker with the new flannel settings
+++ [0302 11:53:58] Launching Kubernetes worker components...
e7e894858c311f1e9c01c1943d390474cf846ee0c42d1f5f856852e235abc03b
+++ [0302 11:53:59] Done. After about a minute the node should be ready.

```

Everything seems to be perfect:
```
root@kubemaster6:~# kubectl get nodes
NAME            STATUS    AGE
192.168.215.6   Ready     2m
192.168.215.7   Ready     47s
```




Finally, I **deploy and expose an standard image of NGINX** and works fine at the external TCP **port 30177**:
```
root@kubemaster6:~# kubectl create -f deploymentYaml/nginx.yaml 
deployment ""nginx-deployment"" created
root@kubemaster6:~# kubectl expose deployment nginx-deployment --type=NodePort --name=example-service
service ""example-service"" exposed
root@kubemaster6:~# kubectl get services
NAME              CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
example-service   10.0.0.66    <nodes>       80:30177/TCP   11s
kubernetes        10.0.0.1     <none>        443/TCP        5m
root@kubemaster6:~# kubectl get pods --all-namespaces --output=wide
NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE       IP              NODE
default       nginx-deployment-148880595-2k316        1/1       Running   0          2m        10.1.12.4       192.168.215.6
kube-system   k8s-master-192.168.215.6                4/4       Running   14         18m       192.168.215.6   192.168.215.6
kube-system   k8s-proxy-v1-d5485                      1/1       Running   0          17m       192.168.215.7   192.168.215.7
kube-system   k8s-proxy-v1-zvs4x                      1/1       Running   0          19m       192.168.215.6   192.168.215.6
kube-system   kube-addon-manager-192.168.215.6        2/2       Running   0          18m       192.168.215.6   192.168.215.6
kube-system   kube-dns-4101612645-vdxk1               4/4       Running   0          19m       10.1.12.3       192.168.215.6
kube-system   kubernetes-dashboard-3543765157-xflvz   1/1       Running   0          19m       10.1.12.2       192.168.215.6
```


I can get the page perfectly from the master:
```
root@kubemaster6:~# curl localhost:30177
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
```

But not from the other machine of the LAN:
```
root@minion7:~# curl 192.168.215.6:30177
curl: (7) Failed to connect to 192.168.215.6 port 30177: Connection timed out

```
Am I doing something wrong?

The ports are UP and exposed to everyone:
```
root@kubemaster6:~# netstat -nlptu
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      18585/hyperkube 
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      19695/hyperkube 
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1348/sshd       
tcp6       0      0 :::10250                :::*                    LISTEN      18585/hyperkube 
tcp6       0      0 :::6443                 :::*                    LISTEN      19309/hyperkube 
tcp6       0      0 :::10251                :::*                    LISTEN      18887/hyperkube 
tcp6       0      0 :::2379                 :::*                    LISTEN      18244/etcd      
tcp6       0      0 :::10252                :::*                    LISTEN      18763/hyperkube 
tcp6       0      0 :::2380                 :::*                    LISTEN      18244/etcd      
tcp6       0      0 :::10255                :::*                    LISTEN      18585/hyperkube 
tcp6       0      0 :::8080                 :::*                    LISTEN      19309/hyperkube 
tcp6       0      0 :::22                   :::*                    LISTEN      1348/sshd       
tcp6       0      0 :::30177                :::*                    LISTEN      19695/hyperkube
tcp6       0      0 :::4001                 :::*                    LISTEN      18244/etcd      
tcp6       0      0 :::4194                 :::*                    LISTEN      18585/hyperkube 
udp        0      0 192.168.215.6:8285      0.0.0.0:*                           18306/flanneld  
```


But from the other machine inside the LAN, I don't see all of that open ports:
```
root@minion7:~# nmap 192.168.215.6
Starting Nmap 7.01 ( https://nmap.org ) at 2017-03-02 12:16 CET
Nmap scan report for 192.168.215.6
Host is up (0.000059s latency).
Not shown: 997 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
4001/tcp open  newoak
8080/tcp open  http-proxy
MAC Address: 00:0C:29:44:BD:1E (VMware)
Nmap done: 1 IP address (1 host up) scanned in 1.65 seconds
```

- Am I reaching the correct IP?
- Is correct that, in netstat appear ports on tcp6 (IPv6)?
- Do I need to configure something in order to connect from outside the master machine?

Thank you in advance!",closed,False,2017-03-03 10:43:00,2017-03-08 10:04:03
kube-deploy,berlogb,https://github.com/kubernetes/kube-deploy/issues/280,https://api.github.com/repos/kubernetes/kube-deploy/issues/280,./master.sh behind proxy.,"Hello team.
Tried to run ./master.sh but it fails with something that seems to be a proxy issue :

```
+++ [0309 23:53:34] Restarting main docker daemon...
+++ [0309 23:53:36] Restarted docker with the new flannel settings
+++ [0309 23:53:36] Launching Kubernetes master components...
Unable to find image 'gcr.io/google_containers/hyperkube-amd64:v1.5.4' locally
docker: Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp 74.125.195.82:443: i/o timeout.
See 'docker run --help'.
+++ [0309 23:54:22] Done. It may take about a minute before apiserver is up.

```
Tried all possible parameters I could think about :

```
env | grep -i proxy
KUBERNETES_HTTPS_PROXY=http://myproxy:port
http_proxy=http://myproxy:port
KUBERNETES_HTTP_PROXY=http:/myproxy:port
ALL_PROXY=http://myproxy:port
all_proxy=http://myproxy:port
HTTPS_PROXY=http://myproxy:port
https_proxy=http://myproxy:port
no_proxy=10.48.49.20,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,localhost,127.0.0.1
HTTP_PROXY=http://myproxy:port
KUBERNETES_NO_PROXY=127.0.0.1

```
No luck. 
Any hints what it needs to be ?
Thank you.",closed,False,2017-03-09 22:59:39,2017-03-09 23:22:45
kube-deploy,urban-1,https://github.com/kubernetes/kube-deploy/issues/281,https://api.github.com/repos/kubernetes/kube-deploy/issues/281,kube-dns on ubuntu,"Hi all,

I am experimenting with `docker-multinode` scripts. I have setup my master node and I am able to resolve cluster-internal names (with the correct /etc/resolv.conf). However, all external ones fail. I know where the problems is: 

- The host's /etc/resolv.conf has nameserver `127.0.1.1` since ubuntu is using dnsmasq when configured with NetworkManager
- This file is copied into the kube-dns container

As a result, when kube-dns cannot resolve the name itself it tries to access `127.0.1.1`. I see this running:

```
kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c kubedns
```

and the logs read:

```
I0312 14:54:15.334358       1 logs.go:41] skydns: failure to forward request ""read udp 127.0.0.1:41478->127.0.1.1:53: i/o timeout""
I0312 14:54:17.334626       1 logs.go:41] skydns: failure to forward request ""read udp 127.0.0.1:56155->127.0.1.1:53: i/o timeout""
I0312 14:54:19.334885       1 logs.go:41] skydns: failure to forward request ""read udp 127.0.0.1:50701->127.0.1.1:53: i/o timeout""
I0312 14:54:21.335167       1 logs.go:41] skydns: failure to forward request ""read udp 127.0.0.1:41831->127.0.1.1:53: i/o timeout""
I0312 14:54:23.335466       1 logs.go:41] skydns: failure to forward request ""read udp 127.0.0.1:40619->127.0.1.1:53: i/o timeout""
```

There is a _similar_ question on SO: http://stackoverflow.com/questions/36694769/kubernetes-skydns-failure-to-forward-request (Similar cause in my case is wrong config and not a routing issue). The accepted answer suggests editing RC config, but in latest version skydns is loaded automatically... 

Question(s): Is there a way to change this behavior when deploying the container? Maybe a parameter or an environment variable in `master.sh`? If not, can I modify this project's code to fix it? I had a quick look and I couldn't find were kube-dns is configured... I get the impression this has to be change in the main kubernetes project...




",closed,False,2017-03-12 15:03:21,2017-11-01 23:03:44
kube-deploy,vhsanche,https://github.com/kubernetes/kube-deploy/issues/282,https://api.github.com/repos/kubernetes/kube-deploy/issues/282,kubelet fails to start when performing ./master.sh,"It seems the current K8S_VERSION=$(curl -sSL ""https://storage.googleapis.com/kubernetes-release/release/stable.txt"") in common.sh renders to v1.6.0, which fails to start the hyperkube  container. After performing a ""docker logs <container_id>"", the --config flag is no longer supported in kubelet. I reverted back to v1.5.4 and everything seems to be working correctly.",closed,False,2017-04-01 23:38:12,2017-11-01 23:04:25
kube-deploy,luxas,https://github.com/kubernetes/kube-deploy/pull/283,https://api.github.com/repos/kubernetes/kube-deploy/issues/283,Remove the docker-multinode guide since it's not working anymore,"ref: https://goo.gl/VxSaKx

cc @jbeda @brendandburns @bgrant0607 @justinsb @mikedanese ",closed,True,2017-04-17 13:55:26,2017-04-19 16:57:36
kube-deploy,bahamoth,https://github.com/kubernetes/kube-deploy/pull/284,https://api.github.com/repos/kubernetes/kube-deploy/issues/284,fixed k8s version conflicts,"#282 
fixed kubelet  #deprecated flag --config to --pod-manifest-path",closed,True,2017-04-19 06:11:44,2017-06-27 09:20:14
kube-deploy,stevei101,https://github.com/kubernetes/kube-deploy/pull/285,https://api.github.com/repos/kubernetes/kube-deploy/issues/285,initial fork,- initial fork,closed,True,2017-04-22 12:06:14,2017-04-22 12:06:28
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/286,https://api.github.com/repos/kubernetes/kube-deploy/issues/286,Updated AWS template for 1.6 image,,closed,True,2017-05-02 15:08:40,2017-06-25 02:33:34
kube-deploy,cmparkinson,https://github.com/kubernetes/kube-deploy/issues/287,https://api.github.com/repos/kubernetes/kube-deploy/issues/287,Add xfsprogs to kops AMIs,"`xfsprogs` is required for Kubernetes to successfully mount EBS volumes containing XFS-formatted file systems. 

If the ask is reasonable, I'll happily submit a PR that adds the package to the templates.",closed,False,2017-06-15 13:30:30,2019-03-30 19:27:23
kube-deploy,edude03,https://github.com/kubernetes/kube-deploy/issues/288,https://api.github.com/repos/kubernetes/kube-deploy/issues/288,Old version of ixgbevf driver installed.,"I'm currently debugging an issue on my cluster where the network is disconnecting frequently and I've found that the template for creating the debian image purposefully removes the 2.16 version of the `ixgbevf` driver, leaving an unsupported version behind. 

```
sudo modinfo ixgbevf
filename:       /lib/modules/4.4.65-k8s/kernel/drivers/net/ethernet/intel/ixgbevf/ixgbevf.ko
version:        2.12.1-k
license:        GPL
description:    Intel(R) 10 Gigabit Virtual Function Network Driver
author:         Intel Corporation, <linux.nics@intel.com>
srcversion:     5AA37E00153CD4F19C91712
alias:          pci:v00008086d000015A8sv*sd*bc*sc*i*
alias:          pci:v00008086d00001565sv*sd*bc*sc*i*
alias:          pci:v00008086d00001515sv*sd*bc*sc*i*
alias:          pci:v00008086d000010EDsv*sd*bc*sc*i*
depends:
intree:         Y
vermagic:       4.4.65-k8s SMP mod_unload modversions
parm:           debug:Debug level (0=none,...,16=all) (int)
```
According to Amazon: 

```
In the above Ubuntu instance, the module is installed, but the version is 2.11.3-k, which does not have all of the latest bug fixes that the recommended version 2.14.2 does. In this case, the ixgbevf module would work, but a newer version can still be installed and loaded on the instance for the best experience.
```


From [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sriov-networking.html#enhanced-networking-ubuntu](here)

Can we get the newer version included in the image? ",closed,False,2017-07-11 21:39:47,2018-04-01 01:03:50
kube-deploy,itskingori,https://github.com/kubernetes/kube-deploy/pull/289,https://api.github.com/repos/kubernetes/kube-deploy/issues/289,Improve imagebuilder setup and documentation,"Main objective is to remove the script in `hack/setup-aws.sh` and replace it using a Terraform plan (motivation explain in commit fe6e48724ff49948410e849c99e7282e2876c4f1).

I've also updated the `Makefile` to make the new process more logical and seemingly simpler. Evidence is in the resulting `README` (which I also edited to improve the flow of how one gets from A to B).",closed,True,2017-07-29 20:21:34,2018-12-31 14:33:39
kube-deploy,gekart,https://github.com/kubernetes/kube-deploy/issues/290,https://api.github.com/repos/kubernetes/kube-deploy/issues/290,AMI snapshot should be public,"When using AWS it is possible to launch k8s AMIs, but is not possible to copy them - instead getting this error message: ""You do not have permission to access the storage of this ami"".

Although the AMI itself is public, the underlying snapshot is not public, therefore preventing copying, and preventing the ability to copy the snapshot to an encrypted snapshot in order to enable root volume encryption.

Compare to https://github.com/coreos/bugs/issues/1090

Can be made public manually in the AWS console or in `ensurePublic` with something like:

```
request := &ec2.ModifySnapshotAttributeInput{
    Attribute: aws.String(""createVolumePermission""),
    GroupNames: []*string{
        aws.String(""all""),
    },
    OperationType: aws.String(""add""),
    SnapshotId:    aws.String(i.snapshotID),
}

result, err := svc.ModifySnapshotAttribute(request)
```",closed,False,2017-08-10 08:50:11,2018-01-31 09:09:03
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/291,https://api.github.com/repos/kubernetes/kube-deploy/issues/291,Create k8s 1.7 image,,closed,True,2017-08-17 16:42:00,2017-08-17 16:43:59
kube-deploy,bcorijn,https://github.com/kubernetes/kube-deploy/pull/292,https://api.github.com/repos/kubernetes/kube-deploy/issues/292,Add ENA support by updating dependencies,"In light of https://github.com/kubernetes/kops/issues/1558, I tried adding ENA support to the KOPS images. In the end the easiest/best way in my eyes was just updating the underlying dependencies, where this was already included. This took included the following steps:
- Pull bootstrap-vz from upstream instead of from @justinsb's fork
- Add the new boto3 requirement
- Use the boto3 syntax for the AWS keys
- Update the version number of the ixgbevf to be removed - we prefer to use the in-kernel driver (see issue linked above)
- Use the latest debian 8.7 jessie images to start from, adding a few new regions while doing so

The upstream master already includes config to install the latest [AWS ENA drivers](https://github.com/andsens/bootstrap-vz/blob/master/bootstrapvz/providers/ec2/tasks/network.py#L126) and set the [--ena-support flag](https://github.com/andsens/bootstrap-vz/blob/master/bootstrapvz/providers/ec2/tasks/ami.py#L128) on the AMI 

An image built with this config from my account cant be found in us-east-1 as ami-7cdee507 (which should be public).

Output of relevant modules on a R4.large instance: 
```
admin:~$ sudo modinfo ena
filename:       /lib/modules/4.4.78-k8s/updates/dkms/ena.ko
version:        1.2.0g
license:        GPL
description:    Elastic Network Adapter (ENA)
author:         Amazon.com, Inc. or its affiliates
srcversion:     6FE02175A610B0C9723C28E
alias:          pci:v00001D0Fd0000EC21sv*sd*bc*sc*i*
alias:          pci:v00001D0Fd0000EC20sv*sd*bc*sc*i*
alias:          pci:v00001D0Fd00001EC2sv*sd*bc*sc*i*
alias:          pci:v00001D0Fd00000EC2sv*sd*bc*sc*i*
depends:
vermagic:       4.4.78-k8s SMP mod_unload modversions
parm:           debug:Debug level (0=none,...,16=all) (int)
admin:~$ sudo modinfo ixgbevf
filename:       /lib/modules/4.4.78-k8s/kernel/drivers/net/ethernet/intel/ixgbevf/ixgbevf.ko
version:        2.12.1-k
license:        GPL
description:    Intel(R) 10 Gigabit Virtual Function Network Driver
author:         Intel Corporation, <linux.nics@intel.com>
srcversion:     5AA37E00153CD4F19C91712
alias:          pci:v00008086d000015A8sv*sd*bc*sc*i*
alias:          pci:v00008086d00001565sv*sd*bc*sc*i*
alias:          pci:v00008086d00001515sv*sd*bc*sc*i*
alias:          pci:v00008086d000010EDsv*sd*bc*sc*i*
depends:
intree:         Y
vermagic:       4.4.78-k8s SMP mod_unload modversions
parm:           debug:Debug level (0=none,...,16=all) (int)
admin:~$ sudo ethtool -i eth0
driver: ena
version: 1.2.0g
firmware-version:
bus-info: 0000:00:03.0
supports-statistics: yes
supports-test: no
supports-eeprom-access: no
supports-register-dump: no
supports-priv-flags: no
```",closed,True,2017-08-21 09:33:31,2017-11-23 02:09:41
kube-deploy,qqshfox,https://github.com/kubernetes/kube-deploy/issues/293,https://api.github.com/repos/kubernetes/kube-deploy/issues/293,imagebuilder fails to query the newly registered AMI,"`imagebuilder` may complain `image not found after build` and the execution failes. But from the logs ahead the exception, we can find the AMI has been registered actually. It seems that the AMI newly created not available yet despite `bootstrap-vz` claims so.

```console
Removing volume partitions mapping
Executing: kpartx -ds /dev/xvdf
Detaching the volume
Deleting mountpoint for the bootstrap volume
Creating a snapshot of the EBS volume
Registering the image as an AMI
Deleting the volume
Deleting workspace
Successfully completed bootstrapping
I0911 14:53:04.619685   89132 interface.go:75] Executing command: [""rm"" ""-rf"" ""/tmp/imagebuilder-8294145453372472625""]
I0911 14:53:04.657912   89132 interface.go:83] Output was:
I0911 14:53:04.657990   89132 aws.go:532] AWS DescribeImages Filter:Name=""k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-09-11"", Owner=self
F0911 14:53:04.827255   89132 main.go:240] image not found after build: ""k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-09-11""
goroutine 1 [running]:
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.stacks(0xc420085f00, 0xc4200a4000, 0x79, 0xc7)
	/Users/hanfei/workspace/go/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:769 +0xcf
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.(*loggingT).output(0x1b55c20, 0xc400000003, 0xc4200b62c0, 0x1b01b13, 0x7, 0xf0, 0x0)
	/Users/hanfei/workspace/go/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:720 +0x345
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.(*loggingT).printf(0x1b55c20, 0x3, 0x177448a, 0x1f, 0xc42012bcb8, 0x1, 0x1)
	/Users/hanfei/workspace/go/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:655 +0x14c
k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog.Fatalf(0x177448a, 0x1f, 0xc42012bcb8, 0x1, 0x1)
	/Users/hanfei/workspace/go/src/k8s.io/kube-deploy/imagebuilder/vendor/github.com/golang/glog/glog.go:1148 +0x67
main.main()
	/Users/hanfei/workspace/go/src/k8s.io/kube-deploy/imagebuilder/main.go:240 +0x1a20
```

Wait one minute or so, the AMI should be available finally.

```console
$ aws ec2 describe-images --filters Name=name,Values=k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-09-11
{
    ""Images"": []
}
$ aws ec2 describe-images --filters Name=name,Values=k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-09-11
{
    ""Images"": []
}
$ aws ec2 describe-images --filters Name=name,Values=k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-09-11
{
    ""Images"": [
        {
            ""VirtualizationType"": ""hvm"",
            ""Name"": ""k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-09-11"",
            ""Hypervisor"": ""xen"",
            ""SriovNetSupport"": ""simple"",
            ""ImageId"": ""ami-775f8f1a"",
            ""State"": ""available"",
            ...
            ""Description"": ""Kubernetes 1.7 Base Image - Debian jessie amd64""
        }
    ]
}
```",closed,False,2017-09-11 07:32:20,2018-04-01 01:03:51
kube-deploy,gekart,https://github.com/kubernetes/kube-deploy/issues/294,https://api.github.com/repos/kubernetes/kube-deploy/issues/294,Use versioned kernel references,"In

```
       - [ 'chroot', '{root}', 'apt-get', 'install', '--yes', 'linux-image-k8s', 'linux-headers-k8s' ]
```

it is not easy to see what is hidden behind linux-image-k8s. Using a versioned linux-image-4.4.78-k8s is more appropriate.",closed,False,2017-09-22 08:03:57,2018-04-01 01:03:51
kube-deploy,gekart,https://github.com/kubernetes/kube-deploy/pull/295,https://api.github.com/repos/kubernetes/kube-deploy/issues/295,Added support for making snapshots public.,As discussed in https://github.com/kubernetes/kube-deploy/issues/290 this enables encrypting root volumes for kops users.,closed,True,2017-09-30 21:10:25,2017-11-23 02:03:15
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/296,https://api.github.com/repos/kubernetes/kube-deploy/issues/296,Remove the docs directory,Since each subdirectory is a standalone project it doesn't make sense to have a root level docs directory.,closed,True,2017-10-17 19:00:22,2017-10-21 04:08:19
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/297,https://api.github.com/repos/kubernetes/kube-deploy/issues/297,Initial commit for a cluster-api directory.,,closed,True,2017-10-18 17:48:39,2017-10-21 04:11:55
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/298,https://api.github.com/repos/kubernetes/kube-deploy/issues/298,Minimalistic Machines API proposal.,"This is a proposal to add a new API for managing Nodes in a declarative way: Machines.

It is part of the overall Cluster API effort.",closed,True,2017-10-19 07:06:32,2018-02-07 18:58:45
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/299,https://api.github.com/repos/kubernetes/kube-deploy/issues/299,Cluster API init,Builds and runs fine. I'll add a README file tomorrow.,closed,True,2017-10-19 17:50:13,2017-11-06 08:07:09
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/300,https://api.github.com/repos/kubernetes/kube-deploy/issues/300,Remove binary file cluster-api,This is generated by go build and added by mistake.,closed,True,2017-10-20 17:56:34,2017-10-20 17:56:43
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/301,https://api.github.com/repos/kubernetes/kube-deploy/issues/301,Add delete cluster command,Add delete cluster command,closed,True,2017-10-20 22:05:44,2017-10-20 22:05:52
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/302,https://api.github.com/repos/kubernetes/kube-deploy/issues/302,Update cluster-api/README with build and run instructions,,closed,True,2017-10-20 22:29:15,2017-10-23 20:26:36
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/303,https://api.github.com/repos/kubernetes/kube-deploy/issues/303,Delete directories that only had placeholders to new repositories,Since those repositories are now well established and the forward links are no longer useful.,closed,True,2017-10-21 04:12:20,2017-10-21 04:20:47
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/304,https://api.github.com/repos/kubernetes/kube-deploy/issues/304,Remove the navops documentation,"Since it hasn't been updated in nine months.

/assign @roblalonde @cameronbrunner -- do you guys want to keep this directory?",closed,True,2017-10-21 04:24:19,2018-01-24 17:40:50
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/305,https://api.github.com/repos/kubernetes/kube-deploy/issues/305,Remove the addon manager code,"Which was copied here from the kubernetes/kubernetes repository but has since not been kept up to date. In particular, this code was prior to the change to use kubectl apply (https://github.com/kubernetes/kubernetes/pull/34513) which pretty significantly changed the code flow.",closed,True,2017-10-23 04:49:28,2018-01-24 17:40:54
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/306,https://api.github.com/repos/kubernetes/kube-deploy/issues/306,A first pass at a minimal cluster control-plane API,,closed,True,2017-10-23 20:27:48,2017-12-22 12:39:17
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/307,https://api.github.com/repos/kubernetes/kube-deploy/issues/307,stub files for machine controller,,closed,True,2017-10-24 20:54:11,2017-10-24 21:08:52
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/308,https://api.github.com/repos/kubernetes/kube-deploy/issues/308,Adding types to temp location so they can be used,These types are still under review in a separate PR.,closed,True,2017-10-24 21:09:22,2017-10-25 20:41:40
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/309,https://api.github.com/repos/kubernetes/kube-deploy/issues/309,Machines API: CRDs and sample controller,"This PR has three commits:

1. An enormous update to our vendored packages, since I added more dependencies
2. Restructure the Machines API types to make them more useful
3. Add example CRD creation and example Machines controller

I demoed this all at today's Cluster API community breakout meeting, so if anyone missed it and wants to see these in action, the recording should be posted soon.

I want to vet that the first commit doesn't break anyone else's code before merging, but the other two should be safe to merge so we can start building on top of them or using them as examples. I'll sync with @ajitak and @jessicaochen to verify.",closed,True,2017-10-25 21:08:01,2017-10-25 21:35:42
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/310,https://api.github.com/repos/kubernetes/kube-deploy/issues/310,Add upgrade param in cmd tool,Add upgrade option in cmd util. Mostly copied from create option. The next step will be updating machine object to perform update.,closed,True,2017-10-28 00:07:19,2017-10-31 20:42:05
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/311,https://api.github.com/repos/kubernetes/kube-deploy/issues/311,Implement a client package for CRUD access for machine object.,"It's implemented by wrapping around RESTFul API by following the
standard k8s object pattern.",closed,True,2017-10-31 17:17:24,2017-10-31 20:55:14
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/312,https://api.github.com/repos/kubernetes/kube-deploy/issues/312,GCE machines controller prototype,"Super rough prototype for now, to mark/share progress.

- Shows how to create/version private `ProviderConfig` types.
- Actually creates/deletes VMs, but ignores updates for now.
  - Does not install/configure Kubernetes on the VM yet.
- Completely ignores race conditions and proper reconcilation.

Broken into two commits: one for vendor changes, the other for actual new code (in `examples/gce-machines-controller`).",closed,True,2017-10-31 18:38:48,2017-10-31 18:43:48
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/313,https://api.github.com/repos/kubernetes/kube-deploy/issues/313,stub for machine actuator and vendor updates,,closed,True,2017-10-31 18:48:09,2017-10-31 18:48:26
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/314,https://api.github.com/repos/kubernetes/kube-deploy/issues/314,Quick plumbing for GCE VM startup scripts.,"Also, adding a few gitignore files to ignore binaries.",closed,True,2017-10-31 20:47:24,2017-10-31 20:47:35
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/315,https://api.github.com/repos/kubernetes/kube-deploy/issues/315,machine controller logic with stubbed actuator,,closed,True,2017-10-31 20:47:40,2017-10-31 20:48:08
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/316,https://api.github.com/repos/kubernetes/kube-deploy/issues/316,Example error status on invalid configuration.,,closed,True,2017-10-31 21:07:59,2017-10-31 21:08:09
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/317,https://api.github.com/repos/kubernetes/kube-deploy/issues/317,Add an empty upgrader place holder.,,closed,True,2017-10-31 21:34:03,2017-10-31 21:34:33
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/318,https://api.github.com/repos/kubernetes/kube-deploy/issues/318,concrete google machine actuator,,closed,True,2017-10-31 22:27:14,2017-10-31 22:27:23
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/319,https://api.github.com/repos/kubernetes/kube-deploy/issues/319,Making Cluster types more useable.,"- Moved types to `api/cluster/v1alpha1`
- Added JSON annotations
- Added `ClusterList` to support list APIs for controllers
- Generated DeepCopy functions

Overall, this mimics what I did in https://github.com/kubernetes/kube-deploy/pull/309/commits/39317ff606fcee039d15f58753a8fb734665bed9 with the Machines types to make them actually useable.

CC @medinatiger @krousey ",closed,True,2017-11-01 20:45:09,2017-11-01 20:58:28
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/320,https://api.github.com/repos/kubernetes/kube-deploy/issues/320,Implement client-go library for Cluster{} in ClusetrAPI,,closed,True,2017-11-01 22:03:21,2017-11-01 22:04:18
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/321,https://api.github.com/repos/kubernetes/kube-deploy/issues/321,First cut of upgrader,,closed,True,2017-11-02 00:01:07,2017-11-02 00:01:19
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/322,https://api.github.com/repos/kubernetes/kube-deploy/issues/322,Move cluster-api/machinecontroller/cloud to cluster-api/cloud,@jessicaochen  FYI.,closed,True,2017-11-02 00:45:11,2017-11-02 03:38:58
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/323,https://api.github.com/repos/kubernetes/kube-deploy/issues/323,Add new nodes to kubernetes cluster,,closed,True,2017-11-02 18:07:20,2017-11-02 18:08:34
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/324,https://api.github.com/repos/kubernetes/kube-deploy/issues/324,Use global scheme for client library.,,closed,True,2017-11-02 18:08:25,2017-11-02 18:10:12
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/325,https://api.github.com/repos/kubernetes/kube-deploy/issues/325,supress uneeded flag warning,,closed,True,2017-11-02 18:18:42,2017-11-02 18:19:00
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/326,https://api.github.com/repos/kubernetes/kube-deploy/issues/326,add structure for upcoming machine controller work,,closed,True,2017-11-02 18:35:49,2017-11-02 18:35:59
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/327,https://api.github.com/repos/kubernetes/kube-deploy/issues/327,Do not update machine on changes like linking the node,,closed,True,2017-11-02 19:05:07,2017-11-02 19:05:23
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/328,https://api.github.com/repos/kubernetes/kube-deploy/issues/328,Use machines.yaml to create machine crd object during cluster creation.,This doesn't create master vm as per machines.yaml. This will be fixed later https://github.com/kubernetes/kube-deploy/issues/329.,closed,True,2017-11-02 20:22:45,2017-11-02 20:28:15
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/329,https://api.github.com/repos/kubernetes/kube-deploy/issues/329,Create master vm as per machine configuration specified in machines.yaml,Currently cluster-api create command ignores machines.yaml while creating master vm. Create master vm as per machine configuration specified in machines.yaml,closed,False,2017-11-02 20:27:39,2017-11-08 18:50:40
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/330,https://api.github.com/repos/kubernetes/kube-deploy/issues/330,cleanup node before machine deletion,,closed,True,2017-11-02 22:27:38,2017-11-04 04:02:06
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/331,https://api.github.com/repos/kubernetes/kube-deploy/issues/331,Some tweaks for the upgrader.,,closed,True,2017-11-02 23:21:18,2017-11-02 23:22:58
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/332,https://api.github.com/repos/kubernetes/kube-deploy/issues/332,Example client-side MachineSet tool.,"Before MachineSets are a part of the Machines API, this is a demo to show how they can be simulated client-side. A single set is defined by a label selector, and Machines are created or destroyed to match the target replicas.

Example usages:

    $ machineset --label set=us-central1-c --replicas 10
    $ machineset --label gpu=true --replicas 2
    $ machineset --label role=node --replicas 25 --kubeconfig ~/kubeconfig",closed,True,2017-11-02 23:26:27,2017-11-02 23:27:01
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/333,https://api.github.com/repos/kubernetes/kube-deploy/issues/333,Make it easy to build/push Docker image for machinecontroller.,,closed,True,2017-11-02 23:50:39,2017-11-02 23:51:11
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/334,https://api.github.com/repos/kubernetes/kube-deploy/issues/334,Move/merge the upgrader into a subcommand of cluster-api util.,,closed,True,2017-11-03 23:29:47,2017-11-06 21:36:42
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/335,https://api.github.com/repos/kubernetes/kube-deploy/issues/335,Switch vendored kubicorn to master branch.,Apparently it was on the `azure` branch. It looks like this comes with improved ssh-agent support.,closed,True,2017-11-04 02:06:36,2017-11-04 04:01:52
kube-deploy,mvladev,https://github.com/kubernetes/kube-deploy/pull/336,https://api.github.com/repos/kubernetes/kube-deploy/issues/336,Remove k8s.io/kubernetes dependency from the API,We should not rely on `k8s.io/kubernetes` at all.,closed,True,2017-11-04 12:16:04,2017-11-09 20:09:49
kube-deploy,mvladev,https://github.com/kubernetes/kube-deploy/pull/337,https://api.github.com/repos/kubernetes/kube-deploy/issues/337,Change api's group to cluster.k8s.io,"Rationale:
- tautology - we know that the group will hold API definitions.
- consistency - all other main k8s groups have a single word subdomain:
  storage.k8s.io, networking.k8s.io and etc..
- bug in [code-generator][0] that prevents clientset generation when
  hyphen is used in the last subdomain of the group.
- do it early that late.

Repercussions for this change will be that existing API resources needs
to be recreated - CRDs, Machines and Clusters.

[0]: https://github.com/kubernetes/code-generator/issues/22",closed,True,2017-11-06 07:00:05,2017-11-09 20:08:21
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/338,https://api.github.com/repos/kubernetes/kube-deploy/issues/338,Remove panic in deploy package,,closed,True,2017-11-06 23:44:58,2017-11-06 23:52:07
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/339,https://api.github.com/repos/kubernetes/kube-deploy/issues/339,Highlight experimental nature of the code in README,,closed,True,2017-11-06 23:57:39,2017-11-06 23:57:55
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/340,https://api.github.com/repos/kubernetes/kube-deploy/issues/340,Install machine controller on cluster creation (optional).,"This is hidden behind a new `--enable-machine-controller` flag while I work out any kinks, and only works on GCE. It creates a service account, gives it the `compute.instanceAdmin.v1` role, imports its credentials into a cluster secret, and creates a pod on the master for the machine controller.",closed,True,2017-11-07 18:52:09,2017-11-07 18:53:35
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/341,https://api.github.com/repos/kubernetes/kube-deploy/issues/341,Rename machinecontroller to machine-controller.,This makes the naming more consistent with other components.,closed,True,2017-11-07 19:12:13,2017-11-07 19:18:59
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/342,https://api.github.com/repos/kubernetes/kube-deploy/issues/342,Add license boilerplate to go files.,,closed,True,2017-11-07 19:27:03,2017-11-07 19:27:17
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/343,https://api.github.com/repos/kubernetes/kube-deploy/issues/343,Centralize gitignore file.,,closed,True,2017-11-07 19:32:20,2017-11-07 19:32:39
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/344,https://api.github.com/repos/kubernetes/kube-deploy/issues/344,Add default MachineSet labels for demo purposes.,,closed,True,2017-11-07 19:49:19,2017-11-07 19:53:55
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/345,https://api.github.com/repos/kubernetes/kube-deploy/issues/345,Have the machine-controller skip masters (for now).,"For the initial prototype, we've agreed to have the client-side creation tool provision the master host, and to not support multiple master hosts in the cluster (for now). Because of this, the machine-controller should skip Machines with a Master role (for now).

Also, run the code through gofmt.",closed,True,2017-11-07 19:57:29,2017-11-07 19:57:45
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/346,https://api.github.com/repos/kubernetes/kube-deploy/issues/346,Add make target for fixing image permissions.,"Until we fix the image bucket defaults, we can run this target after pushing new images to make sure they are publicly readable.",closed,True,2017-11-07 20:04:17,2017-11-07 20:04:25
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/347,https://api.github.com/repos/kubernetes/kube-deploy/issues/347,machine-controller: handle machine updates in parallel,"Right now, the `machine-controller` handles all Machine events synchronously. If you update five machines at once, it will slowly process each event, one at a time, waiting for each reconciliation to become fully resolved.

Instead, it would be nice to use goroutines to handle different Machine updates in parallel.",closed,False,2017-11-07 20:06:36,2017-12-21 19:53:52
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/348,https://api.github.com/repos/kubernetes/kube-deploy/issues/348,machine-controller: link Machine and Node objects,"After a Machine's VM has been created, and the Node object gets registered, we need a mechanism to update the Machine's Status to reference the Node, and for the Node's OwnerRef to point to the Machine.",closed,False,2017-11-07 20:07:39,2017-11-15 22:47:40
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/349,https://api.github.com/repos/kubernetes/kube-deploy/issues/349,machine-controller: update Machine.Status when appropriate,"So far, it doesn't update the Machine's Status at all.",closed,False,2017-11-07 20:08:16,2018-04-12 16:42:00
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/350,https://api.github.com/repos/kubernetes/kube-deploy/issues/350,machine-controller: proper Machine reconciliation,"The machine-controller's event handling is very naive right now: on create events it will always try to start a new VM, on delete events it will always try to delete, and on updates it will always try to delete and then create the same VM.

This doesn't account for Machine objects that exist prior to the machine-controller starting, or if the controller crashes, or many updates for the same Machine in a row that could be batched, or any number of other edge cases. A more sophisticated model would be to treat any event as a need to reconcile, and always check the real world state to figure out what needs to be done for the Machine.",closed,False,2017-11-07 20:12:21,2017-12-21 19:48:22
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/351,https://api.github.com/repos/kubernetes/kube-deploy/issues/351,Start using newer Cluster API proposal types,"Our prototype cluster creator is still using a very simple, placeholder [cluster definition](https://github.com/kubernetes/kube-deploy/blob/a29f1e6467884ea65bf99309df4ee08899382949/cluster-api/api/Cluster.go). It should start using the new types that are under review [here](https://github.com/kubernetes/kube-deploy/pull/306), but checked in [here](https://github.com/kubernetes/kube-deploy/blob/a29f1e6467884ea65bf99309df4ee08899382949/cluster-api/api/machines/v1alpha1/types.go) to code against.",closed,False,2017-11-07 20:54:11,2017-11-08 18:49:54
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/352,https://api.github.com/repos/kubernetes/kube-deploy/issues/352,machine-controller: respect Machine.Spec,"The GCE support in machine-controller respects the Machine's `ProviderConfig` for the basic properties of the VMs it creates, but none of the rest of the `Spec` is honored when installing/configuring Kubernetes on the host.",closed,False,2017-11-07 20:57:20,2017-11-22 04:06:53
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/353,https://api.github.com/repos/kubernetes/kube-deploy/issues/353,Updating the cluster API from the review and fixing cluster upgrade,,closed,True,2017-11-07 21:16:19,2017-11-07 21:16:48
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/354,https://api.github.com/repos/kubernetes/kube-deploy/issues/354,Honor kubelet version when creating GCE VMs.,Part of https://github.com/kubernetes/kube-deploy/issues/352.,closed,True,2017-11-07 22:05:27,2017-11-07 22:05:42
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/355,https://api.github.com/repos/kubernetes/kube-deploy/issues/355,machine-controller: register finalizer,"The machine-controller should register a finalizer with every Machine it's responsible for, so that clients who delete Machine objects can watch their status for errors and be confident that the corresponding Node has been released when the Machine object deletion actually occurs.",closed,False,2017-11-07 22:11:46,2018-02-13 00:53:03
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/356,https://api.github.com/repos/kubernetes/kube-deploy/issues/356,Have nodes annotate themselves with machine name.,"This will make it easier for the machine-controller to link Machine and Node objects together, even if the names don't match (like if we choose to create-then-delete VMs on updates).

Part of https://github.com/kubernetes/kube-deploy/issues/348.",closed,True,2017-11-07 22:58:04,2017-11-07 22:58:54
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/357,https://api.github.com/repos/kubernetes/kube-deploy/issues/357,Wait for master update completion before updating the nodes,,closed,True,2017-11-08 00:58:38,2017-11-08 01:14:49
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/358,https://api.github.com/repos/kubernetes/kube-deploy/issues/358,Create VM for master using GCE API,Currently it creates master vm with name provided in machine yaml and doesn't know how to deal with generatename.,closed,True,2017-11-08 07:57:08,2017-11-08 08:32:32
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/359,https://api.github.com/repos/kubernetes/kube-deploy/issues/359,Use clusterv1.Cluster object to create cluster and Update readme,,closed,True,2017-11-08 09:16:31,2017-11-08 09:16:46
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/360,https://api.github.com/repos/kubernetes/kube-deploy/issues/360,cluster-installer: implement delete command,,closed,False,2017-11-08 18:51:07,2017-11-09 18:34:29
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/361,https://api.github.com/repos/kubernetes/kube-deploy/issues/361,cluster-installer: remove kubicorn dependency,,closed,False,2017-11-08 18:51:44,2017-11-08 21:46:49
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/362,https://api.github.com/repos/kubernetes/kube-deploy/issues/362,"Fix build, add more methods to logging actuator",,closed,True,2017-11-08 19:55:41,2017-11-16 01:59:45
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/363,https://api.github.com/repos/kubernetes/kube-deploy/issues/363,Remove kubicorn dependency,,closed,True,2017-11-08 21:44:32,2017-11-08 21:46:40
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/364,https://api.github.com/repos/kubernetes/kube-deploy/issues/364,Fix our example yaml files.,"The JSON `providerConfig` embedded in `cluster.yaml` failed decoding, and the `machines.yaml` master `name` didn't match its `generateName`.",closed,True,2017-11-08 22:16:49,2017-11-08 22:17:12
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/365,https://api.github.com/repos/kubernetes/kube-deploy/issues/365,Move the machine types to the cluster API group,,closed,True,2017-11-08 22:45:29,2017-11-08 22:45:57
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/366,https://api.github.com/repos/kubernetes/kube-deploy/issues/366,Fix bug in CRD creation code.,This was preventing the installer from actually creating clusters.,closed,True,2017-11-09 00:07:17,2017-11-09 00:08:05
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/367,https://api.github.com/repos/kubernetes/kube-deploy/issues/367,"Get service account projects from Machines, not Cluster.","When creating a service account for the machine-controller, rather than requiring a `Project` be defined in the `Cluster` object `ProviderConfig`, instead look at the `ProviderConfig`s of all of the `Machine` objects, and add permissions for them all for the service account.

Also, change from using `[]Machine` to `[]*Machine` most places to play nicely with other code.",closed,True,2017-11-09 00:16:11,2017-11-09 00:16:56
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/368,https://api.github.com/repos/kubernetes/kube-deploy/issues/368,Implement delete command,,closed,True,2017-11-09 18:11:00,2017-11-09 18:34:12
kube-deploy,mvladev,https://github.com/kubernetes/kube-deploy/pull/369,https://api.github.com/repos/kubernetes/kube-deploy/issues/369,Client generation,"As discussed during the Weekly meeting here is the PR for code generation.

- removed `k8s.io/kubernetes` dependency (over 2 million LoC removed).
- changed the group of the cluster api `cluster-api.k8s.io` -> `cluster.k8s.io`. b387080 for more details.
- added code-generation for `deepcopy`, `client`,  `informer` and `lister` via `./hack/generate.sh`",closed,True,2017-11-09 20:02:17,2017-11-20 19:59:06
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/370,https://api.github.com/repos/kubernetes/kube-deploy/issues/370,Replace log with glog,,closed,True,2017-11-09 22:52:11,2017-11-10 01:48:44
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/371,https://api.github.com/repos/kubernetes/kube-deploy/issues/371,cluster-installer: add a provider flag,Add a provider flag for aws/google/azure,closed,False,2017-11-09 23:49:17,2017-11-09 23:50:21
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/372,https://api.github.com/repos/kubernetes/kube-deploy/issues/372,Add a provider flag,,closed,True,2017-11-09 23:49:33,2017-11-09 23:50:13
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/373,https://api.github.com/repos/kubernetes/kube-deploy/issues/373,cluster-api installer should honor control plane version,"On GCE, it looks like the installer always installs kubeadm 1.7.0 and calls `kubeadm init` without overriding `kubernetes-version`:

https://github.com/kubernetes/kube-deploy/blob/master/cluster-api/cloud/google/templates.go#L105

It should use the versions specified in Cluster API.",closed,False,2017-11-10 00:31:48,2017-11-10 23:27:08
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/374,https://api.github.com/repos/kubernetes/kube-deploy/issues/374,Add machineset usage info,"The `machineset` demo isn't very friendly now: it panics with minimal information if you don't specify any flags. It should be more self-descriptive and demo friendly, reporting usage info if flags are omitted or don't parse.",closed,False,2017-11-10 00:33:58,2017-11-11 01:11:58
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/375,https://api.github.com/repos/kubernetes/kube-deploy/issues/375,Implement cluster deletion,This functionality was regressed when we stopped using kubicorn for cluster creation.,closed,False,2017-11-10 00:37:12,2017-11-10 19:56:43
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/376,https://api.github.com/repos/kubernetes/kube-deploy/issues/376,Add node command,,closed,True,2017-11-10 01:42:56,2017-11-10 01:43:05
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/377,https://api.github.com/repos/kubernetes/kube-deploy/issues/377,Remove unncessary cluster flag and use default kubeconfig if not provided,,closed,True,2017-11-10 18:53:40,2017-11-10 18:53:57
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/378,https://api.github.com/repos/kubernetes/kube-deploy/issues/378,cluster-api installer should honor NetworkingConfig in Cluster object,Currently installer is using calico and setting networking by default and ignoring ClusterNetworkingConfig,closed,False,2017-11-10 20:08:32,2017-11-17 18:54:24
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/379,https://api.github.com/repos/kubernetes/kube-deploy/issues/379,cluster-installer delete service a/c before returning success for delete command,Currently installer creates service a/c during cluster creation but doesn't delete it during cluster delete.,closed,False,2017-11-10 20:11:19,2017-11-10 23:12:24
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/380,https://api.github.com/repos/kubernetes/kube-deploy/issues/380,Add simple retry logic for machine controller creation.,,closed,True,2017-11-10 22:17:42,2017-11-10 22:18:01
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/381,https://api.github.com/repos/kubernetes/kube-deploy/issues/381,Have the master annotate itself as well.,"The node was already self-annotating during startup with its corresponding Machine name so that the machines-controller could more easily link the two objects, but the master wasn't.",closed,True,2017-11-10 22:32:44,2017-11-10 22:33:10
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/382,https://api.github.com/repos/kubernetes/kube-deploy/issues/382,Add health check for apiserver coming up.,,closed,True,2017-11-10 22:58:33,2017-11-10 22:59:11
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/383,https://api.github.com/repos/kubernetes/kube-deploy/issues/383,Clean up service accounts during delete,,closed,True,2017-11-10 23:12:01,2017-11-10 23:12:15
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/384,https://api.github.com/repos/kubernetes/kube-deploy/issues/384,Actually honor kubelet/control plane versions when creating masters.,"Previously, they were using a fixed version of kubeadm, the latest version of kubelet, and the default version of the control plane.",closed,True,2017-11-10 23:20:31,2017-11-10 23:25:45
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/385,https://api.github.com/repos/kubernetes/kube-deploy/issues/385,Remove enable machinecontroller flag,Now installer will always create machine controller pod.,closed,True,2017-11-10 23:31:31,2017-11-10 23:31:45
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/386,https://api.github.com/repos/kubernetes/kube-deploy/issues/386,cluster-installer handle service a/c creation error more gracefullly,"If installer gets service a/c already exists err, it should continue instead of bailing out.",closed,False,2017-11-10 23:33:23,2017-11-22 19:46:45
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/387,https://api.github.com/repos/kubernetes/kube-deploy/issues/387,Cleaning up machineset demo usage.,"Making the `machineset` tool a bit friendlier to use.

fixes https://github.com/kubernetes/kube-deploy/issues/374",closed,True,2017-11-11 01:11:05,2017-11-11 01:11:58
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/388,https://api.github.com/repos/kubernetes/kube-deploy/issues/388,"Friendlier output for ""machineset scale"" command.",,closed,True,2017-11-11 01:20:18,2017-11-11 01:20:43
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/389,https://api.github.com/repos/kubernetes/kube-deploy/issues/389,cluster installer - create cluster CRD during cluster creation,Currently cluster installer is creating machine CRDs during cluster creation phase. It should also create cluster CRD.,closed,False,2017-11-14 21:23:01,2017-11-17 18:54:09
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/390,https://api.github.com/repos/kubernetes/kube-deploy/issues/390,cluster-installer - machine delete should wait for finalizers to finish before returning success,"When a machine CRD is marked for deletion, finalizers should delete node vm. Cluster installer should wait for finalizers to finish deleting vm whenever delete machine code path is triggered.",closed,False,2017-11-14 21:45:58,2018-04-12 16:44:32
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/391,https://api.github.com/repos/kubernetes/kube-deploy/issues/391,Watch nodes and link them to machines.,"When the machine-controller starts up new hosts, those hosts already self-annotate their corresponding `node` objects with the name of the `machine` that was responsible for them. Now, a new watcher in the machine-controller will look for these annotations and link the `Machine.Status.NodeRef` to the proper `node`, and unlink them on `node` deletion.",closed,True,2017-11-15 22:45:50,2017-11-15 22:46:56
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/392,https://api.github.com/repos/kubernetes/kube-deploy/issues/392,Add repair command,,closed,True,2017-11-16 00:00:13,2017-11-16 00:00:21
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/393,https://api.github.com/repos/kubernetes/kube-deploy/issues/393,Fix cluster name in cluster yaml,Now cluster creation works w/o providing master name.,closed,True,2017-11-16 01:50:07,2017-11-16 01:50:57
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/394,https://api.github.com/repos/kubernetes/kube-deploy/issues/394,cluster installer should clean up if cluster creation fails,"Cluster installer creates master vm and service accounts. If anything fails during cluster creation, these resources are not deleted. Ideally installer should delete these.",closed,False,2017-11-16 01:53:58,2017-11-22 22:27:15
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/395,https://api.github.com/repos/kubernetes/kube-deploy/issues/395,Add Upgrade support,,closed,True,2017-11-16 17:50:06,2017-11-16 17:51:43
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/396,https://api.github.com/repos/kubernetes/kube-deploy/issues/396,Start using the cluster API type,,closed,True,2017-11-16 23:00:39,2017-11-16 23:01:09
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/397,https://api.github.com/repos/kubernetes/kube-deploy/issues/397,Add the ability to specify machine controller with env var,"You can set MACHINE_CONTROLLER_IMAGE to the image of the machine
controller you wish to use.",closed,True,2017-11-17 21:09:02,2017-11-17 21:09:10
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/398,https://api.github.com/repos/kubernetes/kube-deploy/issues/398,Go fmt the package.,,closed,True,2017-11-17 21:13:36,2017-11-17 21:14:03
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/399,https://api.github.com/repos/kubernetes/kube-deploy/issues/399,Minimal Makefile to help test builds.,"Individual components can still be built with `go build`, but now you can just run `make` to exercise the building of all of the components in the cluster-api subdir. Useful after refactoring or updating vendored dependencies.",closed,True,2017-11-17 21:45:28,2017-11-17 21:45:36
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/400,https://api.github.com/repos/kubernetes/kube-deploy/issues/400,Ignore machines-client example binary.,,closed,True,2017-11-17 21:47:53,2017-11-17 21:48:12
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/401,https://api.github.com/repos/kubernetes/kube-deploy/issues/401,"Add ""fmt"" make target.",,closed,True,2017-11-17 22:02:00,2017-11-17 22:02:22
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/402,https://api.github.com/repos/kubernetes/kube-deploy/issues/402,Add another command to fix-image-permissions target.,"This only needs to run once per project, but it's biting people now that they're testing machine-controller updates using their own projects. Let's just run it every time to be sure.",closed,True,2017-11-17 22:50:47,2017-11-17 22:51:31
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/403,https://api.github.com/repos/kubernetes/kube-deploy/issues/403,Darin and Uncordon the node before and after kubelet upgrade.,,closed,True,2017-11-17 23:21:38,2017-11-17 23:22:03
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/404,https://api.github.com/repos/kubernetes/kube-deploy/issues/404,"Move the ""cluster-api upgrade"" into a separate directory as a separate",utility.,closed,True,2017-11-18 00:40:36,2017-11-18 00:43:14
kube-deploy,kris-nova,https://github.com/kubernetes/kube-deploy/pull/405,https://api.github.com/repos/kubernetes/kube-deploy/issues/405,Adding ProviderName field so we can switch on clouds,"Thinking it might be useful to have a configurable name for the cloud providers. This way we can easily switch on the cloud provider to serialize/deserialze the cloud provider to/from.

This would be really handy as we work on implementing this in kubicorn.

",closed,True,2017-11-18 17:08:08,2018-03-03 00:22:52
kube-deploy,vielmetti,https://github.com/kubernetes/kube-deploy/issues/406,https://api.github.com/repos/kubernetes/kube-deploy/issues/406,cluster-api is missing summary and detailed documentation on how to add a new cloud provider,"### What I'm looking at

https://github.com/kubernetes/kube-deploy/tree/master/cluster-api/cloud

### What I expected to see

a README.md file that described how to add another cloud provider

### What I actually see

no README.md file, and no summary or detailed documentation.

Happy to contribute a PR if someone has some text I can work from.",closed,False,2017-11-19 15:27:36,2018-02-20 16:46:32
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/issues/407,https://api.github.com/repos/kubernetes/kube-deploy/issues/407,Better handling with calling GCE API,"There are two major issues right now.

1. All API call w/o timeout.
2. All API call w/o retry.",closed,False,2017-11-20 19:48:02,2018-02-13 00:51:38
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/408,https://api.github.com/repos/kubernetes/kube-deploy/issues/408,Switch to use wavenet for network ,,closed,True,2017-11-20 21:47:18,2017-11-21 00:05:23
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/409,https://api.github.com/repos/kubernetes/kube-deploy/issues/409,Move repair to a separate directory as a separate utility,,closed,True,2017-11-21 00:00:33,2017-11-21 00:00:46
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/410,https://api.github.com/repos/kubernetes/kube-deploy/issues/410,Fix logging before flag.Parse error,,closed,True,2017-11-21 00:13:51,2017-11-21 00:14:00
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/411,https://api.github.com/repos/kubernetes/kube-deploy/issues/411,Repair code cleanup,,closed,True,2017-11-21 02:04:32,2017-11-21 02:04:42
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/412,https://api.github.com/repos/kubernetes/kube-deploy/issues/412,Skip master for repair,,closed,True,2017-11-21 18:54:53,2017-11-21 18:55:07
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/413,https://api.github.com/repos/kubernetes/kube-deploy/issues/413,Respect the spec!,"Update Machine status with errors on invalid spec.
    
We don't currently support any other container runtime than Docker 1.12.0, so instead of ignoring this part of the spec, we should at least validate it and report errors on unsupported values.
",closed,True,2017-11-21 19:59:11,2017-11-21 19:59:34
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/414,https://api.github.com/repos/kubernetes/kube-deploy/issues/414,Bump the machine-controller version.,Rebuilding after https://github.com/kubernetes/kube-deploy/pull/413,closed,True,2017-11-21 20:01:53,2017-11-21 20:02:18
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/415,https://api.github.com/repos/kubernetes/kube-deploy/issues/415,Wait for control plan upgrade to finish before upgrading nodes,,closed,True,2017-11-21 21:04:15,2017-11-21 21:04:23
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/416,https://api.github.com/repos/kubernetes/kube-deploy/issues/416,Overwrite annotations when self-annotating nodes.,"Without this, editing a Machine in a way that triggers VM replacement will have the new VM fail the self-annotation step, since the annotation will already exist on the Node.",closed,True,2017-11-21 21:15:58,2017-11-21 21:30:44
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/417,https://api.github.com/repos/kubernetes/kube-deploy/issues/417,Fixing minor typo.,,closed,True,2017-11-21 21:17:42,2017-11-21 21:18:05
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/418,https://api.github.com/repos/kubernetes/kube-deploy/issues/418,Bump machine controller.,After https://github.com/kubernetes/kube-deploy/pull/416,closed,True,2017-11-21 21:32:35,2017-11-21 21:32:50
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/419,https://api.github.com/repos/kubernetes/kube-deploy/issues/419,A little better logging.,,closed,True,2017-11-22 00:10:22,2017-11-22 03:32:46
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/420,https://api.github.com/repos/kubernetes/kube-deploy/issues/420,Add timed wait for GCE API call to replace busy loop,,closed,True,2017-11-22 01:08:32,2017-11-28 01:08:40
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/421,https://api.github.com/repos/kubernetes/kube-deploy/issues/421,Annotate GCE machines with project/zone/name of VM.,"When creating GCE VMs, annotate the corresponding machine object with the project/zone/name of the VM so that we can find it later. Otherwise, we rely on the `providerConfig` in the machine spec, which can be changed at any point.",closed,True,2017-11-22 04:03:04,2017-11-22 15:29:00
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/422,https://api.github.com/repos/kubernetes/kube-deploy/issues/422,"Show changed files during ""fmt"" make target.",,closed,True,2017-11-22 04:04:30,2017-11-22 04:04:58
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/423,https://api.github.com/repos/kubernetes/kube-deploy/issues/423,Refer to annotations when deleting GCE machines.,"Don't rely on the project/zone from the `providerConfig`, or the object name (which now allows the VM name to differ from the machine name), except as a fallback for missing annotations, like during client-side deletion.",closed,True,2017-11-22 04:45:55,2017-11-22 04:46:29
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/424,https://api.github.com/repos/kubernetes/kube-deploy/issues/424,Use context in kubeconfig for kubectl calls,"Currently for each cluster create cluster-api overrrides kubeconfig file. If user wants to create two clusters, the best option at the moment is to write to two different kubeconfig files. Ideally user should able to create two clusters and have a single kubeconfig file with multiple contexts. ",closed,False,2017-11-22 19:27:39,2018-02-14 16:06:55
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/425,https://api.github.com/repos/kubernetes/kube-deploy/issues/425,Create new service a/c for each cluster create,"Currently cluster-api doesn't create different service a/c for different clusters. This results into weird error cases when user creates multiple clusters.

This PR should streamline service a/c maintenance.",closed,True,2017-11-22 19:38:38,2017-11-22 19:39:50
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/pull/426,https://api.github.com/repos/kubernetes/kube-deploy/issues/426,"Delete master vm, service a/c if created, if cluster creation fails",https://github.com/kubernetes/kube-deploy/issues/394,closed,True,2017-11-22 22:24:46,2017-11-22 22:25:43
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/427,https://api.github.com/repos/kubernetes/kube-deploy/issues/427,Ignore upgrader binary.,,closed,True,2017-11-23 00:11:19,2017-11-23 00:13:03
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/428,https://api.github.com/repos/kubernetes/kube-deploy/issues/428,Use version prefix matching for Debian packages.,"When Kubernetes creates Debian packages for a release, they add a suffix to the package version, so `1.8.0` actually becomes `1.8.0-00`. This allows for patching the package and creating new Debian releases based off of the same Kubernetes release (e.g. `1.8.0-01`, `1.8.0-02`, etc) in the event that there was something wrong with the packaging, but not Kubernetes itself.

However, not every Kubernetes version has an `-00` package (if they are particularly troublesome, they get removed from the repo), and if there are multiple such versions available, we should try to use the newest one.

This change uses prefix matching to find the best package version for each component (given the Kubernetes SemVer), instead of always hardcoding the `-00` suffix.",closed,True,2017-11-23 01:14:50,2017-11-23 01:15:15
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/429,https://api.github.com/repos/kubernetes/kube-deploy/issues/429,Deleting new-machines.yaml.,"There used to be a plan for this file, but it's obsolete now.",closed,True,2017-11-23 01:22:05,2017-11-23 01:23:44
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/430,https://api.github.com/repos/kubernetes/kube-deploy/issues/430,Label VMs with CRD UID for later retrieval,,closed,True,2017-11-23 09:49:37,2017-11-23 09:49:46
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/431,https://api.github.com/repos/kubernetes/kube-deploy/issues/431,Fix for snapshot replication,,closed,True,2017-11-27 03:47:35,2017-11-27 04:17:37
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/432,https://api.github.com/repos/kubernetes/kube-deploy/issues/432,Make imagebuilder code goimports-clean,,closed,True,2017-11-27 03:48:45,2017-11-27 04:17:43
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/433,https://api.github.com/repos/kubernetes/kube-deploy/issues/433,imagebuilder: Use a different instance type in us-east-2,The m3 family is not available in us-east-2,closed,True,2017-11-27 03:49:52,2017-11-27 04:17:52
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/434,https://api.github.com/repos/kubernetes/kube-deploy/issues/434,Lock to specific branches of bootstrap-vz,,closed,True,2017-11-27 05:38:43,2017-12-01 03:29:39
kube-deploy,ajitak,https://github.com/kubernetes/kube-deploy/issues/435,https://api.github.com/repos/kubernetes/kube-deploy/issues/435,Repair default dryrun flag should be false,"Currently it is true, change it to false.
code ref https://github.com/kubernetes/kube-deploy/blob/master/cluster-api/repair/cmd/root.go#L61",closed,False,2017-11-27 23:08:56,2018-01-02 06:33:54
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/436,https://api.github.com/repos/kubernetes/kube-deploy/issues/436,Create nice landing README for cluster-api,"We anticipate increased interest/traffic after the upcoming KubeCon demo. We should make sure that when someone visits the `kube-deploy` repository, they can easily find the `cluster-api` subdirectory. Within that subdirectory, we should have a nice landing README that describes the project, links to relevant resources, and helps new users getting started.",closed,False,2017-11-27 23:27:08,2018-01-16 07:27:49
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/437,https://api.github.com/repos/kubernetes/kube-deploy/issues/437,Clean up upgrader output,"We want the `upgrader` tool to have clean, concise output for the upcoming KubeCon demo.",closed,False,2017-11-27 23:33:19,2018-02-07 06:24:37
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/438,https://api.github.com/repos/kubernetes/kube-deploy/issues/438,"Add ""repair"" utility to Makefile / gitignore.",,closed,True,2017-11-27 23:59:18,2017-11-28 20:39:14
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/439,https://api.github.com/repos/kubernetes/kube-deploy/issues/439,Clean up metadata generation and general validation,,closed,True,2017-11-28 00:32:12,2017-11-28 00:32:24
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/440,https://api.github.com/repos/kubernetes/kube-deploy/issues/440,Change repair's dry run flag default to false,,closed,True,2017-11-28 00:37:35,2017-11-28 00:37:42
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/441,https://api.github.com/repos/kubernetes/kube-deploy/issues/441,run actions against a single machine in serial fashion but against di…,…fferent machines in parallel,closed,True,2017-11-28 23:48:37,2017-11-28 23:49:30
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/442,https://api.github.com/repos/kubernetes/kube-deploy/issues/442,Separating script parts for preloaded images,,closed,True,2017-11-29 00:18:18,2017-11-29 00:19:10
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/443,https://api.github.com/repos/kubernetes/kube-deploy/issues/443,Cleanup uid,,closed,True,2017-11-29 23:12:20,2017-11-29 23:12:30
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/444,https://api.github.com/repos/kubernetes/kube-deploy/issues/444,Change %s --> %v since node isn't a string type.,,closed,True,2017-11-29 23:49:11,2018-01-24 17:41:24
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/445,https://api.github.com/repos/kubernetes/kube-deploy/issues/445,Fix null dereference bug in upgrader,,closed,True,2017-11-30 03:45:30,2017-11-30 03:46:28
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/446,https://api.github.com/repos/kubernetes/kube-deploy/issues/446,Fix missing returns in node watcher,Lack of said returns caused exotic loops where machines were not properly deleted if the node appeared while the machine was in the middle of being deleted.,closed,True,2017-11-30 19:30:17,2017-11-30 19:30:27
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/447,https://api.github.com/repos/kubernetes/kube-deploy/issues/447,cleanup nodes on delete and annotations on clone,,closed,True,2017-11-30 20:45:43,2017-11-30 20:45:55
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/issues/448,https://api.github.com/repos/kubernetes/kube-deploy/issues/448,Re-appearing deleted node,"Sometimes a node re-appears after being deleted. Logs show the machine controller **getting an OnAdd() & OnDelete() for the machine seemingly out of nowhere** every 5 - 10 mins forever.

Example Logs:

> I1130 22:14:52.098820       8 machinecontroller.go:121] object created: gce-node-1f6bz e31e49f2-d61b-11e7-b091-42010a800002
I1130 22:14:53.280272       8 machineactuator.go:404] Wait for insert ""operation-1512080092102-55f3a94c2fd71-4813a394-326dc58c""...
I1130 22:15:03.475399       8 machineactuator.go:414] Finish wait for insert ""operation-1512080092102-55f3a94c2fd71-4813a394-326dc58c""...
I1130 22:15:03.479216       8 machinecontroller.go:132] create machine gce-node-1f6bz succeded.
I1130 22:15:03.479298       8 machinecontroller.go:140] object updated: gce-node-1f6bz e31e49f2-d61b-11e7-b091-42010a800002
I1130 22:15:03.479319       8 machinecontroller.go:141]   old k8s version: 1.7.4, new: 1.7.4
I1130 22:15:03.479343       8 machinecontroller.go:144] machine gce-node-1f6bz change does not require any update action to be taken.
I1130 22:16:42.294435       8 nodewatcher.go:94] node created: gce-node-1f6bz
I1130 22:16:43.387353       8 machinecontroller.go:140] object updated: gce-node-1f6bz e31e49f2-d61b-11e7-b091-42010a800002
I1130 22:16:43.387373       8 machinecontroller.go:141]   old k8s version: 1.7.4, new: 1.7.4
I1130 22:16:43.387390       8 machinecontroller.go:144] machine gce-node-1f6bz change does not require any update action to be taken.
I1130 22:16:43.387468       8 nodewatcher.go:128] Successfully linked machine gce-node-1f6bz to node gce-node-1f6bz
I1130 22:19:52.222702       8 machinecontroller.go:160] object deleted: gce-node-1f6bz e31e49f2-d61b-11e7-b091-42010a800002
I1130 22:19:52.227743       8 nodewatcher.go:106] node deleted: gce-node-1f6bz
E1130 22:19:52.229520       8 nodewatcher.go:138] Error getting machine gce-node-1f6bz: machines.cluster.k8s.io ""gce-node-1f6bz"" not found
I1130 22:19:52.890823       8 machineactuator.go:404] Wait for delete ""operation-1512080392234-55f3aa6a6a411-e02ca317-080fdb4d""...
I1130 22:20:33.727266       8 machineactuator.go:414] Finish wait for delete ""operation-1512080392234-55f3aa6a6a411-e02ca317-080fdb4d""...
I1130 22:20:33.734156       8 machinecontroller.go:171] delete machine gce-node-1f6bz succeded.
**I1130 22:25:30.966769       8 machinecontroller.go:121] object created: gce-node-1f6bz e31e49f2-d61b-11e7-b091-42010a800002
I1130 22:25:30.966851       8 machinecontroller.go:160] object deleted: gce-node-1f6bz e31e49f2-d61b-11e7-b091-42010a800002**
E1130 22:25:31.277051       8 machineactuator.go:515] Machine error: error deleting GCE instance: googleapi: Error 404: The resource 'projects/jesschen-gke-dev/zones/us-central1-f/instances/gce-node-1f6bz' was not found, notFound
E1130 22:25:31.277069       8 machinecontroller.go:169] delete machine gce-node-1f6bz failed: error deleting GCE instance: googleapi: Error 404: The resource 'projects/jesschen-gke-dev/zones/us-central1-f/instances/gce-node-1f6bz' was not found, notFound
I1130 22:25:32.692869       8 machineactuator.go:404] Wait for insert ""operation-1512080731280-55f3abadc1280-fe42b359-18941925""...
I1130 22:25:42.893242       8 machineactuator.go:414] Finish wait for insert ""operation-1512080731280-55f3abadc1280-fe42b359-18941925""...
E1130 22:25:42.895734       8 machinecontroller.go:130] create machine gce-node-1f6bz failed: Operation cannot be fulfilled on machines.cluster.k8s.io ""gce-node-1f6bz"": StorageError: invalid object, Code: 4, Key: /registry/cluster.k8s.io/machines/gce-node-1f6bz, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: e31e49f2-d61b-11e7-b091-42010a800002, UID in object meta: 
I1130 22:27:03.763354       8 nodewatcher.go:94] node created: gce-node-1f6bz
**I1130 22:31:57.969265       8 machinecontroller.go:121] object created: gce-node-1f6bz e31e49f2-d61b-11e7-b091-42010a800002
I1130 22:31:57.969443       8 machinecontroller.go:160] object deleted: gce-node-1f6bz e31e49f2-d61b-11e7-b091-42010a800002**
I1130 22:31:57.975923       8 nodewatcher.go:106] node deleted: gce-node-1f6bz
E1130 22:31:57.977982       8 nodewatcher.go:138] Error getting machine gce-node-1f6bz: machines.cluster.k8s.io ""gce-node-1f6bz"" not found
I1130 22:31:58.647274       8 machineactuator.go:404] Wait for delete ""operation-1512081117983-55f3ad1e8b119-d9040ca5-cb3968ff""...
I1130 22:32:34.475749       8 machineactuator.go:414] Finish wait for delete ""operation-1512081117983-55f3ad1e8b119-d9040ca5-cb3968ff""...
I1130 22:32:34.478282       8 machinecontroller.go:171] delete machine gce-node-1f6bz succeded.
I1130 22:32:35.836939       8 machineactuator.go:404] Wait for insert ""operation-1512081154481-55f3ad4159b69-d3529904-b59b9155""...
I1130 22:32:46.057243       8 machineactuator.go:414] Finish wait for insert ""operation-1512081154481-55f3ad4159b69-d3529904-b59b9155""...
E1130 22:32:46.060081       8 machinecontroller.go:130] create machine gce-node-1f6bz failed: Operation cannot be fulfilled on machines.cluster.k8s.io ""gce-node-1f6bz"": StorageError: invalid object, Code: 4, Key: /registry/cluster.k8s.io/machines/gce-node-1f6bz, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: e31e49f2-d61b-11e7-b091-42010a800002, UID in object meta: 
I1130 22:34:16.569190       8 nodewatcher.go:94] node created: gce-node-1f6bz


",closed,False,2017-11-30 22:55:20,2018-01-02 22:10:16
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/449,https://api.github.com/repos/kubernetes/kube-deploy/issues/449,Copy snapshots in parallel,,closed,True,2017-12-01 03:35:49,2017-12-01 03:36:15
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/450,https://api.github.com/repos/kubernetes/kube-deploy/issues/450,AWS images for 1.8,,closed,True,2017-12-01 03:37:03,2018-01-05 18:39:47
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/451,https://api.github.com/repos/kubernetes/kube-deploy/issues/451,Add more information to the landing page,,closed,True,2017-12-01 20:45:45,2017-12-01 20:46:07
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/452,https://api.github.com/repos/kubernetes/kube-deploy/issues/452,check for CRD on machine create,,closed,True,2017-12-01 21:05:05,2017-12-01 21:05:12
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/453,https://api.github.com/repos/kubernetes/kube-deploy/issues/453,Use a prebaked OS image for GCE instances.,"This will speed up the demo by having OS packages and Docker images pre-pulled. In the future, we may want specialized prebaked images based off of different base images, Machine role, Kubernetes version, etc., but for now, a single monolithic image gives us a good speed-up.",closed,True,2017-12-05 19:15:38,2017-12-05 19:53:08
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/454,https://api.github.com/repos/kubernetes/kube-deploy/issues/454,Linking to Cluster API doc in README.,And other minor README edits.,closed,True,2017-12-06 00:10:20,2017-12-06 00:10:52
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/455,https://api.github.com/repos/kubernetes/kube-deploy/issues/455,More context links in README.,,closed,True,2017-12-06 00:19:16,2017-12-06 00:19:27
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/456,https://api.github.com/repos/kubernetes/kube-deploy/issues/456,More README edits.,,closed,True,2017-12-06 00:29:53,2017-12-06 00:30:01
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/457,https://api.github.com/repos/kubernetes/kube-deploy/issues/457,More README cleanup.,,closed,True,2017-12-06 00:44:32,2017-12-06 00:44:57
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/458,https://api.github.com/repos/kubernetes/kube-deploy/issues/458,Iterating on cluster-api READMEs.,,closed,True,2017-12-06 00:58:15,2017-12-06 00:58:28
kube-deploy,delapsley,https://github.com/kubernetes/kube-deploy/issues/459,https://api.github.com/repos/kubernetes/kube-deploy/issues/459,Add firewall configuration section to README,"_cluster-api_ requires the orchestrating computer to have access to TCP port 443 on the master and node VMs.

It would be great to have a section in the README that documents the steps required in GCP to create the firewall rules required to enable _cluster-api_ to orchestrate VMs in GCP.",closed,False,2017-12-14 03:40:11,2018-01-16 07:21:29
kube-deploy,delapsley,https://github.com/kubernetes/kube-deploy/pull/460,https://api.github.com/repos/kubernetes/kube-deploy/issues/460,Add firewall configuration section to README,Adds a section on how to configure GCP firewall rules to enable cluster-api to orchestrate GCP VMs.,closed,True,2017-12-14 03:46:04,2018-01-12 06:39:30
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/461,https://api.github.com/repos/kubernetes/kube-deploy/issues/461,Reconcile machines,,closed,True,2017-12-19 19:09:19,2017-12-19 19:09:38
kube-deploy,spiffxp,https://github.com/kubernetes/kube-deploy/pull/462,https://api.github.com/repos/kubernetes/kube-deploy/issues/462,Add code-of-conduct.md,"Refer to kubernetes/community as authoritative source for code of conduct

ref: kubernetes/community#1527",closed,True,2017-12-20 18:33:01,2018-01-07 01:39:09
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/issues/463,https://api.github.com/repos/kubernetes/kube-deploy/issues/463,What is the proper scoping of our API types?,"This was brought up in the [cluster API review](https://github.com/kubernetes/kube-deploy/pull/306#discussion_r157254538), and I think it was discussed in last week's meeting. I'm creating this issue so that we have a concentrated and recorded place to discuss this.

Do we want to have machines and clusters namespaced or not?

Currently we have them cluster scoped because the prototype was designed for the types to be stored in (or aggregated with) the cluster they represented. The other use case is to store the types in another cluster for remote management. If we leave the types cluster scoped, we can only manage one cluster remotely. This is not a desirable limitation.

So the first major question: Do we want to support both local and remote management? I think the answer to this is obviously yes, but if not, we should go with the scoping that best solved the most desired use-case.

If we want to support both, I think we have a few of options. Feel free to propose others that are drastically different from what I present below.

## Option 1 (Do it both ways!)

We support both namespace and cluster-scoped types in the API aggregation server. I imagine we could implement it in such a way that the aggregated API server either register the types as namespaced or not based on a flag.

### Pros

- Supports both use-cases

### Cons

- Most likely can't support both use-cases simultaneously (can't locally manage a cluster that is remotely managing others).
- Would require 2 different clients to be generated (namespaced and not namespaced).
- Controllers would also have to be coded to work with both scopes.
- Actually anything that uses it would have to be coded to work with both scopes.

## Option 2 (Just namespaces)

We put every cluster in a namespace including local ones. Each namespace would have it's own Cluster object and several Machine objects.

### Pros

- Supports both use-cases
- There's no difference between managing locally and remotely

### Cons

- There's no difference between managing locally and remotely. Local management likely has special considerations so you don't pull the rug out from under yourself. Being able to distinguish the scenarios might be useful at the machine-controller level. Higher-level algorithms shouldn't care.
- Not as clean for local cluster management.

## Option 3 (Just namespaces, with special ones)

Just like option 2, we make all the types namespace-scoped. The only difference is we reserve (by convention or somehow codify it) a special namespace for local cluster management. I propose one of the following:
- ***default***: `kubectl get <any cluster type>` would return the ones representing the local cluster by default.
- ***kube-system***: Puts it in the namespace that most other cluster level components are running in.
- ***local***: Just a special keyword that signifies that this cluster is local.

### Pros
- Supports both use-cases
- There's no difference in representation and can be operated on the same way for either use-case
- Contains enough signal that management tools can do something different for local or remote management if they need to.

### Cons
- Relies on convention that may be unenforceable. ",closed,False,2017-12-21 19:41:42,2018-02-02 23:05:44
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/464,https://api.github.com/repos/kubernetes/kube-deploy/issues/464,Making network ranges more flexible in representation,Addressing final comments from #306,closed,True,2017-12-21 20:39:49,2017-12-21 20:42:52
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/465,https://api.github.com/repos/kubernetes/kube-deploy/issues/465,extract gcp code,"Removing cloud/provider specific code from cluster-api folder to avoid the bloat and dependency issues that can come with it. The goal is that the cluster-api folder would contain the api and common non-provider specific code. This folder would then be vendored to wherever cloud/provider specific machine controllers are.

For simplicity, all the machine controller code was pulled out. As we determine which bits of the machine controller are generic/common, we can promote those bits back into the cluster-api.",closed,True,2017-12-22 19:54:42,2018-01-03 18:38:32
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/466,https://api.github.com/repos/kubernetes/kube-deploy/issues/466,"fix packages, vendor and network ranges","Small fixes to allow the gcp controller to work after issues introduced in:
https://github.com/kubernetes/kube-deploy/pull/465
https://github.com/kubernetes/kube-deploy/pull/464",closed,True,2017-12-22 20:39:57,2017-12-22 20:40:08
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/issues/467,https://api.github.com/repos/kubernetes/kube-deploy/issues/467,Break up MachineActuator interface,"Currently the MachineActuator interface is a mash of what the cloud specific logic that machine controller needs and what the (boostrapping) deployer needs.
https://github.com/kubernetes/kube-deploy/blob/master/cluster-api-gcp/cloud/machineactuator.go

It looks like it would be best to break up the interface into the interfaces needed by the machine controller and the deployer respectively. These interfaces should also be [kept with the code that uses it](https://golang.org/doc/effective_go.html#interfaces) instead of in a generic location. This will help prevent having a catch all interface for cloud specific logic that bloats over time. 
",closed,False,2018-01-03 18:51:29,2018-02-06 23:12:46
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/issues/468,https://api.github.com/repos/kubernetes/kube-deploy/issues/468,Switch the machine controller to using API aggregation,API aggregation has several benefits including versioning and the ability to run the controller outside the cluster.,closed,False,2018-01-03 18:54:48,2018-04-13 16:08:00
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/issues/469,https://api.github.com/repos/kubernetes/kube-deploy/issues/469,Machine Controller Locking,"Have leader election/locking so that only one machine controller is running in the cluster at a time.
https://github.com/kubernetes/kube-deploy/blob/master/cluster-api-gcp/machine-controller/controller/machinecontroller.go#L94

For locking see:
- https://github.com/kubernetes/kubernetes/issues/44857
- https://github.com/kubernetes/client-go/tree/master/tools/leaderelection",closed,False,2018-01-03 18:59:45,2018-04-13 15:36:30
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/issues/470,https://api.github.com/repos/kubernetes/kube-deploy/issues/470,Promote common machine controller logic to cluster-api,"Look various implementations of the machine controller and promote common bits to cluster-api.

Known machine controller implementations (as of now):
https://github.com/kubermatic/machine-controller 
https://github.com/kubernetes/kube-deploy/tree/master/cluster-api-gcp/machine-controller
@tallclair - Do you have link to your spike machine controller?",closed,False,2018-01-03 19:15:50,2018-02-28 23:30:16
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/issues/471,https://api.github.com/repos/kubernetes/kube-deploy/issues/471,Bot for stale vendored cluster API ,"Create a robot that will warn when the vendored cluster API is falling behind master.

Context:
On creation of cluster-api-gcp, there was a discussion on whether we were going to vendor the cluster-api or directly reference it. We decided to vendor it so it would be a better example of the right pattern to folks attempting to make their own machine controller. The downside to this is that the vendored cluster-api could fall behind the latest cluster-api. A robot warning about falling behind was the suggested mitigation.
 ",closed,False,2018-01-03 19:20:05,2018-04-12 16:44:57
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/issues/472,https://api.github.com/repos/kubernetes/kube-deploy/issues/472,E2E Cluster API Tests,Setup E2E tests that use Cluster API for in kubernetes/test-infra,closed,False,2018-01-03 19:26:03,2018-04-12 16:42:43
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/issues/473,https://api.github.com/repos/kubernetes/kube-deploy/issues/473,Create Code Owner Structure,Set up owners for code so that PRs can go to appropriate folks.,closed,False,2018-01-04 19:06:35,2018-01-12 23:56:34
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/issues/474,https://api.github.com/repos/kubernetes/kube-deploy/issues/474,Write docs on contributing to the cluster api effort,We should add a section to [Contributing to SIG Cluster Lifecycle](https://docs.google.com/document/d/1eq0mWjnyQiDXhEGPU7tulbnDuvkaUehSz7u3NRxxpc8/edit) for the cluster API. ,closed,False,2018-01-08 07:30:25,2018-02-07 18:21:35
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/475,https://api.github.com/repos/kubernetes/kube-deploy/issues/475,"Remove Makecerts, which is no longer being kept up to date.",,closed,True,2018-01-08 16:36:09,2018-01-24 17:41:15
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/issues/476,https://api.github.com/repos/kubernetes/kube-deploy/issues/476,create integration test for machine controller.,"We need to have integration test to test machine controller. The following scenario is interested.

1. Basic create/update/delete through API
2. Scale up
3. Scale down
4. Stress test with combination of ScaleUp/ScaleDown
5. Upgrade.
6. Mixed upgrade and scale operation simultaneously 
",closed,False,2018-01-08 19:46:45,2018-04-12 16:43:32
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/477,https://api.github.com/repos/kubernetes/kube-deploy/issues/477,Seperate deployer provider interface,"Break out bits of the MachineActuator that only the deployer uses. For https://github.com/kubernetes/kube-deploy/issues/467

This has benefits like avoiding catch all bloating and decoupling the machine controller and the deployer. Full reasoning on issue.",closed,True,2018-01-09 22:36:22,2018-01-11 00:17:33
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/478,https://api.github.com/repos/kubernetes/kube-deploy/issues/478,Init boilerplate for extention api server,"`apiserver-boot init repo --domain cluster.k8s.io`

- copy vendored dependencies from the apiserver-builder release
- create main.go for apiserver and controller-manager
- create go packages for APIs and controllers
- create go packages for docs
- create bazel WORKSPACE file

Eventually, we will migrate all the types and CRD related code into API server, and rename the new directory as clusterapi. The current clusterapi directory will be deleted.
",closed,True,2018-01-10 00:42:25,2018-01-11 19:27:01
kube-deploy,so0k,https://github.com/kubernetes/kube-deploy/issues/479,https://api.github.com/repos/kubernetes/kube-deploy/issues/479,Pre-create recommended Cgroups ,"See: https://github.com/kubernetes/kops/issues/3762

Also, set `/etc/systemd/system.conf` JoinControllers 

```
JoinControllers=cpu,cpuacct,cpuset,net_cls,net_prio,memory
```

to ensure `/system.slice` is a valid target for `system-reserved` resources",closed,False,2018-01-10 02:31:32,2018-07-07 09:44:13
kube-deploy,spiffxp,https://github.com/kubernetes/kube-deploy/issues/480,https://api.github.com/repos/kubernetes/kube-deploy/issues/480,Setup merge automation for this repo,"I'd like to setup merge automation for this repo. This means:

Use an OWNERS file, /approve, /lgtm to allow people to merge PR's into this repo:
- Create/update an OWNERS_ALIAS file that names the existing teams that have write/admin access to the repo
- Create/update an OWNERS file with approvers based on the write/admin access teams
- Configure prow to turn on the ""approval"" plugin for this repo
- Configure prow/tide to include this repo in the tide query

/assign",closed,False,2018-01-10 22:19:12,2018-01-13 00:45:22
kube-deploy,spiffxp,https://github.com/kubernetes/kube-deploy/pull/481,https://api.github.com/repos/kubernetes/kube-deploy/issues/481,Add OWNERS file,"ref: https://github.com/kubernetes/kube-deploy/issues/480

fixes: https://github.com/kubernetes/kube-deploy/issues/473",closed,True,2018-01-10 22:26:27,2018-01-13 00:04:07
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/issues/482,https://api.github.com/repos/kubernetes/kube-deploy/issues/482,GCE machine actuator should collect current state,"The machine actuator should get current state from the actual sources (eg. kubelet version from machine, VM info from gce) instead of a cached last-applied state. This will help with cases like the state changing under the covers.

https://github.com/kubernetes/kube-deploy/blob/master/cluster-api-gcp/cloud/google/instancestatus.go#L13",closed,False,2018-01-11 17:08:11,2018-04-17 18:53:30
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/issues/483,https://api.github.com/repos/kubernetes/kube-deploy/issues/483,Make Machine Controller Pod Version Not Serial,"Having a serial machine controller pod version makes parallel contribution more difficult. One needs to figure out a unique name for dev pods to avoid stepping on other contributor's toes. When pushing the official image, coordination is needed to not push the same version number. If we make the pod version based on something less serial (eg. commit hash like test-infra does), that will help with making it easier to contribute.",closed,False,2018-01-11 17:17:48,2018-04-17 18:53:22
kube-deploy,lukesiler,https://github.com/kubernetes/kube-deploy/issues/484,https://api.github.com/repos/kubernetes/kube-deploy/issues/484,unable to read Cluster API proposal,"Unable to open the Cluster API Proposal link in https://github.com/kubernetes/kube-deploy/blob/master/cluster-api/README.md which is https://docs.google.com/document/d/1G2sqUQlOYsYX6w1qj0RReffaGXH4ig2rl3zsIzEFCGY

Is it intended to require explicit Request for Access?",closed,False,2018-01-11 19:16:13,2018-01-11 19:35:35
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/485,https://api.github.com/repos/kubernetes/kube-deploy/issues/485,Remove unnecessary file.,,closed,True,2018-01-11 22:32:36,2018-01-24 17:41:14
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/486,https://api.github.com/repos/kubernetes/kube-deploy/issues/486,Add firewall rule instructions,,closed,True,2018-01-11 23:41:54,2018-01-12 00:02:48
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/487,https://api.github.com/repos/kubernetes/kube-deploy/issues/487,Revamp the gcp README,,closed,True,2018-01-12 06:24:11,2018-01-24 17:41:13
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/488,https://api.github.com/repos/kubernetes/kube-deploy/issues/488,"Remove binaries, since they shouldn't be checked in.",Also add .gitignore files to prevent them from being checked in as part of future commits.,closed,True,2018-01-12 06:30:10,2018-01-24 17:41:13
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/489,https://api.github.com/repos/kubernetes/kube-deploy/issues/489,Update the gitignore file to have the correct binary name,,closed,True,2018-01-12 17:57:12,2018-01-24 17:40:56
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/pull/490,https://api.github.com/repos/kubernetes/kube-deploy/issues/490,Remove binaries that were committed.,It looks like some binary builds got committed to the repo. Cleaning them up.,closed,True,2018-01-12 18:19:41,2018-01-12 18:32:09
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/491,https://api.github.com/repos/kubernetes/kube-deploy/issues/491,fix gitignore,Forgot to update the gitignore when pulling out the cluster-api-gcp. This resulted in the cluster-api vendored folder not to get commited.,closed,True,2018-01-12 18:28:58,2018-01-12 18:33:14
kube-deploy,pipejakob,https://github.com/kubernetes/kube-deploy/issues/492,https://api.github.com/repos/kubernetes/kube-deploy/issues/492,cluster-api-gcp doesn't build,"On a fresh checkout of `upstream/master`, running `make` in `kube-deploy/cluster-api-gcp` fails for me:

    $ make
    go build
    # k8s.io/kube-deploy/cluster-api-gcp/util
    util/util.go:87: cannot use machine.ObjectMeta.UID (type ""k8s.io/kube-deploy/cluster-api/vendor/k8s.io/apimachinery/pkg/types"".UID) as type ""k8s.io/kube-deploy/cluster-api-gcp/vendor/k8s.io/apimachinery/pkg/types"".UID in argument to GetMachineIfExists
    util/util.go:97: cannot use ""k8s.io/kube-deploy/cluster-api-gcp/vendor/k8s.io/apimachinery/pkg/apis/meta/v1"".GetOptions literal (type ""k8s.io/kube-deploy/cluster-api-gcp/vendor/k8s.io/apimachinery/pkg/apis/meta/v1"".GetOptions) as type ""k8s.io/kube-deploy/cluster-api/vendor/k8s.io/apimachinery/pkg/apis/meta/v1"".GetOptions in argument to machineClient.Get
    util/util.go:106: invalid operation: machine.ObjectMeta.UID != uid (mismatched types ""k8s.io/kube-deploy/cluster-api/vendor/k8s.io/apimachinery/pkg/types"".UID and ""k8s.io/kube-deploy/cluster-api-gcp/vendor/k8s.io/apimachinery/pkg/types"".UID)
    make: *** [all] Error 2",closed,False,2018-01-12 18:31:37,2018-01-12 18:56:36
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/493,https://api.github.com/repos/kubernetes/kube-deploy/issues/493,Bootstrap new API resource cluster.k8s.io/v1alpha1/Machine and cluster.k8s.io/v1alpha1/Cluster,"Generate skeleton codes (100% generated by tool) for new resources. The following up PR will migrate all the type definition from CRD with code generation. It also fixed the domain name from cluster.k8s.io to k8s.io


apiserver-boot create group version resource --group simple --version v1alpha1  --kind Machine
apiserver-boot create group version resource --group simple --version v1alpha1  --kind Cluster

- create the resource type definition
- create tests for the resource
- create controller that listens for the resource
- create tests for the controller
- create an example for the reference documentation
- create a sample to use for testing
",closed,True,2018-01-12 20:52:21,2018-01-16 06:30:35
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/494,https://api.github.com/repos/kubernetes/kube-deploy/issues/494,s/kubecofig/kubeconfig,,closed,True,2018-01-12 21:50:08,2018-01-13 02:58:20
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/issues/495,https://api.github.com/repos/kubernetes/kube-deploy/issues/495,Add support for the KUBECONFIG environment variable,It's a common use case to use the KUBECONFIG environment variable to specify where to read/write your config. We currently support a command line flag (`-k`) but not the env variable. ,closed,False,2018-01-12 22:01:40,2018-01-16 05:40:24
kube-deploy,kris-nova,https://github.com/kubernetes/kube-deploy/pull/496,https://api.github.com/repos/kubernetes/kube-deploy/issues/496,Fix typo `kubecofig` -> `kubeconfig`,,closed,True,2018-01-12 22:02:29,2018-01-13 00:07:33
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/issues/497,https://api.github.com/repos/kubernetes/kube-deploy/issues/497,Support alternate kubeconfig locations,"Right now you can pass a flag to `cluster-api-gcp` to specify an alternate kubeconfig location. But it isn't properly plumbed through, and if you don't simultaneously set the `KUBECONFIG` environment variable we don't successfully create a cluster. 

We have issues when shelling out to kubectl (e.g. https://github.com/kubernetes/kube-deploy/blob/master/cluster-api-gcp/cloud/google/serviceaccount.go#L75) where the flag isn't plumbed all the way through. ",closed,False,2018-01-12 22:06:01,2018-01-16 23:54:34
kube-deploy,prateekgogia,https://github.com/kubernetes/kube-deploy/pull/498,https://api.github.com/repos/kubernetes/kube-deploy/issues/498,Kubeconfig env variable and alternate locations for kubeconfig fixes ,Kubeconfig env variable and alternate locations for kubeconfig fixes #495 #497,closed,True,2018-01-13 02:26:26,2018-01-16 05:43:33
kube-deploy,prateekgogia,https://github.com/kubernetes/kube-deploy/issues/499,https://api.github.com/repos/kubernetes/kube-deploy/issues/499,Owners docs link in kube-deploy/OWNERS file doesn't work,"See the OWNERS docs: https://git.k8s.io/community/docs/devel/owners.md . <--- this doesn't work. 
",closed,False,2018-01-13 04:04:30,2018-01-16 19:32:26
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/500,https://api.github.com/repos/kubernetes/kube-deploy/issues/500,Fix bash command for picking a node to repair.,,closed,True,2018-01-13 08:15:09,2018-01-24 17:40:57
kube-deploy,mrIncompetent,https://github.com/kubernetes/kube-deploy/pull/501,https://api.github.com/repos/kubernetes/kube-deploy/issues/501,add MachineSet type,"Add a simple MachineSet type.
Basically copied from ReplicaSet",closed,True,2018-01-13 16:52:22,2018-02-17 00:00:54
kube-deploy,mrIncompetent,https://github.com/kubernetes/kube-deploy/pull/502,https://api.github.com/repos/kubernetes/kube-deploy/issues/502,[WIP] Add generic machine api controller,"This adds a generic machine-controller for:
- AWS
- Openstack
- Digitalocean

The current implementation only supports creation of worker nodes.

Also:
Currently it expects the machine.providerConfig is of type `runtime.RawExtension`.
As the providerConfig is of type string atm this is not able to merge and is considered a WIP.",closed,True,2018-01-13 16:55:12,2018-04-16 17:47:16
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/issues/503,https://api.github.com/repos/kubernetes/kube-deploy/issues/503,Extensions to machine status errors,"Extracted from https://github.com/kubernetes/kube-deploy/pull/298#issuecomment-357366871:

Should we add a new `MachineStatusError` of `JoinClusterTimeoutMachineError` (or similar)?",closed,False,2018-01-16 07:13:27,2018-04-12 16:45:48
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/issues/504,https://api.github.com/repos/kubernetes/kube-deploy/issues/504,Change type of providerConfig,"Extracted from https://github.com/kubernetes/kube-deploy/pull/298#issuecomment-357366871:

Should `providerConfig` change from `string` to `runtime.Object` or `runtime.RawExtension`?",closed,False,2018-01-16 07:14:42,2018-03-20 15:38:44
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/505,https://api.github.com/repos/kubernetes/kube-deploy/issues/505,Update the link to the community owners documentation.,Fixes https://github.com/kubernetes/kube-deploy/issues/499,closed,True,2018-01-16 07:19:36,2018-01-24 17:40:58
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/506,https://api.github.com/repos/kubernetes/kube-deploy/issues/506,Run Code generation tools for the skeleton API server and control manager,"1. It generated all the helpful code including client, informer, type conversion, etc.
2. It generated all the bazel build files.
3. This should be redo everytime when a new field is added into type. ",closed,True,2018-01-16 21:50:11,2018-01-17 00:02:29
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/507,https://api.github.com/repos/kubernetes/kube-deploy/issues/507,Merge type definition from CRD to ApiServer and regenerated all the codes,"Due to the issue below, I have to temporarily remove custom type CloudStatusError and MachineStatusError.
https://github.com/kubernetes-incubator/apiserver-builder/issues/176

",closed,True,2018-01-17 18:42:01,2018-01-17 23:06:26
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/issues/508,https://api.github.com/repos/kubernetes/kube-deploy/issues/508,Abstract the consts into common package from versioned package.,We need to take the fix in https://github.com/kubernetes-incubator/apiserver-builder/pull/208 to get const type definition back into cluster api definition,closed,False,2018-01-18 21:05:33,2018-01-19 22:35:29
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/509,https://api.github.com/repos/kubernetes/kube-deploy/issues/509,Add an common package to hold const which will be shared by versioned package,"It required apiserver-builder v0.1-alpha.27.

fixes https://github.com/kubernetes/kube-deploy/issues/508
",closed,True,2018-01-19 21:46:51,2018-01-19 22:35:29
kube-deploy,qqshfox,https://github.com/kubernetes/kube-deploy/pull/510,https://api.github.com/repos/kubernetes/kube-deploy/issues/510,fix template path for imagebuilder,,closed,True,2018-01-22 14:11:15,2018-08-14 07:36:22
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/511,https://api.github.com/repos/kubernetes/kube-deploy/issues/511,Updating links.,,closed,True,2018-01-22 23:09:41,2018-01-24 17:40:59
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/512,https://api.github.com/repos/kubernetes/kube-deploy/issues/512,Update the name of the slack channel.,,closed,True,2018-01-24 17:40:29,2018-01-24 18:16:32
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/513,https://api.github.com/repos/kubernetes/kube-deploy/issues/513,deploy log should tell the user about node creation latency,"After I created my cluster, the current log message implies the full cluster is created. It took a couple of minutes for my node to show up.",closed,True,2018-01-24 18:09:30,2018-01-24 21:15:03
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/514,https://api.github.com/repos/kubernetes/kube-deploy/issues/514,Use a pre-built node and master image,"Currently, spinning up nodes and masters can take ~minutes. Most of this time is due to apt-get installs. We can cache these by using a pre-built image, and then simply run `kubeadm join`. This will make node startups faster.

Similar for master.

https://github.com/kubernetes/kube-deploy/blob/master/cluster-api-gcp/cloud/google/templates.go#L159",closed,False,2018-01-24 19:18:54,2018-04-17 18:52:55
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/515,https://api.github.com/repos/kubernetes/kube-deploy/issues/515,Spec.ObjectMeta in Machine is not a valid path,"Machine actuator currently looks for changes to spec.metadata [1], but that field doesn't exist [2]. So changes to `generateName` won't WAI for example.

Two possible solutions:
1. Look for machine.metadata in the actuator
2. Pull or reference the metadata block in `spec`.

[1] https://github.com/kubernetes/kube-deploy/blob/master/cluster-api-gcp/cloud/google/machineactuator.go#L471
[2] https://github.com/kubernetes/kube-deploy/blob/master/cluster-api-gcp/machines.yaml#L28",closed,False,2018-01-24 21:40:19,2018-04-17 18:52:15
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/516,https://api.github.com/repos/kubernetes/kube-deploy/issues/516,Local dev story for machine-controller,"Currently, testing changes to the machine-controller is very cumbersome and time-consuming.

We need to push a new machine-controller image, then `pods/pods.go` with the new image, then re-create the whole cluster. This can take ~minutes. We should have a faster dev story - ideally machine-controller running locally (which is possible but not documented).",closed,False,2018-01-25 21:22:44,2018-04-13 15:57:34
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/517,https://api.github.com/repos/kubernetes/kube-deploy/issues/517,Init hookup for new machine controller with cloud package,"I took a copy of current cloud package and modify them to point to new type and client interface. For bug fix to cloud-api-gcp/cloud directory in the following days/weeks, I will do manual cherry-picks. The current machine controller in cloud-api-gcp directory will be replaced by the new one in the end of the process.

Remaining ports:
1. Node watcher
2. Finalizer (new work)




",closed,True,2018-01-25 23:23:37,2018-01-29 21:50:34
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/518,https://api.github.com/repos/kubernetes/kube-deploy/issues/518,Change the type of  instance-status,"Writing the current state of the underlying VM as a `machine` object within another machine is confusing - https://github.com/kubernetes/kube-deploy/blob/master/cluster-api-gcp/cloud/google/instancestatus.go

Before #482 is closed, we could change the type of instance status to a type that only has the fields we care about. This would make the code a bit more readable, `requiresUpdate` in machine actuators a bit more explicit, and we wouldn't have to deal with recursive machines.",closed,False,2018-01-26 18:20:10,2018-04-12 16:46:30
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/519,https://api.github.com/repos/kubernetes/kube-deploy/issues/519,Get status of VM from GCE,#482 ,closed,True,2018-01-26 19:53:32,2018-02-01 22:16:07
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/issues/520,https://api.github.com/repos/kubernetes/kube-deploy/issues/520,Machine controller with namespace,The current implementation assume default namespace. The machine controller need to handle resource from multiple namespace.,closed,False,2018-01-26 23:13:38,2018-04-13 15:55:39
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/521,https://api.github.com/repos/kubernetes/kube-deploy/issues/521,Cluster API to manage existing clusters,This item is to provide functionality to have Cluster API to support existing clusters. Currently we require users to create a new cluster (and migrate workloads) to adopt the Cluster API.,closed,False,2018-01-29 19:58:29,2018-04-13 15:54:26
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/522,https://api.github.com/repos/kubernetes/kube-deploy/issues/522,Support adopting a cluster from a set of existing machines,This item tracks supporting users providing a list of machines that will be used to bootstrap the cluster rather than requiring machines to be added via the cluster API.,closed,False,2018-01-29 21:38:54,2018-04-12 16:46:52
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/523,https://api.github.com/repos/kubernetes/kube-deploy/issues/523,Check if master exists as previous command might have been cancelled,,closed,False,2018-01-29 21:40:58,2018-04-13 16:09:43
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/524,https://api.github.com/repos/kubernetes/kube-deploy/issues/524,Support multiple providers for cluster bootstrapping,Currently only GCP is supported. This item is about - more easily - supporting other providers when bootstrapping a cluster.,closed,False,2018-01-29 21:45:12,2018-04-12 16:47:27
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/525,https://api.github.com/repos/kubernetes/kube-deploy/issues/525,E2E tests for cluster bootstrapping,Basic set of E2E tests for cluster bootstrapping across supported providers.,closed,False,2018-01-29 21:49:35,2018-04-12 16:47:52
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/526,https://api.github.com/repos/kubernetes/kube-deploy/issues/526,Cluster Bootstrapping: document best practices (incl. supported scenarios),,closed,False,2018-01-29 21:52:33,2018-04-12 16:48:13
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/527,https://api.github.com/repos/kubernetes/kube-deploy/issues/527,Document how to tear down a cluster created via Cluster API,,closed,False,2018-01-29 21:55:39,2018-04-12 16:48:40
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/528,https://api.github.com/repos/kubernetes/kube-deploy/issues/528,Support for automated cluster tear down via Cluster API,,closed,False,2018-01-29 21:57:49,2018-04-12 16:49:02
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/529,https://api.github.com/repos/kubernetes/kube-deploy/issues/529,Add machine class to the Cluster API,This issue tracks discussion on adding machine class (as used/suggested in several API discussions) and any work required to make that happen if we decide to do it.,closed,False,2018-01-29 22:00:13,2018-04-12 16:49:27
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/530,https://api.github.com/repos/kubernetes/kube-deploy/issues/530,Design and implement a machine lifecycle state machine,Proposal doc: https://docs.google.com/document/d/1Kz8bmAF6AZXcagjE4robuStPG93VB9YHj5xBiLI0bt0/edit,closed,False,2018-01-29 22:06:07,2018-04-12 16:49:38
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/531,https://api.github.com/repos/kubernetes/kube-deploy/issues/531,Support for hooks for resource cleanup during machine deletion,,closed,False,2018-01-29 22:09:44,2018-04-12 16:49:54
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/532,https://api.github.com/repos/kubernetes/kube-deploy/issues/532,Reconciler for Node + Machine objects,"This reconciler would watch/poll for node changes to reconcile machine and nodes objects. Example scenarios: VM gets deleted manually or nodes are created w/o using cluster API.

This could include metrics to detect if tooling are acting on the cluster and not going through the cluster API.",closed,False,2018-01-29 22:15:04,2018-04-13 15:52:53
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/533,https://api.github.com/repos/kubernetes/kube-deploy/issues/533,Support changing machine role (e.g. node => master),,closed,False,2018-01-29 22:21:07,2018-04-12 16:50:41
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/534,https://api.github.com/repos/kubernetes/kube-deploy/issues/534,GCP provider: support in-place property updates on the machine ,,closed,False,2018-01-29 22:22:33,2018-04-17 18:51:52
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/535,https://api.github.com/repos/kubernetes/kube-deploy/issues/535,Machine Set implementation,"API definition here:
https://github.com/kubernetes/kube-deploy/pull/501

This item tracks machine set implementation. Logic should be independent of the cloud provider.",closed,False,2018-01-29 22:24:58,2018-04-12 16:51:00
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/536,https://api.github.com/repos/kubernetes/kube-deploy/issues/536,MachineSet implementation for scaling up & down,"Some thoughts/questions:
- Choose a specific instance to remove rather than arbitrary instance
- Does scaling down use any smart logic to pick the node to remove?
- Does scaling down drain nodes by default? (But allow removing machines w/o draining)
",closed,False,2018-01-29 22:27:32,2018-04-12 16:51:14
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/537,https://api.github.com/repos/kubernetes/kube-deploy/issues/537,Have a production-ready upgrade functionality (as tool or server side),,closed,False,2018-01-29 22:39:46,2018-04-12 16:51:29
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/538,https://api.github.com/repos/kubernetes/kube-deploy/issues/538,Tool to recover if the master is bricked,"In case metadata can't be accessed/reconstructed, this tool would allow the cluster to be restored with the cluster API components.

Related to https://github.com/kubernetes/kube-deploy/issues/532",closed,False,2018-01-29 22:45:16,2018-04-12 16:51:43
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/539,https://api.github.com/repos/kubernetes/kube-deploy/issues/539,Support multi-master clusters,,closed,False,2018-01-29 22:56:46,2018-04-12 16:51:59
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/540,https://api.github.com/repos/kubernetes/kube-deploy/issues/540,Support upgrading/pivoting cluster to multi-master,,closed,False,2018-01-29 22:57:23,2018-04-12 16:52:41
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/541,https://api.github.com/repos/kubernetes/kube-deploy/issues/541,Cloud Provider implementation for AWS,,closed,False,2018-01-29 23:08:50,2018-04-13 15:50:06
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/542,https://api.github.com/repos/kubernetes/kube-deploy/issues/542,Cloud Provider implementation for Digital Ocean,,closed,False,2018-01-29 23:10:33,2018-04-12 21:21:50
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/543,https://api.github.com/repos/kubernetes/kube-deploy/issues/543,Conformance Tests for Cloud Providers,,closed,False,2018-01-29 23:15:35,2018-04-12 20:58:09
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/544,https://api.github.com/repos/kubernetes/kube-deploy/issues/544,Automate repo tasks for kube-deploy and cluster API,"Urgent:
- [x] Build for every PR
- [x] Run unit tests for every PR (set up test grid for this too)

Soon:
- [ ] Auto-assigner for PRs
- [ ] Build bazel for every PR
- [ ] Auto gen checks for every PR
- [ ] When we modify types, serialization, proto, and client code has to be regenerated
- [ ] When we modify dependencies, bazel BUILD files have to be updated
- [ ] We should verify that all dependencies are in vendor (figure out how dep can do this)
- [ ] Verify gofmt style for every PR
- [ ] Run go vet on every PR",closed,False,2018-01-29 23:38:21,2018-02-15 19:50:41
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/545,https://api.github.com/repos/kubernetes/kube-deploy/issues/545,Document how to contribute to Cluster API effort,,closed,False,2018-01-29 23:39:47,2018-04-12 16:52:59
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/546,https://api.github.com/repos/kubernetes/kube-deploy/issues/546,Add architecture diagram for cluster API,,closed,False,2018-01-29 23:41:24,2018-02-15 18:11:40
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/547,https://api.github.com/repos/kubernetes/kube-deploy/issues/547,Rebase E2E tests to use Cluster API (from kube-up) ,"Benefits as per @roberthbailey 
- More test cycles (we will get hundreds of cluster create / delete cycles per day, hook into the upgrade test infra, etc).
- It will also force us to build enough configurability to do all of the edge cases demanded by the tests
- This can be used to convince ourselves that the API is solid and flexible (e.g. if we end up with un-readable templating like kube-up or k8s-anywhere we haven't succeeded)",closed,False,2018-01-29 23:45:09,2018-04-12 16:53:12
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/548,https://api.github.com/repos/kubernetes/kube-deploy/issues/548,Design on cluster API integration with cluster registry,,closed,False,2018-01-30 01:51:41,2018-04-13 15:44:51
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/549,https://api.github.com/repos/kubernetes/kube-deploy/issues/549,Add contributing guidelines for cluster-api and dev instructions for cluster-api-gcp,"#545 

Top-level guidelines from https://github.com/kubernetes/ingress-gce/blob/master/CONTRIBUTING.md

",closed,True,2018-01-30 21:33:39,2018-02-01 23:12:07
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/550,https://api.github.com/repos/kubernetes/kube-deploy/issues/550,Publish the Cluster API contract for provider implementations,,closed,False,2018-01-30 23:05:08,2018-02-06 23:03:16
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/551,https://api.github.com/repos/kubernetes/kube-deploy/issues/551,Add more user friendly message if kubectl isn't configured,,closed,True,2018-02-02 17:28:30,2018-02-02 23:35:41
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/552,https://api.github.com/repos/kubernetes/kube-deploy/issues/552,Add me (rsdcastro) to the list of repo maintainers,,closed,True,2018-02-02 18:00:58,2018-02-02 23:35:49
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/553,https://api.github.com/repos/kubernetes/kube-deploy/issues/553,Add unit tests for cluster-api,,closed,False,2018-02-02 18:37:44,2018-02-06 23:14:37
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/554,https://api.github.com/repos/kubernetes/kube-deploy/issues/554,Add unit tests for GCP machine controller,,closed,False,2018-02-02 18:38:01,2018-04-17 18:50:31
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/555,https://api.github.com/repos/kubernetes/kube-deploy/issues/555,Remove hard-coded project IDs (using gcloud for that info instead) + update CONTRIBUTING,"Update CONTRIBUTING to include required steps, like setting GCP credentials and installing Docker.
Also add gcloud step as it's very helpful if one is developing against GCP and gives us a central project ID store.
Update places where hard-coded projects were used to fetch it from gcloud.",closed,True,2018-02-02 21:45:36,2018-02-02 23:36:13
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/556,https://api.github.com/repos/kubernetes/kube-deploy/issues/556,Add docs explaining basic instructions for adding a new provider,"These are some instructions for adding support for a provider. #550

It's short on code because we are moving to an external apiserver, so our controller is likely going to see some changes. In the meantime, the contracts here should help contributors start implementing controllers and actuators.",closed,True,2018-02-02 23:50:48,2018-02-06 19:36:10
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/557,https://api.github.com/repos/kubernetes/kube-deploy/issues/557,Reconcile instructions in README.md and CONTRIBUTING.md ,"Reconcile instructions to avoid duplicate/conflicting info. In particular, add firewall instructions and use gcloud to help with the credentials (avoiding some manual steps).",closed,True,2018-02-02 23:52:23,2018-02-07 21:05:36
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/558,https://api.github.com/repos/kubernetes/kube-deploy/issues/558,"Cluster creation fails with ""service account kube-system/default was not found""","$ ./cluster-api-gcp create -c cluster.yaml -m machines.yaml
I0202 14:33:23.988067  199613 deploy_helper.go:59] Starting cluster creation test1
I0202 14:33:23.988117  199613 deploy_helper.go:61] Starting master creation gce-master-test1
I0202 14:33:25.919210  199613 machineactuator.go:556] Wait for insert ""operation-1517610804617-564424cea9f29-ded330e2-484dc1dc""...
I0202 14:33:36.205160  199613 machineactuator.go:566] Finish wait for insert ""operation-1517610804617-564424cea9f29-ded330e2-484dc1dc""...
I0202 14:33:36.205183  199613 deploy_helper.go:68] Created master gce-master-test1
I0202 14:33:53.203800  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:34:14.445875  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:34:35.698723  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:34:43.438533  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:35:04.624464  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:35:11.781288  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:35:26.118414  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:35:33.360465  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:35:40.485575  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:35:47.651470  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:35:54.760272  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:36:01.896655  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:36:09.017111  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:36:16.202107  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:36:23.318612  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:36:30.448822  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:36:37.593320  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:36:44.800439  199613 deploy_helper.go:235] Waiting for Kubernetes to come up...
I0202 14:36:51.939569  199613 deploy_helper.go:265] wrote kubeconfig to [/usr/local/google/home/rdc/.kube/config]
I0202 14:36:51.939611  199613 deploy_helper.go:85] Waiting for apiserver to become healthy...
I0202 14:37:52.536023  199613 deploy_helper.go:93] Starting the machine controller...
I0202 14:38:08.461145  199613 pods.go:65] Error scheduling machine controller. Will retry...
I0202 14:38:13.520920  199613 pods.go:65] Error scheduling machine controller. Will retry...
I0202 14:38:18.223187  199613 pods.go:65] Error scheduling machine controller. Will retry...
I0202 14:38:23.380938  199613 pods.go:65] Error scheduling machine controller. Will retry...
I0202 14:38:28.061903  199613 deploy.go:124] Deleting master vm gce-master-test1
I0202 14:38:28.334304  199613 machineactuator.go:530] Skipping uid check since instance gke-rdc-test us-central1-f gce-master-test1 is missing uid label due to being provisioned as part of bootstrap.
I0202 14:38:28.935925  199613 machineactuator.go:556] Wait for delete ""operation-1517611108338-564425f050952-fe833c99-5ce12330""...
I0202 14:39:20.465375  199613 machineactuator.go:566] Finish wait for delete ""operation-1517611108338-564425f050952-fe833c99-5ce12330""...
F0202 14:39:23.390624  199613 create.go:50] can't create machine controller: couldn't start machine controller: couldn't create pod: exit status 1, output: Error from server (Forbidden): error when creating ""STDIN"": pods ""machine-controller"" is forbidden: service account kube-system/default was not found, retry after the service account is created

It seems that the prior to deploying the machine controller, we need to make sure that the kube-system namespace has been created by the controller manager.",closed,False,2018-02-03 00:04:39,2018-02-05 15:10:12
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/559,https://api.github.com/repos/kubernetes/kube-deploy/issues/559,Wait for service account before running machine controller,"Fixes #558 

We'll need to do is update the vendor lock in cluster-api-gcp after this PR is merged.",closed,True,2018-02-03 01:32:41,2018-02-05 18:14:04
kube-deploy,jsleeio,https://github.com/kubernetes/kube-deploy/pull/560,https://api.github.com/repos/kubernetes/kube-deploy/issues/560,(opt) assign an IAM instance profile to EC2 instances,"If the `InstanceProfile` option is included in the configuration file,
assign an IAM instance profile to the instance used to build the AMI.

Example uses:

* access external secrets/resources
* pre-cache private Docker images from ECR (my use case!)

I'm not sufficiently familiar with GCP to contribute a functionally
matching feature for that platform, unfortunately. Sorry.

I haven't completed CLA paperwork yet but will do ASAP.",closed,True,2018-02-05 11:25:06,2018-02-05 20:01:59
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/561,https://api.github.com/repos/kubernetes/kube-deploy/issues/561,Revendor dependencies,"Fixes:
$ go build
deploy/deploy_helper.go:305:17: undefined: ""k8s.io/kube-deploy/cluster-api-gcp/vendor/k8s.io/kube-deploy/cluster-api/util"".NewKubernetesClient

Commands:
dep ensure -add k8s.io/kube-deploy/cluster-api
dep ensure -update",closed,True,2018-02-05 17:44:59,2018-02-07 21:05:30
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/562,https://api.github.com/repos/kubernetes/kube-deploy/issues/562,A common retry logic,"Instead of reimplementing retry logic, we should use a standard way to retrying (ideally an external package that's tested and maintained).",closed,False,2018-02-05 17:49:08,2018-03-06 17:08:36
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/563,https://api.github.com/repos/kubernetes/kube-deploy/issues/563,How to vendor cluster-api in cluster-api-gcp?,It's confusing how dep works. So we need to document how to update the vendored cluster-api into provider-specific controller.,closed,False,2018-02-05 19:06:05,2018-02-06 23:17:31
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/issues/564,https://api.github.com/repos/kubernetes/kube-deploy/issues/564,Automate new docker image builder with new apiserver and controller manager,"It was manually built with ""apiserver-boot build container --image "". However, it needs to be extended with all the additional configuration such as openssh stuff in cluster-api-gcp/machine-controller.",closed,False,2018-02-05 19:53:36,2018-04-17 18:50:19
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/issues/565,https://api.github.com/repos/kubernetes/kube-deploy/issues/565,Remove the dependency with code layout,"Currently, gcp-deployer has to be run from ext-apiserver top directory. Similarly, cluster-api-gcp does the same. We need to break this dependency.",closed,False,2018-02-05 19:56:37,2018-04-13 16:35:41
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/issues/566,https://api.github.com/repos/kubernetes/kube-deploy/issues/566,Make namespace configurable,The current machine controller assumes default namespace. We need to make it configurable. It also include some additional work in deployer tool to make it configurable.,closed,False,2018-02-05 19:58:28,2018-02-07 06:43:17
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/issues/567,https://api.github.com/repos/kubernetes/kube-deploy/issues/567,Use dedicated service account for machine controller pod,The current code is using default service account combined an admin privileged kubeconfig. We need to strip down to appropriate access permission with a dedicated service account.,closed,False,2018-02-05 20:00:10,2018-04-17 18:50:10
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/568,https://api.github.com/repos/kubernetes/kube-deploy/issues/568,Revendor kube-deploy,"Fixes:
$ go build
deploy/deploy_helper.go:305:17: undefined: ""k8s.io/kube-deploy/cluster-api-gcp/vendor/k8s.io/kube-deploy/cluster-api/util"".NewKubernetesClient

Commands:
$ dep ensure -update k8s.io/kube-deploy

(ensure add didn't work: 
$ dep ensure -add k8s.io/kube-deploy/cluster-api
$ dep ensure)
",closed,True,2018-02-05 20:47:34,2018-02-07 21:05:23
kube-deploy,jsleeio,https://github.com/kubernetes/kube-deploy/pull/569,https://api.github.com/repos/kubernetes/kube-deploy/issues/569,(opt) assign an IAM instance profile to EC2 instances (CLA fixed),"If the `InstanceProfile` option is included in the configuration file,
assign an IAM instance profile to the instance used to build the AMI.

Example uses:

access external secrets/resources
pre-cache private Docker images from ECR (my use case!)
I'm not sufficiently familiar with GCP to contribute a functionally
matching feature for that platform, unfortunately. Sorry.",closed,True,2018-02-05 21:03:04,2018-02-08 23:00:53
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/570,https://api.github.com/repos/kubernetes/kube-deploy/issues/570,Update messages on service account to be explicit about GCP service account,,closed,True,2018-02-05 21:20:22,2018-02-07 21:05:48
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/571,https://api.github.com/repos/kubernetes/kube-deploy/issues/571,"Rerun cluster creation fails with ""Error from server (AlreadyExists): secrets ""machine-controller-credential"" already exists""","Repro:
1. Run cluster-api-gcp create successfully
2. Rerun cluster-api-gcp

Console log messages:
I0205 13:32:06.425574  245322 deploy_helper.go:59] Starting cluster creation test1
I0205 13:32:06.425615  245322 deploy_helper.go:61] Starting master creation gce-master-test1
I0205 13:32:07.109758  245322 machineactuator.go:530] Skipping uid check since instance gke-rdc-test us-central1-f gce-master-test1 is missing uid label due to being provisioned as part of bootstrap.
I0205 13:32:07.109801  245322 machineactuator.go:282] Skipped creating a VM that already exists.
I0205 13:32:07.109815  245322 deploy_helper.go:68] Created master gce-master-test1
I0205 13:32:10.202457  245322 deploy_helper.go:274] wrote kubeconfig to [/usr/local/google/home/rdc/.kube/config]
I0205 13:32:10.202494  245322 deploy_helper.go:85] Waiting for apiserver to become healthy...
I0205 13:32:10.368851  245322 deploy_helper.go:94] Waiting for the service account to exist...
I0205 13:32:10.542683  245322 deploy_helper.go:99] Starting the machine controller...
I0205 13:32:17.045775  245322 deploy.go:124] Deleting master vm gce-master-test1
I0205 13:32:17.195977  245322 machineactuator.go:530] Skipping uid check since instance gke-rdc-test us-central1-f gce-master-test1 is missing uid label due to being provisioned as part of bootstrap.
I0205 13:32:17.839638  245322 machineactuator.go:556] Wait for delete ""operation-1517866337201-5647dcbd86b6a-c50b9ed4-a799a00e""...
I0205 13:33:14.843849  245322 machineactuator.go:566] Finish wait for delete ""operation-1517866337201-5647dcbd86b6a-c50b9ed4-a799a00e""...
I0205 13:33:14.843951  245322 serviceaccount.go:107] No service a/c found in cluster.
F0205 13:33:14.843970  245322 create.go:50] can't create machine controller: couldn't import service account key as credential: error: exit status 1, output: Error from server (AlreadyExists): secrets ""machine-controller-credential"" already exists",closed,False,2018-02-05 21:36:13,2018-02-26 20:27:46
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/572,https://api.github.com/repos/kubernetes/kube-deploy/issues/572,Plumbing cluster-api-gcp to use new API type,"1. The first commit took a copy of cluster-api-gcp into ext-apiserver/gcp-deployer, and fixed all the new imports.
2. The second commit finished the remaining plumbing for the new tools to deploy the new apiserver and controller as an in-cluster configuration.
3. Create (nodes) creation works . Deletion still depends on finalizer work.
4. I opened a bunch of issues with apiserver tag which can be worked in parallel once this PR is merged.
",closed,True,2018-02-05 22:41:24,2018-02-12 21:52:36
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/573,https://api.github.com/repos/kubernetes/kube-deploy/issues/573,Allow GCP service account K8s namespace to be configurable,See https://github.com/kubernetes/kube-deploy/pull/570 for context.,closed,False,2018-02-06 17:53:38,2018-04-17 18:49:38
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/574,https://api.github.com/repos/kubernetes/kube-deploy/issues/574,glog.Warn is undefined.,,closed,True,2018-02-06 18:27:01,2018-02-06 18:31:13
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/575,https://api.github.com/repos/kubernetes/kube-deploy/issues/575,Start adding unit tests for GCP actuator,"#554

A few notes:

1. It's clear to me that K8s values e2e over unit tests. So we shouldn't be aiming for high coverage here, but instead testing critical/complex pieces of logic.
2. I can't add all such unit tests myself, so I'll need help from contributors.

cc @kubernetes/sig-cluster-lifecycle",closed,True,2018-02-06 19:30:04,2018-02-08 18:47:44
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/576,https://api.github.com/repos/kubernetes/kube-deploy/issues/576,Have a single CONTRIBUTING.md for kube-deploy and move running instructions to its own file,"This is about having running instructions as a separate doc and have both README.md and CONTRIBUTING.md pointing to it, rather than having README.md linking to the more complex CONTRIBUTING.md doc.",closed,False,2018-02-06 23:13:03,2018-04-12 20:59:36
kube-deploy,prateekgogia,https://github.com/kubernetes/kube-deploy/pull/577,https://api.github.com/repos/kubernetes/kube-deploy/issues/577,"Fixes #424, merge new kube config with existing config file","New cluster kube config files gets merged with existing config file -

- If kubeconfig file flag is passed, new config is merged with provided file, defaults to (~/.kube/config).
- Default context is set to new cluster created.",closed,True,2018-02-06 23:32:37,2018-02-14 23:46:37
kube-deploy,prateekgogia,https://github.com/kubernetes/kube-deploy/issues/578,https://api.github.com/repos/kubernetes/kube-deploy/issues/578,Failed to create multiple clusters using cluster-api-gcp,"The first cluster came up fine -- 

`./cluster-api-gcp create -c cluster.yaml -m machines.yaml.template
I0206 16:16:26.676607   28506 deploy.go:75] You can now kubectl get nodes. Note: you might only see the master for the first few minutes while the machine controller spins up the node machines.`

Tried to create a second cluster with same template file and command, this fails (Logs pasted in the end). 

Problem is --  Re-running the same command as above, it gets the same master VM name, however, the template has name `generateName: gce-master-`. This should create a new master VM name and a new cluster should come up. However, it ends up removing the existing master for the cluster created in above step.


**Logs** for the failure case, after running the above command - 

./cluster-api-gcp create -c cluster.yaml -m machines.yaml.template
I0206 16:18:07.541249   28881 deploy_helper.go:62] Starting cluster creation test1
I0206 16:18:07.541329   28881 deploy_helper.go:64] Starting master creation **_gce-master-test1_**
I0206 16:18:08.563274   28881 machineactuator.go:530] Skipping uid check since instance kube-project-185821 us-west1-a gce-master-test1 is missing uid label due to being provisioned as part of bootstrap.
I0206 16:18:08.564259   28881 machineactuator.go:282] Skipped creating a VM that already exists.
I0206 16:18:08.564267   28881 deploy_helper.go:71] Created master gce-master-test1
I0206 16:18:10.819486   28881 deploy_helper.go:268] wrote kubeconfig to [/Users/pgogia/.kube/config]
I0206 16:18:10.819522   28881 deploy_helper.go:88] Waiting for apiserver to become healthy...
I0206 16:18:10.951917   28881 deploy_helper.go:96] Starting the machine controller...
I0206 16:18:18.186471   28881 deploy.go:124] **_Deleting master vm gce-master-test1_**
I0206 16:18:18.419326   28881 machineactuator.go:530] Skipping uid check since instance kube-project-185821 us-west1-a gce-master-test1 is missing uid label due to being provisioned as part of bootstrap.
I0206 16:18:18.902713   28881 machineactuator.go:556] Wait for delete ""operation-1517962698367-564943b6b2019-bdd74ac4-e4d8ee2d""...
I0206 16:19:00.268373   28881 machineactuator.go:566] Finish wait for delete ""operation-1517962698367-564943b6b2019-bdd74ac4-e4d8ee2d""...
I0206 16:19:00.268838   28881 serviceaccount.go:107] No service a/c found in cluster.
F0206 16:19:00.268863   28881 create.go:50] can't create machine controller: couldn't import service account key as credential: error: exit status 1, output: Error from server (AlreadyExists): secrets ""machine-controller-credential"" already exists
",closed,False,2018-02-07 00:34:13,2018-02-07 01:18:55
kube-deploy,prateekgogia,https://github.com/kubernetes/kube-deploy/issues/579,https://api.github.com/repos/kubernetes/kube-deploy/issues/579,Delete command should allow to delete a specific cluster,"Currently, delete command for `cluster-api-gcp` doesn't take any cluster info. We should be able to specify which cluster to delete. 
If a user has more then one cluster, there is no way right now to delete a specific cluster.
",closed,False,2018-02-07 00:40:40,2018-04-13 15:59:54
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/580,https://api.github.com/repos/kubernetes/kube-deploy/issues/580,Add a pull request template,Copied from the main repo (https://github.com/kubernetes/kubernetes/blob/master/.github/PULL_REQUEST_TEMPLATE.md) with the addition of a cc for the repository maintainers on each pull request. ,closed,True,2018-02-07 06:08:29,2018-02-07 16:09:10
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/581,https://api.github.com/repos/kubernetes/kube-deploy/issues/581,Implement finalizer support for machine resource,gcp-deployer now support machine/cluster deletion.,closed,True,2018-02-07 07:50:44,2018-02-12 22:04:31
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/582,https://api.github.com/repos/kubernetes/kube-deploy/issues/582,Start using apimachinery backoff for retries,"**What this PR does / why we need it**:

First stab at using an existing 3p package for retries/timeout using https://github.com/cenkalti/backoff which seems to be widely used and mature. I've implemented the new retry for 3 steps:

1. Get and write kubeconfig
2. Wait for apiserver
3. Wait for service account

Next step would be to use it for GCE calls as well. (#407)

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
#562 

For now I've implemented exponential backoff, and tested the change using cluster create and delete commands.

@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-07 21:30:04,2018-03-01 00:00:56
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/583,https://api.github.com/repos/kubernetes/kube-deploy/issues/583,Cleanup,"**What this PR does / why we need it**: So we can build.


@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-08 17:41:28,2018-02-08 18:02:45
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/584,https://api.github.com/repos/kubernetes/kube-deploy/issues/584,Add a couple of scripts for CI,"**What this PR does / why we need it**: This is the start of build scripts for basic CI building and testing. 

**Special notes for your reviewer**: This does not turn on CI for this repo, just defines basic steps for CI to run. After this is in, I will work on getting CI to use this.

@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-08 19:32:18,2018-02-12 19:23:25
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/585,https://api.github.com/repos/kubernetes/kube-deploy/issues/585,Firewall creation should be after gcloud was installed,"Firewall creation should be after the gcloud installation instructions, as it depends on gcloud.

@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-08 19:46:11,2018-02-08 19:56:32
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/586,https://api.github.com/repos/kubernetes/kube-deploy/issues/586,Firewall creation should be after gcloud was installed,"Firewall uses gcloud, so instructions to create firewall should be after gcloud's.

@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-08 19:53:34,2018-02-09 19:38:53
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/587,https://api.github.com/repos/kubernetes/kube-deploy/issues/587,Make OS image configurable (and update it to a more recent version),"https://github.com/kubernetes/kube-deploy/blob/master/cluster-api-gcp/cloud/google/machineactuator.go#L670

Code above uses Ubuntu 1604 as GCP's VM image. This issue tracks making this configurable and update the code to use a more recent image.",closed,False,2018-02-08 21:47:43,2018-03-01 00:07:56
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/588,https://api.github.com/repos/kubernetes/kube-deploy/issues/588,Implement node watcher functionality in new controller,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Add Node Watcher functionality into the new controller.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:
Please pay attention to the last commit as it's based off other pending PR. https://github.com/kubernetes/kube-deploy/commit/b7e93e635d39fa16fb93208755b9dd6690c3fb2b

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-08 22:48:01,2018-02-13 23:51:52
kube-deploy,mvladev,https://github.com/kubernetes/kube-deploy/issues/589,https://api.github.com/repos/kubernetes/kube-deploy/issues/589,Split Machine and Cluster into different API groups,"Both `Machine` and `Cluster` are in the same `cluster.k8s.io` group. The main focus right it seems to be on the `Machine` types and can substantially slow the `v1alpha1` goals.

I suggest we move `Machine` API types to new API group `machine.k8s.io` or `machines.k8s.io`.",closed,False,2018-02-09 08:57:10,2018-02-14 22:16:21
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/590,https://api.github.com/repos/kubernetes/kube-deploy/issues/590,Remove newline from test script,"**What this PR does / why we need it**: So it works with our test-infra


@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-09 17:21:29,2018-02-09 17:22:53
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/issues/591,https://api.github.com/repos/kubernetes/kube-deploy/issues/591,Remove the apiserver-boot dependency for generate certificate.,We should remove the need to run apiserver-boot to generate certificate for running ext api server. The logs of apiserver-boot shows the required openssl commands to do the job.,closed,False,2018-02-09 18:09:04,2018-04-12 21:00:03
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/592,https://api.github.com/repos/kubernetes/kube-deploy/issues/592,Cloud Provider implementation for Vmware/vSphere,,closed,False,2018-02-10 00:33:10,2018-04-13 15:58:34
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/593,https://api.github.com/repos/kubernetes/kube-deploy/issues/593,GCP support for Ubuntu image,"This about tracking the current support for Ubuntu to be production ready.

Ideally this should be agnostic so all providers can use it.",closed,False,2018-02-12 18:15:16,2018-04-17 18:49:28
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/594,https://api.github.com/repos/kubernetes/kube-deploy/issues/594,GCP support for RHEL images,See #593 for reference.,closed,False,2018-02-12 18:17:19,2018-04-17 18:49:17
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/595,https://api.github.com/repos/kubernetes/kube-deploy/issues/595,GCP support for Container OS,See #593 for reference.,closed,False,2018-02-12 18:18:08,2018-04-17 18:49:06
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/596,https://api.github.com/repos/kubernetes/kube-deploy/issues/596,Move Ubuntu installation script out of Cluster API GCP,"The goal is to have that as reusable by other providers, as long as the base image is the same.",closed,False,2018-02-12 18:19:55,2018-04-17 18:48:52
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/597,https://api.github.com/repos/kubernetes/kube-deploy/issues/597,Fix the machine controller tests,"Added config initialization to make it use the logging actuator, and
changed it to have a cluster created as well.

**What this PR does / why we need it**: So our tests don't panic and crash.

@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-12 19:18:00,2018-02-12 22:08:50
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/598,https://api.github.com/repos/kubernetes/kube-deploy/issues/598,Clear finalizer in test so it can actually delete,"**What this PR does / why we need it**: It fixes a unit test.

ref: #544 

@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-13 03:11:40,2018-02-13 14:52:51
kube-deploy,prateekgogia,https://github.com/kubernetes/kube-deploy/pull/599,https://api.github.com/repos/kubernetes/kube-deploy/issues/599,"#571 generate random name, if generateName field set in spec","Fixes #571 

Concatenate random name, with generated name, this works currently for node names, but not for master node names. This PR fixes this.",closed,True,2018-02-14 01:57:54,2018-02-26 22:05:29
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/600,https://api.github.com/repos/kubernetes/kube-deploy/issues/600,Backup for cluster API objects (etcd database),,closed,False,2018-02-14 15:41:25,2018-04-12 21:00:29
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/601,https://api.github.com/repos/kubernetes/kube-deploy/issues/601,Machine controller upgrade documentation ,This should consider the different architectures where the controller might be running.,closed,False,2018-02-14 15:43:39,2018-04-12 21:00:44
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/602,https://api.github.com/repos/kubernetes/kube-deploy/issues/602,Fix deps,"**What this PR does / why we need it**: This switches from glide to dep for dependency management of the ext-apiserver directory.

**Special notes for your reviewer**: The first commit is the changes in the configuration files. The second commit is the result of running `dep ensure`.


<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-14 17:38:58,2018-02-14 18:42:53
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/603,https://api.github.com/repos/kubernetes/kube-deploy/issues/603,Add Cluster API architecture diagram to README.md,Fixes #546 ,closed,True,2018-02-14 18:17:16,2018-02-15 18:11:40
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/604,https://api.github.com/repos/kubernetes/kube-deploy/issues/604,Reconcile Cluster API Cloud Provider with the effort of moving Cloud Providers out of core K8s,"This issue is to track the discussion and whether or not those should be combined in some way.

cc @kris-nova @karan @roberthbailey",closed,False,2018-02-14 18:35:24,2018-04-12 21:01:16
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/605,https://api.github.com/repos/kubernetes/kube-deploy/issues/605,bake deploy template into gcp-deployer,"The template was a on-disk file. It implies the gcp-deployer (old
cluster-api-gcp) has to be running from root directory.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
It tries to eliminate the dependency in which gcp-deployer has to be running from root directory of the project. 

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:
It just eliminate one of the dependency. There will be more PR coming to address the remaining dependency.

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-14 23:32:49,2018-02-15 22:48:39
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/606,https://api.github.com/repos/kubernetes/kube-deploy/issues/606,Use openssl to generate certs,"It removes the dependency with apiserver-boot.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-15 06:42:19,2018-02-16 21:24:38
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/607,https://api.github.com/repos/kubernetes/kube-deploy/issues/607,Update client dependency to fix a race condition,"Ran dep ensure -update k8s.io/client-go in the ext-apiserver directory.

**What this PR does / why we need it**: Pulls in a fix for a race condition that is making tests flake.

@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-15 16:39:58,2018-02-15 19:29:21
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/608,https://api.github.com/repos/kubernetes/kube-deploy/issues/608,Document how to access extension API server in case of a disaster,"Clients should be accessing cluster API via the main K8s API server. In case of a disaster with the cluster, clients should be able to connect to the ext API server. This issue is to test and document what would be required for that to happen.

(see PR with architecture diagram below to help: https://github.com/kubernetes/kube-deploy/pull/603/files)",closed,False,2018-02-15 16:42:46,2018-04-12 21:04:00
kube-deploy,dgoodwin,https://github.com/kubernetes/kube-deploy/issues/609,https://api.github.com/repos/kubernetes/kube-deploy/issues/609,Establish a stronger link between Machine and Cluster.,"On Feb 14 Cluster API working group call, a couple parties indicated we would like to make use of the ability to have multiple clusters to exist in one namespace, which would free up namespaces to be used for other levels of organization. (one customer, one cloud provider account, etc) In this case we would not be able to assume all machines or machine sets in a namespace belong to the single cluster in the namespace, and as such a stronger link between the two could be useful for some adopters of the cluster API.",closed,False,2018-02-15 17:12:15,2018-04-12 21:03:11
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/610,https://api.github.com/repos/kubernetes/kube-deploy/issues/610,Switch to listing clusters on demand instead of caching them,"**What this PR does / why we need it**:

Relying on the cluster lister can sometimes cause the machine reconcicliation to fail without being able to find a cluster since the machine creation event can be received before the cluster creation event. Normally this doesn't matter because it is retried in a loop. It is, however, causing the unit test to flake about 2% of the time.  This refactor makes the test more reliable (0 failures out of 500). The difference may not seem like much, but will become annoying when PR testing is turned on.

Outside of testing concerns, we would likely have to make sure we fetch the latest version of the cluster anyway like we do for machine elsewhere. This also allows for getCluster to locate the cluster based on the machine in the future when we change how machines and clusters are associated as #609 proposes.



@kubernetes/kube-deploy-reviewers
/lint",closed,True,2018-02-15 17:54:34,2018-02-15 18:04:40
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/issues/611,https://api.github.com/repos/kubernetes/kube-deploy/issues/611,Convert Ginkgo test to standard testing framework.,"There are currently 3 test suites that use the [Ginkgo and Gomega](http://onsi.github.io/ginkgo/) framework. These tests were generated from the `apiserver-boot` tool. 

Tests needing conversion:

```console
kube-deploy> grep -l  ""github.com/onsi/"" **/*.go | grep -v vendor | xargs dirname | sort -u
ext-apiserver/pkg/apis/cluster/v1alpha1
ext-apiserver/pkg/controller/cluster
ext-apiserver/pkg/controller/machine
```",closed,False,2018-02-15 19:18:10,2018-02-24 00:46:46
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/issues/612,https://api.github.com/repos/kubernetes/kube-deploy/issues/612,Enable auto assigner for kube-deploy PR reviews,Figure out what is needed to auto assign PRs to reviewers. This will help distribute the load and also spread the mind-share amongst interested community members.,closed,False,2018-02-15 19:45:02,2018-04-13 21:32:33
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/issues/613,https://api.github.com/repos/kubernetes/kube-deploy/issues/613,Run more automated checks on kube-deploy repo,"We should run the following checks on PRs and in CI:

- [ ] go vet
- [ ] gofmt
- [ ] verify vendor depencies are correct (this is partially accomplished by build today, but it'd be nice to have better messaging around this)
- [ ] code doesn't need to be regenerated (deepcopy, conversion, clients, ...)
- [ ] BUILD files don't have to be regenerated",closed,False,2018-02-15 19:50:04,2018-04-12 21:04:49
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/614,https://api.github.com/repos/kubernetes/kube-deploy/issues/614,Cherry-pick the MachineAPI chanage from cluster-api to ext-apiserver,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
This PR pick up PR Minimalistic Machines API proposal (https://github.com/kubernetes/kube-deploy/pull/298)

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-15 20:39:33,2018-02-16 19:32:40
kube-deploy,p0lyn0mial,https://github.com/kubernetes/kube-deploy/pull/615,https://api.github.com/repos/kubernetes/kube-deploy/issues/615,updates generated code for MachineSet,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**: this PR updates generated code for MachineSet type

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```
NONE

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-16 14:17:44,2018-02-16 16:52:39
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/616,https://api.github.com/repos/kubernetes/kube-deploy/issues/616,Cherry-pick the machineset API frm clusterapi to ext-apiserver.,"apiserver-boot create group version resource --group cluster --version v1alpha1 --kind MachineSet
apiserver-boot build generated

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
This PR pull the machineset API change. From now on, any machineset API change should update both place until we formally delete cluster-api directory.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-16 21:42:21,2018-02-27 19:35:46
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/617,https://api.github.com/repos/kubernetes/kube-deploy/issues/617,Port the tools to use new API.,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

This PR copies/fixes our demo tools to use new API. The older tools along with cluster-api directory will be deleted once we switched everything over.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:
Note that it's a purely copy with import and type definition fix-up.
**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-16 23:45:46,2018-02-17 01:47:39
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/618,https://api.github.com/repos/kubernetes/kube-deploy/issues/618,Copy the documentation over to the new location.,"Fixed all the new name and links. The older one will be deleted once we switched everything over.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-17 00:20:05,2018-02-28 23:47:56
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/619,https://api.github.com/repos/kubernetes/kube-deploy/issues/619,"Cluster provisioning ""succeeds"" despite machine-controller failing to start","Logs from cluster-api-gcp:

I0220 15:20:25.802792   19570 deploy_helper.go:68] Created master gce-master-test1
I0220 15:21:05.422987   19570 deploy_helper.go:241] Waiting for Kubernetes to come up...
I0220 15:21:13.538578   19570 deploy_helper.go:241] Waiting for Kubernetes to come up...
I0220 15:21:21.449076   19570 deploy_helper.go:241] Waiting for Kubernetes to come up...
I0220 15:21:29.529327   19570 deploy_helper.go:241] Waiting for Kubernetes to come up...
I0220 15:21:37.577458   19570 deploy_helper.go:274] wrote kubeconfig to [/Users/rdc/.kube/config]
I0220 15:21:37.577530   19570 deploy_helper.go:85] Waiting for apiserver to become healthy...
I0220 15:22:14.658776   19570 deploy_helper.go:94] Waiting for the service account to exist...
I0220 15:22:25.097828   19570 deploy_helper.go:99] Starting the machine controller...
I0220 15:22:46.073147   19570 deploy_helper.go:136] Clusters CRD created succuessfully!
I0220 15:22:46.849643   19570 deploy_helper.go:160] Machines CRD created successfully!
I0220 15:22:46.981945   19570 deploy_helper.go:176] Added machine [gce-master-test1]
I0220 15:22:47.088406   19570 deploy_helper.go:176] Added machine [gce-node-l71sp]
I0220 15:22:47.088443   19570 deploy.go:74] The [test1] cluster has been created successfully!
I0220 15:22:47.088455   19570 deploy.go:75] You can now `kubectl get nodes`. Note: you might only see the master for the first few minutes while the machine controller spins up the node machines.

But if one checks the machine controller logs:

$ kubectl logs machine-controller -n kube-system
F0220 23:34:12.548770      80 machinecontroller.go:71] error creating machine actuator: Not recognized cloud provider: terraform

This means that ideally here we can find a way to check if the machine controller is actually up & running to act on the machine objects before returning success to the user.

",closed,False,2018-02-20 23:38:55,2018-04-13 15:48:43
kube-deploy,p0lyn0mial,https://github.com/kubernetes/kube-deploy/pull/620,https://api.github.com/repos/kubernetes/kube-deploy/issues/620,initial implementation of machineset controller,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**: this PR brings an initial implementation of `MachineSet` controller. It watches for `MachineSet` resources and makes sure that the current state is equal to the desired one. At the moment the current state of the cluster is calculated based on the number of machines that are owned by the given machineSet resource. This essentially means that newly created resources have their `OwnerReferences`  field set to the given `MachineSet` resource.

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-21 17:29:52,2018-03-12 02:51:10
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/621,https://api.github.com/repos/kubernetes/kube-deploy/issues/621,Define strategy on mapping OS images across cloud providers to the scripts that support that OS flavor + version ,"- Scripts that install the required K8s on the machine should be made agnostic of cloud provider for reuse (#596)
- Users can specify an image, or if not specified, a default OS image is picked
- OS Images are composed of the flavor (Ubuntu, Debian, CentOS) and version
- Scripts for K8s installation can support one or more versions (and potentially similar flavors) and should be easily mapped to the images that it supports in order to simplify provider actuators",closed,False,2018-02-22 17:42:03,2018-04-12 21:19:28
kube-deploy,kcoronado,https://github.com/kubernetes/kube-deploy/pull/622,https://api.github.com/repos/kubernetes/kube-deploy/issues/622,Convert ginko tests to standard testing framework,"**What this PR does / why we need it**:
This PR converts the three test suites using the Ginkgo and Gomega framework to use the standard Go testing framework.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #611

**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-22 20:58:31,2018-02-26 17:35:50
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/623,https://api.github.com/repos/kubernetes/kube-deploy/issues/623,Define validation strategy for provider config,"As per comment in #405, ""validation is another concern and independent of specifying the cloud provider in the top level object or in the provider specific config. We can't bake provider specific validation logic into the API. That needs to be pluggable somehow through a webhook, admission controller, or some other mechanism. But it's a much larger discussion."" 

This issue tracks methods we'll allow for validation, including recommendations and examples.

cc @karan ",closed,False,2018-02-22 21:13:45,2018-04-12 21:05:42
kube-deploy,kcoronado,https://github.com/kubernetes/kube-deploy/pull/624,https://api.github.com/repos/kubernetes/kube-deploy/issues/624,Make OS image configurable,"**What this PR does / why we need it**:
This PR pulls the OS image from provider configs instead of always using a default image. I added logic to check if the specified image exists before defaulting to a base image.

I also changed the default to use a more recent version
(projects/ubuntu-os-cloud/global/images/family/ubuntu-1710).

**Which issue(s) this PR fixes**:
Fixes #587

**Special notes for your reviewer**:

**Release note**:
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-28 00:38:21,2018-03-01 17:31:14
kube-deploy,p0lyn0mial,https://github.com/kubernetes/kube-deploy/issues/625,https://api.github.com/repos/kubernetes/kube-deploy/issues/625,Define machines deletion policies.,The current implementation of `MachineSet` chooses the first machine from the list to delete. As this doesn't provide any guarantees of consistency regarding the order of the items. It is random. We would like to identify some strategies that would provide better approaches for deletion.,closed,False,2018-02-28 16:57:31,2018-04-12 21:05:58
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/626,https://api.github.com/repos/kubernetes/kube-deploy/issues/626,Rebase machines.yaml generation to ext-apiserver,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**: Rebase the machines.yaml generation and update the gitignore file in ext-apiserver.

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-02-28 18:42:25,2018-03-01 17:32:29
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/627,https://api.github.com/repos/kubernetes/kube-deploy/issues/627,Decide how fault/availability zones should be represented in Cluster API,"This issue tracks how fault zones (or availability zones) should be represented in the Cluster API. Should it be a top-level field? Should we be able to specify provider specific config for them (separately from the current provider config)? This is both related to master and nodes that might have some affinity to resources specific to that zone.

@krousey @maisem @justinsb ",closed,False,2018-02-28 20:52:42,2018-04-12 21:06:29
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/628,https://api.github.com/repos/kubernetes/kube-deploy/issues/628,Only print error during bootstrap if err,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**: Currently the error statement gets logged even if err == nil. Not super helpful.

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-01 00:24:40,2018-03-01 00:30:55
kube-deploy,dirtbag,https://github.com/kubernetes/kube-deploy/issues/629,https://api.github.com/repos/kubernetes/kube-deploy/issues/629,where did the docker-multinode scripts go?,"I was trying to follow the instructions on 
https://github.com/kubernetes/kubernetes/issues/42413
specifically, I was trying to use the 
kube-deploy/docker-multinode/worker.sh
kube-deploy/docker-multinode/master.sh
scripts, to build me a shiny new kubernetes cluster more easily, 
but cant find them in the current version of kube-deploy??

-db",closed,False,2018-03-01 14:57:28,2018-03-03 14:22:03
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/630,https://api.github.com/repos/kubernetes/kube-deploy/issues/630,Remove apimachinery/wait dep from deploy_helper,"**What this PR does / why we need it**: 

**Which issue(s) this PR fixes**: Simple refactor to hide `wait.Poll` in the util package, and remove apimachinery/wait import from deploy_helper.
#562 

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-01 16:28:27,2018-03-01 17:29:07
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/631,https://api.github.com/repos/kubernetes/kube-deploy/issues/631,Separate OWNERS for cloud/*,"In the external apiserver world, the provider-specific code that lives in `cloud/` is disjoint from the controller. As such, for reviews for any provider, it can be a bad experience to be blocked on development.

We should have separate OWNERS for the provider-specific code, so changes there don't (always) need cluster-api owners approvals.",closed,False,2018-03-01 16:49:01,2018-04-17 18:54:40
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/632,https://api.github.com/repos/kubernetes/kube-deploy/issues/632,Define health check strategy for MachineSet,This is to track discussion and documentation on how health checking will be done for machines in a set.,closed,False,2018-03-01 21:10:46,2018-04-12 21:06:46
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/633,https://api.github.com/repos/kubernetes/kube-deploy/issues/633,"UX: consider renaming ""cloud"" param to something else","Users select a ""cloud"" to determine which provider/actuator to use, but some (like Loodse's) can target multiple providers under the covers (like Digital Ocean, AWS). And in their case (see #405), the parameter in the provider specific config is ""cloud"". This issue is to track renaming the top-level param to select the provider/actuator to something else more general.

cc @karan @mrIncompetent ",closed,False,2018-03-02 15:20:35,2018-04-12 21:07:10
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/634,https://api.github.com/repos/kubernetes/kube-deploy/issues/634,Support other docker versions and other container runtimes,"Currently only docker 1.12 is supported. This issue is to track defining supported runtimes and having those implemented in the cluster API. This can be scoped this to a specific OS flavor (Debian-based OSs) which is currently supported.

https://github.com/kubernetes/kube-deploy/blob/dfcdb553268d9f9ffcf0cdb23205bd7063ddf4a8/cluster-api-gcp/cloud/google/machineactuator.go#L644",closed,False,2018-03-03 20:17:11,2018-04-17 18:48:10
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/635,https://api.github.com/repos/kubernetes/kube-deploy/issues/635,Allow other CNI providers to be installed,"Currently CNI provider isn't specified in the API, and the GCP implementation has support for weavenet. This issue tracks API updates to support other CNI providers.

https://github.com/kubernetes/kube-deploy/blob/master/cluster-api/api/cluster/v1alpha1/types.go#L327",closed,False,2018-03-03 20:22:33,2018-04-12 16:38:46
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/636,https://api.github.com/repos/kubernetes/kube-deploy/issues/636,Use the cluster name in cluster.yaml in admin.conf,Currently the cluster config is created with cluster name as `kubernetes`. There is a cluster name in the `cluster.yaml` `metadata.name`. We should start using that name for the cluster so tools can be made to target any cluster in kubeconfig file.,closed,False,2018-03-06 17:29:39,2018-04-12 21:07:30
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/issues/637,https://api.github.com/repos/kubernetes/kube-deploy/issues/637,Configurable installation for versions and OS image matrix,"Currently in the GCE actuator, we compile in a script template that installs Kubernetes on Ubuntu. If we have a bug, or need to change the startup logic slight, we have to recompile and redeploy the controller.

The GCE actuator (and perhaps others but right now let's just limit it to GCE), should have a structured config that can be read from file (and configmap) that can allow us to configure a mapping of supported Kubernetes versions and OS image tuples to startup scripts on how to install and set them up. This would allow us to 

- Support multiple target OSes without having to add support directly to the controller binary
- Use preloaded images. No install steps, just configure.
- Allow admins to have a list of vetted setup combinations.
- Allow admins to bring their own OS and installation method without having to worry about building their own controller.

At the very minimum, we should be able to map a Kubernetes version, CRI version, OS identifier to an image and startup script. I imagine this will affect the GCE actuator and libraries and the GCE prodiver config.",closed,False,2018-03-06 20:02:44,2018-04-17 18:47:57
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/638,https://api.github.com/repos/kubernetes/kube-deploy/issues/638,Allow disk size and type to be configured for GCP,,closed,False,2018-03-07 17:22:17,2018-04-17 18:47:38
kube-deploy,marcosrmendezthd,https://github.com/kubernetes/kube-deploy/pull/639,https://api.github.com/repos/kubernetes/kube-deploy/issues/639,fixes logged error message in deploy_helper,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

fixes logged error message when the error is nil and there is no configuration

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-07 18:52:20,2018-03-08 18:00:49
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/640,https://api.github.com/repos/kubernetes/kube-deploy/issues/640,Add boostrap guidance to contributing guide,"**What this PR does / why we need it**: Add link to boostrap proccess to contributing guidelines to provide guidance.

```release-note
NONE
```
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-07 21:02:21,2018-03-16 01:57:25
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/641,https://api.github.com/repos/kubernetes/kube-deploy/issues/641,Updating owners aliases to reflect changes to sig leadership and the github teams,"
**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-08 08:05:45,2018-03-08 15:19:57
kube-deploy,p0lyn0mial,https://github.com/kubernetes/kube-deploy/issues/642,https://api.github.com/repos/kubernetes/kube-deploy/issues/642,MachineSet controller sets MachineSetStatus,"The current implementation of `MachineSet` controller doesn't set `MachineSetStatus` at all. This item tracks potential implementation.

For reference implementations see:

https://github.com/kubernetes/kubernetes/blob/a0844c17bfb964e3d1709784203e8404f790116c/pkg/controller/replicaset/replica_set_utils.go#L35:1

https://github.com/gardener/machine-controller-manager/blob/master/pkg/controller/machineset.go#L518

See also #535",closed,False,2018-03-08 14:21:55,2018-04-12 21:11:58
kube-deploy,p0lyn0mial,https://github.com/kubernetes/kube-deploy/issues/643,https://api.github.com/repos/kubernetes/kube-deploy/issues/643,MachineSet controller listens for Machine resources.,"The current implementation of `MachineSet` controller watches only the desired state of the cluster. This approach is not sufficient as there are other actors in the system that can easily change the current state (e.g. delete/add a `Machine`) of the cluster. Thus adding event handlers for `Machines` to our `MachineSet` along with appropriate handlers (requeue) seems to be valid approach.

See also:
#535
https://github.com/kubernetes/kube-deploy/pull/620#discussion_r172772002
https://github.com/kubernetes/community/blob/master/contributors/devel/controllers.md",closed,False,2018-03-08 14:40:05,2018-04-12 21:11:41
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/644,https://api.github.com/repos/kubernetes/kube-deploy/issues/644,Change DNSDomain to service domain,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**: It was mentionedby @roberthbailey that this field needs a better name.

**Special notes for your reviewer**: This is an API change. I don't think we're ready to solidify v1alpha1 yet. If we'd rather put this in a v1alpha2, that should be possible, but a lot more work.

@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-08 21:48:02,2018-03-14 17:38:08
kube-deploy,maisem,https://github.com/kubernetes/kube-deploy/issues/645,https://api.github.com/repos/kubernetes/kube-deploy/issues/645,Weave assigns conflicting IPs to Pods,"cc @medinatiger @jessicaochen @rsdcastro 

Weave is assigning conflicting IPs to Pods and they also seem to be from different cidrs.
```
$ k get po --all-namespaces -o wide 
NAMESPACE     NAME                                       READY     STATUS    RESTARTS   AGE       IP           NODE
default       clusterapi-58c564d46b-bdc9g                2/2       Running   0          4m        10.32.0.3    gce-master-test1
default       etcd-clusterapi-0                          1/1       Running   0          4m        10.32.0.2    gce-master-test1
kube-system   etcd-gce-master-test1                      1/1       Running   0          4m        10.128.0.2   gce-master-test1
kube-system   kube-apiserver-gce-master-test1            1/1       Running   0          4m        10.128.0.2   gce-master-test1
kube-system   kube-controller-manager-gce-master-test1   1/1       Running   0          4m        10.128.0.2   gce-master-test1
kube-system   kube-dns-545bc4bfd4-g8mm4                  3/3       Running   0          5m        10.32.0.4    gce-master-test1
kube-system   kube-proxy-j8jpq                           1/1       Running   0          1m        10.128.0.3   gce-node-hwdx7
kube-system   kube-proxy-jtczr                           1/1       Running   0          5m        10.128.0.2   gce-master-test1
kube-system   kube-scheduler-gce-master-test1            1/1       Running   0          4m        10.128.0.2   gce-master-test1
kube-system   weave-net-7hkzx                            2/2       Running   0          1m        10.128.0.3   gce-node-hwdx7
kube-system   weave-net-hzgd8                            2/2       Running   1          5m        10.128.0.2   gce-master-test1

```

```
$ k get no
NAME               STATUS    ROLES     AGE       VERSION
gce-master-test1   Ready     master    5m        v1.8.3
gce-node-hwdx7     Ready     <none>    56s       v1.8.3
```

Repro steps:
```
1. ./generate-yaml.sh
1. ./gcp-deployer create -c cluster.yaml -m machines.yaml
```
",closed,False,2018-03-09 03:41:04,2018-04-17 18:45:47
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/646,https://api.github.com/repos/kubernetes/kube-deploy/issues/646,AWS 1.9 image,"With Docker 17.03.2 preinstalled
",closed,True,2018-03-11 20:51:26,2018-04-18 13:39:45
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/647,https://api.github.com/repos/kubernetes/kube-deploy/issues/647,Delete cluster-api and cluster-api-gcp directory.,"In a follow-up CL, I will try to rename the current ext-apiserver to
cluster-api

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-12 19:01:34,2018-03-12 23:33:09
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/648,https://api.github.com/repos/kubernetes/kube-deploy/issues/648,Add Validation for cluster resource type in apiserver,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
This PR add validation for cluster resource inside API Server. So, invalid resource creation will be rejected. In old CRD code, we did the validation in controller instead.
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-14 17:52:49,2018-03-22 19:42:04
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/649,https://api.github.com/repos/kubernetes/kube-deploy/issues/649,Break the machine controller's dependency on Google Actuator,"This changes the machine controller library to take the machine actuator as a parameter and not construct it based on a flag an factory function. This allows a higher level package to provide an implementation to the library without the library having an import dependency on it.

As an example, I wrote a ext-apiserver/cloud/cmd/gce-machine-controller that passes in the GCE machine actuator. It's less than 40 lines of code. There's nothing special about the actuator's implementation being in-tree here.

This does modify some of the auto-generated code. It changes the way a machine controller is constructed in `ext-apiserver/pkg/controller/machine/zz_generated.api.register.go` to take an actuator as a parameter. It also changes `ext-apiserver/pkg/controller/zz_generated.api.register.go` to remove the machine controller from the set of all controllers that will be pulled into generic controller-manager binary.

This change is not complete, but enough to get the idea across. Specifically tests and the gce deployer need updating, and a new docker image for the new controller need to be created.

Are we OK with a change like this?

@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-14 22:11:52,2018-03-23 22:34:03
kube-deploy,kris-nova,https://github.com/kubernetes/kube-deploy/issues/650,https://api.github.com/repos/kubernetes/kube-deploy/issues/650,Transition to new repository,"As a sig we decided that we wanted a new repository which can be found [here](https://github.com/kubernetes-sigs/clife_cluster-api)

_Also, we can [now rename the repository as we like](https://github.com/kubernetes-sigs/clife_cluster-api/issues/1)_.

For the new repository I would like to propose a migration of the following features:

# In Scope

### 1) The API definition 

All components of the code that define what the API surface is. This traditionally would be a `types.go` file or something similar. All structs, fields, and commentary around them.

### 2) Any API specific implementation

All components of the code that are required to bring the API to life in Kubernetes. This should be the code that will support running an API server (or CRD) so that a client can communicate with the API.

### 3) Any common logic or utility code

Any components that would be useful while creating an implementation, but are not cloud specific. These would be things like watching for a change, or the bits that will say we need to create/destroy a machine. 

### 4) Any client code

Any client code that could be vendored and used to connect with the API server after it is running.

# Out of scope

### 1) Cloud specific code

In general if the code is specific to any specific cloud provider it will be out of scope for the repository. By design, cloud implementations (actuators) should not live in the new repository

# A note on migration

The migration to the new repository should not *change* any thing, but only migrate the code at a single point in time.

We will create a label for issues in the backlog we wish to migrate to the new repository, and use a github issue transfer tool to migrate those issues via a bot.  #",closed,False,2018-03-15 05:33:16,2018-07-16 19:36:54
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/651,https://api.github.com/repos/kubernetes/kube-deploy/issues/651,"Delete the client side machineset, now that we have MachineSet in the API","**What this PR does / why we need it**: code cleanup

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
Removed the client-side machineset example.
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-18 01:34:37,2018-03-18 03:04:58
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/652,https://api.github.com/repos/kubernetes/kube-deploy/issues/652,Update the machine controller image to pick up #644,"**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes: clusters started on GCP would be unable to create nodes using the machine controller.

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-18 02:49:02,2018-03-18 03:08:59
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/issues/653,https://api.github.com/repos/kubernetes/kube-deploy/issues/653,Race condition deleting a machine on GCP,"I deleted a machine from the api, and it took a long time to disappear from the cluster (it was in a NotReady state for quite a while). I went and looked at the GCP Operation logs, and saw that the VM for the machine was being deleted and created repeatedly:

```
operation-1521356457071-567aa67335198-5643dd2b-0d322246  insert  2018-03-18T00:00:58.418-07:00
operation-1521356469907-567aa67f72e38-69619a5e-7dbe34db  delete  2018-03-18T00:01:10.343-07:00
operation-1521356647069-567aa7286754a-2fddc99b-59eaaaac  insert  2018-03-18T00:04:08.652-07:00
operation-1521356660015-567aa734bff98-1f23c6ae-79382bc2  delete  2018-03-18T00:04:20.589-07:00
operation-1521356826986-567aa7d3fc611-2b2354af-fdec772e  insert  2018-03-18T00:07:08.222-07:00
operation-1521356839244-567aa7dfad0e1-5ec08e6c-a52bb2a3  delete  2018-03-18T00:07:19.954-07:00
operation-1521357026896-567aa892a2881-e14cdf0d-c98c80fb  insert  2018-03-18T00:10:28.081-07:00
operation-1521357044545-567aa8a3775e9-5a95baa6-bac3ed27  delete  2018-03-18T00:10:45.044-07:00
operation-1521357216693-567aa947a3b08-76525f51-e787a882  insert  2018-03-18T00:13:37.884-07:00
operation-1521357229025-567aa953666e9-ce8c0243-bbf03729  delete  2018-03-18T00:13:49.546-07:00
operation-1521357406169-567aa9fc567a8-da81fe84-ef1b21f2  insert  2018-03-18T00:16:47.275-07:00
operation-1521357418554-567aaa0826290-3c97df60-7eeab6b4  delete  2018-03-18T00:16:59.043-07:00
```

The gcloud output doesn't show it as well as the cloud console page (which shows the start and end time for each operation), but each operation starts about 5 seconds after the prior operation ends. Eventually the cycle broke and the VM stayed gone, but it looks like there is a race condition in the delete flow. ",closed,False,2018-03-18 07:35:57,2018-04-10 15:31:52
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/issues/654,https://api.github.com/repos/kubernetes/kube-deploy/issues/654,gcp-deployer delete can strand VMs if you are using MachineSets,"If you create a MachineSet in your cluster and then run `./gcp-deployer delete`, the delete command will delete each machine and then tear down the cluster. If you have machines that are part of a Machineset, while the `gcp-deployer` is tearing down the machines, the MachineSet is making sure that they exist. The `gcp-deployer` then deletes the master, leaking VM resources after the cluster is deleted. ",closed,False,2018-03-18 18:00:21,2018-04-17 18:45:23
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/655,https://api.github.com/repos/kubernetes/kube-deploy/issues/655,Directory rename: move the cluster api code back to the cluster-api directory,"**What this PR does / why we need it**: Finishes the migration to the extension apiserver, by renaming the ext-apiserver directory back to cluster-api. 

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers


```
$ ./scripts/ci-test.sh 
?   	k8s.io/kube-deploy/cluster-api/cloud	[no test files]
?   	k8s.io/kube-deploy/cluster-api/cloud/azure	[no test files]
?   	k8s.io/kube-deploy/cluster-api/cloud/google	[no test files]
?   	k8s.io/kube-deploy/cluster-api/cloud/google/cmd/generate-image	[no test files]
?   	k8s.io/kube-deploy/cluster-api/cloud/google/config	[no test files]
?   	k8s.io/kube-deploy/cluster-api/cloud/google/gceproviderconfig	[no test files]
?   	k8s.io/kube-deploy/cluster-api/cloud/google/gceproviderconfig/v1alpha1	[no test files]
?   	k8s.io/kube-deploy/cluster-api/cmd/apiserver	[no test files]
?   	k8s.io/kube-deploy/cluster-api/cmd/controller-manager	[no test files]
?   	k8s.io/kube-deploy/cluster-api/errors	[no test files]
?   	k8s.io/kube-deploy/cluster-api/gcp-deployer	[no test files]
?   	k8s.io/kube-deploy/cluster-api/gcp-deployer/cmd	[no test files]
?   	k8s.io/kube-deploy/cluster-api/gcp-deployer/deploy	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/apis	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/apis/cluster	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/apis/cluster/common	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/apis/cluster/install	[no test files]
ok  	k8s.io/kube-deploy/cluster-api/pkg/apis/cluster/v1alpha1	2.737s
?   	k8s.io/kube-deploy/cluster-api/pkg/client/clientset_generated/clientset	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/client/clientset_generated/clientset/fake	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/client/clientset_generated/clientset/scheme	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/client/clientset_generated/clientset/typed/cluster/v1alpha1	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/client/clientset_generated/clientset/typed/cluster/v1alpha1/fake	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/client/informers_generated/externalversions	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/client/informers_generated/externalversions/cluster	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/client/informers_generated/externalversions/cluster/v1alpha1	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/client/informers_generated/externalversions/internalinterfaces	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/client/listers_generated/cluster/internalversion	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/client/listers_generated/cluster/v1alpha1	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/controller	[no test files]
ok  	k8s.io/kube-deploy/cluster-api/pkg/controller/cluster	1.716s
?   	k8s.io/kube-deploy/cluster-api/pkg/controller/config	[no test files]
ok  	k8s.io/kube-deploy/cluster-api/pkg/controller/machine	2.087s
ok  	k8s.io/kube-deploy/cluster-api/pkg/controller/machineset	1.666s
?   	k8s.io/kube-deploy/cluster-api/pkg/controller/sharedinformers	[no test files]
?   	k8s.io/kube-deploy/cluster-api/pkg/openapi	[no test files]
?   	k8s.io/kube-deploy/cluster-api/tools/repair	[no test files]
?   	k8s.io/kube-deploy/cluster-api/tools/repair/cmd	[no test files]
?   	k8s.io/kube-deploy/cluster-api/tools/repair/util	[no test files]
?   	k8s.io/kube-deploy/cluster-api/tools/upgrader	[no test files]
?   	k8s.io/kube-deploy/cluster-api/tools/upgrader/cmd	[no test files]
?   	k8s.io/kube-deploy/cluster-api/tools/upgrader/util	[no test files]
?   	k8s.io/kube-deploy/cluster-api/util	[no test files]
```",closed,True,2018-03-19 18:08:22,2018-03-19 18:12:03
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/656,https://api.github.com/repos/kubernetes/kube-deploy/issues/656,Change type of providerConfig to runtime.RawExtension,"**What this PR does / why we need it**: Changes the providerConfig from a string into a struct that wraps a `runtime.RawExtension`. This has two benefits. One is that the machine config looks more natural (since we aren't embedding a string inside yaml, you just see more yaml). The other is that it provides us a place to drop alternative representations, in particular machine classes. 

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #504

**Special notes for your reviewer**: I'm happy to squash if you'd rather this be an atomic commit, but I thought it would be easier to review as 3. 

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers

@mvladev
",closed,True,2018-03-20 05:11:28,2018-03-20 14:08:02
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/657,https://api.github.com/repos/kubernetes/kube-deploy/issues/657,Update the sample machineset config with the new provider config,"**What this PR does / why we need it**: Fix up the sample config files. 

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-20 14:18:00,2018-03-20 15:38:01
kube-deploy,maisem,https://github.com/kubernetes/kube-deploy/pull/658,https://api.github.com/repos/kubernetes/kube-deploy/issues/658,Introducing MachineDeployment types.,"This depends on kubernetes-incubator/apiserver-builder#232, which means that we need to update dependencies.

Sending it out now to get comments on the type definition.

cc @roberthbailey @rsdcastro @medinatiger @krousey ",closed,True,2018-03-22 18:25:19,2018-04-03 20:20:04
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/659,https://api.github.com/repos/kubernetes/kube-deploy/issues/659,Machine class,"**What this PR does / why we need it**: Adds a MachineClass type to the Machines API

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
xref #529

**Special notes for your reviewer**: I had to delete some generated code to make this compile. See https://github.com/kubernetes-incubator/apiserver-builder/issues/234

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-03-23 18:17:09,2018-07-27 05:43:14
kube-deploy,maisem,https://github.com/kubernetes/kube-deploy/pull/660,https://api.github.com/repos/kubernetes/kube-deploy/issues/660,Update apimachinery to 1.9 and regenerate the apiserver code,"Update apimachinery to 1.9 and regenerate the apiserver code.

@krousey @medinatiger @roberthbailey ",closed,True,2018-03-24 03:13:40,2018-03-29 20:27:04
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/661,https://api.github.com/repos/kubernetes/kube-deploy/issues/661,Allow CA and key to be provided on cluster creation,"Currently kubeadm init generates a self-signed certificate authority and key, which isn't a solution that suits all users.

This issue tracks allowing CA and key to be provided by the user.",closed,False,2018-04-02 17:00:45,2018-04-12 21:10:18
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/662,https://api.github.com/repos/kubernetes/kube-deploy/issues/662,Document network access required to run bootstrapping tool,"Customers need to understand what kind of ports/protocol access is required to use the bootstrapping tool.

cc @jessicaochen ",closed,False,2018-04-02 17:30:34,2018-04-12 21:10:08
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/issues/663,https://api.github.com/repos/kubernetes/kube-deploy/issues/663,Controller reconciliation loop is serialized ,"Machine controller reconciliation is serialized. We need to find a way to make operation in parallel.
",closed,False,2018-04-03 17:21:50,2018-04-17 18:55:30
kube-deploy,kcoronado,https://github.com/kubernetes/kube-deploy/pull/664,https://api.github.com/repos/kubernetes/kube-deploy/issues/664,Handling configurable machine setup/installation,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
This PR modifies the GCE actuator to handle configurable Kubernetes versions, OS image, and startup scripts when creating clusters and starting up machines. It creates a new machine setup configmap that contains the different configurations that the GCE actuator can handle.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
#637

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-03 21:47:41,2018-04-24 00:03:10
kube-deploy,k4leung4,https://github.com/kubernetes/kube-deploy/pull/665,https://api.github.com/repos/kubernetes/kube-deploy/issues/665,Include step in prereq to login.,"**What this PR does / why we need it**:
Include steps for gcloud login in README

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:

```release-note

```

@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-04 17:10:17,2018-04-04 19:52:09
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/666,https://api.github.com/repos/kubernetes/kube-deploy/issues/666,Document plan on how API can support auto-scaling and other auto* features,"This issue tracks an evaluation of the API to make sure that it can support the use cases for auto-scaling and any other auto* features.

This issue should cover any API changes as well. Specific implementation changes should be tracked as separate issues.

cc @krousey @roberthbailey @mwielgus ",closed,False,2018-04-05 15:24:43,2018-04-12 21:09:36
kube-deploy,k4leung4,https://github.com/kubernetes/kube-deploy/pull/667,https://api.github.com/repos/kubernetes/kube-deploy/issues/667,Add owner reference for machines created via machineset controller,"**What this PR does / why we need it**:
This adds owner reference to machine objects created through the machineset controller. This allows garbage collection and cascading deletion to work.
Deleting the machineset will trigger the corresponding machines to be deleted.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-05 17:54:45,2018-05-18 15:06:50
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/pull/668,https://api.github.com/repos/kubernetes/kube-deploy/issues/668,Add make rules for correcting permissions on GCR public images,"**What this PR does / why we need it**: So everyone can read the images we publish.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Someone complained in the channel that the images weren't readable. This ensures that every time a new image is pushed, we make sure it's readable.

@kubernetes/kube-deploy-reviewers

cc @jessicaochen @medinatiger ",closed,True,2018-04-05 20:40:17,2018-04-05 21:55:08
kube-deploy,k4leung4,https://github.com/kubernetes/kube-deploy/pull/669,https://api.github.com/repos/kubernetes/kube-deploy/issues/669,Remove machine finalizer if underlying instance does not exist,"**What this PR does / why we need it**:
In the case of attempting to delete a machine object where the underlying instance does not exist, it may get stuck due to still having a finalizer.
Removing the machine finalizer will allow the machine object to be removed.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:

@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-05 22:49:31,2018-04-10 15:22:11
kube-deploy,thockin,https://github.com/kubernetes/kube-deploy/pull/670,https://api.github.com/repos/kubernetes/kube-deploy/issues/670,Use k8s GCR vanity URL,"**Release note**:
NONE
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-06 15:54:39,2018-04-09 18:45:24
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/671,https://api.github.com/repos/kubernetes/kube-deploy/issues/671,Remove Machine UID checking,"Remove use of UID for identity checking since machine finalizers prevent machines with same name but different UIDs existing at the same time. The side benefit of not depending on UID is that bootstrapping could potentially pivot by copying objects instead of moving the actual ETCD files.

Bumping the official controller image will come after this PR merges.

Fixes #
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-06 22:50:10,2018-04-09 22:17:08
kube-deploy,craigtracey,https://github.com/kubernetes/kube-deploy/pull/672,https://api.github.com/repos/kubernetes/kube-deploy/issues/672,Standardize ProviderConfig for Cluster and Machine,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
In order to maintain type parity for providerConfig across both Machine
and Cluster types, change the ClusterSpec.ProviderConfig to the
ProviderConfig type.

Additionally, as all changes to the API require regenerating code, add a
Makefile that will simplify that process.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```
None
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-07 01:33:51,2018-04-10 15:14:11
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/673,https://api.github.com/repos/kubernetes/kube-deploy/issues/673,Bump machine controller image to take in PR 671,"Bump machine controller image to take in https://github.com/kubernetes/kube-deploy/pull/671

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-09 22:48:40,2018-04-11 21:50:17
kube-deploy,krousey,https://github.com/kubernetes/kube-deploy/issues/674,https://api.github.com/repos/kubernetes/kube-deploy/issues/674,Enable GCP provider specific logic in the Kubernetes API.,"The Google provider implementation installs Kubernetes without any GCP cloud provider code enabled. This code is needed to enable several key features:

  - Loadbalancing
  - Persistent Volumes
  - Ingresses
  - GCP routes for container networking (no need for CNI)

This largely is making sure the correct flags are specified on the existing k8s components and also installing additional controllers (I think the ingress controller is separate). Also, the master VMs have to have the appropriate permissions to perform these operations.",closed,False,2018-04-10 16:43:58,2018-04-17 18:44:27
kube-deploy,craigtracey,https://github.com/kubernetes/kube-deploy/issues/675,https://api.github.com/repos/kubernetes/kube-deploy/issues/675,cluster-api top-level Makefile currently broken,The top-level Makefile for generating code is currently broken. It assumes that apiregister-gen and conversion-gen are in `$GOPATH`. Fix this so that generation can happen cleanly and with generation tools at the appropriate tags.,closed,False,2018-04-10 17:29:33,2018-04-13 16:04:45
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/676,https://api.github.com/repos/kubernetes/kube-deploy/issues/676,Add a prototype terraform provider and vsphere configs,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**: Adds support for terraform to kube-deploy.

**Special notes for your reviewer**: This is only prototype code that will be contributed to by others. There are a few TODOs that are [created here](https://github.com/karan/kube-deploy/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+vsphere) and I'll migrate those to k8s/kube-deploy.

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-10 22:06:15,2018-04-18 18:29:22
kube-deploy,youhonglian,https://github.com/kubernetes/kube-deploy/pull/677,https://api.github.com/repos/kubernetes/kube-deploy/issues/677,typo fixed,,closed,True,2018-04-11 06:12:46,2018-05-24 11:46:52
kube-deploy,spew,https://github.com/kubernetes/kube-deploy/pull/678,https://api.github.com/repos/kubernetes/kube-deploy/issues/678,Add JetBrains IDE files to gitignore,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
This PR prevents developers using the JetBrains family of IDEs from including IDE files in their commits.

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-11 20:44:13,2018-04-13 16:31:55
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/pull/679,https://api.github.com/repos/kubernetes/kube-deploy/issues/679,Fix cluster.yaml due to providerConfig type change,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**: After providerConfig was changed from string to RawExtension, machines.yaml was updated but not the cluster.yaml. As a result, gcp-deployer is broken currently.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-11 20:59:20,2018-04-11 21:22:56
kube-deploy,wangzhen127,https://github.com/kubernetes/kube-deploy/issues/680,https://api.github.com/repos/kubernetes/kube-deploy/issues/680,service account kube-system/default not found when creating cluster,"I was playing gcp-deployer and following the instructions [README](https://github.com/wangzhen127/kube-deploy/blob/master/cluster-api/gcp-deployer/README.md) and [CONTRIBUTING](https://github.com/wangzhen127/kube-deploy/blob/master/cluster-api/gcp-deployer/CONTRIBUTING.md) pages. I encountered errors about creating service account kube-system/default when creating the cluster.

Here are the error messages:

 $ ./gcp-deployer create -c cluster.yaml -m machines.yaml
I0411 14:36:50.570470  193113 deploy_helper.go:58] Starting cluster creation test1
I0411 14:36:50.570536  193113 deploy_helper.go:60] Starting master creation gce-master-test1
I0411 14:36:52.730439  193113 machineactuator.go:525] Wait for insert ""operation-1523482611246-569996fdd63b1-88cf21e0-e0244426""...
I0411 14:36:57.895885  193113 machineactuator.go:535] Finish wait for insert ""operation-1523482611246-569996fdd63b1-88cf21e0-e0244426""...
I0411 14:36:57.895938  193113 deploy_helper.go:67] Created master gce-master-test1
I0411 14:36:58.067002  193113 deploy_helper.go:210] Waiting for Kubernetes to come up...
I0411 14:37:21.619259  193113 deploy_helper.go:210] Waiting for Kubernetes to come up...
I0411 14:37:33.379354  193113 deploy_helper.go:210] Waiting for Kubernetes to come up...
I0411 14:37:44.897590  193113 deploy_helper.go:210] Waiting for Kubernetes to come up...
I0411 14:37:59.024332  193113 deploy_helper.go:210] Waiting for Kubernetes to come up...
I0411 14:38:15.501733  193113 deploy_helper.go:210] Waiting for Kubernetes to come up...
I0411 14:38:18.288531  193113 deploy_helper.go:219] Kubernetes is up.. Writing kubeconfig to disk.
I0411 14:38:18.337351  193113 deploy_helper.go:251] wrote kubeconfig to [/usr/local/google/home/zhenw/.kube/config]
I0411 14:38:18.337385  193113 deploy_helper.go:267] Waiting for apiserver to become healthy...
I0411 14:38:22.278609  193113 deploy_helper.go:267] Waiting for apiserver to become healthy...
I0411 14:38:27.999262  193113 deploy_helper.go:267] Waiting for apiserver to become healthy...
I0411 14:38:36.539631  193113 deploy_helper.go:267] Waiting for apiserver to become healthy...
I0411 14:38:43.693266  193113 deploy_helper.go:267] Waiting for apiserver to become healthy...
I0411 14:38:53.844676  193113 deploy_helper.go:267] Waiting for apiserver to become healthy...
I0411 14:39:05.950901  193113 deploy_helper.go:267] Waiting for apiserver to become healthy...
I0411 14:39:22.800370  193113 deploy_helper.go:267] Waiting for apiserver to become healthy...
I0411 14:39:41.193097  193113 deploy_helper.go:267] Waiting for apiserver to become healthy...
I0411 14:40:04.354222  193113 deploy_helper.go:267] Waiting for apiserver to become healthy...
I0411 14:40:04.517452  193113 deploy_helper.go:287] Waiting for the service account to exist...
I0411 14:40:13.074464  193113 deploy_helper.go:287] Waiting for the service account to exist...
I0411 14:40:20.730787  193113 deploy_helper.go:287] Waiting for the service account to exist...
I0411 14:40:30.170815  193113 deploy_helper.go:287] Waiting for the service account to exist...
I0411 14:40:43.501165  193113 deploy_helper.go:287] Waiting for the service account to exist...
I0411 14:41:02.714841  193113 deploy_helper.go:287] Waiting for the service account to exist...
I0411 14:41:31.174479  193113 deploy_helper.go:287] Waiting for the service account to exist...
I0411 14:41:55.879174  193113 deploy_helper.go:287] Waiting for the service account to exist...
I0411 14:42:26.844124  193113 deploy_helper.go:287] Waiting for the service account to exist...
I0411 14:43:19.115038  193113 deploy_helper.go:287] Waiting for the service account to exist...
E0411 14:43:19.155052  193113 deploy_helper.go:293] Error waiting for service account: timed out waiting for the condition
I0411 14:43:19.155097  193113 deploy.go:128] Deleting master vm gce-master-test1
I0411 14:43:20.095424  193113 machineactuator.go:525] Wait for delete ""operation-1523482999410-5699987004d51-a4f56963-fb18f34b""...
I0411 14:44:37.557110  193113 machineactuator.go:535] Finish wait for delete ""operation-1523482999410-5699987004d51-a4f56963-fb18f34b""...
I0411 14:44:37.557262  193113 serviceaccount.go:107] No service a/c found in cluster.
F0411 14:44:37.557281  193113 create.go:50] service account kube-system/default not found: timed out waiting for the condition
",closed,False,2018-04-11 22:06:31,2018-04-12 16:39:46
kube-deploy,medinatiger,https://github.com/kubernetes/kube-deploy/pull/681,https://api.github.com/repos/kubernetes/kube-deploy/issues/681,Implement a parallel queueworker for Machine Controller,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
This PR make reconciliation parallel per machine resource. It allows multiple reconciliation actions (create/delete/update) making progress simultaneously while serializing the reconciliation for the same machine resource.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #
fixes #663

**Special notes for your reviewer**:
It was in the original CRD implementation, but get lost in apiserver aggregation migration. We might want to put this into apiserver-builder eventually if other controller can also benefit from this.

 
**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-11 22:33:10,2018-04-17 18:12:45
kube-deploy,jessicaochen,https://github.com/kubernetes/kube-deploy/pull/682,https://api.github.com/repos/kubernetes/kube-deploy/issues/682,Bump for #679,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Need to bump to pull in changes from https://github.com/kubernetes/kube-deploy/pull/672
Verified end to end.
Without Bump Create Cluster Error Message:  Cluster in version ""v1alpha1"" cannot be handled as a Cluster: v1alpha1.Cluster.Spec: v1alpha1.ClusterSpec.ProviderConfig: ReadString: expects "" or n, but found {, error found in #10 byte of ...|rConfig"":{""value"":{""|..., bigger context ...|serviceDomain"":""cluster.local""},""providerConfig"":{""value"":{""apiVersion"":""gceproviderconfig/v1alpha1""|...


**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-11 23:08:16,2018-04-12 22:54:32
kube-deploy,craigtracey,https://github.com/kubernetes/kube-deploy/pull/683,https://api.github.com/repos/kubernetes/kube-deploy/issues/683,Fix API generation ,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:
Previous work to develop a Makefile that would handle API autogenerated
code was less than ideal. This change backs those changes out and solves
this in a more supportable way.

This change adds a one-time Dockerfile that will build an image with the
necessary api generation dependencies and runs against the local source
tree.

In addition, this removes the dependency on conversion-gen source.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #675 

**Special notes for your reviewer**:
Not sure when the new repo will land, but happy to move this to the new repo when it is ready.

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-13 20:57:23,2018-07-16 20:51:34
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/684,https://api.github.com/repos/kubernetes/kube-deploy/issues/684,"Add ""reviewers"" list to OWNERS for blunderbuss to request review","**What this PR does / why we need it**:
Add reviewers block to OWNERS, so blunderbuss can request reviews (currently we only have suggestions that are made by GitHub, not prow).
Update list of maintainers to spread the review load
Update link to OWNERS doc

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #612

**Special notes for your reviewer**:

@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-13 21:18:39,2018-04-13 21:28:01
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/pull/685,https://api.github.com/repos/kubernetes/kube-deploy/issues/685,Remove extra tab from CONTRIBUTING.md,"**What this PR does / why we need it**:
""```bash"" is being shows literally rather than being rendered in markdown

@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-13 21:31:29,2018-04-16 17:51:59
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/686,https://api.github.com/repos/kubernetes/kube-deploy/issues/686,GCP: support VM images that come with container runtime pre-installed,VM images provided by users might have the container runtime. This issue tracks verifying if the container runtime is already pre-installed and matches the version specified by the user.,closed,False,2018-04-14 01:45:35,2018-04-17 18:44:05
kube-deploy,rsdcastro,https://github.com/kubernetes/kube-deploy/issues/687,https://api.github.com/repos/kubernetes/kube-deploy/issues/687,VMware: support VM images that come with container runtime pre-installed,VM images provided by users might have the container runtime. This issue tracks verifying if the container runtime is already pre-installed and matches the version specified by the user.,closed,False,2018-04-14 01:46:23,2018-04-17 18:43:46
kube-deploy,spew,https://github.com/kubernetes/kube-deploy/pull/688,https://api.github.com/repos/kubernetes/kube-deploy/issues/688,Refactor GCEClient: wrap compute.Service in an interface for mocking …,"…GCP compute

This change creates a ComputeService interface which has a runtime
implementation that wraps the compute.Service. This will enable creating
tests that mock GCP Compute Service calls.

**What this PR does / why we need it**:
It is difficult to test changes that interact with GCP without manually running integration tests. This change will enable developers to write unit tests that mock GCP and test that their code correctly handles various scenarios. 

**Release note**:
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-04-14 21:14:25,2018-07-16 21:02:03
kube-deploy,karan,https://github.com/kubernetes/kube-deploy/issues/689,https://api.github.com/repos/kubernetes/kube-deploy/issues/689,New tool name voting.,"There seems to be confusion on the name for the sig-sponsored installation tool proposed last week. 
 We'll do a formal vote here.

Each comment in this issue will be a candidate (please add yours), and by **EOD 04/23 PST**, the candidate with the most number of emoji responses will win.

1. There's no negative votes (thumbs down is still a positive vote)
2. Since GH allows multiple emojis per user per comment, 1 person 1 vote per comment will count.

Votes for multiple entries OK.

cc @kubernetes/sig-cluster-lifecycle-misc ",closed,False,2018-04-18 17:44:52,2018-04-24 16:01:58
kube-deploy,AdamDang,https://github.com/kubernetes/kube-deploy/pull/690,https://api.github.com/repos/kubernetes/kube-deploy/issues/690,Typo fix commmands->commands,"Line 54:  commmand->command
",closed,True,2018-04-26 12:42:08,2018-05-26 20:12:33
kube-deploy,kris-nova,https://github.com/kubernetes/kube-deploy/pull/691,https://api.github.com/repos/kubernetes/kube-deploy/issues/691,Replace old code with submodule,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

Removes the deprecated cluster-api code but replaces with a git submodule so users who vendor the code won't break


**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
Replace cluster-api code with git submodule to new repository
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers

fixes #650 ",closed,True,2018-05-01 07:43:44,2018-05-11 16:45:36
kube-deploy,kawych,https://github.com/kubernetes/kube-deploy/issues/692,https://api.github.com/repos/kubernetes/kube-deploy/issues/692,gcp-deployer doesn't work with newer Kubernetes versions,"I've tried `./gcp-deployer create -c cluster.yaml -m machines.yaml` with versions 1.9.6 and 1.10.2 (for both Kubelet and control plane). Output from deployer:
 ```
I0501 19:07:57.493144   67527 deploy_helper.go:58] Starting cluster creation test1
I0501 19:07:57.493209   67527 deploy_helper.go:60] Starting master creation gce-master-test1
I0501 19:08:00.067163   67527 machineactuator.go:525] Wait for insert ""operation-1525194478467-56b28031a37b9-7bfce688-f2be25a8""...
I0501 19:08:42.656880   67527 machineactuator.go:535] Finish wait for insert ""operation-1525194478467-56b28031a37b9-7bfce688-f2be25a8""...
I0501 19:08:42.656922   67527 deploy_helper.go:67] Created master gce-master-test1
I0501 19:08:42.922144   67527 deploy_helper.go:210] Waiting for Kubernetes to come up...
[...]
I0501 19:12:25.747012   67527 deploy_helper.go:210] Waiting for Kubernetes to come up...
I0501 19:12:29.051570   67527 deploy.go:128] Deleting master vm gce-master-test1
I0501 19:12:30.195419   67527 machineactuator.go:525] Wait for delete ""operation-1525194749405-56b2813406749-fc725735-a4008f1f""...
I0501 19:14:00.366549   67527 machineactuator.go:535] Finish wait for delete ""operation-1525194749405-56b2813406749-fc725735-a4008f1f""...
I0501 19:14:00.366762   67527 serviceaccount.go:107] No service a/c found in cluster.
F0501 19:14:00.366779   67527 create.go:50] unable to write kubeconfig: timedout writing kubeconfig: timed out waiting for the condition
```
My machines.yaml file:
```
items:                                                                                                                                                       
- apiVersion: ""cluster.k8s.io/v1alpha1""                                                                                                                      
  kind: Machine                                                                                                                                              
  metadata:                                                                                                                                                  
    generateName: gce-master-                                                                                                                                
    labels:                                                                                                                                                  
      set: master                                                                                                                                            
  spec:                                                                                                                                                      
    providerConfig:                                                                                                                                          
      value:                                                                                                                                                 
        apiVersion: ""gceproviderconfig/v1alpha1""                                                                                                             
        kind: ""GCEProviderConfig""                                                                                                                            
        project: ""kawych-test""                                                                                                                               
        zone: ""us-central1-f""                                                                                                                                
        machineType: ""n1-standard-2""                                                                                                                         
        image: ""projects/ubuntu-os-cloud/global/images/family/ubuntu-1604-lts""                                                                               
    versions:                                                                                                                                                
      kubelet: 1.9.6                                                                                                                                         
      controlPlane: 1.9.6                                                                                                                                    
      containerRuntime:                                                                                                                                      
        name: docker                                                                                                                                         
        version: 1.12.0                                                                                                                                      
    roles:                                                                                                                                                   
    - Master
- apiVersion: ""cluster.k8s.io/v1alpha1""                                                                                                                      
  kind: Machine                                                                                                                                              
  metadata:                                                                                                                                                  
    generateName: gce-node-                                                                                                                                  
    labels:                                                                                                                                                  
      set: node                                                                                                                                              
  spec:                                                                                                                                                      
    providerConfig:                                                                                                                                          
      value:                                                                                                                                                 
        apiVersion: ""gceproviderconfig/v1alpha1""                                                                                                             
        kind: ""GCEProviderConfig""                                                                                                                            
        project: ""kawych-test""                                                                                                                               
        zone: ""us-central1-f""                                                                                                                                
        machineType: ""n1-standard-8""                                                                                                                         
        image: ""projects/ubuntu-os-cloud/global/images/family/ubuntu-1604-lts""                                                                               
    versions:                                                                                                                                                
      kubelet: 1.9.6                                                                                                                                         
      containerRuntime:                                                                                                                                      
        name: docker                                                                                                                                         
        version: 1.12.0                                                                                                                                      
    roles:                                                                                                                                                   
    - Node
```

cc @karan ",closed,False,2018-05-01 17:16:25,2018-05-01 18:33:45
kube-deploy,m1093782566,https://github.com/kubernetes/kube-deploy/issues/693,https://api.github.com/repos/kubernetes/kube-deploy/issues/693,Cloud Provider implementation for Huawei Cloud,/cc @rsdcastro @kevin-wangzefeng ,closed,False,2018-05-09 12:26:01,2018-05-10 01:42:03
kube-deploy,kris-nova,https://github.com/kubernetes/kube-deploy/pull/694,https://api.github.com/repos/kubernetes/kube-deploy/issues/694,Removing cluster API directory,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

Removes the cluster-api code from the kube deploy repository

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #650 

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
Removing cluster API code from kube-deploy repository
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-05-11 16:45:50,2018-07-16 21:14:21
kube-deploy,AdamDang,https://github.com/kubernetes/kube-deploy/pull/695,https://api.github.com/repos/kubernetes/kube-deploy/issues/695,Typo fix: Kuberenetes->Kubernetes,"Line 37: Kuberenetes->Kubernetes
",closed,True,2018-05-13 08:18:04,2018-05-14 20:20:19
kube-deploy,ichekrygin,https://github.com/kubernetes/kube-deploy/issues/696,https://api.github.com/repos/kubernetes/kube-deploy/issues/696,GCE: ControlPlane Update Fails,"## Steps to repro:

- create `gce` cluster using [CONTRIBUTING.md](https://github.com/kubernetes/kube-deploy/blob/master/cluster-api/gcp-deployer/CONTRIBUTING.md)
- observe cluster crated
```
kubectl get nodes
NAME                          STATUS    ROLES     AGE       VERSION
gce-master-cluster-api-test   Ready     master    37m       v1.8.3
gce-node-pkhws                Ready     <none>    33m       v1.8.3
```
- update controlPlane version `v1.8.3`->`v1.8.8.` and apply changes
- confirm changes
```
kubectl get machines gce-master-cluster-api-test -o json | jq -r "".spec.versions.controlPlane""
1.8.8
```
## Expectation
`gce-master-cluster-api-test` control plane is updated to `v1.8.8`

## Actual Results
`gce-master-cluster-api-test` control plane is not updated (remains on `v1.8.3`)

## Logs:
- `gce-machine-controller` reports several repeated attempts/errors
```
ERROR: logging before flag.Parse: I0515 21:08:43.338264       1 ssh.go:98] Remote SSH execution 'export KUBEADM_VERSION=$(curl -sSL https://dl.k8s.io/release/stable.txt); curl -sSL https://dl.k8s.io/release/${KUBEADM_VERSION}/bin/linux/amd64/kubeadm | sudo tee /usr/bin/kubeadm > /dev/null; sudo chmod a+rx /usr/bin/kubeadm' on gce-master-cluster-api-test
ERROR: logging before flag.Parse: I0515 21:08:43.450659       1 machineactuator.go:573] remotesshcomand error: error: exec: ""ssh"": executable file not found in $PATH, output:
ERROR: logging before flag.Parse: E0515 21:08:43.450688       1 machineactuator.go:373] master inplace update failed: error: exec: ""ssh"": executable file not found in $PATH, output:
ERROR: logging before flag.Parse: I0515 21:08:46.010934       1 controller.go:83] Running reconcile Machine for gce-master-cluster-api-test
ERROR: logging before flag.Parse: I0515 21:08:46.205356       1 controller.go:117] reconciling machine object gce-master-cluster-api-test triggers idempotent update.
ERROR: logging before flag.Parse: I0515 21:08:46.214195       1 machineactuator.go:370] Doing an in-place upgrade for master.
```

## Other Info
```
kubectl version                                                                                                                                                                               14:25:35
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.2"", GitCommit:""81753b10df112992bf51bbc2c2f85208aad78335"", GitTreeState:""clean"", BuildDate:""2018-04-27T09:22:21Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.3"", GitCommit:""f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd"", GitTreeState:""clean"", BuildDate:""2017-11-08T18:27:48Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
",closed,False,2018-05-15 21:27:18,2018-05-15 21:43:30
kube-deploy,jessfraz,https://github.com/kubernetes/kube-deploy/issues/697,https://api.github.com/repos/kubernetes/kube-deploy/issues/697,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",closed,False,2018-05-24 14:35:59,2018-07-16 23:11:39
kube-deploy,AdamDang,https://github.com/kubernetes/kube-deploy/pull/698,https://api.github.com/repos/kubernetes/kube-deploy/issues/698,Typo fix: kubernets->kubernetes,"Line 65: kubernets->kubernetes
",closed,True,2018-05-25 05:17:05,2018-05-29 18:11:16
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/699,https://api.github.com/repos/kubernetes/kube-deploy/issues/699,Kops / k8s-optimized images for 1.10 and 1.11,"Preinstalling Docker 17.03.2


<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers",closed,True,2018-05-26 22:28:21,2018-08-17 01:51:19
kube-deploy,AdamDang,https://github.com/kubernetes/kube-deploy/pull/700,https://api.github.com/repos/kubernetes/kube-deploy/issues/700,Typo fix in returned message: retriving->retrieving,Line 213: retriving->retrieving,closed,True,2018-06-02 11:33:01,2018-06-04 23:00:15
kube-deploy,AdamDang,https://github.com/kubernetes/kube-deploy/pull/701,https://api.github.com/repos/kubernetes/kube-deploy/issues/701,Typo fix: kubernets->kubernetes,Line 65: kubernets->kubernetes,closed,True,2018-06-21 05:21:14,2018-07-16 20:52:38
kube-deploy,AdamDang,https://github.com/kubernetes/kube-deploy/pull/702,https://api.github.com/repos/kubernetes/kube-deploy/issues/702,Typo fix:kubernets->kubernetes,Line 65: kubernets->kubernetes,closed,True,2018-06-22 09:50:30,2018-07-16 20:50:25
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/703,https://api.github.com/repos/kubernetes/kube-deploy/issues/703,Add SECURITY_CONTACTS to the root of the repository.,"**What this PR does / why we need it**: Adds a required file to the repository.

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #697

**Special notes for your reviewer**: Same contents as https://raw.githubusercontent.com/kubernetes-sigs/cluster-api/master/SECURITY_CONTACTS

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-07-16 19:42:37,2018-07-16 21:09:53
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/704,https://api.github.com/repos/kubernetes/kube-deploy/issues/704,Remove the cluster-api code,"It has been migrated to https://github.com/kubernetes-sigs/cluster-api/

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
NONE
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-07-16 19:45:04,2018-07-17 05:27:38
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/705,https://api.github.com/repos/kubernetes/kube-deploy/issues/705,Remove references to the cluster api from the contributing docs.,**Special notes for your reviewer**: Assigning to @justinsb since the only code remaining in this repository in the image builder code. ,closed,True,2018-07-16 20:48:33,2018-07-16 20:58:52
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/706,https://api.github.com/repos/kubernetes/kube-deploy/issues/706,Fix typo.,Simple typo fix. ,closed,True,2018-07-16 21:02:28,2018-07-16 21:05:51
kube-deploy,roberthbailey,https://github.com/kubernetes/kube-deploy/pull/707,https://api.github.com/repos/kubernetes/kube-deploy/issues/707,Promote image builder to the top-level of the kube-deploy repository,"This makes it a single purpose repository rather than a playground for
creating new cluster automation.

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",open,True,2018-07-17 14:08:11,2019-03-22 19:07:04
kube-deploy,jsleeio,https://github.com/kubernetes/kube-deploy/pull/708,https://api.github.com/repos/kubernetes/kube-deploy/issues/708,[imagebuilder] new flag for injecting extra tags,"**What this PR does / why we need it**:

Add a flag `-addtags` to allow specifying one or more additional tags to be injected into the `imagebuilder` configuration when using AWS. This has two purposes:

1. users operating in the same AWS account can use tags to select images

2. it also allows injecting extra runtime config into the `bootstrap-vz` templates. Possibly best explained with an example from my employer's internal CI job:

```
---
{{- define ""gitref"" -}}
{{- $ref := or .Tags.GitRef """" -}}
{{- if ne $ref ""master"" -}}
{{- printf ""%s"" $ref -}}
{{- end -}}
{{- end -}}
{{ if eq .Cloud ""aws"" }}
name: sm-k8s-1.8-debian-{system.release}-{system.architecture}-{provider.virtualization}-ebs-{%Y}-{%m}-{%d}{{ template ""gitref"" . }}
{{ else }}
name: k8s-1.8-debian-{system.release}-{system.architecture}-{%Y}-{%m}-{%d}{{- template ""gitref"" . }}
{{ end }}
```

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:

N/A

**Special notes for your reviewer**:

Implemented for GCE... from what I can tell it looks correct, but untested

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
Imagebuilder can now inject additional image tags into the AWS and GCE configuration to be applied to images and also visible in Go template context

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-08-09 23:27:03,2018-10-26 07:36:08
kube-deploy,ercliou,https://github.com/kubernetes/kube-deploy/pull/709,https://api.github.com/repos/kubernetes/kube-deploy/issues/709,Increase default IO timeout submitted to NVMe devices,"**What this PR does / why we need it**:
For EC2 m5/c5 families, EBS volumes are exposed as NVMe block devices.
If there's a delay for AWS EBS to respond, we can reach NVMe IO timeout.
When this happens, the disk is re-mounted as read-only and no further writes can happen.
The default is 30 seconds.
AWS recommends to set it as high as possible. The newest Amazon Linux has this configured too.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/nvme-ebs-volumes.html#timeout-nvme-ebs-volumes
For the kernel version, the highest value is 255.

**Which issue(s) this PR fixes**:
https://github.com/kubernetes/kops/issues/5694

**Release note**:
```release-note
Increase nvme io timeout to better support EC2 c5/m5 instances.
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-09-06 02:35:45,2019-03-25 06:04:22
kube-deploy,willthames,https://github.com/kubernetes/kube-deploy/pull/710,https://api.github.com/repos/kubernetes/kube-deploy/issues/710,Add acpi-support to stretch images,"

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

acpi-support is needed for AWS to trigger systemd shutdown jobs
on instance shutdown or termination

Without acpi-support, the instance just gets hard powered off
approx 5 minutes after termination event fires without tidying
up at all

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes https://github.com/kubernetes/kops/issues/4402

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
Adds acpi-support to 1.10 and 1.11 stretch images. This causes the instance to recognise that it is being shut down, allowing shutdown scripts to fire, and possibly reducing instance termination time.
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",open,True,2018-10-08 11:37:49,2019-03-25 10:55:03
kube-deploy,jsleeio,https://github.com/kubernetes/kube-deploy/pull/711,https://api.github.com/repos/kubernetes/kube-deploy/issues/711,don't passthru AWS creds when IAM role specified,"**What this PR does / why we need it**:

`imagebuilder` passes through AWS creds to the target instance. When the target instance is started with an IAM role specified in the instance profile, passthru-creds are almost certainly not what the user actually wants; it makes it unnecessarily efforty to do AWS things in the `bootstrap-vz` config.

**Which issue(s) this PR fixes**

No obviously-relevant issues found when I searched

I'm actually a little surprised by this and it leaves me wondering if anyone else actually uses the `InstanceProfile` option I added! Without this change it's really annoying to use. Sorry :-(

**Special notes for your reviewer**:

Should have no effect on configurations without IAM roles. We've been running our daily build pipeline with this change for months, I just forgot to feed it back upstream. Cheers! 🍻 

**Release note**:

```release-note
Don't pass-thru AWS creds when IAM role specified in EC2 instance profile
```

@kubernetes/kube-deploy-reviewers
",open,True,2018-10-25 02:00:10,2019-02-25 09:59:44
kube-deploy,jsleeio,https://github.com/kubernetes/kube-deploy/pull/712,https://api.github.com/repos/kubernetes/kube-deploy/issues/712,RFC: use Debian/stretch buildhost because dependency hell,"This PR is intended at least partly as an RFC, and I don't expect it to be merged as-is. However, it does address some real-world problems.

**What this PR does / why we need it**:

2.5 things convinced me to submit this PR:

* There haven't been any new Debian/jessie AMIs recently. The most recent (`ami-221ea342`) was created on 2017-01-15. This means a new AMI build host started now would be missing nearly two years of security patches
* My private `imagebuilder` AMI pipeline stopped working with `pip` failures related to `urllib3`. I was not able to get it working again with Debian/jessie. I was able to replicate these issues in a `debian:jessie` Docker container locally by copy/pasting the commands from `imagebuilder/pkg/imagebuilder/config.go`, so figured it wasn't my pipeline (which had been working fine for months) at fault
* the AWS Paris region was not obviously supported 

This commit:

* switches to current (as of time of writing) Debian/stretch AMIs (as listed: https://wiki.debian.org/Cloud/AmazonEC2Image/Stretch)
* adds AWS Paris region `eu-west-3`
* explicitly installs Python `requests` module
* forces `pip` to upgrade things during module installation if possible

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:

Did not find any relevant issues

**Special notes for your reviewer**:

My private `imagebuilder` pipeline has been building AWS AMIs with this change since submitting it. No GCP testing, however.

**Release note**:

```release-note
ACTION REQUIRED: imagebuilder buildhosts now use Debian/stretch. This may affect operations run outside the target filesystem (ie. no chroot) in the build template
Add a default AMI for AWS Paris (`eu-west-3`) region
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2018-10-25 07:48:05,2019-03-01 10:59:26
kube-deploy,jsleeio,https://github.com/kubernetes/kube-deploy/pull/713,https://api.github.com/repos/kubernetes/kube-deploy/issues/713,Dockerfile for imagebuilder,"**What this PR does / why we need it**:

This can simplify usage of `imagebuilder` in CI pipelines.

**Which issue(s) this PR fixes**

Didn't find any obviously-relevant issues.

**Release note**:
```release-note
Dockerfile for imagebuilder
```

@kubernetes/kube-deploy-reviewers
",closed,True,2018-10-26 05:26:40,2018-12-07 14:38:27
kube-deploy,jsleeio,https://github.com/kubernetes/kube-deploy/pull/714,https://api.github.com/repos/kubernetes/kube-deploy/issues/714,imagebuilder README tidyups/improvements,"**What this PR does / why we need it**:

Improve `imagebuilder` docs:

* much format tidying
* pointify intro section
* added section with config file options
* added `--addtags` in advanced options section
* demo of using tags in `bootstrap-vz` templates

**Release note**:

```release-note
NONE
```

@kubernetes/kube-deploy-reviewers
",closed,True,2018-10-26 08:20:49,2018-12-07 14:31:33
kube-deploy,azman0101,https://github.com/kubernetes/kube-deploy/issues/715,https://api.github.com/repos/kubernetes/kube-deploy/issues/715,SSH key error ," I'm trying to build a custom KOPS AMI with imagebuild but the command issue an error `error parsing SSH private key: ssh: unhandled key type`


I've tried with 2048 and 4096 RSA key: 

```
imagebuilder --replicate=false \
  --publish=false \
  --config aws-1.9-stretch.yaml \
  --v=10
I1101 19:25:27.679671    7604 main.go:161] Parsed template ""templates/1.9-stretch.yml""; will build image with name k8s-1.9-debian-stretch-amd64-hvm-ebs-2018-11-01
I1101 19:25:27.680024    7604 aws.go:204] AWS DescribeInstances Filter:tag-key=k8s.io/role/imagebuilder
I1101 19:25:28.690855    7604 aws.go:224] Ignoring instance ""i-XXXXXXXXXXXXXX"" in state ""terminated""
I1101 19:25:28.837228    7604 aws.go:378] Creating AWS KeyPair with Name:""imagebuilder-XXXXXXXXXXXXXXXXXXXXXXX""
I1101 19:25:29.546688    7604 aws.go:256] AWS DescribeSubnets Filter:tag-key=k8s.io/role/imagebuilder
I1101 19:25:29.920820    7604 aws.go:301] AWS DescribeSubnetsInput ID:""subnet-XXXXX""
I1101 19:25:30.120487    7604 aws.go:283] AWS DescribeSecurityGroups Filter:tag-key=k8s.io/role/imagebuilder
I1101 19:25:30.327250    7604 aws.go:470] AWS RunInstances InstanceType=""m3.medium"" ImageId=""ami-XXXXXXX"" KeyName=""imagebuilder-XXXXXXXXXXXXXXXXXXXXXXXX""
I1101 19:25:31.354259    7604 aws.go:320] AWS CreateTags Resource=""i-XXXXXXXXX""
I1101 19:25:31.558458    7604 aws.go:546] AWS DescribeImages Filter:Name=""k8s-1.9-debian-stretch-amd64-hvm-ebs-2018-11-01"", Owner=self
F1101 19:25:31.968585    7604 main.go:211] error parsing SSH private key: ssh: unhandled key type
```

However, AWS doesn't support ED25519 but when
```
/kube-deploy/imagebuilder/vendor/golang.org/x/crypto/ssh/keys.go
  830,32: 		return nil, errors.New(""ssh: unhandled key type"")
	// we only handle ed25519 keys currently
	if pk1.Keytype != KeyAlgoED25519 {
		return nil, errors.New(""ssh: unhandled key type"")
	}
```

Is this imagebuild vendor that needs to be updated ?

Although the command fail, the EC2 is launched and I'm able to open an ssh session to the machine.
",closed,False,2018-11-01 18:39:35,2018-11-02 08:53:57
kube-deploy,yujunz,https://github.com/kubernetes/kube-deploy/issues/716,https://api.github.com/repos/kubernetes/kube-deploy/issues/716,Error parsing SSH private key: asn1: structure error: tags don't match,"Got an error when running imagebuilder

```
F1213 14:43:51.082659   87321 main.go:211] error parsing SSH private key: asn1: structure error: tags don't match (16 vs {class:2 tag:15 length:124 isCompound:false}) {optional:false explicit:false application:false private:false defaultValue:<nil> tag:<nil> stringType:0 timeType:0 set:false omitEmpty:false} pkcs1PrivateKey @2
```",closed,False,2018-12-13 07:42:17,2019-01-07 00:53:51
kube-deploy,granular-ryanbonham,https://github.com/kubernetes/kube-deploy/pull/717,https://api.github.com/repos/kubernetes/kube-deploy/issues/717,Set NVME Timeout to 255 istead of default 30. Per https://docs.aws.am…," Per https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/nvme-ebs-volumes.html. Causes EBS Volumes to go read only on C5/M5 instance types.
",open,True,2018-12-14 05:34:13,2019-03-21 21:46:31
kube-deploy,JBOClara,https://github.com/kubernetes/kube-deploy/pull/718,https://api.github.com/repos/kubernetes/kube-deploy/issues/718,Dockerfile for imagebuilder (#713),"This can simplify usage of `imagebuilder` in CI pipelines.

<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",closed,True,2019-01-12 20:47:32,2019-01-12 20:47:52
kube-deploy,jsleeio,https://github.com/kubernetes/kube-deploy/issues/719,https://api.github.com/repos/kubernetes/kube-deploy/issues/719,update Debian base images to patch DSA-4371-1,"https://www.debian.org/security/2019/dsa-4371

This pertains to `apt` in Debian . I can see this being a problem in two ways:

1. in the `imagebuilder` target instance, but outside the target filesystem for the AMI, `pkg/config.go` invokes `apt-get` to install `debootstrap` and friends. I think the least-troublesome approach here may be to simply update the list of AMI IDs baked into `imagebuilder` to ensure that the target instance already has a patched `apt` before anything else happens

2. in the target filesystem for the AMI, `apt-get` is invoked to install and upgrade things. I don't yet understand how the `debootstrap` process actually works, so I'm not sure what, if anything, we need to do here.",open,False,2019-01-22 22:32:59,2019-01-22 22:32:59
kube-deploy,justinsb,https://github.com/kubernetes/kube-deploy/pull/720,https://api.github.com/repos/kubernetes/kube-deploy/issues/720,"Use go modules, remove vendor, update code & fix SSH","<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews
3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed
4. If the PR is unfinished, see how to mark it: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#marking-unfinished-pull-requests
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
Updated to latest go and go modules
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers",open,True,2019-02-25 02:56:17,2019-02-25 02:56:50
kube-deploy,jsleeio,https://github.com/kubernetes/kube-deploy/pull/721,https://api.github.com/repos/kubernetes/kube-deploy/issues/721,[WIP] AWS ARM64 support for Stretch builds,"Debian have updated their cloud images page for Stretch to include ARM64
AMIs: https://wiki.debian.org/Cloud/AmazonEC2Image/Stretch

* rearranged AMI selection
* updated AMI IDs for `amd64`
* added AMI IDs for `arm64`
* added a default instance type for `arm64`
* `amd64` still builds
* `arm64` does not quite work yet and requires some modifications to
  `bootstrap-vz`: https://github.com/jsleeio/bootstrap-vz/tree/arm64-support
  - current status: build breaks at `grub-install` with an EFI error
* while working on `bootstrap-vz` I branched from @andsens' current [`master`](https://github.com/andsens/bootstrap-vz) branch. Separately from this PR, I think we can move forward from @justinsb's [`image18`](https://github.com/justinsb/bootstrap-vz/tree/image18) branch to the current upstream release. TBC.
* there does not seem to be a Docker 17.03.2 [package](https://download.docker.com/linux/debian/dists/stretch/pool/stable/arm64/) for `arm64`.

**What this PR does / why we need it**:

Multi-architecture support for `imagebuilder` on AWS. This really means `arm64` for now, but I'm trying to squash as many arch-specific bits as possible

**Which issue(s) this PR fixes**

N/A

**Special notes for your reviewer**:

WIP, don't merge please!

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, include the string ""action required"".
2. If no release note is required, just write ""NONE"".
-->
```release-note
Nothing yet.
```

<!-- All reviews default to cc'ing the kube-deploy-reviewers github group. -->
@kubernetes/kube-deploy-reviewers
",open,True,2019-03-03 10:49:16,2019-03-03 10:51:41
kube-deploy,joelsmith,https://github.com/kubernetes/kube-deploy/pull/722,https://api.github.com/repos/kubernetes/kube-deploy/issues/722,Update embargo doc link in SECURITY_CONTACTS and change PST to PSC,See https://github.com/kubernetes/security/issues/8 for more information,closed,True,2019-03-08 18:08:55,2019-03-20 15:57:11
kube-deploy,ganchandrasekaran,https://github.com/kubernetes/kube-deploy/issues/723,https://api.github.com/repos/kubernetes/kube-deploy/issues/723,EXTERNAL-IP is always none when using NodePort,"For instance following the tutorial on: [https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/](url)

When I run 
`kubectl get services`
Output:
```
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
kubernetes            ClusterIP   10.96.0.1        <none>        443/TCP          15m
kubernetes-bootcamp   NodePort    10.108.199.215   <none>        8080:31348/TCP   10m

```
Note: I ran the minikube behind the Poxy and using Cert by following: [https://github.com/kubernetes/minikube/blob/master/docs/http_proxy.md](url)

```kubectl describe pods


Name:               kubernetes-bootcamp-6bf84cb898-7lwq4
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               minikube/10.0.2.15
Start Time:         Tue, 26 Mar 2019 10:31:21 +0900
Labels:             pod-template-hash=6bf84cb898
                    run=kubernetes-bootcamp
Annotations:        <none>
Status:             Running
IP:                 172.17.0.4
Controlled By:      ReplicaSet/kubernetes-bootcamp-6bf84cb898
Containers:
  kubernetes-bootcamp:
    Container ID:   docker://a2cb13a0bdc422dbb22df33f7d74469f8f6a9187f2d34bfd6076b74b0eac02f4
    Image:          gcr.io/google-samples/kubernetes-bootcamp:v1
    Image ID:       docker-pullable://gcr.io/google-samples/kubernetes-bootcamp@sha256:0d6b8ee63bb57c5f5b6156f446b3bc3b3c143d233037f3a2f00e279c8fcc64af
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 26 Mar 2019 10:31:31 +0900
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2z54m (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-2z54m:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-2z54m
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m55s  default-scheduler  Successfully assigned default/kubernetes-bootcamp-6bf84cb898-7lwq4 to minikube
  Normal  Pulling    9m55s  kubelet, minikube  pulling image ""gcr.io/google-samples/kubernetes-bootcamp:v1""
  Normal  Pulled     9m46s  kubelet, minikube  Successfully pulled image ""gcr.io/google-samples/kubernetes-bootcamp:v1""
  Normal  Created    9m45s  kubelet, minikube  Created container
  Normal  Started    9m45s  kubelet, minikube  Started container
```",closed,False,2019-03-26 01:41:37,2019-03-26 01:44:57
