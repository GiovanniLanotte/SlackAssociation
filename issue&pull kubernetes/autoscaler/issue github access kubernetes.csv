name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/1,https://api.github.com/repos/kubernetes/autoscaler/issues/1,Fix imports in cluster autoscaler after migrating it from contrib,cc: @MaciekPytel ,closed,True,2017-04-18 13:48:58,2017-04-18 13:55:14
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/2,https://api.github.com/repos/kubernetes/autoscaler/issues/2,Copy /hack and travis config from contrib repo,"Copy /hack and travis from contrib (with minimal modifications to paths, etc). Skipping verification of flags for now.

Fix gofmt on a few files, so that travis can actually pass.",closed,True,2017-04-18 14:57:57,2017-04-18 15:00:07
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/3,https://api.github.com/repos/kubernetes/autoscaler/issues/3,Top level readme and license,,closed,True,2017-04-18 15:05:51,2017-04-18 15:29:27
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/4,https://api.github.com/repos/kubernetes/autoscaler/issues/4,Add travis status to README,The build icon is already green :) https://travis-ci.org/kubernetes/autoscaler.svg?branch=master,closed,True,2017-04-18 15:53:20,2017-04-18 17:29:38
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/5,https://api.github.com/repos/kubernetes/autoscaler/issues/5,Base Recommender model classes: Histogram and CircularBuffer.,,closed,True,2017-04-19 09:18:46,2017-04-19 12:16:13
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/6,https://api.github.com/repos/kubernetes/autoscaler/issues/6,Increase unit test coverage for cluster-autoscaler/simulator,Current test level is 69. Our target should be above 80% as the code is quite testable.,closed,False,2017-04-19 11:48:06,2017-04-20 09:21:27
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/7,https://api.github.com/repos/kubernetes/autoscaler/issues/7,Increase unit test coverage for cluster-autoscaler/core,Current coverage is 50%. We should reach 75% here.,closed,False,2017-04-19 11:55:28,2017-11-03 10:10:58
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/8,https://api.github.com/repos/kubernetes/autoscaler/issues/8,Base Recommender model classes: Histogram and CircularBuffer.,,closed,True,2017-04-19 11:56:34,2017-04-24 14:32:20
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/9,https://api.github.com/repos/kubernetes/autoscaler/issues/9,Increase unit test coverage for cluster-autoscaler/cloud-provider/gce.,The current coverage is 10%. Should be way higher.,closed,False,2017-04-19 11:56:42,2017-11-03 10:10:39
autoscaler,kenm47,https://github.com/kubernetes/autoscaler/pull/10,https://api.github.com/repos/kubernetes/autoscaler/issues/10,Minor Typo Change,Kubernets changed to Kubernetes,closed,True,2017-04-19 13:40:46,2017-04-19 17:17:27
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/pull/11,https://api.github.com/repos/kubernetes/autoscaler/issues/11,cluster-autoscaler: Re: AWS Autoscaler autodiscover ASG names and sizes,"A redo of https://github.com/kubernetes/contrib/pull/2549

---

This is an alternative implementation of https://github.com/kubernetes/contrib/pull/1982

Notable differences from the original PR are:

* A new flag named `--node-group-auto-discovery` is introduced for opting in to enable the auto-discovery feature.
  * For example, specifying `--cloud-provider aws --node-group-auto-discovery asg:tag=k8s.io/cluster-autoscaler/enabled` instructs CA to auto-discover ASGs tagged with `k8s.io/cluster-autoscaler/enabled` to be used as target node groups
* The new code path introduced by this PR is executed only when `node-group-auto-discovery` is specified. There is relatively less chance to break existing features by introducing this change

Resolves https://github.com/kubernetes/contrib/issues/1956

---

Other notes:

* ~~I thought it might be a good idea to implement this feature on top of the dynamic reconfiguration #2181 initially~~
  * ~~However it turned out it is easier and less verbose to just implement an alternative aws cloud provider like what I've done for this PR~~
* We rely mainly on the `DescribeTags` API rather than `DescribeAutoScalingGroups` so that AWS can filter out unnecessary ASGs which doesn't belong to the k8s cluster, for us.
  * If we relied on `DescribeAutoScalingGroups` here, as it doesn't support `Filter`ing, we'd need to iterate over ALL the ASGs available in an AWS account, which isn't desirable due to unnecessary excessive API calls and network usages

TODO/Possible future improvements before recommending this to everyone:

* Cache the result of an auto-discovery for a configurable period, so that we won't invoke DescribeTags and DescribeAutoScalingGroup APIs too many times
",closed,True,2017-04-19 14:18:15,2017-05-12 04:50:10
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/12,https://api.github.com/repos/kubernetes/autoscaler/issues/12,Added UT for cluster_autoscaler/simulator/cluster/FindNodesToRemove,"This was one major function in simulator, that wasn't covered. A few remaining uncovered methods have little custom logic and are just functions for fetching stuff from API server - testing them would require a lot of mocking for little real value.

This brings simulator coverage to 79.4%, which I'm willing to call close enough to target of 80% stated in https://github.com/kubernetes/autoscaler/issues/6.",closed,True,2017-04-19 17:14:46,2017-04-19 17:38:08
autoscaler,GFilipek,https://github.com/kubernetes/autoscaler/pull/13,https://api.github.com/repos/kubernetes/autoscaler/issues/13,Vertical Pod Autoscaler : updater first version,"Updater component for Vertical Pod Autoscaler described in  https://github.com/kubernetes/community/pull/338

Runs in loop. On one iteration performs:
 - Fetching Vertical Pod Autoscaler configuration 
 - Fetching Pods information and resource allocation recommendations for pods. 
 - Calculating if pod update is required and how many replicas can be evicted
 - Evicting pods if recommended resources significantly vary from actual resources allocation.

Mocked parts - to be implemented:
 - Recommendation API for fetching data from Vertical Pod Autoscaler Recommender.
 - Vertical Pod Autoscaler lister for fetching Vertical Pod Autoscaler config.

cc: @mwielgus @kgrygiel @KarolKraskiewicz",closed,True,2017-04-20 06:12:12,2018-02-13 16:15:17
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/14,https://api.github.com/repos/kubernetes/autoscaler/issues/14,fix a link in readme,Also removed trailing whitespace,closed,True,2017-04-20 09:19:10,2017-04-20 10:08:02
autoscaler,bruceauyeung,https://github.com/kubernetes/autoscaler/pull/15,https://api.github.com/repos/kubernetes/autoscaler/issues/15,fix grammar error and links,"1. grammar error fix
2. replace GCE link with final page that the original link will redirect to",closed,True,2017-04-21 03:36:42,2017-04-21 09:24:48
autoscaler,gbergere,https://github.com/kubernetes/autoscaler/issues/16,https://api.github.com/repos/kubernetes/autoscaler/issues/16,cluster-autoscaler with AWS detect all nodes as unregister and delete them all...,"As you can see below in the cluster-autoscaler logs it keep retrying to kill my node because it detects it as unregistered...

```
I0420 13:07:54.181348       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:07:56.185240       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:07:58.278044       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:00.281941       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:02.291554       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:02.887964       1 aws_manager.go:187] Regenerating ASG information for k8s-jenkins-nodes
I0420 13:08:02.935320       1 static_autoscaler.go:124] 5 unregistered nodes present
I0420 13:08:02.935335       1 utils.go:161] Removing unregistered node aws:///eu-west-1b/i-0c9cff088fa234079
W0420 13:08:03.005527       1 utils.go:173] Failed to remove node aws:///eu-west-1b/i-0c9cff088fa234079: ValidationError: Currently, desiredSize equals minSize (5). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
        status code: 400, request id: 63fa02d8-25ca-11e7-bd04-5525bf04935c
W0420 13:08:03.005554       1 static_autoscaler.go:131] Failed to remove unregistered nodes: ValidationError: Currently, desiredSize equals minSize (5). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
        status code: 400, request id: 63fa02d8-25ca-11e7-bd04-5525bf04935c
I0420 13:08:04.374476       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:06.574583       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:08.578247       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:10.582628       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:12.674304       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:13.108901       1 aws_manager.go:187] Regenerating ASG information for k8s-jenkins-nodes
I0420 13:08:13.150615       1 static_autoscaler.go:124] 5 unregistered nodes present
I0420 13:08:13.150632       1 utils.go:161] Removing unregistered node aws:///eu-west-1b/i-0caa38d264ea8e5bd
W0420 13:08:13.227270       1 utils.go:173] Failed to remove node aws:///eu-west-1b/i-0caa38d264ea8e5bd: ValidationError: Currently, desiredSize equals minSize (5). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
        status code: 400, request id: 6a11c420-25ca-11e7-bd04-5525bf04935c
W0420 13:08:13.227300       1 static_autoscaler.go:131] Failed to remove unregistered nodes: ValidationError: Currently, desiredSize equals minSize (5). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
        status code: 400, request id: 6a11c420-25ca-11e7-bd04-5525bf04935c
I0420 13:08:14.679535       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:16.684021       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:18.778081       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:20.781735       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:22.784877       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:23.371558       1 aws_manager.go:187] Regenerating ASG information for k8s-jenkins-nodes
I0420 13:08:23.410711       1 static_autoscaler.go:124] 5 unregistered nodes present
I0420 13:08:23.410729       1 utils.go:161] Removing unregistered node aws:///eu-west-1b/i-02d32be84bef14074
W0420 13:08:23.468063       1 utils.go:173] Failed to remove node aws:///eu-west-1b/i-02d32be84bef14074: ValidationError: Currently, desiredSize equals minSize (5). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
        status code: 400, request id: 702e6712-25ca-11e7-9208-1b1cd043bb1a
W0420 13:08:23.468097       1 static_autoscaler.go:131] Failed to remove unregistered nodes: ValidationError: Currently, desiredSize equals minSize (5). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
        status code: 400, request id: 702e6712-25ca-11e7-9208-1b1cd043bb1a
I0420 13:08:24.788189       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:26.874330       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:28.877450       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:29.780194       1 reflector.go:405] k8s.io/contrib/cluster-autoscaler/utils/kubernetes/listers.go:156: Watch close - *v1.Pod total 0 items received
I0420 13:08:30.880923       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:32.886314       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:33.589574       1 aws_manager.go:187] Regenerating ASG information for k8s-jenkins-nodes
I0420 13:08:33.640812       1 static_autoscaler.go:124] 5 unregistered nodes present
I0420 13:08:33.640831       1 utils.go:161] Removing unregistered node aws:///eu-west-1b/i-02d32be84bef14074
W0420 13:08:33.707148       1 utils.go:173] Failed to remove node aws:///eu-west-1b/i-02d32be84bef14074: ValidationError: Currently, desiredSize equals minSize (5). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
        status code: 400, request id: 764786f0-25ca-11e7-aff8-b5aec7e2f86b
W0420 13:08:33.707175       1 static_autoscaler.go:131] Failed to remove unregistered nodes: ValidationError: Currently, desiredSize equals minSize (5). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
        status code: 400, request id: 764786f0-25ca-11e7-aff8-b5aec7e2f86b
I0420 13:08:34.889530       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:36.892950       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:38.977885       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:40.981849       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:42.985896       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0420 13:08:44.066389       1 aws_manager.go:187] Regenerating ASG information for k8s-jenkins-nodes
I0420 13:08:44.119056       1 static_autoscaler.go:124] 5 unregistered nodes present
I0420 13:08:44.119070       1 utils.go:161] Removing unregistered node aws:///eu-west-1b/i-02d32be84bef14074
W0420 13:08:44.185569       1 utils.go:173] Failed to remove node aws:///eu-west-1b/i-02d32be84bef14074: ValidationError: Currently, desiredSize equals minSize (5). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
        status code: 400, request id: 7c86a6a7-25ca-11e7-b7f1-890a3321955a
W0420 13:08:44.185598       1 static_autoscaler.go:131] Failed to remove unregistered nodes: ValidationError: Currently, desiredSize equals minSize (5). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
        status code: 400, request id: 7c86a6a7-25ca-11e7-b7f1-890a3321955a
```

So after a few research everywhere and finally in the code I've found this :

Kubernetes repo : `kubernetes/pkg/kubelet/kubelet.go` (l 2096)
```go
func (kl *Kubelet) updateCloudProviderFromMachineInfo(node *v1.Node, info *cadvisorapi.MachineInfo) {
	if info.CloudProvider != cadvisorapi.UnknownProvider &&
		info.CloudProvider != cadvisorapi.Baremetal {
		// The cloud providers from pkg/cloudprovider/providers/* that update ProviderID
		// will use the format of cloudprovider://project/availability_zone/instance_name
		// here we only have the cloudprovider and the instance name so we leave project
		// and availability zone empty for compatibility.
		node.Spec.ProviderID = strings.ToLower(string(info.CloudProvider)) +
			"":////"" + string(info.InstanceID)
	}
}
```

They build ProviderID like `aws:////INSTANC_ID`

`autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go` (l 220)
```go
// GetAsgNodes returns Asg nodes.
func (m *AwsManager) GetAsgNodes(asg *Asg) ([]string, error) {
	result := make([]string, 0)
	group, err := m.getAutoscalingGroup(asg.Name)
	if err != nil {
		return []string{}, err
	}
	for _, instance := range group.Instances {
		result = append(result,
			fmt.Sprintf(""aws:///%s/%s"", *instance.AvailabilityZone, *instance.InstanceId))
	}
	return result, nil
}
```

You build ProviderID like `aws:///AWS_REGION/INSTANCE_ID`

`autoscaler/cluster-autoscaler/clusterstate/clusterstate.go` (l 705)
```go
// Calculates which of the existing cloud provider nodes are not registered in Kuberenetes.
func getNotRegisteredNodes(allNodes []*apiv1.Node, cloudProvider cloudprovider.CloudProvider, time time.Time) ([]UnregisteredNode, error) {
	registered := sets.NewString()
	for _, node := range allNodes {
		registered.Insert(node.Spec.ProviderID)
	}
	notRegistered := make([]UnregisteredNode, 0)
	for _, nodeGroup := range cloudProvider.NodeGroups() {
		nodes, err := nodeGroup.Nodes()
		if err != nil {
			return []UnregisteredNode{}, err
		}
		for _, node := range nodes {
			if !registered.Has(node) {
				notRegistered = append(notRegistered, UnregisteredNode{
					Node: &apiv1.Node{
						ObjectMeta: metav1.ObjectMeta{
							Name: node,
						},
						Spec: apiv1.NodeSpec{
							ProviderID: node,
						},
					},
					UnregisteredSince: time,
				})
			}
		}
	}
	return notRegistered, nil
}
```

So when you try to check if the instance is registered it's return false on `registered.Has(node)` due to a different way to implement it when it should be true.",closed,False,2017-04-21 10:40:51,2017-06-06 08:21:46
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/17,https://api.github.com/repos/kubernetes/autoscaler/issues/17,Table of contents for FAQ,+ generator in python,closed,True,2017-04-21 15:48:45,2017-04-25 17:13:44
autoscaler,jkinkead,https://github.com/kubernetes/autoscaler/pull/18,https://api.github.com/repos/kubernetes/autoscaler/issues/18,Clean up some grammar and punctuation.,Random readability edits & grammar fixes.,closed,True,2017-04-21 17:40:54,2017-04-24 08:56:23
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/19,https://api.github.com/repos/kubernetes/autoscaler/issues/19,Fix PVC informer issue,Fix for https://github.com/kubernetes/contrib/issues/2507,closed,True,2017-04-24 12:14:23,2017-04-24 12:19:15
autoscaler,gbergere,https://github.com/kubernetes/autoscaler/pull/20,https://api.github.com/repos/kubernetes/autoscaler/issues/20,Fix aws ProviderID pattern to match with the kubernetes version.,Fix this issue : https://github.com/kubernetes/autoscaler/issues/16,closed,True,2017-04-24 12:22:39,2017-05-05 22:21:31
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/21,https://api.github.com/repos/kubernetes/autoscaler/issues/21,Bump cluster autoscaler version to 0.5.2,,closed,True,2017-04-24 12:27:21,2017-04-24 12:33:43
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/22,https://api.github.com/repos/kubernetes/autoscaler/issues/22,Add CA e2e using volumes,https://github.com/kubernetes/autoscaler/pull/19 was not covered by existing tests and we should add an e2e to make sure something similar won't happen again.,closed,False,2017-04-24 15:39:27,2017-07-05 12:31:37
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/23,https://api.github.com/repos/kubernetes/autoscaler/issues/23,Fix error handling for updating node status,"Previously we were ignoring potential error in ClusterStateRegistry.UpdateNodes (which actually hits cloud provider internally and so may actually fail, leaving ClusterStateRegistry reflecting old state).",closed,True,2017-04-25 15:38:55,2017-04-26 12:30:51
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/24,https://api.github.com/repos/kubernetes/autoscaler/issues/24,Update readme for CA 0.5.2,cc: @MaciekPytel ,closed,True,2017-04-25 21:17:46,2017-04-27 14:29:37
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/25,https://api.github.com/repos/kubernetes/autoscaler/issues/25,Override hostname label when building a template node,"Existing hostname may interfere with pod affinity/antiaffinity.

cc: @MaciekPytel @wojtek-t ",closed,True,2017-04-27 14:09:03,2017-04-27 15:25:44
autoscaler,fgrzadkowski,https://github.com/kubernetes/autoscaler/issues/26,https://api.github.com/repos/kubernetes/autoscaler/issues/26,"Create ""concept"" documentation to explain autoscaling","We should have overall description of autoscaling and more detailed docs for HPA, VPA and cluster autoscaling.",closed,False,2017-04-28 07:54:24,2018-02-22 01:09:06
autoscaler,fbellagamba,https://github.com/kubernetes/autoscaler/issues/27,https://api.github.com/repos/kubernetes/autoscaler/issues/27,Question: Should nodes be deleted from kube after scale down?,"I'm writing a ""cloud_provider"" for Racspace, and while everything seems to be working _okay_, after a node is deleted it keeps in Kuebernetes as NotReady.

Is there anything my cloud provider should be doing to fire that node deletion?

Thanks!",closed,False,2017-04-28 15:05:26,2017-05-05 11:54:57
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/28,https://api.github.com/repos/kubernetes/autoscaler/issues/28,Overwrite pod.spec.nodename and node.name in template nodes for scale up,"To keep labels, nodename and pods more consistent in template nodes.
",closed,True,2017-04-28 15:23:27,2017-04-28 16:09:10
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/29,https://api.github.com/repos/kubernetes/autoscaler/issues/29,Create e2e test for PVCs in Cluster Autoscaler scale up.,,closed,False,2017-04-28 16:10:15,2017-04-28 16:11:36
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/30,https://api.github.com/repos/kubernetes/autoscaler/issues/30,Create e2e test for PodAntiAffinity in Cluster Autoscaler Scaler Up,,closed,False,2017-04-28 16:10:58,2017-06-23 16:37:17
autoscaler,rsmitty,https://github.com/kubernetes/autoscaler/issues/31,https://api.github.com/repos/kubernetes/autoscaler/issues/31,"Feature Question/Request: Support other metrics when determining ""scale-up""","Hello,

I wasn't sure where the best place to ask this question would be, as I am not sure what Slack channel the cluster-autoscaler tool falls under. Hope this works. 

I was wondering if there could be any other metrics used to determine when a cluster is scaled up. Specifically, I was hoping to see something along the lines of ""okay, let's scale up when the average utilization across the cluster is greater than 80%"" in addition to the current ""scale when pods are pending"" approach. Has there been any discussion around this? Our current scale-up takes a few minutes and I want to be able to scale in anticipation of more Pods coming online in addition to when they've already been requested.

Thanks!",closed,False,2017-05-02 15:29:02,2017-05-12 21:44:41
autoscaler,yawboateng,https://github.com/kubernetes/autoscaler/issues/32,https://api.github.com/repos/kubernetes/autoscaler/issues/32,Failed to drain node - pods remaining after timeout,"Is it possible to increase the node drain timeout?

seeing this in logs:
`Failed to scale down: Failed to delete ip-10-100-6-220.ec2.internal: Failed to drain node /ip-10-100-6-220.ec2.internal: pods remaining after timeout`",closed,False,2017-05-03 14:10:21,2018-04-27 21:23:03
autoscaler,emielvanlankveld,https://github.com/kubernetes/autoscaler/issues/33,https://api.github.com/repos/kubernetes/autoscaler/issues/33,StatefulSet with AntiAffinity prevents cluster-autoscaler from working,"**Is this a BUG REPORT or FEATURE REQUEST? (choose one):** Bug report

**Kubernetes version (use kubectl version):**
```
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.0"", GitCommit:""fff5156092b56e6bd60fff75aad4dc9de6b6ef37"", GitTreeState:""clean"", BuildDate:""2017-03-28T16:36:33Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T20:22:08Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
```
**Environment:**
- **Cloud provider or hardware configuration:** gke (machine type n1-highcpu-4)
- **OS (e.g. from /etc/os-release):** Alpine Linux v3.5
- **Kernel (e.g. uname -a):** Linux cc332daac761 4.9.13-moby SMP Sat Mar 25 02:48:44 UTC 2017 x86_64 Linux

**What happened:**
We have an issue with the cluster-autoscaler where new pods are stuck on Pending and a new node isn't being created. We see these events in the pod:

```
  FirstSeen	LastSeen	Count	From			SubObjectPath	Type		Reason			Message
  ---------	--------	-----	----			-------------	--------	------			-------
  3m		7s		22	cluster-autoscaler			Normal		NotTriggerScaleUp	pod didn't trigger scale-up (it wouldn't fit if a new node is added)
  4m		0s		17	default-scheduler			Warning		FailedScheduling	No nodes are available that match all of the following predicates:: Insufficient cpu (2).
```

**What you expected to happen:**
The pods request only 1 CPU resource, so pods would definitely fit on a new node of instance type n1-highcpu-4. 

**How to reproduce it (as minimally and precisely as possible):**
We can reproduce this by creating a new simple cluster with the following command:

`gcloud container clusters create scale-test --cluster-version 1.6.2 --zone us-east1-b --additional-zones us-east1-c --machine-type n1-highcpu-4 --num-nodes 1 --preemptible --enable-autoupgrade --enable-autorepair --enable-autoscaling --min-nodes 1 --max-nodes 10`

We then run `kubectl apply -f ""deploy.yml""` with the following configuration:

```
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: busy-loop
spec:
  replicas: 1
  revisionHistoryLimit: 5
  template:
    metadata:
      labels:
        tier: core
        app: busy-loop
    spec:
      nodeSelector:
        cloud.google.com/gke-preemptible: ""true""
      containers:
      - name: busy-loop
        image: <SIMPLE BUSY LOOP IMAGE>
        ports:
        - containerPort: 5950
          name: busy-loop
        resources:
          requests:
            cpu: ""1000m""
            memory: ""256Mi""
        livenessProbe:
          exec:
            command:
            - cat
            - deploy.yml
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: busy-loop
spec:
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: busy-loop
  minReplicas: 2
  maxReplicas: 100
  targetCPUUtilizationPercentage: 10
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: rethinkdb
spec:
  serviceName: rethinkdb
  replicas: 3
  template:
    metadata:
      labels:
        tier: data
        app: rethinkdb
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: rethinkdb
              topologyKey: kubernetes.io/hostname
      containers:
      - name: rethinkdb
        image: <SIMPLE RETHINKDB IMAGE>
        readinessProbe:
          tcpSocket:
            port: 28015
```

With this configuration new nodes are not being created at all with the NotTriggerScaleUp event message returned by the cluster-autoscaler. When we perform the exact same steps except remove the `affinity` setting from the configuration new nodes are created without a problem. It seems that the AntiAffinity in some way makes the cluster-autoscaler incorrectly think that there wouldn't be any room on a new node.",closed,False,2017-05-04 09:19:02,2017-05-05 11:53:52
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/34,https://api.github.com/repos/kubernetes/autoscaler/issues/34,Cluster autoscaler 0.5.3,,closed,True,2017-05-04 14:13:33,2017-05-04 14:24:02
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/35,https://api.github.com/repos/kubernetes/autoscaler/issues/35,Release nodes in readme for ca 0.5.3.,,closed,True,2017-05-05 11:46:49,2017-05-05 11:56:17
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/36,https://api.github.com/repos/kubernetes/autoscaler/issues/36,Cherry pick #28 #34 #45 to cluster-autoscaler-release-0.5 branch,,closed,True,2017-05-05 12:06:48,2017-05-05 12:22:12
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/37,https://api.github.com/repos/kubernetes/autoscaler/issues/37,Add deletion safety margin to node drain,"If a pod is ignoring SIGTERMs and a bad timing happens we may give up waiting for the pod to be killed by Kubelet after MaxGracefulTermination before this timeout actually happen. This PR adds an additional time to the pod checking routine that should get ensure that we catch the pod deletion by SIGKILL and correctly proceed with node drain.

cc: @MaciekPytel @andrewsykim 

ref: #32 ",closed,True,2017-05-05 22:36:08,2017-05-08 10:22:12
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/issues/38,https://api.github.com/repos/kubernetes/autoscaler/issues/38,cluster-autoscaler: Adding ca-certificates package to the docker image,Extracted from https://github.com/kubernetes/autoscaler/pull/11#discussion_r113835912,closed,False,2017-05-08 03:24:17,2017-05-09 21:52:39
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/39,https://api.github.com/repos/kubernetes/autoscaler/issues/39,Fix typo in graceful termination flag in CA,To do before next major release. Also includes fixing all the typos in related variables in code. ,closed,False,2017-05-08 09:58:23,2017-06-14 12:54:28
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/40,https://api.github.com/repos/kubernetes/autoscaler/issues/40,Bump CA version to 0.5.4,Fixes  drain timeouts when pods are ignoring SIGTERM.,closed,True,2017-05-08 11:36:03,2017-05-08 11:45:16
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/41,https://api.github.com/repos/kubernetes/autoscaler/issues/41,Readme update for 0.5.4,cc: @MaciekPytel ,closed,True,2017-05-08 16:51:59,2017-05-08 16:53:35
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/42,https://api.github.com/repos/kubernetes/autoscaler/issues/42,Sync release-0.5 branch after 0.5.4,,closed,True,2017-05-08 17:01:55,2017-05-08 17:26:58
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/43,https://api.github.com/repos/kubernetes/autoscaler/issues/43,Allow scaling node group to/from 0.,"This is an umbrella bug for allowing scaling some node groups to/from 0. 

cc: @MaciekPytel @fgrzadkowski ",closed,False,2017-05-08 19:25:14,2017-07-13 21:39:06
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/44,https://api.github.com/repos/kubernetes/autoscaler/issues/44,Add a method to NodeGroup for providing a template NodeInfo,"... that can be used in scale-ups in case of no live node example.

Part of #43.

cc: @MaciekPytel @fgrzadkowski ",closed,True,2017-05-08 19:50:40,2017-05-09 11:47:07
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/issues/45,https://api.github.com/repos/kubernetes/autoscaler/issues/45,cluster-autoscaler: Excessive calls to DescribeAutoScalingGroup,"a.k.a https://github.com/kubernetes/contrib/issues/2541.
I'm recreating the issue here so that we can better track it from the corresponding and upcoming PR.
",closed,False,2017-05-09 03:06:35,2017-06-23 09:58:27
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/pull/46,https://api.github.com/repos/kubernetes/autoscaler/issues/46,cluster-autoscaler: Fix excessive calls to DescribeAutoScalingGroup,"By caching AWS refs for nodes/EC2 instances already known to be not in any of ASGs managed by cluster-autoscaler(CA).

Please beware of the edge case - this method is safe as long as users don't attach nodes by calling AttachInstances API after CA cached them. I believe, even if it was necessary, a warning in the documentation about the edge case is enough for now. If we really need to support the case, I will submit an another PR to invalidate the cache periodically so that CA can detect the formerly cached nodes are attached to ASG(s).

The docker image built from this branch is available for testing at `mumoshu/fix-excessive-describe-asg-calls`.

You can see that CA detects and remembers nodes in unmanaged ASGs so that it can prevent the nodes from triggering unnecessary `regenerateCache` invocations afterwards:

```
I0509 05:37:28.165739       1 leaderelection.go:204] succesfully renewed lease kube-system/cluster-autoscaler
I0509 05:37:28.824668       1 aws_manager.go:197] Regenerating ASG information for kube4-Asg1-5GX1OEW9YLPA-Workers-1WBDQUOBGXA6A
I0509 05:37:28.882084       1 aws_manager.go:188] Instance {Name:i-06079265fc8999d4c} is not in any ASG managed by CA. CA is now memorizing the fact not to unnecessarily call AWS API afterwards trying to find the unexistent managed ASG for the instance
I0509 05:37:28.882149       1 aws_manager.go:197] Regenerating ASG information for kube4-Asg1-5GX1OEW9YLPA-Workers-1WBDQUOBGXA6A
I0509 05:37:28.918931       1 aws_manager.go:188] Instance {Name:i-0c6c700cbcc62ed1d} is not in any ASG managed by CA. CA is now memorizing the fact not to unnecessarily call AWS API afterwards trying to find the unexistent managed ASG for the instance
I0509 05:37:28.918984       1 aws_manager.go:197] Regenerating ASG information for kube4-Asg1-5GX1OEW9YLPA-Workers-1WBDQUOBGXA6A
I0509 05:37:28.946945       1 aws_manager.go:188] Instance {Name:i-0e48a422f75ae6500} is not in any ASG managed by CA. CA is now memorizing the fact not to unnecessarily call AWS API afterwards trying to find the unexistent managed ASG for the instance
I0509 05:37:28.947044       1 aws_manager.go:197] Regenerating ASG information for kube4-Asg1-5GX1OEW9YLPA-Workers-1WBDQUOBGXA6A
I0509 05:37:28.984235       1 aws_manager.go:188] Instance {Name:i-0a8cb4932d32e1296} is not in any ASG managed by CA. CA is now memorizing the fact not to unnecessarily call AWS API afterwards trying to find the unexistent managed ASG for the instance
I0509 05:37:28.984272       1 static_autoscaler.go:197] Filtering out schedulables
I0509 05:37:28.984338       1 static_autoscaler.go:205] No schedulable pods
I0509 05:37:28.984356       1 static_autoscaler.go:211] No unschedulable pods
```

Resolves #45",closed,True,2017-05-09 03:15:18,2017-06-23 09:58:27
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/issues/47,https://api.github.com/repos/kubernetes/autoscaler/issues/47,cluster-autoscaler: A simple mechanism to grow node groups simultaneously,"a.k.a https://github.com/kubernetes/contrib/issues/2352
I'm recreating the issue here so that we can better track it from the corresponding and upcoming PR.
",closed,False,2017-05-09 03:26:30,2017-06-20 02:17:14
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/pull/48,https://api.github.com/repos/kubernetes/autoscaler/issues/48,cluster-autoscaler: Add ca-certificates to the docker image,"This commit is manually tested by running `TAG=add-ca-certificates REGISTRY=mydockerrepo make release` and running it inside author's k8s cluster

The docker image is available at https://hub.docker.com/r/mumoshu/cluster-autoscaler
Looking into the various tags in the repo, you'll see that the image size is increased by 2MB, which is acceptable IMHO
![image](https://cloud.githubusercontent.com/assets/22009/25836283/674a9a3a-34c0-11e7-8386-c35d77e62b6e.png)

No errors seem to be occurring while accessing AWS API with the ca-certificates bundled in the image hence I assume it is working as expected:

```
I0509 05:07:58.029029       1 aws_manager.go:187] Regenerating ASG information for kube4-Asg1-5GX1OEW9YLPA-Workers-1WBDQUOBGXA6A
I0509 05:07:58.083395       1 aws_manager.go:187] Regenerating ASG information for kube4-Asg3-1QWZDGNI50QW-Workers-1OD7QVXAHMALV
I0509 05:07:58.125027       1 scale_down.go:218] Skipping ip-10-0-0-133.ap-northeast-1.compute.internal - no node group config
```

Resolves #38",closed,True,2017-05-09 05:04:39,2017-05-09 23:41:09
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/pull/49,https://api.github.com/repos/kubernetes/autoscaler/issues/49,cluster-autoscaler: Provide a mechanism to grow node groups simultaneously,"By introducing the `balance` expander.
It will try to balance number of nodes among target node groups by returning an `expander.Option` with the least `NodeCount`.

The docker image `mumoshu:grow-node-groups-simultaneously` built from this branch is available for testing at https://hub.docker.com/r/mumoshu/cluster-autoscaler/

With the `balance` expander, CA always select a node group with the least NodeCount from target groups.

Say we have two ASGs `kube4-Asg3-1QWZDGNI50QW-Workers-1OD7QVXAHMALV` with the node count of 1 and `kube4-Asg1-5GX1OEW9YLPA-Workers-1WBDQUOBGXA6A` with the node count of 5, CA with the `balance` expander scales out Asg3 and then Asg1:

```
I0509 05:57:07.133547       1 scale_up.go:145] Best option to resize: kube4-Asg3-1QWZDGNI50QW-Workers-1OD7QVXAHMALV
*snip*
I0509 05:57:18.171048       1 scale_up.go:145] Best option to resize: kube4-Asg1-5GX1OEW9YLPA-Workers-1WBDQUOBGXA6A
```

Resolves #47",closed,True,2017-05-09 05:06:18,2017-06-05 20:04:49
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/50,https://api.github.com/repos/kubernetes/autoscaler/issues/50,CA metrics proposal,,closed,True,2017-05-09 09:57:25,2017-05-11 08:34:32
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/51,https://api.github.com/repos/kubernetes/autoscaler/issues/51,Build node template in GCE cloud provider,"Step 2 for scale to/from 0.
Ref: #43
cc @MaciekPytel @fgrzadkowski ",closed,True,2017-05-09 21:51:43,2017-05-11 15:28:38
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/pull/52,https://api.github.com/repos/kubernetes/autoscaler/issues/52,cluster-autoscaler: Fix typos in comments,,closed,True,2017-05-10 01:09:37,2017-05-10 08:17:31
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/pull/53,https://api.github.com/repos/kubernetes/autoscaler/issues/53,cluster-autoscaler: Fix an incorrect message when CA failed to find a place for a to-be-removed pod,"The message has been always `<evaluation type>: node <node name> is not suitable for removal nil`.
It should be `<evaluation type>: node <node name> is not suitable for removal: failed to find place for <pod key>`",closed,True,2017-05-10 01:39:57,2017-05-10 09:04:45
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/54,https://api.github.com/repos/kubernetes/autoscaler/issues/54,Set ready status in gce node template,"So that the readiness checking code work properly.

cc: @MaciekPytel 

Ref: #43",closed,True,2017-05-10 11:03:24,2017-05-10 11:45:31
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/55,https://api.github.com/repos/kubernetes/autoscaler/issues/55,Daemonset lister,Ref: #43,closed,True,2017-05-10 21:18:39,2017-05-11 11:35:08
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/pull/56,https://api.github.com/repos/kubernetes/autoscaler/issues/56,cluster-autoscaler: Resource slack for faster pod startup,"Ref https://github.com/kubernetes/contrib/issues/2251

* `--min-extra-capacity-rate=0.1` to make 10% resource slack

A working example of CA running inside my test cluster with `--min-extra-capacity-rate=2`(=200%) with 2 node groups:
```
I0511 07:32:48.984318       1 aws_manager.go:187] Regenerating ASG information for kube5-Asg1-1EAU86YWRJGT6-Workers-5FQ8MWC9B6QG
I0511 07:32:49.019873       1 aws_manager.go:187] Regenerating ASG information for kube5-Asg3-PTR2VK0YHXGP-Workers-W0KCGPQKD92H
I0511 07:32:49.057607       1 resource_slack.go:156] Creating placeholder pods based on the following observations: cluster size = 3, cluster cpu sum = 6 cores, cluster memory sum = 12148524Ki, avg milli cpu per node = 2000, avg memory per node = 3.9GB, cluster total milli cpu = 6000, cluster total memory = 11.6GB, cluster extra milli cpu = 12000, cluster extra memory = 23.2GB, max pod milli cpu = 200, max pod memory = 0.3GB, remaining cluster extra milli cpu = 11800, remaining cluster extra memory = 22.9GB, min node milli cpu = 2000, min node memory = 3.9GB, milli cpu per extra placeholder pod = 200, memory per extra placeholder pod = 0.3GB, extra placeholder pods count = 59
I0511 07:32:49.057786       1 resource_slack.go:191] Created 61 placeholder pods
I0511 07:32:49.057851       1 resource_slack.go:205] Pod placeholder-largestcpu-200m-0MB is virtually scheduled on ip-10-0-0-240.ap-northeast-1.compute.internal
I0511 07:32:49.057884       1 resource_slack.go:205] Pod placeholder-largestmemory-0m-300MB is virtually scheduled on ip-10-0-0-240.ap-northeast-1.compute.internal
I0511 07:32:49.057921       1 resource_slack.go:205] Pod placeholder-standard-0-200m-300MB is virtually scheduled on ip-10-0-0-176.ap-northeast-1.compute.internal
I0511 07:32:49.057941       1 resource_slack.go:205] Pod placeholder-standard-1-200m-300MB is virtually scheduled on ip-10-0-0-176.ap-northeast-1.compute.internal
I0511 07:32:49.057966       1 resource_slack.go:205] Pod placeholder-standard-2-200m-300MB is virtually scheduled on ip-10-0-0-240.ap-northeast-1.compute.internal
I0511 07:32:49.057995       1 resource_slack.go:205] Pod placeholder-standard-3-200m-300MB is virtually scheduled on ip-10-0-0-240.ap-northeast-1.compute.internal
I0511 07:32:49.058015       1 resource_slack.go:205] Pod placeholder-standard-4-200m-300MB is virtually scheduled on ip-10-0-0-240.ap-northeast-1.compute.internal
I0511 07:32:49.058035       1 resource_slack.go:205] Pod placeholder-standard-5-200m-300MB is virtually scheduled on ip-10-0-0-240.ap-northeast-1.compute.internal
I0511 07:32:49.058060       1 resource_slack.go:205] Pod placeholder-standard-6-200m-300MB is virtually scheduled on ip-10-0-0-240.ap-northeast-1.compute.internal
I0511 07:32:49.058083       1 resource_slack.go:205] Pod placeholder-standard-7-200m-300MB is virtually scheduled on ip-10-0-0-240.ap-northeast-1.compute.internal
I0511 07:32:49.058117       1 resource_slack.go:205] Pod placeholder-standard-8-200m-300MB is virtually scheduled on ip-10-0-0-176.ap-northeast-1.compute.internal
I0511 07:32:49.058138       1 resource_slack.go:205] Pod placeholder-standard-9-200m-300MB is virtually scheduled on ip-10-0-0-176.ap-northeast-1.compute.internal
I0511 07:32:49.058158       1 resource_slack.go:205] Pod placeholder-standard-10-200m-300MB is virtually scheduled on ip-10-0-0-176.ap-northeast-1.compute.internal
I0511 07:32:49.058180       1 resource_slack.go:205] Pod placeholder-standard-11-200m-300MB is virtually scheduled on ip-10-0-0-176.ap-northeast-1.compute.internal
I0511 07:32:49.058210       1 resource_slack.go:205] Pod placeholder-standard-12-200m-300MB is virtually scheduled on ip-10-0-0-176.ap-northeast-1.compute.internal
I0511 07:32:49.058851       1 static_autoscaler.go:211] Filtering out schedulables
I0511 07:32:49.059482       1 static_autoscaler.go:219] No schedulable pods
I0511 07:32:49.059492       1 scale_up.go:44] Pod default/placeholder-standard-13-200m-300MB is unschedulable
I0511 07:32:49.059496       1 scale_up.go:44] Pod default/placeholder-standard-14-200m-300MB is unschedulable
I0511 07:32:49.059498       1 scale_up.go:44] Pod default/placeholder-standard-15-200m-300MB is unschedulable
I0511 07:32:49.059501       1 scale_up.go:44] Pod default/placeholder-standard-16-200m-300MB is unschedulable
*snip*
I0511 07:32:49.059613       1 scale_up.go:44] Pod default/placeholder-standard-57-200m-300MB is unschedulable
I0511 07:32:49.059615       1 scale_up.go:44] Pod default/placeholder-standard-58-200m-300MB is unschedulable
I0511 07:32:49.059652       1 aws_manager.go:187] Regenerating ASG information for kube5-Asg1-1EAU86YWRJGT6-Workers-5FQ8MWC9B6QG
I0511 07:32:49.115802       1 aws_manager.go:187] Regenerating ASG information for kube5-Asg3-PTR2VK0YHXGP-Workers-W0KCGPQKD92H
I0511 07:32:49.164903       1 round_trippers.go:417] GET https://10.3.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-0-0-240.ap-northeast-1.compute.internal 200 OK in 3 milliseconds
I0511 07:32:49.169433       1 round_trippers.go:417] GET https://10.3.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-0-0-176.ap-northeast-1.compute.internal 200 OK in 2 milliseconds
I0511 07:32:49.170768       1 scale_up.go:62] Upcoming 0 nodes
I0511 07:32:49.250576       1 waste.go:57] Expanding Node Group kube5-Asg1-1EAU86YWRJGT6-Workers-5FQ8MWC9B6QG would waste 8.00% CPU, 30.21% Memory, 19.10% Blended
I0511 07:32:49.250606       1 waste.go:57] Expanding Node Group kube5-Asg3-PTR2VK0YHXGP-Workers-W0KCGPQKD92H would waste 8.00% CPU, 30.21% Memory, 19.10% Blended
I0511 07:32:49.250620       1 scale_up.go:145] Best option to resize: kube5-Asg3-PTR2VK0YHXGP-Workers-W0KCGPQKD92H
I0511 07:32:49.250628       1 scale_up.go:149] Estimated 5 nodes needed in kube5-Asg3-PTR2VK0YHXGP-Workers-W0KCGPQKD92H
```

Although this work is rough around the edges but I hope it is reviewable.
@Raffo Would you mind taking a look?
",closed,True,2017-05-11 07:34:30,2017-07-07 09:41:47
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/57,https://api.github.com/repos/kubernetes/autoscaler/issues/57,Min at zero - design doc,"Ref: #43
cc: @MaciekPytel ",closed,True,2017-05-11 10:23:42,2017-05-11 12:45:25
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/58,https://api.github.com/repos/kubernetes/autoscaler/issues/58,Added CA metrics related to cluster state,"Part 1 of implementation of #59  - ""Cluster state"" metrics",closed,True,2017-05-11 11:57:13,2017-05-11 12:38:38
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/59,https://api.github.com/repos/kubernetes/autoscaler/issues/59,Add metrics endpoint to allow monitoring cluster-autoscaler,Design doc PR: https://github.com/kubernetes/autoscaler/pull/50,closed,False,2017-05-11 12:18:58,2017-06-06 18:01:10
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/60,https://api.github.com/repos/kubernetes/autoscaler/issues/60,Update scale to 0 proposal with allocatable estimation,"Follow up on: https://github.com/kubernetes/autoscaler/pull/57.

cc: @MaciekPytel @fgrzadkowski ",closed,False,2017-05-11 12:46:28,2017-11-03 10:09:59
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/61,https://api.github.com/repos/kubernetes/autoscaler/issues/61,Added CA metrics related to autoscaler execution,"Part 2 of implementation of #59 - ""CA execution"" metrics
",closed,True,2017-05-11 12:53:06,2017-05-12 10:40:14
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/62,https://api.github.com/repos/kubernetes/autoscaler/issues/62,Daemonset helper function that returns pods that DaemonSet controller would start on the given node.,ref: #43 ,closed,True,2017-05-11 18:57:04,2017-05-12 09:59:05
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/63,https://api.github.com/repos/kubernetes/autoscaler/issues/63,Fix predicate error messages,"Previously, on predicate checker errors, the failure reasons were improperly printed and predicate name was missing. This PR fixes both issues.

cc: @MaciekPytel ",closed,True,2017-05-11 19:46:58,2017-05-12 09:20:25
autoscaler,adamrp,https://github.com/kubernetes/autoscaler/issues/64,https://api.github.com/repos/kubernetes/autoscaler/issues/64,scaling-down nodes that are running only system pods,"I'm using:
```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T20:33:11Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T20:22:08Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
```

What I did:
- I created a one-node GKE cluster, cluster autoscaling disabled
- At the time of creation, all kube-system pods were running on the one node
- I subsequently enabled cluster autoscaling with a minimum cluster size of 1 and a maximum of 5
- All kube-system pods continued to run on the one original node
- I ran a Job that taxed the system to the extent that CA added a new node
- After the job was finished, all pods were removed, leaving only kube-system pods running

What I expected:
- CA would scale down the cluster to its initial (minimum) size (one node) shortly after the taxing job was complete

What happened:
- The cluster stayed at two nodes
- When CA added a new node, the kube-system pods had been redistributed to run across *both* nodes
- According to [the FAQ here](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#i-have-a-couple-of-nodes-with-low-utilization-but-they-are-not-scaled-down-why), this happened because CA will not remove nodes running system pods

Suggestion:
- If a cluster is above its configured minimum size *only* because kube-system pods are preventing it from shrinking, the superfluous node(s) should be automatically drained, the kube-system pods should be consolidated onto a single node (or rather, onto a number of nodes equal to the cluster's configured minimum), and the superfluous nodes should be removed from the cluster",closed,False,2017-05-11 19:52:02,2018-08-02 11:37:01
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/65,https://api.github.com/repos/kubernetes/autoscaler/issues/65,Docker builder for cluster autoscaler,"Our current build process is prone for gopath pollution and embeds homedir paths in binaries. With this PR the build will be done in a dedicated Docker container with empty gopath and correct golang binary.  

cc: @MaciekPytel @mumoshu @andrewsykim @fgrzadkowski ",closed,True,2017-05-11 21:00:40,2017-05-12 09:58:55
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/66,https://api.github.com/repos/kubernetes/autoscaler/issues/66,Contact information in the main README.md,"To promote weekly sig meetings and boost autoscaling slack channel.

cc: @MaciekPytel @mumoshu  @fgrzadkowski @andrewsykim ",closed,True,2017-05-12 11:20:31,2017-05-12 11:34:35
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/67,https://api.github.com/repos/kubernetes/autoscaler/issues/67,Whats inside - summary of projects available in the repository,cc: @MaciekPytel ,closed,True,2017-05-12 11:29:45,2017-05-12 11:41:18
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/68,https://api.github.com/repos/kubernetes/autoscaler/issues/68,Remove : from headers in CA readme,,closed,True,2017-05-12 12:25:47,2017-05-12 12:29:11
autoscaler,ohaiwalt,https://github.com/kubernetes/autoscaler/pull/69,https://api.github.com/repos/kubernetes/autoscaler/issues/69,cluster-autoscaler: Correct typos for `deamon` -> `daemon` as in `DaemonSet`,,closed,True,2017-05-12 17:29:56,2017-05-15 08:51:53
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/70,https://api.github.com/repos/kubernetes/autoscaler/issues/70,Add metrics counting CA operations,"Part 3 of implementation of #59 - ""CA operations"" metrics, except for 'errors_total'
",closed,True,2017-05-15 11:07:13,2017-05-15 11:26:08
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/71,https://api.github.com/repos/kubernetes/autoscaler/issues/71,Use TemplateNodeInfo in scale up,ref: #43,closed,True,2017-05-15 21:48:05,2017-05-16 09:57:52
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/72,https://api.github.com/repos/kubernetes/autoscaler/issues/72,Prefer using ready nodes and cloudprovider template nodes over unready/unschedulable nodes in scale-up,ref: #43 ,closed,True,2017-05-16 11:11:13,2017-05-16 12:00:45
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/73,https://api.github.com/repos/kubernetes/autoscaler/issues/73,Add manifest-run kube-proxy to GCE template node,"By default in 1.6 GCE runs kube-proxy from manifest and fluentd as daemonset. Cloud provider is expected to set only the manifest run pods. DS are added in scale-up.

ref: #43 
",closed,True,2017-05-16 14:19:25,2017-05-17 09:23:28
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/74,https://api.github.com/repos/kubernetes/autoscaler/issues/74,"Add typed errors, add errors_total metric",Ref: https://github.com/kubernetes/autoscaler/issues/59,closed,True,2017-05-16 15:08:57,2017-05-19 09:48:52
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/75,https://api.github.com/repos/kubernetes/autoscaler/issues/75,Enable min size 0 in gce,ref: #43 ,closed,True,2017-05-16 15:30:55,2017-05-16 21:40:01
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/76,https://api.github.com/repos/kubernetes/autoscaler/issues/76,Cluster Autoscaler does a list of pods for nodegroup instead of using informer,"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/simulator/nodes.go#L36

This is called for by BuildNodeInfoForNode, which is called for each node by GetNodeInfosForGroups. We should change it to informer.

cc: @mwielgus ",closed,False,2017-05-17 12:47:40,2018-02-22 18:26:05
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/pull/77,https://api.github.com/repos/kubernetes/autoscaler/issues/77,cluster-autoscaler: Resource Slack Proposal,"This is a proposal for #56 
Any comment is appreciated.
@mwielgus @MaciekPytel Could you take a look into this?
",closed,True,2017-05-17 13:11:25,2018-01-03 23:09:53
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/78,https://api.github.com/repos/kubernetes/autoscaler/issues/78,Handle custom machine types in GCE cloud provider,ref: #43 ,closed,True,2017-05-18 14:14:50,2017-05-18 14:38:31
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/79,https://api.github.com/repos/kubernetes/autoscaler/issues/79,Add health check,Basic health check to use with liveness probe.,closed,True,2017-05-19 13:52:10,2017-05-25 15:18:01
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/80,https://api.github.com/repos/kubernetes/autoscaler/issues/80,Handle empty node groups in cluster state,"Previously we were treating empty groups as unready and is scaling up could be incorrect.

Ref: #43  ",closed,True,2017-05-19 15:37:38,2017-05-19 16:04:58
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/81,https://api.github.com/repos/kubernetes/autoscaler/issues/81,Add a class to aggregate container usage samples.,"Introducing ContainerStats, a type which stores the aggregated usage history for a single container (CPU histogram + a sliding window with daily memory peaks from the last week). The history stored in this object will be populated from usage samples fetched from the real time usage data source with the AddSample() method.

The ContainerStats objects will be the main input for the Recommender to produce a recommendation for a container.",closed,True,2017-05-22 20:40:43,2017-07-04 12:31:14
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/82,https://api.github.com/repos/kubernetes/autoscaler/issues/82,Cost & preferred_node expander ,"This is an umbrella bug for an effort to provide a decent expander function for CA. The function
will :
* Include cost of nodes and how well the money are spent
* Promote small nodes in small clusters and larger in large clusters
* Balance the two above based on the number of pending pods and/or requested capacity.",closed,False,2017-05-22 21:49:21,2017-06-20 16:50:25
autoscaler,jansel,https://github.com/kubernetes/autoscaler/pull/83,https://api.github.com/repos/kubernetes/autoscaler/issues/83,Update cluster-autoscaler version number in AWS example deployment,,closed,True,2017-05-23 17:22:08,2017-05-25 11:59:05
autoscaler,ejether,https://github.com/kubernetes/autoscaler/pull/84,https://api.github.com/repos/kubernetes/autoscaler/issues/84,Update README.md,"Empirically determined that the policy ""autoscaling:DescribeAutoScalingTags"" is actually ""autoscaling:DescribeTags"",",closed,True,2017-05-24 21:04:03,2017-05-31 20:44:17
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/issues/85,https://api.github.com/repos/kubernetes/autoscaler/issues/85,Scale down never occur when the node group auto-discovery feature is enabled,"This was originally [reported in the kube-aws repo](https://github.com/kubernetes-incubator/kube-aws/issues/677).

In nutshell, PollingAutoscaler, which is used only when the auto-discovery feature is enabled, is resetting `lastScaleDownFailedTrial` to the current time before each iteration of CA.
`lastScaleDownFailedTrial` must be maintained properly among different instances of StaticAutoscaler created per an iteration of CA=a node groups poll.
",closed,False,2017-05-25 07:27:49,2017-06-23 09:31:45
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/86,https://api.github.com/repos/kubernetes/autoscaler/issues/86,Pricing model interface for CA,"Ref: #82 
",closed,True,2017-05-25 08:33:30,2017-05-26 12:22:37
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/87,https://api.github.com/repos/kubernetes/autoscaler/issues/87,Price-based node group ranking function proposal,Ref: #82 ,closed,True,2017-05-25 09:49:08,2017-05-29 11:56:44
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/88,https://api.github.com/repos/kubernetes/autoscaler/issues/88,Add description of testing PR to CA FAQ,"Currently we don't have any standard process for testing Cluster Autoscaler, other than passing travis (which only run unittests, etc). Ideally we should setup automatic e2e run on presubmit, but I don't think we'll manage to do it before k8s 1.7.

So in the meantime I propose the following process for more complicated pull requests. What do you think?",closed,True,2017-05-25 13:13:39,2017-05-25 16:46:03
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/89,https://api.github.com/repos/kubernetes/autoscaler/issues/89,"e2e test ""should correctly scale down after a node is not needed and one node is broken"" is flaky",cc: @mwielgus ,closed,False,2017-05-25 14:03:01,2017-06-20 02:05:50
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/90,https://api.github.com/repos/kubernetes/autoscaler/issues/90,Add failing health check if autoscaler loop consistently returns error,Extend HealthCheck to verify that a successful run of autoscaler loop occured within given (configurable by setting --max-failing-time flag) time.,closed,True,2017-05-26 11:08:24,2017-05-29 11:39:03
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/91,https://api.github.com/repos/kubernetes/autoscaler/issues/91,GCE pricing model,"ref: #82 
",closed,True,2017-05-26 14:15:57,2017-05-26 19:52:52
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/92,https://api.github.com/repos/kubernetes/autoscaler/issues/92,Price-preferred node expander - part 1,"ref: #82 
design: #87 

Missing: Preferred node part.",closed,True,2017-05-29 21:47:31,2017-05-30 11:51:54
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/93,https://api.github.com/repos/kubernetes/autoscaler/issues/93,Proposal for balancing node groups for multizone,"Ref: #47 

@mumoshu - does that look good to you?",closed,True,2017-05-30 15:45:47,2017-05-31 09:23:30
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/94,https://api.github.com/repos/kubernetes/autoscaler/issues/94,Preferred node in price-preferred expander,Ref: #82,closed,True,2017-05-30 16:55:22,2017-05-30 18:44:50
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/95,https://api.github.com/repos/kubernetes/autoscaler/issues/95,Proposals in FAQ,To make them more discoverable.,closed,True,2017-05-30 18:57:47,2017-05-31 16:41:41
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/96,https://api.github.com/repos/kubernetes/autoscaler/issues/96,Combine price and preferred node in price expander,"Ref: #82 

The tests will be rewritten to be specified in a table-like form in the next PR.",closed,True,2017-05-30 22:18:44,2017-06-01 10:04:57
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/97,https://api.github.com/repos/kubernetes/autoscaler/issues/97,Function to compare nodeinfos to find similar nodegroups,Ref: #47 ,closed,True,2017-05-31 08:59:23,2017-05-31 16:49:27
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/98,https://api.github.com/repos/kubernetes/autoscaler/issues/98,Node drain simulator disregards PodDistributionBudget namespace,"CA is matching pods to PDBs by label only, without considering namespace. So if there's a PDB in different namespace than the pod, but matches its labels and has allowed-disruptions=0, the pod will be considered impossible to evict (and block node drain attempt), even if it's not actually the case.
",closed,False,2017-05-31 09:21:13,2017-06-01 01:30:27
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/99,https://api.github.com/repos/kubernetes/autoscaler/issues/99,Add checking namespace to matching pods with PDBs,Fix for #98 ,closed,True,2017-05-31 14:19:04,2017-06-01 08:58:37
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/100,https://api.github.com/repos/kubernetes/autoscaler/issues/100,Add allowing eviction of kube-system pods with PDB,This addresses #64 on CA side by allowing it to drain nodes with kube-system pods running if these pods match a PodDistrubutionBudget with allowed-disruptions > 0.,closed,True,2017-05-31 14:57:17,2017-06-02 17:33:40
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/101,https://api.github.com/repos/kubernetes/autoscaler/issues/101,Add checking namespace to matching pods with PDBs,"Fix for #98, cherry-picked from #99",closed,True,2017-06-01 08:57:25,2017-06-01 09:50:29
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/102,https://api.github.com/repos/kubernetes/autoscaler/issues/102,Function to find similar nodegroups,Ref: #47,closed,True,2017-06-01 09:57:04,2017-06-01 10:27:54
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/103,https://api.github.com/repos/kubernetes/autoscaler/issues/103,Function to balance scale-up between node groups,Ref: #47 ,closed,True,2017-06-01 14:07:15,2017-06-02 14:04:50
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/104,https://api.github.com/repos/kubernetes/autoscaler/issues/104,Update to go1.8.3,"Same version as k8s. Full godeps update will go out
in separate commit.",closed,True,2017-06-01 15:21:26,2017-06-02 13:13:57
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/105,https://api.github.com/repos/kubernetes/autoscaler/issues/105,Balance sizes of similar nodegroups in scale-up,Ref: #47 ,closed,True,2017-06-02 15:48:20,2017-06-06 17:46:14
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/106,https://api.github.com/repos/kubernetes/autoscaler/issues/106,Failing to write status configmap discards the original error message,"The only thing logged is ""Failed to write status configmap"", without the original error message. Obviously that is not particularly helpful for debugging.",closed,False,2017-06-05 20:15:15,2017-06-14 12:19:35
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/pull/107,https://api.github.com/repos/kubernetes/autoscaler/issues/107,cluster-autoscaler: Fix scale-down when the node group auto-discovery feature is enabled,"By fixing CA not to reset states of `StaticAutoscaler` and `ScaleDown` before each iteration.

Resolves #85 

This is how I'm going to test it:

- [x] Build and release a CA docker image `quay.io/kube-aws/cluster-autoscaler:8b7d410fc5b9dbc9b7c707994259770f15613676`
- [x] Deploy CA with [the example yaml](https://github.com/kubernetes/autoscaler/pull/11#issuecomment-297737376)
- [x] Create a k8s deployment as a scaling target: `kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=1000m,memory=1000M --expose --port=8`
- [x] Do a few manual tests with scaling a deployment up and down (Accordingly to [FAQ](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-should-i-test-my-code-before-submitting-pr))
  - [x] Confirm that scale-up occurs
  - [x] Confirm that scale-down occurs (Added 8b7d410 in order to fix the issue found here)
    - `kubectl scale deployment php-apache --replicas 1` and wait for the default scale-down-delay - 10 minutes
- [x] ~~Run e2e test suite on AWS, confirm it passes and all metrics have reasonable values after it's done~~
  * It turned out that e2e for CA doesn't work on AWS, probably due to e2e's inability to determine original cluster size which is required in order to reset the cluster size after each test. [Revelant log can be seen in my gist](https://gist.github.com/mumoshu/f73d2bbf45f82d732cc577524e7ca2f7)

cc @MaciekPytel ",closed,True,2017-06-07 01:18:38,2017-06-23 09:31:45
autoscaler,mtcode,https://github.com/kubernetes/autoscaler/issues/108,https://api.github.com/repos/kubernetes/autoscaler/issues/108,cluster-autoscaler-status ConfigMap should support other namespaces besides kube-system,"CA can run in multiple namespaces, and supports the `namespace` command line argument.  This should propagate to the `cluster-autoscaler-status` ConfigMap so that it exists in the same namespace as the CA.  Currently it is a constant, `kube-system`, in `clusterstate/utils/status.go`.",closed,False,2017-06-07 16:57:29,2017-06-14 19:53:25
autoscaler,tomfotherby,https://github.com/kubernetes/autoscaler/issues/109,https://api.github.com/repos/kubernetes/autoscaler/issues/109,Node deleted but still streaming DeletingNode events,"I installed a Kubernetes cluster on AWS and CoreOS hosts with [Tack](https://github.com/kz8s/tack) and the cluster-autoscaler is included as a add-on. This is the yaml they use: https://github.com/kz8s/tack/blob/master/addons/autoscaler/cluster-autoscaler.yml (uses v0.5.2)

After a bit of time with a successful but empty cluster, the autoscaler kicked in and killed 1 or the 3 workers.

The node is no longer shown when doing `kubeclt get nodes`.

The problem is, the worker node is stuck as `DeletingNode` which can be seen from thousands of events along the lines of:

> Deleting Node ip-10-56-0-138.ec2.internal because it's not present according to cloud provider

Example:
```
$ kubectl get events
LASTSEEN   FIRSTSEEN   COUNT     NAME                          KIND      SUBOBJECT   TYPE      REASON         SOURCE              MESSAGE
3s         6h          4780      ip-10-56-0-138.ec2.internal   Node                  Normal    DeletingNode   controllermanager   Node ip-10-56-0-138.ec2.internal event: Deleting Node ip-10-56-0-138.ec2.internal because it's not present according to cloud provider
```
(note: count: 4780!)

Checking the configmap that the autoscaler creates shows the worker node that was removed is still somehow registered. i.e.

>       Nodes: Healthy (ready=5 unready=0 notStarted=0 longNotStarted=0 registered=6)

Is there a problem with the autoscaler? Is it supposed to unregister the node or is this normal?

Is there a way I can get more info about why `DeletingNode` event is appearing so often. There must be a reason for the node not able to be fully deleted. At one point, a stateful set put a pv and pvc on the worker that was deleted - I'm not sure if this could cause a issue with it being unregistered. The pv and pvc were manually removed with no luck curbing the continuing DeletingNode event stream.

Sorry if this issue is not appropriate. Feel free to remove if this is the case. ( It's hard to tell if it could be a bug with the autoscaler or just my use-case.)

----

The config map in full:
```
$ kubectl get configmap cluster-autoscaler-status -n kube-system -o yaml
apiVersion: v1
data:
  status: |+
    Cluster-autoscaler status at 2017-06-08 17:30:00.417692456 +0000 UTC:
    Cluster-wide:
      Health:      Healthy (ready=5 unready=0 notStarted=0 longNotStarted=0 registered=6)
                   LastProbeTime:      2017-06-08 17:29:59.812893761 +0000 UTC
                   LastTransitionTime: 2017-06-08 10:26:35.872670968 +0000 UTC
      ScaleUp:     NoActivity (ready=5 registered=6)
                   LastProbeTime:      2017-06-08 17:29:59.812893761 +0000 UTC
                   LastTransitionTime: 2017-06-08 10:26:35.872670968 +0000 UTC
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2017-06-08 17:30:00.119227722 +0000 UTC
                   LastTransitionTime: 2017-06-08 10:46:54.809754422 +0000 UTC

    NodeGroups:
      Name:        worker-general-test
      Health:      Healthy (ready=2 unready=0 notStarted=0 longNotStarted=0 registered=2 cloudProviderTarget=2 (minSize=1, maxSize=5))
                   LastProbeTime:      2017-06-08 17:29:59.812893761 +0000 UTC
                   LastTransitionTime: 2017-06-08 10:26:35.872670968 +0000 UTC
      ScaleUp:     NoActivity (ready=2 cloudProviderTarget=2)
                   LastProbeTime:      2017-06-08 17:29:59.812893761 +0000 UTC
                   LastTransitionTime: 2017-06-08 10:26:35.872670968 +0000 UTC
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2017-06-08 17:30:00.119227722 +0000 UTC
                   LastTransitionTime: 2017-06-08 10:46:54.809754422 +0000 UTC

kind: ConfigMap
metadata:
  annotations:
    cluster-autoscaler.kubernetes.io/last-updated: 2017-06-08 17:30:00.417692456 +0000
      UTC
  creationTimestamp: 2017-06-08T10:26:25Z
  name: cluster-autoscaler-status
  namespace: kube-system
  resourceVersion: ""60900""
  selfLink: /api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status
  uid: ed1780d0-4c34-11e7-bb12-0afa88f15a64
```",closed,False,2017-06-08 17:47:27,2017-06-09 08:36:35
autoscaler,chapati23,https://github.com/kubernetes/autoscaler/issues/110,https://api.github.com/repos/kubernetes/autoscaler/issues/110,What to apply AWS Policy to?,"Excuse the stupid question please but I'm confused about the README a bit.
What do I need to apply the suggested IAM policy to?

I've set this up with helm and it's not working. No error messages but nodes are not scaling up when running against capacity limit and i suspect the policy might be the culprit:

```sh
echo ""Installing helm chart 'aws-cluster-autoscaler'…""
envsubst < autoscaling.template.yml > autoscaling.yml
helm install stable/aws-cluster-autoscaler -f autoscaling.yml
rm autoscaling.yml
```

With this `autoscaling.template.yml`: 

```yml
autoscalingGroups:
  - name: ${AUTO_SCALING_GROUP_NAME}
    maxSize: 12
    minSize: 4

awsRegion: ${AWS_REGION}

image:
  repository: gcr.io/google_containers/cluster-autoscaler
  tag: v0.5.4
  pullPolicy: IfNotPresent
```

For further debugging purposes, here's `kubectl describe pod aws-cluster-autoscaler`:

```
Name:		loping-armadillo-aws-cluster-autoscaler-3021239351-mwxgp
Namespace:	default
Node:		xxx.eu-central-1.compute.internal/xxx.xxx.xxx.xxx
Start Time:	Wed, 07 Jun 2017 19:38:22 +0200
Labels:		app=aws-cluster-autoscaler
		pod-template-hash=3021239351
		release=loping-armadillo
Annotations:	kubernetes.io/created-by={""kind"":""SerializedReference"",""apiVersion"":""v1"",""reference"":{""kind"":""ReplicaSet"",""namespace"":""default"",""name"":""loping-armadillo-aws-cluster-autoscaler-3021239351"",""uid"":""19ea6...
		kubernetes.io/limit-ranger=LimitRanger plugin set: cpu request for container aws-cluster-autoscaler
Status:		Running
IP:		xxx.xxx.xxx.xxx
Controllers:	ReplicaSet/loping-armadillo-aws-cluster-autoscaler-3021239351
Containers:
  aws-cluster-autoscaler:
    Container ID:	docker://8449a1d6fbc09a2e52d71f2cc67b520720125743f2f0384887b94cafddb6a44f
    Image:		gcr.io/google_containers/cluster-autoscaler:v0.5.4
    Image ID:		docker-pullable://gcr.io/google_containers/cluster-autoscaler@sha256:abe1ed1410c6ea58a80afec69e2b4397740cfa4ffc02484eb0cfbe96d3e81984
    Port:		8085/TCP
    Command:
      ./cluster-autoscaler
      --cloud-provider=aws
      --nodes=4:12:nodes.my-domain.com
      --scale-down-delay=10m
      --skip-nodes-with-local-storage=false
      --skip-nodes-with-system-pods=true
      --v=4
    State:		Running
      Started:		Wed, 07 Jun 2017 19:38:22 +0200
    Ready:		True
    Restart Count:	0
    Requests:
      cpu:	100m
    Environment:
      AWS_REGION:	eu-central-1
    Mounts:
      /etc/ssl/certs/ca-certificates.crt from ssl-certs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jdpp7 (ro)
Conditions:
  Type		Status
  Initialized 	True
  Ready 	True
  PodScheduled 	True
Volumes:
  ssl-certs:
    Type:	HostPath (bare host directory volume)
    Path:	/etc/ssl/certs/ca-certificates.crt
  default-token-jdpp7:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-jdpp7
    Optional:	false
QoS Class:	Burstable
Node-Selectors:	<none>
Tolerations:	node.alpha.kubernetes.io/notReady=:Exists:NoExecute for 300s
		node.alpha.kubernetes.io/unreachable=:Exists:NoExecute for 300s
Events:		<none>
```
",closed,False,2017-06-09 13:15:01,2017-06-17 16:22:38
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/111,https://api.github.com/repos/kubernetes/autoscaler/issues/111,Move Autoscaler Builder to a new file,,closed,True,2017-06-09 17:06:04,2017-06-09 17:18:58
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/112,https://api.github.com/repos/kubernetes/autoscaler/issues/112,Enable pricing in expander factory,ref: #82,closed,True,2017-06-09 18:12:00,2017-06-12 21:51:07
autoscaler,pluttrell,https://github.com/kubernetes/autoscaler/issues/113,https://api.github.com/repos/kubernetes/autoscaler/issues/113,AWS Cluster Autoscaler Permissions,"Using v0.5.4 of the aws-cluster-autoscaler, we're getting this error:

```
E0609 23:20:59.162974       1 static_autoscaler.go:108] Failed to update node registry: Unable to get first autoscaling.Group for node-us-west-2a.dev.clusters.mydomain.io
```

It sure looks like a permission problem... But per the instructions, I have the following policy on my instance role named `nodes.dev.clusters.mydomain.io`:

```
{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""autoscaling:DescribeAutoScalingGroups"",
                ""autoscaling:DescribeAutoScalingInstances"",
                ""autoscaling:SetDesiredCapacity"",
                ""autoscaling:TerminateInstanceInAutoScalingGroup""
            ],
            ""Resource"": ""*""
        }
    ]
}
```

Without this addition, I get a different error:
```
E0609 23:05:48.475214       1 static_autoscaler.go:108] Failed to update node registry: AccessDenied: User: arn:aws:sts::11111111111:assumed-role/nodes.dev.clusters.mydomain.io/i-0472257b3f8d4ec43 is not authorized to perform: autoscaling:DescribeAutoScalingGroups
	status code: 403, request id: 2cf17af0-4d68-11e7-825c-73c99354b20d
```
So we're thinking that we have the necessary permissions. 

For reference here's our execution config:
```
./cluster-autoscaler
--cloud-provider=aws
--nodes=1:10:node-us-west-2a.dev.clusters.mydomain.io
--nodes=1:10:node-us-west-2b.dev.clusters.mydomain.io
--nodes=1:10:node-us-west-2c.dev.clusters.mydomain.io
--scale-down-delay=10m
--skip-nodes-with-local-storage=false
--skip-nodes-with-system-pods=true
--v=4
```

Any ideas on what to do?
Is there any strategy for debugging this?",closed,False,2017-06-09 23:24:05,2018-12-31 17:49:27
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/114,https://api.github.com/repos/kubernetes/autoscaler/issues/114,Bump cluster autoscaler version to 0.6-alpha1,cc: @MaciekPytel @aleksandra-malinowska ,closed,True,2017-06-12 20:16:20,2017-06-13 09:15:44
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/115,https://api.github.com/repos/kubernetes/autoscaler/issues/115,Update GODEPS,"Will require another update once
* k8s.io/apimachinery
* k8s.io/apiserver
* k8s.io/go-client
* k8s.io/metrics

are synced with k8s.io/kubernetes/staging.
Until then godep restore will not work (will require manual handling/symlinking of the mentioned packages to the staging version).

",closed,True,2017-06-13 12:51:48,2017-06-13 13:31:40
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/116,https://api.github.com/repos/kubernetes/autoscaler/issues/116,Update godeps in vpa,"Will require another update once
* k8s.io/apimachinery
* k8s.io/apiserver
* k8s.io/go-client
* k8s.io/metrics

are synced with k8s.io/kubernetes/staging.
Until then godep restore will not work (will require manual handling/symlinking of the mentioned packages to the staging version).

cc: @MaciekPytel @bskiba @kgrygiel ",closed,True,2017-06-13 13:32:52,2017-06-13 13:41:37
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/117,https://api.github.com/repos/kubernetes/autoscaler/issues/117,Fix NPE in kubernetes config,The problems started after godeps update - we don't use groupversion field anymore.,closed,True,2017-06-14 06:32:44,2017-06-14 06:48:57
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/118,https://api.github.com/repos/kubernetes/autoscaler/issues/118,Bump CA version to 0.6.0-alpha2,,closed,True,2017-06-14 06:51:41,2017-06-14 06:59:31
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/119,https://api.github.com/repos/kubernetes/autoscaler/issues/119,Status configmap fixes,"Ref: #106 #108 

This fixes the following issues:
- No longer discards original error when failing to read/write status configmap
- No longer panics if a configmap already exists, but has no annotations on it
- Write status configmap in namespace specified by --namespace flag",closed,True,2017-06-14 11:19:49,2017-06-14 12:18:46
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/120,https://api.github.com/repos/kubernetes/autoscaler/issues/120,Fix typos related to max-graceful-termination-sec,Ref: #39 ,closed,True,2017-06-14 12:16:10,2017-06-14 12:42:36
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/121,https://api.github.com/repos/kubernetes/autoscaler/issues/121,Merge addon resizer into kubernetes/autoscaler,"We want to gather all autoscalers inside kubernetes/autoscaler. 

cc: @Q-Lee @piosz @wojtek-t ",closed,True,2017-06-16 06:46:51,2017-06-16 08:22:29
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/122,https://api.github.com/repos/kubernetes/autoscaler/issues/122,Divide CA's FAQ TOC to sections,"As the CA FAQ is growing it was getting hard to navigate. This
will make FAQ table of contents reflect different sections to make
it easier to navigate.",closed,True,2017-06-16 10:32:37,2017-06-16 12:02:48
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/123,https://api.github.com/repos/kubernetes/autoscaler/issues/123,Update the main README.md for Addon Resizer,,closed,True,2017-06-16 12:53:02,2017-06-16 13:09:26
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/124,https://api.github.com/repos/kubernetes/autoscaler/issues/124,CA 0.6 FAQ update,Added questions about new features coming in 0.6: metrics / livenessProbe and balancing node groups.,closed,True,2017-06-16 13:06:36,2017-06-19 08:44:04
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/125,https://api.github.com/repos/kubernetes/autoscaler/issues/125,Update docs regarding system pods eviction,Add info about draining nodes with system pods.,closed,True,2017-06-19 11:22:21,2017-06-22 11:46:16
autoscaler,johanneswuerbach,https://github.com/kubernetes/autoscaler/pull/126,https://api.github.com/repos/kubernetes/autoscaler/issues/126,AWS: Fixed link to the detailed explanation,https://github.com/kubernetes/contrib/pull/1552#r75532949 seems to explain the reasoning behind multiple ASGs much better then the previous link target.,closed,True,2017-06-19 12:34:17,2017-06-20 14:18:18
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/127,https://api.github.com/repos/kubernetes/autoscaler/issues/127,Don't calculate utilization for nodes that don't belong to autoscaled node groups.,Currently we do some initial scanning for them wasting cpu cycles.,closed,False,2017-06-20 02:19:47,2017-06-20 15:18:16
autoscaler,x13n,https://github.com/kubernetes/autoscaler/pull/128,https://api.github.com/repos/kubernetes/autoscaler/issues/128,Rewrite addon-resizer nanny,"There will be a single estimator class, providing two ranges: acceptable
and recommended range. As long as current pod requirements and limits fall into
acceptable range, nothing happens. Once either limits or requirements
fall out of acceptable range, they are both updated to lower (when
upscaling) or higher (when downscaling) end of recommended range. This
approach prevents flapping, which took place in previous implementation,
when cluster size oscillated around certain values.

More details: https://docs.google.com/a/google.com/document/d/1T0A7GHwNU_w6gpq_eCN166aRth6usbRvxgJJqSZeESU/edit?usp=sharing (shared with kubernetes-sig-instrumentation and kubernetes-sig-autoscaling)

Also, fix the code to actually use poll-period flag.

(PR moved from https://github.com/kubernetes/contrib/pull/2623)",closed,True,2017-06-20 09:41:18,2017-06-21 21:20:16
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/129,https://api.github.com/repos/kubernetes/autoscaler/issues/129,Don't calculate utilisation and run scale down simulations for unmanaged nodes,Fixes: #127 ,closed,True,2017-06-20 14:41:41,2017-06-20 15:18:17
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/130,https://api.github.com/repos/kubernetes/autoscaler/issues/130,Move docker builder image to the top of the repository,So that addon-resizer can use it for builds.,closed,True,2017-06-20 15:29:44,2017-06-20 16:41:19
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/131,https://api.github.com/repos/kubernetes/autoscaler/issues/131,Provide PricingModel for AWS,"Follow up of #82 

cc: @mumoshu ",closed,False,2017-06-20 16:51:54,2018-06-21 07:42:40
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/132,https://api.github.com/repos/kubernetes/autoscaler/issues/132,Allow scaling to/from 0 on AWS,"Follow up of #43 on AWS.

cc: @mumoshu @7chenko ",closed,False,2017-06-20 16:53:42,2017-07-11 11:39:30
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/133,https://api.github.com/repos/kubernetes/autoscaler/issues/133,Review Azure cloud provider,Mirror issue in kubernetes/kubernetes : https://github.com/kubernetes/kubernetes/issues/47511,closed,False,2017-06-20 17:06:43,2017-11-22 14:28:57
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/134,https://api.github.com/repos/kubernetes/autoscaler/issues/134,Don't skip node groups that help only besteffort pods in price expander,,closed,True,2017-06-21 13:55:39,2017-06-21 14:16:56
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/135,https://api.github.com/repos/kubernetes/autoscaler/issues/135,Set version to 0.6.0-beta1,,closed,True,2017-06-21 14:36:43,2017-06-21 14:48:01
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/136,https://api.github.com/repos/kubernetes/autoscaler/issues/136,Fix comments in simulator/drain.go,"cc: @MaciekPytel 
Fixes: https://github.com/kubernetes/contrib/issues/1998",closed,True,2017-06-22 01:01:55,2017-06-22 08:19:37
autoscaler,mumoshu,https://github.com/kubernetes/autoscaler/pull/137,https://api.github.com/repos/kubernetes/autoscaler/issues/137,cluster-autoscaler: Fix node group auto discovery for AWS not to mix up ASGs from different k8s clusters,"This is the fix for the issue that CA become unable to scale down/up the cluster when auto-discovery is enabled and there are two or more k8s clusters with ASGs to which the same tag for auto-discovery is associated.
We should use [the kubernetes cluster tag `kubernetes.io/cluster/<YOUR CLUSTER NAME>`](https://github.com/kubernetes/kubernetes/blob/9ef85a7/pkg/cloudprovider/providers/aws/tags.go#L30-L34) to differentiate ASGs from different k8s clusters.

Resolves an issue other than the one addressed in #107
Depends on #107

Please see the last commit in this PR to see what has been exactly changed since #107.

@mwielgus @MaciekPytel @andrewsykim Could you review this?",closed,True,2017-06-22 07:06:56,2017-06-27 04:38:39
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/138,https://api.github.com/repos/kubernetes/autoscaler/issues/138,Fix removing dash in section title,This fixes generating broken links for sections with dashes in title,closed,True,2017-06-22 11:27:17,2017-06-22 11:47:07
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/139,https://api.github.com/repos/kubernetes/autoscaler/issues/139,Change function duration metric to histogram,"Many functions take an order of magnitude more time
if they actually decide to take an action (like deleting
node in scale-down) and it's ok if executing action is
slow. That makes summary less useful, as we expect to
have large outliers on some percentile, depending on
churn in cluster. Instead having a histogram gives
us the fuller picture of how the distribution of
function runtimes look like.",closed,True,2017-06-23 10:10:20,2017-06-23 11:27:43
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/140,https://api.github.com/repos/kubernetes/autoscaler/issues/140,Set version to 0.6.0-beta2,,closed,True,2017-06-23 10:31:05,2017-06-23 11:06:53
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/141,https://api.github.com/repos/kubernetes/autoscaler/issues/141,Set CA version to 0.6.0,,closed,True,2017-06-26 15:29:48,2017-06-26 16:55:43
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/142,https://api.github.com/repos/kubernetes/autoscaler/issues/142,Release notes for CA 0.6,cc: @MaciekPytel @aleksandra-malinowska @mumoshu ,closed,True,2017-06-27 10:05:04,2017-06-27 10:31:10
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/143,https://api.github.com/repos/kubernetes/autoscaler/issues/143,"VPA Initializer, initial version.","VPA admission plugin equivalent implemented as an Initializer.
Changes declared requests of containers when applicable.

Moved mocks and test utils out of updater to reuse.

cc: @mwielgus @kgrygiel  ",closed,True,2017-06-27 17:23:02,2017-10-10 16:01:27
autoscaler,sharpevo,https://github.com/kubernetes/autoscaler/issues/144,https://api.github.com/repos/kubernetes/autoscaler/issues/144,Scale down bug on AWS,"There're 3 nodes in an Autoscaling Group of AWS:
- Node-A, utilization: 0.3, kube-system pods present, ""Protect From Scale In"".
- Node-B, utilization: 0.0, may be removed
- Node-C, utilization: 0.8

Cluster Autoscaler reports that the `Node-B` will be removed in a given `scale-down-delay`, but AWS removes `Node-A` due to the desired capacity shrinking request by user, rather than the `Node-B`.

```
At 2017-06-28T06:06:36Z instance i-0b9d was taken out of service 
in response to a user request, shrinking the capacity from 3 to 2.
```
`Node-A` has pods in namespace `kube-system`, so I guess the termination should be triggered by AWS , before the Cluster Autoscaler takes any actions. Does Cluster Autoscaler send requests to change desired capacity first? But there's already `ShouldDecrementDesiredCapacity` in [aws_manager.go](https://github.com/kubernetes/autoscaler/blob/28caf01651b517d3c0cd47de2bd5daed7e75e7ad/cluster-autoscaler/cloudprovider/aws/aws_manager.go#L141)

kubernetes: v1.6.6
cluster-autoscaler: v0.5.4
autoscaling group:
```
{
    ""AutoScalingGroups"": [
        {
            ""AutoScalingGroupName"": ""kube-test-asg"",
            ""AutoScalingGroupARN"": ""arn"",
            ""LaunchConfigurationName"": ""kube-test"",
            ""MinSize"": 1,
            ""MaxSize"": 5,
            ""DesiredCapacity"": 2,
            ""DefaultCooldown"": 300,
            ""LoadBalancerNames"": [],
            ""TargetGroupARNs"": [],
            ""HealthCheckType"": ""EC2"",
            ""HealthCheckGracePeriod"": 300,
            ""Instances"": [
                {
                    ""InstanceId"": ""i-0309"",
                    ""LifecycleState"": ""InService"",
                    ""HealthStatus"": ""Healthy"",
                    ""LaunchConfigurationName"": ""kube-test"",
                    ""ProtectedFromScaleIn"": false
                },
                {
                    ""InstanceId"": ""i-0af4"",
                    ""LifecycleState"": ""InService"",
                    ""HealthStatus"": ""Healthy"",
                    ""LaunchConfigurationName"": ""kube-test"",
                    ""ProtectedFromScaleIn"": false
                }
            ],
            ""CreatedTime"": ""2017-06-27T01:02:31.582Z"",
            ""SuspendedProcesses"": [],
            ""VPCZoneIdentifier"": ""subnet"",
            ""EnabledMetrics"": [],
            ""Tags"": [],
            ""TerminationPolicies"": [
                ""Default""
            ],
            ""NewInstancesProtectedFromScaleIn"": false
        }
    ]
}
```",closed,False,2017-06-28 06:39:11,2018-01-31 01:16:11
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/145,https://api.github.com/repos/kubernetes/autoscaler/issues/145,Fix comment in cloud_provider.go,,closed,True,2017-06-29 08:26:12,2017-06-29 08:39:18
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/146,https://api.github.com/repos/kubernetes/autoscaler/issues/146,Move expanders documentation to faq,,closed,True,2017-06-29 11:27:21,2017-06-29 11:47:59
autoscaler,vendrov,https://github.com/kubernetes/autoscaler/issues/147,https://api.github.com/repos/kubernetes/autoscaler/issues/147,Allow pod's graceful termination to override CA GracefulTermination ,"In a scenario when a PreStop duration is longer than 1 min (CA current default GracefulTermination time),
the pod will be killed in the middle of a ""graceful shutdown"". It might harm the user experience.
For example let's say that we would like to drain web-sockets connection and we decide that 20 min is enough time for the user to finish his task/action, we expect that setting the pod's graceful termination time to 20 will be enough, but currently it will just stop the socket right after 1 minute.",closed,False,2017-06-29 21:37:59,2017-09-02 19:32:32
autoscaler,yuvipanda,https://github.com/kubernetes/autoscaler/issues/148,https://api.github.com/repos/kubernetes/autoscaler/issues/148,Allow setting 'minimum headroom' for autoscaling,"I want to be able to say 'if the cluster is more than X% full, scale up until it is not'. This is important in super dynamic clusters that are very spiky - we run a Kubernetes cluster for a University, and a large spike of pods start up when classes start. If we waited for them to fail Scheduling before adding more nodes, this provides them with a suboptimal experience (since it might take several minutes for a new node to spin up).

One problem would be defining what 'full' is, in a way that doesn't duplicate what's in the scheduler. ",closed,False,2017-06-29 22:46:10,2018-06-14 08:09:56
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/149,https://api.github.com/repos/kubernetes/autoscaler/issues/149,Update CA FAQ for PDB,,closed,True,2017-06-30 11:51:23,2017-06-30 12:04:22
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/150,https://api.github.com/repos/kubernetes/autoscaler/issues/150,FAQ-ReleaseNotes cross links,cc: @aleksandra-malinowska @MaciekPytel ,closed,True,2017-06-30 12:10:09,2017-06-30 12:14:02
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/151,https://api.github.com/repos/kubernetes/autoscaler/issues/151,Fix update_toc.py script to stop appending empty lines,,closed,True,2017-06-30 12:19:27,2017-06-30 12:20:10
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/152,https://api.github.com/repos/kubernetes/autoscaler/issues/152,Move FAQ up in README for better discoverability,,closed,True,2017-06-30 12:37:35,2017-06-30 12:55:36
autoscaler,sorenmat,https://github.com/kubernetes/autoscaler/issues/153,https://api.github.com/repos/kubernetes/autoscaler/issues/153,Cluster autoscaler fails to start,"After removing the the pod I get the following error.
```
I0630 12:37:26.683009       1 leaderelection.go:248] lock is held by cluster-autoscaler-37505376-64n1p and has not yet expired
I0630 12:37:26.683032       1 leaderelection.go:185] failed to acquire lease kube-system/cluster-autoscaler
```
Tried to remove everything related to CA from the cluster, but it seems to be keep failing",closed,False,2017-06-30 12:55:24,2017-07-01 10:46:44
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/154,https://api.github.com/repos/kubernetes/autoscaler/issues/154,Kubemark integration proposal.,"Proposal for running Cluster Autoscaler on [kubemark](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/kubemark.md). This will be useful in scalability tests of Cluster Autoscaler.

@mwielgus 
@MaciekPytel
@aleksandra-malinowska  ",closed,True,2017-06-30 14:00:36,2017-09-11 11:32:28
autoscaler,xiang90,https://github.com/kubernetes/autoscaler/pull/155,https://api.github.com/repos/kubernetes/autoscaler/issues/155,updater: cleanup updater.go,"including:

- import cleanup
- logging cleanup
- minor code cleanup",closed,True,2017-06-30 19:52:08,2017-06-30 21:23:00
autoscaler,xiang90,https://github.com/kubernetes/autoscaler/pull/156,https://api.github.com/repos/kubernetes/autoscaler/issues/156,recommender: cleanup,,closed,True,2017-06-30 20:06:51,2017-06-30 21:23:55
autoscaler,xiang90,https://github.com/kubernetes/autoscaler/pull/157,https://api.github.com/repos/kubernetes/autoscaler/issues/157,priority: cleanup,"Some minor fixes while I was reading the code.

Probably we should kill all long local variable names to follow go style.",closed,True,2017-06-30 21:59:41,2017-08-07 22:27:01
autoscaler,xiang90,https://github.com/kubernetes/autoscaler/pull/158,https://api.github.com/repos/kubernetes/autoscaler/issues/158,eviction: cleanup,,closed,True,2017-06-30 22:59:40,2017-07-04 09:42:20
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/159,https://api.github.com/repos/kubernetes/autoscaler/issues/159,[work in progress] Recommender metrics client,"Do not merge. It will not compile, due to godep issues.

Feedback especially welcomed on:
- `ContainerUtilizationSnapshot` structure
- Tests and creating mocks

The code is a Metrics client and a simple recomender, which constructs the client, uses it once and print out a result.

cc: @mwielgus @kgrygiel ",closed,True,2017-07-02 20:39:49,2017-08-30 20:03:21
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/160,https://api.github.com/repos/kubernetes/autoscaler/issues/160,Strip rescheduler taint from node templates,"This is a temporary taint added by rescheduler, we shouldn't assume it will be present on all nodes in node group.",closed,True,2017-07-03 12:48:55,2017-07-04 08:53:00
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/161,https://api.github.com/repos/kubernetes/autoscaler/issues/161,Godeps bump for CA,,closed,True,2017-07-03 19:12:13,2017-07-04 09:41:01
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/162,https://api.github.com/repos/kubernetes/autoscaler/issues/162,Update VPA godeps,,closed,True,2017-07-03 19:12:58,2017-07-04 09:41:42
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/163,https://api.github.com/repos/kubernetes/autoscaler/issues/163,Strip rescheduler taint from node templates,Cherry-pick of #160 on 0.6 branch,closed,True,2017-07-04 08:59:35,2017-07-04 09:43:00
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/164,https://api.github.com/repos/kubernetes/autoscaler/issues/164,"Support ""opt-out of autoscaling"" annotation on nodes","We had some user requests to allow node to ""opt-out"" of being scaled down. We could support that by not scaling down nodes that have a specific annotation. This has an additional benefit of making e2e test behavior more predictable by preventing unexpected scale-down during test setup, etc.

WDYT @mwielgus?",closed,False,2017-07-04 13:01:25,2017-07-06 13:45:18
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/165,https://api.github.com/repos/kubernetes/autoscaler/issues/165,Add note about running CA 0.6 with k8s 1.6,,closed,True,2017-07-04 15:33:30,2017-07-04 15:51:06
autoscaler,sethpollack,https://github.com/kubernetes/autoscaler/pull/166,https://api.github.com/repos/kubernetes/autoscaler/issues/166,Enable min size 0 in AWS,fixes #132 part of #43,closed,True,2017-07-04 23:05:44,2017-07-13 11:30:07
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/167,https://api.github.com/repos/kubernetes/autoscaler/issues/167,Add respecting scale down disabled annotation,"This addresses #164. Tested manually that CA doesn't touch nodes with this annotation. Removing it or setting a different value allows CA to resume scaling down the node.

PR with FAQ update coming.
",closed,True,2017-07-05 10:57:00,2017-07-06 13:44:28
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/168,https://api.github.com/repos/kubernetes/autoscaler/issues/168,Add taints to GCE node template,"https://github.com/kubernetes/kubernetes/pull/47632 have added support for automatically adding taints on newly created nodes. We should include that in template node used to scale-from-0, similar to how we handle labels https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/gce/gce_manager.go#L329.",closed,False,2017-07-05 14:39:08,2017-07-17 12:54:44
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/169,https://api.github.com/repos/kubernetes/autoscaler/issues/169,Rename testprovider package,Renaming to maintain consistent package and directory names,closed,True,2017-07-06 14:26:42,2017-07-13 10:20:31
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/170,https://api.github.com/repos/kubernetes/autoscaler/issues/170,Change scope of findUnneeded metric,"Excluding API call from ""findUnneeded"" duration metric (intended to measure CA performance.)",closed,True,2017-07-07 14:35:36,2017-07-07 14:47:33
autoscaler,davidopp,https://github.com/kubernetes/autoscaler/issues/171,https://api.github.com/repos/kubernetes/autoscaler/issues/171,Think about how to handle dedicated node groups,"The way we implement dedicated node groups in Kubernetes is 
1) attach a taint to one or more nodes (like `dedicated=foo`); pods that are *allowed* to use those nodes tolerate that taint
2) attach a label to the same set of nodes (like `dedicated=foo`; the key/value space of taints is separate from the key/value space of labels); pods that *must* use those nodes have node affinity or nodeSelector for that label (in addition to having the toleration from (1))

Say we have a pending pod that is supposed to run on a dedicated node. The node created for this pod must have the label (part (2) above). But it does not need to have the taint (part (1) above) since a pod with a toleration will just as well schedule onto a node with or without the corresponding taint. When CA sees such a pending pod, it should presumably create a node with both the label and taint, not just the label, but the logic today probably does not do this.
",closed,False,2017-07-08 18:39:41,2017-07-31 15:42:27
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/172,https://api.github.com/repos/kubernetes/autoscaler/issues/172,add taints to GCE node template,"This addresses #168.

Tested manually in the following steps:
1. Deploy new image of CA
2. Create an empty node group with NODE_TAINTS set in kube-env
3. Taint any other nodes in the cluster
4. Create pod with tolerations matching taints of empty node group, but not those of other nodes
5. Verify CA increased this node group",closed,True,2017-07-10 15:39:57,2017-07-13 08:35:03
autoscaler,fate-grand-order,https://github.com/kubernetes/autoscaler/pull/173,https://api.github.com/repos/kubernetes/autoscaler/issues/173,correct some misspells for cluster-autoscaler/core,,closed,True,2017-07-13 09:56:19,2017-07-13 10:14:22
autoscaler,ryanwalls,https://github.com/kubernetes/autoscaler/pull/174,https://api.github.com/repos/kubernetes/autoscaler/issues/174,Update readme with permissions for scaling node groups from 0,The code used to scale node groups from 0 requires the `DescribeLaunchConfigurations` permission.,closed,True,2017-07-13 22:20:29,2017-07-17 09:00:23
autoscaler,qqshfox,https://github.com/kubernetes/autoscaler/pull/175,https://api.github.com/repos/kubernetes/autoscaler/issues/175,fix typo for logging,,closed,True,2017-07-14 05:15:19,2017-07-14 09:41:46
autoscaler,lwolf,https://github.com/kubernetes/autoscaler/issues/176,https://api.github.com/repos/kubernetes/autoscaler/issues/176,autodiscovery with several tags does not work,"I'm trying to setup autoscaler for 2 clusters in a single aws zone like described [here](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws#auto-discovery-setup):
having any single tag out of two works fine (`k8s.io/cluster-autoscaler/enabled` or `kubernetes.io/cluster/production`)
but as soon as I set 2 tags container starts to crush
```
- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/production
```

```
I0714 12:48:42.177681       1 status.go:122] Succesfully wrote status configmap with body ""Cluster-autoscaler status at 2017-07-14 12:48:41.487387224 +0000 UTC:
Initializing""
I0714 12:48:42.177952       1 auto_scaling.go:96] Starting getAutoscalingGroupsByTag with key=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/production
F0714 12:48:42.397512       1 cloud_provider_builder.go:91] Failed to create AWS cloud provider: Failed to get ASGs: Unable to find ASGs for tag key k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/production
goroutine 54 [running]:
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.stacks(0xc4201c8400, 0xc42086a480, 0xd8, 0x101)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:766 +0xa7
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.(*loggingT).output(0x2d8c7e0, 0xc400000003, 0xc4206a6370, 0x2cda4be, 0x19, 0x5b, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:717 +0x348
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.(*loggingT).printf(0x2d8c7e0, 0x3, 0x1f0319e, 0x27, 0xc4205bd220, 0x1, 0x1)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:655 +0x14f
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.Fatalf(0x1f0319e, 0x27, 0xc4205bd220, 0x1, 0x1)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:1145 +0x67
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/builder.CloudProviderBuilder.Build(0x7fff3c1589d0, 0x3, 0x0, 0x0, 0x0, 0x0, 0x0, 0x7fff3c158a2d, 0x4a, 0x0, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/builder/cloud_provider_builder.go:91 +0x68d
k8s.io/autoscaler/cluster-autoscaler/core.NewAutoscalingContext(0xa, 0x3fe0000000000000, 0x8bb2c97000, 0x1176592e000, 0x0, 0x7fff3c158a2d, 0x4a, 0xd18c2e2800, 0x1ed7132, 0xa, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/autoscaling_context.go:115 +0x13d
k8s.io/autoscaler/cluster-autoscaler/core.NewStaticAutoscaler(0xa, 0x3fe0000000000000, 0x8bb2c97000, 0x1176592e000, 0x0, 0x7fff3c158a2d, 0x4a, 0xd18c2e2800, 0x1ed7132, 0xa, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:54 +0x113
k8s.io/autoscaler/cluster-autoscaler/core.(*AutoscalerBuilderImpl).Build(0xc4203366c0, 0x410db8, 0x120)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/dynamic_autoscaler.go:131 +0x10e
k8s.io/autoscaler/cluster-autoscaler/core.NewPollingAutoscaler(0x2d10c80, 0xc4203366c0, 0x1ce0860)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/polling_autoscaler.go:36 +0x35
k8s.io/autoscaler/cluster-autoscaler/core.NewAutoscaler(0xa, 0x3fe0000000000000, 0x8bb2c97000, 0x1176592e000, 0x0, 0x7fff3c158a2d, 0x4a, 0xd18c2e2800, 0x1ed7132, 0xa, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/autoscaler.go:59 +0x5c6
main.run(0xc4202f2320)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:184 +0x253
main.main.func2(0xc4202e6060)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:276 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/leaderelection.(*LeaderElector).Run
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/leaderelection/leaderelection.go:150 +0x97
```

clusters deployed using kube-aws
Cluster Autoscaler version 0.5.4
",closed,False,2017-07-14 13:02:49,2017-07-18 23:33:23
autoscaler,rushtehrani,https://github.com/kubernetes/autoscaler/pull/177,https://api.github.com/repos/kubernetes/autoscaler/issues/177,Add autoscaling:DescribeLaunchConfigurations,"I needed to add the `autoscaling:DescribeLaunchConfigurations` action for scaling from/to 0 nodes for AWS to work.  

This is the error I was getting prior to adding this action:

```
Failed to scale up: failed to build node infos for node groups: AccessDenied: User: arn:aws:sts::[REDACTED]:assumed-role/[REDACTED]/i-09097b6eabc4f689d is not authorized to perform: autoscaling:DescribeLaunchConfigurations
```",closed,True,2017-07-16 22:05:30,2017-07-16 22:21:18
autoscaler,rushtehrani,https://github.com/kubernetes/autoscaler/pull/178,https://api.github.com/repos/kubernetes/autoscaler/issues/178,Add autoscaling:DescribeLaunchConfigurations,"I needed to add the `autoscaling:DescribeLaunchConfigurations` action for scaling from/to 0 nodes for AWS to work.  

This is the error I was getting prior to adding this action:

```
Failed to scale up: failed to build node infos for node groups: AccessDenied: User: arn:aws:sts::[REDACTED]:assumed-role/[REDACTED]/i-09097b6eabc4f689d is not authorized to perform: autoscaling:DescribeLaunchConfigurations
```",closed,True,2017-07-16 22:22:10,2017-07-17 00:04:25
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/179,https://api.github.com/repos/kubernetes/autoscaler/issues/179,Change scope of scaleUp metric,Move collecting data after API call.,closed,True,2017-07-18 10:19:50,2017-07-18 10:58:32
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/180,https://api.github.com/repos/kubernetes/autoscaler/issues/180,fix some logs in scale down,Node name wasn't logged due to typo,closed,True,2017-07-20 08:32:08,2017-07-20 08:56:35
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/181,https://api.github.com/repos/kubernetes/autoscaler/issues/181,Fix hangout link for sig meetings,,closed,True,2017-07-20 16:16:15,2017-07-20 17:59:27
autoscaler,mkumatag,https://github.com/kubernetes/autoscaler/pull/182,https://api.github.com/repos/kubernetes/autoscaler/issues/182,Mutiarch addon-resizer image,This PR is for creating multi architecture image for addon-resizer,closed,True,2017-07-20 17:33:37,2017-08-11 00:00:48
autoscaler,sethpollack,https://github.com/kubernetes/autoscaler/pull/183,https://api.github.com/repos/kubernetes/autoscaler/issues/183,Add taints to AWS node template,"As discussed in #166 part of #43

cc @MaciekPytel @mumoshu  @justinsb",closed,True,2017-07-21 02:59:35,2017-07-21 12:58:32
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/184,https://api.github.com/repos/kubernetes/autoscaler/issues/184,Add godoc link to readme,,closed,True,2017-07-21 09:36:33,2017-07-21 09:58:07
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/185,https://api.github.com/repos/kubernetes/autoscaler/issues/185,Correctly handle zone failures and stock-outs in CA,"Currently we might end up retrying the same zone over and over. After we notice that a particular zone (or node group in general) is not providing the requested nodes in reasonable time we should block it for XX minutes and allow the pending pods to go somewhere else.

cc: @MaciekPytel @aleksandra-malinowska @bskiba ",closed,False,2017-07-24 12:30:50,2017-08-18 14:55:58
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/186,https://api.github.com/repos/kubernetes/autoscaler/issues/186,Add a flag to turn off pod status condition reseting for performance tests,"Temporary change to unblock @aleksandra-malinowska and @bskiba who are running performance tests. Pod condition restets proved to be a big bottleneck. We need more time to provide a good replacement for pod condition reset and within that time we will allow to turn off reseting to find other performance problems.  

cc: @aleksandra-malinowska @bskiba @MaciekPytel ",closed,True,2017-07-24 13:13:59,2017-07-24 14:14:56
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/187,https://api.github.com/repos/kubernetes/autoscaler/issues/187,[CA performance] Pod condition reseting takes too much time.,"With lots of pending pods we try to reset all of their unschedulable pod conditions every time a new node is added. That leads to performance problems (updates are synchronous) and large api server traffic.

cc: @aleksandra-malinowska @bskiba @MaciekPytel ",closed,False,2017-07-24 13:17:02,2017-08-18 15:00:01
autoscaler,djsly,https://github.com/kubernetes/autoscaler/issues/188,https://api.github.com/repos/kubernetes/autoscaler/issues/188,[addon-resizer] deployment update drops existing toleration fields,"Probably due to the version of the k8s go client used by the addon-resizer 

```
#kubectl create -f heapster-controller.yaml
deployment ""heapster"" created
# kubectl get deployment heapster -o yaml | grep tolerations -A6
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoSchedule
        key: role
        operator: Equal
        value: k8s-edge-node
# kubectl get deployment heapster -o yaml | grep tolerations -A6
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoSchedule
        key: role
        operator: Equal
        value: k8s-edge-node
# kubectl get deployment heapster -o yaml | grep tolerations -A6
# kubectl get deployment heapster -o yaml | grep tolerations -A6
```

Go client version
```
       {
            ""ImportPath"": ""k8s.io/kubernetes/pkg/client/clientset_generated/release_1_3"",
            ""Comment"": ""v1.3.0-alpha.5-165-g7476d97"",
            ""Rev"": ""7476d97781563b70e8b89a8bd3f99ea75ae6c290""
        },
```
",closed,False,2017-07-25 19:06:20,2018-08-15 18:13:25
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/189,https://api.github.com/repos/kubernetes/autoscaler/issues/189,Remove PodScheduled filter,"Ref: #187 (includes my long comment explaining this change).

cc: @aleksandra-malinowska @bskiba ",closed,True,2017-07-27 09:29:37,2017-08-16 16:07:39
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/190,https://api.github.com/repos/kubernetes/autoscaler/issues/190,Stop spamming warnings for every 0 size nodegroup,Currently we log 2 warnings per node group with 0 size per loop.,closed,True,2017-07-28 13:31:02,2017-07-31 11:32:57
autoscaler,VioletRainbows,https://github.com/kubernetes/autoscaler/issues/191,https://api.github.com/repos/kubernetes/autoscaler/issues/191,Custom/external cloud provider?,"Hello,

Would you be interested in an external cloud provider? This would allow the creation of new machines with specific requirements.

**Implementation**
https://github.com/VioletRainbows/autoscaler/blob/external/cluster-autoscaler/cloudprovider/external/external_cloud_provider.go
Please note that it is a work in progress.

**Implementation details**
New cloud provider that calls (http) an external server that can add and remove nodes.
autoscaler -> external cloud provider -> [homemade API server able to create machines]

Three endpoints would need to be implemented by the user for a default configuration; for instance:
- nodeGroups - http://127.0.0.1/api/v1/nodes
- IncreaseSize - http://127.0.0.1/api/v1/scaleUp/{size}
- DeleteNodes - http://127.0.0.1/api/v1/scaleDown/{name}

We are currently using it to scale our Azure architecture at my workplace.
",closed,False,2017-07-28 16:06:02,2018-06-23 16:34:20
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/192,https://api.github.com/repos/kubernetes/autoscaler/issues/192,Utility script for testing CA cloudprovider failure handling,"I needed to simulate some scale up failures for testing #185. Since I've already done it multiple times manually I decided to write a script for automatically breaking new nodes as they come up. I think it may be useful for other people testing CA, so upstreaming it. ",closed,True,2017-08-02 12:10:07,2017-08-07 20:24:11
autoscaler,itowlson,https://github.com/kubernetes/autoscaler/pull/193,https://api.github.com/repos/kubernetes/autoscaler/issues/193,Fixed typoes of name 'Kubernetes',Corrected 'kuberentes' -> 'Kubernetes' in some log messages and CLI help message.,closed,True,2017-08-03 02:25:32,2017-08-03 08:46:28
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/194,https://api.github.com/repos/kubernetes/autoscaler/issues/194,Add k8s.io/metrics to vpa godeps,Requested by @KarolKraskiewicz due to some godeps problems.,closed,True,2017-08-04 10:12:38,2017-08-04 10:56:03
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/195,https://api.github.com/repos/kubernetes/autoscaler/issues/195,Change histogram buckets.,,closed,True,2017-08-04 11:56:49,2017-09-11 11:32:25
autoscaler,fisherxu,https://github.com/kubernetes/autoscaler/pull/196,https://api.github.com/repos/kubernetes/autoscaler/issues/196,fix: add . to end of entry in FAQ.md,"I see some entry end up with ""."", but some others don't. So I fix it.",closed,True,2017-08-04 12:25:09,2017-08-05 14:20:23
autoscaler,mashayev,https://github.com/kubernetes/autoscaler/issues/197,https://api.github.com/repos/kubernetes/autoscaler/issues/197,Autoscaler not scale instance when only 1 pod is unschedulable,"Hi All,

K8S version is: `v1.6.1`
Autoscaler version is: `v0.5.4` the same happened in `v0.5.1`
Provider: `AWS`

Not sure it's a bug or maybe configuration problem. Autoscaler not scaling another instance in case only 1 pod is unschedulable I saw it happens when 1 is unschedulable maybe the same goes for more.

I'm able to fix this only if I will create another pod and then the `scale_up.go` will `Estimated 1 nodes needed`

Logs:
```
I0804 13:23:44.597641       1 static_autoscaler.go:130] 1 unregistered nodes present
I0804 13:23:44.597684       1 static_autoscaler.go:197] Filtering out schedulables
I0804 13:23:44.597870       1 static_autoscaler.go:205] No schedulable pods
I0804 13:23:44.597893       1 scale_up.go:44] Pod <namespace>/<pod> is unschedulable
I0804 13:23:44.620660       1 scale_up.go:62] Upcoming 1 nodes
I0804 13:23:44.657738       1 scale_up.go:124] No need for any nodes in k8s-production-asg
I0804 13:23:44.657767       1 scale_up.go:132] No expansion options
I0804 13:23:44.657787       1 static_autoscaler.go:247] Scale down status: unneededOnly=true lastScaleUpTime=2017-08-04 13:17:36.225268892 +0000 UTC lastScaleDownFailedTrail=2017-08-03 18:27:44.967241708 +0000 UTC schedulablePodsPresent=false
```
After creating another pod:
```
0804 13:23:55.090329       1 scale_up.go:62] Upcoming 1 nodes
I0804 13:23:55.127798       1 scale_up.go:145] Best option to resize: k8s-production-asg
I0804 13:23:55.127826       1 scale_up.go:149] Estimated 1 nodes needed in k8s-production-asg
I0804 13:23:55.164701       1 scale_up.go:169] Scale-up: setting group k8s-production-asg size to 8
I0804 13:23:55.203865       1 aws_manager.go:124] Setting asg k8s-production-asg size to 8
```

YAML:
```
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: autoscaler-default-pool
  namespace: kube-system
  labels:
    app: autoscaler-default-pool
    asg-name: k8s-production-asg
spec:
  # replicas not specified on purpose, default 1
  selector:
    matchLabels:
      app: autoscaler-default-pool
  template:
    metadata:
      labels:
        app: autoscaler-default-pool
    spec:
      containers:
        - image: gcr.io/google_containers/cluster-autoscaler:v0.5.4
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --kubernetes=https://elb.amazonaws.com:6443
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=true
            - --nodes=2:50:k8s-production-asg
          env:
            - name: AWS_REGION
              value: us-west-1
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
          imagePullPolicy: Always
      volumes:
        - name: ssl-certs
          hostPath:
            path: /etc/pki/tls/certs/ca-bundle.crt
```",closed,False,2017-08-04 13:54:53,2017-08-07 09:56:42
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/198,https://api.github.com/repos/kubernetes/autoscaler/issues/198,Backoff for node group after failed scale-up,"Ref: #185 

This temporarily removes ('backoffs') node group from scale-up considerations after a failed scale-up. The scale-up request will be considered 'failed' if the cloudprovider returns an error, or if new nodes are not ready after --max-node-provision-time.",closed,True,2017-08-04 14:33:05,2017-08-16 15:16:46
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/199,https://api.github.com/repos/kubernetes/autoscaler/issues/199,Log long function execution,,closed,True,2017-08-07 09:22:34,2017-08-07 09:41:50
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/200,https://api.github.com/repos/kubernetes/autoscaler/issues/200,Cherrypick 0.6.1,Cherry-pick for 0.6.1. Contains godeps update.,closed,True,2017-08-07 15:24:38,2017-08-08 08:42:24
autoscaler,nicksardo,https://github.com/kubernetes/autoscaler/issues/201,https://api.github.com/repos/kubernetes/autoscaler/issues/201,GCE: Allow unknown key-value pairs in /etc/gce.conf,"Hello,

I'm requesting that the vendored GCE cloudprovider be updated so the autoscaler will have https://github.com/kubernetes/kubernetes/pull/49594 which allows config readers of `/etc/gce.conf` to ignore unknown key-value pairs. The latest GCE Ingress Controller (v0.9.6) was updated to have this change and has been cherry picked into K8s 1.7.4. 

Thanks

cc @bowei @freehan @MrHohn",closed,False,2017-08-07 19:10:38,2017-08-18 14:55:24
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/202,https://api.github.com/repos/kubernetes/autoscaler/issues/202,Fix getEmptyNodes function in CA,Previously only one node could be deleted at a time.,closed,True,2017-08-07 20:20:43,2017-08-08 08:46:01
autoscaler,asifdxtreme,https://github.com/kubernetes/autoscaler/pull/203,https://api.github.com/repos/kubernetes/autoscaler/issues/203,Split Verification & Test in seperate stage in Travis,,closed,True,2017-08-08 11:33:25,2017-08-10 10:59:15
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/204,https://api.github.com/repos/kubernetes/autoscaler/issues/204,cluster-autoscaler works improperly with several kubernetes clusters in the same region,"Moved from: https://github.com/kubernetes/contrib/issues/2711
Author: @it-svit

I have two clusters in two different VPCs. 
But single cluster autoscaler is trying to affect both clusters.
```
I0808 15:01:14.854588       1 aws_manager.go:190] Regenerating ASG information for kube1-Nodepool2-1BK23XPH5999G-Workers
I0808 15:01:14.888882       1 aws_manager.go:190] Regenerating ASG information for kube2-Nodepool1-1303RAB6NXCBX-Workers
E0808 15:01:14.925243       1 static_autoscaler.go:219] Failed to scale up: failed to find template node for node group kube2-Nodepool1-1303RAB6NXCBX-Workers
W0808 15:01:14.925281       1 clusterstate.go:237] Failed to find readiness information for kube2-Nodepool1-1303RAB6NXCBX-Workers
W0808 15:01:14.925287       1 clusterstate.go:271] Failed to find readiness information for kube2-Nodepool1-1303RAB6NXCBX-Workers
```",closed,False,2017-08-08 15:35:12,2018-03-10 18:42:34
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/205,https://api.github.com/repos/kubernetes/autoscaler/issues/205,Cherry-pick: Fix getEmptyNodes function in CA,"cherrypick of #202 
",closed,True,2017-08-08 15:57:25,2017-08-08 16:34:02
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/206,https://api.github.com/repos/kubernetes/autoscaler/issues/206,Cluster autoscaler 0.6.1-beta1,,closed,True,2017-08-08 17:56:35,2017-08-08 17:56:47
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/207,https://api.github.com/repos/kubernetes/autoscaler/issues/207,Set version to 0.6.1-beta1 for beta image for K8S 1.7,,closed,True,2017-08-08 18:02:14,2017-08-10 10:32:07
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/208,https://api.github.com/repos/kubernetes/autoscaler/issues/208,Node Auto-provisioning proposal doc,cc: @MaciekPytel @aleksandra-malinowska @bskiba @mumoshu @kubernetes/sig-autoscaling-feature-requests ,closed,True,2017-08-09 12:01:15,2017-08-16 08:09:36
autoscaler,JoelSpeed,https://github.com/kubernetes/autoscaler/issues/209,https://api.github.com/repos/kubernetes/autoscaler/issues/209,AWS AutoDiscovery ASG Limits,"I'm running Kubernetes V1.6.4, CA V0.6.0 with autodiscovery of ASGs on AWS. If I change the maximum or minimum number of nodes in the AWS console, these changes are not reflected in the AutoScaler's config.

I chose to use the autodiscovery as I thought it would enable me to keep a single source of truth for ASG limits.

If I decrease the maximum number of nodes in AWS, CA can try to scale up but will throw an error when aws declines the request.

If I increase the maximum number of nodes in AWS, because the cluster has run out of resource, CA doesn't notice this and won't scale up even though there is capacity to

Given that AWS is constantly being polled for updates to the ASG, shouldn't changes in max/min number of nodes be updated? 

I'm not sure whether this behaviour has been implemented and isn't working or whether this would be classed as a feature request but it definitely seems like the intuitive behaviour of the autodiscovery",closed,False,2017-08-09 12:32:19,2019-03-24 07:42:26
autoscaler,007,https://github.com/kubernetes/autoscaler/pull/210,https://api.github.com/repos/kubernetes/autoscaler/issues/210,bump container image version to 0.6.0,"I don't see any docs as to a release process, but ideally this process should be added to such a thing if it exists.",closed,True,2017-08-09 23:08:49,2017-08-10 10:31:43
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/211,https://api.github.com/repos/kubernetes/autoscaler/issues/211,Update e2e config in FAQ,,closed,True,2017-08-10 13:06:00,2017-08-10 13:25:31
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/212,https://api.github.com/repos/kubernetes/autoscaler/issues/212,Add script for parsing metrics from CA e2e tests,,closed,True,2017-08-10 15:29:50,2017-08-11 09:50:31
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/213,https://api.github.com/repos/kubernetes/autoscaler/issues/213,CA godeps update to K8S head at 2017-10-08,"Another godep update will be need after all client repositories are synced with the main repo.

cc: @MaciekPytel @bskiba ",closed,True,2017-08-11 15:59:29,2017-08-11 16:53:29
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/214,https://api.github.com/repos/kubernetes/autoscaler/issues/214,Bump CA version to 0.7 alpha1,,closed,True,2017-08-11 16:04:15,2017-08-11 16:09:07
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/215,https://api.github.com/repos/kubernetes/autoscaler/issues/215,Re-enable stateful sets test for drain.,,closed,False,2017-08-11 16:07:02,2018-03-09 23:23:33
autoscaler,mbssaiakhil,https://github.com/kubernetes/autoscaler/pull/216,https://api.github.com/repos/kubernetes/autoscaler/issues/216,Fix Typo in Kubernetes Autoscaler README,,closed,True,2017-08-13 10:49:55,2017-08-13 22:43:40
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/217,https://api.github.com/repos/kubernetes/autoscaler/issues/217,Fix listers in CA after godep update,After the update listers don't automatically start in separate goroutines. ,closed,True,2017-08-13 22:17:11,2017-08-13 22:38:15
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/218,https://api.github.com/repos/kubernetes/autoscaler/issues/218,Godep update for K8s 1.7 / CA 0.6 branch,,closed,True,2017-08-14 12:32:18,2017-08-14 14:10:52
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/219,https://api.github.com/repos/kubernetes/autoscaler/issues/219,Set CA version to 0.6.1-beta2 in 0.6 release branch,,closed,True,2017-08-14 12:45:20,2017-08-14 14:11:04
autoscaler,crimsonfaith91,https://github.com/kubernetes/autoscaler/issues/220,https://api.github.com/repos/kubernetes/autoscaler/issues/220,Autoscaler should not use CreatedByAnnotation,"Based on this [announcement](https://groups.google.com/forum/#!msg/kubernetes-dev/juMOsdaCml0/FwVNJA9uBAAJ), `CreatedByAnnotation` will be deprecated in 1.8 in favor of `ControllerRef`. However, the annotation is still used in this repo:
(1) [drain.go](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/utils/drain/drain.go#L212) 
(2) [test_utils.go](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/updater/test/test_utils.go#L49)
(3) [pods_eviction_restriction.go](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/updater/eviction/pods_eviction_restriction.go#L230)
(4) [drain_test.go](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/utils/drain/drain_test.go#L55)

There is a necessary change to use `ControllerRef`. Does anyone have bandwidth to work on this?",closed,False,2017-08-15 00:40:38,2017-08-30 00:36:44
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/221,https://api.github.com/repos/kubernetes/autoscaler/issues/221,Set CA version to 0.6.1 in 0.6 release branch,cc: @MaciekPytel @bskiba @aleksandra-malinowska ,closed,True,2017-08-15 19:37:14,2017-08-15 19:58:35
autoscaler,drinktee,https://github.com/kubernetes/autoscaler/pull/222,https://api.github.com/repos/kubernetes/autoscaler/issues/222,Use glog to replace fmt in azure cloudprovider,"In azure cloudprovider some log lines use fmt and others use glog.
I think this should be consistent.",closed,True,2017-08-16 09:05:44,2017-08-17 09:30:24
autoscaler,drinktee,https://github.com/kubernetes/autoscaler/issues/223,https://api.github.com/repos/kubernetes/autoscaler/issues/223,Cluster Autoscaler should suppurt using a kubeconfig file to create kube-client,I run ca out of cluster and I think it is more convenient to use a kubeconfig file to create kube-client.,closed,False,2017-08-16 11:32:42,2017-08-30 00:37:45
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/224,https://api.github.com/repos/kubernetes/autoscaler/issues/224,Labels in NAP proposal update,,closed,True,2017-08-16 12:12:59,2017-08-16 12:28:46
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/225,https://api.github.com/repos/kubernetes/autoscaler/issues/225,Node Auto-provisioning in GKE,"This is an umbrella bug for the Node Auto-Provisioning effort for GKE. 
Design doc: #208 

cc: @MaciekPytel @aleksandra-malinowska @bskiba ",closed,False,2017-08-17 00:58:51,2017-11-03 10:08:40
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/226,https://api.github.com/repos/kubernetes/autoscaler/issues/226,NAP - pick best labels for pods,Ref: #225 ,closed,True,2017-08-17 00:59:49,2017-08-17 09:16:04
autoscaler,ryanwalls,https://github.com/kubernetes/autoscaler/issues/227,https://api.github.com/repos/kubernetes/autoscaler/issues/227,cluster-autoscaler not scaling down unready nodes if utilization is over threshold,"Not sure if this is expected...

I have a node that has been unready for over 20 mins, and is therefore eligible for scale-in, but it isn't getting scaled in because the last reported utilization (before the kubelet stopped sending updates) was 60%.  

Would it make sense to have an unready node ignore utilization when determining if it can scale in?",closed,False,2017-08-17 15:01:47,2017-08-22 13:54:24
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/228,https://api.github.com/repos/kubernetes/autoscaler/issues/228,Update sig meeting link,,closed,True,2017-08-17 15:17:42,2017-08-17 20:17:39
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/229,https://api.github.com/repos/kubernetes/autoscaler/issues/229,Remove Azure support,With deep regret I have to remove the Azure-related code from Cluster Autoscaler. There are reports that it doesn't work and doesn't use the correct azure resources. Moreover the code doesn't compile with the newest Azure sdk and we couldn't find anyone to actually review and fix the existing code (https://github.com/kubernetes/autoscaler/issues/133  https://github.com/kubernetes/kubernetes/issues/47511).,closed,True,2017-08-17 20:33:58,2017-10-03 00:51:16
autoscaler,praseodym,https://github.com/kubernetes/autoscaler/issues/230,https://api.github.com/repos/kubernetes/autoscaler/issues/230,Implement OpenStack cloud provider,"To support cluster autoscaling for Kubernetes clusters hosted on OpenStack, a new cloud provider should be added.

[Previous work was done on this in kubernetes/contrib](https://github.com/kubernetes/contrib/pull/1980) leveraging OpenStack Heat and the [Gophercloud](https://github.com/gophercloud/gophercloud/) library.",closed,False,2017-08-17 20:43:32,2018-03-22 12:46:14
autoscaler,crimsonfaith91,https://github.com/kubernetes/autoscaler/pull/231,https://api.github.com/repos/kubernetes/autoscaler/issues/231,"Deprecate created-by annotation for pod drain, in favor of ControllerRef",Addressing https://github.com/kubernetes/autoscaler/issues/220,closed,True,2017-08-17 21:29:49,2017-08-30 00:32:50
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/232,https://api.github.com/repos/kubernetes/autoscaler/issues/232,Update dependencies for kubemark,Requires #229 to be merged,closed,True,2017-08-17 22:15:04,2017-08-18 10:07:26
autoscaler,Parag08,https://github.com/kubernetes/autoscaler/issues/233,https://api.github.com/repos/kubernetes/autoscaler/issues/233,cluster able to autoscale [spawn new aws instance] but fails to add it to k8s-cluster,I created a asg group and its profile using terraform then I created a kubernetes cluster using kargo. When i try to autoscale it successfully start the instances in aws but never adds it to kubernetes cluster. Can anyone point me to code where how exactly a node is added to kubernetes cluster is.,closed,False,2017-08-18 06:30:12,2017-08-18 08:26:02
autoscaler,simnalamburt,https://github.com/kubernetes/autoscaler/issues/234,https://api.github.com/repos/kubernetes/autoscaler/issues/234,Update documentation: AWS now supports ARNs for autoscaling groups,"In the past, AWS did not support ARNs for autoscaling groups.

> Unfortunately AWS does not support ARNs for autoscaling groups yet so you must use ""*"" as the resource. More information here.
>
> ###### Reference: [autoscaler/cluster-autoscaler/cloudprovider/aws/README.md](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws#permissions)

Now, however, AWS seems to have started supporting ARNs for the Autoscale group.

> ### Auto Scaling Resources
>
> For actions that support resource-level permissions, you can control the Auto Scaling group or launch configuration that users are allowed to access.
>
> To specify an Auto Scaling group, you must specify its Amazon Resource Name (ARN) as follows:
>
> ```
> ""Resource"": ""arn:aws:autoscaling:region:123456789012:autoScalingGroup:uuid:autoScalingGroupName/asg-name""
> ```
>
> ###### Reference: [Controlling Access to Your Auto Scaling Resources - Auto Scaling Resources](https://docs.aws.amazon.com/autoscaling/latest/userguide/control-access-using-iam.html#policy-auto-scaling-resources)

Documentation of this repository is also likely to require updates. Shall we?",closed,False,2017-08-18 06:52:16,2018-02-09 08:45:36
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/235,https://api.github.com/repos/kubernetes/autoscaler/issues/235,Add libseccomp-dev to dockerbuilder,Needed by updated godeps - followup of #232.,closed,True,2017-08-18 10:22:43,2017-08-18 13:49:07
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/236,https://api.github.com/repos/kubernetes/autoscaler/issues/236,Godep update in FAQ,cc: @MaciekPytel @bskiba @aleksandra-malinowska @wasylkowski,closed,True,2017-08-18 10:52:28,2017-08-18 11:42:24
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/237,https://api.github.com/repos/kubernetes/autoscaler/issues/237,Drill down scale down metrics,"Split scale down duration into three parts:
1. Find nodes to remove
2. Node deletion
3. Misc operations",closed,True,2017-08-18 11:26:16,2017-09-11 11:32:18
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/238,https://api.github.com/repos/kubernetes/autoscaler/issues/238,Node Autoprovisioning expansion of CloudProvider api,Ref: #225 #208 ,closed,True,2017-08-18 12:31:09,2017-08-18 14:11:04
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/239,https://api.github.com/repos/kubernetes/autoscaler/issues/239,Re-use results for similar pods in FilterOutSchedulable,"This should help performance problems in FilterOutSchedulable if there are thousands of pending pods, provided they were created by a smaller set of controllers.",closed,True,2017-08-18 15:20:32,2017-08-25 00:09:48
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/240,https://api.github.com/repos/kubernetes/autoscaler/issues/240,Split kubemark cloud provider implementation to linux and non-linux,"Kubemark cloud provider dependencies require some constants that are only available on linux, forcing to compile CA using docker even for development. As docker compilation on macos is much much longer and cpu-intensive this PR splits kubemark implementation to 2 files - one with the real implementation that is used on linux and one with fake/mock that is used everywhere else. ",closed,True,2017-08-19 00:15:44,2017-08-21 10:16:38
autoscaler,Lion-Wei,https://github.com/kubernetes/autoscaler/pull/241,https://api.github.com/repos/kubernetes/autoscaler/issues/241,Update README.md,,closed,True,2017-08-19 04:05:45,2017-08-21 10:15:58
autoscaler,Rajadeepan,https://github.com/kubernetes/autoscaler/pull/242,https://api.github.com/repos/kubernetes/autoscaler/issues/242,Fix minor typo,Fix minor typo,closed,True,2017-08-21 18:24:14,2017-08-22 07:56:52
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/243,https://api.github.com/repos/kubernetes/autoscaler/issues/243,Cloud Provider Interface for Kubemark,"This allows to run Custer Autoscaler on Kubemark.
See autoscaler/cluster-autoscaler/proposals/kubemark_integration.md
for more details.",closed,True,2017-08-21 19:20:27,2017-09-11 11:31:58
autoscaler,rushtehrani,https://github.com/kubernetes/autoscaler/pull/244,https://api.github.com/repos/kubernetes/autoscaler/issues/244,Print pod namespace/name instead of func signature,,closed,True,2017-08-21 21:44:01,2017-09-15 01:21:56
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/245,https://api.github.com/repos/kubernetes/autoscaler/issues/245,Temporarily blacklist under-utilised nodes whose pods cannot be moved elsewhere,"There is no point of trying to move pods that failed to be moved just after 10 seconds. It is heavily unlikely that the situation improved during that 10 seconds. We can skip them for minute or two. Thanks to that we won't have to do super-expensive pod migration attempt that is unlikely to succeed. This should significantly improve scale down performance.  

cc: @MaciekPytel @bskiba @aleksandra-malinowska ",closed,False,2017-08-21 23:22:58,2017-08-25 15:54:42
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/246,https://api.github.com/repos/kubernetes/autoscaler/issues/246,Keep only N non-empty candidates for scale down,"As CA deletes only 1 non-empty candidate every 1 min or so there is no reason to monitor more than, lets say 30-50 non-empty candidates at the same time. If we have more of them then we can temporarily skip them. Once the current candidates are deleted or they stop being eligible for deletion we will get back to the other potentially removeable nodes. 

cc: @MaciekPytel @bskiba @aleksandra-malinowska 
",closed,False,2017-08-21 23:28:44,2017-09-14 21:48:15
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/247,https://api.github.com/repos/kubernetes/autoscaler/issues/247,Check only K new possible candidates for scale-down per iteration,"Works with combination with #245 and #246. With this, every iteration we will :

* Check if all N previously removeable, non-empty (RNE) nodes are still removeable.  N is lower or equal 30 (see #246).

* If N  < 30 (removeable nodes set can be expanded) then CA check K non-blacklisted underutilized nodes are removeable. If a node is unremoveble it will be blacklisted (#245) and retried after 2 min (or more). If it is removeable it will go to RNE nodes until the RNE node set reaches 30 elements. 

K should be around cluster_size / 10. This should guarantee that every node will be either reconsidered every 2 min and the number of nodes to investigate per iteration is limited (and thus CA is responsive)

cc: @MaciekPytel @bskiba @aleksandra-malinowska  ",closed,False,2017-08-21 23:47:38,2017-09-07 09:15:32
autoscaler,jkamenik,https://github.com/kubernetes/autoscaler/issues/248,https://api.github.com/repos/kubernetes/autoscaler/issues/248,Pod not deleted preventing scaling down on all nodes,"Platform: AWS
Kube Version: 1.7.0

I have two nodes that are can be scheduled for delete: 142 and 167.  One of those nodes has a pod which is failing to terminate in a reasonable amount of time; [which is a separate issue](https://github.com/kubernetes/helm/issues/2772). The issue with the autoscaler is that the pod is scheduled on 167, but it preventing 142 from scaling down.

Based on the logs it appears that the autoscaler is trying to delete the `datanode` pod (which is on node 167) even when it is only trying to scale down node 142.  This likely happened because it previously tried to scale down 167 and could not because the pod refused to terminate.

It doesn't seem correct that autoscaler gets stuck trying to re-terminate a pod on a node that it isn't currently trying to scale down.


Here is the scale down log:

```
I0822 11:27:46.913485       1 static_autoscaler.go:272] Calculating unneeded nodes
I0822 11:27:46.913889       1 utils.go:343] Skipping ip-10-0-0-48.us-east-2.compute.internal - no node group config
I0822 11:27:46.914077       1 scale_down.go:148] Node ip-10-0-0-98.us-east-2.compute.internal - utilization 0.625000
I0822 11:27:46.914095       1 scale_down.go:152] Node ip-10-0-0-98.us-east-2.compute.internal is not suitable for removal - utilization too big (0.625000)
I0822 11:27:46.914101       1 scale_down.go:148] Node ip-10-0-0-167.us-east-2.compute.internal - utilization 0.000000
I0822 11:27:46.914107       1 scale_down.go:148] Node ip-10-0-0-209.us-east-2.compute.internal - utilization 0.625000
I0822 11:27:46.914112       1 scale_down.go:152] Node ip-10-0-0-209.us-east-2.compute.internal is not suitable for removal - utilization too big (0.625000)
I0822 11:27:46.914122       1 scale_down.go:148] Node ip-10-0-0-142.us-east-2.compute.internal - utilization 0.125000
I0822 11:27:46.914131       1 scale_down.go:148] Node ip-10-0-0-130.us-east-2.compute.internal - utilization 0.540988
I0822 11:27:46.914135       1 scale_down.go:152] Node ip-10-0-0-130.us-east-2.compute.internal is not suitable for removal - utilization too big (0.540988)
I0822 11:27:46.914141       1 scale_down.go:148] Node ip-10-0-0-29.us-east-2.compute.internal - utilization 0.024250
I0822 11:27:46.914148       1 scale_down.go:148] Node ip-10-0-0-236.us-east-2.compute.internal - utilization 0.432500
I0822 11:27:46.914154       1 scale_down.go:148] Node ip-10-0-0-70.us-east-2.compute.internal - utilization 0.500000
I0822 11:27:46.914159       1 scale_down.go:152] Node ip-10-0-0-70.us-east-2.compute.internal is not suitable for removal - utilization too big (0.500000)
I0822 11:27:46.914165       1 scale_down.go:148] Node ip-10-0-0-105.us-east-2.compute.internal - utilization 0.762500
I0822 11:27:46.914171       1 scale_down.go:152] Node ip-10-0-0-105.us-east-2.compute.internal is not suitable for removal - utilization too big (0.762500)
I0822 11:27:46.914180       1 scale_down.go:148] Node ip-10-0-0-10.us-east-2.compute.internal - utilization 0.698750
I0822 11:27:46.914184       1 scale_down.go:152] Node ip-10-0-0-10.us-east-2.compute.internal is not suitable for removal - utilization too big (0.698750)
I0822 11:27:46.914262       1 cluster.go:75] Fast evaluation: ip-10-0-0-167.us-east-2.compute.internal for removal
I0822 11:27:46.914345       1 cluster.go:104] Fast evaluation: node ip-10-0-0-167.us-east-2.compute.internal may be removed
I0822 11:27:46.914353       1 cluster.go:75] Fast evaluation: ip-10-0-0-142.us-east-2.compute.internal for removal
I0822 11:27:46.914411       1 cluster.go:219] Looking for place for redknight/datanode-2
I0822 11:27:46.914485       1 cluster.go:192] Evaluation ip-10-0-0-70.us-east-2.compute.internal for redknight/datanode-2 -> <nil>
I0822 11:27:46.914503       1 cluster.go:104] Fast evaluation: node ip-10-0-0-142.us-east-2.compute.internal may be removed
I0822 11:27:46.914506       1 cluster.go:75] Fast evaluation: ip-10-0-0-29.us-east-2.compute.internal for removal
I0822 11:27:46.914530       1 cluster.go:89] Fast evaluation: node ip-10-0-0-29.us-east-2.compute.internal cannot be removed: non-daemonset, non-mirrored, non-pdb-assigned kube-system pod present: heapster-v1.3.0-3188190561-jk78w
I0822 11:27:46.914553       1 cluster.go:75] Fast evaluation: ip-10-0-0-236.us-east-2.compute.internal for removal
I0822 11:27:46.914744       1 cluster.go:89] Fast evaluation: node ip-10-0-0-236.us-east-2.compute.internal cannot be removed: non-daemonset, non-mirrored, non-pdb-assigned kube-system pod present: kube-dns-2272871451-zvzps
I0822 11:27:46.914818       1 static_autoscaler.go:287] ip-10-0-0-167.us-east-2.compute.internal is unneeded since 2017-08-22 11:27:46.913940528 +0000 UTC duration 869.715µs
I0822 11:27:46.914831       1 static_autoscaler.go:287] ip-10-0-0-142.us-east-2.compute.internal is unneeded since 2017-08-22 11:17:44.99497527 +0000 UTC duration 10m1.919849207s
I0822 11:27:46.914837       1 static_autoscaler.go:292] Starting scale down
I0822 11:27:46.914844       1 scale_down.go:206] ip-10-0-0-167.us-east-2.compute.internal was unneeded for 900.473µs
I0822 11:27:46.914850       1 scale_down.go:206] ip-10-0-0-142.us-east-2.compute.internal was unneeded for 10m1.919865731s
I0822 11:27:46.960524       1 cluster.go:75] Detailed evaluation: ip-10-0-0-142.us-east-2.compute.internal for removal
I0822 11:27:46.972156       1 cluster.go:219] Looking for place for redknight/datanode-2
I0822 11:27:46.972268       1 cluster.go:192] Evaluation ip-10-0-0-70.us-east-2.compute.internal for redknight/datanode-2 -> <nil>
I0822 11:27:46.972295       1 cluster.go:104] Detailed evaluation: node ip-10-0-0-142.us-east-2.compute.internal may be removed
I0822 11:27:46.972309       1 scale_down.go:319] Scale-down: removing node ip-10-0-0-142.us-east-2.compute.internal, utilization: 0.125, pods to reschedule: redknight/datanode-2
I0822 11:27:46.972568       1 event.go:218] Event(v1.ObjectReference{Kind:""ConfigMap"", Namespace:""kube-system"", Name:""cluster-autoscaler-status"", UID:""2c988929-8698-11e7-9a68-021cc5e37cda"", APIVersion:""v1"", ResourceVersion:""1759182"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' Scale-down: removing node ip-10-0-0-142.us-east-2.compute.internal, utilization: 0.125, pods to reschedule: redknight/datanode-2
I0822 11:27:46.988036       1 delete.go:53] Successfully added toBeDeletedTaint on node ip-10-0-0-142.us-east-2.compute.internal
I0822 11:27:46.988520       1 event.go:218] Event(v1.ObjectReference{Kind:""Node"", Namespace:"""", Name:""ip-10-0-0-142.us-east-2.compute.internal"", UID:""01de3899-86d5-11e7-9a68-021cc5e37cda"", APIVersion:""v1"", ResourceVersion:""1759206"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' marked the node as toBeDeleted/unschedulable
I0822 11:27:46.988602       1 event.go:218] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""redknight"", Name:""datanode-2"", UID:""db263386-872c-11e7-9a68-021cc5e37cda"", APIVersion:""v1"", ResourceVersion:""1759066"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' deleting pod for node scale down
E0822 11:27:47.003477       1 scale_down.go:471] Not deleted yet &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:datanode-2,GenerateName:datanode-,Namespace:redknight,SelfLink:/api/v1/namespaces/redknight/pods/datanode-2,UID:db263386-872c-11e7-9a68-021cc5e37cda,ResourceVersion:1759216,Generation:0,CreationTimestamp:2017-08-22 11:27:18 +0000 UTC,DeletionTimestamp:2017-08-22 11:29:46 +0000 UTC,DeletionGracePeriodSeconds:*120,Labels:map[string]string{component: datanode,controller-revision-hash: datanode-1788916607,},Annotations:map[string]string{kubernetes.io/created-by: {""kind"":""SerializedReference"",""apiVersion"":""v1"",""reference"":{""kind"":""StatefulSet"",""namespace"":""redknight"",""name"":""datanode"",""uid"":""4ca94065-82b5-11e7-8306-020d9e0d1cb8"",""apiVersion"":""apps/v1beta1"",""resourceVersion"":""1759054""}}
,},OwnerReferences:[{apps/v1beta1 StatefulSet datanode 4ca94065-82b5-11e7-8306-020d9e0d1cb8 0xc422f38ab8 0xc422f38ab9}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{hdfs {nil nil nil nil nil nil nil nil nil PersistentVolumeClaimVolumeSource{ClaimName:hdfs-datanode-2,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {default-token-jksp4 {nil nil nil nil nil &SecretVolumeSource{SecretName:default-token-jksp4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{datanode fourv/hadoop-datanode:2.8.0-1f99732 [] []  [{ 0 50075 TCP } { 0 50475 TCP } { 0 50010 TCP } { 0 50020 TCP }] [] [{CORE_CONF_fs_defaultFS hdfs://namenode:8020 nil} {HDFS_CONF_dfs_replication  nil} {HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check false nil} {HDFS_CONF_dfs_datanode_use_datanode_hostname false nil} {HDFS_CONF_dfs_client_use_datanode_hostname false nil} {HDFS_CONF_dfs_webhdfs_enabled true nil} {HDFS_CONF_dfs_datanode_data_dir file:///hadoop-data/app nil}] {map[cpu:{{1 0} {<nil>} 1 DecimalSI} memory:{{1610612736 0} {<nil>}  BinarySI}] map[cpu:{{1 0} {<nil>} 1 DecimalSI} memory:{{1610612736 0} {<nil>}  BinarySI}]} [{hdfs false /hadoop-data } {default-token-jksp4 true /var/run/secrets/kubernetes.io/serviceaccount }] Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/,Port:50075,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:15,TimeoutSeconds:2,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-142.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:datanode-2,Subdomain:datanode,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2017-08-22 11:27:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2017-08-22 11:27:18 +0000 UTC ContainersNotReady containers with unready status: [datanode]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2017-08-22 11:27:18 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.142,PodIP:,StartTime:2017-08-22 11:27:18 +0000 UTC,ContainerStatuses:[{datanode {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 fourv/hadoop-datanode:2.8.0-1f99732  }],QOSClass:Guaranteed,InitContainerStatuses:[],},}
I0822 11:27:47.439350       1 reflector.go:405] k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72: Watch close - *v1.PersistentVolumeClaim total 0 items received
...
I0822 11:30:07.262791       1 delete.go:106] Releasing taint {Key:ToBeDeletedByClusterAutoscaler Value:1503401266 Effect:NoSchedule TimeAdded:0001-01-01 00:00:00 +0000 UTC} on node ip-10-0-0-142.us-east-2.compute.internal
I0822 11:30:07.277288       1 delete.go:119] Successfully released toBeDeletedTaint on node ip-10-0-0-142.us-east-2.compute.internal
E0822 11:30:07.277383       1 static_autoscaler.go:301] Failed to scale down: <nil>
I0822 11:30:07.277750       1 event.go:218] Event(v1.ObjectReference{Kind:""Node"", Namespace:"""", Name:""ip-10-0-0-142.us-east-2.compute.internal"", UID:""01de3899-86d5-11e7-9a68-021cc5e37cda"", APIVersion:""v1"", ResourceVersion:""1759206"", FieldPath:""""}): type: 'Warning' reason: 'ScaleDownFailed' failed to drain the node, aborting ScaleDown
```

Here are the full list of pods.  there is nothing currently running on the node being scaled down, so the failure appears to be due to terminating a pod on a different node.  Note: the current namespace and the `kube-system` are the only namespaces.

```
$ kubectl get pods -o wide
NAME                                                   READY     STATUS    RESTARTS   AGE       IP           NODE
airflow-metastore-2704145463-4m2ph                     1/1       Running   0          18h       10.2.70.16   ip-10-0-0-236.us-east-2.compute.internal
airflow-redis-3133176153-klv28                         1/1       Running   0          18h       10.2.70.12   ip-10-0-0-236.us-east-2.compute.internal
airflow-worker-0                                       1/1       Running   0          18h       10.2.70.8    ip-10-0-0-236.us-east-2.compute.internal
airflow-worker-1                                       1/1       Running   0          18h       10.2.61.9    ip-10-0-0-70.us-east-2.compute.internal
airflow-worker-2                                       1/1       Running   0          16h       10.2.69.12   ip-10-0-0-105.us-east-2.compute.internal
airflow-worker-3                                       1/1       Running   0          18h       10.2.81.8    ip-10-0-0-130.us-east-2.compute.internal
airflow-worker-4                                       1/1       Running   1          18h       10.2.87.21   ip-10-0-0-10.us-east-2.compute.internal
airflow-worker-5                                       1/1       Running   0          18h       10.2.69.9    ip-10-0-0-105.us-east-2.compute.internal
airflow-worker-6                                       1/1       Running   0          18h       10.2.14.7    ip-10-0-0-209.us-east-2.compute.internal
airflow-worker-7                                       1/1       Running   0          17h       10.2.39.8    ip-10-0-0-98.us-east-2.compute.internal
datanode-0                                             1/1       Running   1          11h       10.2.69.14   ip-10-0-0-105.us-east-2.compute.internal
datanode-1                                             1/1       Running   0          18h       10.2.70.9    ip-10-0-0-236.us-east-2.compute.internal
datanode-2                                             1/1       Running   0          2m        10.2.62.32   ip-10-0-0-167.us-east-2.compute.internal
elasticsearch-client-0                                 1/1       Running   1          18h       10.2.87.16   ip-10-0-0-10.us-east-2.compute.internal
elasticsearch-data-0                                   1/1       Running   0          18h       10.2.39.7    ip-10-0-0-98.us-east-2.compute.internal
elasticsearch-data-1                                   1/1       Running   0          18h       10.2.14.8    ip-10-0-0-209.us-east-2.compute.internal
elasticsearch-data-2                                   1/1       Running   0          18h       10.2.81.10   ip-10-0-0-130.us-east-2.compute.internal
elasticsearch-master-0                                 1/1       Running   0          18h       10.2.61.11   ip-10-0-0-70.us-east-2.compute.internal
elasticsearch-master-1                                 1/1       Running   0          18h       10.2.69.8    ip-10-0-0-105.us-east-2.compute.internal
elasticsearch-master-2                                 1/1       Running   0          18h       10.2.39.6    ip-10-0-0-98.us-east-2.compute.internal
greyspark-0                                            1/1       Running   0          16h       10.2.87.20   ip-10-0-0-10.us-east-2.compute.internal
gsc-api-3447112860-tkmpx                               1/1       Running   0          18h       10.2.70.14   ip-10-0-0-236.us-east-2.compute.internal
gsc-db-4220490965-20tmd                                1/1       Running   0          16h       10.2.70.20   ip-10-0-0-236.us-east-2.compute.internal
gsc-jobs-1155235937-jw80t                              1/1       Running   0          18h       10.2.70.13   ip-10-0-0-236.us-east-2.compute.internal
gsc-ui-3090011021-5skbw                                1/1       Running   0          16h       10.2.87.18   ip-10-0-0-10.us-east-2.compute.internal
hadoop-rm-0                                            1/1       Running   0          18h       10.2.81.9    ip-10-0-0-130.us-east-2.compute.internal
hadoop-spark-0                                         1/1       Running   0          16h       10.2.69.11   ip-10-0-0-105.us-east-2.compute.internal
k8s-statefulset-autoscaler-188211746-qh80l             1/1       Running   0          18h       10.2.70.15   ip-10-0-0-236.us-east-2.compute.internal
kibana-3505250902-4xb6x                                1/1       Running   0          16h       10.2.70.19   ip-10-0-0-236.us-east-2.compute.internal
kibana-nginx-lb-3154679849-rqgr0                       1/1       Running   0          18h       10.2.70.10   ip-10-0-0-236.us-east-2.compute.internal
knox-4274376749-ltsr9                                  1/1       Running   0          17h       10.2.70.18   ip-10-0-0-236.us-east-2.compute.internal
namenode-0                                             1/1       Running   0          17h       10.2.14.10   ip-10-0-0-209.us-east-2.compute.internal
nginx-ingress-deployment-2117343614-hlmlb              1/1       Running   0          18h       10.2.70.11   ip-10-0-0-236.us-east-2.compute.internal
redknight-greyspark-gs-api-1547871659-mhxxl            1/1       Running   0          18h       10.2.14.9    ip-10-0-0-209.us-east-2.compute.internal
redknight-greyspark-gs-api-internal-3393963378-tz8tk   1/1       Running   0          17h       10.2.70.17   ip-10-0-0-236.us-east-2.compute.internal
redknight-greyspark-gs-reporting-2201872284-sjljw      1/1       Running   0          18h       10.2.69.10   ip-10-0-0-105.us-east-2.compute.internal
redknight-greyspark-gs-ui-124688771-z9bbw              1/1       Running   0          16h       10.2.87.19   ip-10-0-0-10.us-east-2.compute.internal
redknight-greyspark-postgresql-0                       1/1       Running   0          18h       10.2.61.10   ip-10-0-0-70.us-east-2.compute.internal
timeline-server-1278787603-g2szz                       1/1       Running   0          16h       10.2.70.21   ip-10-0-0-236.us-east-2.compute.internal
```",closed,False,2017-08-22 12:45:45,2017-12-19 14:22:22
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/249,https://api.github.com/repos/kubernetes/autoscaler/issues/249,Add measuring of FilterOutSchedulable,,closed,True,2017-08-22 14:45:32,2017-09-11 11:31:52
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/250,https://api.github.com/repos/kubernetes/autoscaler/issues/250,Aim for 10s loop duration instead of always sleeping 10s,,closed,True,2017-08-22 15:30:10,2017-08-25 15:30:55
autoscaler,ryanwalls,https://github.com/kubernetes/autoscaler/issues/251,https://api.github.com/repos/kubernetes/autoscaler/issues/251,Feature request: Incorporate nearest billing hour into scale down algorithm,"AWS bills instances by the hour, so if we scale down an instance after it has been up for 15 minutes, we still get billed for 60 minutes.  Ideally, the cluster autoscaler could take into account when the next billing hour is.  E.g. have a `scale-down-minutes-before-billing-hour` parameter that when set to a non-negative number, only makes the node eligible to scale down when it is within `x` mins of the billing hour.

Not sure if the billing hour is something we can determine... so this may be hard/impossible.",closed,False,2017-08-22 16:28:13,2017-09-22 18:15:27
autoscaler,kylegato,https://github.com/kubernetes/autoscaler/issues/252,https://api.github.com/repos/kubernetes/autoscaler/issues/252,AWS DescribeAutoScalingGroups requests too aggressive - API limits reached,"Our cluster-autoscaler pods are dying frequently due to the fact that AWS is rate limiting the API calls.

In a 5 minute period, we logged over 1,509 calls in CloudTrail for ""DescribeAutoScalingGroups""

Here's the error that causes cluster-autoscaler to crash: https://gist.github.com/kylegato/9e2a183eca549572ce0e0082c4381dab

This also prevents us from loading the ASG page on the console.
```
      - command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/s2-us-west-2
        env:
        - name: AWS_REGION
          value: us-west-2
        image: gcr.io/google_containers/cluster-autoscaler:v0.6.0
```

It's also important to note that I run two clusters with this.

Node Count (Cluster 1): 5 nodes
Node Count (Cluster 2): 50 Nodes
",closed,False,2017-08-23 00:50:54,2017-11-20 07:21:35
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/253,https://api.github.com/repos/kubernetes/autoscaler/issues/253,Run basic predicates first in every check,"Previously CA run predicates in random order, this change makes it
run them starting with relatively cheap and most likely to fail
predicates, to avoid running more complex ones if they're not required.",closed,True,2017-08-23 10:14:38,2017-08-23 12:51:36
autoscaler,klausenbusk,https://github.com/kubernetes/autoscaler/issues/254,https://api.github.com/repos/kubernetes/autoscaler/issues/254,Implement DigitalOcean cloud provider,"I'm not exactly sure how to implement this, but I think the easiest way would be creating droplet from a snapshot already configured to join the existing cluster.",closed,False,2017-08-23 16:47:17,2019-03-28 16:35:39
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/255,https://api.github.com/repos/kubernetes/autoscaler/issues/255,Don't create verbose errors in predicates if we ignore them,Turns out all this string formatting is pretty damn expensive.,closed,True,2017-08-23 18:15:51,2017-08-24 13:40:02
autoscaler,locriani,https://github.com/kubernetes/autoscaler/pull/256,https://api.github.com/repos/kubernetes/autoscaler/issues/256,Update main.go,Fix a typo (`waints` -> `waits`),closed,True,2017-08-24 12:19:41,2017-08-24 12:50:34
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/257,https://api.github.com/repos/kubernetes/autoscaler/issues/257,Figure out and implement custom handling for MatchInterPodAffinity predicate,"As part of working on CA performance we run a large scale-up test with some additional logging including call count and total duration spent in each predicate. The results are as follows: 
```
E0824 09:27:54.815425       8 predicates.go:192] Predicate statistics for MaxAzureDiskVolumeCount: called 59985 times, total time 25.328538ms, mean duration 422ns
E0824 09:27:54.815489       8 predicates.go:192] Predicate statistics for MatchInterPodAffinity: called 59985 times, total time 1m48.73252767s, mean duration 1.812661ms
E0824 09:27:54.815497       8 predicates.go:192] Predicate statistics for CheckNodeCondition: called 59985 times, total time 49.05121ms, mean duration 817ns
E0824 09:27:54.815502       8 predicates.go:192] Predicate statistics for MaxEBSVolumeCount: called 59985 times, total time 24.906838ms, mean duration 415ns
E0824 09:27:54.815508       8 predicates.go:192] Predicate statistics for GeneralPredicates: called 59985 times, total time 114.434325ms, mean duration 1.907µs
E0824 09:27:54.815534       8 predicates.go:192] Predicate statistics for NoDiskConflict: called 59985 times, total time 26.067526ms, mean duration 434ns
E0824 09:27:54.815553       8 predicates.go:192] Predicate statistics for NoVolumeNodeConflict: called 59985 times, total time 38.554035ms, mean duration 642ns
E0824 09:27:54.815559       8 predicates.go:192] Predicate statistics for CheckNodeDiskPressure: called 59985 times, total time 19.062642ms, mean duration 317ns
E0824 09:27:54.815564       8 predicates.go:192] Predicate statistics for PodToleratesNodeTaints: called 59985 times, total time 22.448605ms, mean duration 374ns
E0824 09:27:54.815568       8 predicates.go:192] Predicate statistics for MaxGCEPDVolumeCount: called 59985 times, total time 61.944698ms, mean duration 1.032µs
E0824 09:27:54.815572       8 predicates.go:192] Predicate statistics for NoVolumeZoneConflict: called 59985 times, total time 64.231254ms, mean duration 1.07µs 
E0824 09:27:54.815578       8 predicates.go:192] Predicate statistics for PodFitsResources: called 357952 times, total time 514.838808ms, mean duration 1.438µs
E0824 09:27:54.815584       8 predicates.go:192] Predicate statistics for ready: called 59985 times, total time 50.931691ms, mean duration 849ns
E0824 09:27:54.815588       8 predicates.go:192] Predicate statistics for CheckNodeMemoryPressure: called 59985 times, total time 285.070472ms, mean duration 4.752µs
```

It turns out that MatchInterPodAffinity predicate is **3 orders of magnitude** slower compared to other predicates. This is likely because contrary to scheduler we don't do any precomputation for it and we don't maintain predicateMeta object. 

After a quick glance at predicate code it makes sense - it needs to iterate over all existing pods to check if any of them has pod antiaffinity on the pod we're running predicates for. This brings up another problem - how does it get all pods and nodes? We only provide NodeInfo for a single node, the rest comes out of informer. However, that means it reflects the real state of the cluster, not our simulated state. If we've already placed a pod with zone-level antiaffinity on a simulated node it won't prevent adding pods to other simulated nodes in the same zone.

Bottom line is that using zone-level antiaffinity can cause CA to ""overshoot"" creating some nodes for pods that won't be able to schedule on them anyway. Fortunately, this is a pretty unlikely edge case and we will scale-down the unnecessary nodes without any problem.",open,False,2017-08-24 12:48:39,2018-06-05 02:10:33
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/258,https://api.github.com/repos/kubernetes/autoscaler/issues/258,NAP interface implementation - part 1,"* compilation fixes in kubemark
* autoprovisioned() bool function in NodeGroup
* minor refactoring in GCE cloud provider",closed,True,2017-08-24 21:52:25,2017-08-25 18:37:49
autoscaler,andyxning,https://github.com/kubernetes/autoscaler/pull/259,https://api.github.com/repos/kubernetes/autoscaler/issues/259,fix addon_resizer backword image deletion,"Fix addon_resizer clean target for backward image.

/cc @mwielgus @bskiba ",closed,True,2017-08-25 12:52:52,2017-08-25 13:18:45
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/260,https://api.github.com/repos/kubernetes/autoscaler/issues/260,Node autoprovisioning - CP interface impl - part 2,"* NAP machine list
* NAP machine template
* minor modifications to MIG",closed,True,2017-08-25 14:05:22,2017-08-25 18:38:04
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/261,https://api.github.com/repos/kubernetes/autoscaler/issues/261,Skip node in scale-down if it was recently found unremovable,Ref: #245 ,closed,True,2017-08-25 14:05:32,2017-08-25 15:43:46
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/262,https://api.github.com/repos/kubernetes/autoscaler/issues/262,GCE node templates refactoring,"Moving all template-related stuff to a separate file.
As promised in #260.

",closed,True,2017-08-26 12:42:16,2017-08-28 09:24:58
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/263,https://api.github.com/repos/kubernetes/autoscaler/issues/263,Run node drain/delete in a separate goroutine,"* Increases maxGracefulTerminationTime to 10 min.
* Allows scale ups and unneeded node calculations while the node deletion is in progress.
fixes: #147",closed,True,2017-08-26 14:24:02,2017-08-28 14:09:58
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/264,https://api.github.com/repos/kubernetes/autoscaler/issues/264,Dont block scale downs if no nodes can be removed,"#261 puts ""on hold"" unremovable nodes.",closed,True,2017-08-26 14:31:49,2017-08-28 10:16:12
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/265,https://api.github.com/repos/kubernetes/autoscaler/issues/265,Skip nodes in min-sized groups in scale-down simulation,"Currently we track if those nodes can be removed and only
skip them at the execution step. Since checking if node is
unneeded is pretty expensive it's better to filter them out
early.",closed,True,2017-08-28 12:13:00,2017-08-28 14:10:54
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/266,https://api.github.com/repos/kubernetes/autoscaler/issues/266,Don't increase pod graceful termination,,closed,True,2017-08-28 14:57:02,2017-08-28 15:22:21
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/267,https://api.github.com/repos/kubernetes/autoscaler/issues/267,Add GKE mode to GCE cloud provider,"Wit this, all autoscaling information will be taken from node pool config, not from the command line. 
",closed,True,2017-08-28 18:58:30,2017-08-29 12:56:07
autoscaler,drinktee,https://github.com/kubernetes/autoscaler/pull/268,https://api.github.com/repos/kubernetes/autoscaler/issues/268,add kubeconfig flag to create kube-client,"Add kubeconfig flag to create kube-client.
@MaciekPytel  Please review this.Thanks.
#223 ",closed,True,2017-08-29 07:43:13,2017-08-29 10:44:37
autoscaler,AndreaGiardini,https://github.com/kubernetes/autoscaler/issues/269,https://api.github.com/repos/kubernetes/autoscaler/issues/269,'No candidates for scale down' fills the logs,"The message:

```
scale_down.go:246] No candidates for scale down
```

Is repeated several time in our logs and gives no valuable information. Is it possible to exclude it from logging? If not, should we introduce a verbosity level for it?",closed,False,2017-08-29 10:03:45,2017-09-01 11:12:12
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/270,https://api.github.com/repos/kubernetes/autoscaler/issues/270,precompute predicateMetadata in scale-down,Ref: #257 ,closed,True,2017-08-29 14:32:13,2017-08-29 17:00:27
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/271,https://api.github.com/repos/kubernetes/autoscaler/issues/271,Use OwnerReferences in place of deprecated created by annotation,Move to using OwnerReferences for identifying pod's controller instead of deprecated annotation. This is based on #231 and addresses #220,closed,True,2017-08-29 15:09:39,2017-08-30 00:53:17
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/272,https://api.github.com/repos/kubernetes/autoscaler/issues/272,Fix basename population in gce manager,"Basename was set on miginfo copy, not on the original.",closed,True,2017-08-29 16:33:11,2017-08-29 16:56:20
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/273,https://api.github.com/repos/kubernetes/autoscaler/issues/273,Keep maximum 30 candidates for scale down with drain,Issue #246 ,closed,True,2017-08-29 17:33:18,2017-09-11 11:31:49
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/274,https://api.github.com/repos/kubernetes/autoscaler/issues/274,Compile only what is needed when building a docker image,,closed,True,2017-08-29 17:38:51,2017-08-30 08:41:39
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/275,https://api.github.com/repos/kubernetes/autoscaler/issues/275,Create and delete node pool operations for gce_manager,Ref: #225 ,closed,True,2017-08-29 20:37:48,2017-08-30 11:25:45
autoscaler,praseodym,https://github.com/kubernetes/autoscaler/pull/276,https://api.github.com/repos/kubernetes/autoscaler/issues/276,Minor fixes,,closed,True,2017-08-29 21:12:34,2017-08-30 06:01:54
autoscaler,jurgenweber,https://github.com/kubernetes/autoscaler/issues/277,https://api.github.com/repos/kubernetes/autoscaler/issues/277,unable to connect to metrics endpoint,"Hi, I have tried on v0.6.1 and v0.7.0-alpha1. 

```Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.4"", GitCommit:""793658f2d7ca7f064d2bdf606519f9fe1229c381"", GitTreeState:""clean"", BuildDate:""2017-08-17T08:48:23Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.4"", GitCommit:""793658f2d7ca7f064d2bdf606519f9fe1229c381"", GitTreeState:""clean"", BuildDate:""2017-08-17T08:30:51Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

I can install telnet and nettools in the container:

```
# netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0     42 100.96.1.10:34398       100.64.0.1:443          ESTABLISHED
tcp        0      0 100.96.1.10:40320       54.240.206.114:443      TIME_WAIT
tcp        0      0 100.96.1.10:49860       91.189.88.162:80        TIME_WAIT
tcp        0      0 100.96.1.10:40476       54.240.206.114:443      TIME_WAIT
tcp        0      0 100.96.1.10:41686       54.240.206.120:443      TIME_WAIT
tcp        0      0 100.96.1.10:40148       54.240.206.114:443      TIME_WAIT
tcp        0      0 100.96.1.10:40258       54.240.206.114:443      TIME_WAIT
tcp        0      0 100.96.1.10:40202       54.240.206.114:443      TIME_WAIT
tcp        0      0 100.96.1.10:40550       54.240.206.114:443      ESTABLISHED
tcp6       0      0 :::8085                 :::*                    LISTEN
```

```
# curl ""http://localhost:8500""
curl: (7) Failed to connect to localhost port 8500: Connection refused

# curl -g -6 ""http://[::1]:8500/metrics""
curl: (7) Failed to connect to ::1 port 8500: Connection refused
```

from my prometheus container using a service:

``` # telnet cluster-autoscaler.kube-system.svc.cluster.local 8500
telnet: can't connect to remote host (100.69.205.249): Connection refused
```

why can I not connect and see it? I see nothing in the options and I am taking the default. It works for me.

also config options:

```
FLAG: --address="":8085""
FLAG: --alsologtostderr=""false""
FLAG: --balance-similar-node-groups=""false""
FLAG: --cloud-config=""""
FLAG: --cloud-provider=""aws""
FLAG: --cloud-provider-gce-lb-src-cidrs=""130.211.0.0/22,35.191.0.0/16,209.85.152.0/22,209.85.204.0/22""
FLAG: --configmap=""""
FLAG: --estimator=""binpacking""
FLAG: --expander=""least-waste""
FLAG: --internal-reset-unschedulable-pod-condition=""true""
FLAG: --kubernetes=""""
FLAG: --leader-elect=""true""
FLAG: --leader-elect-lease-duration=""15s""
FLAG: --leader-elect-renew-deadline=""10s""
FLAG: --leader-elect-resource-lock=""endpoints""
FLAG: --leader-elect-retry-period=""2s""
FLAG: --log-backtrace-at="":0""
FLAG: --log-dir=""""
FLAG: --logtostderr=""true""
FLAG: --max-empty-bulk-delete=""10""
FLAG: --max-failing-time=""15m0s""
FLAG: --max-graceful-termination-sec=""60""
FLAG: --max-inactivity=""10m0s""
FLAG: --max-node-provision-time=""15m0s""
FLAG: --max-nodes-total=""0""
FLAG: --max-total-unready-percentage=""33""
FLAG: --min-replica-count=""0""
FLAG: --namespace=""kube-system""
FLAG: --node-group-auto-discovery=""""
FLAG: --nodes=""[3:10:nodes-mem.luke.kubernetes.example.com.au 3:10:nodes-cpu.luke.kubernetes.example.com.au]""
FLAG: --ok-total-unready-count=""3""
FLAG: --scale-down-delay=""10m0s""
FLAG: --scale-down-enabled=""true""
FLAG: --scale-down-trial-interval=""1m0s""
FLAG: --scale-down-unneeded-time=""10m0s""
FLAG: --scale-down-unready-time=""20m0s""
FLAG: --scale-down-utilization-threshold=""0.5""
FLAG: --scan-interval=""10s""
FLAG: --skip-nodes-with-local-storage=""false""
FLAG: --skip-nodes-with-system-pods=""true""
FLAG: --stderrthreshold=""2""
FLAG: --unregistered-node-removal-time=""15m0s""
FLAG: --v=""4""
FLAG: --verify-unschedulable-pods=""true""
FLAG: --vmodule=""""
FLAG: --write-status-configmap=""true""
```",closed,False,2017-08-30 06:17:38,2017-08-30 06:27:38
autoscaler,JoelSpeed,https://github.com/kubernetes/autoscaler/issues/278,https://api.github.com/repos/kubernetes/autoscaler/issues/278,Autodiscovery chooses all clusters if no match for multiple tags,"Yesterday, we wanted to turn off the autoscaling from ASGs in one of our clusters. We currently use two tags for Autodiscovery, `cluster-autoscaler/enabled` and `cluster/<CLUSTER_NAME>`.

To disable the autoscaling, we changed the tag `cluster-autoscaler/enabled` to `cluster-autoscaler/disabled` (Helps us know what's going on). Then CA regenerates it's config as it has detected changes, as it should.

Once we had changed this on all ASGs in the cluster, we noticed a bunch of nodes got turned off in other clusters. Inspecting the logs, the CA instance on this cluster now had awareness of all of the ASGs in our environment.

This morning I have been playing around and believe there is a problem with the code here https://github.com/kubernetes/autoscaler/blob/927021dbfad86c8833f3ffd03b9f2badf0077e9c/cluster-autoscaler/cloudprovider/aws/auto_scaling.go#L164-L174

If no ASG has both tags, then `asgNames` is empty when we call `m.getAutoscalingGroupsByNames(asgNames)`

As demonstrated by these log lines I collected earlier:
```
I0830 08:26:56.511613       1 auto_scaling.go:96] Starting getAutoscalingGroupsByTag with keys=[cluster-autoscaler/enabled cluster/<CLUSTER_NAME>]
I0830 08:26:56.765726       1 auto_scaling.go:58] Starting getAutoscalingGroupsByNames with names=[]
```

In the case this list is empty, `nameRefs` on line 83 is also empty and so CA then calls `DescribeAutoScalingGroups` with an empty filter. This returns all ASGs and thus causes the problem I've described above.
https://github.com/kubernetes/autoscaler/blob/927021dbfad86c8833f3ffd03b9f2badf0077e9c/cluster-autoscaler/cloudprovider/aws/auto_scaling.go#L82-L86

I would like to suggest that we add a check to `getAutoscalingGroupsByNames` that verifies that the list it is passed is non-empty",closed,False,2017-08-30 09:14:50,2017-08-30 17:26:02
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/279,https://api.github.com/repos/kubernetes/autoscaler/issues/279,Create node pool asynchronously in NAP,Synchronous call was introduced in #275.,closed,False,2017-08-30 11:27:52,2018-03-10 17:41:34
autoscaler,JoelSpeed,https://github.com/kubernetes/autoscaler/pull/280,https://api.github.com/repos/kubernetes/autoscaler/issues/280,Check ASG name list not empty,"Fixes #278 

I have built and tested this on one of our clusters. 

If no autoscaling group matches both tags, the autoscaler exits and is restarted by kubernetes.

As soon as one ASG matches both tags, CA will stay up and run as designed. If this group is then removed (by changing the tags as described in the issue), then CA will also exit and get into a restart loop as above.

I considered returning an empty list of ASGs, but I believe this isn't the correct approach. If you return an empty list of ASGs then CA will continue through it's working process and only when it gets to the point where it wants to scale up or scale down will it fail. I feel users would prefer to know CA can't do anything earlier than this.

I also considered the fact that maybe CA should just emit a message and keep running and keep trying to build the cloudprovider on an interval so that it doesn't constantly restart the container (given it's run as a deployment, it will be in constant crash loops), but this requires discussion and is beyond the scope of this PR.",closed,True,2017-08-30 13:11:55,2017-10-19 09:09:01
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/281,https://api.github.com/repos/kubernetes/autoscaler/issues/281,Disable MatchInterPodAffinity if there are no pods using affinity,Ref: #257 ,closed,True,2017-08-30 14:21:45,2017-08-31 11:20:34
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/282,https://api.github.com/repos/kubernetes/autoscaler/issues/282,"metrics client for VPA, using metrics-server","It requires working version of metrics-server to work properly. Such version can be currently found [here](https://github.com/kubernetes-incubator/metrics-server/pull/4).

Recommender currently just uses the client to calculate utilization for all the container and prints it out to the console.

@mwielgus @kgrygiel - please review",closed,True,2017-08-30 20:00:03,2017-10-22 17:24:58
autoscaler,smarterclayton,https://github.com/kubernetes/autoscaler/issues/283,https://api.github.com/repos/kubernetes/autoscaler/issues/283,Support for stop / start as opposed to node deletion?,"Has there been discussion (could not find it in issues or docs) about supporting alternate scaler mode that can stop/start nodes rather than created delete them?  We'd like to start using the autoscaler and help support it, but node creation is still fairly complicated (because of security requirements) and not suitable for autoscaling?

I'd be happy to get warm bodies around it.",closed,False,2017-08-30 20:11:49,2018-03-10 19:43:32
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/284,https://api.github.com/repos/kubernetes/autoscaler/issues/284,Node autoprovisioning in scale up,ref #225 ,closed,True,2017-08-30 23:43:30,2017-08-31 12:17:26
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/285,https://api.github.com/repos/kubernetes/autoscaler/issues/285,Set verbosity for each of the glog.Info logs,Fixes #269,closed,True,2017-08-31 21:41:56,2017-09-01 11:12:12
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/286,https://api.github.com/repos/kubernetes/autoscaler/issues/286,Do not return error from exist,"We should not invoke any remote calls for exist. Node groups are not expected to go away. This function is to check whether the group has been created or not, not to do a health-check ping.
Following the pattern as we have in func autoprovisioned().",closed,True,2017-08-31 22:31:53,2017-09-01 10:35:53
autoscaler,alok87,https://github.com/kubernetes/autoscaler/pull/287,https://api.github.com/repos/kubernetes/autoscaler/issues/287,Run CA in master node - aws,"**What?**
Information on running CA in master node in aws. 
This PR updates the documentation for it.",closed,True,2017-09-01 03:57:38,2017-09-04 23:27:49
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/288,https://api.github.com/repos/kubernetes/autoscaler/issues/288,Write events on failed scaleup,,closed,True,2017-09-01 12:20:15,2017-09-01 12:59:38
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/289,https://api.github.com/repos/kubernetes/autoscaler/issues/289,Improve scale-down simulation logs,"Currently scale-down simulation (""findUnneeded"") logs every time it checks if a pod could be moved to other machine (examples: `cluster.go:224] Looking for place for <pod_name>`, `cluster.go:196] Evaluation for -> result`). On one hand those are pretty useful in debugging and understanding autoscaling decisions on small clusters, on the other they easily log 10k+ lines per loop on very large clusters (I've seen as much as 70k in 1k nodes perf test).

We should look at all those logs inside loops iterating a lot of times per main loop and figure out how to keep the useful information in the log without writing tons of spam on larger clusters.",closed,False,2017-09-01 13:10:08,2018-04-23 09:05:44
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/290,https://api.github.com/repos/kubernetes/autoscaler/issues/290,Limit autoprovisioned groups to 15,,closed,True,2017-09-01 15:24:45,2017-09-01 18:19:34
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/291,https://api.github.com/repos/kubernetes/autoscaler/issues/291,Clean up empty autoprovisioned node groups,,closed,True,2017-09-01 15:45:10,2017-09-04 13:03:55
autoscaler,smarterclayton,https://github.com/kubernetes/autoscaler/pull/292,https://api.github.com/repos/kubernetes/autoscaler/issues/292,Allow testing against GCE without running on GCE,"Fixes issues hit while trying to test the autoscaler outside of a GCE cluster.

On the node template check, we warn, but I'm wondering whether we should fail harder. If the template can't be loaded is that a permanent blocker to autoscaling, or does it just make estimation less efficient?",closed,True,2017-09-02 16:04:55,2017-09-04 14:04:53
autoscaler,smarterclayton,https://github.com/kubernetes/autoscaler/pull/293,https://api.github.com/repos/kubernetes/autoscaler/issues/293,DO NOT MERGE - Example of unmanaged instance group autoscale,"As discussed on the SIG meeting, this fleshes out the concept of unmanaged instance group scaling vs managed instance group scaling. I did not fork the GCE cloud provider because the vast majority of the code is the same, but a few small refactors would make it easy to separate the cloud provider scale behavior from other logic.

Some things it doesn't do

* doesn't get a node template, has no concept of node name prefix (that's fairly easy)
* may perform worse because it gets the instance group list instead of simply reading the MIG scale

Worked pretty well in light testing.",closed,True,2017-09-02 16:17:52,2017-12-07 04:30:40
autoscaler,smarterclayton,https://github.com/kubernetes/autoscaler/pull/294,https://api.github.com/repos/kubernetes/autoscaler/issues/294,Support resource-lock type configmap for leader election,"The lock type parameter was being ignored. Use the new factory method to
instantiate the lock type.",closed,True,2017-09-03 22:14:54,2017-09-04 10:18:04
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/295,https://api.github.com/repos/kubernetes/autoscaler/issues/295,Limit total cluster resources during scale up,"CPU and memory limits for the entire cluster are configurable by setting --cores-total and --memory-total flags. For now, only the upper limit is respected, during scale up. PR with scale down respecting lower limit coming soon.",closed,True,2017-09-04 13:46:32,2017-09-08 13:27:40
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/296,https://api.github.com/repos/kubernetes/autoscaler/issues/296,Describe scale down disabled annotation in Cluster Autoscaler FAQ,Documentation for #167,closed,True,2017-09-04 14:32:42,2017-09-07 17:46:25
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/297,https://api.github.com/repos/kubernetes/autoscaler/issues/297,Only consider up to 10% of the nodes as additional candidates for scale down,"When looking for non empty candidates for scale down limit the size of the pool from which additional candidates (not found in previous iteration) are taken.
Intended to increase performance of CA scale down.

Issue https://github.com/kubernetes/autoscaler/issues/247

",closed,True,2017-09-04 15:00:37,2017-09-06 23:04:24
autoscaler,sergeylanzman,https://github.com/kubernetes/autoscaler/pull/298,https://api.github.com/repos/kubernetes/autoscaler/issues/298,Move regexp.MustCompile in AWS provider to global variable,"Function AwsRefFromProviderId call many times. If scale down enable this call default every 10 seconds per each node.
regexp.MustCompile it very heavy function and not need call each time",closed,True,2017-09-04 17:43:32,2017-09-04 22:40:25
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/299,https://api.github.com/repos/kubernetes/autoscaler/issues/299,Expand test cloud provider with autoprovisioning options,To allow ScaleUp/ScaleDown unit test for Node Autoprovisioning.,closed,True,2017-09-04 17:49:36,2017-09-05 12:03:51
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/300,https://api.github.com/repos/kubernetes/autoscaler/issues/300,Graceful termination update in CA FAQ,,closed,True,2017-09-04 18:21:53,2017-09-05 09:34:14
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/301,https://api.github.com/repos/kubernetes/autoscaler/issues/301,Link to 0.6 FAQ in CA FAQ,,closed,True,2017-09-04 18:27:24,2017-09-05 09:25:37
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/302,https://api.github.com/repos/kubernetes/autoscaler/issues/302,Version information also in FAQ,Some web pages (like https://cloud.google.com/container-engine/docs/cluster-autoscaler) link directly to FAQ and the compatibility/used-version information is not visible to the users.,closed,True,2017-09-04 18:53:02,2017-09-05 09:25:11
autoscaler,sergeylanzman,https://github.com/kubernetes/autoscaler/pull/303,https://api.github.com/repos/kubernetes/autoscaler/issues/303,Small optimize code and fix typos,"1. Change from deprecated Core to CoreV1 for kube client
2. Fix typo GetAvilableMachineTypes() => GetAvailableMachineTypes() 
3. Fix small typos 
4. Small optimize code
",closed,True,2017-09-04 19:20:51,2017-09-04 22:34:25
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/304,https://api.github.com/repos/kubernetes/autoscaler/issues/304,Bump CA version to 0.7.0-alpha2,So that we can release a test version and push to official CI.,closed,True,2017-09-04 19:36:24,2017-09-05 09:07:55
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/305,https://api.github.com/repos/kubernetes/autoscaler/issues/305,Introduce the Cluster class holding the runtime state of the VPA recommender,"The Cluster object keeps a map with all pods. Each pod keeps a map with its containers.
Additionally each pod keeps information about labels (this will be necessary to handle VPA label selectors).
The Cluster.AddSample method dispatches usage sample to the proper ContainerStats objects.",closed,True,2017-09-05 07:13:01,2017-10-03 22:47:04
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/306,https://api.github.com/repos/kubernetes/autoscaler/issues/306,Test-in-docker in CA Makefile,,closed,True,2017-09-05 10:11:27,2017-09-05 10:47:47
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/307,https://api.github.com/repos/kubernetes/autoscaler/issues/307,Build template NodeInfo for node autoprovisioning,,closed,True,2017-09-05 15:30:28,2017-09-05 15:57:07
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/308,https://api.github.com/repos/kubernetes/autoscaler/issues/308,Tests for add autoprovisioned node groups,,closed,True,2017-09-06 00:45:22,2017-09-06 11:10:16
autoscaler,genz10,https://github.com/kubernetes/autoscaler/issues/309,https://api.github.com/repos/kubernetes/autoscaler/issues/309,"New EC2 instance isn't joining the cluster, ends up by cluster-master got terminated","```
I0906 10:09:08.720635       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:10.726325       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:12.442233       5 reflector.go:405] k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:207: Watch close - *v1.Node total 163 items received
I0906 10:09:12.818587       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:14.825380       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:16.831362       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:17.215894       5 static_autoscaler.go:145] 4 unregistered nodes present
I0906 10:09:17.216018       5 static_autoscaler.go:213] Filtering out schedulables
I0906 10:09:17.216111       5 static_autoscaler.go:221] No schedulable pods
I0906 10:09:17.216160       5 scale_up.go:50] Pod default/tlr-app-149334622-fn2t5 is unschedulable
I0906 10:09:17.222887       5 scale_up.go:71] Upcoming 1 nodes
I0906 10:09:17.269291       5 scale_up.go:112] Scale-up predicate failed: PodToleratesNodeTaints predicate mismatch, cannot put default/tlr-app-149334622-fn2t5 on template-node-for-playground-group-2477346412368114538, reason: PodToleratesNodeTaints
I0906 10:09:17.269385       5 scale_up.go:141] No pod can fit to %splayground-group
I0906 10:09:17.269433       5 scale_up.go:146] No expansion options
I0906 10:09:17.269474       5 static_autoscaler.go:270] Scale down status: unneededOnly=false lastScaleUpTime=2017-09-06 09:56:10.231357815 +0000 UTC lastScaleDownFailedTrail=2017-09-06 09:54:17.846805595 +0000 UTC schedulablePodsPresent=false
I0906 10:09:17.269501       5 static_autoscaler.go:272] Calculating unneeded nodes
I0906 10:09:17.269671       5 scale_down.go:148] Node ip-xxx-xxx-xx-01 - utilization 0.800000
I0906 10:09:17.269695       5 scale_down.go:152] Node ip-xxx-xxx-xx-01 is not suitable for removal - utilization too big (0.800000)
I0906 10:09:17.269732       5 scale_down.go:148] Node ip-xxx-xxx-xx-02 - utilization 0.750000
I0906 10:09:17.269751       5 scale_down.go:152] Node ip-xxx-xxx-xx-02 is not suitable for removal - utilization too big (0.750000)
I0906 10:09:17.269766       5 scale_down.go:148] Node ip-xxx-xxx-xx-03 - utilization 0.510000
I0906 10:09:17.269779       5 scale_down.go:152] Node ip-xxx-xxx-xx-03 is not suitable for removal - utilization too big (0.510000)
I0906 10:09:17.269814       5 static_autoscaler.go:292] Starting scale down
I0906 10:09:17.269836       5 scale_down.go:246] No candidates for scale down
I0906 10:09:17.270978       5 event.go:218] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""tlr-app-149334622-fn2t5"", UID:""837d1d5d-92e9-11e7-b204-062e864003e6"", APIVersion:""v1"", ResourceVersion:""14450"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0906 10:09:18.922015       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:20.927994       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:22.934054       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:24.940185       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:26.946032       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:27.424886       5 static_autoscaler.go:145] 4 unregistered nodes present
I0906 10:09:27.425007       5 static_autoscaler.go:213] Filtering out schedulables
I0906 10:09:27.425096       5 static_autoscaler.go:221] No schedulable pods
I0906 10:09:27.425156       5 scale_up.go:50] Pod default/tlr-app-149334622-fn2t5 is unschedulable
I0906 10:09:27.431522       5 scale_up.go:71] Upcoming 1 nodes
I0906 10:09:27.476468       5 scale_up.go:112] Scale-up predicate failed: PodToleratesNodeTaints predicate mismatch, cannot put default/tlr-app-149334622-fn2t5 on template-node-for-playground-group-7580655651538548596, reason: PodToleratesNodeTaints
I0906 10:09:27.476562       5 scale_up.go:141] No pod can fit to %splayground-group
I0906 10:09:27.476585       5 scale_up.go:146] No expansion options
I0906 10:09:27.476622       5 static_autoscaler.go:270] Scale down status: unneededOnly=false lastScaleUpTime=2017-09-06 09:56:10.231357815 +0000 UTC lastScaleDownFailedTrail=2017-09-06 09:54:17.846805595 +0000 UTC schedulablePodsPresent=false
I0906 10:09:27.476649       5 static_autoscaler.go:272] Calculating unneeded nodes
I0906 10:09:27.476807       5 scale_down.go:148] Node ip-xxx-xxx-xx-01 - utilization 0.800000
I0906 10:09:27.476833       5 scale_down.go:152] Node ip-xxx-xxx-xx-01 is not suitable for removal - utilization too big (0.800000)
I0906 10:09:27.476848       5 scale_down.go:148] Node ip-xxx-xxx-xx-02 - utilization 0.750000
I0906 10:09:27.476908       5 scale_down.go:152] Node ip-xxx-xxx-xx-02 is not suitable for removal - utilization too big (0.750000)
I0906 10:09:27.476930       5 scale_down.go:148] Node ip-xxx-xxx-xx-03 - utilization 0.510000
I0906 10:09:27.476944       5 scale_down.go:152] Node ip-xxx-xxx-xx-03 is not suitable for removal - utilization too big (0.510000)
I0906 10:09:27.476979       5 static_autoscaler.go:292] Starting scale down
I0906 10:09:27.477002       5 scale_down.go:246] No candidates for scale down
I0906 10:09:27.478082       5 event.go:218] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""tlr-app-149334622-fn2t5"", UID:""837d1d5d-92e9-11e7-b204-062e864003e6"", APIVersion:""v1"", ResourceVersion:""14450"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0906 10:09:29.022462       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:37.614979       5 static_autoscaler.go:145] 4 unregistered nodes present
I0906 10:09:37.615116       5 utils.go:285] Removing unregistered node aws:///ap-southeast-1a/i-XXXXXXX01
**I0906 10:09:37.804807       5 aws_manager.go:147] Terminating EC2 instance: i-XXXXXXX01**
**I0906 10:09:37.804824       5 utils.go:285] Removing unregistered node aws:///ap-southeast-1a/i-XXXXXXX02
W0906 10:09:37.847752       5 utils.go:297] Failed to remove node aws:///ap-southeast-1a/i-XXXXXXX02: min size reached, nodes will not be deleted**
W0906 10:09:37.848548       5 static_autoscaler.go:150] Some unregistered nodes were removed, but got error: min size reached, nodes will not be deleted
I0906 10:09:39.118614       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0906 10:09:41.124693       5 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
```
PS : Some redundant logs was deleted due too much lines.

## Deployment yml

```
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
        - image: gcr.io/google_containers/cluster-autoscaler:v0.6.0
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 500m
              memory: 500Mi
            requests:
              cpu: 500m
              memory: 500Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --nodes=1:10:group-name
          env:
            - name: AWS_REGION
              value: us-east-1
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
          imagePullPolicy: ""Always""
      volumes:
        - name: ssl-certs
          hostPath:
            path: ""/etc/ssl/certs/ca-certificates.crt""
```

## Scenario

I deploy a Deployment that won't fit on current available nodes, and then autoscaler detect that ASG should add new **desired** instance, by increasing desired intance into 4 (before was 3), instance spawned and then instance status checks was 2/2, and for arround 10 minutes those new instance still not joining the cluster, after couple minutes later, autoscaler killed my cluster-master as shows in the log.

## Node Formation

1 cluster master (admin)
1 active node
1 tainted **NoSchedule** node

All nodes was fresh installed

## Info

Lsb_release : Release:	16.04
Kubectl version : `Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5"", GitCommit:""17d7182a7ccbb167074be7a87f0a68bd00d58d97"", GitTreeState:""clean"", BuildDate:""2017-08-31T09:14:02Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}`

Please let me know if there's some information that I can provide.",closed,False,2017-09-06 10:23:19,2017-09-07 04:11:34
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/310,https://api.github.com/repos/kubernetes/autoscaler/issues/310,Core/utils.go unit tests,,closed,True,2017-09-06 11:41:39,2017-09-07 11:45:59
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/311,https://api.github.com/repos/kubernetes/autoscaler/issues/311,Refactor FindNodesToRemove test,Refactored for easier maintenance (reduces repetitive code).,closed,True,2017-09-06 12:43:22,2017-09-06 23:00:39
autoscaler,darkstarmv,https://github.com/kubernetes/autoscaler/issues/312,https://api.github.com/repos/kubernetes/autoscaler/issues/312,Cluster Autoscaler in GCE marks instance group unhealthy if kubelet uses cgroup to limit allocated system resources,"Cluster Autoscaler compares GCE instance type capacity to allocatable capacity returned from Kubernetes API.
If kubelet and docker configured to use cgroup, CA marks Instance group unhealthy and stops managing all Instance groups in the cluster, even if they are healthy.

https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/gce/templates.go#L98-L107
```
	capacity, err := t.buildCapacity(template.Properties.MachineType)
	if err != nil {
		return nil, err
	}
	node.Status = apiv1.NodeStatus{
		Capacity: capacity,
	}
	// TODO: use proper allocatable!!
	node.Status.Allocatable = node.Status.Capacity
``` 

",closed,False,2017-09-06 13:49:04,2017-11-03 10:05:19
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/313,https://api.github.com/repos/kubernetes/autoscaler/issues/313,Scale down fails if a pod that was about to be evicted is gone.,"Recommended fix: error check or pod lookup in

https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/core/scale_down.go#L569

",closed,False,2017-09-06 22:12:56,2017-09-11 09:40:28
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/314,https://api.github.com/repos/kubernetes/autoscaler/issues/314,Pass empty nodes as candidates regardless of quota,"After the performance improvements were made we reduced the number of candidates to 30. However this number includes both empty and non-empty nodes. Deletion of empty nodes (that don't have any user pods, only kubeproxy, fluentd, etc) should have a priority over deletion of non-empty nodes.  ",closed,False,2017-09-07 09:30:17,2017-09-11 12:16:21
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/315,https://api.github.com/repos/kubernetes/autoscaler/issues/315,unremovableNodes may contain nodes that are not there anymore ,Non-existing nodes are never removed.,closed,False,2017-09-07 09:43:21,2017-09-12 08:13:49
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/316,https://api.github.com/repos/kubernetes/autoscaler/issues/316,Godeps update for CA,A new field was added to etc/gce.conf and CA fails to start up. Another godep update will be needed once 1.8 release branch stabilises a bit.,closed,True,2017-09-08 09:14:59,2017-09-16 12:18:14
autoscaler,alok87,https://github.com/kubernetes/autoscaler/issues/317,https://api.github.com/repos/kubernetes/autoscaler/issues/317,Mark nodes on scale up,"**What?**

CA should taint/label nodes on scale up.

**Use case**
We have a use case in which we have written a custom node monitor. Node monitor gets the list of nodes, perform custom checks on the node and send alerts on finding issues.

When CA scales up nodes. A new node that comes up might take some time before `weave-net` comes up, and in that time node monitor sends false alerts.

We want to avoid this situation, this can be done if the node is labelled/tainted for something. And nodes can be ignored in such situations",closed,False,2017-09-08 10:27:59,2018-01-06 02:42:23
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/318,https://api.github.com/repos/kubernetes/autoscaler/issues/318,Always add empty nodes to unneeded nodes,"Regardless of the limits we have for scale down candidates, always mark all empty nodes as unneeded.

https://github.com/kubernetes/autoscaler/issues/314",closed,True,2017-09-08 11:33:21,2017-09-11 11:31:36
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/319,https://api.github.com/repos/kubernetes/autoscaler/issues/319,Core/static_autoscaler.go unit tests.,,closed,True,2017-09-08 12:39:23,2017-09-12 11:11:12
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/320,https://api.github.com/repos/kubernetes/autoscaler/issues/320,Bump cluster autoscaler to 0.7.0-alpha3,,closed,True,2017-09-08 13:35:59,2017-09-08 13:47:27
autoscaler,7chenko,https://github.com/kubernetes/autoscaler/issues/321,https://api.github.com/repos/kubernetes/autoscaler/issues/321,"Scaling up from 0 nodes on AWS, CA not aware of custom resources","When **scaling up from 0 nodes** on AWS, how can I make cluster-autoscaler aware of custom resources on the nodes, such as ""alpha.kubernetes.io/nvidia-gpu""? 

Using kops 1.7.0, kubernetes 1.7.5, cluster-autoscaler 0.6.1, when I have 0 nodes running, starting a job with ""resources: limits: alpha.kubernetes.io/nvidia-gpu: 1"" results in CA inaction due to (note ""Insufficient alpha.kubernetes.io/nvidia-gpu""):


```
I0909 03:36:26.255878       1 scale_up.go:50] Pod default/0dd3d1fc-3e73-90e7-84e0-f24009dc3784-08t94 is unschedulable
I0909 03:36:26.351319       1 scale_up.go:71] Upcoming 0 nodes
I0909 03:36:26.385779       1 scale_up.go:112] Scale-up predicate failed: GeneralPredicates predicate mismatch, cannot put default/0dd3d1fc-3e73-90e7-84e0-f24009dc3784-08t94 on template-node-for-nodes.uswest2.metamoto.net-7573953041833664557, reason: Insufficient alpha.kubernetes.io/nvidia-gpu
I0909 03:36:26.385809       1 scale_up.go:141] No pod can fit to %snodes.uswest2.metamoto.net
I0909 03:36:26.385819       1 scale_up.go:146] No expansion options
...
I0909 02:55:41.381017       1 event.go:218] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""2f0b2cb6-782b-f2e6-4c3c-c37f648f45b2-6fhl2"", UID:""21a5fb02-94fe-11e7-beff-06c6424932c2"", APIVersion:""v1"", ResourceVersion:""7436"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```


It looks like the ""template-node-for-nodes"" doesn't have the resources listed. However if I start a job without the gpu requirement, a node is spun up, and then I can start the original gpu job and it gets scheduled on the node! The node looks like this (kubectl describe nodes) (note ""alpha.kubernetes.io/nvidia-gpu: 1""):


```
Name:			ip-172-31-121-22.us-west-2.compute.internal
Role:
Labels:			beta.kubernetes.io/arch=amd64
			beta.kubernetes.io/instance-type=g2.2xlarge
			beta.kubernetes.io/os=linux
			failure-domain.beta.kubernetes.io/region=us-west-2
			failure-domain.beta.kubernetes.io/zone=us-west-2a
			kubernetes.io/hostname=ip-172-31-121-22.us-west-2.compute.internal
			kubernetes.io/role=node
			node-role.kubernetes.io/node=
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:			<none>
CreationTimestamp:	Fri, 08 Sep 2017 19:57:32 -0700
Conditions:
  Type			Status	LastHeartbeatTime			LastTransitionTime			Reason				Message
  ----			------	-----------------			------------------			------				-------
  NetworkUnavailable 	False 	Fri, 08 Sep 2017 19:57:37 -0700 	Fri, 08 Sep 2017 19:57:37 -0700 	RouteCreated 			RouteController created a route
  OutOfDisk 		False 	Fri, 08 Sep 2017 20:06:43 -0700 	Fri, 08 Sep 2017 19:57:32 -0700 	KubeletHasSufficientDisk 	kubelet has sufficient disk space available
  MemoryPressure 	False 	Fri, 08 Sep 2017 20:06:43 -0700 	Fri, 08 Sep 2017 19:57:32 -0700 	KubeletHasSufficientMemory 	kubelet has sufficient memory available
  DiskPressure 		False 	Fri, 08 Sep 2017 20:06:43 -0700 	Fri, 08 Sep 2017 19:57:32 -0700 	KubeletHasNoDiskPressure 	kubelet has no disk pressure
  Ready 		True 	Fri, 08 Sep 2017 20:06:43 -0700 	Fri, 08 Sep 2017 19:57:52 -0700 	KubeletReady 			kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:	172.31.121.22
  ExternalIP:	34.213.162.221
  InternalDNS:	ip-172-31-121-22.us-west-2.compute.internal
  ExternalDNS:	ec2-34-213-162-221.us-west-2.compute.amazonaws.com
  Hostname:	ip-172-31-121-22.us-west-2.compute.internal
Capacity:
 alpha.kubernetes.io/nvidia-gpu:	1
 cpu:					8
 memory:				15399064Ki
 pods:					110
Allocatable:
 alpha.kubernetes.io/nvidia-gpu:	1
 cpu:					8
 memory:				15296664Ki
 pods:					110
System Info:
 Machine ID:			2118324e509d4582ae925c3ed83d8f2a
 System UUID:			EC2DF760-2914-FF5B-B89E-6B85AEF7C8C2
 Boot ID:			984a55a1-ca22-45cb-9c17-39f71e8315cb
 Kernel Version:		4.4.0-1017-aws
 OS Image:			Ubuntu 16.04.2 LTS
 Operating System:		linux
 Architecture:			amd64
 Container Runtime Version:	docker://1.12.6
 Kubelet Version:		v1.7.5
 Kube-Proxy Version:		v1.7.5
PodCIDR:			100.96.3.0/24
ExternalID:			i-04de825c788bf994e
Non-terminated Pods:		(2 in total)
  Namespace			Name								CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ---------			----								------------	----------	---------------	-------------
  default			93caffb3-2868-a526-8810-2ddc0dd1140a-fv5t4			100m (1%)	0 (0%)		0 (0%)		0 (0%)
  kube-system			kube-proxy-ip-172-31-121-22.us-west-2.compute.internal		100m (1%)	0 (0%)		0 (0%)		0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  200m (2%)	0 (0%)		0 (0%)		0 (0%)
```

New nodes are also spun up correctly as long as there is already at least 1 node running. Any idea how to make the ""template"" for nodes list the correct resources? Thanks!",closed,False,2017-09-09 03:39:41,2018-07-16 19:59:29
autoscaler,sethpollack,https://github.com/kubernetes/autoscaler/pull/322,https://api.github.com/repos/kubernetes/autoscaler/issues/322,"AWS Scaling from/to 0, extract GPU information",fixes #321,closed,True,2017-09-10 16:28:32,2017-09-17 01:31:33
autoscaler,smarterclayton,https://github.com/kubernetes/autoscaler/pull/323,https://api.github.com/repos/kubernetes/autoscaler/issues/323,Do not include ToBeDeleted taint when constructing a template,This results in the simulator being unable to place candidate pods because the taint blocks all scheduling.  The taint could be on the node because it's transitioning to unready and no other nodes are available to extract a template from.,closed,True,2017-09-11 02:32:05,2017-09-11 09:38:14
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/324,https://api.github.com/repos/kubernetes/autoscaler/issues/324,Add checking for pod not found error on eviction,"Addresses #313 by checking for error. 

This has been tested manually by deleting a pod after an eviction was scheduled, but before it was attempted (using modified version of CA which introduced sufficient delay.)",closed,True,2017-09-11 08:51:54,2017-09-11 09:40:08
autoscaler,arashadahamad,https://github.com/kubernetes/autoscaler/issues/325,https://api.github.com/repos/kubernetes/autoscaler/issues/325,Getting error while running `make build` cluster autoscaler code,"Following are the steps which I followed

1- Get k8s.io/kubernetes and k8s.io/autoscaler source code via `go get`
2- Ran `godep restore` for k8s.io/kubernetes
3- Deleted `Godeps` and `vendor` dir from k8s.io/autoscaler/cluster-autoscaler
4- ran `fix-gopath.sh`
5- Ran `godep save ./...` in k8s.io/autoscaler/cluster-autoscaler
6- Then I ran `make build`

I am getting following error, 2-3 days before I was getting some other error and I fixed that manually  after that I was able to build now this errors is different.

rm -f cluster-autoscaler
go get github.com/tools/godep
GOOS=linux godep go build ./...
vendor/github.com/google/cadvisor/container/crio/factory.go:24:2: no buildable Go source files in /home/ubuntu/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/google/cadvisor/container/libcontainer
godep: go exit status 1
make: *** [build] Error 1



Please suggest the build steps if these are not the correct one.",closed,False,2017-09-11 09:43:43,2017-11-03 10:12:55
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/326,https://api.github.com/repos/kubernetes/autoscaler/issues/326,Update Allocatable logic for scale from 0,cc: @bskiba @aleksandra-malinowska ,closed,False,2017-09-11 13:25:59,2017-10-03 22:50:39
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/327,https://api.github.com/repos/kubernetes/autoscaler/issues/327,Remove nodes that are not in the cluster from unremovableNodes,"Fixes #315

@mwielgus ",closed,True,2017-09-11 16:41:24,2017-09-12 08:13:49
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/328,https://api.github.com/repos/kubernetes/autoscaler/issues/328,Port stockout avoidance code to CA 0.6,"So that K8S 1.7 users can enjoy proper behaviour on node stockouts. 

cc: @MaciekPytel ",closed,True,2017-09-11 19:11:25,2017-09-12 09:49:03
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/329,https://api.github.com/repos/kubernetes/autoscaler/issues/329,Core/scale_down.go unit tests.,,closed,True,2017-09-12 08:04:17,2017-09-12 11:12:46
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/330,https://api.github.com/repos/kubernetes/autoscaler/issues/330,Core/autoscaling_context_test.go unit tests.,,closed,True,2017-09-12 11:27:30,2017-09-12 13:07:24
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/331,https://api.github.com/repos/kubernetes/autoscaler/issues/331,Respect minimum cores/memory limit during scale down,"Follow up to #295, adding support for lower limits of total cluster cores/memory.",closed,True,2017-09-12 12:24:07,2017-09-13 09:12:30
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/332,https://api.github.com/repos/kubernetes/autoscaler/issues/332,Fix filtering for autoprovisioned node groups and add unit test.,,closed,True,2017-09-12 13:58:40,2017-09-12 14:40:30
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/333,https://api.github.com/repos/kubernetes/autoscaler/issues/333,Taint empty nodes to be deleted,Mark empty nodes to be deleted with the same taint that is used for node drain.,closed,True,2017-09-12 15:43:07,2017-09-14 22:06:40
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/334,https://api.github.com/repos/kubernetes/autoscaler/issues/334,Bump CA version to 0.6.2 on 0.6 branch,,closed,True,2017-09-12 19:58:19,2017-09-12 20:02:26
autoscaler,mtcode,https://github.com/kubernetes/autoscaler/pull/335,https://api.github.com/repos/kubernetes/autoscaler/issues/335,Move calculateUnneededOnly check just before scale down,"This fix moves the calculateUnneededOnly check after expensive unneeded calculations, to avoid a scenario where the scale down goroutine is still running when the next main loop starts.  This could cause a scale down to be skipped because scale down was still in progress at the beginning of the main loop, but is no longer after calculating unneeded.",closed,True,2017-09-13 03:18:55,2017-09-13 04:35:55
autoscaler,mtcode,https://github.com/kubernetes/autoscaler/pull/336,https://api.github.com/repos/kubernetes/autoscaler/issues/336,Move calculateUnneededOnly check after unneeded calculations,"This fix moves the calculateUnneededOnly check after expensive unneeded calculations, to avoid a scenario where the scale down goroutine is still running when the next main loop starts. This could cause a scale down to be skipped because scale down was still in progress at the beginning of the main loop, but is no longer after calculating unneeded.

The new log message is also intentional; to make it clear when the next main loop iteration starts, even when another goroutine may still be writing log messages.",closed,True,2017-09-13 04:39:16,2017-09-13 17:32:24
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/337,https://api.github.com/repos/kubernetes/autoscaler/issues/337,Core/static_autoscaler_test.go unit tests.,,closed,True,2017-09-13 07:52:59,2017-09-13 08:23:53
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/338,https://api.github.com/repos/kubernetes/autoscaler/issues/338,Cloudprovider unit tests.,,closed,True,2017-09-13 08:51:45,2017-09-13 09:11:41
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/339,https://api.github.com/repos/kubernetes/autoscaler/issues/339,Bump CA version to 0.7.0-beta1,,closed,True,2017-09-13 11:53:16,2017-09-13 12:01:20
autoscaler,ryanwalls,https://github.com/kubernetes/autoscaler/issues/340,https://api.github.com/repos/kubernetes/autoscaler/issues/340,Feature request: Add skip-nodes-with-jobs flag,"We are running many long running jobs on our infrastructure.  If utilization drops below the threshold, the node gets scaled down and the jobs get restarted.  Would be nice to have a `skip-nodes-with-jobs` flag to avoid this issue.",closed,False,2017-09-13 18:28:14,2018-04-03 19:45:18
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/341,https://api.github.com/repos/kubernetes/autoscaler/issues/341,Remove TargetSize() from loops iterating over nodes,"This function talks to cloud provider api and can be throttled (especially on AWS).

cc: @mtcode ",closed,True,2017-09-13 20:35:50,2017-09-14 08:14:38
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/342,https://api.github.com/repos/kubernetes/autoscaler/issues/342,Cloudprovider/gce/gce_cloud_provider.go unit tests.,,closed,True,2017-09-14 06:33:23,2017-09-14 07:37:17
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/343,https://api.github.com/repos/kubernetes/autoscaler/issues/343,Compute allocatable for scale up from 0 based on kube-reserved,,closed,True,2017-09-14 13:23:31,2017-09-15 14:33:43
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/issues/344,https://api.github.com/repos/kubernetes/autoscaler/issues/344,Update Allocatable logic for Node Autoprovisioning,This is different from #326 in that in Node Autoprovisioning we cannot read kube-reserved from kubelet args.,closed,False,2017-09-14 13:28:09,2017-10-03 22:51:28
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/345,https://api.github.com/repos/kubernetes/autoscaler/issues/345,Internal data model for recommender,"to be used by both metrics client and histogram aggregator

@mwielgus @kgrygiel - please reveiw",closed,True,2017-09-14 21:15:12,2017-09-28 12:56:39
autoscaler,innovia,https://github.com/kubernetes/autoscaler/issues/346,https://api.github.com/repos/kubernetes/autoscaler/issues/346,CA0.6.1 AWS - able to scale down to 0 but back up,"Hi

Im using kops on AWS with CA 0.6.1

I am able to scale down to zero but not back up from zero

also not the format of ""No pod can fit to %sgpu-nodes.server.com"" the %s is printed - i think its only on the logs...

```
I0914 20:59:01.756751       1 static_autoscaler.go:213] Filtering out schedulables
I0914 20:59:01.756852       1 static_autoscaler.go:221] No schedulable pods
I0914 20:59:01.756869       1 scale_up.go:50] Pod default/ami-skysegment-l1knx is unschedulable
W0914 20:59:01.855358       1 aws_manager.go:201] Found multiple availability zones, using us-east-1a
I0914 20:59:01.855580       1 scale_up.go:71] Upcoming 0 nodes
I0914 20:59:01.904324       1 scale_up.go:112] Scale-up predicate failed: GeneralPredicates predicate mismatch, cannot put default/ami-skysegment-l1knx on template-node-for-gpu-nodes.server.com-6748296357877885074, reason: Insufficient alpha.kubernetes.io/nvidia-gpu
I0914 20:59:01.904350       1 scale_up.go:141] No pod can fit to %sgpu-nodes.server.com
I0914 20:59:01.904359       1 scale_up.go:146] No expansion options
I0914 20:59:01.904396       1 static_autoscaler.go:270] Scale down status: unneededOnly=false lastScaleUpTime=2017-09-07 07:13:31.488444337 +0000 UTC lastScaleDownFailedTrail=2017-09-03 10:53:59.066502018 +0000 UTC schedulablePodsPresent=false
I0914 20:59:01.904412       1 static_autoscaler.go:272] Calculating unneeded nodes
I0914 20:59:01.904463       1 utils.go:360] Skipping ip-172-10-100-72.ec2.internal - no node group config
I0914 20:59:01.904496       1 utils.go:360] Skipping ip-172-10-165-18.ec2.internal - no node group config
I0914 20:59:01.904572       1 static_autoscaler.go:292] Starting scale down
I0914 20:59:01.904588       1 scale_down.go:260] No candidates for scale down
W0914 20:59:01.904600       1 clusterstate.go:271] Failed to find readiness information for gpu-nodes.server.com
I0914 20:59:01.905081       1 event.go:218] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""ami-skysegment-l1knx"", UID:""6db6c815-998f-11e7-ac58-06fcf22dd9b0"", APIVersion:""v1"", ResourceVersion:""20001417"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```

my command on CA pod is
```
./cluster-autoscaler
--v=4
--stderrthreshold=info
--cloud-provider=aws
--skip-nodes-with-local-storage=false
--nodes=0:10:gpu-nodes.server.com
```
my node config

```yaml

apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2017-05-15T11:01:53Z
  labels:
    kops.k8s.io/cluster: server.com
  name: gpu-nodes
spec:
  cloudLabels:
    gpu_ml: spot
    k8s.io/cluster-autoscaler/node-template/label/node_type: gpu
    service_name: machine-learning
  image: k8s-1.6-alpha-debian-jessie-amd64-hvm-ebs-2017-05-15
  machineType: p2.xlarge
  maxPrice: ""0.40""
  maxSize: 10
  minSize: 0
  role: Node
  rootVolumeSize: 30
  rootVolumeType: gp2
  subnets:
  - us-east-1a
  - us-east-1b
  - us-east-1c
  - us-east-1d
  - us-east-1e
  - us-east-1f
```

Would appreciate help in resolving this as this will help us a lot.
Thanks

Ami",closed,False,2017-09-14 21:17:24,2017-12-13 10:28:11
autoscaler,markine,https://github.com/kubernetes/autoscaler/issues/347,https://api.github.com/repos/kubernetes/autoscaler/issues/347,Correctly taint empty nodes (bring PR #333 to 0.6.x release),@mwielgus Please cherrypick/port https://github.com/kubernetes/autoscaler/pull/333 to the 0.6.x release branch. I'm debugging v0.6.0 for a race between an empty node being terminated and the k8s scheduler placing new pods on the node as it is going down. The empty vs non-empty node drain code paths are distinct. The lack of a taint on the empty node during that process causes unexpected loss of one-shot job pods. Thank you!,closed,False,2017-09-14 22:06:14,2017-11-03 10:09:15
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/348,https://api.github.com/repos/kubernetes/autoscaler/issues/348,Taint empty nodes to be deleted,"Cherry pick of #333, addresses #347",closed,True,2017-09-15 14:24:18,2017-10-18 11:05:54
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/349,https://api.github.com/repos/kubernetes/autoscaler/issues/349,Fix calculating autoprovisioned property for node pool.,,closed,True,2017-09-15 15:14:53,2017-09-15 21:12:26
autoscaler,rhenretta,https://github.com/kubernetes/autoscaler/issues/350,https://api.github.com/repos/kubernetes/autoscaler/issues/350,No candidates for scale down,"I just deployed node-autoscaler into our kubernetes cluster (1.7.6).  However, it can't scale down from the current 4 nodes:

```
I0915 16:59:40.636472       1 cluster.go:75] Fast evaluation: ip-10-176-50-10.ec2.internal for removal
I0915 16:59:40.636534       1 cluster.go:89] Fast evaluation: node ip-10-176-50-10.ec2.internal cannot be removed: non-daemonset, non-mirrored, non-pdb-assigned kube-system pod present: nginx-internal-backend-2910303434-0rh7v
I0915 16:59:40.636544       1 cluster.go:75] Fast evaluation: ip-10-176-51-149.ec2.internal for removal
I0915 16:59:40.636612       1 cluster.go:89] Fast evaluation: node ip-10-176-51-149.ec2.internal cannot be removed: non-daemonset, non-mirrored, non-pdb-assigned kube-system pod present: nginx-apigateway-backend-3016603240-t1vd0
I0915 16:59:40.636621       1 cluster.go:75] Fast evaluation: ip-10-176-50-250.ec2.internal for removal
I0915 16:59:40.636641       1 cluster.go:89] Fast evaluation: node ip-10-176-50-250.ec2.internal cannot be removed: non-daemonset, non-mirrored, non-pdb-assigned kube-system pod present: cluster-autoscaler-148424491-043dw
I0915 16:59:40.636648       1 cluster.go:75] Fast evaluation: ip-10-176-52-16.ec2.internal for removal
I0915 16:59:40.636667       1 cluster.go:89] Fast evaluation: node ip-10-176-52-16.ec2.internal cannot be removed: non-daemonset, non-mirrored, non-pdb-assigned kube-system pod present: ingress-nginx-external-1518343398-v215p
I0915 16:59:40.636682       1 static_autoscaler.go:292] Starting scale down
I0915 16:59:40.636695       1 scale_down.go:246] No candidates for scale down
```

I don't know what non-pdb-assigned means, but all of these are controlled via a replicaset and are mirrored with 2 pods each.

For example:
```
$ kubectl describe replicaset nginx-internal-backend-2910303434 --namespace kube-system
Name:		nginx-internal-backend-2910303434
Namespace:	kube-system
Selector:	app=nginx-internal-backend,k8s-addon=ingress-nginx.addons.k8s.io,pod-template-hash=2910303434
Labels:		app=nginx-internal-backend
		k8s-addon=ingress-nginx.addons.k8s.io
		pod-template-hash=2910303434
Annotations:	deployment.kubernetes.io/desired-replicas=2
		deployment.kubernetes.io/max-replicas=3
		deployment.kubernetes.io/revision=1
Controlled By:	Deployment/nginx-internal-backend
Replicas:	2 current / 2 desired
Pods Status:	2 Running / 0 Waiting / 0 Succeeded / 0 Failed
```",closed,False,2017-09-15 17:05:26,2017-11-03 09:24:12
autoscaler,felipejfc,https://github.com/kubernetes/autoscaler/issues/351,https://api.github.com/repos/kubernetes/autoscaler/issues/351,Make it optional if user wants cluster autoscaler to delete nodes despite of it having not replicated pods,"I have an use case where I need to deploy a lot of pods without any replication controller, cluster autoscaler detects that some nodes are under used but refuses to scale them down because they contains this pods that are not under any replication controller... can we make this optional? I'ld be very nice to have a flag for this.

Regards",closed,False,2017-09-15 21:32:48,2019-01-25 15:42:06
autoscaler,rushtehrani,https://github.com/kubernetes/autoscaler/pull/352,https://api.github.com/repos/kubernetes/autoscaler/issues/352,add missing GPU counts for p2 instance types,Add the missing GPU counts for [p2 instances types](https://aws.amazon.com/ec2/instance-types/),closed,True,2017-09-17 01:08:03,2017-09-17 05:01:19
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/353,https://api.github.com/repos/kubernetes/autoscaler/issues/353,Move sig vc call to Monday,cc: @DirectXMan12 @MaciekPytel @aleksandra-malinowska @bskiba,closed,True,2017-09-18 21:56:28,2017-09-19 09:06:37
autoscaler,mtcode,https://github.com/kubernetes/autoscaler/pull/354,https://api.github.com/repos/kubernetes/autoscaler/issues/354,Introduce new flags to control scale down behavior,"Added scale-down-delay-after-delete to control delay after deleting a node.
Renamed scale-down-trial-interval to scale-down-delay-after-failure and fixed comments to reflect its actual intent per @mwielgus
Renamed scale-down-delay to scale-down-delay-after-add
Updated some logging and comments",closed,True,2017-09-19 00:17:58,2017-09-19 16:35:38
autoscaler,Anson2048,https://github.com/kubernetes/autoscaler/issues/355,https://api.github.com/repos/kubernetes/autoscaler/issues/355,Why the kubernetes node has not been deleted,"From autoscale log

```
I0919 13:59:01.857268   79806 scale_down.go:502] Scale-down: removing empty node 192.168.161.62
I0919 13:59:01.857338   79806 scale_down.go:502] Scale-down: removing empty node 192.168.161.64
I0919 13:59:01.857444   79806 factory.go:33] Event(v1.ObjectReference{Kind:""ConfigMap"", Namespace:""kube-system"", Name:""cluster-autoscaler-status"", UID:""daaaff49-9c3b-11e7-a00b-001ec9386b01"", APIVersion:""v1"", ResourceVersion:""1356511"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDownEmpty' Scale-down: removing empty node 192.168.161.62
I0919 13:59:01.857483   79806 factory.go:33] Event(v1.ObjectReference{Kind:""ConfigMap"", Namespace:""kube-system"", Name:""cluster-autoscaler-status"", UID:""daaaff49-9c3b-11e7-a00b-001ec9386b01"", APIVersion:""v1"", ResourceVersion:""1356511"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDownEmpty' Scale-down: removing empty node 192.168.161.64
I0919 13:59:01.857503   79806 factory.go:33] Event(v1.ObjectReference{Kind:""Node"", Namespace:"""", Name:""192.168.161.64"", UID:""9f05e446-946d-11e7-a00b-001ec9386b01"", APIVersion:""v1"", ResourceVersion:""1356513"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' node removed by cluster autoscaler
I0919 13:59:01.857519   79806 factory.go:33] Event(v1.ObjectReference{Kind:""Node"", Namespace:"""", Name:""192.168.161.62"", UID:""905ca33b-946d-11e7-a00b-001ec9386b01"", APIVersion:""v1"", ResourceVersion:""1356526"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' node removed by cluster autoscaler
I0919 13:59:02.144584   79806 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0919 13:59:04.156980   79806 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0919 13:59:06.178452   79806 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
```

From kubectl event

```
❯ kubectl get event
LASTSEEN   FIRSTSEEN   COUNT     NAME                        KIND        SUBOBJECT   TYPE      REASON           SOURCE               MESSAGE
1m         1m          1         cluster-autoscaler-status   ConfigMap               Normal    ScaleDownEmpty   cluster-autoscaler   Scale-down: removing empty node 192.168.161.62
1m         1m          1         cluster-autoscaler-status   ConfigMap               Normal    ScaleDownEmpty   cluster-autoscaler   Scale-down: removing empty node 192.168.161.64
1m         1m          1         cluster-autoscaler-status   ConfigMap               Normal    ScaleDownEmpty   cluster-autoscaler   Scale-down: removing empty node 192.168.161.246
```

Node list

```
❯ kubectl get node
NAME              STATUS     AGE       VERSION
192.168.161.246   Ready      5d        v1.6.4
192.168.161.62    Ready      10d       v1.6.4
192.168.161.63    Ready      10d       v1.6.4
192.168.161.64    Ready      10d       v1.6.4
```

You can see that the node has not been deleted and no node events recorded",closed,False,2017-09-19 06:05:46,2017-09-21 09:46:46
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/356,https://api.github.com/repos/kubernetes/autoscaler/issues/356,Cloudprovider/gce/gce_manager.go unit tests.,,closed,True,2017-09-19 08:24:44,2017-09-19 14:51:40
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/357,https://api.github.com/repos/kubernetes/autoscaler/issues/357,Cloudprovider/gce/gce_cloud_provider.go unit tests.,,closed,True,2017-09-20 05:36:48,2017-09-20 19:33:18
autoscaler,moos3,https://github.com/kubernetes/autoscaler/issues/358,https://api.github.com/repos/kubernetes/autoscaler/issues/358,Odd memory issue with cluster-autoscaler.,"I'm trying to get the autoscaler working with a cluster that was created with KOPS. I'm seeing this error when the pod comes up.

```I0920 20:37:26.489510       1 reflector.go:236] Listing and watching *v1.ReplicationController from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72
I0920 20:37:26.489846       1 reflector.go:198] Starting reflector *v1beta1.ReplicaSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72
I0920 20:37:26.489903       1 reflector.go:236] Listing and watching *v1beta1.ReplicaSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72
I0920 20:37:26.490205       1 reflector.go:198] Starting reflector *v1beta1.StatefulSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72
I0920 20:37:26.490259       1 reflector.go:236] Listing and watching *v1beta1.StatefulSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72
I0920 20:37:26.490452       1 reflector.go:236] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:144
I0920 20:37:26.490655       1 reflector.go:236] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:169
I0920 20:37:26.490747       1 reflector.go:236] Listing and watching *v1.Service from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72
I0920 20:37:26.490946       1 reflector.go:236] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:207
I0920 20:37:26.491113       1 reflector.go:236] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:237
I0920 20:37:26.491289       1 reflector.go:236] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:259
I0920 20:37:26.491654       1 reflector.go:198] Starting reflector *v1.PersistentVolumeClaim (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72
I0920 20:37:26.491712       1 reflector.go:236] Listing and watching *v1.PersistentVolumeClaim from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72
I0920 20:37:26.492011       1 reflector.go:198] Starting reflector *v1.PersistentVolume (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72
I0920 20:37:26.492075       1 reflector.go:236] Listing and watching *v1.PersistentVolume from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/factory.go:72
I0920 20:37:26.688727       1 request.go:638] Throttling request took 196.449667ms, request: GET:https://100.64.0.1:443/api/v1/persistentvolumes?resourceVersion=0
I0920 20:37:26.896315       1 request.go:638] Throttling request took 97.840641ms, request: PUT:https://100.64.0.1:443/api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status
I0920 20:37:26.988531       1 main.go:162] Registered cleanup signal handler
I0920 20:37:28.410225       1 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0920 20:37:30.420620       1 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0920 20:37:32.595659       1 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0920 20:37:34.689220       1 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
I0920 20:37:36.700092       1 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x30 pc=0x208794a]
 goroutine 45 [running]:
k8s.io/autoscaler/cluster-autoscaler/core.(*PollingAutoscaler).Poll(0xc420885180, 0x4, 0xed154c890)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/polling_autoscaler.go:95 +0x9a
k8s.io/autoscaler/cluster-autoscaler/core.(*PollingAutoscaler).RunOnce(0xc420885180, 0xed154c890, 0xe3aeeac2c, 0x39bb020, 0x39bb020, 0x7ffe9508661e)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/polling_autoscaler.go:68 +0x76
main.run(0xc420367bd0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:201 +0x4d8
main.main.func2(0xc42061e060)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:279 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/leaderelection.(*LeaderElector).Run
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/leaderelection/leaderelection.go:150 +0x97```",closed,False,2017-09-20 20:40:18,2017-09-22 18:08:08
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/359,https://api.github.com/repos/kubernetes/autoscaler/issues/359,Keep graceful termination timeout consistent,"After increasing max graceful termination timeout to 10 minutes when evicting pods, we should probably also increase the time we allow other pods to terminate before disregarding them in simulation.",closed,True,2017-09-21 10:42:31,2017-09-21 11:55:02
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/360,https://api.github.com/repos/kubernetes/autoscaler/issues/360,Fix leaking taints in case of cloud provider error on node deletion,"In case when node was successfully drained, but cloud provider error prevented its deletion, ToBeDeleted taint remains on the node. It happened in this test run:

https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce-autoscaling-migs/6052",closed,True,2017-09-21 15:36:36,2017-09-22 20:43:56
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/361,https://api.github.com/repos/kubernetes/autoscaler/issues/361,Add metric counting failed scale-ups,A minor refactor was required to avoid cyclic imports,closed,True,2017-09-22 14:59:05,2017-09-22 20:36:40
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/362,https://api.github.com/repos/kubernetes/autoscaler/issues/362,Log event when removing unregistered node,,closed,True,2017-09-22 15:49:03,2017-09-22 21:21:40
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/363,https://api.github.com/repos/kubernetes/autoscaler/issues/363,Godep sync 0.7 rc,"Sync with 
```
+commit bfab46cf2543ac17f8c0e75b80631555ac7a6aed (origin/release-1.8)
+Merge: a213f6cf88 46ff2c44c7
+Author: Adam Worrall <abw@google.com>
+Date:   Thu Sep 21 16:17:36 2017 -0700
+
+    Merge remote-tracking branch 'origin/master' into release-1.8
+
```


",closed,True,2017-09-22 16:09:07,2017-09-22 23:00:07
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/364,https://api.github.com/repos/kubernetes/autoscaler/issues/364,Move spammy logs to V(5),"This logs can sometimes be useful for debugging, but with more than a few hundred pods generate massive amount of spam in logs.",closed,True,2017-09-22 17:36:20,2017-09-22 21:53:59
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/365,https://api.github.com/repos/kubernetes/autoscaler/issues/365,Remove overriding allocatable in simulations,,closed,True,2017-09-22 20:56:41,2017-09-22 21:18:03
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/366,https://api.github.com/repos/kubernetes/autoscaler/issues/366,Bump CA version to 0.7.0-beta2,,closed,True,2017-09-22 22:07:43,2017-09-22 23:00:17
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/367,https://api.github.com/repos/kubernetes/autoscaler/issues/367,"Unit tests for scale-up, scale-down and price expander",,closed,True,2017-09-25 09:37:56,2017-09-25 11:59:22
autoscaler,sttts,https://github.com/kubernetes/autoscaler/issues/368,https://api.github.com/repos/kubernetes/autoscaler/issues/368,Cut off k8s.io/kubernetes dependencies,"This repo has quite a number of k8s.io/kubernetes dependencies which probably should not exist. E.g. the `schedulercache.NodeInfo` is an internal of the scheduler and should not leak into other repos. 

Hence, I suggest to proactively cut off these dependencies, either by copying them here or by promoting the relevant k8s.io/kubernetes code to k8s.io/client-go or k8s.io/api.",closed,False,2017-09-25 12:17:48,2017-11-03 09:23:17
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/369,https://api.github.com/repos/kubernetes/autoscaler/issues/369,Unregistered node can't be removed when min size of group is reached,"In [this](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/ci-kubernetes-e2e-gke-ubuntustable1-k8sdev-autoscaling/1144) failed test run, we encountered an edge case where a broken node couldn't be removed from the cluster due to node group minimum size restriction. This causes CA to fall into a loop attempting to remove this node. The impact of this bug is significantly reduced by liveness probe (see below.)

Scenario:
1. node group with size n and minSize <=n, with one of the nodes in this node group remaining unregistered 
2. after 15 minutes of normal operation, CA will attempt to remove the node
3. if node group is still at or below its minSize, CA will repeat this attempt in every iteration (and keep failing)
4. within 10-20 minutes, liveness probe will fail due to repeated failures and restart CA
5. go to 2.

Impact:
1. restarts by liveness probe give 15 minutes in which, if a demand for scale up of the affected node group occurs, the problem could be resolved (after the affected node group's size is increased, it should be possible to remove unregistered node) 
2. however, if cluster activity causes only other node groups to be scaled up/down, the result is that CA works approximately 50% of the time (15 minutes on, ~15 minutes off)
3. in e2e tests, it causes all the remaining scenarios to fail.

Proposed solutions:
1. implement a backoff when attempting to remove unregistered node
2. extend cloudprovider interface to allow for overriding min size restriction on node deletion

cc @MaciekPytel ",closed,False,2017-09-25 13:51:44,2017-09-28 13:01:33
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/370,https://api.github.com/repos/kubernetes/autoscaler/issues/370,Add reason field to faied_scale_ups_total metric,"For now it's just a placeholder, will add proper logic
for next release",closed,True,2017-09-25 14:35:36,2017-09-25 15:36:58
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/371,https://api.github.com/repos/kubernetes/autoscaler/issues/371,Ignore unregistered nodes if nodegroup min size reached,This is a quick temporary fix for #369,closed,True,2017-09-25 14:55:38,2017-09-25 17:28:29
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/372,https://api.github.com/repos/kubernetes/autoscaler/issues/372,Mark Cluster Autoscaler as GA (1.0.0),,closed,True,2017-09-25 17:43:24,2017-09-25 18:46:42
autoscaler,nikhita,https://github.com/kubernetes/autoscaler/pull/373,https://api.github.com/repos/kubernetes/autoscaler/issues/373,Fix link after design proposal move,The design proposals were organized according to SIGs in kubernetes/community#1010. This led to a broken link.,closed,True,2017-09-25 18:58:28,2017-09-25 21:18:50
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/374,https://api.github.com/repos/kubernetes/autoscaler/issues/374,Fix setting target size for group in core/static_autoscaler_test.go.,,closed,True,2017-09-26 08:55:07,2017-09-26 12:23:49
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/375,https://api.github.com/repos/kubernetes/autoscaler/issues/375,Add failed scale-up reason in metric,,closed,True,2017-09-26 11:43:50,2017-09-26 17:23:48
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/376,https://api.github.com/repos/kubernetes/autoscaler/issues/376,Cluster Autoscaler scalability testing report,This report will be linked to from CA release notes.,closed,True,2017-09-26 16:33:45,2017-09-26 17:22:38
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/377,https://api.github.com/repos/kubernetes/autoscaler/issues/377,Compute allocatable for auto-provisioned migs,Compute allocatable for auto-provisioned mig templates. Reserved is calculated as a ratio of capacity. Allocatable is capacity - reserved. ,closed,True,2017-09-27 10:11:43,2017-09-27 10:51:51
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/378,https://api.github.com/repos/kubernetes/autoscaler/issues/378,Update script for testing broken nodes in CA,Added option for only breaking a few nodes not all of them.,closed,True,2017-09-27 12:25:58,2017-09-27 13:20:12
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/379,https://api.github.com/repos/kubernetes/autoscaler/issues/379,Keep track of nodes that failed to register for a long time,"Fix: #369 

Previously a node that failed to register and couldn't be deleted basically broke CA. Now we will keep track of it in clusterstate and continue operating mostly normally.",closed,True,2017-09-27 14:56:18,2017-11-17 10:58:20
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/380,https://api.github.com/repos/kubernetes/autoscaler/issues/380,Add failed scale-up reason in metric,Cherry-pick of #375 to 1.0 branch,closed,True,2017-09-28 15:54:27,2017-10-03 15:05:56
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/381,https://api.github.com/repos/kubernetes/autoscaler/issues/381,Keep track of nodes that failed to register for a long time,Cherry-pick of #379 to release 1.0,closed,True,2017-09-28 15:58:37,2017-10-18 11:03:49
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/382,https://api.github.com/repos/kubernetes/autoscaler/issues/382,Update README with CA release 1.0 info,,closed,True,2017-10-02 11:34:43,2017-10-02 14:54:52
autoscaler,shyamjvs,https://github.com/kubernetes/autoscaler/issues/383,https://api.github.com/repos/kubernetes/autoscaler/issues/383,Create a well-defined RBAC role for cluster-autoscaler,"FWIU currently cluster-autoscaler runs as part of the master node's hostNetwork talking to the apiserver over localhost, bypassing auth. It has cluster-admin powers.
It should instead use a least privilege role (with corresponding binding), just like other control-plane components and addons.

cc @MaciekPytel @bskiba @aleksandra-malinowska @mwielgus ",closed,False,2017-10-03 10:22:27,2018-06-12 00:29:13
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/384,https://api.github.com/repos/kubernetes/autoscaler/issues/384,Refresh mig cache if node pool config has changed,"When updating mig config (min/max nodes) we were not refreshing mig cache, meaning some methods would still return old version of a given mig. This lead to inconsistent state, where different parts of CA code used different min/max size constraints for the same mig.

Also updated the code to only build test node template when the mig config has changed in anyway, not every time we poll GKE API.

Finally updated unittests to work with this changes, decoupling some unittests from fetchAllNodePools() call they were using for test setup and not as a part of actual test.",closed,True,2017-10-03 14:43:52,2017-10-03 15:05:35
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/385,https://api.github.com/repos/kubernetes/autoscaler/issues/385,CA in GA - FAQ,,closed,True,2017-10-03 14:55:15,2017-10-03 15:31:22
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/386,https://api.github.com/repos/kubernetes/autoscaler/issues/386,SLO for CA in FAQ,,closed,True,2017-10-03 16:18:06,2017-10-05 19:36:50
autoscaler,kawych,https://github.com/kubernetes/autoscaler/pull/387,https://api.github.com/repos/kubernetes/autoscaler/issues/387,Use ComponentConfig in Addon Resizer,,closed,True,2017-10-09 11:46:18,2017-10-18 11:05:31
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/issues/388,https://api.github.com/repos/kubernetes/autoscaler/issues/388,VPA Initializer refactoring,"Folowup to #143

Need to:
1. Add RBACs needed for Initializer to work with new Kubernetes versions
2. Restructure code to be more like a regular controller
3. Add configmap to keep initializer config (see [kubernetes initializer tutorial](https://github.com/kelseyhightower/kubernetes-initializer-tutorial) and [initializers best practices](https://github.com/kelseyhightower/kubernetes-initializer-tutorial/blob/master/docs/best-practices.md))",closed,False,2017-10-10 16:27:24,2018-01-08 17:35:37
autoscaler,komljen,https://github.com/kubernetes/autoscaler/issues/389,https://api.github.com/repos/kubernetes/autoscaler/issues/389,Replace large node with a smaller one when using least-waste,"I would like for autoscaler to be able to replace the larger node with a smaller one. For example, I have one pod running on m3.large and another one on m3.medium. Those nodes are two IGs in the same zone. Both can scale up to 5 nodes (0->5).

It should probably start another node in m3.medium IG, move pod to it and terminate the larger instance.

AC 0.6.2 with Kubernetes 1.6.6.
",closed,False,2017-10-12 14:42:55,2018-03-15 10:31:44
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/390,https://api.github.com/repos/kubernetes/autoscaler/issues/390,Add scalability testing report to 1.0.0 release notes,,closed,True,2017-10-13 11:56:05,2017-10-13 19:12:20
autoscaler,orkhanM,https://github.com/kubernetes/autoscaler/issues/391,https://api.github.com/repos/kubernetes/autoscaler/issues/391,Autoscaler scales down AWS ASGs unrelated to cluster.,Noticed kube-autoscaler was scaling ASGs unrelated to the cluster. I spun up a cluster with kube-aws with kube-autoscaler enabled in an existing VPC and subnet. Noticed Autoscaling groups being scaled by kube-autoscaler that shouldn't have been. ,closed,False,2017-10-13 15:34:52,2017-10-13 17:43:28
autoscaler,vishh,https://github.com/kubernetes/autoscaler/issues/392,https://api.github.com/repos/kubernetes/autoscaler/issues/392,Handle autoscaling of GPUs in GKE,"Instance templates have accelerators included in them. The resource type expected to be used for GKE clusters to access GPUs is `nvidia.com/gpus`. Autoscalar should add to and remove from node-pools that have `nvidia-tesla-[k80|p100]` accelerator type included in them.

If the node pool containing GPUs is not the only node pool it is preferrable to scale all the way down to zero.
Output from gcloud instance-templates describe
```
guestAccelerators:
  - acceleratorCount: 8
    acceleratorType: nvidia-tesla-k80
```

cc @mwielgus ",closed,False,2017-10-13 23:20:32,2017-10-18 09:31:21
autoscaler,mattdornfeld,https://github.com/kubernetes/autoscaler/issues/393,https://api.github.com/repos/kubernetes/autoscaler/issues/393,Does the use of cluster autoscaler require cloud provider to be specified for the whole cluster or just for the autoscaler deployment?,I don't see this anywhere in the docs. Maybe this could be made a little clearer? Thanks!,closed,False,2017-10-14 19:28:26,2017-11-03 09:21:43
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/394,https://api.github.com/repos/kubernetes/autoscaler/issues/394,Fix setting target size for group in core/static_autoscaler_test.go.,https://github.com/kubernetes/autoscaler/pull/374,closed,True,2017-10-16 12:53:18,2017-10-16 13:53:09
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/395,https://api.github.com/repos/kubernetes/autoscaler/issues/395,Gke fixit,"First batch of bugfixes for GKE cloudprovider:
 * Mig.TargetSize tried to query non-existing mig if autoprovisioning
      was enabled.
 * GKE client was created using hosted master project instead of users
   project.
 * Renamed clusterName flag to cluster-name for consistency.


",closed,True,2017-10-16 13:13:53,2017-10-16 13:43:38
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/396,https://api.github.com/repos/kubernetes/autoscaler/issues/396,NAP bugfixes,"Fixed some issues related to NAP on GKE: 
 * Make autoprovisioned node pools names shorter (previously it
    was exceeding max name length).
 * Set autoscaling fields when creating node pool.
 * Handle NodeGroup id changing when NG is created.

Fixes in this PR are enough for CA to create and delete autoprovisioned nodepool, though there are some smaller issues remaining (new node group is reported as unhealthy when it's created, errors in logs, missing events). I'll fix those issues in follow-up PR.
",closed,True,2017-10-17 12:11:00,2017-10-17 12:24:55
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/397,https://api.github.com/repos/kubernetes/autoscaler/issues/397,Add the VPA object to the Recommender model.,"Each VPA has a pod selector that matches pods in the cluster based on their
labels. Each VPA object keeps track of all pods it matches and conversely:
each pod keeps track of all VPAs that match it.
While it is incorrect to let multiple VPA objects match the same pod, the model
has no means to prevent such situation. In such case the pod is controlled by
one of the matching VPAs.",closed,True,2017-10-17 13:12:05,2017-10-17 14:24:55
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/398,https://api.github.com/repos/kubernetes/autoscaler/issues/398,Update clusterstate after scale-up,"This stops CA from considering newly created node group as unhealthy and prevents error messages showing in log when NAP is enabled. Also previously status configmap would only inform about scale-up after next loop has completed (10+s delay), now it does it immediately.",closed,True,2017-10-17 14:21:27,2017-10-18 08:59:28
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/399,https://api.github.com/repos/kubernetes/autoscaler/issues/399,Add the VPA object to the Recommender model.,"Each VPA has a pod selector that matches pods in the cluster based on their
labels. Each VPA object keeps track of all pods it matches and conversely:
each pod keeps track of all VPAs that match it.
While it is incorrect to let multiple VPA objects match the same pod, the model
has no means to prevent such situation. In such case the pod is controlled by
one of the matching VPAs.",closed,True,2017-10-17 14:25:41,2017-11-15 15:56:39
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/400,https://api.github.com/repos/kubernetes/autoscaler/issues/400,PollingAutoscaler leaking memory and goroutines?,"In AWS cloudprovider newAutoScalingGroups is creating a goroutine for regenerating ASG cache, which is never stopped. After cursory glance at PollingAutoscaler and surrounding code it looks as if a new such goroutine is created in every loop (as part of creating AWS cloudprovider). If that's the case we're probably leaking a goroutine + autoScalingGroups data structure every loop. 
@mumoshu am I missing something?",closed,False,2017-10-17 14:47:18,2017-10-18 15:39:47
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/401,https://api.github.com/repos/kubernetes/autoscaler/issues/401,Support GPUs in scale from 0,"When scaling up from 0, read information about GPUs from instance template. This allows us to support scaling up a relevant NodeGroup from 0 when we have a pending pod that needs GPUs.",closed,True,2017-10-18 09:17:47,2017-10-19 17:30:03
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/402,https://api.github.com/repos/kubernetes/autoscaler/issues/402,Cherrypick allocatable,Ref: https://github.com/kubernetes/autoscaler/pull/377,closed,True,2017-10-18 11:09:11,2017-10-18 11:09:33
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/403,https://api.github.com/repos/kubernetes/autoscaler/issues/403,Cherrypick Compute allocatable for auto-provisioned migs,Ref: https://github.com/kubernetes/autoscaler/pull/377,closed,True,2017-10-18 11:10:50,2017-10-18 12:06:00
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/404,https://api.github.com/repos/kubernetes/autoscaler/issues/404,Cherrypick Fix some bugs in GKE cloudprovider,Ref: https://github.com/kubernetes/autoscaler/pull/395 (one of 2 commits in this PR),closed,True,2017-10-18 11:16:48,2017-10-18 12:06:26
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/405,https://api.github.com/repos/kubernetes/autoscaler/issues/405,Bump 1.0 release to 1.0.1-beta1,,closed,True,2017-10-18 12:16:18,2017-10-18 12:17:16
autoscaler,orkhanM,https://github.com/kubernetes/autoscaler/issues/406,https://api.github.com/repos/kubernetes/autoscaler/issues/406,Autoscaler modifies ASG resources that it shouldn't,"On a kube-aws ( version v0.9.8 ) cluster with cluster autoscaler addon enabled I noticed the autoscaler was scaling down ASG's unrelated to the cluster. 

The autoscaler manifest looks like:

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels: 
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels: 
        app: cluster-autoscaler
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:   
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: ""kube-aws.coreos.com/cluster-autoscaler-supported""
                operator: ""In""
                values: 
                - ""true""
      tolerations:
      - key: ""node.alpha.kubernetes.io/role""
        operator: ""Equal"" 
        value: ""master""
        effect: ""NoSchedule""
      - key: ""CriticalAddonsOnly""
        operator: ""Exists""
      containers:
        - image: gcr.io/google_containers/cluster-autoscaler:v0.6.0
          name: cluster-autoscaler
          resources:
            limits: 
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4 
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/production-k8s-02
          env:    
            - name: AWS_REGION
              value: us-east-1
          imagePullPolicy: ""Always""
```

An excerpt from the cluster-autoscaler logs:

```
I1009 22:20:48.400952       1 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler\
I1009 22:20:48.662164       1 auto_scaling_groups.go:94] Regenerating ASG information for CoreUpdate-ASG-1TFRMDY7ZBDXB\
I1009 22:20:48.814109       1 auto_scaling_groups.go:94] Regenerating ASG information for Jenkins-Linux-HCM-ASG-CXZPKHJBVBMC\
I1009 22:20:48.848760       1 auto_scaling_groups.go:94] Regenerating ASG information for Jenkins-Linux-HCM-ASG-Spot\
I1009 22:20:48.893549       1 auto_scaling_groups.go:94] Regenerating ASG information for Jenkins-Linux-Slaves-ASG-KW2EKO1HQGSY\
I1009 22:20:48.939883       1 auto_scaling_groups.go:94] Regenerating ASG information for Jenkins-Windows-OPS01-ASG-KA5DIDGLDP5B\
I1009 22:20:49.000438       1 auto_scaling_groups.go:94] Regenerating ASG information for Production-vpc-00fd2379-Services01-ASG-UAFQVWLSRY6Z\
I1009 22:20:49.064799       1 auto_scaling_groups.go:94] Regenerating ASG information for Production-vpc-00fd2379-Services02-ASG-1VMX2FTPJ1ZT5\
I1009 22:20:49.115805       1 auto_scaling_groups.go:94] Regenerating ASG information for Production-vpc-00fd2379-Web01-ASG-1N4JRPQU0MYZ2\
I1009 22:20:49.161246       1 auto_scaling_groups.go:94] Regenerating ASG information for Production-vpc-00fd2379-Web02-ASG-1ANYJ6OIK5C2\
I1009 22:20:49.196010       1 auto_scaling_groups.go:94] Regenerating ASG information for Registry-Example-Com-AutoScalingGroup-FG40W31U9ZHL\
I1009 22:20:49.236423       1 auto_scaling_groups.go:94] Regenerating ASG information for Registry-Example-Com-CLAIR-AutoScalingGroup-Y0JX9W3VA29A\
I1009 22:20:49.275025       1 auto_scaling_groups.go:94] Regenerating ASG information for looker-nodes.production-k8s-01.example.com\
I1009 22:20:49.316710       1 auto_scaling_groups.go:94] Regenerating ASG information for looker-nodes.production-k8s.example.com\
I1009 22:20:49.360140       1 auto_scaling_groups.go:94] Regenerating ASG information for master-us-east-1a.masters.production-k8s-01.example.com\
I1009 22:20:49.433501       1 auto_scaling_groups.go:94] Regenerating ASG information for master-us-east-1a.masters.production-k8s.example.com\
I1009 22:20:49.471324       1 auto_scaling_groups.go:94] Regenerating ASG information for master-us-east-1b.masters.production-k8s-01.example.com\
I1009 22:20:49.501557       1 auto_scaling_groups.go:94] Regenerating ASG information for master-us-east-1b.masters.production-k8s.example.com\
I1009 22:20:49.569452       1 auto_scaling_groups.go:94] Regenerating ASG information for master-us-east-1c.masters.production-k8s-01.example.com\
I1009 22:20:49.707034       1 auto_scaling_groups.go:94] Regenerating ASG information for master-us-east-1c.masters.production-k8s.example.com\
I1009 22:20:49.746283       1 auto_scaling_groups.go:94] Regenerating ASG information for master-us-east-1d.masters.production-k8s-01.example.com\
I1009 22:20:49.778591       1 auto_scaling_groups.go:94] Regenerating ASG information for master-us-east-1d.masters.production-k8s.example.com\
I1009 22:20:49.830307       1 auto_scaling_groups.go:94] Regenerating ASG information for master-us-east-1e.masters.production-k8s-01.example.com\
I1009 22:20:49.873971       1 auto_scaling_groups.go:94] Regenerating ASG information for master-us-east-1e.masters.production-k8s.example.com\
I1009 22:20:49.919504       1 auto_scaling_groups.go:94] Regenerating ASG information for nodes-core.production-k8s.example.com\
I1009 22:20:50.414791       1 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler\
I1009 22:20:50.536753       1 auto_scaling_groups.go:94] Regenerating ASG information for nodes.production-k8s-01.example.com\
I1009 22:20:50.581501       1 auto_scaling_groups.go:94] Regenerating ASG information for production-k8s-02-Controlplane-1EQ9EQZSGTVAN-Controllers-1MJB0LU5KF4TO\
I1009 22:20:50.627064       1 auto_scaling_groups.go:94] Regenerating ASG information for production-k8s-02-Controlplane-1EQ9EQZSGTVAN-Etcd0-1XUNPV1PHJF9G\
I1009 22:20:50.659466       1 auto_scaling_groups.go:94] Regenerating ASG information for production-k8s-02-Controlplane-1EQ9EQZSGTVAN-Etcd1-WE30HM714T51\
I1009 22:20:50.699992       1 auto_scaling_groups.go:94] Regenerating ASG information for production-k8s-02-Controlplane-1EQ9EQZSGTVAN-Etcd2-R2SAF483UQAJ\
I1009 22:20:50.742502       1 auto_scaling_groups.go:94] Regenerating ASG information for production-k8s-02-Controlplane-1EQ9EQZSGTVAN-Etcd3-DVGPKGKIHKH7\
I1009 22:20:50.773041       1 auto_scaling_groups.go:94] Regenerating ASG information for production-k8s-02-Controlplane-1EQ9EQZSGTVAN-Etcd4-LOZU27BQ9UFO\
I1009 22:20:50.806447       1 auto_scaling_groups.go:94] Regenerating ASG information for production-k8s-02-Spotworkers-LJSGSQAREX7R-Workers-ZNY07LBGNIQI\
I1009 22:20:50.857753       1 auto_scaling_groups.go:94] Regenerating ASG information for production-k8s-02-Workers-1650RNVJX2B6F-Workers-NEGMM0Y4SX9G\
I1009 22:20:50.935470       1 auto_scaling_groups.go:94] Regenerating ASG information for spot-nodes-core.production-k8s.example.com\
I1009 22:20:50.974711       1 auto_scaling_groups.go:94] Regenerating ASG information for spot-nodes.production-k8s-01.example.com\
I1009 22:20:52.430606       1 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler\
I1009 22:20:54.444916       1 leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler\
I1009 22:20:56.364412       1 polling_autoscaler.go:105] Poll finished\
```

All ASGs referenced in the above log with the prefix `production-k8s-02-Controlplane` are the ASG's I expected to be effected and they provision the EC2 instances on which the cluster-autoscaler runs. All other ASG's mentioned are not part of the cluster and should not be touched by autoscaler. ",closed,False,2017-10-18 17:48:56,2018-03-30 00:15:54
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/407,https://api.github.com/repos/kubernetes/autoscaler/issues/407,Fix GPU resource name.,"The v1.resourceNvidiaGPU resource name will soon no longer be valid. The correct resource name to use is ""nvidia.com/gpu""",closed,True,2017-10-19 12:04:06,2018-02-13 14:58:04
autoscaler,chhibber,https://github.com/kubernetes/autoscaler/issues/408,https://api.github.com/repos/kubernetes/autoscaler/issues/408,CA 1.0 - Unable to scale p2.xlarge or p2.8xlarge from 0,"Running into the exact same issue documented here: https://github.com/kubernetes/autoscaler/issues/346

The AWS json file being pulled down still has missing data regarding GPUs.  The p2.16xlarge has the GPU enumerated and scales up, the rest do not.   

Logs...
```
I1013 22:10:11.784396       1 scale_up.go:140] Scale-up predicate failed: PodFitsResources predicate mismatch, cannot put default/tf-deployment-84fffcbdc-vnkl2 on template-node-for-gpunodes.gpu.sono-k8.xxxxx-2740376916591569721, reason: Insufficient alpha.kubernetes.io/nvidia-gpu
I1013 22:10:11.784519       1 scale_up.go:140] Scale-up predicate failed: PodFitsResources predicate mismatch, cannot put default/tf-deployment-6d6fd6cd-k77r5 on template-node-for-gpunodes.gpu.sono-k8.xxxxx-2740376916591569721, reason: Insufficient alpha.kubernetes.io/nvidia-gpu
I1013 22:10:11.784553       1 scale_up.go:140] Scale-up predicate failed: PodFitsResources predicate mismatch, cannot put default/tf-deployment-6d6fd6cd-nnhjr on template-node-for-gpunodes.gpu.sono-k8.xxxxx-2740376916591569721, reason: Insufficient alpha.kubernetes.io/nvidia-gpu
```

I applied the the fix described in issue 346,  though I added info explicitly for all p2 instance types as I am not sure if this a general inconsistency with the AWS json file.   After the fix I was able to scale P2 instances up from 0.

Diff:

```
diff --git a/cluster-autoscaler/cloudprovider/aws/ec2_instance_types/gen.go b/cluster-autoscaler/cloudprovider/aws/ec2_instance_types/gen.go
index 6f8b347e..4537037c 100644
--- a/cluster-autoscaler/cloudprovider/aws/ec2_instance_types/gen.go
+++ b/cluster-autoscaler/cloudprovider/aws/ec2_instance_types/gen.go
@@ -144,6 +144,15 @@ func main() {
 					if attr.GPU != """" {
 						instanceTypes[attr.InstanceType].GPU = parseCPU(attr.GPU)
 					}
+					if attr.InstanceType == ""p2.xlarge"" {
+						instanceTypes[attr.InstanceType].GPU = 1
+					}
+					if attr.InstanceType == ""p2.8xlarge"" {
+						instanceTypes[attr.InstanceType].GPU = 8
+					}
+					if attr.InstanceType == ""p2.16xlarge"" {
+						instanceTypes[attr.InstanceType].GPU = 16
+					}
 				}
 			}
 		}
```

It might be worth leveraging something like http://www.ec2instances.info/ in the interim? As I understand it internal teams in AWS leverage the site even. 

I have a build that I put up and tested here:

- https://hub.docker.com/r/chhibber/cluster-scaler/
- or docker pull chhibber/cluster-scaler",closed,False,2017-10-19 15:43:17,2017-11-03 09:21:05
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/409,https://api.github.com/repos/kubernetes/autoscaler/issues/409,Fix typo in test,,closed,True,2017-10-20 08:03:04,2017-10-20 08:19:07
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/410,https://api.github.com/repos/kubernetes/autoscaler/issues/410,Cluster Autoscaler 1.0.1,,closed,True,2017-10-20 13:23:07,2017-10-20 13:25:01
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/411,https://api.github.com/repos/kubernetes/autoscaler/issues/411,Adds priority preemption support to cluster autoscaler.,,closed,True,2017-10-20 14:40:12,2017-11-08 18:02:03
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/412,https://api.github.com/repos/kubernetes/autoscaler/issues/412,spec client for VPA recommender ,"+ recommender containerization and deployment

Currently it just prints out specification to the logs.

@mwielgus 
@kgrygiel ",closed,True,2017-10-22 17:22:56,2018-01-09 13:53:10
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/413,https://api.github.com/repos/kubernetes/autoscaler/issues/413,Use GKE alpha client when autoprovisioning is enabled,"Use v1alpha1 API if autoprovisioning is enabled. This is a temporary solution until NAP fields are available in normal client.

This is likely my ugliest PR in kubernetes to date :(",closed,True,2017-10-23 13:22:38,2017-10-23 18:36:42
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/414,https://api.github.com/repos/kubernetes/autoscaler/issues/414,Adds resource limits to cloud provider.,,closed,True,2017-10-23 13:30:31,2017-10-23 18:38:05
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/415,https://api.github.com/repos/kubernetes/autoscaler/issues/415,Enable cluster-level CA config in GKE alpha client,,closed,True,2017-10-23 14:27:39,2017-10-24 11:13:31
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/416,https://api.github.com/repos/kubernetes/autoscaler/issues/416,Add Refresh method to cloud provider,"This can be used to dynamically update cloud provider
config (in particular list of managed NodeGroups and their
min/max constraints).
Add GKE implementation.",closed,True,2017-10-24 16:37:41,2017-10-25 04:38:33
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/417,https://api.github.com/repos/kubernetes/autoscaler/issues/417,Fix a bug with parsing int64 in GKE alpha client,,closed,True,2017-10-24 16:54:02,2017-10-24 17:59:57
autoscaler,Henrod,https://github.com/kubernetes/autoscaler/pull/418,https://api.github.com/repos/kubernetes/autoscaler/issues/418,"If correct annotation on pod, drain node even if not replicated","# Non replicated pod annotation

If a pod is not replicated, the auto-scaler doesn't drain the node it is in. This PR enables to do it if the pods have an annotation. ",closed,True,2017-10-24 22:52:24,2018-05-25 17:34:40
autoscaler,drinktee,https://github.com/kubernetes/autoscaler/issues/419,https://api.github.com/repos/kubernetes/autoscaler/issues/419,Shall we add some trace log for core pkg?,"In k8s there are trace logs such as
```
	trace := utiltrace.New(""syncReplicationController: "" + key)
	defer trace.LogIfLong(250 * time.Millisecond)
```
I think it's useful to add some similar trace log for methods in core pkg...
I will do this if this's helpful.",closed,False,2017-10-25 08:56:12,2018-03-25 05:22:51
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/420,https://api.github.com/repos/kubernetes/autoscaler/issues/420,Write events when autoprovisioned nodegroup is created / deleted,,closed,True,2017-10-25 12:33:32,2017-10-26 05:35:02
autoscaler,mmerrill3,https://github.com/kubernetes/autoscaler/pull/421,https://api.github.com/repos/kubernetes/autoscaler/issues/421,AWS DescribeAutoScalingGroups requests too aggressive - API limits reached,"This is a pull request for https://github.com/kubernetes/autoscaler/issues/252, @MaciekPytel 
",closed,True,2017-10-25 18:50:48,2017-10-25 19:38:58
autoscaler,mmerrill3,https://github.com/kubernetes/autoscaler/pull/422,https://api.github.com/repos/kubernetes/autoscaler/issues/422,AWS DescribeAutoScalingGroups requests too aggressive - API limits reached,Fix for issue 252 where excessive API calls to AWS are the result of leaking go routines from the Polling auto scaler.,closed,True,2017-10-25 21:01:01,2017-11-01 21:04:08
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/423,https://api.github.com/repos/kubernetes/autoscaler/issues/423,Gets resource limits from GKE API.,,closed,True,2017-10-26 07:53:20,2017-10-30 11:21:36
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/424,https://api.github.com/repos/kubernetes/autoscaler/issues/424,Adds gke-api-endpoint flag to GCE could provider.,,closed,True,2017-10-26 07:58:29,2017-10-26 08:54:22
autoscaler,Henrod,https://github.com/kubernetes/autoscaler/pull/425,https://api.github.com/repos/kubernetes/autoscaler/issues/425,Observation about safe-to-evict annotation on FAQ,"# safe-to-evict annotation on FAQ

Simple observation about the new annotation from commit 56135db3b01c379b6fbb65e9d23d75e1a5c90130. 

@MaciekPytel Would it be better to have a separate question? Or is this enough?",closed,True,2017-10-26 15:41:28,2017-11-17 10:57:18
autoscaler,sethpollack,https://github.com/kubernetes/autoscaler/pull/426,https://api.github.com/repos/kubernetes/autoscaler/issues/426,update aws instance types,fixes gpu issue in #321 and #346,closed,True,2017-10-29 01:51:28,2017-10-31 18:25:29
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/427,https://api.github.com/repos/kubernetes/autoscaler/issues/427,Fixes for GKE cloudprovider and autoprovisioning,Cherrypick various fixes for node autoprovisioning on GKE.,closed,True,2017-10-30 11:31:17,2017-10-30 11:55:01
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/428,https://api.github.com/repos/kubernetes/autoscaler/issues/428,Cluster Autoscaler integration with regional clusters.,"Make GKE cloud provider work with locations instead of zones.
Use beta GKE endpoint for regional clusters.",closed,True,2017-10-30 13:22:43,2017-10-30 14:21:26
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/429,https://api.github.com/repos/kubernetes/autoscaler/issues/429,update GKE cloudprovider to match changes in GKE API,,closed,True,2017-10-30 18:06:29,2017-11-21 10:22:55
autoscaler,amila-ku,https://github.com/kubernetes/autoscaler/pull/430,https://api.github.com/repos/kubernetes/autoscaler/issues/430,updated IAM policy resource section with specifying aws arn for autos…,Updated the IAM policy to have resources specified as AWS arn based syntax for auto scaling groups.,closed,True,2017-10-31 05:31:24,2017-10-31 05:43:24
autoscaler,amila-ku,https://github.com/kubernetes/autoscaler/pull/431,https://api.github.com/repos/kubernetes/autoscaler/issues/431,updated IAM policy resource section with aws arn for autoscaling,Updated the IAM policy to have resources specified as AWS arn based syntax for auto scaling groups.,closed,True,2017-10-31 05:45:32,2017-11-08 16:01:11
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/432,https://api.github.com/repos/kubernetes/autoscaler/issues/432,Call location API for regional clusters.,,closed,True,2017-10-31 09:28:30,2017-10-31 11:18:03
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/433,https://api.github.com/repos/kubernetes/autoscaler/issues/433,Cherrypick changes to support GPUs in scale from 0.,,closed,True,2017-10-31 14:35:44,2017-10-31 14:48:14
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/434,https://api.github.com/repos/kubernetes/autoscaler/issues/434,Cherrypick changes to support regional clusters.,,closed,True,2017-10-31 14:55:52,2017-10-31 15:28:24
autoscaler,SafPlusPlus,https://github.com/kubernetes/autoscaler/pull/435,https://api.github.com/repos/kubernetes/autoscaler/issues/435,Trivial typo fix,,closed,True,2017-10-31 15:28:17,2017-12-06 12:45:19
autoscaler,hemanthmalla,https://github.com/kubernetes/autoscaler/pull/436,https://api.github.com/repos/kubernetes/autoscaler/issues/436,Updating zoom meeting link,Updating README with the new zoom link based on the previous meeting.,closed,True,2017-10-31 17:10:01,2017-10-31 18:49:21
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/437,https://api.github.com/repos/kubernetes/autoscaler/issues/437,Bump CA version to 1.0.2-beta1,,closed,True,2017-10-31 17:14:07,2017-10-31 17:37:23
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/438,https://api.github.com/repos/kubernetes/autoscaler/issues/438,Make ResourceLimiter memory representation consistent,"Our API accepts memory limits in gigabytes, kubernetes keeps memory in bytes and we store it internally in megabytes. We should decide on one approach (bytes I think?) and update the code accordingly.

This issue is a follow up after discussion with @mwielgus on #429.

cc: @aleksandra-malinowska @krzysztof-jastrzebski ",closed,False,2017-10-31 17:45:38,2018-05-05 09:02:05
autoscaler,sethpollack,https://github.com/kubernetes/autoscaler/pull/439,https://api.github.com/repos/kubernetes/autoscaler/issues/439,update aws instance types,,closed,True,2017-10-31 19:50:01,2017-11-01 08:13:01
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/440,https://api.github.com/repos/kubernetes/autoscaler/issues/440,Add metrics for autoprovisioning,,closed,True,2017-10-31 20:23:48,2017-11-01 05:36:54
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/441,https://api.github.com/repos/kubernetes/autoscaler/issues/441,Cherrypick update GKE cloudprovider on release 1.0,Cherrypick #429 ,closed,True,2017-11-01 09:52:20,2017-11-01 15:02:24
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/442,https://api.github.com/repos/kubernetes/autoscaler/issues/442,Cherrypick metrics for autoprovisioning,Cherrypick #440 ,closed,True,2017-11-01 09:56:11,2017-11-01 15:01:54
autoscaler,etsangsplk,https://github.com/kubernetes/autoscaler/issues/443,https://api.github.com/repos/kubernetes/autoscaler/issues/443,"[minor] misspelling ""overriden"" is a misspelling of ""overridden"" ","/kind bug
k8s.io/autoscaler/addon-resizer/nanny
nanny/nanny_lib.go:55:44:warning: ""overriden"" is a misspelling of ""overridden"" (misspell)",closed,False,2017-11-01 18:28:18,2017-11-03 09:19:27
autoscaler,etsangsplk,https://github.com/kubernetes/autoscaler/pull/444,https://api.github.com/repos/kubernetes/autoscaler/issues/444,fix spelling for issue 443,fix spelling ,closed,True,2017-11-01 18:30:30,2017-11-03 07:36:34
autoscaler,johanneswuerbach,https://github.com/kubernetes/autoscaler/pull/445,https://api.github.com/repos/kubernetes/autoscaler/issues/445,Cherry-pick AWS DescribeAutoScalingGroups requests too aggressive to 1.0,Cherry-pick https://github.com/kubernetes/autoscaler/pull/422,closed,True,2017-11-01 21:03:49,2017-11-02 19:15:06
autoscaler,ashish-amarnath,https://github.com/kubernetes/autoscaler/issues/446,https://api.github.com/repos/kubernetes/autoscaler/issues/446,Cluster autoscaler views all nodes in the cluster as empty and beings bulk deleting them,"This caused a cluster meltdown for us and unfortunately, we don't have enough data to exactly know what happened at that point. This was seen in the v0.5.4 release.

Also after we upgraded to 0.6.2, we saw log messages like 
```
I1025 18:52:49.810360       1 scale_down.go:156] Node ip-172-27-190-244.us-west-2.compute.internal - utilization 0.000000
I1025 18:52:49.810365       1 scale_down.go:156] Node ip-172-27-190-60.us-west-2.compute.internal - utilization 0.000000
I1025 18:52:49.810371       1 scale_down.go:156] Node ip-172-27-186-53.us-west-2.compute.internal - utilization 0.000000 
```
This happened for ~5s and then non-zero the utilization numbers started to show up. 

Were there any other reports of such behaviour from the cluster-autoscaler. If so, were there any specific changes targetting this either in 0.6.2 or the next release?
",closed,False,2017-11-02 23:48:22,2017-11-06 18:52:00
autoscaler,etsangsplk,https://github.com/kubernetes/autoscaler/issues/447,https://api.github.com/repos/kubernetes/autoscaler/issues/447,possible to upgrade aws-sdk-go tp v1.8x?,"Looks like we are stil using v.1.6.10.... possible to update to v1.8.x?

",closed,False,2017-11-03 19:31:21,2017-11-06 08:43:44
autoscaler,etsangsplk,https://github.com/kubernetes/autoscaler/pull/448,https://api.github.com/repos/kubernetes/autoscaler/issues/448,update aws.sdk.go to v.1.8.x (issues/447),"for issue:
https://github.com/kubernetes/autoscaler/issues/447",closed,True,2017-11-03 19:32:43,2017-11-03 22:28:06
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/issues/449,https://api.github.com/repos/kubernetes/autoscaler/issues/449,Add Azure support,"https://github.com/kubernetes/autoscaler/pull/229 removed Azure support temporarily, we should add Azure support back.

TODOs:

- [x] Bring azure cloud provider back #454 
- [x] Add VMSS support
- [x] Add Availability Set ( standard VM type) support #514
- [x] Multiple node groups #542
- ~~TemplateNodeInfo not supported~~
- [x] Reduce API calls and avoid touching Azure rate limits #552
- [ ] Add AKS support
- [x] Unit tests
- [x] Documentations #641

Depends on: Support VMSS in azure cloud provider https://github.com/kubernetes/kubernetes/issues/43287",closed,False,2017-11-06 05:26:06,2018-03-29 02:01:11
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/450,https://api.github.com/repos/kubernetes/autoscaler/issues/450,Cluster Autoscaler 1.0.2,,closed,True,2017-11-06 11:00:10,2017-11-06 17:32:05
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/451,https://api.github.com/repos/kubernetes/autoscaler/issues/451,[WIP] Add a workaround for scaling of VMs with GPUs,"When a machine with GPU becomes ready it can take
up to 15 minutes before it reports that GPU is allocatable.
This can cause Cluster Autoscaler to trigger a second
unnecessary scale up.
The workaround sets allocatable to capacity for GPU so that
a node that waits for GPUs to become ready to use will be
considered as a place where pods requesting GPUs can be
scheduled.",closed,True,2017-11-06 12:30:43,2017-11-08 08:06:17
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/452,https://api.github.com/repos/kubernetes/autoscaler/issues/452,Update README with release notes for 1.0.1 and 1.0.2,,closed,True,2017-11-06 13:08:58,2017-11-17 10:57:30
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/453,https://api.github.com/repos/kubernetes/autoscaler/issues/453,WIP: get GPU capacity from node template,"DO NOT MERGE

This is a POC hack to temporarily solve the problem of GPU showing up in resources up to 15 minutes after node is created. Based on https://github.com/kubernetes/autoscaler/pull/451, except this is an even worse hack. Also no unittest and the code is messy. I'll clean it up if we decide to actually do this.",closed,True,2017-11-06 18:14:43,2017-11-10 17:07:05
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/454,https://api.github.com/repos/kubernetes/autoscaler/issues/454,Add azure cloud provider,"This is the first step of adding Azure cloud provider back (#449 and https://github.com/kubernetes/kubernetes/issues/47511). Mainly changes includes:

- bring old azure cloud provider code back
- update azure sdk to latest stable v11.1.1
- update azure sdk interfaces
- add cloud provider interface initial implementations
- unit tests

Note: Kubernetes Azure cloud provider VMSS support is not finished yet, so this work is tested togather with https://github.com/kubernetes/kubernetes/pull/55833.

And there are more work to do to enable fully azure support, which is tracking at #449.

Closes: #133",closed,True,2017-11-07 01:39:11,2017-11-23 01:24:16
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/455,https://api.github.com/repos/kubernetes/autoscaler/issues/455,Fix release notes for 1.0.2,,closed,True,2017-11-07 10:42:06,2017-11-17 10:57:28
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/456,https://api.github.com/repos/kubernetes/autoscaler/issues/456,Update metrics documentation,,closed,True,2017-11-07 12:47:18,2017-11-17 10:57:27
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/457,https://api.github.com/repos/kubernetes/autoscaler/issues/457,Adds Priority and Pod Preemption to Cluster Autoscaled FAQ.,,closed,True,2017-11-09 09:17:07,2017-11-09 10:04:45
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/458,https://api.github.com/repos/kubernetes/autoscaler/issues/458,Fix typo in CA FAQ.,,closed,True,2017-11-09 10:08:55,2017-11-09 10:09:23
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/459,https://api.github.com/repos/kubernetes/autoscaler/issues/459,Adds KUBE_AUTOSCALER_ENABLE_SCALE_DOWN export to FAQ,Adds KUBE_AUTOSCALER_ENABLE_SCALE_DOWN export to e2e test question in CA FAQ.,closed,True,2017-11-09 12:15:35,2017-11-17 10:57:25
autoscaler,sergeylanzman,https://github.com/kubernetes/autoscaler/pull/460,https://api.github.com/repos/kubernetes/autoscaler/issues/460,Replace deprecate kubernetes client functions,,closed,True,2017-11-09 17:50:31,2017-11-22 14:58:01
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/461,https://api.github.com/repos/kubernetes/autoscaler/issues/461,Consider GPU nodes unready until allocatable GPU is > 0,"Fix for https://github.com/kubernetes/kubernetes/issues/54959.

The idea is to internally override status of nodes which should have GPUs to unready until they actually have allocatable GPU. That way our normal logic for handling long starting up nodes kicks in and counts those nodes in simulations.

The downside is we have to use the same timeout for GPU node start-up + driver installation as for normal nodes (defaults to 15 minutes, which may not be enough - we may need to temporarily increase this for 1.8).

Also fixes the issue where GCP pricing model was using old GPU resource name and not taking GPU into account when calculating node cost (leading to very sad scale-up decisions in my test...).",closed,True,2017-11-10 16:57:00,2017-11-13 14:29:28
autoscaler,negz,https://github.com/kubernetes/autoscaler/pull/462,https://api.github.com/repos/kubernetes/autoscaler/issues/462,Support autodetection of GCE managed instance groups by name prefix,"This commit adds a new usage of the `--node-group-auto-discovery` flag intended for use with the GCE cloud provider. GCE instance groups can be automatically discovered based on a prefix of their group name. Example usage:
```
--node-group-auto-discovery=mig:prefix=k8s-mig,minNodes=0,maxNodes=10
```
Note that unlike the existing AWS ASG autodetection functionality we must specify the min and max nodes in the flag. This is because MIGs store only a target size in the GCE API - they do not have a min and max size we can infer via the API.

In order to alleviate this limitation a little we allow multiple uses of the autodiscovery flag. For example to discover two classes (big and small) of instance groups with different size limits:
```
./cluster-autoscaler \
  --node-group-auto-discovery=mig:prefix=k8s-a-small,minNodes=1,maxNodes=10 \
  --node-group-auto-discovery=mig:prefix=k8s-a-big,minNodes=1,maxNodes=100
```
Zonal clusters (i.e. `multizone = false` in the cloud config) will detect all managed instance groups within the cluster's zone. Regional clusters will detect all matching (zonal) managed instance groups within any of the cluster's region's zones.",closed,True,2017-11-11 04:56:23,2018-02-27 22:42:38
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/463,https://api.github.com/repos/kubernetes/autoscaler/issues/463,ClusterAutoscaler 1.1.0-alpha1,,closed,True,2017-11-13 12:13:56,2017-11-13 14:30:12
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/464,https://api.github.com/repos/kubernetes/autoscaler/issues/464,Use node's allocatable resources for computing utilization,"Currently, we're considering node's allocatable resources during scale-up, but capacity when considering whether the node should be removed. We should probably use allocatable value there as well.",closed,False,2017-11-13 14:22:18,2017-12-28 14:49:21
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/465,https://api.github.com/repos/kubernetes/autoscaler/issues/465,Update FAQ,"Fixed multiple typos, commas, refactored sections a bit.",closed,True,2017-11-13 17:59:23,2017-11-16 10:33:02
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/466,https://api.github.com/repos/kubernetes/autoscaler/issues/466,Use a single flag for dealing with unregistered nodes,"Currently there are two flags, --max-node-provisioning-time and --unregistered-node-removal-time, which we implicitly expect to be set to the same value. These are closely coupled as they both determine whether we consider an unregistered node as not-yet-ready or broken. We should merge them and use one flag instead of two for those parameters.

cc @MaciekPytel ",closed,False,2017-11-14 14:05:14,2017-11-16 12:07:53
autoscaler,negz,https://github.com/kubernetes/autoscaler/pull/467,https://api.github.com/repos/kubernetes/autoscaler/issues/467,Add an expander that always expands the group with the fewest nodes.,"Consider this an RFC. I haven't really thought through the implications.

Typically my clusters are setup to scale a homogenous set of node groups. I need only one flavour of node, but I want my nodes to be spread across all zones in an AWS or GCE region. I create one node group per zone so the CA can make intelligent scaling decisions for pods that must be created in a particular zone (i.e. to access cloud provider backed physical volumes).

When using the random expander with 3-4 node groups I notice it often picks an already quite large node group to expand. I end up with some node groups that have orders of magnitude more nodes than others for no real reason. My hope is that defaulting to embiggening the smallest node group will help smooth this out a little.",closed,True,2017-11-15 03:44:49,2017-11-15 20:21:47
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/468,https://api.github.com/repos/kubernetes/autoscaler/issues/468,Cherry-pick safe-to-evict annotation to 1.0.3,Cherry-pick of #418,closed,True,2017-11-15 10:34:10,2017-11-15 15:55:32
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/469,https://api.github.com/repos/kubernetes/autoscaler/issues/469,Remove --unregistered-node-removal-time flag,Fixes #466 by removing one of the flags. Left --max-node-provision-time as the name is more precise.,closed,True,2017-11-15 12:50:16,2017-11-16 12:07:53
autoscaler,kawych,https://github.com/kubernetes/autoscaler/issues/470,https://api.github.com/repos/kubernetes/autoscaler/issues/470,Use ComponentConfig in version 2 of addon resizer,Commits from #387 need to be applied on master.,closed,False,2017-11-15 13:51:55,2018-05-04 15:45:05
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/471,https://api.github.com/repos/kubernetes/autoscaler/issues/471,Fix unit static autoscaler unit tests.,,closed,True,2017-11-15 14:29:13,2017-11-17 10:57:21
autoscaler,JackQuincy,https://github.com/kubernetes/autoscaler/issues/472,https://api.github.com/repos/kubernetes/autoscaler/issues/472,Need expander that understands the difference between linux/windows workloads and nodes,"I'm running clusters that have both windows and linux workloads. As far as I can see the expanders don't understand which NodeGroup to expand if only a subset of the NodeGroups can handle the unscheduled workloads is that correct? Or am I missing something here?

I also can see a future where some work loads need a GPU vs not and not all nodes have GPUs available, so this could be something more general like let a node group declare labels it can satisfy. So the expander can look at the unscheduled work load and see if it has any label requirements and only consider NodeGroups that satisfy those requirements. ",closed,False,2017-11-15 18:30:46,2017-11-15 18:32:42
autoscaler,kawych,https://github.com/kubernetes/autoscaler/pull/473,https://api.github.com/repos/kubernetes/autoscaler/issues/473,Bump Addon Resizer version to 1.8.0,,closed,True,2017-11-16 09:45:22,2017-11-16 10:31:25
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/474,https://api.github.com/repos/kubernetes/autoscaler/issues/474,Add support for GPU for NAP,"The user experience will be as follows:
 - If there is a pending pod with GPU request NAP will create a node pool with nvidia P80 for it
 - If the pod uses nodeSelector on `cloud.google.com/gke-accelerator` label NAP will use the value of node selector to determine GPU type (you can get nvidia P100 by setting nodeSelector `cloud.google.com/gke-accelerator: nvidia-tesla-p100`).
 - New node pools with gpu will come with taint `gke-accelerator: <gpu_type>` with no schedule effect (to mitigate the risk of pods not requiring GPU landing on expensive node with GPU, there is no way user can do it themselves in NAP)
 - We validate if a node pool with GPU can be created against criteria specified in https://cloud.google.com/compute/docs/gpus/ (zone with GPU enabled, maximum CPU/GPU ratio, number of GPUs on node)

@vishh Does that look reasonable to you?",closed,True,2017-11-16 16:26:13,2017-12-11 14:20:07
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/475,https://api.github.com/repos/kubernetes/autoscaler/issues/475,Cherry-pick doc updates,"Cherry-pick of #425, #452, #455, #456, and #459.",closed,True,2017-11-17 11:07:13,2017-11-17 11:27:43
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/476,https://api.github.com/repos/kubernetes/autoscaler/issues/476,"Cherry-pick ""Consider GPU nodes unready until allocatable GPU is > 0""",Cherry-pick of #461.,closed,True,2017-11-17 11:10:34,2017-11-17 11:27:14
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/477,https://api.github.com/repos/kubernetes/autoscaler/issues/477,"Cherry-pick ""Fix unit static autoscaler unit tests.""",Cherry-pick of #471.,closed,True,2017-11-17 11:15:00,2017-11-17 11:28:12
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/478,https://api.github.com/repos/kubernetes/autoscaler/issues/478,Extra checks when pushing an image to gcr repository,"Adds:
* Confirmation of pushed image.
* Check whether the image is already there.",closed,True,2017-11-17 11:47:00,2017-11-17 15:12:51
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/479,https://api.github.com/repos/kubernetes/autoscaler/issues/479,Cluster Autoscaler 1.0.3,,closed,True,2017-11-17 12:07:59,2017-11-17 14:40:01
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/480,https://api.github.com/repos/kubernetes/autoscaler/issues/480,Release notes for Cluster Autoscaler 1.0.3,Release notes for Cluster Autoscaler 1.0.3,closed,True,2017-11-17 12:36:23,2017-11-17 12:52:49
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/481,https://api.github.com/repos/kubernetes/autoscaler/issues/481,Release notes for Cluster Autoscaler 1.0.3,Cherry pick of #480.,closed,True,2017-11-17 13:04:18,2017-11-17 14:39:22
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/482,https://api.github.com/repos/kubernetes/autoscaler/issues/482,Extra checks when pushing an image to gcr repository,Cherry-pick of #478.,closed,True,2017-11-17 15:15:34,2017-11-17 15:40:40
autoscaler,miry,https://github.com/kubernetes/autoscaler/pull/483,https://api.github.com/repos/kubernetes/autoscaler/issues/483,Added aws c5 instance types.,"C5 instances: https://www.ec2instances.info/?filter=c5

Added new instances types to aws provider, otherwise we have exception everytime:

```
I1122 07:03:30.997937       1 scale_up.go:54] Pod bi/presto.worker-59788c9f65-565tx is unschedulable
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x145189d]
goroutine 76 [running]:
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).buildNodeFromTemplate(0xc420b1c040, 0xc421bcc000, 0xc421e0c900, 0xc421e0c900, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:252 +0x48d
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*Asg).TemplateNodeInfo(0xc421bcc000, 0xc421dd3b00, 0xc421a2f380, 0x20)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider.go:355 +0x78
k8s.io/autoscaler/cluster-autoscaler/core.GetNodeInfosForGroups(0xc421201080, 0x27, 0x2a, 0x50ba120, 0xc421bc1f80, 0x50c44e0, 0xc420ab5c70, 0xc42116ba80, 0x2, 0x2, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/utils.go:211 +0x65a
k8s.io/autoscaler/cluster-autoscaler/core.ScaleUp(0xc420b9b520, 0xc42116e360, 0x4, 0x4, 0xc421201080, 0x27, 0x2a, 0xc42116ba80, 0x2, 0x2, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:57 +0x28e
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc4218d4230, 0xed1a7183c, 0xe37618c72, 0x518d0a0, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:246 +0x1378
k8s.io/autoscaler/cluster-autoscaler/core.(*PollingAutoscaler).RunOnce(0xc421bc4a40, 0xed1a7183c, 0xe37618c72, 0x518d0a0, 0x518d0a0, 0x4e200)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/polling_autoscaler.go:72 +0x146
main.run(0xc4207158b0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:262 +0x46f
main.main.func2(0xc420bcf2c0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:344 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:145 +0x97
```",closed,True,2017-11-22 07:29:56,2017-11-23 00:13:38
autoscaler,mrcrgl,https://github.com/kubernetes/autoscaler/pull/484,https://api.github.com/repos/kubernetes/autoscaler/issues/484,Added OnDemand and Spot Price models addressing #131,,closed,True,2017-11-22 21:44:10,2017-11-23 12:04:12
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/485,https://api.github.com/repos/kubernetes/autoscaler/issues/485,Azure in Readme(s).md,,closed,True,2017-11-23 07:28:57,2017-11-24 02:36:28
autoscaler,mrcrgl,https://github.com/kubernetes/autoscaler/pull/486,https://api.github.com/repos/kubernetes/autoscaler/issues/486,Added OnDemand and Spot Price models addressing #131,,open,True,2017-11-23 10:27:00,2019-04-03 09:48:54
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/487,https://api.github.com/repos/kubernetes/autoscaler/issues/487,Fix image name for azure,The official image name sould be used instead of a pernal one.,closed,True,2017-11-24 08:24:34,2017-11-24 09:07:40
autoscaler,Gazler,https://github.com/kubernetes/autoscaler/pull/488,https://api.github.com/repos/kubernetes/autoscaler/issues/488,Set code types for syntax highlighting in README.md,,closed,True,2017-11-28 12:47:43,2017-11-28 13:33:40
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/489,https://api.github.com/repos/kubernetes/autoscaler/issues/489,Godep update,"Update against HEAD.
Before K8S 1.9 release there should be yet another godep update, against the final 1.9 branch. ",closed,True,2017-11-28 13:05:01,2017-11-28 14:50:46
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/490,https://api.github.com/repos/kubernetes/autoscaler/issues/490,Cluster Autoscaler 1.1.0-beta1,,closed,True,2017-12-01 11:21:59,2017-12-01 12:09:30
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/491,https://api.github.com/repos/kubernetes/autoscaler/issues/491,DO NOT MERGE - WIP / POC - Refactor cloud provider builder to allow builds for a single provider,Just a POC of building a release with a code for a single cloud provider. Let me know what you think,closed,True,2017-12-01 17:01:49,2018-01-16 13:59:51
autoscaler,kawych,https://github.com/kubernetes/autoscaler/pull/492,https://api.github.com/repos/kubernetes/autoscaler/issues/492,Re-introduce flags that are used to configure Addon Resizer resources if not specified by config map,"This introduces following logic:
- By default, the hardcoded values will be used.
- If flags are specified, they will overwrite defaults in code.
- If config map is specified, any parameters present there will overwrite both flags and defaults from the code.

This covers following scenario:
- Customer wants to overwrite resources requirements for some addon with config map.
- In next kubernetes version, addon resources are adjusted.
- The customer upgrades his cluster. If he has overwritten resource requirements, the values he specified still apply. If he hasn't, addon resource requirements are updated correctly.",closed,True,2017-12-04 14:30:23,2017-12-06 14:13:59
autoscaler,timpalpant,https://github.com/kubernetes/autoscaler/pull/493,https://api.github.com/repos/kubernetes/autoscaler/issues/493,Add h1 and m5 AWS instance types,Regenerated `ec2_instance_types.go` by running `make generate`. This adds support for the new AWS h1 and m5 instance types as in https://github.com/kubernetes/autoscaler/pull/483.,closed,True,2017-12-04 18:31:07,2017-12-07 04:27:48
autoscaler,utsav2307,https://github.com/kubernetes/autoscaler/issues/494,https://api.github.com/repos/kubernetes/autoscaler/issues/494,Mechanism for moving the pods to another node?,"I understand that CA moves the pods to another nodes before deleting a node, but does it do a rolling deploy of pods ensuring no downtime when moving the pods or does it just delete the pod first and then wait for a new pod to come up on another node?",closed,False,2017-12-06 06:27:41,2018-05-07 12:53:07
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/495,https://api.github.com/repos/kubernetes/autoscaler/issues/495,Use bytes instead of MB for memory limits,Fixes #438.,closed,True,2017-12-06 17:20:05,2018-06-08 12:47:23
autoscaler,negz,https://github.com/kubernetes/autoscaler/issues/496,https://api.github.com/repos/kubernetes/autoscaler/issues/496,Support whitelisting labels to ignore when balancing similar node groups,"Running the Cluster Autoscaler with `--balance-similar-node-groups` attempts to keep the size of identical node groups balanced when scaling up.

https://github.com/kubernetes/autoscaler/blob/d43029c/cluster-autoscaler/utils/nodegroupset/compare_nodegroups.go#L57

Per `IsNodeInfoSimilar` node groups are considered similar when a sample node from each node group has approximately the same resource capacity and labels. A fixed set of labels are ignored during this comparison - specifically hostname, failure domain, and region.

At Planet Labs we label nodes with their node group's unique identifier, which we call their pool. This label looks like `kubernetes.planet.com/pool_id=worker-7kd9`. Our node groups are typically identical and thus we would like to balance them, but they are considered dissimilar due to the existence of the `pool_id` label.

We'd like to be able to configure a set of labels for `IsNodeInfoSimilar` to ignore in addition to the hostname, failure domain, and region. This would involve passing the CA a flag like `--ignore-labels-when-comparing-node-groups=kubernetes.planet.com/pool_id,someotherlabel`. This flag would be used to set `IgnoreLabelsWhenComparingNodeGroups` in the autoscaling context, and that setting would be passed down to `IsNodeInfoSimilar` via `FindSimilarNodeGroups`.

Does this sound like a reasonable approach? If so we're happy to send a PR to implement it.",closed,False,2017-12-06 21:00:30,2017-12-18 19:51:32
autoscaler,kawych,https://github.com/kubernetes/autoscaler/pull/497,https://api.github.com/repos/kubernetes/autoscaler/issues/497,Bump Addon-Resizer version to 1.8.1,,closed,True,2017-12-08 11:18:24,2017-12-08 11:50:28
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/498,https://api.github.com/repos/kubernetes/autoscaler/issues/498,Use allocatable instead of capacity to calculate node utilization,Fixes #464.,closed,True,2017-12-08 11:55:36,2017-12-28 14:49:21
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/499,https://api.github.com/repos/kubernetes/autoscaler/issues/499,Cluster Autoscaler 1.1.0,,closed,True,2017-12-08 16:17:35,2017-12-08 16:29:18
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/500,https://api.github.com/repos/kubernetes/autoscaler/issues/500,A few small improvements to the model types.,Small preparatory changes necessary before adding the recommender logic.,closed,True,2017-12-11 14:52:25,2017-12-12 22:27:59
autoscaler,jakexks,https://github.com/kubernetes/autoscaler/issues/501,https://api.github.com/repos/kubernetes/autoscaler/issues/501,Deploying StatefulSet doesn't trigger scale up (AWS with node group auto discovery),"I've been testing AWS Auto Scaling groups that start at 0 replicas, so Kubernetes should spin up worker nodes on demand. With most deployments, this works fine, but StatefulSets seem to fail scale up predicates.

My set-up: ""master"" nodes with a NoSchedule taint, and cluster autoscaler deployed with a toleration
Worker nodes in an ASG that starts at 0 desired.

It seems the NoVolumeZoneConflict predicate isn't satisfied, but if I manually scale the ASG to 3, the nodes that join the cluster are labelled by the AWS cloud provider with their associated zones, and the AWS provisioner creates volumes in the appropriate zones that nodes are in.

Cluster autoscaler config:

```
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=kubernetes.io/cluster-autoscaler/enabled,kubernetes.io/cluster/sandbox
          env:
            - name: AWS_REGION
              value: us-east-1
```

Relevant logs:
```
I1208 18:45:50.157077       1 static_autoscaler.go:97] Starting main loop
I1208 18:45:50.252318       1 static_autoscaler.go:226] Filtering out schedulables
I1208 18:45:50.252734       1 static_autoscaler.go:236] No schedulable pods
I1208 18:45:50.252762       1 scale_up.go:54] Pod default/prometheus-server-6c69d47db7-59cv2 is unschedulable
I1208 18:45:50.252768       1 scale_up.go:54] Pod default/cassandr-0 is unschedulable
I1208 18:45:50.252773       1 scale_up.go:54] Pod default/kafka-0 is unschedulable
I1208 18:45:50.252778       1 scale_up.go:54] Pod default/zookeeper-0 is unschedulable
I1208 18:45:50.310334       1 scale_up.go:86] Upcoming 0 nodes
I1208 18:45:50.366792       1 scale_up.go:146] Scale-up predicate failed: MaxEBSVolumeCount predicate error, cannot put default/prometheus-server-6c69d47db7-59cv2 on template-node-for-asg-worker-node-prod-us-east-1-5732434843220369820 due to, error PersistentVolumeClaim is not bound: “prometheus-server”
I1208 18:45:50.367125       1 scale_up.go:146] Scale-up predicate failed: NoVolumeZoneConflict predicate mismatch, cannot put default/cassandr-0 on template-node-for-asg-worker-node-prod-us-east-1-5732434843220369820, reason: NoVolumeZoneConflict
I1208 18:45:50.367201       1 scale_up.go:146] Scale-up predicate failed: NoVolumeZoneConflict predicate mismatch, cannot put default/kafka-0 on template-node-for-asg-worker-node-prod-us-east-1-5732434843220369820, reason: NoVolumeZoneConflict
I1208 18:45:50.367467       1 scale_up.go:146] Scale-up predicate failed: NoVolumeZoneConflict predicate mismatch, cannot put default/zookeeper-0 on template-node-for-asg-worker-node-prod-us-east-1-5732434843220369820, reason: NoVolumeZoneConflict
I1208 18:45:50.367486       1 scale_up.go:175] No pod can fit to asg-worker-node-prod-us-east-1
I1208 18:45:50.367518       1 scale_up.go:180] No expansion options
I1208 18:45:50.372452       1 factory.go:33] Event(v1.ObjectReference{Kind:“Pod”, Namespace:“default”, Name:“prometheus-server-6c69d47db7-59cv2"", UID:“75843c35-dc47-11e7-9b20-0a839b0802b4”, APIVersion:“v1"", ResourceVersion:“25815”, FieldPath:“”}): type: ‘Normal’ reason: ‘NotTriggerScaleUp’ pod didn’t trigger scale-up (it wouldn’t fit if a new node is added)
I1208 18:45:50.374761       1 static_autoscaler.go:276] Calculating unneeded nodes
I1208 18:45:50.374845       1 factory.go:33] Event(v1.ObjectReference{Kind:“Pod”, Namespace:“default”, Name:“cassandr-0”, UID:“75ad314c-dc47-11e7-9b20-0a839b0802b4"", APIVersion:“v1”, ResourceVersion:“26005"", FieldPath:“”}): type: ‘Normal’ reason: ‘NotTriggerScaleUp’ pod didn’t trigger scale-up (it wouldn’t fit if a new node is added)
I1208 18:45:50.374900       1 factory.go:33] Event(v1.ObjectReference{Kind:“Pod”, Namespace:“default”, Name:“kafka-0"", UID:“75c3289b-dc47-11e7-9b20-0a839b0802b4”, APIVersion:“v1"", ResourceVersion:“26012”, FieldPath:“”}): type: ‘Normal’ reason: ‘NotTriggerScaleUp’ pod didn’t trigger scale-up (it wouldn’t fit if a new node is added)
I1208 18:45:50.374994       1 factory.go:33] Event(v1.ObjectReference{Kind:“Pod”, Namespace:“default”, Name:“zookeeper-0”, UID:“75b8ec24-dc47-11e7-9b20-0a839b0802b4"", APIVersion:“v1”, ResourceVersion:“26008"", FieldPath:“”}): type: ‘Normal’ reason: ‘NotTriggerScaleUp’ pod didn’t trigger scale-up (it wouldn’t fit if a new node is added)
I1208 18:45:50.413399       1 utils.go:395] Skipping ip-10-37-1-122.ec2.internal - no node group config
I1208 18:45:50.413435       1 utils.go:395] Skipping ip-10-37-0-33.ec2.internal - no node group config
I1208 18:45:50.413587       1 scale_down.go:173] Scale-down calculation: ignoring 1 nodes, that were unremovable in the last 5m0s
I1208 18:45:50.413735       1 static_autoscaler.go:305] Scale down status: unneededOnly=false lastScaleUpTime=2017-12-08 18:09:48.785356299 +0000 UTC lastScaleDownDeleteTime=2017-12-08 16:21:46.066594759 +0000 UTC lastScaleDownFailTime=2017-12-08 16:09:35.537744739 +0000 UTC schedulablePodsPresent=false isDeleteInProgress=false
```",closed,False,2017-12-12 16:03:22,2018-07-21 17:21:53
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/502,https://api.github.com/repos/kubernetes/autoscaler/issues/502,Wait another iteration if the oldest pending pod is just 1-2 seconds old,"Autoscaling / NAP will give better results if it sees the whole picture - for example all pods created by the newly-created deployment, not just the initial fraction.",closed,False,2017-12-13 22:03:44,2017-12-28 16:22:38
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/503,https://api.github.com/repos/kubernetes/autoscaler/issues/503,Don't start metrics endpoint when not on leading master,"Currently, Cluster Autoscaler running on non-leading master will do nothing except publish metrics indicating cluster is unhealthy. 

cc @MaciekPytel ",closed,False,2017-12-14 10:32:41,2017-12-15 17:23:09
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/504,https://api.github.com/repos/kubernetes/autoscaler/issues/504,Don't register metrics unless on leading master,Fix for #503 ,closed,True,2017-12-14 14:59:46,2017-12-15 17:22:30
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/505,https://api.github.com/repos/kubernetes/autoscaler/issues/505,Cleanup comments in azure provider,Cleanup comments and format error message.,closed,True,2017-12-19 03:14:30,2017-12-20 01:40:42
autoscaler,suker200,https://github.com/kubernetes/autoscaler/issues/506,https://api.github.com/repos/kubernetes/autoscaler/issues/506,[CA] Pod pending with Custom Scheduler request for scaling node,"I have written a custom-scheduler in scheduling pod based on node workload. 

[Repo reference](https://github.com/suker200/kube_scheduler_extender)

The ""--verify-unschedulable-pods=false"" option was removed in #189 , which lead to CA will not scale up with custom-scheduler

[IMG reference](https://imgur.com/a/IDLZI)

Do we have any way to trigger CA (api, etc..)?
",closed,False,2017-12-19 09:29:55,2017-12-21 04:26:51
autoscaler,spiffxp,https://github.com/kubernetes/autoscaler/pull/507,https://api.github.com/repos/kubernetes/autoscaler/issues/507,Add code-of-conduct.md,"Refer to kubernetes/community as authoritative source for code of conduct

ref: kubernetes/community#1527",closed,True,2017-12-20 18:31:31,2017-12-21 23:38:45
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/508,https://api.github.com/repos/kubernetes/autoscaler/issues/508,Skip iteration if pending pods are too new,Fixes #502,closed,True,2017-12-20 20:20:39,2017-12-28 16:22:39
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/509,https://api.github.com/repos/kubernetes/autoscaler/issues/509,Introduce the PodResourceRecommender type.,"Provide a simple implementation of the recommender that calculates
recommended resources based on provided percentiles of the CPU usage
and memory peaks distributions.",closed,True,2017-12-21 17:13:02,2018-01-03 09:51:37
autoscaler,vainu-arto,https://github.com/kubernetes/autoscaler/issues/510,https://api.github.com/repos/kubernetes/autoscaler/issues/510,Add annotation to stop the autoscaler from moving a specific pod,"There is an annotation to stop specific nodes from being removed, but not one for marking pods as ""do not touch"". The ""cluster-autoscaler.kubernetes.io/safe-to-evict"": ""true"" annotation only seems to work one way (you can use it to mark a pod as safe to move, not to mark a pod as unsafe to move).",closed,False,2017-12-22 08:07:39,2018-07-11 12:45:17
autoscaler,brianwallace,https://github.com/kubernetes/autoscaler/issues/511,https://api.github.com/repos/kubernetes/autoscaler/issues/511,CA v0.6.2 on AWS: Unable to scale up from zero nodes,"This is likely not a bug report, but more of a request for help.

I am running `cluster-autoscaler` v0.6.2 in a 1.6.x k8s environment on AWS.  I used kops to setup this environment.  (I realize that there are newer CA versions out there, but I am trying to use one that is compatible with k8s 1.6.x.)

Scale down appears to work, but I am unable to scale up nodes from zero.  This seems very similar to https://github.com/kubernetes/autoscaler/issues/346, but the main difference is that this is not for GPU hosts.

Here are the `cluster-autoscaler` logs.  I am attempting to scale up my test instance group called `scale-zero`:

```
I1221 21:51:55.791507       1 scale_up.go:114] Scale-up predicate failed: GeneralPredicates predicate mismatch, cannot put default/nettools-813038286-4w8xs on template-node-for-compute.mycluster.k8s.mysite.io-8210888063207898651, reason: MatchNodeSelector
I1221 21:51:55.791542       1 scale_up.go:143] No pod can fit to %scompute.mycluster.k8s.mysite.io
I1221 21:51:55.827193       1 scale_up.go:114] Scale-up predicate failed: GeneralPredicates predicate mismatch, cannot put default/nettools-813038286-4w8xs on template-node-for-compute-gpu.mycluster.k8s.mysite.io-344101245308204321, reason: MatchNodeSelector
I1221 21:51:55.827227       1 scale_up.go:143] No pod can fit to %scompute-gpu.mycluster.k8s.mysite.io
I1221 21:51:55.866515       1 scale_up.go:114] Scale-up predicate failed: GeneralPredicates predicate mismatch, cannot put default/nettools-813038286-4w8xs on template-node-for-scale-zero.mycluster.k8s.mysite.io-792335616856378590, reason: MatchNodeSelector
I1221 21:51:55.866549       1 scale_up.go:143] No pod can fit to %sscale-zero.mycluster.k8s.mysite.io
I1221 21:51:55.902377       1 scale_up.go:114] Scale-up predicate failed: GeneralPredicates predicate mismatch, cannot put default/nettools-813038286-4w8xs on template-node-for-nodes.mycluster.k8s.mysite.io-8738462660441552742, reason: MatchNodeSelector
I1221 21:51:55.902411       1 scale_up.go:143] No pod can fit to %snodes.mycluster.k8s.mysite.io
I1221 21:51:55.902420       1 scale_up.go:148] No expansion options
I1221 21:51:55.902458       1 static_autoscaler.go:270] Scale down status: unneededOnly=false lastScaleUpTime=2017-12-21 21:36:40.03458167 +0000 UTC lastScaleDownFailedTrail=2017-12-21 21:36:40.034582092 +0000 UTC schedulablePodsPresent=false
I1221 21:51:55.902476       1 static_autoscaler.go:272] Calculating unneeded nodes
I1221 21:51:55.902521       1 utils.go:360] Skipping ip-10-0-224-154.us-west-2.compute.internal - no node group config
I1221 21:51:55.902689       1 event.go:218] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""nettools-813038286-4w8xs"", UID:""5038258c-e696-11e7-a1f4-0a49a4dc60e0"", APIVersion:""v1"", ResourceVersion:""66932870"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```

And CA configs:

```
        - --cloud-provider=aws
        - --alsologtostderr
        - --v=4
        - --expander=least-waste
        - --scale-down-enabled=true
        - --scale-down-unneeded-time=10m
        - --nodes=1:50:compute.mycluster.k8s.mysite.io
        - --nodes=1:30:compute-gpu.mycluster.k8s.mysite.io
        - --nodes=0:3:scale-zero.mycluster.k8s.mysite.io
        - --nodes=2:30:nodes.mycluster.k8s.mysite.io
```

The pod I am attempting so schedule in the `scale-zero` instance group looks like this:

```
> kubectl describe po nettools-813038286-4w8xs
Name:           nettools-813038286-4w8xs
Namespace:      default
Node:           <none>
Labels:         pod-template-hash=813038286
                run=nettools
Annotations:    kubernetes.io/created-by={""kind"":""SerializedReference"",""apiVersion"":""v1"",""reference"":{""kind"":""ReplicaSet"",""namespace"":""default"",""name"":""nettools-813038286"",""uid"":""5035fbe1-e696-11e7-a1f4-0a49a4dc60e0""...
                kubernetes.io/limit-ranger=LimitRanger plugin set: cpu request for container nettools
Status:         Pending
IP:
Created By:     ReplicaSet/nettools-813038286
Controlled By:  ReplicaSet/nettools-813038286
Containers:
  nettools:
    Image:  ejether/nettools
    Port:   <none>
    Command:
      sleep
      3600
    Requests:
      cpu:        100m
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-k091d (ro)
Conditions:
  Type           Status
  PodScheduled   False
Volumes:
  default-token-k091d:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-k091d
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  mysite.io/role=scale-zero
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason             Age                   From                Message
  ----     ------             ----                  ----                -------
  Warning  FailedScheduling   48s (x3153 over 16h)  default-scheduler   No nodes are available that match all of the following predicates:: MatchNodeSelector (24), PodToleratesNodeTaints (3).
  Normal   NotTriggerScaleUp  6s (x6378 over 19h)   cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```

I attempted to introspect the `scale-zero` template node, but it does not appear to exist (am I doing this correctly??):

```
> kubectl describe no template-node-for-scale-zero.mycluster.k8s.mysite.io-792335616856378590
Error from server (NotFound): nodes ""template-node-for-scale-zero.mycluster.k8s.mysite.io-792335616856378590"" not found
```",closed,False,2017-12-22 17:20:16,2017-12-31 17:58:44
autoscaler,thockin,https://github.com/kubernetes/autoscaler/pull/512,https://api.github.com/repos/kubernetes/autoscaler/issues/512,Convert registry to k8s.gcr.io,"This PR was auto-generated.  Please apply human expertise to review for correctness.

Followup to https://github.com/kubernetes/kubernetes/pull/54174

xref https://github.com/kubernetes/release/issues/281",closed,True,2017-12-22 18:00:59,2017-12-28 09:19:27
autoscaler,riywo,https://github.com/kubernetes/autoscaler/issues/513,https://api.github.com/repos/kubernetes/autoscaler/issues/513,Update documentation for CA on AWS about IAM policy,"Hi,

In the document below says ""Unfortunately AWS does not support ARNs for autoscaling groups yet so you must use ""*"" as the resource.""
https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws#permissions

AWS have launched the feature in May 2017, so the document can now say ""You can control fine-grained resource level permission using `Resource`"".
https://aws.amazon.com/about-aws/whats-new/2017/05/introducing-auto-scaling-resource-level-permissions/

Best,",closed,False,2017-12-28 03:27:23,2018-05-17 00:44:22
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/514,https://api.github.com/repos/kubernetes/autoscaler/issues/514,Add support for Azure virtual machine availability sets (VMAS),"Continue of #449.

This PR adds initial support for Azure virtual machine availability sets (VMAS). Significant changes include

- Add `vmType` parameters to indicate different virtual machine types. Candidate values are 'vmss' and 'standard'
- Add agent pools implementation (standard vmType) of NodeGroup
- Add more options for scaling agent pools
- Add more fake clients and unit tests

Special notes for reviewers:

- Only one node group supported yet
- Some functions (in azure_util.go) are copied from acs-engine. They are not ventured because of various vendor problems, e.g. i18n and go template. acs-engine should be adjusted before vendoring those functions
- Azure API calls may hit requests count limits which need to be addressed in further PRs 

/cc @brendandburns @andyzhangx @jdumars

/assign @mwielgus @MaciekPytel 

",closed,True,2017-12-28 04:45:51,2017-12-30 02:16:42
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/515,https://api.github.com/repos/kubernetes/autoscaler/issues/515,Support go1.9 when verifying gofmt,We should support go1.9 when verifying gofmt.,closed,True,2017-12-28 05:27:43,2017-12-29 01:59:13
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/516,https://api.github.com/repos/kubernetes/autoscaler/issues/516,Flake in AWS cloud provider tests,"```
--- FAIL: TestFetchAutoAsgs (0.00s)
panic: 
mock: Unexpected Method Call
-----------------------------
DescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)
		0: {
  AutoScalingGroupNames: [""coolasg""],
  MaxRecords: 100
}
		1: (func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)(0x12e7290)
The closest call I have is: 
DescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,mock.AnythingOfTypeArgument)
		0: {
  AutoScalingGroupNames: [""coolasg""],
  MaxRecords: 100
}
		1: ""func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool""
 [recovered]
	panic: 
mock: Unexpected Method Call
-----------------------------
DescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)
		0: {
  AutoScalingGroupNames: [""coolasg""],
  MaxRecords: 100
}
		1: (func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)(0x12e7290)
The closest call I have is: 
DescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,mock.AnythingOfTypeArgument)
		0: {
  AutoScalingGroupNames: [""coolasg""],
  MaxRecords: 100
}
		1: ""func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool""
goroutine 46 [running]:
testing.tRunner.func1(0xc4201d3110)
	/home/travis/.gimme/versions/go1.8.3.linux.amd64/src/testing/testing.go:622 +0x29d
panic(0x14058e0, 0xc42034ba90)
	/home/travis/.gimme/versions/go1.8.3.linux.amd64/src/runtime/panic.go:489 +0x2cf
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/stretchr/testify/mock.(*Mock).MethodCalled(0xc4202b1680, 0x21ad60a, 0x1e, 0xc420302420, 0x2, 0x2, 0x4, 0x10, 0xc4204c56d8)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/stretchr/testify/mock/mock.go:311 +0x654
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/stretchr/testify/mock.(*Mock).Called(0xc4202b1680, 0xc420302420, 0x2, 0x2, 0x10, 0x153cb40, 0x1)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/stretchr/testify/mock/mock.go:288 +0x14c
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AutoScalingMock).DescribeAutoScalingGroupsPages(0xc4202b1680, 0xc420344300, 0xc42034ae80, 0x227c6b8, 0x0)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider_test.go:40 +0xa9
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*autoScalingWrapper).getAutoscalingGroupsByNames(0xc4201afa20, 0xc42034ad40, 0x1, 0x1, 0x1, 0x1, 0x30, 0x7fc0a2bbd000, 0x0)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/auto_scaling.go:89 +0x1c2
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*asgCache).regenerate(0xc4201af9f0, 0x1764fc0, 0xc4201afa18)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/auto_scaling_groups.go:154 +0x33a
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).regenerateCache(0xc4201afa40, 0x0, 0x0)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:266 +0x8d
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).fetchAutoAsgs(0xc4201afa40, 0x0, 0x22600e0)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:198 +0x685
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).forceRefresh(0xc4201afa40, 0x0, 0x0)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:235 +0x40
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.createAWSManagerInternal(0x0, 0x0, 0x0, 0x0, 0x0, 0xc420045ef0, 0x1, 0x1, 0xc420045ee0, 0x0, ...)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:111 +0x21f
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.TestFetchAutoAsgs(0xc4201d3110)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager_test.go:221 +0xab7
testing.tRunner(0xc4201d3110, 0x1762518)
	/home/travis/.gimme/versions/go1.8.3.linux.amd64/src/testing/testing.go:657 +0x96
created by testing.(*T).Run
	/home/travis/.gimme/versions/go1.8.3.linux.amd64/src/testing/testing.go:697 +0x2ca
FAIL	k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws	0.149s
```",closed,False,2017-12-28 15:19:11,2018-08-25 19:07:35
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/517,https://api.github.com/repos/kubernetes/autoscaler/issues/517,Fix error branch in pod initialization,,closed,True,2017-12-28 16:08:29,2017-12-28 22:41:05
autoscaler,whereisaaron,https://github.com/kubernetes/autoscaler/issues/518,https://api.github.com/repos/kubernetes/autoscaler/issues/518,Crashes with cluster-autoscaler 1.0.3 and AWS during cluster start-up,"During cluster start-up the `cluster-autoscaler` may have difficulty auto-discovering AWS ASGs. In these cases, rather than retry, the `cluster-autoscaler` can crash.

I'm launching an cluster-autoscaler pod running on controller/master nodes early during cluster creation and I noticed it crashes a couple of times before settling down. Checking the logs it starts up fine and establishes all its cluster watches. Then a couple minutes after start-up to goes to query AWS to discover ASGs and experiences an TCP timeout. Given this is during the start-up the worker ASGs are still being created, there may be none at this point. The timeout triggers and outright crash.

```
I1228 18:49:16.303288       1 auto_scaling.go:138] Failed to describe ASG tags for keys [k8s.io/cluster-autoscaler/enabled kubernetes.io/cluster/gnat] : RequestError: send request failed
caused by: Post https://autoscaling.us-east-2.amazonaws.com/: dial tcp: i/o timeout
F1228 18:49:16.303338       1 cloud_provider_builder.go:111] Failed to create AWS cloud provider: Failed to get ASGs: RequestError: send request failed
caused by: Post https://autoscaling.us-east-2.amazonaws.com/: dial tcp: i/o timeout
goroutine 77 [running]:
>crash<
...
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/builder.CloudProviderBuilder.Build(0x7fff352eb876, 0x3, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/builder/cloud_provider_builder.go:111 +0x756
```

Larger selection of log:

```
...
I1228 18:47:01.215194       1 leaderelection.go:184] successfully acquired lease kube-system/cluster-autoscaler
I1228 18:47:01.300081       1 factory.go:33] Event(v1.ObjectReference{Kind:""Endpoints"", Namespace:""kube-system"", Name:""cluster-autoscaler"", UID:""3323df5f-ebff-11e7-bb4b-023c3416291e"", APIVersion:""v1"", ResourceVersion:""920"", FieldPath:""""}): type: 'Normal' reason: 'LeaderElection' cluster-autoscaler-74f958855-kthlz became leader
I1228 18:47:01.300952       1 predicates.go:123] Using predicate PodFitsResources
I1228 18:47:01.300967       1 predicates.go:123] Using predicate GeneralPredicates
I1228 18:47:01.300972       1 predicates.go:123] Using predicate PodToleratesNodeTaints
I1228 18:47:01.300976       1 predicates.go:123] Using predicate MaxGCEPDVolumeCount
I1228 18:47:01.300980       1 predicates.go:123] Using predicate CheckNodeCondition
I1228 18:47:01.300983       1 predicates.go:123] Using predicate CheckNodeMemoryPressure
I1228 18:47:01.300987       1 predicates.go:123] Using predicate MaxEBSVolumeCount
I1228 18:47:01.300991       1 predicates.go:123] Using predicate MaxAzureDiskVolumeCount
I1228 18:47:01.300996       1 predicates.go:123] Using predicate NoDiskConflict
I1228 18:47:01.301000       1 predicates.go:123] Using predicate CheckNodeDiskPressure
I1228 18:47:01.301005       1 predicates.go:123] Using predicate MatchInterPodAffinity
I1228 18:47:01.301009       1 predicates.go:123] Using predicate NoVolumeZoneConflict
I1228 18:47:01.301013       1 predicates.go:123] Using predicate NoVolumeNodeConflict
I1228 18:47:01.301017       1 predicates.go:123] Using predicate ready
I1228 18:47:01.301213       1 reflector.go:202] Starting reflector *v1beta1.DaemonSet (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I1228 18:47:01.301243       1 reflector.go:240] Listing and watching *v1beta1.DaemonSet from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I1228 18:47:01.301398       1 reflector.go:202] Starting reflector *v1.ReplicationController (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
...
I1228 18:47:01.303435       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I1228 18:47:01.303378       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I1228 18:47:01.303669       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I1228 18:47:01.801875       1 request.go:462] Throttling request took 300.987699ms, request: PUT:https://10.30.0.1:443/api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status
I1228 18:47:01.903872       1 request.go:462] Throttling request took 600.142539ms, request: GET:https://10.30.0.1:443/api/v1/nodes?resourceVersion=0
I1228 18:47:03.225966       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1228 18:47:05.236322       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
...
I1228 18:49:09.910630       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1228 18:49:11.921510       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1228 18:49:13.934630       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1228 18:49:15.945377       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1228 18:49:16.303288       1 auto_scaling.go:138] Failed to describe ASG tags for keys [k8s.io/cluster-autoscaler/enabled kubernetes.io/cluster/gnat] : RequestError: send request failed
caused by: Post https://autoscaling.us-east-2.amazonaws.com/: dial tcp: i/o timeout
F1228 18:49:16.303338       1 cloud_provider_builder.go:111] Failed to create AWS cloud provider: Failed to get ASGs: RequestError: send request failed
caused by: Post https://autoscaling.us-east-2.amazonaws.com/: dial tcp: i/o timeout
goroutine 77 [running]:
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.stacks(0xc420afeb00, 0xc4206180f0, 0xec, 0xed)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:766 +0xa7
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.(*loggingT).output(0x518f4e0, 0xc400000003, 0xc4201dcbb0, 0x4e10623, 0x19, 0x6f, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:717 +0x348
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.(*loggingT).printf(0x518f4e0, 0x3, 0x3435f58, 0x27, 0xc420b06db0, 0x1, 0x1)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:655 +0x14f
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.Fatalf(0x3435f58, 0x27, 0xc420b06db0, 0x1, 0x1)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:1145 +0x67
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/builder.CloudProviderBuilder.Build(0x7fff352eb876, 0x3, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/builder/cloud_provider_builder.go:111 +0x756
k8s.io/autoscaler/cluster-autoscaler/core.NewAutoscalingContext(0xa, 0x3fe0000000000000, 0x8bb2c97000, 0x1176592e000, 0x0, 0x4e200, 0x0, 0x186a00000, 0x0, 0x7fff352eb8d3, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/autoscaling_context.go:147 +0x466
k8s.io/autoscaler/cluster-autoscaler/core.NewStaticAutoscaler(0xa, 0x3fe0000000000000, 0x8bb2c97000, 0x1176592e000, 0x0, 0x4e200, 0x0, 0x186a00000, 0x0, 0x7fff352eb8d3, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:56 +0x14d
k8s.io/autoscaler/cluster-autoscaler/core.(*AutoscalerBuilderImpl).Build(0xc42071c9c0, 0x412358, 0xc420b07850, 0x412358, 0x1a0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/autoscaler_builder.go:71 +0x10e
k8s.io/autoscaler/cluster-autoscaler/core.NewPollingAutoscaler(0x5094380, 0xc42071c9c0, 0x78, 0x98, 0xc4203c9b80)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/polling_autoscaler.go:38 +0x35
k8s.io/autoscaler/cluster-autoscaler/core.NewAutoscaler(0xa, 0x3fe0000000000000, 0x8bb2c97000, 0x1176592e000, 0x0, 0x4e200, 0x0, 0x186a00000, 0x0, 0x7fff352eb8d3, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/autoscaler.go:64 +0x5f2
main.run(0xc420af8d70)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:246 +0x263
main.main.func2(0xc42067dec0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:344 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
>end of log<
```",closed,False,2017-12-28 19:31:54,2019-01-03 20:05:32
autoscaler,whereisaaron,https://github.com/kubernetes/autoscaler/issues/519,https://api.github.com/repos/kubernetes/autoscaler/issues/519,Implement cluster autoscaling on AWS by manipulating spot fleet target?,"These days you can increase and decrease an AWS spot fleet's target size without re-creating the whole fleet. (As long as the launch specifications stay the same.) This enables the cluster-autoscaler to smoothly increase or decrease a node group's capacity, whilst benefiting from using instance type diversity, and AWS's automatic price-based selection, to keep the total price low.

For basic support the cluster-autoscaler doesn't need to have a [pricing model](https://github.com/kubernetes/autoscaler/pull/486), just a range of min target fleet size to max target fleet size, and a unit amount to increase/decrease the target, that should guarantee and increase or decrease in size. E.g. min = 2, max = 10, unit = 2. If it is easier, you could limit the fleet launch specifications so the unit of increase/decrease is always '1' as with an ASG.

If this viable/sensible? What do people think?

cc: @mumoshu ",closed,False,2017-12-28 20:03:24,2018-10-17 14:26:31
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/520,https://api.github.com/repos/kubernetes/autoscaler/issues/520,Use docker push for non-gcr images,"For images not hosted on gcr.io, we should use `docker push` instead of `gcloud docker -- push`.",closed,True,2017-12-29 04:59:13,2018-01-04 02:04:21
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/521,https://api.github.com/repos/kubernetes/autoscaler/issues/521,VPA Godeps update,"**What this PR does / why we need it:**
VPA recommender requires the latest version of metrics API to work.
The latest version of metrics API required other dependencies to be updated as well.
This, in turn, required some modifications in code. The most serious one is switching from `CreatedByAnnotation` to `OwnerReferences` in updater eviction algorithm.

@mwielgus @kgrygiel - please review
",closed,True,2018-01-01 19:37:22,2018-01-03 18:29:49
autoscaler,mwieczorek,https://github.com/kubernetes/autoscaler/pull/522,https://api.github.com/repos/kubernetes/autoscaler/issues/522,Add operator and value to pod tolerations in azure example yaml,"I created an acs-engine k8s cluster and tried to deploy 
https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/cluster-autoscaler-standard-master.yaml

but got:
```
Warning  FailedScheduling  1s (x6 over 16s)  default-scheduler  No nodes are available that match all of the predicates: MatchNodeSelector (2), PodToleratesNodeTaints (1).
```

With this small fix pod got scheduled. ",closed,True,2018-01-02 14:47:01,2018-01-03 01:25:52
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/523,https://api.github.com/repos/kubernetes/autoscaler/issues/523,Add info about CA limitations in multi-zonal cluster with stateful sets,,closed,True,2018-01-02 17:51:09,2018-01-02 18:44:02
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/524,https://api.github.com/repos/kubernetes/autoscaler/issues/524,Add operator and pod tolerations,"Continue of #522. Add operator and value to pod tolerations in azure example yaml.

/assign @mwielgus ",closed,True,2018-01-03 02:47:35,2018-01-03 13:08:10
autoscaler,Stono,https://github.com/kubernetes/autoscaler/issues/525,https://api.github.com/repos/kubernetes/autoscaler/issues/525,Advice as to why my cluster may not be scaling down,"I appreciate you're probably getting sick of these sorts of questions, but I can't work out from the logs available to me why the cluster is not scaling down:

Pods:
```
❯ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                                       READY     STATUS    RESTARTS   AGE       IP            NODE
delivery-engineering   java-vuln-0                                                3/3       Running   0          24m       10.177.1.46   gke-platform-dev-node-pool-java-8f2a3423-mmk6
kube-system            event-exporter-v0.1.7-7cb7c5d4bf-cj7lt                     2/2       Running   0          2d        10.177.0.8    gke-platform-dev-node-pool-java-1e83bdef-9z62
kube-system            fluentd-gcp-v2.0.9-88vln                                   2/2       Running   0          1d        10.177.1.2    gke-platform-dev-node-pool-java-8f2a3423-mmk6
kube-system            fluentd-gcp-v2.0.9-9t2fz                                   2/2       Running   0          2d        10.177.3.2    gke-platform-dev-node-pool-java-1e83bdef-x4tn
kube-system            fluentd-gcp-v2.0.9-bqlm5                                   2/2       Running   0          2d        10.177.0.2    gke-platform-dev-node-pool-java-1e83bdef-9z62
kube-system            fluentd-gcp-v2.0.9-td9gr                                   2/2       Running   0          2d        10.177.5.2    gke-platform-dev-node-pool-java-8f2a3423-tpn7
kube-system            heapster-v1.4.3-686b4b84df-wl2b4                           3/3       Running   0          10m       10.177.5.67   gke-platform-dev-node-pool-java-8f2a3423-tpn7
kube-system            helm-platform-external-dns-deployment-85b675d8c-jbtg7      1/1       Running   0          5h        10.177.3.42   gke-platform-dev-node-pool-java-1e83bdef-x4tn
kube-system            kube-dns-778977457c-j5pg6                                  3/3       Running   0          2d        10.177.0.9    gke-platform-dev-node-pool-java-1e83bdef-9z62
kube-system            kube-dns-778977457c-qw5d7                                  3/3       Running   0          2d        10.177.0.17   gke-platform-dev-node-pool-java-1e83bdef-9z62
kube-system            kube-dns-autoscaler-7db47cb9b7-lvc5p                       1/1       Running   0          5m        10.177.1.47   gke-platform-dev-node-pool-java-8f2a3423-mmk6
kube-system            kube-proxy-gke-platform-dev-node-pool-java-1e83bdef-9z62   1/1       Running   0          2d        10.176.0.2    gke-platform-dev-node-pool-java-1e83bdef-9z62
kube-system            kube-proxy-gke-platform-dev-node-pool-java-1e83bdef-x4tn   1/1       Running   0          2d        10.176.0.5    gke-platform-dev-node-pool-java-1e83bdef-x4tn
kube-system            kube-proxy-gke-platform-dev-node-pool-java-8f2a3423-mmk6   1/1       Running   0          1d        10.176.0.3    gke-platform-dev-node-pool-java-8f2a3423-mmk6
kube-system            kube-proxy-gke-platform-dev-node-pool-java-8f2a3423-tpn7   1/1       Running   0          2d        10.176.0.12   gke-platform-dev-node-pool-java-8f2a3423-tpn7
kube-system            kubernetes-dashboard-76c679977c-8jzgs                      1/1       Running   0          2d        10.177.0.4    gke-platform-dev-node-pool-java-1e83bdef-9z62
kube-system            l7-default-backend-6497bcdb4d-7czf2                        1/1       Running   0          2d        10.177.0.19   gke-platform-dev-node-pool-java-1e83bdef-9z62
kube-system            tiller-deploy-5fc57c7447-85qzd                             1/1       Running   0          2d        10.177.0.14   gke-platform-dev-node-pool-java-1e83bdef-9z62
```

No restrictive pdbs:
```
❯ kubectl get pdb --all-namespaces
No resources found.
```

Nodes:
```
❯ kubectl get nodes
NAME                                            STATUS    ROLES     AGE       VERSION
gke-platform-dev-node-pool-java-1e83bdef-9z62   Ready     <none>    2d        v1.8.4-gke.1
gke-platform-dev-node-pool-java-1e83bdef-x4tn   Ready     <none>    2d        v1.8.4-gke.1
gke-platform-dev-node-pool-java-8f2a3423-mmk6   Ready     <none>    1d        v1.8.4-gke.1
gke-platform-dev-node-pool-java-8f2a3423-tpn7   Ready     <none>    2d        v1.8.4-gke.1
```

Autoscaler logs:
```
❯ kubectl -n kube-system logs --follow kube-dns-autoscaler-7db47cb9b7-lvc5p
I0103 21:06:23.224599       1 autoscaler.go:49] Scaling Namespace: kube-system, Target: deployment/kube-dns
I0103 21:06:23.256787       1 plugin.go:50] Set control mode to linear
I0103 21:06:23.256825       1 linear_controller.go:59] ConfigMap version change (old:  new: 417) - rebuilding params
I0103 21:06:23.256840       1 linear_controller.go:60] Params from apiserver:
{""coresPerReplica"":256,""nodesPerReplica"":16,""preventSinglePointFailure"":true}
I0103 21:06:23.256886       1 linear_controller.go:79] Defaulting min replicas count to 1 for linear controller
```

Autoscaler configmap:
```
❯ kubectl get configmap cluster-autoscaler-status -n kube-system -o yaml
apiVersion: v1
data:
  status: |+
    Cluster-autoscaler status at 2018-01-03 21:13:55.891679786 +0000 UTC:
    Cluster-wide:
      Health:      Healthy (ready=4 unready=0 notStarted=0 longNotStarted=0 registered=4 longUnregistered=0)
                   LastProbeTime:      2018-01-03 21:13:53.835418577 +0000 UTC
                   LastTransitionTime: 2018-01-03 21:00:48.916580853 +0000 UTC
      ScaleUp:     NoActivity (ready=4 registered=4)
                   LastProbeTime:      2018-01-03 21:13:53.835418577 +0000 UTC
                   LastTransitionTime: 2018-01-03 21:00:48.916580853 +0000 UTC
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2018-01-03 21:13:53.835418577 +0000 UTC
                   LastTransitionTime: 2018-01-03 21:06:25.238502144 +0000 UTC

    NodeGroups:
      Name:        https://content.googleapis.com/compute/v1/projects/k8-discovery-185615/zones/europe-west2-b/instanceGroups/gke-platform-dev-node-pool-java-1e83bdef-grp
      Health:      Healthy (ready=2 unready=0 notStarted=0 longNotStarted=0 registered=2 longUnregistered=0 cloudProviderTarget=2 (minSize=0, maxSize=5))
                   LastProbeTime:      2018-01-03 21:13:53.835418577 +0000 UTC
                   LastTransitionTime: 2018-01-03 21:00:48.916580853 +0000 UTC
      ScaleUp:     NoActivity (ready=2 cloudProviderTarget=2)
                   LastProbeTime:      2018-01-03 21:13:53.835418577 +0000 UTC
                   LastTransitionTime: 2018-01-03 21:00:48.916580853 +0000 UTC
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2018-01-03 21:13:53.835418577 +0000 UTC
                   LastTransitionTime: 2018-01-03 21:00:48.916580853 +0000 UTC

      Name:        https://content.googleapis.com/compute/v1/projects/k8-discovery-185615/zones/europe-west2-a/instanceGroups/gke-platform-dev-node-pool-java-8f2a3423-grp
      Health:      Healthy (ready=2 unready=0 notStarted=0 longNotStarted=0 registered=2 longUnregistered=0 cloudProviderTarget=2 (minSize=0, maxSize=5))
                   LastProbeTime:      2018-01-03 21:13:53.835418577 +0000 UTC
                   LastTransitionTime: 2018-01-03 21:00:48.916580853 +0000 UTC
      ScaleUp:     NoActivity (ready=2 cloudProviderTarget=2)
                   LastProbeTime:      2018-01-03 21:13:53.835418577 +0000 UTC
                   LastTransitionTime: 2018-01-03 21:00:48.916580853 +0000 UTC
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2018-01-03 21:13:53.835418577 +0000 UTC
                   LastTransitionTime: 2018-01-03 21:06:25.238502144 +0000 UTC
```
 ",closed,False,2018-01-03 21:14:49,2019-03-15 10:24:16
autoscaler,x13n,https://github.com/kubernetes/autoscaler/pull/526,https://api.github.com/repos/kubernetes/autoscaler/issues/526,Version info in README,FYI @mwielgus ,closed,True,2018-01-04 13:55:48,2018-01-04 18:40:56
autoscaler,simnalamburt,https://github.com/kubernetes/autoscaler/pull/527,https://api.github.com/repos/kubernetes/autoscaler/issues/527,Update documentation: AWS now supports ARNs for autoscaling groups,Closes #234,closed,True,2018-01-05 02:59:06,2018-02-09 08:47:50
autoscaler,kawych,https://github.com/kubernetes/autoscaler/pull/528,https://api.github.com/repos/kubernetes/autoscaler/issues/528,Addon resizer release 1.8,Fast-forward release-1.8 branch to release-1 branch (release 1.8.1 was created from release-1 branch).,closed,True,2018-01-05 16:52:21,2018-01-08 19:44:00
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/529,https://api.github.com/repos/kubernetes/autoscaler/issues/529,running updater podLister in a separate go routine,"bugfix after recent godep update
- without it, updater would just hang on running reflector.",closed,True,2018-01-07 12:35:06,2018-01-08 19:01:38
autoscaler,Stono,https://github.com/kubernetes/autoscaler/issues/530,https://api.github.com/repos/kubernetes/autoscaler/issues/530,Failed to scale up node pool from 0,"Hi, 
I've just observed some strange behaviour.  I have a cluster with three node pools, tagged with a `purpose` label, in the following ways

- default
- hot
- warm

Default was running 2 nodes, whilst hot and warm were at 0 (as nothing was deployed).

I then deployed a pod which had a nodeSelector of `purpose=hot`, but observed the following:

```
Events:
  Type     Reason             Age                From                Message
  ----     ------             ----               ----                -------
  Warning  FailedScheduling   1m (x22 over 6m)   default-scheduler   No nodes are available that match all of the predicates: Insufficient cpu (1), MatchNodeSelector (1).
  Normal   NotTriggerScaleUp  51s (x23 over 6m)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```

I had to go and manually increase the pool size to 1 to get this pod to deploy.
When i then deployed more instances of the application, the pool continued to scale up as expected.",closed,False,2018-01-07 19:49:18,2018-08-06 17:39:27
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/531,https://api.github.com/repos/kubernetes/autoscaler/issues/531,VPA api definition,"As agreed in https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/vertical-pod-autoscaler.md (almost - notable change - VPAStatusCondion, just like in HPA).

cc: @tkulczynski @kgrygiel ",closed,True,2018-01-08 22:21:27,2018-01-09 13:12:11
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/532,https://api.github.com/repos/kubernetes/autoscaler/issues/532,CustomResourceDefinition for vpa,cc: @kgrygiel @tkulczynski ,closed,True,2018-01-09 22:55:52,2018-01-11 15:40:58
autoscaler,dmlemos,https://github.com/kubernetes/autoscaler/issues/533,https://api.github.com/repos/kubernetes/autoscaler/issues/533,Azure fails to initialise - Invalid API URL,"After deploying on Azure I am getting these errors:
```
I0110 14:12:36.277380       1 cloud_provider_builder.go:128] Creating Azure Manager with default configuration.
I0110 14:12:36.277485       1 azure_manager.go:137] read configuration: 45333ffa-fd6c-4f69-8b92-78dfc938c148
I0110 14:12:36.277573       1 azure_manager.go:154] Created scale set client with authorizer: {{{0xc4202d94e0 0xc4210eef90 <nil> <nil> 1m0s 15m0s 3 30s Go/go1.8.3 (amd64-linux) go-autorest/8.0.0 Azure-SDK-For-Go/v11.0.0-beta arm-compute/ <nil>} https://management.azure.com 45333ffa-fd6c-4f69-8b92-78dfc938c148
}}
I0110 14:12:36.277864       1 azure_manager.go:162] Created scale set vm client with authorizer: {{{0xc4202d95a0 <nil> 0x15f4570 0x15f4780 1m0s 15m0s 3 30s Go/go1.8.3 (amd64-linux) go-autorest/8.0.0 Azure-SDK-For-Go/v11.0.0-beta arm-compute/ <nil>} https://management.azure.com 45333ffa-fd6c-4f69-8b92-78dfc938c148
}}
I0110 14:12:36.277999       1 main.go:223] Registered cleanup signal handler
E0110 14:12:37.479914       1 azure_manager.go:316] Failed to get scaleSet with name k8s-agent-86549744: azure.BearerAuthorizer#WithAuthorization: Failed to refresh the Token for request to https://management.azure.com/subscriptions/45333ffa-fd6c-4f69-8b92-78dfc938c148%0A/resourceGroups/pltsrv-dk8s-eun-k8s-clstr-rg%0A/providers/Microsoft.Compute/virtualMachineScaleSets/k8s-agent-86549744?api-version=2017-03-30: StatusCode=0 -- Original Error: adal: Refresh request failed. Status Code = '400'. Response body: <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01//EN""""http://www.w3.org/TR/html4/strict.dtd"">
<HTML><HEAD><TITLE>Bad Request</TITLE>
<META HTTP-EQUIV=""Content-Type"" Content=""text/html; charset=us-ascii""></HEAD>
<BODY><h2>Bad Request - Invalid URL</h2>
<hr><p>HTTP Error 400. The request URL is invalid.</p>
</BODY></HTML>
E0110 14:12:37.480056       1 azure_manager.go:179] Error while regenerating AS cache: azure.BearerAuthorizer#WithAuthorization: Failed to refresh the Token for request to https://management.azure.com/subscriptions/45333ffa-fd6c-4f69-8b92-78dfc938c148%0A/resourceGroups/pltsrv-dk8s-eun-k8s-clstr-rg%0A/providers/Microsoft.Compute/virtualMachineScaleSets/k8s-agent-86549744?api-version=2017-03-30: StatusCode=0 -- Original Error: adal: Refresh request failed. Status Code = '400'. Response body: <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01//EN""""http://www.w3.org/TR/html4/strict.dtd"">
<HTML><HEAD><TITLE>Bad Request</TITLE>
<META HTTP-EQUIV=""Content-Type"" Content=""text/html; charset=us-ascii""></HEAD>
<BODY><h2>Bad Request - Invalid URL</h2>
<hr><p>HTTP Error 400. The request URL is invalid.</p>
</BODY></HTML>
E0110 14:12:46.396326       1 static_autoscaler.go:135] Failed to update node registry: azure.BearerAuthorizer#WithAuthorization: Failed to refresh the Token for request to https://management.azure.com/subscriptions/45333ffa-fd6c-4f69-8b92-78dfc938c148%0A/resourceGroups/pltsrv-dk8s-eun-k8s-clstr-rg%0A/providers/Microsoft.Compute/virtualMachineScaleSets/k8s-agent-86549744?api-version=2017-03-30: StatusCode=0 -- Original Error: adal: Refresh request failed. Status Code = '400'. Response body: <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01//EN""""http://www.w3.org/TR/html4/strict.dtd"">
<HTML><HEAD><TITLE>Bad Request</TITLE>
<META HTTP-EQUIV=""Content-Type"" Content=""text/html; charset=us-ascii""></HEAD>
<BODY><h2>Bad Request - Invalid URL</h2>
<hr><p>HTTP Error 400. The request URL is invalid.</p>
</BODY></HTML>
```

The URL in the API calls looks to be mixed up with the hex code for newline.

Using `vmType: standard`

**Kubernetes versions:** `1.7`, `1.8`
**Cluster auto-scaler version:** `1.1.0`

## Questions

1. It tries to create a ScaleSet even when not using it? Is this behaviour normal?
2. What exactly is the NodeGroup secret? It's not clear by the description. I used the pool name that I see on `kubectl get nodes`, which is the same shown in the Azure portal.

---

This is unrelated, but the `$(ARM_NODE_GROUP)` in the manifests causes a newline to be passed as an argument, as seen in the logs:
```
1 flags.go:52] FLAG: --nodes=""[1:10:k8s-agent-86549744\n]""
```",closed,False,2018-01-10 21:02:08,2018-01-28 03:06:49
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/534,https://api.github.com/repos/kubernetes/autoscaler/issues/534,Rewrite Initializer as External Admission Plugin,Initialisers won't be graduated to beta anytime soon.,closed,False,2018-01-10 22:39:15,2018-02-07 14:51:08
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/535,https://api.github.com/repos/kubernetes/autoscaler/issues/535,Implement Prometheus client,,closed,True,2018-01-11 14:38:49,2018-01-11 15:49:05
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/536,https://api.github.com/repos/kubernetes/autoscaler/issues/536,Trim spaces for user-specified configs,"Trim spaces for user-specified Azure configs.

Fix #533.",closed,True,2018-01-11 14:49:49,2018-01-11 22:55:35
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/537,https://api.github.com/repos/kubernetes/autoscaler/issues/537,Split ContainerUsageSample to per resource,cc @kgrygiel ,closed,True,2018-01-11 16:41:21,2018-01-12 10:00:30
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/538,https://api.github.com/repos/kubernetes/autoscaler/issues/538,Remove accidentaly added files to vpa/api,,closed,True,2018-01-12 12:01:21,2018-01-12 12:03:09
autoscaler,vovkanaz,https://github.com/kubernetes/autoscaler/issues/539,https://api.github.com/repos/kubernetes/autoscaler/issues/539,Is it issue with Kubernetes 1.9?,"Our Cluster based on Kubernetes 1.9, seted up via kubeadmin
We are also create a role, gave it needed permissions, and attach it with  a working nodes(in AWS console).
When we try to run our pod, we've see always something like that!
`
 kubectl logs cluster-autoscaler-55797ff9db-r8qps
I0112 12:08:07.299859       1 main.go:225] Cluster Autoscaler 0.6.0
F0112 12:08:07.402421       1 main.go:257] Failed to get nodes from apiserver: nodes is forbidden: User ""system:serviceaccount:default:default"" cannot list nodes at the cluster scope
goroutine 1 [running]:`
Our Cloud Provider AWS. Thanks a lot, if you'll advice a little bit how to resolve this trouble
",closed,False,2018-01-12 12:21:55,2018-05-15 16:14:28
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/540,https://api.github.com/repos/kubernetes/autoscaler/issues/540,VPA go API client,"Adds config and tools for generating the API client + auto-generated client code + starts using the client in the recommender binary: periodically fetching VPA objects and printing them to the stdout.

NOTE: golint will fail on the auto-generated code.",closed,True,2018-01-13 12:49:32,2018-01-15 14:07:29
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/541,https://api.github.com/repos/kubernetes/autoscaler/issues/541,metrics client for recommender,"MetricsClient complements existing SpecClient

cc: @kgrygiel @mwielgus - please review",closed,True,2018-01-13 13:36:26,2018-01-15 10:44:11
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/542,https://api.github.com/repos/kubernetes/autoscaler/issues/542,Support multiple node groups on Azure,"Continue of #449. This PR adds support of multiple node groups on Azure.

It also improve code organization by wrapping clients, config validation.",closed,True,2018-01-15 09:20:43,2018-01-15 14:00:54
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/543,https://api.github.com/repos/kubernetes/autoscaler/issues/543,VPA go API client,"Adds config and tools for generating the API client + auto-generated client code + starts using the client in the recommender binary: periodically fetching VPA objects and printing them to the stdout.

NOTE: hack/verify-golint.sh - golint checks were disabled for auto-generated code and api definition (codegen input) ",closed,True,2018-01-15 13:47:03,2018-01-15 14:08:12
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/544,https://api.github.com/repos/kubernetes/autoscaler/issues/544,Don't register metrics unless on leading master,Cherry-pick of  #504,closed,True,2018-01-15 16:43:52,2018-01-15 16:48:02
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/545,https://api.github.com/repos/kubernetes/autoscaler/issues/545,Don't register metrics unless on leading master,Cherry-pick of #504,closed,True,2018-01-15 16:49:53,2018-01-15 18:33:56
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/546,https://api.github.com/repos/kubernetes/autoscaler/issues/546,Add extra eventing/doc for long unregistered nodes causing cluster autoscaler to consider cluster as broken,,closed,False,2018-01-15 18:53:09,2018-03-03 08:16:24
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/issues/547,https://api.github.com/repos/kubernetes/autoscaler/issues/547,Fix situation when scale up overshot 33% broken nodes while hitting qouta,,closed,False,2018-01-15 18:53:55,2018-02-02 12:30:55
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/548,https://api.github.com/repos/kubernetes/autoscaler/issues/548,Move test/test_utils to pkg/utils,cc @mwielgus ,closed,True,2018-01-16 11:47:14,2018-01-16 13:29:39
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/549,https://api.github.com/repos/kubernetes/autoscaler/issues/549,Don't register metrics unless on leading master,Cherry pick of #545 ,closed,True,2018-01-16 12:11:02,2018-01-16 12:38:34
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/550,https://api.github.com/repos/kubernetes/autoscaler/issues/550,Don't register metrics unless on leading master,Cherry pick of #545 ,closed,True,2018-01-16 12:17:41,2018-01-16 13:58:57
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/551,https://api.github.com/repos/kubernetes/autoscaler/issues/551,Better reporting of cluster state,"This PR somehow improves monitoring (metrics, events and configmap status) in the following cases:
- nodes unregistered for a long time,
- cluster is empty,
- cluster not healthy enough to autoscale.

This is intended as a temporary solution. We should probably process empty cluster in ClusterStateRegistry to cover following cases:
- accurately report unregistered nodes when there are no ready nodes,
- attempt to remove unregistered nodes while cluster is unhealthy,
- (feature) scale from 0 nodes in the cluster.",closed,True,2018-01-16 14:44:36,2018-01-16 18:51:11
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/552,https://api.github.com/repos/kubernetes/autoscaler/issues/552,Reduce API calls and avoid touching Azure rate limits,Continue of #449. Reduce API calls and avoid touching Azure rate limits.,closed,True,2018-01-17 01:13:38,2018-01-17 14:47:12
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/553,https://api.github.com/repos/kubernetes/autoscaler/issues/553,Add support for configuring etcd private keys,"acs-engine [v0.12.0](https://github.com/Azure/acs-engine/releases/tag/v0.12.0) adds a breaking change of etcd TLS. It introduces two extra etcd private keys: EtcdClientPrivateKey and EtcdServerPrivateKey.

This PR adds the configuration for them and updates the deploy documentation.",closed,True,2018-01-17 06:30:56,2018-01-17 14:47:01
autoscaler,thockin,https://github.com/kubernetes/autoscaler/pull/554,https://api.github.com/repos/kubernetes/autoscaler/issues/554,Pushes go to staging-k8s.gcr.io,,closed,True,2018-01-17 22:23:55,2018-01-18 11:21:24
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/555,https://api.github.com/repos/kubernetes/autoscaler/issues/555,Add support for dealing with empty clusters in ClusterStateRegistry,"Currently, ClusterStateRegistry isn't updated when cluster has no ready nodes, or no nodes at all. This means that tracking of unready and unregistered nodes is also stopped. Node count metric for all types of nodes is artificially set to 0, even if there are non-ready nodes which would otherwise be detected and reported. For clusters in such state, all Cluster Autoscaler operations are stopped - there's no attempt to remove broken nodes, or add new ones, even if there are pending pods. As there will be no attempt to recover, user intervention is usually required to restore normal operation (unless this state is transient, e.g. just after cluster creation, Cluster Autoscaler may start before nodes become ready.) 

It's important to note that none of this affects healthy clusters, and in genuinely broken clusters, recovery isn't guaranteed, nor is it the purpose of Cluster Autoscaler to provide it. So this isn't a bug, but rather an edge case that could be handled better.

Sufficient implementation should include:
- no errors on updating ClusterStateRegistry when there are no ready nodes, or no nodes at all,
- tracking of unregistered and unready nodes while in this state.

This would open way to:
- accurate reporting of cluster health when there are no ready nodes (including number of unregistered, long unregistered, unready nodes),
- attempting to remove long unregistered nodes when there are no ready nodes,
- (feature) scale from 0 nodes in the cluster.",open,False,2018-01-18 12:16:35,2019-04-02 15:24:12
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/556,https://api.github.com/repos/kubernetes/autoscaler/issues/556,Update VPA CRD objects' statuses,"* Allow patch on VPA CRD objects instead of put (to avoid race condition with user actions).
* Store VPA and Pod info in ClusterState.
* Update VPA status fields with recommendations for containers.
* Minor fixes.",closed,True,2018-01-18 13:19:10,2018-01-25 12:54:06
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/557,https://api.github.com/repos/kubernetes/autoscaler/issues/557,Sample YAMLs with VPA spec and vpa-enabled deployment.,,closed,True,2018-01-18 13:36:28,2018-01-19 22:38:40
autoscaler,rickard-von-essen,https://github.com/kubernetes/autoscaler/pull/558,https://api.github.com/repos/kubernetes/autoscaler/issues/558,CA AWS: Update README regarding scale-down-delay,"63310ef41ac1d586944287e66545bcb41b444887 replaced the `--scale-down-delay` flag with the `--scale-down-delay-after-add`, `--scale-down-delay-after-delete`, and `--scale-down-delay-after-failure` flags. This updates the README accordingly.",closed,True,2018-01-18 15:26:57,2018-01-22 11:38:14
autoscaler,tesharp,https://github.com/kubernetes/autoscaler/pull/559,https://api.github.com/repos/kubernetes/autoscaler/issues/559,Bug fix for auto-resizer 1.8.1 not reading command arguments,"Cpu and memory arguments from command line are not read at all, leaving them empty. ```nannyConfigurationFromFlags``` sets default values to argument values. In ```loadNannyConfiguration``` it attempts to read from config, but if config is not set it just creates an empty ```configMapConfig```. Since this is empty calling ```nannyconfigalpha.FillInDefaults_NannyConfiguration``` doesn't actually do anything as it compares the values against default values, so when all values are empty this does nothing. In the end function just returns empty values for everything and never uses command line arguments.

Starting container in acs-engine leads to these errors now:
```
ERROR: logging before flag.Parse: I0118 13:57:09.157760       1 pod_nanny.go:64] Invoked by [/pod_nanny --cpu=80m --extra-cpu=0.5m --memory=140Mi --extra-memory=4Mi --threshold=5 --deployment=heapster --container=heapster --poll-period=300000 --estimator=exponential]
ERROR: logging before flag.Parse: I0118 13:57:09.157850       1 pod_nanny.go:76] Watching namespace: kube-system, pod: heapster-548bd647cc-7fpgv, container: heapster.
ERROR: logging before flag.Parse: I0118 13:57:09.157856       1 pod_nanny.go:77] storage: MISSING, extra_storage: 0Gi
ERROR: logging before flag.Parse: I0118 13:57:09.159734       1 pod_nanny.go:162] Failed to read data from config file ""MISSING/NannyConfiguration"": open MISSING/NannyConfiguration: no such file or directory, using default parameters
ERROR: logging before flag.Parse: I0118 13:57:09.159804       1 pod_nanny.go:166]
ERROR: logging before flag.Parse: I0118 13:57:09.159917       1 pod_nanny.go:101] cpu: , extra_cpu: , memory: , extra_memory:
panic: cannot parse '': quantities must match the regular expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$'
```",closed,True,2018-01-18 16:13:02,2018-02-27 14:21:51
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/560,https://api.github.com/repos/kubernetes/autoscaler/issues/560,Remove initializer code,"Part of #534.

Removing now to enable clearing of mock code without unnecessarily investing in fixing initializer.

cc @mwielgus ",closed,True,2018-01-19 12:05:28,2018-01-19 14:02:02
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/561,https://api.github.com/repos/kubernetes/autoscaler/issues/561,APImock replaced with a real API in the Updater. Recommender_mock removed too.,,closed,True,2018-01-19 13:39:13,2018-01-19 14:50:39
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/562,https://api.github.com/repos/kubernetes/autoscaler/issues/562,Fix deleting VPA from the ClusterState.,,closed,True,2018-01-19 14:32:11,2018-01-19 22:37:37
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/563,https://api.github.com/repos/kubernetes/autoscaler/issues/563,Implement admission controller,"Fixing #534. 
Tested e2e: I run a cluster with a single VPA that had recommendation set and it used it when new nodes were created.",closed,True,2018-01-19 16:45:29,2018-01-23 21:07:36
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/564,https://api.github.com/repos/kubernetes/autoscaler/issues/564,[VPA Recommender] ClusterState fed by clients via ClusterStateFeeder,"Updating ClusterState with data from  spec and metrics clients + 2 small refactorings:
- rename ""cluster"" package to ""clients"" (since cluster is already used in model package in a different context)
- move conversion functions form `model/types.go` to a new file `model/converters.go`

(refactorings in separate commits)

cc: @mwielgus @kgrygiel - please review!",closed,True,2018-01-21 17:40:34,2018-01-25 19:57:59
autoscaler,rickard-von-essen,https://github.com/kubernetes/autoscaler/pull/565,https://api.github.com/repos/kubernetes/autoscaler/issues/565,CA AWS: Clarify that auto-discovery configuration,"- Clarify that the only the tag key is used for auto-discovery
- Remove some obsolete information on how to obtain the code with this
feature.",closed,True,2018-01-22 12:07:47,2018-01-23 19:40:36
autoscaler,spencersugarman,https://github.com/kubernetes/autoscaler/pull/566,https://api.github.com/repos/kubernetes/autoscaler/issues/566,Fix typos,,closed,True,2018-01-22 19:58:56,2018-01-23 04:23:05
autoscaler,zegl,https://github.com/kubernetes/autoscaler/issues/567,https://api.github.com/repos/kubernetes/autoscaler/issues/567,Typo in relase notes for v1.1.0,"https://github.com/kubernetes/autoscaler/releases/tag/cluster-autoscaler-1.1.0

The image URL is incorrect and pointing to the wrong version: `Image: gcr.io/google_containers/cluster-autoscaler:v1.0.3`",closed,False,2018-01-23 09:33:51,2018-01-23 10:41:26
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/568,https://api.github.com/repos/kubernetes/autoscaler/issues/568,Updater Deployment related YAMLs + VPA Lister fix in Updater,,closed,True,2018-01-23 13:11:32,2018-01-23 18:38:33
autoscaler,jacobstr,https://github.com/kubernetes/autoscaler/pull/569,https://api.github.com/repos/kubernetes/autoscaler/issues/569,add us-central zones to supported gce gpu zones.,"Title says most of it. These zones were added about a month ago, and are reflected in the official docs. The intent here was to address something similar to: https://github.com/kubernetes/autoscaler/issues/321

This doesn't appear to have address my issue by itself. Closing for now while I dig some more.
",closed,True,2018-01-23 22:17:01,2018-01-23 22:55:03
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/570,https://api.github.com/repos/kubernetes/autoscaler/issues/570,"Select all pods, so pods in state Unknown are not deleted from the Cluster State.","State Unknown is may happen due to transient error (https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase), so it shouldn't be deleted from the Cluster State.",closed,True,2018-01-24 13:16:41,2018-01-29 20:21:07
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/571,https://api.github.com/repos/kubernetes/autoscaler/issues/571,Convenience scripts to turn VPA up and down.,,closed,True,2018-01-24 14:10:27,2018-01-24 14:15:48
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/572,https://api.github.com/repos/kubernetes/autoscaler/issues/572,Convenience scripts to turn VPA up and down.,,closed,True,2018-01-24 14:19:03,2018-01-24 17:53:12
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/573,https://api.github.com/repos/kubernetes/autoscaler/issues/573,Fix small errors,,closed,True,2018-01-24 17:58:48,2018-01-24 18:18:50
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/574,https://api.github.com/repos/kubernetes/autoscaler/issues/574,"Get CPU, memory and labels history from Prometheus","This also refactors out history provider from recommender.go (as you originally requested) and fixes issues with the recommender flags, client queries etc. I now tested it e2e in my cluster and it pushes some samples and labels to clusterState.",closed,True,2018-01-24 18:10:50,2018-01-25 18:09:03
autoscaler,domhauton,https://github.com/kubernetes/autoscaler/pull/575,https://api.github.com/repos/kubernetes/autoscaler/issues/575,Tighten CF ASG permissions for prod systems,The tighter CF only allows modifications to the ASGs specified by the users. A much safer alternative to the previous.,closed,True,2018-01-24 22:58:12,2018-07-16 12:19:11
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/576,https://api.github.com/repos/kubernetes/autoscaler/issues/576,VPA CRD api utils extracted to a separate package.,,closed,True,2018-01-25 09:23:55,2018-01-25 16:19:31
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/issues/577,https://api.github.com/repos/kubernetes/autoscaler/issues/577,Disable InterPodAffinity predicate if only soft affinity is used by pods,"As described in #281 we disable affinity predicate (for efficiency) if no pod in the cluster is using affinity. Looking at the code it looks like we don't distinguish between soft (preferedDuringScheduling) and hard (requiredDuringScheduling) affinity. CA completely ignores soft affinity anyway, so we should only enable the predicate if at least a single pod uses hard affinity.

cc: @mwielgus @aleksandra-malinowska @bskiba ",closed,False,2018-01-25 10:23:42,2018-02-14 16:10:38
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/578,https://api.github.com/repos/kubernetes/autoscaler/issues/578,[VPA recommender] Loading metrics to clusterState,"minimal change needed to load metrics into clusterState
follow-up refactorings will follow in separate PRs

@mwielgus @kgrygiel please review",closed,True,2018-01-25 19:31:34,2018-01-29 19:02:12
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/579,https://api.github.com/repos/kubernetes/autoscaler/issues/579,Refinements recommended in https://github.com/kubernetes/autoscaler/pull/556#pullrequestreview-91141334,,closed,True,2018-01-26 08:47:19,2018-01-28 04:09:17
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/580,https://api.github.com/repos/kubernetes/autoscaler/issues/580,Cleanup model atoms,"* Change float64 to ResourceAmount in cotainer samples
* Rename MetricName to ResourceName to better reflect what it is and improve consistence with core API
* Move struct that are not used for storage from model package to where they are used (cluster package)",closed,True,2018-01-26 14:32:25,2018-01-30 12:49:50
autoscaler,DaveTaddei-CR,https://github.com/kubernetes/autoscaler/issues/581,https://api.github.com/repos/kubernetes/autoscaler/issues/581,Aggressive scale down followed by scale up,"Since upgrading to kubernetes 1.8 we also upgraded the autoscaler to 1.1.0.  We have noticed an odd behaviour where the autoscaler appears to try and scale to the minimum number of nodes and then immediately scale up again.

To reproduce:
* start with a single node and increase the number of replicas of a pod one at a time, continue until there are three nodes
* decrease the number of replicas gradually allowing the autoscaler time to adjust

The autoscaler appears to mark multiple nodes for removal without considering the accumulated resources required.  The scheduler is then unable to accommodate the required pods on the remaining nodes which triggers a scale up.  When dealing with a minimum of 1 node its not uncommon to observe a flip-flop behaviour. ",closed,False,2018-01-26 14:52:22,2018-05-15 08:12:33
autoscaler,DaveTaddei-CR,https://github.com/kubernetes/autoscaler/issues/582,https://api.github.com/repos/kubernetes/autoscaler/issues/582,Autoscaler unable to reallocate itself or contact DNS after scale down,"The autoscaler occasionally triggers a scale down which involves reallocating itself and kube-dns pods.  This has resulted in a deadlock situation whereby the remaining nodes are over-requested but a new node cannot start because no autoscaler is running.  A similar problem exists with kube-dns pods whereby the autoscaler is unable to communicate with AWS ASGs while the kube-dns is down, again resulting in a cluster which is unable to scale up.

Attempted resolutions:
* PodDisruptionBudget - no effect on small clusters (1-4 nodes)
* Allocated autoscaler to master - successful in keeping autoscaler alive but does not help DNS pods",closed,False,2018-01-26 14:56:42,2018-07-02 11:48:12
autoscaler,DaveTaddei-CR,https://github.com/kubernetes/autoscaler/issues/583,https://api.github.com/repos/kubernetes/autoscaler/issues/583,Autoscaler not honouring PDB,"The CA has been observed to not honour PodDisruptionBudgets.  In this case when the cluster is scaled from 1 to 4 nodes and then scaled back down to 1 the pods which should be protected by the PodDisruptionBudget are often removed.

dns-pdb.yml
```
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: dns-pdb
  namespace: kube-system
spec:
  minAvailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
```

CA configuration
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
        - image: gcr.io/google_containers/cluster-autoscaler:v1.1.0
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --nodes=1:5:nodes.mycluster
          env:
            - name: AWS_REGION
              value: eu-west-2
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
          imagePullPolicy: ""Always""
      volumes:
        - name: ssl-certs
          hostPath:
            path: ""/etc/ssl/certs/ca-certificates.crt""
      tolerations:
        - key: ""node-role.kubernetes.io/master""
          effect: NoSchedule
      nodeSelector:
        kubernetes.io/role: master
      dnsPolicy: Default
      hostNetwork: true
```

I will endeavour to capture logs when an event occurs however it is intermittent.

===
edit: Changed the formating to make yaml easier to read -- MaciekPytel

",closed,False,2018-01-26 16:14:19,2018-06-25 18:11:30
autoscaler,dineshsckloud9,https://github.com/kubernetes/autoscaler/issues/584,https://api.github.com/repos/kubernetes/autoscaler/issues/584,Showing scaling up in MIG but not scaling in GCE - Even when creation it not launching more than 3 nodes across zones,"Multizone scaling with MIG per zone seems working but could not launch more than 3 instances.. MIG showing 1 -> 2 but its not launching in gce... What could be the problem?

I0127 04:52:37.565409 5 scale_up.go:110] Skipping node group https://content.googleapis.com/compute/v1/projects/global-snow-193212/zones/europe-west1-b/instanceGroups/kubernetes-minion-group - max size reached
W0127 04:52:37.566044 5 scale_up.go:99] Node group https://content.googleapis.com/compute/v1/projects/global-snow-193212/zones/europe-west1-d/instanceGroups/kubernetes-minion-group is not ready for scaleup
W0127 04:52:37.566348 5 scale_up.go:99] Node group https://content.googleapis.com/compute/v1/projects/global-snow-193212/zones/europe-west1-c/instanceGroups/kubernetes-minion-group is not ready for scaleup

Why it is not ready?

Kubernetes - v1.9.2
Cluster-Autoscaler - v1.1.0

/sig scalability",closed,False,2018-01-27 05:09:14,2018-03-23 15:56:41
autoscaler,sak0,https://github.com/kubernetes/autoscaler/pull/585,https://api.github.com/repos/kubernetes/autoscaler/issues/585,fix addon-resizer help info.,"fix addon-resizer help info.

Signed-off-by: CuiHaozhi <cuihaozhi@chinacloud.com.cn>",closed,True,2018-01-27 08:09:04,2018-01-28 04:08:24
autoscaler,alexanderkjeldaas,https://github.com/kubernetes/autoscaler/issues/586,https://api.github.com/repos/kubernetes/autoscaler/issues/586,cluster-autoscaler release for kubernetes 1.9.x missing,https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#releases does not list a kubernetes 1.9.x release.,closed,False,2018-01-28 18:59:59,2018-02-07 16:36:04
autoscaler,integrii,https://github.com/kubernetes/autoscaler/pull/587,https://api.github.com/repos/kubernetes/autoscaler/issues/587,Burst capacity,"Here is a proposal for burstable headroom.  Please review.  The goal here is to have the autoscaler also factor in ""burstable headroom"" when deciding to change the scale of a cluster.

We really appreciate the project and use it in many production clusters globally.",closed,True,2018-01-29 06:03:48,2018-01-30 14:37:02
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/588,https://api.github.com/repos/kubernetes/autoscaler/issues/588,Add a missing error return in prometheus client. Add a unit test,,closed,True,2018-01-29 12:42:32,2018-01-29 20:20:15
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/589,https://api.github.com/repos/kubernetes/autoscaler/issues/589,Make VPAs only point at Pods in the same namespace,,closed,True,2018-01-29 14:56:06,2018-01-29 20:18:29
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/590,https://api.github.com/repos/kubernetes/autoscaler/issues/590,Use real recommender logic (based on usage).,,closed,True,2018-01-29 22:52:08,2018-01-30 14:24:51
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/591,https://api.github.com/repos/kubernetes/autoscaler/issues/591,Improvements to deploy scripts,Also unify default registries and tags in all vpa components.,closed,True,2018-01-30 13:20:08,2018-02-01 12:12:11
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/592,https://api.github.com/repos/kubernetes/autoscaler/issues/592,Update updater README.md to reflect current code,,closed,True,2018-01-30 15:31:06,2018-01-31 09:51:07
autoscaler,pjama,https://github.com/kubernetes/autoscaler/issues/593,https://api.github.com/repos/kubernetes/autoscaler/issues/593,Node Readiness not found + Unregistered Node in Group,"Hello,

I am trying to add a second nodePool to my k8s cluster in AWS. For reference, the there are two nodePools: `default` and `bigger`.

It seems that the `bigger` node instance _is_ being created successfully in AWS, but it is not available in the nodePool.

**Component Versions**

**K8s:** 1.7.4
**Kube-AWS:** 0.9.8
**CA:** 0.6

I have also tried upgrading CA to v0.6.1, and added `autoscaling:DescribeLaunchConfigurations` to the IAM role.

From the `cluster-autoscaler` logs:

```
auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  auto_scaling_groups.go:94] Regenerating ASG information for data-k8s-facepalm-test-Default-HBLCWNKLQOGX-Workers-1OQTHBHWTZ2XD
  leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
  polling_autoscaler.go:105] Poll finished
  clusterstate.go:397] Readiness for node group data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO not found
  static_autoscaler.go:145] 1 unregistered nodes present
  utils.go:285] Removing unregistered node aws:///us-east-1c/i-097ef5790575cd23d
  utils.go:297] Failed to remove node aws:///us-east-1c/i-097ef5790575cd23d: min size reached, nodes will not be deleted
  static_autoscaler.go:152] Failed to remove unregistered nodes: min size reached, nodes will not be deleted
  clusterstate.go:237] Failed to find readiness information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  clusterstate.go:271] Failed to find readiness information for data-k8s-facepalm-test-Bigger-B2KPINOPUD2Q-Workers-1JJ3JT4O395CO
  leaderelection.go:204] successfully renewed lease kube-system/cluster-autoscaler
```

The section of `cluster.yaml`:

```
worker:
  apiEndpointName: internal
  nodePools:
    - name: default
      iam:
        instanceProfile:
          arn: ""{{ default_node_pool_iam_profile_arn }}""
      subnets:
        - name: PrivateInstance1
        - name: PrivateInstance2
        - name: PrivateInstance3
      securityGroupIds:
        - ""{{ default_node_pool_sg_id }}""
        - ""{{ bastion_ssh_access_sg_id }}""
        - ""{{ default_node_pool_access_sg_id }}""
      autoScalingGroup:
        minSize: {{ default_worker_asg_min_size }}
        maxSize: {{ default_worker_asg_max_size }}
        rollingUpdateMinInstancesInService: {{ default_worker_asg_min_in_service }}
      autoscaling:
        clusterAutoscaler:
          enabled: true
      instanceType: ""{{ default_worker_asg_instance_type }}""
      rootVolume:
        # Disk size (GiB) for controller node
        size: {{ default_worker_volume_size }}
      kube2IamSupport:
        enabled: true
      nodeLabels:
        data.ub.net/role: worker
      loadBalancer:
        enabled: true
        subnets:
          - name: PublicLoadBalancer1
          - name: PublicLoadBalancer2
          - name: PublicLoadBalancer3
    - name: bigger
      iam:
        instanceProfile:
          arn: ""{{ default_node_pool_iam_profile_arn }}""
      subnets:
        - name: PrivateInstance1
        - name: PrivateInstance2
        - name: PrivateInstance3
      securityGroupIds:
        - ""{{ default_node_pool_sg_id }}""
        - ""{{ bastion_ssh_access_sg_id }}""
        - ""{{ default_node_pool_access_sg_id }}""
      autoScalingGroup:
        minSize: {{ bigger_worker_asg_min_size }}
        maxSize: {{ bigger_worker_asg_max_size }}
        rollingUpdateMinInstancesInService: {{ bigger_worker_asg_min_in_service }}
      autoscaling:
        clusterAutoscaler:
          enabled: true
      instanceType: ""{{ bigger_worker_asg_instance_type }}""
      rootVolume:
        # Disk size (GiB) for controller node
        size: {{ bigger_worker_volume_size }}
      kube2IamSupport:
        enabled: true
      nodeLabels:
        data.ub.net/role: worker
        data.ub.net/owner: xyz
      loadBalancer:
        enabled: true
        subnets:
          - name: PublicLoadBalancer1
          - name: PublicLoadBalancer2
          - name: PublicLoadBalancer3
```",closed,False,2018-01-30 21:10:14,2018-02-06 19:37:45
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/594,https://api.github.com/repos/kubernetes/autoscaler/issues/594,Write some READMEs for VPA,cc @kgrygiel ,closed,True,2018-01-31 16:21:37,2018-02-02 00:16:21
autoscaler,ericln,https://github.com/kubernetes/autoscaler/issues/595,https://api.github.com/repos/kubernetes/autoscaler/issues/595,Scaling does not take nodeAffinity into account,"We have a few node groups running in our cluster and based on `taints` and `nodeAffinity` to ensure pods can only be scheduled onto certain group of the node. In the configuration of the cluster autoscaler deployment we specified all the AWS ASGs we have in the cluster, however as the pods scale, the one that has `tolerations` and `nodeAffinity` assgined, they do not seem to trigger the ASG to scale up.

",closed,False,2018-01-31 18:54:56,2018-07-22 12:40:53
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/596,https://api.github.com/repos/kubernetes/autoscaler/issues/596,metrics and specs clients package refactoring,"Proposal of a simple refactoring.
I'm not 100% sure about repeating package name in Interface name like: `spec.SpecClient`, however having two different `Client` interfaces imported in a single file also seemed a little confusing.

Let me know if it seems good to you or if you suggest a different approach.

CC: @kgrygiel @mwielgus ",closed,True,2018-01-31 19:43:44,2018-02-05 16:25:40
autoscaler,peter-bitfusion,https://github.com/kubernetes/autoscaler/pull/597,https://api.github.com/repos/kubernetes/autoscaler/issues/597,allow specifying arbitrary resource capacity via aws asg tags,"We're starting to work with device plugins and want to be able to scale up node groups from 0 when an unschedulable pod requests custom resources provided by a device plugin

for example my pod requests
```
resources:
  requests:
    acme.inc/widget: 5
```

This changeset allows adding aws asgs tags to specify additional resources capacity, using a autoscaler prefix similar to how node labels are specified. example tags: 
```
Key: k8s.io/cluster-autoscaler/node-template/resource-capacity/acme.inc/widget
Value: 5
```
Is this an acceptable approach?

Making the PR against master. Please advise if master isn't the right branch.",closed,True,2018-01-31 21:01:32,2018-07-03 00:00:43
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/598,https://api.github.com/repos/kubernetes/autoscaler/issues/598,CA 0.6.3,,closed,True,2018-02-01 00:24:17,2018-02-01 09:32:56
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/599,https://api.github.com/repos/kubernetes/autoscaler/issues/599,Merge examples into one directory.,+ change the number of hamster replicas to 2 so that VPA updater can actually update it.,closed,True,2018-02-01 10:45:21,2018-02-01 10:58:41
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/600,https://api.github.com/repos/kubernetes/autoscaler/issues/600,Make admission controller omit VPAs that are Off,,closed,True,2018-02-01 12:06:13,2018-02-02 00:18:18
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/601,https://api.github.com/repos/kubernetes/autoscaler/issues/601,Make updater disregard VPAs with status different than auto,,closed,True,2018-02-01 16:14:47,2018-02-02 00:19:26
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/602,https://api.github.com/repos/kubernetes/autoscaler/issues/602,Remove old unregistered nodes before checking cluster healthiness,Fixes #547 ,closed,True,2018-02-01 16:27:46,2018-02-02 12:30:55
autoscaler,zparnold,https://github.com/kubernetes/autoscaler/pull/603,https://api.github.com/repos/kubernetes/autoscaler/issues/603,update docs with RBAC info,Addresses #383 by creating both a ClusterRole (for cluster administration tasks) and a Role (for updating the `kube-system` namespace ConfigMap.) ,closed,True,2018-02-01 22:09:08,2018-02-06 10:01:08
autoscaler,rongshen,https://github.com/kubernetes/autoscaler/issues/604,https://api.github.com/repos/kubernetes/autoscaler/issues/604,"Failed to scale up: failed to build node infos for node groups: Wrong id: expected format aws:///&lt;zone&gt;/&lt;name&gt;, got ","Hello,

I'm looking for some help for CA. Forgive me if I did not post it at the right place.
My set up is the following:
1. Existing k8s clusters are all running on bare metal
2. Cluster is managed by kubeadm
3. kubeadm/kubectl/kubelet version: 1.9

What I want to achieve is an elastic k8s cluster with auto scale up and down on AWS ec2 instances.
I have done the following:
1. created an AMI with all required k8s node configs
2. created IAM role with all auto scaling permissions
3. created an AWS ec2 instance with the IAM role attached, and added it to existing cluster with kubeadm
4. Deployed CA on that ec2 node

My CA yaml file is like this:
```
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
        - image: k8s.gcr.io/cluster-autoscaler:v1.1.0
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --node=1:5:k8s-asg
          env:
            - name: AWS_REGION
              value: us-west-1
          imagePullPolicy: ""Always""
      nodeSelector:
          dedicated: test
```
I did not mount on ca-certificate.crt because my ec2 instances does not have it. It is a Centos based instance.

Now I'm seeing this error in log:
```
 W0202 01:21:48.983111       1 clusterstate.go:514] Failed to get nodegroup for a.k8s: Wrong id: expected format aws:///&lt;zone&gt;/&lt;name&gt;, got 
W0202 01:21:48.983116       1 clusterstate.go:514] Failed to get nodegroup for b.k8s: Wrong id: expected format aws:///&lt;zone&gt;/&lt;name&gt;, got 
I0202 01:21:48.983183       1 static_autoscaler.go:230] Filtering out schedulables
I0202 01:21:48.983848       1 static_autoscaler.go:240] No schedulable pods
I0202 01:21:48.983861       1 scale_up.go:54] Pod default/rong-test-j7p4q is unschedulable
I0202 01:21:48.983865       1 scale_up.go:54] Pod default/rong-test-hvrl5 is unschedulable
I0202 01:21:48.983868       1 scale_up.go:54] Pod default/rong-test-gjmr6 is unschedulable
E0202 01:21:48.983881       1 static_autoscaler.go:262] Failed to scale up: failed to build node infos for node groups: Wrong id: expected format aws:///&lt;zone&gt;/&lt;name&gt;, got 
```

Can anyone give me some tips on how I can make it work?

Thanks a lot",closed,False,2018-02-02 01:51:28,2018-06-13 12:51:56
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/605,https://api.github.com/repos/kubernetes/autoscaler/issues/605,Ground works for e2e testing for VPA,"- part of k8s/test/e2e/e2e.go forked to vpa
- vendor dependencies updated
- placeholder for VPA e2e test
",closed,True,2018-02-02 09:03:11,2018-02-05 16:27:27
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/606,https://api.github.com/repos/kubernetes/autoscaler/issues/606,Remove old unregistered nodes before checking cluster healthiness,,closed,True,2018-02-02 12:57:06,2018-02-02 14:05:42
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/607,https://api.github.com/repos/kubernetes/autoscaler/issues/607,Delete VPA pointer from PodState,,closed,True,2018-02-02 13:22:08,2018-02-02 18:14:26
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/issues/608,https://api.github.com/repos/kubernetes/autoscaler/issues/608,Cluster Autoscaler AWS cloudprovider tests are failing,"I believe this is a flake (example: https://travis-ci.org/kubernetes/autoscaler/jobs/336570454)
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws tests are sometimes failing. Logs from the failed test below

```
E0202 14:19:07.636800   11264 runtime.go:66] Observed a panic: ""\n\nmock: Unexpected Method Call\n-----------------------------\n\nDescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)\n\t\t0: {\n  AutoScalingGroupNames: [\""coolasg\""],\n  MaxRecords: 100\n}\n\t\t1: (func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)(0x12e7a50)\n\nThe closest call I have is: \n\nDescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,mock.AnythingOfTypeArgument)\n\t\t0: {\n  AutoScalingGroupNames: [\""coolasg\""],\n  MaxRecords: 100\n}\n\t\t1: \""func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool\""\n\n\n"" (
mock: Unexpected Method Call
-----------------------------
DescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)
		0: {
  AutoScalingGroupNames: [""coolasg""],
  MaxRecords: 100
}
		1: (func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)(0x12e7a50)
The closest call I have is: 
DescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,mock.AnythingOfTypeArgument)
		0: {
  AutoScalingGroupNames: [""coolasg""],
  MaxRecords: 100
}
		1: ""func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool""
)
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:72
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:65
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:51
/home/travis/.gimme/versions/go1.8.3.linux.amd64/src/runtime/asm_amd64.s:514
/home/travis/.gimme/versions/go1.8.3.linux.amd64/src/runtime/panic.go:489
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/stretchr/testify/mock/mock.go:311
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/stretchr/testify/mock/mock.go:288
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider_test.go:40
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/auto_scaling.go:89
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/auto_scaling_groups.go:154
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/auto_scaling_groups.go:53
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134
/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88
/home/travis/.gimme/versions/go1.8.3.linux.amd64/src/runtime/asm_amd64.s:2197
panic: 
mock: Unexpected Method Call
-----------------------------
DescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)
		0: {
  AutoScalingGroupNames: [""coolasg""],
  MaxRecords: 100
}
		1: (func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)(0x12e7a50)
The closest call I have is: 
DescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,mock.AnythingOfTypeArgument)
		0: {
  AutoScalingGroupNames: [""coolasg""],
  MaxRecords: 100
}
		1: ""func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool""
 [recovered]
	panic: 
mock: Unexpected Method Call
-----------------------------
DescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)
		0: {
  AutoScalingGroupNames: [""coolasg""],
  MaxRecords: 100
}
		1: (func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool)(0x12e7a50)
The closest call I have is: 
DescribeAutoScalingGroupsPages(*autoscaling.DescribeAutoScalingGroupsInput,mock.AnythingOfTypeArgument)
		0: {
  AutoScalingGroupNames: [""coolasg""],
  MaxRecords: 100
}
		1: ""func(*autoscaling.DescribeAutoScalingGroupsOutput, bool) bool""
goroutine 48 [running]:
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:58 +0x126
panic(0x1405a20, 0xc4203050c0)
	/home/travis/.gimme/versions/go1.8.3.linux.amd64/src/runtime/panic.go:489 +0x2cf
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/stretchr/testify/mock.(*Mock).MethodCalled(0xc4202aec40, 0x21adbda, 0x1e, 0xc420354320, 0x2, 0x2, 0x4, 0x10, 0xc420042b30)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/stretchr/testify/mock/mock.go:311 +0x654
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/stretchr/testify/mock.(*Mock).Called(0xc4202aec40, 0xc420354320, 0x2, 0x2, 0x10, 0x153cd60, 0x1)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/stretchr/testify/mock/mock.go:288 +0x14c
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AutoScalingMock).DescribeAutoScalingGroupsPages(0xc4202aec40, 0xc420344180, 0xc420304440, 0x227c6b8, 0x0)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider_test.go:40 +0xa9
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*autoScalingWrapper).getAutoscalingGroupsByNames(0xc420311c00, 0xc420304410, 0x1, 0x1, 0x1, 0x1, 0x0, 0x227c6b8, 0x50)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/auto_scaling.go:89 +0x1c2
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*asgCache).regenerate(0xc420311bd0, 0x1765240, 0xc420311bf8)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/auto_scaling_groups.go:154 +0x33a
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.newASGCache.func1()
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/auto_scaling_groups.go:53 +0x7d
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil.func1(0xc4201e1b20)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133 +0x5e
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc4201e1b20, 0x34630b8a000, 0x0, 0xc42053ed01, 0xc4201a5980)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134 +0xbd
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait.Until(0xc4201e1b20, 0x34630b8a000, 0xc4201a5980)
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88 +0x4d
created by k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.newASGCache
	/home/travis/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/auto_scaling_groups.go:56 +0x21d
FAIL	k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws	0.201s
```",closed,False,2018-02-02 14:26:17,2018-03-28 16:49:59
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/609,https://api.github.com/repos/kubernetes/autoscaler/issues/609,Cluster Autoscaler 1.0.4,,closed,True,2018-02-02 14:49:22,2018-02-02 15:33:22
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/610,https://api.github.com/repos/kubernetes/autoscaler/issues/610,Remove old unregistered nodes before checking cluster healthiness,,closed,True,2018-02-02 14:58:19,2018-02-02 15:43:52
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/611,https://api.github.com/repos/kubernetes/autoscaler/issues/611,Cluster Autoscaler 1.1.1,,closed,True,2018-02-02 15:32:32,2018-02-02 15:33:10
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/612,https://api.github.com/repos/kubernetes/autoscaler/issues/612,Refactoring: loading ClusterState,"Comming back with the idea of ""ClusterStateFeeder"", now in a different form. 
The goal of this refactoring is to:
1. encapsulate logic which loads data into clusterState
2. limit the number of Recommender dependencies

@kgrygiel for a while, I was playing around with the idea of making feeder stateless, but then Recommender would stay with a number of dependencies, which would be only passed as parameters to external functions.

@kgrygiel @mwielgus - please review!

(Please merge https://github.com/kubernetes/autoscaler/pull/596 before review, as this PR depends on it)",closed,True,2018-02-03 21:59:23,2018-02-20 09:29:57
autoscaler,hangyan,https://github.com/kubernetes/autoscaler/pull/613,https://api.github.com/repos/kubernetes/autoscaler/issues/613,Remove unused function parameter in PollAPIServer,,closed,True,2018-02-05 08:01:11,2018-02-05 16:28:44
autoscaler,ankon,https://github.com/kubernetes/autoscaler/pull/614,https://api.github.com/repos/kubernetes/autoscaler/issues/614,Fix reference to the docker image in the example,"It seems 1.8 doesn't exist, but 1.8.0 does -- and there is a 1.8.1 already.

----
Note that on the _1.8_ branch pointed to by the README the example actually talks about _1.7_, so I'm somewhat assuming that master is more ""up-to-date"" than that branch.",closed,True,2018-02-05 17:11:05,2018-03-14 15:53:23
autoscaler,tunblr,https://github.com/kubernetes/autoscaler/pull/615,https://api.github.com/repos/kubernetes/autoscaler/issues/615,Fix: var name typo,,closed,True,2018-02-06 02:57:27,2018-02-06 08:46:37
autoscaler,hangyan,https://github.com/kubernetes/autoscaler/pull/616,https://api.github.com/repos/kubernetes/autoscaler/issues/616,Fix const string typo,,closed,True,2018-02-06 06:17:01,2018-02-06 08:47:01
autoscaler,hangyan,https://github.com/kubernetes/autoscaler/pull/617,https://api.github.com/repos/kubernetes/autoscaler/issues/617,Fix function name typo,,closed,True,2018-02-06 09:16:19,2018-02-06 09:56:47
autoscaler,johanneswuerbach,https://github.com/kubernetes/autoscaler/issues/618,https://api.github.com/repos/kubernetes/autoscaler/issues/618,Release Cluster Autoscaler 1.2 rc,"Looks like there have been big changes to the AWS provider (e.g. https://github.com/kubernetes/autoscaler/pull/462) and I would like to check whether the new code resolves https://github.com/kubernetes/autoscaler/issues/252#issuecomment-345611287

Would there be a chance to release a 1.2 RC including those changes?

//cc @mwielgus 

",closed,False,2018-02-06 09:29:35,2018-03-20 14:40:36
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/619,https://api.github.com/repos/kubernetes/autoscaler/issues/619,VPA e2e contribution,"- e2e test to verify that recommender publishes recommendation.
- Script for running VPA e2e tests.
",closed,True,2018-02-06 11:08:06,2018-02-06 16:37:46
autoscaler,lmxia,https://github.com/kubernetes/autoscaler/pull/620,https://api.github.com/repos/kubernetes/autoscaler/issues/620,fix typos in balance_similar.md and pricing.md,fix some typos,closed,True,2018-02-06 14:52:47,2018-02-06 16:36:52
autoscaler,yank1,https://github.com/kubernetes/autoscaler/pull/621,https://api.github.com/repos/kubernetes/autoscaler/issues/621,fix typo in main file,fix typo in main file,closed,True,2018-02-06 16:28:22,2018-02-06 16:44:51
autoscaler,jamiefang,https://github.com/kubernetes/autoscaler/pull/622,https://api.github.com/repos/kubernetes/autoscaler/issues/622,Fix typos paramters -> [parameters]  multipe -> [multiple],Fix typos paramters -> [parameters]  multipe -> [multiple],closed,True,2018-02-07 04:01:34,2018-02-07 08:57:08
autoscaler,ddongyu,https://github.com/kubernetes/autoscaler/pull/623,https://api.github.com/repos/kubernetes/autoscaler/issues/623,Typo fix unneded->[unneeded],,closed,True,2018-02-07 05:41:56,2018-02-07 16:36:58
autoscaler,hangyan,https://github.com/kubernetes/autoscaler/pull/624,https://api.github.com/repos/kubernetes/autoscaler/issues/624,Fix various typos in clusterstate package,,closed,True,2018-02-07 08:04:11,2018-02-07 08:58:07
autoscaler,sailingwithoutwind,https://github.com/kubernetes/autoscaler/pull/625,https://api.github.com/repos/kubernetes/autoscaler/issues/625,some typo,,closed,True,2018-02-07 08:09:51,2018-02-07 08:58:40
autoscaler,hangyan,https://github.com/kubernetes/autoscaler/pull/626,https://api.github.com/repos/kubernetes/autoscaler/issues/626,Fix various typos in proposals,,closed,True,2018-02-07 08:12:40,2018-02-07 08:59:07
autoscaler,2eron,https://github.com/kubernetes/autoscaler/pull/627,https://api.github.com/repos/kubernetes/autoscaler/issues/627,Fix typo in code comment,"likekly -> likely
intialize -> initialize
ppod -> pod",closed,True,2018-02-07 08:41:50,2018-02-07 08:59:28
autoscaler,andyxning,https://github.com/kubernetes/autoscaler/pull/628,https://api.github.com/repos/kubernetes/autoscaler/issues/628,replace deprecated maintainer instruction with label,"As Dockerfile `MAINTAINER` instruction has been deprecated, we should use `LABEL maintainer=xxx` to replace original `MAINTAINER`. 

/cc @bskiba @mwielgus ",closed,True,2018-02-07 14:40:48,2018-02-08 07:07:45
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/629,https://api.github.com/repos/kubernetes/autoscaler/issues/629,Add release notes for 1.1.0 and 1.1.1 to readme,fixes #586 ,closed,True,2018-02-07 15:08:59,2018-02-07 16:36:04
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/630,https://api.github.com/repos/kubernetes/autoscaler/issues/630,Introduce a common deterministic mechanism for matching pods to VPAs,,closed,True,2018-02-07 15:15:32,2018-02-07 21:22:01
autoscaler,andyxning,https://github.com/kubernetes/autoscaler/issues/631,https://api.github.com/repos/kubernetes/autoscaler/issues/631,Update deployment group from extensions to apps,"Since deployment has been move to apps group since Kubernetes 1.8 and has been reached `v1` in Kubernetes 1.9. Maybe we can bump deployment group from extensions to apps.

/cc @bskiba @mwielgus ",closed,False,2018-02-07 15:27:35,2018-10-06 01:55:09
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/632,https://api.github.com/repos/kubernetes/autoscaler/issues/632,Make admission controller set default mode for VPA objects,,closed,True,2018-02-07 16:45:44,2018-02-07 21:23:09
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/633,https://api.github.com/repos/kubernetes/autoscaler/issues/633,Support for Conditions in VPA.,,closed,True,2018-02-07 22:36:03,2018-02-13 13:01:08
autoscaler,xfu8309,https://github.com/kubernetes/autoscaler/issues/634,https://api.github.com/repos/kubernetes/autoscaler/issues/634,typo of cloudprovider aws README.md,"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup

Command for ""Auto-Discovery Setup"" is 
`kubectl apply -f examples/cluster-autoscaler-run-on-master.yaml `
but should is
`kubectl apply -f examples/cluster-autoscaler-autodiscover.yaml `
",closed,False,2018-02-08 09:57:37,2018-07-12 12:45:12
autoscaler,carlossg,https://github.com/kubernetes/autoscaler/pull/635,https://api.github.com/repos/kubernetes/autoscaler/issues/635,storageclasses permission is required,"Getting this permission error on deployment

```
Listing and watching *v1.StorageClass from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:86
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:86: Failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User ""system:serviceaccount:kube-system:cluster-autoscaler"" cannot list storageclasses.storage.k8s.io at the cluster scope
```",closed,True,2018-02-08 12:17:23,2018-02-08 14:30:13
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/636,https://api.github.com/repos/kubernetes/autoscaler/issues/636,Merge admission controller RBAC into main vpa-rbac file.,,closed,True,2018-02-08 15:01:46,2018-02-08 15:36:53
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/637,https://api.github.com/repos/kubernetes/autoscaler/issues/637,Scripts for handing E2E testing in test-infra,,closed,True,2018-02-08 15:06:40,2018-02-08 16:04:55
autoscaler,dineshsckloud9,https://github.com/kubernetes/autoscaler/issues/638,https://api.github.com/repos/kubernetes/autoscaler/issues/638,Will cluster-autoscaler work for openshift cluster created in GCE,"We have a openshift cluster setup with instance group in GCE. We could not achieve node autoscaling for openshift nodes. 

Can we run this cluster-autoscaler as a pod in openshift master instance to make autoscaling work?

Will it work? Suggest me any other way if cluster-autoscaler does not work.

Note: We have taken cluster-autoscaler v0.6.1 pod config from kubernetes and have modified the yaml for openshift.

/sig scalability
/sig autoscaling
/sig openshift",closed,False,2018-02-09 07:11:05,2018-09-20 05:40:26
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/639,https://api.github.com/repos/kubernetes/autoscaler/issues/639,Use Debian base to build Cluster Autoscaler image,Use Debian base to build Cluster Autoscaler image.,closed,True,2018-02-09 13:54:45,2018-02-14 12:17:32
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/640,https://api.github.com/repos/kubernetes/autoscaler/issues/640,Disable checking inter pod affinity predicate if not necessary,Add disabling inter pod affinity predicate if only preferred / node affinity used.,closed,True,2018-02-09 14:31:34,2018-02-14 14:35:52
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/641,https://api.github.com/repos/kubernetes/autoscaler/issues/641,Document useInstanceMetadata settings for Azure standard vmType,"useInstanceMetadata should be set to false. Or else, node's providerID will be in `azure://uuid` format and autoscaler won't be able to recognize it. 

In the future kubernetes releases, node's providerID will be updated to correct version (https://github.com/kubernetes/kubernetes/pull/59539).

",closed,True,2018-02-09 15:16:45,2018-02-14 03:55:13
autoscaler,gmagniez,https://github.com/kubernetes/autoscaler/issues/642,https://api.github.com/repos/kubernetes/autoscaler/issues/642,Support for extension parameters on Azure,"We use a custom extension (to be precise, a preprovision extension) which needs a parameter, but when trying to scale we face the following error:
``` 
E0209 14:20:37.252154       1 static_autoscaler.go:298] Failed to scale up: failed to increase node group size: resources.DeploymentsClient#CreateOrUpdate: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code=""InvalidDeploymentParameterValue"" Message=""The value of deployment parameter 'register-dnsParameters' is null. Please specify the value or use the parameter reference. See https://aka.ms/arm-deploy/#parameter-file for details.""
``` 
It seems that the generated ARM deployment is missing the parameter 'register-dnsParameters' which is defined in our acs-engine cluster definition:
``` 
""extensionProfiles"": [
      {
          ""name"": ""register-dns"",
          ""rootURL"": ""https://raw.githubusercontent.com/tesharp/acs-engine/register-dns-extension/"",
          ""version"": ""v1"",
          ""script"": ""register-dns.sh"",
          ""extensionParameters"": ""my.domain.name.com""
      }
    ],
``` ",closed,False,2018-02-09 16:37:34,2018-03-02 03:27:31
autoscaler,mfaizanse,https://github.com/kubernetes/autoscaler/issues/643,https://api.github.com/repos/kubernetes/autoscaler/issues/643,Is the Cluster-AutoScaler restarts/manages if any node in the cluster dies or crashes? [AWS],"Is the Cluster-AutoScaler restarts/manages the node if any node in the cluster dies or crashes? I can see that it can scale up or scale down but what if a node dies? Does it maintains the desired number of nodes in the cluster?

AWS",closed,False,2018-02-10 06:37:51,2018-05-14 09:46:41
autoscaler,alexquintero,https://github.com/kubernetes/autoscaler/issues/644,https://api.github.com/repos/kubernetes/autoscaler/issues/644,Azure based acs-engine autoscaler failing with standard vm type (availability set),"**Issue**:
Cluster-autoscaler appears to be requesting a scaleset from Azure despite the vmtype being set to **standard**.

**Log Output**:
```
E0212 18:25:12.947818       1 static_autoscaler.go:135] Failed to update node registry: compute.VirtualMachineScaleSetsClient#Get: Failure responding to request: StatusCode=404 -- Original Error: autorest/azure: Service returned an error. Status=404 Code=""ResourceNotFound"" Message=""The Resource 'Microsoft.Compute/virtualMachineScaleSets/genpop' under resource group 'k8s' was not found.""
```

**Versions**:
Kubernetes version 1.9.2
ACS-Engine version 0.12.5
cluster-autoscaler version v1.1.1

**Additional Details**:

My deployment is based entirely on the azure cloudprovider readme's [cluster-autoscaler-standard-master.yaml](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/cluster-autoscaler-standard-master.yaml).

I made the following modifications:

- Set the image line in the deployment to match my Kubernetes version `image: gcr.io/google_containers/cluster-autoscaler:v1.1.1`
- Added additional rights to the clusterrole for storageclasses
```
- apiGroups: [""storage.k8s.io""]
  resources: [""storageclasses""]
  verbs: [""get"", ""list"", ""watch""]
```
- I am not going to post my secret.yaml file for obvious reasons but the VMType is set to `c3RhbmRhcmQ=` which is the base64 of `standard`, but as I mentioned this value seems to be getting ignored. I filled in the rest of the values based on my service principal and the acs-engine parameters as directed by the readme.
",closed,False,2018-02-12 18:44:11,2018-02-26 12:18:36
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/645,https://api.github.com/repos/kubernetes/autoscaler/issues/645,storageclass permission is required,"Without this, cluster-autoscaler would report errors:

```
Listing and watching *v1.StorageClass from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:86
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:86: Failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User ""system:serviceaccount:kube-system:cluster-autoscaler"" cannot list storageclasses.storage.k8s.io at the cluster scope
```
",closed,True,2018-02-13 05:48:12,2018-02-14 12:15:05
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/646,https://api.github.com/repos/kubernetes/autoscaler/issues/646,Export REGISTRY/TAG variables for VPA e2e test.,,closed,True,2018-02-13 11:27:21,2018-02-13 16:12:39
autoscaler,TimJones,https://github.com/kubernetes/autoscaler/issues/647,https://api.github.com/repos/kubernetes/autoscaler/issues/647,Min & max not set for AWS ASG,"I have a small K8S v1.8.6 cluster deployed using kops with `--node-count 2`. This sets up an ASG with Min\Max instances both set to 2.

I then install an autoscaler Deployment with `--nodes=1:3:<asg-name>` but listing the ASG still shows Min\Max both set to 2.

I have to run `kops edit instancegroup nodes` to set the min\max to match before autoscaler can work.

I would expect autoscaler to set the min\max based on its config at startup.",closed,False,2018-02-13 15:16:09,2018-11-02 03:32:39
autoscaler,discordianfish,https://github.com/kubernetes/autoscaler/pull/648,https://api.github.com/repos/kubernetes/autoscaler/issues/648,"AWS: Fix nvidia gpu resource name, re-enable ScaleToZero","These are two somewhat separate changes, so let me know if I should split this PR:

1) Rename gpu resource name, the same as #407 but for AWS
2) Set scaleToZero = true. It looks like this was changed (by accident?) in #462

I've tested the changes manually and scaling to and from 0 works on my cluster with the nvidia k8s device plugin.",closed,True,2018-02-13 16:14:58,2018-07-23 13:27:36
autoscaler,bobhenkel,https://github.com/kubernetes/autoscaler/issues/649,https://api.github.com/repos/kubernetes/autoscaler/issues/649,Error opening stream to...(Trouble Hitting K8s API EndPoint from Pod),"Seeing this message about 5 mins after deploying this to my cluster(Cluster is stood up with kops1.8.0):
```
> stern aws-cluster -n kube-tools
Error opening stream to kube-tools/workers-aws-cluster-autoscaler-6b79944dcd-sn67w: aws-cluster-autoscaler
: Get https://api.debug-cluster.bob.io/api/v1/namespaces/kube-tools/pods/workers-aws-cluster-autoscaler-6b79944dcd-sn67w/log?container=aws-cluster-autoscaler&follow=true: unexpected EOF
```
Other than that log message the container has no logs in it.

Kubernetes Version:
```
> kubectl version
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.8"", GitCommit:""2f73858c9e6ede659d6828fe5a1862a48034a0fd"", GitTreeState:""clean"", BuildDate:""2018-02-09T21:30:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.6"", GitCommit:""6260bb08c46c31eea6cb538b34a9ceb3e406689c"", GitTreeState:""clean"", BuildDate:""2017-12-21T06:23:29Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Installed autoscaler with helm chart from [https://github.com/kubernetes/charts/tree/master/stable/cluster-autoscaler](https://github.com/kubernetes/charts/tree/master/stable/cluster-autoscaler)

- Using image v1.04 found here [https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/cluster-autoscaler?gcrImageListsize=50](https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/cluster-autoscaler?gcrImageListsize=50)

If I try and hit this end point from my workstation with wget I this(my workstation ip is allowed access to api, but normally I hit it by using kubectl proxy and then hitting localhost:8001 which works, but hitting DNS for the API doesn't as seen below):
```
wget --no-check-certificate https://api.debug-cluster.bob.io/api/v1                                                                                                                      17:40:06
--2018-02-13 17:40:20--  https://api.debug-cluster.bob.io/api/v1
Resolving api.debug-cluster.bob.io... zz.224.zz.zz, zz.198.zz.zz, zz.54.zz.zz
Connecting to api.debug-cluster.bob.io|zz.224.zz.zz|:443... connected.
WARNING: cannot verify api.debug-cluster.bob.io's certificate, issued by ‘CN=kubernetes’:
  Unable to locally verify the issuer's authority.
HTTP request sent, awaiting response... 401 Unauthorized

Username/Password Authentication Failed.
```

```
> kubectl get configmap cluster-autoscaler-status -n kube-tools -o yaml                                                                                                       17:58:40
apiVersion: v1
data:
  status: |+
    Cluster-autoscaler status at 2018-02-13 23:58:51.208177195 +0000 UTC:
    Cluster-wide:
      Health:      Healthy (ready=9 unready=0 notStarted=0 longNotStarted=0 registered=9 longUnregistered=0)
                   LastProbeTime:      2018-02-13 23:58:50.975019509 +0000 UTC
                   LastTransitionTime: 2018-02-13 23:07:15.203463379 +0000 UTC
      ScaleUp:     NoActivity (ready=9 registered=9)
                   LastProbeTime:      2018-02-13 23:58:50.975019509 +0000 UTC
                   LastTransitionTime: 2018-02-13 23:07:15.203463379 +0000 UTC
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2018-02-13 23:58:50.975019509 +0000 UTC
                   LastTransitionTime: 2018-02-13 23:07:15.203463379 +0000 UTC

    NodeGroups:
      Name:        nodes.debug-cluster.bob.io
      Health:      Healthy (ready=6 unready=0 notStarted=0 longNotStarted=0 registered=6 longUnregistered=0 cloudProviderTarget=6 (minSize=6, maxSize=30))
                   LastProbeTime:      2018-02-13 23:58:50.975019509 +0000 UTC
                   LastTransitionTime: 2018-02-13 23:07:15.203463379 +0000 UTC
      ScaleUp:     NoActivity (ready=6 cloudProviderTarget=6)
                   LastProbeTime:      2018-02-13 23:58:50.975019509 +0000 UTC
                   LastTransitionTime: 2018-02-13 23:07:15.203463379 +0000 UTC
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2018-02-13 23:58:50.975019509 +0000 UTC
                   LastTransitionTime: 2018-02-13 23:07:15.203463379 +0000 UTC

kind: ConfigMap
metadata:
  annotations:
    cluster-autoscaler.kubernetes.io/last-updated: 2018-02-13 23:58:51.208177195 +0000
      UTC
  creationTimestamp: 2018-02-13T23:07:03Z
  name: cluster-autoscaler-status
  namespace: kube-tools
  resourceVersion: ""1592956""
  selfLink: /api/v1/namespaces/kube-tools/configmaps/cluster-autoscaler-status
  uid: 9ac056c2-1112-11e8-94c2-06666fa33b54
```
Any ideas on what I need to do to allow that pod to hit my API endpoint or is this message a false alarm. So far I have only seen it show up once?

Thanks for any help you can provide!


",closed,False,2018-02-13 23:48:08,2018-07-30 12:49:22
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/650,https://api.github.com/repos/kubernetes/autoscaler/issues/650,Add reviewers,"Add me to cluster-autoscaler reviewers.

/assign @mwielgus @MaciekPytel",closed,True,2018-02-14 12:27:51,2018-02-16 12:16:33
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/651,https://api.github.com/repos/kubernetes/autoscaler/issues/651,Decaying histograms,,closed,True,2018-02-14 13:20:30,2018-02-20 15:39:56
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/652,https://api.github.com/repos/kubernetes/autoscaler/issues/652,Use Debian image,Cherry-pick of #639 ,closed,True,2018-02-14 13:21:28,2018-02-14 14:35:16
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/653,https://api.github.com/repos/kubernetes/autoscaler/issues/653,Use Debian image,Cherry pick of #639 ,closed,True,2018-02-14 13:30:04,2018-02-14 14:34:53
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/654,https://api.github.com/repos/kubernetes/autoscaler/issues/654,Use Debian image,Cherry-pick of #639 ,closed,True,2018-02-14 13:30:40,2018-02-14 14:34:32
autoscaler,rvkubiak,https://github.com/kubernetes/autoscaler/issues/655,https://api.github.com/repos/kubernetes/autoscaler/issues/655,cluster-autoscaler tests broken by missing dependency,"I've been working on build automation for the cluster-autoscaler, so I believe this was broken fairly recently.  I have a build result from 2 days ago where the tests passed as expected.

Today, if I try to run 'make test-in-docker' for cluster-autoscaler it fails.  It seems like this is probably related to the recent change away from ubuntu-slim.
https://github.com/kubernetes/autoscaler/commit/8070f370239526e5e75166fc612454b24d63a55f#diff-c414d0c9ddb3d63d0f645e12211318af

~/autoscaler-broken/cluster-autoscaler$ make test-in-docker
rm -f cluster-autoscaler
docker build -t autoscaling-builder ../builder
Sending build context to Docker daemon  3.584kB
Step 1/8 : FROM golang:1.8.3
 ---> 7e62a8729fa7
Step 2/8 : LABEL maintainer ""Marcin Wielgus <mwielgus@google.com>""
 ---> Using cache
 ---> 00d9b65460cf
Step 3/8 : ENV GOPATH /gopath/
 ---> Using cache
 ---> 8cf7b9d9dfbf
Step 4/8 : ENV PATH $GOPATH/bin:$PATH
 ---> Using cache
 ---> 4e8a44b814d2
Step 5/8 : RUN go version
 ---> Using cache
 ---> a9270542558d
Step 6/8 : RUN go get github.com/tools/godep
 ---> Using cache
 ---> 86671c0a4972
Step 7/8 : RUN godep version
 ---> Using cache
 ---> ebf2c59b5d4f
Step 8/8 : CMD /bin/bash
 ---> Using cache
 ---> 3a50faf81849
Successfully built 3a50faf81849
Successfully tagged autoscaling-builder:latest
docker run -v `pwd`:/gopath/src/k8s.io/autoscaler/cluster-autoscaler/ autoscaling-builder:latest bash -c 'cd /gopath/src/k8s.io/autoscaler/cluster-autoscaler && godep go test ./... '
# k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/seccomp/libseccomp-golang
vendor/github.com/seccomp/libseccomp-golang/seccomp.go:25:22: fatal error: seccomp.h: No such file or directory
 // #include <seccomp.h>
                      ^
compilation terminated.
<snip>
godep: go exit status 2
Makefile:52: recipe for target 'test-in-docker' failed
make: *** [test-in-docker] Error 1",closed,False,2018-02-15 00:32:53,2018-02-15 23:48:16
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/656,https://api.github.com/repos/kubernetes/autoscaler/issues/656,Add installing libseccomp-devel to builder,Add installing libseccomp-devel to builder.,closed,True,2018-02-15 10:32:23,2018-02-15 16:05:07
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/657,https://api.github.com/repos/kubernetes/autoscaler/issues/657,Loosen test conditions for patching VPA objects.,,closed,True,2018-02-15 13:06:01,2018-02-15 16:05:41
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/658,https://api.github.com/repos/kubernetes/autoscaler/issues/658,Fix handling computing priorities for milli quantities.,,closed,True,2018-02-15 15:44:47,2018-02-15 16:06:18
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/659,https://api.github.com/repos/kubernetes/autoscaler/issues/659,VPA updater e2e test introduced.,,closed,True,2018-02-15 15:50:19,2018-02-15 18:11:59
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/660,https://api.github.com/repos/kubernetes/autoscaler/issues/660,Cherry pick update dockerfiles fix 0.6,Cherry-pick of #656 ,closed,True,2018-02-15 16:14:43,2018-02-15 16:14:53
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/661,https://api.github.com/repos/kubernetes/autoscaler/issues/661,Add installing libseccomp-devel to builder,Cherry-pick of #656 ,closed,True,2018-02-15 16:16:02,2018-02-15 18:05:59
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/662,https://api.github.com/repos/kubernetes/autoscaler/issues/662,Add installing libseccomp-devel to builder,Cherry-pick of #656 ,closed,True,2018-02-15 16:16:45,2018-02-15 18:05:42
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/663,https://api.github.com/repos/kubernetes/autoscaler/issues/663,Cherry pick update dockerfiles fix 1.1,Cherry-pick of #656 ,closed,True,2018-02-15 16:17:09,2018-02-15 16:17:17
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/664,https://api.github.com/repos/kubernetes/autoscaler/issues/664,Add installing libseccomp-devel to builder,Cherry-pick of #656 ,closed,True,2018-02-15 16:18:12,2018-02-15 18:05:27
autoscaler,rvkubiak,https://github.com/kubernetes/autoscaler/pull/665,https://api.github.com/repos/kubernetes/autoscaler/issues/665,Add GCB config for cluster-autoscaler,"Enables building of the cluster-autoscaler container using Google
Container Builder.  This is part of a larger effort to automate and
establish provenance for Kubernetes add-on images.",closed,True,2018-02-16 00:11:47,2018-02-19 12:52:58
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/666,https://api.github.com/repos/kubernetes/autoscaler/issues/666,Clean up node pool creation in tests,"This is to track progress for investigating and cleaning up issues discussed in kubernetes/kubernetes#59913 : 
- possibly redundant waits after synchronous node-pool create
- possibly redundant functions for getting node-pool's size",closed,False,2018-02-16 10:38:27,2018-10-14 12:14:05
autoscaler,pradeeppandeyy,https://github.com/kubernetes/autoscaler/issues/667,https://api.github.com/repos/kubernetes/autoscaler/issues/667,Getting  x509: certificate signed by unknown authority  after deploying autoscaler. ,"Getting below after deploying autoscaler. 

E0217 01:43:51.183076       5 static_autoscaler.go:135] Failed to update node registry: Get https://www.googleapis.com/compute/v1/projects/kubernetesforwebsite/zones/us-east1-b/instanceGroupManagers/openshift-node?alt=json: x509: certificate signed by unknown authority
I0217 01:43:52.820515       5 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0217 01:43:54.827981       5 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0217 01:43:56.834152       5 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0217 01:43:58.840372       5 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0217 01:44:00.846470       5 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler",closed,False,2018-02-17 01:49:56,2018-02-19 18:39:43
autoscaler,pradeeppandeyy,https://github.com/kubernetes/autoscaler/issues/668,https://api.github.com/repos/kubernetes/autoscaler/issues/668,getting error: Failed to get nodes from apiserver,"getting below error, 

0217 02:44:16.958401       5 main.go:320] Failed to get nodes from apiserver: Get https://openshift-master-1:8443/api/v1/nodes: x509: failed to load system roots and no roots provided
I0217 02:44:39.824677       5 flags.go:52] FLAG: --address="":8085""
I0217 02:44:39.824813       5 flags.go:52] FLAG: --alsologtostderr=""false""
",closed,False,2018-02-17 02:50:03,2018-02-19 18:39:01
autoscaler,pradeeppandeyy,https://github.com/kubernetes/autoscaler/issues/669,https://api.github.com/repos/kubernetes/autoscaler/issues/669,Couldn't read config: warning: Failed to create GCE Manager,"I0217 19:37:05.849940       5 reflector.go:236] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:144
I0217 19:37:05.850063       5 reflector.go:236] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:169
I0217 19:37:05.850162       5 reflector.go:236] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:207
I0217 19:37:05.850269       5 reflector.go:236] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:237
I0217 19:37:05.850309       5 reflector.go:236] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:259
I0217 19:37:06.044719       5 request.go:638] Throttling request took 194.371932ms, request: GET:https://172.30.0.1:443/api/v1/nodes?resourceVersion=0
I0217 19:37:06.244718       5 request.go:638] Throttling request took 378.657428ms, request: PUT:https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status
E0217 19:37:06.252461       5 gce_manager.go:71] Couldn't read config: warning:
can't store data at section ""global"", variable ""network-project-id""
F0217 19:37:06.252509       5 cloud_provider_builder.go:65] Failed to create GCE Manager: <nil>",closed,False,2018-02-17 19:37:23,2018-02-19 18:37:33
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/670,https://api.github.com/repos/kubernetes/autoscaler/issues/670, Refine support for VPA e2e test suites.,+ VPA creation and listing for debugging test-infra.,closed,True,2018-02-19 10:34:31,2018-02-19 15:09:25
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/671,https://api.github.com/repos/kubernetes/autoscaler/issues/671,Pass pod namespace into admission-controller logic.,,closed,True,2018-02-19 15:03:27,2018-02-19 16:12:00
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/672,https://api.github.com/repos/kubernetes/autoscaler/issues/672,VPA admission controller e2e test.,,closed,True,2018-02-20 10:48:44,2018-02-20 13:10:39
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/673,https://api.github.com/repos/kubernetes/autoscaler/issues/673,VPA e2e test infra debug cont.,,closed,True,2018-02-20 12:04:14,2018-02-20 12:23:29
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/674,https://api.github.com/repos/kubernetes/autoscaler/issues/674,Dump kubectl version and context name for VPA e2e tests.,,closed,True,2018-02-21 10:03:37,2018-02-21 10:16:21
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/675,https://api.github.com/repos/kubernetes/autoscaler/issues/675,Set KUBECONFIG variable in VPA e2e script.,,closed,True,2018-02-21 13:23:19,2018-02-21 13:25:46
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/676,https://api.github.com/repos/kubernetes/autoscaler/issues/676,Make requests smaller or equal to limits,This also extracts common code from updater & admission controller to a library.,closed,True,2018-02-21 16:37:24,2018-02-26 18:11:09
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/677,https://api.github.com/repos/kubernetes/autoscaler/issues/677,Use decaying histograms in the recommender.,,closed,True,2018-02-22 09:10:47,2018-02-22 09:25:33
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/678,https://api.github.com/repos/kubernetes/autoscaler/issues/678,Dump VPA tests report.,,closed,True,2018-02-22 13:02:17,2018-02-22 13:17:00
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/679,https://api.github.com/repos/kubernetes/autoscaler/issues/679,First VPA full stack e2e tests.,,closed,True,2018-02-22 15:00:23,2018-02-26 12:22:58
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/680,https://api.github.com/repos/kubernetes/autoscaler/issues/680,VPA actuation e2e test for pods stuck at pending.,,closed,True,2018-02-23 14:47:45,2018-02-26 20:16:58
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/681,https://api.github.com/repos/kubernetes/autoscaler/issues/681,Improve azure node's providerID handling,"This PR improves azure node's providerID handling by:

- Avoid converting cases for azure node's providerID 
- Use GET API to fetch node's ID for vmss virtual machines. This is because the virtual machines ID format in LIST is not consistent with GET results. We use GET here because azure cloud provider also uses GET API.",closed,True,2018-02-24 06:18:04,2018-03-29 03:14:00
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/682,https://api.github.com/repos/kubernetes/autoscaler/issues/682,Make deployment parameters robust for various acs-engine versions,"This PR makes deployment parameters robust for various acs-engine versions by

-  reading deployment parameters from acs-engine generated `azuredeploy.parameters.json` file
- simplifier deployment secrets

/kind bug

Fixes #644 and #642",closed,True,2018-02-24 07:59:19,2018-03-29 03:13:55
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/683,https://api.github.com/repos/kubernetes/autoscaler/issues/683,VPA Admission controller e2e tests for boundary cases.,,closed,True,2018-02-26 10:16:33,2018-02-27 14:03:27
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/684,https://api.github.com/repos/kubernetes/autoscaler/issues/684,Generated code for VPA Snapshot CRD object.,,closed,True,2018-02-26 14:42:38,2018-02-28 22:27:46
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/685,https://api.github.com/repos/kubernetes/autoscaler/issues/685,Introduce recommendation confidence.,"The confidence grows with the amount of aggregated utilization history.
The recommender multiplies MaxRecommendation by the factor of (1+1/confidence) and divides MinRecommendation by the same factor. This means that the [Min..Max] range is very wide when there is small amount of available history (either the VPA recommender has just started and didn't have access to the historical data or all pods matched by the VPA are new). The [Min..Max] range gets more narrow as the history accumulates.
As the effect, the Updater will be less likely to evict a pod when the confidence of the recommendation is low.",closed,True,2018-02-27 08:44:33,2018-03-02 10:56:23
autoscaler,KarolKraskiewicz,https://github.com/kubernetes/autoscaler/pull/686,https://api.github.com/repos/kubernetes/autoscaler/issues/686,pod life cycle phase added to the model,"just an extra field added, without any extra logic based on it.

cc for review: @kgrygiel @mwielgus 
",closed,True,2018-02-27 19:33:24,2018-03-07 12:40:01
autoscaler,resouer,https://github.com/kubernetes/autoscaler/issues/687,https://api.github.com/repos/kubernetes/autoscaler/issues/687,Scaled up twice due to pods become RUNNING need some time.,"Here's a issue when I using with cluster-autoscaler.

```bash
I0227 19:21:19.864441   22817 static_autoscaler.go:258] Filtering out schedulables
I0227 19:21:19.864711   22817 static_autoscaler.go:268] No schedulable pods
I0227 19:21:19.864748   22817 scale_up.go:56] Pod default/nginx-deployment-54dc474b5-6tvzx is unschedulable
I0227 19:21:19.864766   22817 scale_up.go:56] Pod default/nginx-deployment-54dc474b5-llwv9 is unschedulable
I0227 19:21:20.043259   22817 scale_up.go:88] Upcoming 0 nodes
I0227 19:21:20.123901   22817 scale_up.go:195] Best option to resize: dlws-worker-asg
I0227 19:21:20.123936   22817 scale_up.go:199] Estimated 1 nodes needed in dlws-worker-asg
I0227 19:21:20.205062   22817 scale_up.go:288] Final scale-up plan: [{dlws-worker-asg 2->3 (max: 10)}]
I0227 19:21:20.205132   22817 scale_up.go:340] Scale-up: setting group dlws-worker-asg size to 3
...
I0227 19:32:37.019279   22817 metrics.go:248] Function scaleUp took 11m17.154526618s to complete
I0227 19:32:37.174294   22817 metrics.go:248] Function main took 11m17.464380135s to complete
I0227 19:32:47.174453   22817 static_autoscaler.go:108] Starting main loop
I0227 19:32:47.329947   22817 clusterstate.go:191] Scale up in group dlws-worker-asg finished successfully in 10.309260066s
I0227 19:32:47.330025   22817 utils.go:435] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0227 19:32:47.330048   22817 static_autoscaler.go:258] Filtering out schedulables
I0227 19:32:47.330332   22817 static_autoscaler.go:268] No schedulable pods
I0227 19:32:47.330363   22817 scale_up.go:56] Pod default/nginx-deployment-54dc474b5-6tvzx is unschedulable
I0227 19:32:47.330381   22817 scale_up.go:56] Pod default/nginx-deployment-54dc474b5-llwv9 is unschedulable
I0227 19:32:47.496404   22817 scale_up.go:88] Upcoming 0 nodes
I0227 19:32:47.577254   22817 scale_up.go:195] Best option to resize: dlws-worker-asg
I0227 19:32:47.577279   22817 scale_up.go:199] Estimated 1 nodes needed in dlws-worker-asg
I0227 19:32:47.657121   22817 scale_up.go:288] Final scale-up plan: [{dlws-worker-asg 3->4 (max: 10)}]
I0227 19:32:47.657157   22817 scale_up.go:340] Scale-up: setting group dlws-worker-asg size to 4
...
```
**My understanding is:**
It seems that cluster-autoscaler will report scale up success when the node is ready. But in this issue, it actually took some time for my Pending pods become Running, and this triggered another scale up.
If it's the case, we should give a ""cool down"" time for recently scaled up cluster, or use pod status change as a flag. Feel free to correct me if I'm wrong.

**Test env**
```
Kubernetes v1.9.0

./cluster-autoscaler --v=4 --stderrthreshold=error --logtostderr=true --cloud-provider=mytools --skip-nodes-with-local-storage=false --nodes=1:10:dlws-worker-asg --leader-elect=false --scale-down-enabled=false --kubeconfig=./deploy/kubeconfig.yaml
```

btw, I am using my own ""Cloud Provider"" which is for bare metal cluster (autoprovisioning == false), but this should be unrelated.

Any ideas?
",closed,False,2018-02-28 00:58:17,2018-03-01 22:06:19
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/688,https://api.github.com/repos/kubernetes/autoscaler/issues/688,Better GPU handling in price expander,"Use constant, very high unfitness for nodes with GPU to make them very unattractive for any pod that doesn't require GPU and to avoid situation where CA values CPU usage efficiency over GPU price optimization.

Also additionally delay scale-up if there are very new pods requesting GPU. We lose a bit of latency for pods requesting GPU, but we can pack them more densely that way.",closed,True,2018-03-02 15:19:51,2018-03-02 15:43:01
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/689,https://api.github.com/repos/kubernetes/autoscaler/issues/689,VPA - serialization of histograms to snapshots.,,closed,True,2018-03-05 09:30:02,2018-03-05 22:45:24
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/690,https://api.github.com/repos/kubernetes/autoscaler/issues/690,Extend the Updater policy with new rules,"1. Update priority is calculated based on the ratio between the total request and total recommended target for all containers, as opposed to sum of ratio for each container (previous policy favored many small containers).
2. Pods with a container that needs to grow get higher priority.
3. Pods with any request outside the [MinRecommended...MaxRecommended] range can be updated regardless of the relative diff between request and target.
4. Pods with all requests within the [MinRecommended...MaxRecommended] range can be updated if they live for at least 12h and the relative diff exceeds the threshold.",closed,True,2018-03-05 09:52:41,2018-03-09 11:04:28
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/691,https://api.github.com/repos/kubernetes/autoscaler/issues/691,Verify target in admission-controller unit tests.,,closed,True,2018-03-05 10:23:46,2018-03-05 11:04:14
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/692,https://api.github.com/repos/kubernetes/autoscaler/issues/692,Increase timeout to reduce flakiness of autoscaling-vpa-full,,closed,True,2018-03-05 10:44:48,2018-03-05 11:04:33
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/693,https://api.github.com/repos/kubernetes/autoscaler/issues/693,Cherrypick gpu expander fix on CA 1.1 branch,Cherry-pick https://github.com/kubernetes/autoscaler/pull/688 on CA 1.1 branch. Additionally includes https://github.com/kubernetes/autoscaler/pull/508 as the additional wait in #688 depends on it.,closed,True,2018-03-05 11:36:30,2018-03-05 16:01:52
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/694,https://api.github.com/repos/kubernetes/autoscaler/issues/694,VPA AC e2e tests - change min/max recommended to resource policy,…source policy,closed,True,2018-03-05 12:18:06,2018-03-05 22:44:40
autoscaler,kawych,https://github.com/kubernetes/autoscaler/pull/695,https://api.github.com/repos/kubernetes/autoscaler/issues/695,Document Addon Resizer configuration,Preview available at https://github.com/kawych/autoscaler/tree/addon-resizer-release-1.8/addon-resizer#addon-resizer-configuration,closed,True,2018-03-05 15:21:13,2018-03-06 10:21:35
autoscaler,kawych,https://github.com/kubernetes/autoscaler/pull/696,https://api.github.com/repos/kubernetes/autoscaler/issues/696,Document Addon Resizer configuration,Preview available at https://github.com/kawych/autoscaler/tree/master/addon-resizer#addon-resizer-configuration,closed,True,2018-03-06 10:16:45,2018-03-06 13:51:13
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/697,https://api.github.com/repos/kubernetes/autoscaler/issues/697,Bump CA version to 1.1.2,,closed,True,2018-03-06 11:21:35,2018-03-06 11:23:18
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/698,https://api.github.com/repos/kubernetes/autoscaler/issues/698,VPA AggregateContainerState serialization.,,closed,True,2018-03-06 11:22:20,2018-03-06 13:48:24
autoscaler,nlamirault,https://github.com/kubernetes/autoscaler/issues/699,https://api.github.com/repos/kubernetes/autoscaler/issues/699,Which version for ARM ?,"According to the GCR website, addon-resizer-arm tag is [2.1](https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/addon-resizer-arm?gcrImageListsize=50)

On the README, it is written : 
```Currently recommended version is 1.8, on addon-resizer-release-1.8 branch.```

Where does come from the v2.1 release ? We can't use the 1.8 release on ARM ?

Thanks.",closed,False,2018-03-07 09:48:29,2018-08-11 13:31:22
autoscaler,fulder,https://github.com/kubernetes/autoscaler/issues/700,https://api.github.com/repos/kubernetes/autoscaler/issues/700,Old GPU resource name for AWS,"It looks like the GPU resource name is still the old alpha one (`v1.resourceNvidiaGPU`=`alpha.kubernetes.io/nvidia-gpu`) instead of the new: `nvidia.com/gpu` for AWS. 

The master branch got this updated here: 
https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/aws_manager.go#L408

But it is not present in any of the autoscaler releases. This makes it impossible to scale up an instancegroup from 0 nodes. Changing the pod resource annotation to `alpha.kubernetes.io/nvidia-gpu` scales it up correctly but then the node itself will get the new one and the pod won't be able to be put on that node anyways.

Could a release for v. 1.0.X be made including a name update fix for this?",closed,False,2018-03-08 11:55:39,2018-08-05 14:12:28
autoscaler,pradeeppandeyy,https://github.com/kubernetes/autoscaler/issues/701,https://api.github.com/repos/kubernetes/autoscaler/issues/701,how to compile cluster-autoscaler binary,how to compile cluster-autoscaler binary,closed,False,2018-03-08 20:15:31,2018-03-16 07:45:36
autoscaler,normanjoyner,https://github.com/kubernetes/autoscaler/pull/702,https://api.github.com/repos/kubernetes/autoscaler/issues/702,fix misspelling of Kubernetes,Fixes typo of Kubernetes in comment,closed,True,2018-03-10 15:35:59,2018-03-12 09:36:17
autoscaler,jayanthpurushothaman,https://github.com/kubernetes/autoscaler/issues/703,https://api.github.com/repos/kubernetes/autoscaler/issues/703,Scale up operation logs DeploymentFailed error for Azure VMAS-based Kubernetes 1.9.3 Cluster,"When attempting to scale up a K8S v1.9.3 Cluster (1 Master, 2 Nodes) with Azure Managed Disks _(K8S Cluster was deployed and upgraded using ACS Engine v0.13.0)_, the **cluster autoscaler** _(Docker image built from the **master** branch of the current code base as per [https://github.com/kubernetes/autoscaler/issues/533](url) and [https://github.com/kubernetes/autoscaler/issues/644](url))_ logs the following errors, even though the Scale Up operation seems to be successful and the Pods are scheduled.


I0310 15:01:54.964776       1 scale_up.go:195] Best option to resize: agentpool1
I0310 15:01:54.964796       1 scale_up.go:199] Estimated 1 nodes needed in agentpool1
I0310 15:01:54.964807       1 scale_up.go:288] Final scale-up plan: [{agentpool1 2->3 (max: 3)}]
I0310 15:01:54.964819       1 scale_up.go:340] Scale-up: setting group agentpool1 size to 3
I0310 15:01:54.996988       1 azure_agent_pool.go:209] Waiting for deploymentsClient.CreateOrUpdate(_ResourceGroupName_, cluster-autoscaler-761424867, {%!s(*resources.DeploymentProperties=&{0xc420cebd08 <nil> 0xc420cebd10 <nil> Incremental <nil>})})
W0310 15:09:56.391166       1 clusterstate.go:248] Disabling scale-up for node group agentpool1 until 2018-03-10 15:39:56.391154808 +0000 UTC m=+8749.309110125
**E0310 15:09:56.391219       1 static_autoscaler.go:302] Failed to scale up: failed to increase node group size: resources.DeploymentsClient#CreateOrUpdate: Failure sending request: StatusCode=200 -- Original Error: Long running operation terminated with status 'Failed': Code=""DeploymentFailed"" Message=""At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/arm-debug for usage details.""**
I0310 15:10:06.456531       1 azure_manager.go:242] Refreshed ASG list, next refresh after 2018-03-10 15:11:06.456517902 +0000 UTC m=+7019.374473118


Looking at the Deployments in Azure, the following message was found:

	""error"": {
		""code"": ""DeploymentFailed"",
		""details"": [{
			""code"": ""Conflict"",
			""message"": ""{\r\n  \""error\"": {\r\n    \""code\"": \""PropertyChangeNotAllowed\"",\r\n    \""target\"": \""dataDisk.createOption\"",\r\n    \""message\"": \""Changing property 'dataDisk.createOption' is not allowed.\""\r\n  }\r\n}""
		}],
		""message"": ""At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/arm-debug for usage details.""
	}


Any feedback on this would be definitely helpful. Thanks.
",closed,False,2018-03-10 16:54:57,2018-05-16 09:22:25
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/704,https://api.github.com/repos/kubernetes/autoscaler/issues/704,Add regional flag,Use flag to identify regional clusters iinstead of relying on multi-zone property.,closed,True,2018-03-12 11:31:18,2018-04-23 12:27:54
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/705,https://api.github.com/repos/kubernetes/autoscaler/issues/705,"Typo fix ""record""->""records""","""record"" should be replaced with ""records"" in line 48 and line 55.",closed,True,2018-03-12 13:07:06,2018-03-14 15:04:51
autoscaler,jamespinkerton,https://github.com/kubernetes/autoscaler/issues/706,https://api.github.com/repos/kubernetes/autoscaler/issues/706,Decrease Size of Cluster Question,"Hi. I have a quick question. If I launch a pod to kubernetes and it increases the size of the cluster and then completes, how long does it take for the cluster to shrink again? Is this configurable?

Thanks!
James",closed,False,2018-03-13 06:12:00,2018-03-14 08:16:06
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/707,https://api.github.com/repos/kubernetes/autoscaler/issues/707,Use the builder pattern in tests to make them more readable and extensible.,,closed,True,2018-03-13 10:39:19,2018-03-14 11:19:10
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/708,https://api.github.com/repos/kubernetes/autoscaler/issues/708,"Typo fix ""typ""->""type""","line 31 and line 82: ""the typ of AutoscalerError""
here shoule be type",closed,True,2018-03-13 11:50:34,2018-03-13 13:54:52
autoscaler,WeiCheng1992,https://github.com/kubernetes/autoscaler/issues/709,https://api.github.com/repos/kubernetes/autoscaler/issues/709,Autoscaler is unable to scale up when a node is isolated and ASG is in its minimum size,"Currently, I am running into this issue:

I set the autoscaler with min:max = 30:50. When the group size is 30.
At this moment, if a node is isolated, k8s master cannot talk to this node and even cannot allocate pod into this node. Then the cpu and memory will be 0%. 
Then autoscaler try to terminate this machine, but the group  size reaches its minimum. Therefore the scale-down will fail.
Finally it will do not scale-up in this cycle, even if there are many unscheduled pods, because autoscaler try to do scale-down in this cycle.
",closed,False,2018-03-13 18:46:18,2018-11-02 14:43:40
autoscaler,mtaufen,https://github.com/kubernetes/autoscaler/pull/710,https://api.github.com/repos/kubernetes/autoscaler/issues/710,Extend cluster autoscaler to check AUTOSCALER_ENV_VARS in kube-env,"This is the second part of the fix in https://github.com/kubernetes/kubernetes/pull/61119

This provides a temporary way for the cluster autoscaler to get at
values that were removed from kube-env in https://github.com/kubernetes/kubernetes/pull/60020.
Ideally this information will eventually be available via e.g. the Cluster API,
because kube-env is an internal interface that carries no stability
guarantees.

/cc @roberthbailey @bskiba @mwielgus ",closed,True,2018-03-13 19:41:24,2018-03-19 08:46:58
autoscaler,jcrowthe,https://github.com/kubernetes/autoscaler/issues/711,https://api.github.com/repos/kubernetes/autoscaler/issues/711,Document Azure MSI support,"Azure's [Managed Service Identity](https://docs.microsoft.com/en-us/azure/active-directory/managed-service-identity/overview) is supported as an alternative to utilizing a service principal for the authorization to the backend Azure cloud provider. This is a request to support MSI in the Azure autoscaler in addition to the current service principal.

If accepted, this task would be added to this list: https://github.com/kubernetes/autoscaler/issues/449",closed,False,2018-03-14 05:32:26,2018-04-19 04:28:06
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/712,https://api.github.com/repos/kubernetes/autoscaler/issues/712,Initial implementation of reading and writing VPA histogram checkpoints.,,closed,True,2018-03-14 10:01:48,2018-03-21 10:47:39
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/713,https://api.github.com/repos/kubernetes/autoscaler/issues/713,Fix command for e2e tests after changes in test infra,"The semantics of -v flag changed - it is currently used to controll log
output for glog. The -v flag for kubetest has been replaced by
--verbose-commands.

See https://github.com/kubernetes/test-infra/pull/7073",closed,True,2018-03-14 10:36:13,2018-03-14 15:04:27
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/714,https://api.github.com/repos/kubernetes/autoscaler/issues/714,VPA e2e tests fixes. Increased timeout and min/max recommendation set.,,closed,True,2018-03-14 14:52:46,2018-03-14 15:05:27
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/715,https://api.github.com/repos/kubernetes/autoscaler/issues/715,Limit logging of pods found to be schedulable,"Cluster Autoscaler logs every pod it finds to be schedulable once per loop, which causes excessive logging if the number of such pods is large. This changes it to log up to 1000 pods per loop if running with `--v=10`, and up to 5 with lower verbosity levels.",closed,True,2018-03-14 16:54:41,2018-03-22 12:13:29
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/716,https://api.github.com/repos/kubernetes/autoscaler/issues/716,Typo fix in kubernetes.go,"line 31: a spell err  ""structur"" should be ""structure""",closed,True,2018-03-15 11:54:51,2018-03-15 12:36:18
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/717,https://api.github.com/repos/kubernetes/autoscaler/issues/717,Run spellchecker,Fix typos in batch.,closed,True,2018-03-15 15:19:28,2018-03-15 15:40:26
autoscaler,mtaufen,https://github.com/kubernetes/autoscaler/issues/718,https://api.github.com/repos/kubernetes/autoscaler/issues/718,Stop using kube-env for node information,"We should find a more stable source of information than kube-env for information the autoscaler needs.
Ideally this will be provided by the upcoming Machines API.
See:
- [Add AUTOSCALER_ENV_VARS to kube-env to hotfix cluster autoscaler](https://github.com/kubernetes/kubernetes/pull/61119)
- [Extend cluster autoscaler to check AUTOSCALER_ENV_VARS in kube-env](https://github.com/kubernetes/autoscaler/pull/710)

/cc @bskiba @roberthbailey @mwielgus",open,False,2018-03-15 17:07:16,2018-09-13 19:53:51
autoscaler,gmaxwell94,https://github.com/kubernetes/autoscaler/issues/719,https://api.github.com/repos/kubernetes/autoscaler/issues/719,Scale down in AWS does not remove node from Cluster,"using v1.1.1  scales up correctly,  
scales instances down as expected but as part of the tear down of a instance should be deleted from the k8 cluster

in 
static_autoscaler does not seems to  get any nodes from 
unregisteredNodes := a.ClusterStateRegistry.GetUnregisteredNodes()
removeOldUnregisteredNodes   
 
which means the  call in clusterstage.go  in func UpdateNodes

notRegistered, err := getNotRegisteredNodes(nodes, csr.cloudProvider, currentTime)
is returning empty for nodes in ""NotReady"" state that is > a few minutes

NAME                            STATUS     AGE
ip-172-16-88-136.ec2.internal   NotReady   57m
ip-172-16-88-146.ec2.internal   NotReady   1h
ip-172-16-88-148.ec2.internal   Ready      1h
ip-172-16-88-153.ec2.internal   NotReady   1h
ip-172-16-88-154.ec2.internal   NotReady   1h
ip-172-16-88-159.ec2.internal   NotReady   15h
ip-172-16-88-167.ec2.internal   NotReady   15h
ip-172-16-88-176.ec2.internal   NotReady   1h
ip-172-16-88-179.ec2.internal   Ready      15h
ip-172-16-88-200.ec2.internal   NotReady   1h
ip-172-16-88-204.ec2.internal   NotReady   1h
ip-172-16-88-209.ec2.internal   NotReady   1h
ip-172-16-88-211.ec2.internal   NotReady   1h
ip-172-16-88-226.ec2.internal   NotReady   1h
ip-172-16-88-244.ec2.internal   NotReady   15h
ip-172-16-88-253.ec2.internal   NotReady   1h

the api call gives the following
curl http://10.2.0.1:8080/api/v1/nodes
info for a node
note the taints
--------------------------------
        ""taints"": [
          {
            ""key"": ""ToBeDeletedByClusterAutoscaler"",
            ""value"": ""1521144316"",
---------------------------------
So in all aspects this nodes should have been deleted. 

  ""items"": [
    {
      ""metadata"": {
        ""name"": ""ip-172-16-88-136.ec2.internal"",
        ""selfLink"": ""/api/v1/nodes/ip-172-16-88-136.ec2.internal"",
        ""uid"": ""d8eebf45-288a-11e8-a356-0e75d3579f66"",
        ""resourceVersion"": ""70753"",
        ""creationTimestamp"": ""2018-03-15T19:55:44Z"",
        ""labels"": {
          ""node-role.kubernetes.io/vanilla"": ""true"",
          ""beta.kubernetes.io/arch"": ""amd64"",
          ""beta.kubernetes.io/instance-type"": ""c5.xlarge"",
          ""beta.kubernetes.io/os"": ""linux"",
          ""failure-domain.beta.kubernetes.io/region"": ""us-east-1"",
          ""failure-domain.beta.kubernetes.io/zone"": ""us-east-1a"",
          ""kubernetes.io/hostname"": ""10.2.136.1""
        },
        ""annotations"": {
          ""node.alpha.kubernetes.io/ttl"": ""0"",
          ""volumes.kubernetes.io/controller-managed-attach-detach"": ""true""
        }
      },
      ""spec"": {
        ""podCIDR"": ""10.2.15.0/24"",
        ""externalID"": ""i-08b22eee77e88d42f"",
        ""providerID"": ""aws:///us-east-1a/i-08b22eee77e88d42f"",
        ""taints"": [
          {
            ""key"": ""ToBeDeletedByClusterAutoscaler"",
            ""value"": ""1521144316"",
            ""effect"": ""NoSchedule""
          }
        ]
      },
      ""status"": {
        ""capacity"": {
          ""cpu"": ""4"",
          ""memory"": ""7817136Ki"",
          ""pods"": ""110""
        },
        ""allocatable"": {
          ""memory"": ""7714736Ki"",
          ""pods"": ""110"",
          ""cpu"": ""4""
        },
        ""conditions"": [
          {
            ""type"": ""OutOfDisk"",
            ""status"": ""False"",
            ""lastHeartbeatTime"": ""2018-03-15T20:05:25Z"",
            ""lastTransitionTime"": ""2018-03-15T19:55:44Z"",
            ""reason"": ""KubeletHasSufficientDisk"",
            ""message"": ""kubelet has sufficient disk space available""
          },
          {
            ""type"": ""MemoryPressure"",
            ""status"": ""Unknown"",
            ""lastHeartbeatTime"": ""2018-03-15T20:05:25Z"",
            ""lastTransitionTime"": ""2018-03-15T20:06:06Z"",
            ""reason"": ""NodeStatusUnknown"",
            ""message"": ""Kubelet stopped posting node status.""
          },
          {
            ""type"": ""DiskPressure"",
            ""status"": ""Unknown"",
            ""lastHeartbeatTime"": ""2018-03-15T20:05:25Z"",
            ""lastTransitionTime"": ""2018-03-15T20:06:06Z"",
            ""reason"": ""NodeStatusUnknown"",
            ""message"": ""Kubelet stopped posting node status.""
          },
          {
            ""type"": ""Ready"",
            ""status"": ""Unknown"",
            ""lastHeartbeatTime"": ""2018-03-15T20:05:25Z"",
            ""lastTransitionTime"": ""2018-03-15T20:06:06Z"",
            ""reason"": ""NodeStatusUnknown"",
            ""message"": ""Kubelet stopped posting node status.""
          }
        ],
        ""daemonEndpoints"": {
          ""kubeletEndpoint"": {
            ""Port"": 10250
          }
        },
        ""nodeInfo"": {
          ""machineID"": ""6c0dbeffde9846e48691730623978522"",
          ""systemUUID"": ""EC2DDA3B-88DA-AC74-676C-460A410A0384"",
          ""bootID"": ""12e8a887-d13b-482c-9199-9fd68396e6cd"",
          ""kernelVersion"": ""4.9.76-38.79.amzn2.x86_64"",
          ""osImage"": ""Debian GNU/Linux 9 (stretch)"",
          ""containerRuntimeVersion"": ""docker://17.12.0-ce"",
          ""kubeletVersion"": ""v1.9.1"",
          ""kubeProxyVersion"": ""v1.9.1"",
          ""operatingSystem"": ""linux"",
          ""architecture"": ""amd64""
        },
        ""images"": [
          {
            ""names"": [
              ""gcr.io/google-containers/hyperkube@sha256:69cd6e29c8bd7658e6597cba6aa0110e91c8ef9a993844717e7819cf91d77563"",
              ""gcr.io/google-containers/hyperkube:v1.9.1""
            ],
            ""sizeBytes"": 617915509
          }
        ]
      }
    },

The current date on the host is Thu Mar 15 21:04:16 UTC 2018  so all heartbeats are older than a few minutes.

Thanks
Greg Maxwell


",closed,False,2018-03-15 21:10:14,2018-06-21 11:30:11
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/720,https://api.github.com/repos/kubernetes/autoscaler/issues/720,"Update metadata.go, a clerical error","line 48: a clerical error ""This is variable name is not defined by any spec""
It should be ""This variable name is not defined by any spec""",closed,True,2018-03-16 05:35:28,2018-03-16 07:46:43
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/721,https://api.github.com/repos/kubernetes/autoscaler/issues/721,Add safety margin and tune confidence in the recommender.,,closed,True,2018-03-16 12:16:55,2018-03-16 22:55:09
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/722,https://api.github.com/repos/kubernetes/autoscaler/issues/722,Do not delete terminated pods from the model.,"We can enable deleting terminated pods once we start collecting
aggregated history of usage in another object, otherwise history
of usage is lost once the pod is removed.",closed,True,2018-03-16 14:29:25,2018-06-22 14:09:11
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/723,https://api.github.com/repos/kubernetes/autoscaler/issues/723,VPA initial detection of OOMs caused by limit violation.,,closed,True,2018-03-16 17:38:17,2018-03-19 11:49:16
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/724,https://api.github.com/repos/kubernetes/autoscaler/issues/724,VPA recommender definition moved to the library,,closed,True,2018-03-19 08:45:01,2018-03-19 11:51:29
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/725,https://api.github.com/repos/kubernetes/autoscaler/issues/725,CA 0.6.4,Update Cluster Autoscaler version to 0.6.4.,closed,True,2018-03-19 12:05:17,2018-03-19 12:43:10
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/726,https://api.github.com/repos/kubernetes/autoscaler/issues/726,CA 1.0.5,Update version to 1.0.5,closed,True,2018-03-19 12:26:11,2018-03-19 12:43:56
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/727,https://api.github.com/repos/kubernetes/autoscaler/issues/727,VPA artificial samples generation for OOMs.,,closed,True,2018-03-19 13:24:44,2018-03-20 15:17:42
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/728,https://api.github.com/repos/kubernetes/autoscaler/issues/728,Cluster Autoscaler 1.2.0-beta1,,closed,True,2018-03-19 14:49:25,2018-03-19 15:58:47
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/729,https://api.github.com/repos/kubernetes/autoscaler/issues/729,Godeps for CA 1.2,,closed,True,2018-03-20 19:15:23,2018-03-21 10:07:11
autoscaler,fqsghostcloud,https://github.com/kubernetes/autoscaler/pull/730,https://api.github.com/repos/kubernetes/autoscaler/issues/730,fix some typo,Here are some spelling mistakes that need to be corrected,closed,True,2018-03-21 02:10:24,2018-03-21 02:56:08
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/731,https://api.github.com/repos/kubernetes/autoscaler/issues/731,VPA OOM support in admission controller,"+ admission-controller sets limits
+ hooking up oom info in cluster feeder",closed,True,2018-03-21 15:30:12,2018-03-22 11:39:53
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/732,https://api.github.com/repos/kubernetes/autoscaler/issues/732,VPA OOM handling e2e test,,closed,True,2018-03-21 15:32:00,2018-03-22 11:40:24
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/733,https://api.github.com/repos/kubernetes/autoscaler/issues/733,VPA restore serialization of confidence data in checkpoints,,closed,True,2018-03-21 15:48:18,2018-03-22 11:41:02
autoscaler,dklyle,https://github.com/kubernetes/autoscaler/issues/734,https://api.github.com/repos/kubernetes/autoscaler/issues/734,Cluster-autoscaler support for OpenStack cloud provider,"There are numerous potential ways to support cluster autoscaling in OpenStack. Services like Heat and Senlin are not guaranteed to be support in an OpenStack deployment. So base support using only core OpenStack services should be targeted. The eventual path could be to optionally support using other services, but that would be outside the scope of this issue, other than driving the implementation to be abstract around ties to underlying services.

I found #230, but it has been closed as stale. ",open,False,2018-03-21 22:43:32,2019-02-03 03:50:42
autoscaler,thomasjungblut,https://github.com/kubernetes/autoscaler/issues/735,https://api.github.com/repos/kubernetes/autoscaler/issues/735,Node over-provisioning,"Hey guys,

we're a heavy user of cluster-autoscaler in AWS for quite sometime, but we now want to improve the scale-up times by over-provisioning our autoscaling groups by some margin (say +n or 10%). 

Is there an easy way to enable this with the current set of features? 
If not, I'm happy to contribute it if you can give me a pointer where it would fit best.

Cheers,
Thomas",closed,False,2018-03-22 10:52:55,2019-01-16 19:50:34
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/736,https://api.github.com/repos/kubernetes/autoscaler/issues/736,CA 1.2.0-rc1,,closed,True,2018-03-22 12:15:43,2018-03-22 12:26:14
autoscaler,jsenon,https://github.com/kubernetes/autoscaler/pull/737,https://api.github.com/repos/kubernetes/autoscaler/issues/737,update readme,Change example yaml file used for auto-discovery setup,closed,True,2018-03-22 15:03:48,2018-03-22 16:09:22
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/738,https://api.github.com/repos/kubernetes/autoscaler/issues/738,Disable VolumeScheduling predicate,Disable VolumeScheduling predicate (which breaks scale-up) until there's a workaround.,closed,True,2018-03-22 15:36:52,2018-03-22 16:00:42
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/739,https://api.github.com/repos/kubernetes/autoscaler/issues/739,CA 1.2.0-rc2,,closed,True,2018-03-22 16:03:02,2018-03-22 16:04:35
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/740,https://api.github.com/repos/kubernetes/autoscaler/issues/740,Fix VolumeBindingChecker predicate,"VolumeBindingChecker predicate verifies that the node passed to it actually exists. This breaks scale-up, as we need to run it against node object simulating the node we intend to request. Proposed solution is to remove the underlying call to informer and use the node object passed to the predicate checker.

#738 temporarily disabled it. It should be re-enabled once this is fixed.",closed,False,2018-03-22 16:10:41,2019-02-16 12:43:17
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/741,https://api.github.com/repos/kubernetes/autoscaler/issues/741,Fix for multi line KUBELET_ARGS,,closed,True,2018-03-22 19:45:43,2018-03-22 19:59:51
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/742,https://api.github.com/repos/kubernetes/autoscaler/issues/742,Add overprovisioning to CA FAQ.,,closed,True,2018-03-23 07:05:30,2018-03-23 13:02:25
autoscaler,jsenon,https://github.com/kubernetes/autoscaler/pull/743,https://api.github.com/repos/kubernetes/autoscaler/issues/743,Update Readme,Add value for autoscalling group tag,closed,True,2018-03-23 09:21:40,2018-03-23 13:00:36
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/744,https://api.github.com/repos/kubernetes/autoscaler/issues/744,Cluster Autoscaler 1.2.0,,closed,True,2018-03-23 11:05:47,2018-03-23 12:13:34
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/issues/745,https://api.github.com/repos/kubernetes/autoscaler/issues/745,Properly fix parsing multi line values in kube-env,Multi line kube env values mess up our parsing. Temporary fix was introduced in https://github.com/kubernetes/autoscaler/pull/741. Let's do a proper one.,closed,False,2018-03-23 11:49:03,2018-03-30 10:17:56
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/746,https://api.github.com/repos/kubernetes/autoscaler/issues/746,VPA e2e test for loading checkpoints,,closed,True,2018-03-25 09:57:55,2018-03-28 09:18:59
autoscaler,johnharris85,https://github.com/kubernetes/autoscaler/issues/747,https://api.github.com/repos/kubernetes/autoscaler/issues/747,Node Group at min size but taint remains?,"Not sure if this is by design behavior or not, but we have a couple of nodes with a taint indicated they should have been scaled down by CA in January, however logs of CA show that doing so would have taken it below the min size, so they clearly weren't. However the taints remain meaning we actually have less nodes available for scheduling than min size.

k8s: 1.8.4
CA: 0.6.0",closed,False,2018-03-26 00:15:47,2018-03-29 08:50:42
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/748,https://api.github.com/repos/kubernetes/autoscaler/issues/748,Add Cluster Autoscaler 1.2 as recommended version with Kubernetes 1.10,Add Cluster Autoscaler 1.2 as recommended version with Kubernetes 1.10,closed,True,2018-03-26 11:12:32,2018-03-27 09:05:45
autoscaler,resouer,https://github.com/kubernetes/autoscaler/pull/749,https://api.github.com/repos/kubernetes/autoscaler/issues/749,Rely on cluster.yaml to restore cluster state,"Rely on cluster.yaml to restore cluster state

Do not modify config.yaml",closed,True,2018-03-26 23:16:05,2018-03-26 23:16:30
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/750,https://api.github.com/repos/kubernetes/autoscaler/issues/750,Improve reaction time for CPU starved workloads,"Tune the recommender to converge faster for workloads that need a lot of CPU.
(1) Use CPU request as the weight in the CPU histogram, so that low usage caused by CPU starvation is forgotten faster.
(2) Increase the CPU safety margin.",closed,True,2018-03-27 09:17:27,2018-03-28 09:20:19
autoscaler,sunlintong,https://github.com/kubernetes/autoscaler/pull/751,https://api.github.com/repos/kubernetes/autoscaler/issues/751,add dot,,closed,True,2018-03-28 02:42:28,2018-03-28 07:42:57
autoscaler,sunlintong,https://github.com/kubernetes/autoscaler/pull/752,https://api.github.com/repos/kubernetes/autoscaler/issues/752,Update README,"small change,maybe i'm an obsession",closed,True,2018-03-28 03:02:23,2018-03-28 09:14:11
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/issues/753,https://api.github.com/repos/kubernetes/autoscaler/issues/753,Add Azure Container Service (AKS) support,"Azure VMSS and VMAS support has been added to cluster-autoscaler. We should also add AKS support.

",closed,False,2018-03-28 04:20:56,2018-05-15 09:09:20
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/754,https://api.github.com/repos/kubernetes/autoscaler/issues/754,Fix method name,Fix method name.,closed,True,2018-03-28 11:24:41,2018-03-28 11:40:06
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/755,https://api.github.com/repos/kubernetes/autoscaler/issues/755,Update pricing.md,"""n1-standard-..."" should be changed to ""N1-standard-...""
In this doc, the machine type is defined as ""N1-standard-..."", but there are several ""n1-standard-..."".
It's better to use same format.",closed,True,2018-03-28 13:32:50,2018-04-01 06:48:25
autoscaler,jcrowthe,https://github.com/kubernetes/autoscaler/issues/756,https://api.github.com/repos/kubernetes/autoscaler/issues/756,Pre-provisioned nodes cause errors in cluster-autoscaler,"When using cluster-autoscaler with an Azure VM Scale Set that already has a node in it, I receive the following incessantly after a ScaleUp and ScaleDown cycle:

```
I0328 23:35:35.620211       1 static_autoscaler.go:184] 1 unregistered nodes present
I0328 23:35:35.690141       1 utils.go:330] Removing unregistered node
I0328 23:35:35.690224       1 azure_scale_set.go:273] Deleting vmss instances [%!q(*azure.azureRef=&{})]
E0328 23:35:35.690245       1 azure_scale_set.go:293] getLastSegment failed with error: identifier '/' not found in resource name """"
W0328 23:35:35.690253       1 utils.go:353] Failed to remove node : identifier '/' not found in resource name """"
E0328 23:35:35.690270       1 static_autoscaler.go:191] Failed to remove unregistered nodes: identifier '/' not found in resource name """"
```

In looking at successful ScaleDown operations, the third line from above should appear more like `I0328 22:50:57.080662       1 azure_scale_set.go:273] Deleting vmss instances [%!q(*azure.azureRef=&{azure:///subscriptions/<sub>/resourceGroups/aaakube56/providers/Microsoft.Compute/virtualMachineScaleSets/vmss-agent-worker-aaakube56/virtualMachines/9})]`

This ID does appear properly on all nodes under their ProviderID property, such as:
```
$ k describe node vmss-agent-worker-aaakube56000000
...
ProviderID:                  azure:///subscriptions/<sub>/resourceGroups/aaakube56/providers/Microsoft.Compute/virtualMachineScaleSets/vmss-agent-worker-aaakube56/virtualMachines/0
```
",closed,False,2018-03-28 23:49:19,2018-04-07 13:35:16
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/757,https://api.github.com/repos/kubernetes/autoscaler/issues/757,Ensure azure reference non-empty,"Fix #756

/assign @mwielgus",closed,True,2018-03-29 03:24:36,2018-03-29 08:08:12
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/758,https://api.github.com/repos/kubernetes/autoscaler/issues/758,Ensure azure reference non-empty,"Automatically cherry-pick #757: Ensure azure reference non-empty

Fix #756",closed,True,2018-03-29 03:25:07,2018-03-29 08:07:51
autoscaler,Drupi,https://github.com/kubernetes/autoscaler/issues/759,https://api.github.com/repos/kubernetes/autoscaler/issues/759,[AZURE] ,"Hello, I properly setup the autoscaler on Azure but .. I have this communication in POD logs. Every value is correct, base64 encrypted. Anyone had this problem ?


Failed to update node registry: azure.BearerAuthorizer#WithAuthorization: Failed to refresh the Token for request to https://management.azure.com/subscriptions/<Sub-ID>/resourceGroups/<Resource-group>/providers/Microsoft.Compute/virtualMachineScaleSets/agentpool1?api-version=2017-03-30: StatusCode=0 -- Original Error: adal: Refresh request failed. Status Code = '400'. Response body: <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01//EN""""http://www.w3.org/TR/html4/strict.dtd"">
<HTML><HEAD><TITLE>Bad Request</TITLE>
<META HTTP-EQUIV=""Content-Type"" Content=""text/html; charset=us-ascii""></HEAD>
<BODY><h2>Bad Request - Invalid URL</h2>
<hr><p>HTTP Error 400. The request URL is invalid.</p>
</BODY></HTML>",closed,False,2018-03-29 07:51:23,2018-03-30 08:11:48
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/760,https://api.github.com/repos/kubernetes/autoscaler/issues/760,"Fix full-vpa e2e test: add missing assertions, increase timeouts",@kgrygiel @mwielgus ,closed,True,2018-03-29 09:14:04,2018-03-29 09:48:42
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/761,https://api.github.com/repos/kubernetes/autoscaler/issues/761,Fix a typo,"@bskiba: sorry, I made a typo in the previous one and it seems test code is not unit tested... ;)",closed,True,2018-03-29 14:10:52,2018-03-29 15:20:22
autoscaler,tkulczynski,https://github.com/kubernetes/autoscaler/pull/762,https://api.github.com/repos/kubernetes/autoscaler/issues/762,Clean up resource consumer in full VPA e2e test,,closed,True,2018-03-29 15:42:27,2018-03-29 21:47:18
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/763,https://api.github.com/repos/kubernetes/autoscaler/issues/763,Update types.go," line 35-36: ""ClusterAutoscalerScaleUp is a condition that explains what is the current status of a node group with regard to scale down activities"".
It should be ""...up activities""",closed,True,2018-03-29 16:13:53,2018-03-29 23:46:51
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/764,https://api.github.com/repos/kubernetes/autoscaler/issues/764,Improvements to the VPA deployment scripts and documentation,,closed,True,2018-03-29 17:05:24,2018-03-30 08:16:56
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/765,https://api.github.com/repos/kubernetes/autoscaler/issues/765,Fix parsing kube-env,"Use the yaml-parsing library to parse KUBE_ENV in gce instance templates to deal with possible multi line values.
Fixes #745 ",closed,True,2018-03-30 08:21:08,2018-03-30 10:17:56
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/766,https://api.github.com/repos/kubernetes/autoscaler/issues/766,Print version number on startup.,,closed,True,2018-03-30 09:03:36,2018-03-30 09:32:20
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/767,https://api.github.com/repos/kubernetes/autoscaler/issues/767,Fix parsing kube-env,,closed,True,2018-03-30 12:03:19,2018-04-03 08:25:10
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/768,https://api.github.com/repos/kubernetes/autoscaler/issues/768,Switch images to debian in VPA,,closed,True,2018-03-30 12:21:24,2018-03-30 12:32:51
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/769,https://api.github.com/repos/kubernetes/autoscaler/issues/769,Version update in vpa deployments,cc: @kgrygiel @tkulczynski ,closed,True,2018-03-30 14:16:03,2018-03-30 14:56:29
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/770,https://api.github.com/repos/kubernetes/autoscaler/issues/770,Mark VPA as alpha in the main Readme.md file,,closed,True,2018-03-30 15:06:20,2018-03-30 15:16:33
autoscaler,resouer,https://github.com/kubernetes/autoscaler/issues/771,https://api.github.com/repos/kubernetes/autoscaler/issues/771,Calculate GPU count for least waste,"Current least waste model may give you a GPU machine if CPU memory waste is similar.

We should calculate GPU in the score as well.",closed,False,2018-03-31 00:30:30,2018-11-18 14:00:00
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/772,https://api.github.com/repos/kubernetes/autoscaler/issues/772,More introduction in VPA readme,cc: @tkulczynski @kgrygiel ,closed,True,2018-04-03 09:00:08,2018-04-03 11:34:32
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/773,https://api.github.com/repos/kubernetes/autoscaler/issues/773,Garbage collection of VPA checkpoints,,closed,True,2018-04-03 14:00:43,2018-04-09 17:53:20
autoscaler,thockin,https://github.com/kubernetes/autoscaler/pull/774,https://api.github.com/repos/kubernetes/autoscaler/issues/774,2nd pass: k8s.gcr.io,"I will be sending out more messages before eventually disabling the legacy URL.  In the mean time, please be aware that gcr.io/google_containers is to be replaced with k8s.gcr.io in almost every case.",closed,True,2018-04-03 19:02:34,2018-04-06 14:50:15
autoscaler,EPinci,https://github.com/kubernetes/autoscaler/issues/775,https://api.github.com/repos/kubernetes/autoscaler/issues/775,Unregistered nodes present,"I'm using the autoscaler v1.2 on a Azure, VMAS, ACS-Engine deployed Kubernetes cluster.
I keep getting this kind of entries in the autoscaler logs:

```
I0404 09:37:54.025617       1 static_autoscaler.go:184] 3 unregistered nodes present
```

What is this supposed to mean and how do I fix it?

Thank you.",closed,False,2018-04-04 16:11:39,2018-05-02 08:18:50
autoscaler,EPinci,https://github.com/kubernetes/autoscaler/issues/776,https://api.github.com/repos/kubernetes/autoscaler/issues/776,Unable to scale up: no node info,"I'm using the autoscaler v1.2 on a Azure, VMAS, ACS-Engine deployed Kubernetes cluster.
Autoscaler is unable to scale up my cluster with:

```
I0405 08:17:06.844074       1 scale_up.go:59] Pod default/busybox-deployment-5b6d9ff767-vld8x is unschedulable
E0405 08:17:06.844119       1 scale_up.go:446] No node info for: nodepool1
E0405 08:17:06.844187       1 scale_up.go:122] No node info for: nodepool1
I0405 08:17:06.844259       1 scale_up.go:186] No expansion options
```

What am I missing in here?

Thank you.",closed,False,2018-04-05 09:28:52,2018-05-02 08:19:44
autoscaler,jpds,https://github.com/kubernetes/autoscaler/issues/777,https://api.github.com/repos/kubernetes/autoscaler/issues/777,Metrics for fast evalution decisions,"Occasionally, I will see something like the following in my logs:

```
I0405 15:41:41.099802       1 scale_down.go:175] Scale-down calculation: ignoring 3 nodes, that were unremovable in the last 5m0s
I0405 15:41:41.099853       1 scale_down.go:207] Node xxx.compute.internal - utilization 0.275000
I0405 15:41:41.100215       1 scale_down.go:257] Finding additional 1 candidates for scale down.
I0405 15:41:41.100292       1 cluster.go:78] Fast evaluation: xxx.compute.internal for removal
I0405 15:41:41.100614       1 cluster.go:200] Pod testing-namespace/thing-ffcd966b6-9ncfl can be moved to yyy.compute.internal
I0405 15:41:41.100684       1 cluster.go:200] Pod testing-namespace/thing2-57fb85f47b-jmqhd can be moved to ip-172-20-119-209.eu-west-1.compute.internal
I0405 15:41:41.100769       1 cluster.go:114] Fast evaluation: node xxx.compute.internal is not suitable for removal: failed to find place for production/thing3-79b679cc78-rv586
I0405 15:41:41.101028       1 scale_down.go:294] 1 nodes found unremovable in simulation, will re-check them at 2018-04-05 15:46:40.901428594 +0000 UTC
```

And also:

```
I0405 15:53:16.695936       1 cluster.go:92] Fast evaluation: node zzz.compute.internal cannot be removed: production-ci/ci-job-concurrent-24f2lz is not replicated
```

However, I cannot see any available metrics for these particular actions. Would it be possible to expose:

1) What number of pods can be moved?
2) What number of pods cannot be moved?",closed,False,2018-04-05 15:52:06,2018-11-18 12:59:00
autoscaler,jpds,https://github.com/kubernetes/autoscaler/issues/778,https://api.github.com/repos/kubernetes/autoscaler/issues/778,Node count metrics do not contain node group names,"Under metric keys such as: `cluster_autoscaler_scaled_up_nodes_total`, `cluster_autoscaler_nodes_count`, `cluster_autoscaler_unneeded_nodes_count` - I do not see anything about which group name the nodes belonged too.

I would like to know which group cluster-autoscaler decided to scale up or down at a particular moment in time for example, right now I only have the totals in my metrics dashboards. Another example of this is that `cluster_autoscaler_nodes_count` also happens to include the masters, which I know will probably always be `3`, but I can't drill down to a non-master number.",closed,False,2018-04-05 16:16:13,2019-02-02 07:36:46
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/779,https://api.github.com/repos/kubernetes/autoscaler/issues/779,Modify the returned error message,"Line 70: return nil, fmt.Errorf(""failed to load dyamic config: %v"", err)
dyamic->dynamic",closed,True,2018-04-06 09:46:01,2018-05-01 03:02:25
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/780,https://api.github.com/repos/kubernetes/autoscaler/issues/780,Fix retured err message,"Line 55: glog.Errorf(""Cannot serialize checkpotint for vpa %v container %v. Reason: %+v"", vpa.ID.VpaName, container, err)
checkpotint->checkpoint",closed,True,2018-04-06 11:25:07,2018-05-01 03:02:12
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/781,https://api.github.com/repos/kubernetes/autoscaler/issues/781,Add spellchecking script & batch fix typos,"Add simple spellchecking script based on https://github.com/kubernetes/kubernetes/blob/master/hack/verify-spelling.sh

Unfortunately, it doesn't seem to catch all typos (e.g. didn't find #780). Patches welcome!",closed,True,2018-04-06 11:52:41,2018-04-09 08:35:53
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/782,https://api.github.com/repos/kubernetes/autoscaler/issues/782,Disable flaky AWS test,"Disabling the test as it's causing presubmits to fail on unrelated PRs (for example, #780.)  Progress on fixing it tracked in #516.",closed,True,2018-04-06 12:09:15,2018-04-06 12:35:24
autoscaler,discordianfish,https://github.com/kubernetes/autoscaler/pull/783,https://api.github.com/repos/kubernetes/autoscaler/issues/783,AWS: Support selecting ASG by values,"So far aws asg discovery only works by matching on tag keys, not values.
But I have a tag ""KubernetesCluster"" with a value to identify the cluster, so this isn't working for me.

This PR (should) adds support for filtering the ASG by key *and* value, e.g:
```
--node-group-auto-discovery=asg:tag=KubernetesCluster=my-cluster
```

Not working since something appears to modify the flag value before reaching the cloudprovider stuff. Will pick this up next week but though some early feedback would be useful.",closed,True,2018-04-06 14:31:09,2018-04-10 14:47:22
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/784,https://api.github.com/repos/kubernetes/autoscaler/issues/784,correct the returned error message in aggregate_container_state.go,Line 107: Unssuported->Unsupported,closed,True,2018-04-08 14:57:17,2018-05-01 02:58:12
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/785,https://api.github.com/repos/kubernetes/autoscaler/issues/785,correct the returned message in ready.go,line 71: feadiness->readiness,closed,True,2018-04-08 15:00:44,2018-05-01 02:57:55
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/786,https://api.github.com/repos/kubernetes/autoscaler/issues/786,Update Cluster Autoscaler dependencies,Update Cluster Autoscaler  dependencies before 1.2.2 release,closed,True,2018-04-10 12:15:30,2018-04-10 14:42:48
autoscaler,nak3,https://github.com/kubernetes/autoscaler/pull/787,https://api.github.com/repos/kubernetes/autoscaler/issues/787,Fix absent image tag version in README,"Since addon-resizer does not have the tag `1.8`, this patch updates README to use `1.8.1`.

https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/addon-resizer?gcrImageListsize=50",closed,True,2018-04-10 13:12:21,2018-04-11 04:25:26
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/788,https://api.github.com/repos/kubernetes/autoscaler/issues/788,Typo fix in FAQ.md,"In the main text of this doc, mostly Kubernetes is capitalized, while line 430 not.
It's better to be same.",closed,True,2018-04-10 14:33:31,2018-05-01 02:56:26
autoscaler,julianvmodesto,https://github.com/kubernetes/autoscaler/issues/789,https://api.github.com/repos/kubernetes/autoscaler/issues/789,AWS: updated instance type doesn't update cluster-autoscaler instance template,"In AWS, I updated my instance type for my autoscaling group's launch configuration and doubled the cpu and memory.

However, this didn't update the cluster-autoscaler instance template and I see that my Pods still won't schedule even though they'll fit on the new instance type:

```
  Normal   NotTriggerScaleUp  6s                 cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```

Version:
```
          image: gcr.io/google_containers/cluster-autoscaler:v1.0.4
```",open,False,2018-04-10 17:43:25,2019-03-29 13:48:21
autoscaler,sujithvs74,https://github.com/kubernetes/autoscaler/issues/790,https://api.github.com/repos/kubernetes/autoscaler/issues/790,aws - autoscaler failing with Wrong id: expected format aws:///<zone>/<name>,"I have deployed autoscaler in aws with auto discovery option and application fails with below error,

> I0411 03:07:37.172013       1 polling_autoscaler.go:111] Poll finished
I0411 03:07:37.172031       1 static_autoscaler.go:97] Starting main loop
I0411 03:07:37.353080       1 reflector.go:428] k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174: Watch close - *v1.Pod total 0 items received
I0411 03:07:37.355165       1 round_trippers.go:436] GET https://10.96.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded&resourceVersion=3644534&timeoutSeconds=422&watch=true 200 OK in 1 milliseconds
W0411 03:07:37.393124       1 clusterstate.go:514] Failed to get nodegroup for dev-k8s-node-asg-230-i-089e4d2f163533989: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.393145       1 clusterstate.go:514] Failed to get nodegroup for stg-k8s-w2-npe-master-3: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.393152       1 clusterstate.go:514] Failed to get nodegroup for dev-k8s-node-prm-1: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.393158       1 clusterstate.go:514] Failed to get nodegroup for dev-k8s-node-asg-230-i-0eb3341fce85be39c: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.393164       1 clusterstate.go:514] Failed to get nodegroup for dev-k8s-node-asg-230-i-091d1a037311d5daf: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.393169       1 clusterstate.go:514] Failed to get nodegroup for dev-k8s-node-asg-230-i-041dd54f2baaa4553: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.393188       1 clusterstate.go:560] Readiness for node group dev-k8s-node-asg-230 not found
W0411 03:07:37.393203       1 clusterstate.go:560] Readiness for node group stg-k8s-agent-w2-asg not found
I0411 03:07:37.393235       1 static_autoscaler.go:162] 7 unregistered nodes present
I0411 03:07:37.393266       1 static_autoscaler.go:230] Filtering out schedulables
I0411 03:07:37.393358       1 static_autoscaler.go:240] No schedulable pods
I0411 03:07:37.393378       1 static_autoscaler.go:244] No unschedulable pods
I0411 03:07:37.393392       1 static_autoscaler.go:280] Calculating unneeded nodes
W0411 03:07:37.476803       1 utils.go:394] Error while checking node group for dev-k8s-node-asg-230-i-089e4d2f163533989: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.476828       1 utils.go:394] Error while checking node group for stg-k8s-w2-npe-master-3: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.476835       1 utils.go:394] Error while checking node group for dev-k8s-node-prm-1: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.476839       1 utils.go:394] Error while checking node group for dev-k8s-node-asg-230-i-0eb3341fce85be39c: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.476845       1 utils.go:394] Error while checking node group for dev-k8s-node-asg-230-i-091d1a037311d5daf: Wrong id: expected format aws:///<zone>/<name>, got
W0411 03:07:37.476849       1 utils.go:394] Error while checking node group for dev-k8s-node-asg-230-i-041dd54f2baaa4553: Wrong id: expected format aws:///<zone>/<name>, got
I0411 03:07:37.477001       1 static_autoscaler.go:309] Scale down status: unneededOnly=true lastScaleUpTime=2018-04-11 03:00:49.050134466 +0000 UTC lastScaleDownDeleteTime=2018-04-11 03:00:49.050135304 +0000 UTC lastScaleDownFailTime=2018-04-11 03:00:49.05013595 +0000 UTC schedulablePodsPresent=false isDeleteInProgress=false
W0411 03:07:37.477028       1 clusterstate.go:346] Failed to find readiness information for dev-k8s-node-asg-230
W0411 03:07:37.477037       1 clusterstate.go:403] Failed to find readiness information for dev-k8s-node-asg-230
W0411 03:07:37.477041       1 clusterstate.go:346] Failed to find readiness information for dev-k8s-node-asg-230
W0411 03:07:37.477049       1 clusterstate.go:346] Failed to find readiness information for stg-k8s-agent-w2-asg
W0411 03:07:37.477060       1 clusterstate.go:403] Failed to find readiness information for stg-k8s-agent-w2-asg
W0411 03:07:37.477064       1 clusterstate.go:346] Failed to find readiness information for stg-k8s-agent-w2-asg
I0411 03:07:37.479834       1 round_trippers.go:436] GET https://10.96.0.1:443/api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status 200 OK in 2 milliseconds",closed,False,2018-04-11 03:13:52,2018-11-01 14:20:16
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/791,https://api.github.com/repos/kubernetes/autoscaler/issues/791,Ignore TPU resource in simulations,Ignore TPU resource in Cluster Autoscaler simulations.,closed,True,2018-04-11 10:21:19,2018-04-13 11:19:14
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/792,https://api.github.com/repos/kubernetes/autoscaler/issues/792,Cluster Autoscaler 1.2.1,,closed,True,2018-04-12 09:49:01,2018-04-12 09:52:42
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/793,https://api.github.com/repos/kubernetes/autoscaler/issues/793,VPA admission controller gcr path fixed for Docker,,closed,True,2018-04-12 09:54:52,2018-04-12 13:19:39
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/794,https://api.github.com/repos/kubernetes/autoscaler/issues/794,Refactor cluster autoscaler builder and add pod list processor.,,closed,True,2018-04-12 12:35:32,2018-04-26 11:08:56
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/795,https://api.github.com/repos/kubernetes/autoscaler/issues/795,Use explicit timestamp format in status configmap,"Go 1.9 changed the default string format, see golang/go#20876

As discussed there, default format is intended to be readable for humans only and can change without warning, so we should probably set it explicitly when writing status configmap.",closed,False,2018-04-12 12:41:27,2018-05-29 11:58:09
autoscaler,EPinci,https://github.com/kubernetes/autoscaler/issues/796,https://api.github.com/repos/kubernetes/autoscaler/issues/796,Autoscaling Azure nodepool to zero,Is it possible to scale down to zero nodes a nodepool on Azure?,closed,False,2018-04-13 10:31:50,2018-05-03 18:48:16
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/797,https://api.github.com/repos/kubernetes/autoscaler/issues/797,VPA untangle consts for setting memory limits and bumping up recommendation.,,closed,True,2018-04-16 13:17:27,2018-04-16 18:45:41
autoscaler,michaelburch,https://github.com/kubernetes/autoscaler/issues/798,https://api.github.com/repos/kubernetes/autoscaler/issues/798,Azure scale up fails,"Cluster autoscaler 1.2, kubernetes 1.10.1, availabilityset

Scale up successfully creates nodes, but they fail to join the cluster. Azure portal shows that the custom script extensions fails with this error
Enable failed: failed to execute command: command terminated with exit status=2
[stdout]

[stderr]
/bin/sh: 1: Syntax error: ""("" unexpected
",closed,False,2018-04-16 15:22:45,2018-07-16 16:48:20
autoscaler,dobesv,https://github.com/kubernetes/autoscaler/issues/799,https://api.github.com/repos/kubernetes/autoscaler/issues/799,Account for AWS instance limit (or any limit in the number of instances),"I hit the limit on the maximum number of instances globally on the AWS account.  I was thinking it would be nice if cluster-autoscaler would somehow have an algorithm that works within a maximum number of nodes and somehow ""merges"" nodes into larger instances to reduce fragmentation as demand increases.",closed,False,2018-04-16 18:30:25,2018-04-17 16:10:10
autoscaler,thaniyarasu,https://github.com/kubernetes/autoscaler/issues/800,https://api.github.com/repos/kubernetes/autoscaler/issues/800,Important : Autoscaler is not stable on load testing,"i have create a fresh cluster(for testing purpose) with aws provider.
My Cluster info is kops(1.9.0),kubernetes(1.10.1),autoscaler(1.2.1),helm(v2.9.0-rc3).
i installed autoscaler by ""Auto-Discovery Setup"" way by following this https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
i have enabled proper IAM role,policy,ASG tags, settings.
it was seems after 5 min autoscaller start working by add/remove nodes.

at next day, i have installed 3 helm packages where each contain 12 deployments(each deploy contain only one container), ingress,hpa(min=1, max=3) .
so the autoscaller start to add more nodes to support the deployments. and start handle the traffic load.
all are meaning full, everything was fine. 
BUT, i have started a load testing by calling each of three application url.
after a minute,within 60 sec my containers are not serving traffic. so i open dashboard. 
i was shocked , because most of the running container are rebooting or reallocating into another nodes so not reachable.
after 2 mins every container start rebooting by showing the status ""Creating Container"",etc,etc.

so i tried to delete 2 helm charts, which make worse than the previous, this time all the container from ""kube-system"" namespace are rebooting. i see that a lot of kube-dsn,cluster-autoscaler container are not started from ""kube-system"" namespace , i saw those on dashboard.
after 5 mins kubernetes api is not reachable,  
 
so i have stopped load testing, left and came back after 2 hours , than i see that dashboard, api, all service are working fine.
My point here is Auto scaler is not stable when more traffic reach cluster.
sometimes ""helm upgrade chart-name"" also making autoscaler unstable.
",closed,False,2018-04-17 06:18:12,2018-05-03 07:41:13
autoscaler,cupofcat,https://github.com/kubernetes/autoscaler/pull/801,https://api.github.com/repos/kubernetes/autoscaler/issues/801,Update README.md,Fixed a few minor issues to make it flow better and be slightly more clear.,closed,True,2018-04-17 10:10:16,2018-04-21 21:42:38
autoscaler,GFilipek,https://github.com/kubernetes/autoscaler/pull/802,https://api.github.com/repos/kubernetes/autoscaler/issues/802,VPA readme update.,"Fixed architecture doc link
Added command to see current VPA recommendations",closed,True,2018-04-17 10:10:30,2018-04-17 16:58:28
autoscaler,tcolgate,https://github.com/kubernetes/autoscaler/issues/803,https://api.github.com/repos/kubernetes/autoscaler/issues/803,Unregistered nodes are not removed.,"The logic for handling unregistered nodes seems off. If we have an unregistered node, we clearly aren't going to be able to schedule workloads on it. If the cluster size is already at minimum capacity, then removing an unregistered node fails:

```
I0417 10:50:13.797723       1 static_autoscaler.go:162] 1 unregistered nodes present
I0417 10:50:13.797744       1 utils.go:324] Removing unregistered node aws:///eu-west-1c/i-XXXXXXX
W0417 10:50:13.835734       1 utils.go:340] Failed to remove node aws:///eu-west-1c/i-XXXXXX: node group min size reached, skipping unregistered node removal
```
Under these conditions we can't remove nodes that have failed to boot. Surely it would make more senese to forcibly terminate the node?

On a related note, a metric for unregistered nodes would be useful, since the CA is the only thing that has a view of the cloud providers list of nodes, and noticing this is quite hard without it.

Happy to contribute if you agree. Was considering adding a force option to the provider node deletions.",open,False,2018-04-17 11:09:24,2019-01-09 11:36:22
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/804,https://api.github.com/repos/kubernetes/autoscaler/issues/804,Rephrase unremovable node warning,"If there were just one unremovable node, 'were' would be grating:

`scale_down.go:175] Scale-down calculation: ignoring 1 nodes, that were unremovable in the last 5m0s`",closed,True,2018-04-18 11:45:12,2018-04-18 15:38:53
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/805,https://api.github.com/repos/kubernetes/autoscaler/issues/805,Add support for Azure Managed Service Identity (MSI),"This PR adds support for Azure Managed Service Identity (MSI). It also updates azure compute client to api version 2017-12-01, which is required to set ipforwarding for virtual machines. 

Fixes #711.",closed,True,2018-04-18 15:04:51,2018-04-27 01:55:32
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/806,https://api.github.com/repos/kubernetes/autoscaler/issues/806,Updates to VPA example deployment & README + adjust resources,"Change the hamster deployment to use 0.5 CPU / pod, so that the example
can be used on a small cluster. Update README.
Make safety margin flag controllable.
Change the default of safety-margin-min-cpu from 300 to 200 millicores.
Change the CPU request of VPA components to 50 millicores.",closed,True,2018-04-18 16:30:43,2018-04-19 14:28:21
autoscaler,kkmsft,https://github.com/kubernetes/autoscaler/pull/807,https://api.github.com/repos/kubernetes/autoscaler/issues/807,FAQ and fix_gopath.sh fixes,"* Fix the fix_gopath name in FAQ.
* Fix the fix_gopath.sh to create the v1alpha1 directory for cases where the k8s did not have v1alpha dependency.",closed,True,2018-04-19 00:58:57,2018-04-19 07:32:22
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/808,https://api.github.com/repos/kubernetes/autoscaler/issues/808,Cherry-pick of #805: Add support for Azure Managed Service Identity (MSI),Cherry-pick of #805: Add support for Azure Managed Service Identity (MSI),closed,True,2018-04-19 07:36:01,2018-04-22 10:58:04
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/809,https://api.github.com/repos/kubernetes/autoscaler/issues/809,Factor the checkpoint specific code out of aggregations.,,closed,True,2018-04-19 14:25:15,2018-04-25 12:52:53
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/issues/810,https://api.github.com/repos/kubernetes/autoscaler/issues/810,./cluster-autoscaler --version fails if configuration is invalid ,"After fresh build of `cluster-autoscaler` running ends up with an error. 
Version string is not printed out.

```
$ ./cluster-autoscaler --version
F0423 14:11:01.201039  173926 main.go:219] Failed to build Kubernetes client configuration: unable to load in-cluster configuration, KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined
```

It seems proper configuration should not be need to get version info.",closed,False,2018-04-23 12:18:09,2018-12-20 13:34:33
autoscaler,Globegitter,https://github.com/kubernetes/autoscaler/issues/811,https://api.github.com/repos/kubernetes/autoscaler/issues/811,VPA: jsonpatch add operation does not apply,"I have just been starting to test the VPA (currently I am mostly interested in the ""initial"" to get request and limit resources set based on some metrics). So I created a statefulset without anything in the resource section and getting the following error:
```
  Warning  FailedCreate      3m (x6 over 6m)   statefulset-controller  create Pod api-proxy-redis-master-0 in StatefulSet api-proxy-redis-master failed error: Internal error occurred: Internal error occurred: jsonpatch add operation does not apply: doc is missing path: /spec/containers/0/resources/requests/memory
  Warning  FailedCreate      1m (x20 over 6m)  statefulset-controller  create Pod api-proxy-redis-master-0 in StatefulSet api-proxy-redis-master failed error: Internal error occurred: Internal error occurred: jsonpatch add operation does not apply: doc is missing path: /spec/containers/0/resources/requests/cpu
```
I would have expected the VPA to be able to set these fields even if they did not exist before.

Once I updated the `Statefulset` it all worked fine.",closed,False,2018-04-23 16:14:33,2018-06-07 06:32:24
autoscaler,resouer,https://github.com/kubernetes/autoscaler/issues/812,https://api.github.com/repos/kubernetes/autoscaler/issues/812,How to protect pods from node scale down,"Is there any way to prevent scale down happen if there's some specific pods running on it?

The use case is sometimes, there are some pods that can not be killed until they exit by themselves.",closed,False,2018-04-24 04:24:46,2018-04-24 18:09:34
autoscaler,kkmsft,https://github.com/kubernetes/autoscaler/pull/813,https://api.github.com/repos/kubernetes/autoscaler/issues/813,Autoscaler for Azure Container Service (AKS and ACS),,closed,True,2018-04-24 22:50:54,2018-05-07 23:08:36
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/issues/814,https://api.github.com/repos/kubernetes/autoscaler/issues/814,DynamicAutoscaler lacks CleanUp call in Reconfigure,"DynamicAutoscaler's Reconfigure method can create a [new Autoscaler](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/core/dynamic_autoscaler.go#L88) but it does not call CleanUp for it, breaking Autoscaler contract for the wrapped instance.

To be decided whether it's worth fixing, I've heard that DynamicAutoscaler is not actually used anymore.",closed,False,2018-04-25 15:32:23,2018-05-15 09:41:12
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/815,https://api.github.com/repos/kubernetes/autoscaler/issues/815,Switch from per-pod aggregations to per-labelset aggregations.,"* Usage samples are no longer aggregated per pod (which prevented from garbage collecting terminated pods and could lead to very large number of aggregations, when many pods with different names were created in the cluster).
* We introduce a new concept of the AggregateStateKey, which determines the set of containers
for which usage samples are aggregated. In the current implementation the AggregateStateKey
consists of { namespace, containerName, label set }, however it can be easily changed/extended in future, if necessary.
* Each container holds a pointer to the AggregateContainerState to which it contributes usage samples.
* Each VPA holds a set of all AggregateContainerStates which it matches.
* Pod no longer keeps links to the VPAs that match it. VPA no longer keeps links to Pods it matches.
* Checkpoint specific code is moved out from AggregateContainerState to Vpa.
* Memory peaks are stored in a histogram, similarly to CPU samples. Each container keeps its last peak.
  Sliding window is no longer used.
* Added more test cases.
* TODO: Garbage collect old Pods and AggregateContainerStates.
* TODO: Delete sliding window.",closed,True,2018-04-26 09:02:59,2018-05-07 09:00:44
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/816,https://api.github.com/repos/kubernetes/autoscaler/issues/816,Use less verbose sort.Slice instead sort.Sort,,closed,True,2018-04-26 09:31:52,2018-04-26 09:45:48
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/817,https://api.github.com/repos/kubernetes/autoscaler/issues/817,Use less verbose sort.Slice instead sort.Sort,,closed,True,2018-04-26 09:46:31,2018-04-26 16:44:46
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/818,https://api.github.com/repos/kubernetes/autoscaler/issues/818,"""pod"" and ""Pod"" are mixed except that in proper noun","In this doc, ""pod"" and ""Pod"" are mixed. For the proper noun like ""Vertical Pod Autoscaler"", it's in capital. That's OK.
But in other situations, it's mixed in capital and lowercase. It's better to use same format.",closed,True,2018-04-26 14:37:06,2018-05-01 02:06:41
autoscaler,syscll,https://github.com/kubernetes/autoscaler/issues/819,https://api.github.com/repos/kubernetes/autoscaler/issues/819, cluster-autoscaler-status configmap does not mirror changes in AWS ASG,"I'm not entirely sure if this is an issue, or my misconfiguration, but I am running the `cluster-autoscaler:1.1.0` on `kubernetes:1.9.7` and the cluster-autoscaler-status configmap does not dynamically pull changes made to an AWS Auto Scaling Group.

I manually updated the maxSize of an ASG in the AWS console and the cluster-autoscaler-status configmap was never updated. I waited at least 2 hours before I manually restarted the pod and it pulled the latest config.

Is this the desired functionality or do I have something misconfigured? I am using the example [deployment](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml).",closed,False,2018-04-26 15:33:23,2018-04-26 15:46:04
autoscaler,prashanth26,https://github.com/kubernetes/autoscaler/issues/820,https://api.github.com/repos/kubernetes/autoscaler/issues/820,Issue with the vendoring,"Hi,
    I was trying to re-vendor (on the release-1.1, release-1.2 and master) using to Godep but was running into these errors. Is the vendoring really broken, or am I missing something here? Any inputs would be helpful.

prashanth@C02V64QBHTD6 ~/G/s/k/a/cluster-autoscaler> git checkout cluster-autoscaler-release-1.1
Branch cluster-autoscaler-release-1.1 set up to track remote branch cluster-autoscaler-release-1.1 from origin.
Switched to a new branch 'cluster-autoscaler-release-1.1'
prashanth@C02V64QBHTD6 ~/G/s/k/a/cluster-autoscaler> godep save
godep: Package (github.com/Microsoft/hcsshim) not found
prashanth@C02V64QBHTD6 ~/G/s/k/a/cluster-autoscaler> godep restore
\# cd /Users/prashanth/Gotemp3/src/github.com/appc/spec; git checkout 191882f834bc779082c4664ed5d464e28d22b270
fatal: reference is not a tree: 191882f834bc779082c4664ed5d464e28d22b270
godep: error downloading dep (github.com/appc/spec/schema): exit status 128
\# cd /Users/prashanth/Gotemp3/src/github.com/aws/aws-sdk-go; git checkout 9c92d9960e4dc397caa84af12ad6513cf78c0b3f
fatal: reference is not a tree: 9c92d9960e4dc397caa84af12ad6513cf78c0b3f
godep: error downloading dep (github.com/aws/aws-sdk-go/aws): exit status 128
\# cd /Users/prashanth/Gotemp3/src/github.com/containerd/containerd; git checkout c8106ff35532072e317a71cce55f18e7917cafcb
fatal: reference is not a tree: c8106ff35532072e317a71cce55f18e7917cafcb
godep: error downloading dep (github.com/containerd/containerd/api/services/containers/v1): exit status 128
\# cd /Users/prashanth/Gotemp3/src/github.com/containernetworking/cni; git checkout 63a263a1942c52411f51677dde8b56710874cbb5
fatal: reference is not a tree: 63a263a1942c52411f51677dde8b56710874cbb5
godep: error downloading dep (github.com/containernetworking/cni/libcni): exit status 128
^C⏎           
                                                                                                                                                                           
prashanth@C02V64QBHTD6 ~/G/s/k/a/cluster-autoscaler> git checkout cluster-autoscaler-release-1.2
Checking out files: 100% (10542/10542), done.
Branch cluster-autoscaler-release-1.2 set up to track remote branch cluster-autoscaler-release-1.2 from origin.
Switched to a new branch 'cluster-autoscaler-release-1.2'
prashanth@C02V64QBHTD6 ~/G/s/k/a/cluster-autoscaler> godep save
godep: Package (bitbucket.org/ww/goautoneg) not found
prashanth@C02V64QBHTD6 ~/G/s/k/a/cluster-autoscaler>

Thanks in advance. 

Regards,
Prashanth",closed,False,2018-04-26 15:54:32,2018-05-04 17:05:00
autoscaler,innovia,https://github.com/kubernetes/autoscaler/issues/821,https://api.github.com/repos/kubernetes/autoscaler/issues/821,helm chart install panic: runtime error: invalid memory address or nil pointer dereference,"**kubernetes ver:** 1.8.7
**ca version:** 1.0.5

**values.yaml:**

```yaml
autoDiscovery: {}
# Only cloudProvider `aws` and `gce` are supported by auto-discovery at this time
# AWS: Set tags as described in https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup
#   clusterName:  # cluster.local

autoscalingGroups:
- name: on-demand-nodes.k8.domain.com
  maxSize: 20
  minSize: 3
- name: spot-nodes.k8.domain.com
  maxSize: 20
  minSize: 0

autoscalingGroupsnamePrefix:
- name: on-demand-nodes
  maxSize: 20
  minSize: 3
- name: spot-nodes
  maxSize: 10
  minSize: 0

# Required if cloudProvider=aws
awsRegion: us-west-2

# Currently only `gce`, `aws` & `spotinst` are supported
cloudProvider: aws

image:
  repository: k8s.gcr.io/cluster-autoscaler
  tag: v1.0.5
  pullPolicy: IfNotPresent

tolerations: []

extraArgs:
  v: 5
  stderrthreshold: info
  logtostderr: true
  # write-status-configmap: true
  leader-elect: true
  expander: least-waste
  scale-down-enabled: true
  balance-similar-node-groups: true
  min-replica-count: 2
  scale-down-utilization-threshold: 0.5
  scale-down-non-empty-candidates-count: 5
  max-node-provision-time: 15m0s
  scan-interval: 10s
  scale-down-unneeded-time: 5m
  skip-nodes-with-local-storage: false
  skip-nodes-with-system-pods: true


## Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## affinity: {}

podDisruptionBudget: |
  maxUnavailable: 1
  # minAvailable: 2

## Node labels for pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

podAnnotations: {}
podLabels: {}
replicaCount: 1

rbac:
  ## If true, create & use RBAC resources
  ##
  create: true
  ## If true, create & use Pod Security Policy resources
  ## https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  pspEnabled: true
  ## Ignored if rbac.create is true
  ##
  serviceAccountName: default

resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 300Mi
  # requests:
  #   cpu: 100m
  #   memory: 300Mi

service:
  annotations: {}
  clusterIP: """"

  ## List of IP addresses at which the service is available
  ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
  ##
  externalIPs: []

  loadBalancerIP: """"
  loadBalancerSourceRanges: []
  servicePort: 8085
  portName: http
  type: ClusterIP

spotinst:
  account: """"
  token: """"
  image:
    repository: spotinst/kubernetes-cluster-autoscaler
    tag: 0.6.0
    pullPolicy: IfNotPresent

```

**Rendered Yaml deployment:**
```yaml
# Source: cluster-autoscaler/templates/deployment.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: aws-cluster-autoscaler
    chart: cluster-autoscaler-0.6.1
    heritage: Tiller
    release: autoscale
  name: autoscale-aws-cluster-autoscaler
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: aws-cluster-autoscaler
        release: autoscale
    spec:
      containers:
        - name: aws-cluster-autoscaler
          image: ""k8s.gcr.io/cluster-autoscaler:v1.0.5""
          imagePullPolicy: ""IfNotPresent""
          command:
            - ./cluster-autoscaler
            - --cloud-provider=aws
            - --namespace=default
            - --nodes=3:20:on-demand-nodes.k8.domain.com
            - --nodes=0:20:spot-nodes.k8.domain.com
            - --balance-similar-node-groups=true
            - --expander=least-waste
            - --leader-elect=true
            - --logtostderr=true
            - --max-node-provision-time=15m0s
            - --min-replica-count=2
            - --scale-down-enabled=true
            - --scale-down-non-empty-candidates-count=5
            - --scale-down-unneeded-time=5m
            - --scale-down-utilization-threshold=0.5
            - --scan-interval=10s
            - --skip-nodes-with-local-storage
            - --skip-nodes-with-system-pods=true
            - --stderrthreshold=info
            - --v=4

          env:
            - name: AWS_REGION
              value: ""us-west-2""
          livenessProbe:
            httpGet:
              path: /health-check
              port: 8085
          ports:
            - containerPort: 8085
          resources:
            {}

          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
      serviceAccountName: autoscale-aws-cluster-autoscaler
      tolerations:
        []

      volumes:
        - name: ssl-certs
          hostPath:
            path: /etc/ssl/certs/ca-certificates.crt
```
**ca logs**
```
I0430 20:52:11.790915       1 flags.go:52] FLAG: --address="":8085""
I0430 20:52:11.791060       1 flags.go:52] FLAG: --alsologtostderr=""false""
I0430 20:52:11.791089       1 flags.go:52] FLAG: --application-metrics-count-limit=""100""
I0430 20:52:11.791113       1 flags.go:52] FLAG: --azure-container-registry-config=""""
I0430 20:52:11.791139       1 flags.go:52] FLAG: --balance-similar-node-groups=""true""
I0430 20:52:11.791159       1 flags.go:52] FLAG: --boot-id-file=""/proc/sys/kernel/random/boot_id""
I0430 20:52:11.791194       1 flags.go:52] FLAG: --cloud-config=""""
I0430 20:52:11.791219       1 flags.go:52] FLAG: --cloud-provider=""aws""
I0430 20:52:11.791240       1 flags.go:52] FLAG: --cloud-provider-gce-lb-src-cidrs=""35.191.0.0/16,209.85.152.0/22,209.85.204.0/22,130.211.0.0/22""
I0430 20:52:11.791268       1 flags.go:52] FLAG: --cluster-name=""""
I0430 20:52:11.791288       1 flags.go:52] FLAG: --configmap=""""
I0430 20:52:11.791307       1 flags.go:52] FLAG: --container-hints=""/etc/cadvisor/container_hints.json""
I0430 20:52:11.791337       1 flags.go:52] FLAG: --cores-total=""0:320000""
I0430 20:52:11.791359       1 flags.go:52] FLAG: --docker=""unix:///var/run/docker.sock""
I0430 20:52:11.791383       1 flags.go:52] FLAG: --docker-env-metadata-whitelist=""""
I0430 20:52:11.791405       1 flags.go:52] FLAG: --docker-only=""false""
I0430 20:52:11.791428       1 flags.go:52] FLAG: --docker-root=""/var/lib/docker""
I0430 20:52:11.791450       1 flags.go:52] FLAG: --docker-tls=""false""
I0430 20:52:11.791472       1 flags.go:52] FLAG: --docker-tls-ca=""ca.pem""
I0430 20:52:11.791496       1 flags.go:52] FLAG: --docker-tls-cert=""cert.pem""
I0430 20:52:11.791529       1 flags.go:52] FLAG: --docker-tls-key=""key.pem""
I0430 20:52:11.791552       1 flags.go:52] FLAG: --enable-load-reader=""false""
I0430 20:52:11.791575       1 flags.go:52] FLAG: --estimator=""binpacking""
I0430 20:52:11.791596       1 flags.go:52] FLAG: --event-storage-age-limit=""default=0""
I0430 20:52:11.791616       1 flags.go:52] FLAG: --event-storage-event-limit=""default=0""
I0430 20:52:11.791638       1 flags.go:52] FLAG: --expander=""least-waste""
I0430 20:52:11.791659       1 flags.go:52] FLAG: --gke-api-endpoint=""""
I0430 20:52:11.791678       1 flags.go:52] FLAG: --global-housekeeping-interval=""1m0s""
I0430 20:52:11.791704       1 flags.go:52] FLAG: --google-json-key=""""
I0430 20:52:11.791727       1 flags.go:52] FLAG: --housekeeping-interval=""10s""
I0430 20:52:11.791751       1 flags.go:52] FLAG: --httptest.serve=""""
I0430 20:52:11.791783       1 flags.go:52] FLAG: --kubeconfig=""""
I0430 20:52:11.791806       1 flags.go:52] FLAG: --kubernetes=""""
I0430 20:52:11.791828       1 flags.go:52] FLAG: --leader-elect=""true""
I0430 20:52:11.791853       1 flags.go:52] FLAG: --leader-elect-lease-duration=""15s""
I0430 20:52:11.791878       1 flags.go:52] FLAG: --leader-elect-renew-deadline=""10s""
I0430 20:52:11.791899       1 flags.go:52] FLAG: --leader-elect-resource-lock=""endpoints""
I0430 20:52:11.791921       1 flags.go:52] FLAG: --leader-elect-retry-period=""2s""
I0430 20:52:11.791943       1 flags.go:52] FLAG: --log-backtrace-at="":0""
I0430 20:52:11.791974       1 flags.go:52] FLAG: --log-cadvisor-usage=""false""
I0430 20:52:11.791997       1 flags.go:52] FLAG: --log-dir=""""
I0430 20:52:11.792024       1 flags.go:52] FLAG: --logtostderr=""true""
I0430 20:52:11.792049       1 flags.go:52] FLAG: --machine-id-file=""/etc/machine-id,/var/lib/dbus/machine-id""
I0430 20:52:11.792073       1 flags.go:52] FLAG: --max-autoprovisioned-node-group-count=""15""
I0430 20:52:11.792094       1 flags.go:52] FLAG: --max-empty-bulk-delete=""10""
I0430 20:52:11.792114       1 flags.go:52] FLAG: --max-failing-time=""15m0s""
I0430 20:52:11.792135       1 flags.go:52] FLAG: --max-graceful-termination-sec=""600""
I0430 20:52:11.792155       1 flags.go:52] FLAG: --max-inactivity=""10m0s""
I0430 20:52:11.792176       1 flags.go:52] FLAG: --max-node-provision-time=""15m0s""
I0430 20:52:11.792200       1 flags.go:52] FLAG: --max-nodes-total=""0""
I0430 20:52:11.792223       1 flags.go:52] FLAG: --max-total-unready-percentage=""33""
I0430 20:52:11.792248       1 flags.go:52] FLAG: --memory-total=""0:6400000""
I0430 20:52:11.792282       1 flags.go:52] FLAG: --min-replica-count=""2""
I0430 20:52:11.792305       1 flags.go:52] FLAG: --namespace=""default""
I0430 20:52:11.792327       1 flags.go:52] FLAG: --node-autoprovisioning-enabled=""false""
I0430 20:52:11.792348       1 flags.go:52] FLAG: --node-group-auto-discovery=""""
I0430 20:52:11.792367       1 flags.go:52] FLAG: --nodes=""[3:20:on-demand-nodes.k8.domain.com 0:20:spot-nodes.k8.domain.com]""
I0430 20:52:11.792392       1 flags.go:52] FLAG: --ok-total-unready-count=""3""
I0430 20:52:11.792413       1 flags.go:52] FLAG: --scale-down-candidates-pool-min-count=""50""
I0430 20:52:11.792438       1 flags.go:52] FLAG: --scale-down-candidates-pool-ratio=""0.1""
I0430 20:52:11.792463       1 flags.go:52] FLAG: --scale-down-delay-after-add=""10m0s""
I0430 20:52:11.792487       1 flags.go:52] FLAG: --scale-down-delay-after-delete=""10s""
I0430 20:52:11.792509       1 flags.go:52] FLAG: --scale-down-delay-after-failure=""3m0s""
I0430 20:52:11.792570       1 flags.go:52] FLAG: --scale-down-enabled=""true""
I0430 20:52:11.792595       1 flags.go:52] FLAG: --scale-down-non-empty-candidates-count=""5""
I0430 20:52:11.792619       1 flags.go:52] FLAG: --scale-down-unneeded-time=""5m0s""
I0430 20:52:11.792640       1 flags.go:52] FLAG: --scale-down-unready-time=""20m0s""
I0430 20:52:11.792662       1 flags.go:52] FLAG: --scale-down-utilization-threshold=""0.5""
I0430 20:52:11.792685       1 flags.go:52] FLAG: --scan-interval=""10s""
I0430 20:52:11.792707       1 flags.go:52] FLAG: --skip-nodes-with-local-storage=""true""
I0430 20:52:11.792727       1 flags.go:52] FLAG: --skip-nodes-with-system-pods=""true""
I0430 20:52:11.792748       1 flags.go:52] FLAG: --stderrthreshold=""0""
I0430 20:52:11.792774       1 flags.go:52] FLAG: --storage-driver-buffer-duration=""1m0s""
I0430 20:52:11.792796       1 flags.go:52] FLAG: --storage-driver-db=""cadvisor""
I0430 20:52:11.792828       1 flags.go:52] FLAG: --storage-driver-host=""localhost:8086""
I0430 20:52:11.792852       1 flags.go:52] FLAG: --storage-driver-password=""root""
I0430 20:52:11.792874       1 flags.go:52] FLAG: --storage-driver-secure=""false""
I0430 20:52:11.792895       1 flags.go:52] FLAG: --storage-driver-table=""stats""
I0430 20:52:11.792914       1 flags.go:52] FLAG: --storage-driver-user=""root""
I0430 20:52:11.792935       1 flags.go:52] FLAG: --unregistered-node-removal-time=""15m0s""
I0430 20:52:11.792956       1 flags.go:52] FLAG: --v=""5""
I0430 20:52:11.792976       1 flags.go:52] FLAG: --version=""false""
I0430 20:52:11.793007       1 flags.go:52] FLAG: --vmodule=""""
I0430 20:52:11.793033       1 flags.go:52] FLAG: --write-status-configmap=""true""
I0430 20:52:11.793063       1 main.go:287] Cluster Autoscaler 1.0.5
I0430 20:52:11.837697       1 leaderelection.go:174] attempting to acquire leader lease...
I0430 20:52:11.878225       1 leaderelection.go:184] successfully acquired lease default/cluster-autoscaler
I0430 20:52:11.878761       1 factory.go:33] Event(v1.ObjectReference{Kind:""Endpoints"", Namespace:""default"", Name:""cluster-autoscaler"", UID:""d650f8f5-ceb7-11e7-bf8c-06b53f858278"", APIVersion:""v1"", ResourceVersion:""50510892"", FieldPath:""""}): type: 'Normal' reason: 'LeaderElection' autoscale-aws-cluster-autoscaler-6b7b69d5f7-hpnpg became leader
I0430 20:52:11.879699       1 predicates.go:123] Using predicate PodFitsResources
I0430 20:52:11.879750       1 predicates.go:123] Using predicate GeneralPredicates
I0430 20:52:11.879772       1 predicates.go:123] Using predicate PodToleratesNodeTaints
I0430 20:52:11.879797       1 predicates.go:123] Using predicate NoVolumeZoneConflict
I0430 20:52:11.879819       1 predicates.go:123] Using predicate CheckNodeDiskPressure
I0430 20:52:11.879835       1 predicates.go:123] Using predicate MatchInterPodAffinity
I0430 20:52:11.879852       1 predicates.go:123] Using predicate MaxAzureDiskVolumeCount
I0430 20:52:11.879868       1 predicates.go:123] Using predicate NoDiskConflict
I0430 20:52:11.879954       1 predicates.go:123] Using predicate NoVolumeNodeConflict
I0430 20:52:11.879994       1 predicates.go:123] Using predicate CheckNodeCondition
I0430 20:52:11.880015       1 predicates.go:123] Using predicate MaxEBSVolumeCount
I0430 20:52:11.880010       1 reflector.go:202] Starting reflector *v1.Node (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.880048       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.880035       1 predicates.go:123] Using predicate CheckNodeMemoryPressure
I0430 20:52:11.880279       1 predicates.go:123] Using predicate MaxGCEPDVolumeCount
I0430 20:52:11.880288       1 predicates.go:123] Using predicate ready
I0430 20:52:11.880439       1 reflector.go:202] Starting reflector *v1beta1.ReplicaSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.880534       1 reflector.go:240] Listing and watching *v1beta1.ReplicaSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.880626       1 reflector.go:202] Starting reflector *v1beta1.DaemonSet (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I0430 20:52:11.880639       1 reflector.go:240] Listing and watching *v1beta1.DaemonSet from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I0430 20:52:11.881070       1 reflector.go:202] Starting reflector *v1.PersistentVolume (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.881118       1 reflector.go:240] Listing and watching *v1.PersistentVolume from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.881356       1 reflector.go:202] Starting reflector *v1beta1.StatefulSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.881374       1 reflector.go:240] Listing and watching *v1beta1.StatefulSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.881682       1 reflector.go:202] Starting reflector *v1.PersistentVolumeClaim (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.881728       1 reflector.go:240] Listing and watching *v1.PersistentVolumeClaim from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.881912       1 reflector.go:202] Starting reflector *v1.Pod (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.881969       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.882186       1 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149
I0430 20:52:11.882205       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149
I0430 20:52:11.882326       1 reflector.go:202] Starting reflector *v1.Service (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.882354       1 reflector.go:240] Listing and watching *v1.Service from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.882414       1 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I0430 20:52:11.882426       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I0430 20:52:11.882717       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212
I0430 20:52:11.882736       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212
I0430 20:52:11.882942       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I0430 20:52:11.882955       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I0430 20:52:11.883049       1 reflector.go:202] Starting reflector *v1.ReplicationController (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.883079       1 reflector.go:240] Listing and watching *v1.ReplicationController from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0430 20:52:11.883216       1 reflector.go:202] Starting reflector *v1beta1.PodDisruptionBudget (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0430 20:52:11.883233       1 reflector.go:240] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0430 20:52:12.079362       1 request.go:462] Throttling request took 196.135393ms, request: GET:https://100.64.0.1:443/api/v1/replicationcontrollers?resourceVersion=0
I0430 20:52:12.279367       1 request.go:462] Throttling request took 313.490271ms, request: PUT:https://100.64.0.1:443/api/v1/namespaces/default/configmaps/cluster-autoscaler-status
I0430 20:52:12.284473       1 main.go:222] Registered cleanup signal handler
I0430 20:52:12.284524       1 auto_scaling_groups.go:83] Regenerating ASG information for on-demand-nodes.k8.domain.com
I0430 20:52:12.373991       1 auto_scaling_groups.go:83] Regenerating ASG information for spot-nodes.k8.domain.com
I0430 20:52:13.885820       1 leaderelection.go:199] successfully renewed lease default/cluster-autoscaler
I0430 20:52:15.893793       1 leaderelection.go:199] successfully renewed lease default/cluster-autoscaler
I0430 20:52:17.901802       1 leaderelection.go:199] successfully renewed lease default/cluster-autoscaler
I0430 20:52:19.910163       1 leaderelection.go:199] successfully renewed lease default/cluster-autoscaler
I0430 20:52:21.918931       1 leaderelection.go:199] successfully renewed lease default/cluster-autoscaler
I0430 20:52:22.284700       1 static_autoscaler.go:97] Starting main loop
I0430 20:52:22.423825       1 auto_scaling_groups.go:83] Regenerating ASG information for on-demand-nodes.k8.domain.com
I0430 20:52:22.445508       1 auto_scaling_groups.go:83] Regenerating ASG information for spot-nodes.k8.domain.com
I0430 20:52:22.470275       1 static_autoscaler.go:225] Filtering out schedulables
I0430 20:52:22.470729       1 static_autoscaler.go:235] No schedulable pods
I0430 20:52:22.470782       1 scale_up.go:54] Pod redash/redash-server-gunicorn-5597786cc5-cmwtn is unschedulable
I0430 20:52:22.470793       1 scale_up.go:54] Pod redash/redash-server-gunicorn-5597786cc5-bttf2 is unschedulable
I0430 20:52:22.470797       1 scale_up.go:54] Pod redash/redash-server-gunicorn-5597786cc5-cqlnx is unschedulable
W0430 20:52:22.555857       1 aws_manager.go:225] Found multiple availability zones, using us-west-2a
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x1451b0d]

goroutine 46 [running]:
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).buildNodeFromTemplate(0xc420c37aa0, 0xc420f5e330, 0xc42130a780, 0xc42130a780, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:252 +0x48d
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*Asg).TemplateNodeInfo(0xc420f5e330, 0xc4208eec30, 0x7fff30213764, 0x21)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider.go:355 +0x78
k8s.io/autoscaler/cluster-autoscaler/core.GetNodeInfosForGroups(0xc420edf040, 0x4, 0x4, 0x50bd320, 0xc420f5e210, 0x50c76e0, 0xc420905e10, 0xc420d2f120, 0x4, 0x4, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/utils.go:211 +0x65a
k8s.io/autoscaler/cluster-autoscaler/core.ScaleUp(0xc420c8b860, 0xc420d2f0a0, 0x3, 0x4, 0xc420edf040, 0x4, 0x4, 0xc420d2f120, 0x4, 0x4, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:57 +0x28e
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc420f7c070, 0xed2797906, 0xe10f7a4a1, 0x51902a0, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:252 +0x13ea
main.run(0xc420a53ae0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:263 +0x474
main.main.func2(0xc420265860)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:345 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:145 +0x97
```",closed,False,2018-04-30 20:57:21,2018-11-08 18:08:40
autoscaler,pkelleratwork,https://github.com/kubernetes/autoscaler/issues/822,https://api.github.com/repos/kubernetes/autoscaler/issues/822,"Pods hanging in ""Pending"" - pod didn't trigger scale-up (it wouldn't fit if a new node is added)","Created cluster via kops on AWS 
Desired 4, min 2, max 8
Currently 1 master, 4 nodes. C4/t2.medium respectively
IAM per CA created & assign to master & node groups 
CA running in kube-system/master node
Kubernetes v1.8.7
Resource limits on pods are mem/cpu 300/100, matching the CA. 
Nodes are not scaling, as expected. Requesting help to move forward, thanks in advance 

```kind: InstanceGroup
metadata:
  creationTimestamp: 2018-04-20T20:45:38Z
  labels:
    kops.k8s.io/cluster: <name>.k8s.local
  name: nodes
spec:
  image: kope.io/k8s-1.8-debian-jessie-amd64-hvm-ebs-2018-02-08
  machineType: t2.medium
  maxSize: 8
  minSize: 2
  nodeLabels:
    kops.k8s.io/instancegroup: nodes
  role: Node
  subnets:
  - us-xx-xx


errors from kubectl describe pod

Events:
  Type     Reason             Age                 From                Message
  ----     ------             ----                ----                -------
  Warning  FailedScheduling   20m (x11 over 23m)  default-scheduler   No nodes are available that match all of the predicates: Insufficient cpu (3), Insufficient memory (3), NodeUnschedulable (1), PodToleratesNodeTaints (1).
  Warning  FailedScheduling   19m (x5 over 20m)   default-scheduler   No nodes are available that match all of the predicates: Insufficient cpu (3), Insufficient memory (3), PodToleratesNodeTaints (1).
  Normal   NotTriggerScaleUp  2m (x114 over 22m)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added)
  Warning  FailedScheduling   2m (x58 over 18m)   default-scheduler   No nodes are available that match all of the predicates: Insufficient cpu (3), Insufficient memory (4), PodToleratesNodeTaints (1).```

my ca.yaml file
```---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""""]
  resources: [""events"",""endpoints""]
  verbs: [""create"", ""patch""]
- apiGroups: [""""]
  resources: [""pods/eviction""]
  verbs: [""create""]
- apiGroups: [""""]
  resources: [""pods/status""]
  verbs: [""update""]
- apiGroups: [""""]
  resources: [""endpoints""]
  resourceNames: [""cluster-autoscaler""]
  verbs: [""get"",""update""]
- apiGroups: [""""]
  resources: [""nodes""]
  verbs: [""watch"",""list"",""get"",""update""]
- apiGroups: [""""]
  resources: [""pods"",""services"",""replicationcontrollers"",""persistentvolumeclaims"",""persistentvolumes""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""extensions""]
  resources: [""replicasets"",""daemonsets""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""policy""]
  resources: [""poddisruptionbudgets""]
  verbs: [""watch"",""list""]
- apiGroups: [""apps""]
  resources: [""statefulsets""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""storage.k8s.io""]
  resources: [""storageclasses""]
  verbs: [""watch"",""list"",""get""]

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""""]
  resources: [""configmaps""]
  verbs: [""create""]
- apiGroups: [""""]
  resources: [""configmaps""]
  resourceNames: [""cluster-autoscaler-status""]
  verbs: [""delete"",""get"",""update""]

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      nodeSelector:
        kubernetes.io/role: master
      containers:
        - image: k8s.gcr.io/cluster-autoscaler:v1.0.3
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --nodes=3:9:master-us-xx-xx.masters.<name>.k8s.local
          env:
            - name: AWS_REGION
              value: us-xx-xx
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
          imagePullPolicy: ""Always""
      volumes:
        - name: ssl-certs
          hostPath:
            path: ""/etc/ssl/certs/ca-certificates.crt""",closed,False,2018-05-01 20:54:27,2018-05-04 20:41:02
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/823,https://api.github.com/repos/kubernetes/autoscaler/issues/823,Add zero nodes support for Azure VMSS,"This PR adds zero nodes support for Azure VMSS.

Fixes #796.

Note that: Azure VMAS (vmType: standard) doesn't support this feature.",closed,True,2018-05-02 09:10:08,2018-05-03 03:11:11
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/824,https://api.github.com/repos/kubernetes/autoscaler/issues/824,Typo fix in FAQ.md : kubernets->kubernetes,kubernets->kubernetes,closed,True,2018-05-02 10:59:43,2018-05-03 08:46:30
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/825,https://api.github.com/repos/kubernetes/autoscaler/issues/825,Add TemplateNodeInfo support for Azure vmss,"This PR adds TemplateNodeInfo() support for Azure vmss, which is required when scaling up from 0 nodes.

Note that Azure vmas (vmType is standard) doesn't support this feature.",closed,True,2018-05-03 08:37:08,2018-05-07 01:56:58
autoscaler,kkmsft,https://github.com/kubernetes/autoscaler/pull/826,https://api.github.com/repos/kubernetes/autoscaler/issues/826,Update FAQ with note on referring kubernetes.sync,,closed,True,2018-05-03 23:15:01,2018-05-05 16:48:34
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/827,https://api.github.com/repos/kubernetes/autoscaler/issues/827,Use ResourceEventHandlerFuncs to create vpa lister.,,closed,True,2018-05-04 10:10:16,2018-05-05 16:48:08
autoscaler,forkjoseph,https://github.com/kubernetes/autoscaler/issues/828,https://api.github.com/repos/kubernetes/autoscaler/issues/828,VPA metrics not retrieved,,closed,False,2018-05-06 18:06:38,2018-05-06 19:21:36
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/829,https://api.github.com/repos/kubernetes/autoscaler/issues/829,Cherry-pick #823 and #825 to CA 1.2.X,Automated cherry pick of  #823 and #825 to CA 1.2.X: fix zero nodes support for Azure virtual machine scale sets (VMSS).,closed,True,2018-05-07 02:21:05,2018-05-07 09:17:11
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/830,https://api.github.com/repos/kubernetes/autoscaler/issues/830,Add support for rescheduled pods with the same name in drain,Add support for rescheduled pods with the same name in drain.,closed,True,2018-05-07 10:02:27,2018-05-11 11:47:54
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/831,https://api.github.com/repos/kubernetes/autoscaler/issues/831,Remove implementation details (CleanUp) from the interface.,"The CleanUp method is instead called directly from the implementation,
when required.
Test updated in a quick way since the mock we're using does not support
AtLeast(1) - thus Times(2).",closed,True,2018-05-07 12:51:51,2018-05-07 15:31:13
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/832,https://api.github.com/repos/kubernetes/autoscaler/issues/832,Typo fix: memeory->memory,memeory->memory,closed,True,2018-05-07 15:47:48,2018-05-07 16:42:10
autoscaler,pkelleratwork,https://github.com/kubernetes/autoscaler/issues/833,https://api.github.com/repos/kubernetes/autoscaler/issues/833,CA tries to add pods to master instead of scaling a new node,"I'm expecting new nodes to scale up instead of adding pods to master. I want to avoid adding pods to master. I am getting `reason: PodToleratesNodeTaints` when it tries to add to master.

What am i missing to scale up nodes?

kops ing
```apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2018-05-04T20:31:55Z
  labels:
    kops.k8s.io/cluster: <cluster>.k8s.local
  name: nodes
spec:
  image: kope.io/k8s-1.8-debian-jessie-amd64-hvm-ebs-2018-02-08
  machineType: t2.medium
  maxSize: 9
  minSize: 3
  nodeLabels:
    kops.k8s.io/instancegroup: nodes
  role: Node
  subnets:
  - us-east-2a`


NAME                                          STATUS    ROLES     AGE       VERSION
ip-172-xx-xx-xxx.us-xx-xx.compute.internal   Ready     node      2d        v1.9.3
ip-172-xx-xx-xxx.us-xx-xx.compute.internal   Ready     node      2d        v1.9.3
ip-172-xx-xx-xxx.us-xx-xx.compute.internal   Ready     node      2d        v1.9.3
ip-172-xx-xx-xxx.us-xx-xx.compute.internal   Ready     master    2d        v1.9.3


I0507 16:25:08.160505       1 static_autoscaler.go:108] Starting main loop
I0507 16:25:08.312549       1 utils.go:436] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0507 16:25:08.312573       1 static_autoscaler.go:240] Filtering out schedulables
I0507 16:25:08.313112       1 static_autoscaler.go:250] No schedulable pods
I0507 16:25:08.313139       1 scale_up.go:54] Pod default/qafeature2-api-7d68478c9b-86s6z is unschedulable
I0507 16:25:08.313193       1 scale_up.go:54] Pod default/qafeature2-api-7d68478c9b-76g2s is unschedulable
I0507 16:25:08.313232       1 scale_up.go:54] Pod default/qafeature2-web-d478cd9b9-ssxjj is unschedulable
I0507 16:25:08.313245       1 scale_up.go:54] Pod default/qafeature2-web-d478cd9b9-dswvl is unschedulable
I0507 16:25:08.313249       1 scale_up.go:54] Pod default/qafeature2-bff-7f7489cf84-r4j2d is unschedulable
I0507 16:25:08.313253       1 scale_up.go:54] Pod default/qafeature2-bff-7f7489cf84-grgqm is unschedulable
I0507 16:25:08.313277       1 scale_up.go:54] Pod redis/qafeature2-redis-redis-6859d6646f-hk2v7 is unschedulable
I0507 16:25:08.341165       1 scale_up.go:86] Upcoming 0 nodes
I0507 16:25:08.360344       1 scale_up.go:146] Scale-up predicate failed: PodToleratesNodeTaints predicate mismatch, cannot put default/qafeature2-api-7d68478c9b-86s6z on template-node-for-master-us-east-2a.masters.<cluster>.k8s.local-9128099766993471710, reason: PodToleratesNodeTaints
I0507 16:25:08.360370       1 scale_up.go:146] Scale-up predicate failed: PodToleratesNodeTaints predicate mismatch, cannot put default/qafeature2-api-7d68478c9b-76g2s on template-node-for-master-us-east-2a.masters.<cluster>.k8s.local-9128099766993471710, reason: PodToleratesNodeTaints
I0507 16:25:08.360386       1 scale_up.go:146] Scale-up predicate failed: PodToleratesNodeTaints predicate mismatch, cannot put default/qafeature2-web-d478cd9b9-ssxjj on template-node-for-master-us-east-2a.masters.<cluster>.k8s.local-9128099766993471710, reason: PodToleratesNodeTaints
I0507 16:25:08.360400       1 scale_up.go:146] Scale-up predicate failed: PodToleratesNodeTaints predicate mismatch, cannot put default/qafeature2-web-d478cd9b9-dswvl on template-node-for-master-us-east-2a.masters.<cluster>.k8s.local-9128099766993471710, reason: PodToleratesNodeTaints
I0507 16:25:08.360419       1 scale_up.go:146] Scale-up predicate failed: PodToleratesNodeTaints predicate mismatch, cannot put default/qafeature2-bff-7f7489cf84-r4j2d on template-node-for-master-us-east-2a.masters.<cluster>.k8s.local-9128099766993471710, reason: PodToleratesNodeTaints
I0507 16:25:08.360432       1 scale_up.go:146] Scale-up predicate failed: PodToleratesNodeTaints predicate mismatch, cannot put default/qafeature2-bff-7f7489cf84-grgqm on template-node-for-master-us-east-2a.masters.<cluster>.k8s.local-9128099766993471710, reason: PodToleratesNodeTaints
I0507 16:25:08.360447       1 scale_up.go:146] Scale-up predicate failed: PodToleratesNodeTaints predicate mismatch, cannot put redis/qafeature2-redis-redis-6859d6646f-hk2v7 on template-node-for-master-us-east-2a.masters.<cluster>.k8s.local-9128099766993471710, reason: PodToleratesNodeTaints
I0507 16:25:08.360464       1 scale_up.go:175] No pod can fit to master-us-east-2a.masters.<cluster>.k8s.local
I0507 16:25:08.360477       1 scale_up.go:180] No expansion options
I0507 16:25:08.360547       1 static_autoscaler.go:299] Calculating unneeded nodes
I0507 16:25:08.361467       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""qafeature2-web-d478cd9b9-dswvl"", UID:""d8401c7a-5212-11e8-b554-022b7a31eac4"", APIVersion:""v1"", ResourceVersion:""476122"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0507 16:25:08.361516       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""qafeature2-bff-7f7489cf84-r4j2d"", UID:""d8d3596d-5212-11e8-b554-022b7a31eac4"", APIVersion:""v1"", ResourceVersion:""476139"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0507 16:25:08.361605       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""qafeature2-bff-7f7489cf84-grgqm"", UID:""d8d43c86-5212-11e8-b554-022b7a31eac4"", APIVersion:""v1"", ResourceVersion:""476140"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0507 16:25:08.361624       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""redis"", Name:""qafeature2-redis-redis-6859d6646f-hk2v7"", UID:""d4482e1c-5212-11e8-b554-022b7a31eac4"", APIVersion:""v1"", ResourceVersion:""476078"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0507 16:25:08.361636       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""qafeature2-api-7d68478c9b-86s6z"", UID:""d7a6002f-5212-11e8-b554-022b7a31eac4"", APIVersion:""v1"", ResourceVersion:""476101"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0507 16:25:08.361647       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""qafeature2-api-7d68478c9b-76g2s"", UID:""d7a8c070-5212-11e8-b554-022b7a31eac4"", APIVersion:""v1"", ResourceVersion:""476103"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0507 16:25:08.362476       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""qafeature2-web-d478cd9b9-ssxjj"", UID:""d83e44fd-5212-11e8-b554-022b7a31eac4"", APIVersion:""v1"", ResourceVersion:""476119"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0507 16:25:08.391999       1 utils.go:408] Skipping ip-172-xx-xx-xx.xx-xx-xx.compute.internal - node group min size reached
I0507 16:25:08.392018       1 utils.go:399] Skipping ip-172-xx-xx-xx.xx-xx-xx.compute.internal - no node group config
I0507 16:25:08.392052       1 utils.go:399] Skipping ip-172-xx-xx-xx.xx-xx-xx.compute.internal - no node group config
I0507 16:25:08.392063       1 utils.go:399] Skipping ip-172-xx-xx-xx.xx-xx-xx.compute.internal - no node group config
I0507 16:25:08.393659       1 static_autoscaler.go:329] Scale down status: unneededOnly=false lastScaleUpTime=2018-05-05 14:53:03.343699447 +0000 UTC lastScaleDownDeleteTime=2018-05-05 14:53:03.343699997 +0000 UTC lastScaleDownFailTime=2018-05-05 14:53:03.343700358 +0000 UTC schedulablePodsPresent=false isDeleteInProgress=false
I0507 16:25:08.393684       1 static_autoscaler.go:332] Starting scale down
I0507 16:25:08.443351       1 scale_down.go:446] No candidates for scale down`",closed,False,2018-05-07 16:37:09,2018-05-07 18:06:26
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/834,https://api.github.com/repos/kubernetes/autoscaler/issues/834,Remove unused CloudProvider() method.,,closed,True,2018-05-08 09:26:27,2018-05-08 10:48:14
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/835,https://api.github.com/repos/kubernetes/autoscaler/issues/835,Make the code slightly more idiomatic go,,closed,True,2018-05-08 09:30:24,2018-05-08 10:58:15
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/836,https://api.github.com/repos/kubernetes/autoscaler/issues/836,Use timestamp argument,,closed,True,2018-05-08 14:25:42,2018-05-08 20:48:54
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/837,https://api.github.com/repos/kubernetes/autoscaler/issues/837,Add missing classifier,,closed,True,2018-05-08 14:26:24,2018-05-08 14:45:19
autoscaler,zdoherty,https://github.com/kubernetes/autoscaler/issues/838,https://api.github.com/repos/kubernetes/autoscaler/issues/838,EC2 Fleet Autoscaling Support?,"Amazon recently released a feature called [EC2 Fleets](https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/) which appears to consolidate spot fleet requests with EC2 on-demand/auto-scaling group requests. Per [their documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/manage-ec2-fleet.html), this appears to support a similar feature to desired capacity in auto-scaling:

> You can modify the following parameters of an EC2 Fleet:
> - target-capacity – Increase or decrease the target capacity.

target-capacity appears to be very similar to spot fleets weighted capactity, but you're able to change it over time. Being able to change the target-capacity parameter over time seems to align closely with changing the DesiredCapacity parameter of an auto-scaling group. Are there any plans to support EC2 fleets with Kubernetes autoscaling?",open,False,2018-05-09 03:36:56,2019-03-18 10:05:47
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/839,https://api.github.com/repos/kubernetes/autoscaler/issues/839,Fix autoscaler manifests yaml for AKS,"There is an indent error in AKS manifest file.

/cc @kkmsft @mwielgus ",closed,True,2018-05-09 08:16:34,2018-05-14 03:20:51
autoscaler,markbooch,https://github.com/kubernetes/autoscaler/issues/840,https://api.github.com/repos/kubernetes/autoscaler/issues/840,Documents not comprehensible,"the document shows ""The /etc/ssl/certs/ca-certificates.crt should exist by default on your ec2 instance.""
but no other information indicates the purpose of this requirment, and what does it means the specific certificate?

see more:
https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#common-notes-and-gotchas",closed,False,2018-05-09 10:32:52,2018-12-07 02:13:32
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/841,https://api.github.com/repos/kubernetes/autoscaler/issues/841,Remove unused error handling,,closed,True,2018-05-10 10:17:31,2018-05-10 12:10:46
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/842,https://api.github.com/repos/kubernetes/autoscaler/issues/842,Merge two variables into one.,"Both are used simply to prevent scale-down and, moreover, I'd like to abstract ""scale-down forbidden"" logic even more in the future refactoring(s).",closed,True,2018-05-10 12:54:19,2018-05-14 08:54:44
autoscaler,beaudryj,https://github.com/kubernetes/autoscaler/issues/843,https://api.github.com/repos/kubernetes/autoscaler/issues/843,Windows Nodes?,"With the Azure Support, will this work with Hybrid ACS-Engine Built Environments with Windows Nodes and linux nodes ? 

i.e. 

```
{
    ""apiVersion"": ""vlabs"",
    ""properties"": {
      ""orchestratorProfile"": {
        ""orchestratorType"": ""Kubernetes"",
        ""orchestratorRelease"": ""1.10"",
        ""kubernetesConfig"": {
          ""enableRbac"": true,
          ""apiServerConfig"" : {
            ""--feature-gates"": ""HyperVContainer=true""
          },
          ""kubeletConfig"" : {
            ""--feature-gates"": ""HyperVContainer=true""
           }
        }
      },
      ""aadProfile"": {
        ""serverAppID"":  """",
        ""clientAppID"":  """",
        ""tenantID"":  """",
        ""adminGroupID"":  """"
      },
      ""masterProfile"": {
        ""count"": 1,
        ""dnsPrefix"": ""ebk8s"",
        ""vmSize"": ""Standard_F2s_v2""
      },
      ""agentPoolProfiles"": [
        {
          ""name"": ""linuxpool1"",
          ""count"": 2,
          ""vmSize"": ""Standard_F4s_v2"",
          ""availabilityProfile"": ""VirtualMachineScaleSets"",
          ""storageProfile"": ""ManagedDisks"",
          ""OSDiskSizeGB"": 256
        },
        {
          ""name"": ""windowspool2"",
          ""count"": 2,
          ""vmSize"": ""Standard_F4s_v2"",
          ""availabilityProfile"": ""VirtualMachineScaleSets"",
          ""storageProfile"": ""ManagedDisks"",
          ""OSDiskSizeGB"": 256,
          ""osType"": ""Windows""
        }
      ],
      ""windowsProfile"": {
        ""adminUsername"": """",
        ""adminPassword"": """"      },
      ""linuxProfile"": {
        ""adminUsername"": """",
        ""ssh"": {
          ""publicKeys"": [
            {
              ""keyData"":  """"
            }
          ]
        }
      },
      ""servicePrincipalProfile"": {
        ""clientId"":  """",
        ""secret"":  """"
      }
    }
  }
```",closed,False,2018-05-10 16:17:58,2018-08-12 13:34:05
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/844,https://api.github.com/repos/kubernetes/autoscaler/issues/844,Cherry pick stateful set drain 1.2,Cherry-pick of #830 ,closed,True,2018-05-11 11:53:36,2018-05-11 12:48:35
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/845,https://api.github.com/repos/kubernetes/autoscaler/issues/845,Cherry pick stateful set drain 1.1,Cherry-pick of #830 ,closed,True,2018-05-11 12:12:11,2018-05-11 12:48:03
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/846,https://api.github.com/repos/kubernetes/autoscaler/issues/846,Cherry pick stateful set drain 1.0,Cherry-pick of #830,closed,True,2018-05-11 12:12:55,2018-05-11 12:13:09
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/847,https://api.github.com/repos/kubernetes/autoscaler/issues/847,Cherry pick stateful set drain 1.0,Cherry-pick of #830,closed,True,2018-05-11 12:15:29,2018-05-11 12:47:07
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/848,https://api.github.com/repos/kubernetes/autoscaler/issues/848,Add caching to get machine type calls.,,closed,True,2018-05-11 13:08:06,2018-05-25 14:51:51
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/issues/849,https://api.github.com/repos/kubernetes/autoscaler/issues/849,Autoscaler AKS connection timeout on scaling,"When using autoscaler with Azure Container Service (AKS), the connection of Kubernetes master will timeout for a while when scaling up new nodes.

```sh
$ kubectl -n kube-system logs -f cluster-autoscaler-fd9f8c6f4-c98zn
...
I0514 02:52:12.173125       1 scale_up.go:200] Best option to resize: agentpool
I0514 02:52:12.173142       1 scale_up.go:204] Estimated 2 nodes needed in agentpool
I0514 02:52:12.248041       1 scale_up.go:293] Final scale-up plan: [{agentpool 3->5 (max: 50)}]
I0514 02:52:12.248061       1 scale_up.go:345] Scale-up: setting group agentpool size to 5
I0514 02:52:12.325217       1 azure_container_service_pool.go:206] Set size request: 5
I0514 02:52:12.402896       1 azure_container_service_pool.go:241] Current size: 3, Target size requested: 5

error: http2: server sent GOAWAY and closed the connection; LastStreamID=3, ErrCode=NO_ERROR, debug=""""
```

Try again immediately, still timeout:

```sh
$ kubectl -n kube-system logs -f cluster-autoscaler-fd9f8c6f4-c98zn
Unable to connect to the server: net/http: TLS handshake timeout
```

After scaling up done, the connection will recover. If checking autoscaler logs again, there're also connection timeout errors there:

```sh
$ kubectl -n kube-system logs -f cluster-autoscaler-fd9f8c6f4-c98zn
E0514 03:05:11.803703       1 reflector.go:205] k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87: Failed to list *v1.Node: Get https://10.0.0.1:443/api/v1/nodes?limit=500&resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0514 03:05:11.803795       1 reflector.go:205] k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87: Failed to list *v1.Service: Get https://10.0.0.1:443/api/v1/services?limit=500&resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0514 03:05:11.803797       1 reflector.go:205] k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266: Failed to list *v1beta1.PodDisruptionBudget: Get https://10.0.0.1:443/apis/policy/v1beta1/poddisruptionbudgets?limit=500&resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0514 03:05:11.803894       1 reflector.go:205] k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212: Failed to list *v1.Node: Get https://10.0.0.1:443/api/v1/nodes?limit=500&resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0514 03:05:11.803908       1 reflector.go:205] k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87: Failed to list *v1beta1.StatefulSet: Get https://10.0.0.1:443/apis/apps/v1beta1/statefulsets?limit=500&resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0514 03:05:11.803916       1 reflector.go:205] k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174: Failed to list *v1.Pod: Get https://10.0.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded&limit=500&resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0514 03:05:11.804039       1 reflector.go:205] k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293: Failed to list *v1beta1.DaemonSet: Get https://10.0.0.1:443/apis/extensions/v1beta1/daemonsets?limit=500&resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0514 03:05:11.804164       1 reflector.go:205] k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239: Failed to list *v1.Node: Get https://10.0.0.1:443/api/v1/nodes?limit=500&resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0514 03:05:11.804181       1 reflector.go:205] k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87: Failed to list *v1.StorageClass: Get https://10.0.0.1:443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
E0514 03:05:11.804278       1 reflector.go:205] k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87: Failed to list *v1beta1.PodDisruptionBudget: Get https://10.0.0.1:443/apis/policy/v1beta1/poddisruptionbudgets?limit=500&resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
```

By the way, there is **no such issue for acs-engine deployed clusters (both VMAS and VMSS)**.

cc  @kkmsft @khenidak @JackQuincy",closed,False,2018-05-14 03:23:27,2019-03-08 04:43:09
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/850,https://api.github.com/repos/kubernetes/autoscaler/issues/850,Delete not needed test of VPA admission controller.,,closed,True,2018-05-14 08:36:47,2018-05-14 09:40:53
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/851,https://api.github.com/repos/kubernetes/autoscaler/issues/851,Remove DynamicAutoscaler since it's unused,As discussed in #814 it's no longer needed.,closed,True,2018-05-14 12:36:49,2018-05-14 18:22:43
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/852,https://api.github.com/repos/kubernetes/autoscaler/issues/852,Fix DecreaseTargetSize for AKS,/cc @kkmsft @mwielgus,closed,True,2018-05-15 04:47:24,2018-05-15 09:02:11
autoscaler,eliasbalasis,https://github.com/kubernetes/autoscaler/issues/853,https://api.github.com/repos/kubernetes/autoscaler/issues/853,Cannot pull autoscaler Docker image + errors,"Having followed the instructions, we are getting in Kubernetes Dashboard:

Failed to pull image ""k8s.gcr.io/cluster-autoscaler"": rpc error: code = Unknown desc = unauthorized: authentication required
Error syncing pod

My suspicion is that login info is missing for Docker registry k8s.gcr.io

any ideas?
",closed,False,2018-05-15 06:58:31,2018-05-16 10:09:08
autoscaler,GFilipek,https://github.com/kubernetes/autoscaler/pull/854,https://api.github.com/repos/kubernetes/autoscaler/issues/854,Refactor recommendation post processing.,"- RecommendationProcessor interface - all modifications to recommendations should be done by RecommendationProcessor objects
- Refactoring of resource request capping code : use RecommendationProcessor interface

cc/ @mwielgus ",closed,True,2018-05-15 07:49:33,2018-05-15 08:59:27
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/855,https://api.github.com/repos/kubernetes/autoscaler/issues/855,Add gpu related tests to scale_up_test,,closed,True,2018-05-15 08:00:38,2018-05-16 09:25:43
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/856,https://api.github.com/repos/kubernetes/autoscaler/issues/856,Update go version to 1.10 and deprecate all <1.9,,closed,True,2018-05-15 09:17:00,2018-05-16 09:23:43
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/857,https://api.github.com/repos/kubernetes/autoscaler/issues/857,"Release Cluster Autoscaler 1.2.2, 1.1.3 and 1.0.6","If you believe something should be included that isn't yet merged to relevant branch, please link a PR with cherry-pick here. Short description of why it's needed would be welcome. ",closed,False,2018-05-15 09:36:12,2018-05-31 08:59:52
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/858,https://api.github.com/repos/kubernetes/autoscaler/issues/858, Split Autoscaler.RunOnce into separate logical steps,"Please have a look whether we can use the initial commit or do we really need the error-handling boilerplate to make the code look more like a ""normal"" go.",closed,True,2018-05-15 09:37:16,2018-05-16 20:32:46
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/859,https://api.github.com/repos/kubernetes/autoscaler/issues/859,Small refactor of main.go,,closed,True,2018-05-15 10:40:09,2018-05-15 15:05:53
autoscaler,ofcourseican,https://github.com/kubernetes/autoscaler/issues/860,https://api.github.com/repos/kubernetes/autoscaler/issues/860,Autoscaling a acs-engine deployed cluster after upgrading deploys the wrong Kubernetes version,"I have the node autoscaler installed in a cluster that I deployed through acs-engine.

After the initial deployment I upgraded the cluster from v1.9.6 to v1.10.1 through the acs-engine upgrade command.

Now whenever the autoscaler scales up, I get a new v1.9.6 node up and running.

I think the autoscaler should deploy a node with the version that cluster is currently running on.

A workaround I can think of would be to just modify the azuredeploy.parameters.json file/cluster-autoscaler-azure-deploy-parameters secret and put in the new version. But that would mean, that whenever updating I also would need to patch that file.",open,False,2018-05-15 15:52:25,2019-04-03 02:20:25
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/861,https://api.github.com/repos/kubernetes/autoscaler/issues/861,Cherry pick #813 and #852 to cluster-autoscaler-release-1.2,"Cherry pick #813 and #852 to cluster-autoscaler-release-1.2, these commits add support of AKS.

/assign @aleksandra-malinowska",closed,True,2018-05-16 02:28:54,2018-05-16 09:31:32
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/862,https://api.github.com/repos/kubernetes/autoscaler/issues/862,Add tips for upgrading Kubernetes clusters on Azure VMAS,Fixes #703 #860 ,closed,True,2018-05-16 03:37:35,2018-05-16 09:31:38
autoscaler,bytesofdhiren,https://github.com/kubernetes/autoscaler/issues/863,https://api.github.com/repos/kubernetes/autoscaler/issues/863,"Azure AKS Kubernetes error ""runtime error: invalid memory address or nil pointer dereference""","Kuberentes version: 1.8.7
Cluster-autoscaler image version: v1.0.3

While autoscaling, cluster-autoscaler generates the following error

```
$ kubectl -n kube-system logs cluster-autoscaler-7458cd4887-gzlvh
.......
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x60 pc=0x2805da6]
goroutine 83 [running]:
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc420b75b90, 0xed28dac2e, 0xe331d0368, 0x518f0a0, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:99 +0x256
main.run(0xc420717bd0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:262 +0x46f
main.main.func2(0xc420010720)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:344 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:145 +0x97
bash-4.3# kubectl -n kube-system logs cluster-autoscaler-7458cd4887-gzlvh
```
",closed,False,2018-05-16 04:42:59,2018-07-23 05:56:30
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/864,https://api.github.com/repos/kubernetes/autoscaler/issues/864,"Revert ""Cherry pick #813 and #852 to cluster-autoscaler-release-1.2""","Reverts kubernetes/autoscaler#861

cc @feiskyer This added a dependency that isn't in kubernetes/kubernetes: https://github.com/kubernetes/kubernetes/blob/release-1.10/Godeps/Godeps.json#L33 . Can you add it to kubernetes/kubernetes 1.10 release branch (to which we sync Godeps for Cluster Autoscaler 1.2) before we merge it back?

cc @MaciekPytel @mwielgus ",closed,True,2018-05-16 09:59:03,2019-03-20 15:39:03
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/865,https://api.github.com/repos/kubernetes/autoscaler/issues/865,Update Godeps FAQ,Update instructions for adding dependencies to Cluster Autoscaler.,closed,True,2018-05-16 10:52:34,2018-05-16 17:40:27
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/866,https://api.github.com/repos/kubernetes/autoscaler/issues/866,Add override for AKS API,"Alternative way for #864, add override for AKS API.

This PR should be cherry picked to 1.2 branch.

/assign @aleksandra-malinowska 
cc @MaciekPytel @mwielgus
",closed,True,2018-05-16 13:40:02,2018-05-17 12:33:14
autoscaler,johanneswuerbach,https://github.com/kubernetes/autoscaler/pull/867,https://api.github.com/repos/kubernetes/autoscaler/issues/867,AWS: Reduce amount of API calls significantly ,"Instead of doing auto-discovery, nodes per ASG and target size per ASG separately, fetch all
of those ASG details in a single request on each `Refresh()` with maximum 10 seconds granularity.

This reduced the amount of DescribeAutoScalingGroups API calls for our setup (3 ASGs / one per AZ) from ~55 calls / min to ~5 calls / min.

As the DescribeAutoScalingGroups supports up to 1600 members per call, this approach should be fairly scalable https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_DescribeAutoScalingGroups.html and work for more complicated setups.

I'm still testing this PR with our clusters, but I can't see any major downsides in requesting an up-to-date status only every 10s vs. on-demand as the main loop isn't executed more frequently and its unlikely that ASG API is changing responses frequently, but I'm not sure that I understand all nuances of the code yet.

Technically I would be possible to keep the auto-discovery tag to ASG names matching at a 1 minute cadence, but I didn't though the added complexity is worth saving those 5 calls / min.

//cc @mumoshu @negz as you both worked on the AWS provider",closed,True,2018-05-16 23:55:39,2018-05-22 23:42:11
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/868,https://api.github.com/repos/kubernetes/autoscaler/issues/868,Move metrics update to proper place,,closed,True,2018-05-17 07:51:14,2018-05-17 12:52:19
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/869,https://api.github.com/repos/kubernetes/autoscaler/issues/869,Check glog.V once,,closed,True,2018-05-17 07:51:36,2018-05-17 08:45:36
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/870,https://api.github.com/repos/kubernetes/autoscaler/issues/870,Set lastScaleDownFailTime properly,The ScaleDownError check was ureachable,closed,True,2018-05-17 07:52:04,2018-05-17 10:09:56
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/871,https://api.github.com/repos/kubernetes/autoscaler/issues/871,Extract duplicate code into a single method,,closed,True,2018-05-17 08:03:23,2018-05-17 09:49:50
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/872,https://api.github.com/repos/kubernetes/autoscaler/issues/872,Move ClusterStateRegistry to StaticAutoscaler,"AutoscalingContext is basically a configuration and few static helpers and API handles.
ClusterStateRegistry is a state and thus moved closer to other state-keeping objects.",closed,True,2018-05-17 10:15:32,2018-05-24 13:12:59
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/873,https://api.github.com/repos/kubernetes/autoscaler/issues/873,Update max unready nodes to 45%,Update max unready nodes to 45%,closed,True,2018-05-17 10:52:34,2018-05-17 11:49:01
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/874,https://api.github.com/repos/kubernetes/autoscaler/issues/874,Update max unready nodes to 45%,Update max unready nodes to 45%,closed,True,2018-05-17 11:50:56,2018-05-17 12:09:39
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/875,https://api.github.com/repos/kubernetes/autoscaler/issues/875,Cherry pick of #866: Add override for AKS API,"Cherry pick of #866: Add override for AKS API

cc @aleksandra-malinowska @mwielgus PTAL",closed,True,2018-05-17 12:44:38,2018-05-17 13:15:02
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/876,https://api.github.com/repos/kubernetes/autoscaler/issues/876,Cluster Autoscaler 1.2.2,Update version to 1.2.2,closed,True,2018-05-17 13:16:53,2018-05-17 13:32:35
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/issues/877,https://api.github.com/repos/kubernetes/autoscaler/issues/877,Simplify vendor updates for cluster-autoscaler,"Cluster-autoscaler introduces a `fix_gopath.sh` scripts for packages not in kubernetes/kubernetes repo. It makes vendor updates complicated and `godep restore` not working because of commits changed in original packages.

I suggest we switch to godep for all packages, including k/k vendors and outside packages. With this method, a simple vendor update process is only with two commands:

```
godep restore
godep save ./...
```
",closed,False,2018-05-17 13:20:37,2019-02-03 15:07:44
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/878,https://api.github.com/repos/kubernetes/autoscaler/issues/878,Cluster Autoscaler 1.1.3,Update version to 1.1.3,closed,True,2018-05-17 13:25:38,2018-05-17 13:41:14
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/879,https://api.github.com/repos/kubernetes/autoscaler/issues/879,Cluster Autoscaler 1.0.6,Update version to  version 1.0.6,closed,True,2018-05-17 13:26:19,2018-05-17 13:41:24
autoscaler,jbartosik,https://github.com/kubernetes/autoscaler/pull/880,https://api.github.com/repos/kubernetes/autoscaler/issues/880,Allow passing taints to Node Group creation.,,closed,True,2018-05-17 16:28:53,2018-10-31 09:12:23
autoscaler,rofreitas,https://github.com/kubernetes/autoscaler/issues/881,https://api.github.com/repos/kubernetes/autoscaler/issues/881,Failed to (Auto)Scale nodes on Azure using VMSS,"Kubernetes 1.10.2
Auto Scaler 1.2.1

Error: Specified api-version 2017-03-30 does not meet the minimum required api-version 2017-12-01 to set this property enableIPForwarding.

LOG's POD: 
{""log"":""E0517 17:01:46.612618       1 azure_scale_set.go:138] virtualMachineScaleSetsClient.CreateOrUpdate for scale set \""k8s-medium-11014877-vmss\"" failed: compute.VirtualMachineScaleSetsClient#CreateOrUpdate: Failure sending request: StatusCode=200 -- Original Error: Long running operation terminated with status 'Failed': Code=\""MinimumApiVersionNotSpecifiedToSetTheProperty\"" Message=\""Specified api-version 2017-03-30 does not meet the minimum required api-version 2017-12-01 to set this property enableIPForwarding.\""\n"",""stream"":""stderr"",""time"":""2018-05-17T17:01:46.612903996Z""}
{""log"":""W0517 17:01:46.612720       1 clusterstate.go:248] Disabling scale-up for node group k8s-medium-11014877-vmss until 2018-05-17 17:11:46.612713696 +0000 UTC\n"",""stream"":""stderr"",""time"":""2018-05-17T17:01:46.612960796Z""}",closed,False,2018-05-17 17:08:44,2018-05-21 14:25:45
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/882,https://api.github.com/repos/kubernetes/autoscaler/issues/882,Simplify the code by removing superfluous variable,,closed,True,2018-05-18 08:27:04,2018-05-18 09:05:07
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/883,https://api.github.com/repos/kubernetes/autoscaler/issues/883,Reorder & extract initial parts of RunOnce,"For a code review I suggest looking at the commits separately, the intent should be more visible then.",closed,True,2018-05-18 08:28:41,2018-05-22 08:06:27
autoscaler,tobiasrosenhoff,https://github.com/kubernetes/autoscaler/issues/884,https://api.github.com/repos/kubernetes/autoscaler/issues/884,Failing to scale nodes with AKS,"I'm trying to get cluster-autoscaler working with AKS, and I get the following errors. 

Kubernetes version: v1.9.6
Cluster-autoscaler version: v1.2.2

1 scale_up.go:59] Pod default/php-apache-7ccc68c5cd-vrxzp is unschedulable
1 scale_up.go:446] No node info for: agentpool
1 static_autoscaler.go:304] Failed to scale up: failed to find template node for node group agentpool
1 static_autoscaler.go:186] 3 unregistered nodes present


I'm using the following [deployment](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/cluster-autoscaler-containerservice.yaml) 

**My Node labels are:**
agentpool=agentpool
beta.kubernetes.io/arch=amd64
beta.kubernetes.io/instance-type=Standard_DS1_v2
beta.kubernetes.io/os=linux
failure-domain.beta.kubernetes.io/region=westeurope
failure-domain.beta.kubernetes.io/zone=1
kubernetes.io/hostname=aks-agentpool-16882496-0
kubernetes.io/role=agent
storageprofile=managed,storagetier=Premium_LRS


I've tried looking around for answers but I haven't been able to find any clues. 
",closed,False,2018-05-20 21:14:36,2018-05-28 02:18:16
autoscaler,kkmsft,https://github.com/kubernetes/autoscaler/pull/885,https://api.github.com/repos/kubernetes/autoscaler/issues/885,Minor typos and NodeResourceGroup clarification,"Fix couple of typos and clarify the documentation about the NodeResourceGroup. 
Better documentation from the feedback in: https://github.com/kubernetes/autoscaler/issues/884",closed,True,2018-05-21 23:13:36,2018-05-22 01:34:56
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/886,https://api.github.com/repos/kubernetes/autoscaler/issues/886,Remove deployment parameter from AKS docs,"The deployment parameter is not used in aks manifests.

cc @kkmsft ",closed,True,2018-05-22 03:00:31,2018-05-22 07:43:43
autoscaler,GFilipek,https://github.com/kubernetes/autoscaler/pull/887,https://api.github.com/repos/kubernetes/autoscaler/issues/887,Make admission controller server and updater importable,Moved admission server and updater from main to importable packages.,closed,True,2018-05-22 07:06:33,2018-05-22 07:26:30
autoscaler,GFilipek,https://github.com/kubernetes/autoscaler/pull/888,https://api.github.com/repos/kubernetes/autoscaler/issues/888,Pod vpa annotation,Add annotation to pods modified by VPA.,closed,True,2018-05-22 08:36:10,2018-05-22 23:44:02
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/889,https://api.github.com/repos/kubernetes/autoscaler/issues/889,"Typo fix: ""it's reaction time is short""->""its reaction time is short"""," Line 14: 
""it's reaction time is short""->""its reaction time is short""",closed,True,2018-05-22 14:08:52,2018-05-22 23:42:46
autoscaler,tmwtp,https://github.com/kubernetes/autoscaler/issues/890,https://api.github.com/repos/kubernetes/autoscaler/issues/890,Autoscaler 0.6.2 crashing - Failed to describe ASG tags - NoCredentialProviders,"Hi,

We have the 0.6.2 version installed and running on our clusters for some time now.

I am trying to deploy it into one of our lab clusters now and getting the following error:

I0523 13:41:00.590344       1 auto_scaling.go:133] Failed to describe ASG tags for keys [k8s.io/cluster-autoscaler/enabled kubernetes.io/cluster/XXXXXXXXXX] : NoCredentialProviders: no valid providers in chain. Deprecated.
        For verbose messaging see aws.Config.CredentialsChainVerboseErrors
F0523 13:41:00.590414       1 cloud_provider_builder.go:91] Failed to create AWS cloud provider: Failed to get ASGs: NoCredentialProviders: no valid providers in chain. Deprecated.
        For verbose messaging see aws.Config.CredentialsChainVerboseErrors

1. We are working with k8s version 1.7.5

2. We have both tags needed on both masters and nodes:

k8s.io/cluster-autoscaler/enabled & kubernetes.io/cluster/XXXXXX

3. We have the proper IAM permissions for the service to work:

{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""autoscaling:DescribeAutoScalingGroups"",
                ""autoscaling:DescribeAutoScalingInstances"",
                ""autoscaling:DescribeTags"",
                ""autoscaling:SetDesiredCapacity"",
                ""autoscaling:TerminateInstanceInAutoScalingGroup""
            ],
            ""Resource"": ""*""
        }
    ]
}

Will appreciate your help here

Thanks,

Roiy",closed,False,2018-05-23 13:53:15,2019-03-04 08:55:09
autoscaler,lchen223,https://github.com/kubernetes/autoscaler/issues/891,https://api.github.com/repos/kubernetes/autoscaler/issues/891,cluster-autoscaler not scaling down due to daemonsets,"cluster-autoscaler cannot scale down due to kube-flannel-ds daemonset running in kube-system namespace and dd-agent (datadog) daemonset running in default namespace:

```
I0523 21:41:45.599646       1 cluster.go:75] Fast evaluation: ip-10-1-1-2.us-west-2.compute.internal for removal
I0523 21:41:45.599651       1 cluster.go:89] Fast evaluation: node ip-10-1-1-2.us-west-2.compute.internal cannot be removed: kube-system/kube-flannel-ds-vx9lm is not replicated
I0523 21:41:45.599659       1 cluster.go:75] Fast evaluation: ip-10-1-1-3.us-west-2.compute.internal for removal
I0523 21:41:45.599664       1 cluster.go:89] Fast evaluation: node ip-10-1-1-3.us-west-2.compute.internal cannot be removed: default/dd-agent-sz7tc is not replicated
I0523 21:41:45.599666       1 cluster.go:75] Fast evaluation: ip-10-1-1-4.us-west-2.compute.internal for removal
I0523 21:41:45.599671       1 cluster.go:89] Fast evaluation: node ip-10-1-1-4.us-west-2.compute.internal cannot be removed: default/dd-agent-vpn6l is not replicated
```

I don't believe the replicated rule apply here.  How do I get around this so that cluster-autoscaler can scale down?

",closed,False,2018-05-23 21:49:55,2018-05-23 21:56:23
autoscaler,Sturgelose,https://github.com/kubernetes/autoscaler/issues/892,https://api.github.com/repos/kubernetes/autoscaler/issues/892,Autoscaler + Jobs + Scheduler (RoundRobin) = overrequest of resources,"We are using Node Autoscaler in AWS in order to manage our batch processing spikes done with kubernetes Jobs at certain hours of the day. However we are seeing a bit of incompatibility between the default kube-scheduler logic and Autoscaler.

1. Autoscaler won't kill nodes if there are Jobs running (expected). We can't stop a Job mid way.
2. kube-scheduler always deploys pods with a round-robin schedule. 

This means that if we start many jobs one after the other, after the spike, we end with multiple nodes started (because we needed more resources before during the spike). After the spike, most of the nodes end with a  usage under 50% of resources because round-robin scheduling works as expected and Jobs get distributed but autoscaler cannot kill nodes because they are jobs. Also, autoscaler will only decrease the cluster if the node has no Job scheduled during 10 min.

Could there be an option to maybe mark a node with under usage as non-schedulable (cordonated) when there are **many** nodes with low resource usage?  This way, we give time to the Job to end but without adding extra jobs in the node. If we would need to start a new node, we just need to allow scheduling in the node again instead of starting a new node!

Otherwise, while we are waiting for it to end, other jobs will be scheduled in the same node and start the loop again and we will end with tons of nodes with just a single node running in each node... (costly in terms of money....)

An other option is to create a custom scheduler instance, but I thought that as the issue comes with the autoscaler logic, it would make more sense to implement here.

I'd be willing to help developing this if anyone can give me some pointers on where the logic is implemented :)",closed,False,2018-05-24 11:45:27,2018-10-21 19:05:13
autoscaler,jessfraz,https://github.com/kubernetes/autoscaler/issues/893,https://api.github.com/repos/kubernetes/autoscaler/issues/893,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",closed,False,2018-05-24 14:43:05,2019-02-24 23:22:48
autoscaler,a200462790,https://github.com/kubernetes/autoscaler/issues/894,https://api.github.com/repos/kubernetes/autoscaler/issues/894,CA Scale Down Fails because of Daemonset utilization,"Hello,

Is there a way to disregard daemonsets(or certain pods) when considering the utilization of a node to scale down?

I have tried adding the annotation:
`cluster-autoscaler.kubernetes.io/safe-to-evict: ""true""`

but the cluster autoscaler still seems to not kill nodes with high utilization due to daemonsets:

`<node name> is not suitable for removal - utilization too big (0.631092)`",closed,False,2018-05-24 21:17:42,2019-02-15 09:30:33
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/895,https://api.github.com/repos/kubernetes/autoscaler/issues/895,Disable VolumeScheduling predicate,Disable VolumeScheduling predicate (accidentaly re-enabled by #866),closed,True,2018-05-25 09:54:03,2018-05-25 11:02:20
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/896,https://api.github.com/repos/kubernetes/autoscaler/issues/896,Cluster Autoscaler release 1.3,"This issue is for tracking tasks that are must-have for 1.3:
- [x] update godeps to include kubernetes/kubernetes#62180",closed,False,2018-05-25 10:50:57,2018-06-21 11:02:28
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/897,https://api.github.com/repos/kubernetes/autoscaler/issues/897,VPA vendor synced with k8s 1.10.3,,closed,True,2018-05-25 12:10:33,2018-05-25 13:02:56
autoscaler,ChMarco,https://github.com/kubernetes/autoscaler/issues/898,https://api.github.com/repos/kubernetes/autoscaler/issues/898,Helm - Rbac - Autoscaling nodes apiserver:nodes is foridden,"I have an issue with autoscaler seems that is missing some authorizations ... on the master it works ...
I created even a separated namespace with a user but still the same #issue
helm version
Client: &version.Version{SemVer:""v2.9.1"", GitCommit:""20adb27c7c5868466912eebdf6664e7390ebe710"", GitTreeState:""clean""}
Server: &version.Version{SemVer:""v2.9.1"", GitCommit:""20adb27c7c5868466912eebdf6664e7390ebe710"", GitTreeState:""clean""}

Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:17:39Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-07T11:55:20Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}

logs deployment/cluster-autoscaler --namespace=test-autosc
I0527 20:37:59.475210 1 main.go:225] Cluster Autoscaler 0.6.0
F0527 20:37:59.487483 1 main.go:257] Failed to get nodes from apiserver: nodes is forbidden: User ""system:serviceaccount:test-autosc:default"" cannot list nodes at the cluster scope
goroutine 1 [running]:
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.stacks(0xc420139600, 0xc4206d8000, 0xbb, 0x1bf)
/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:766 +0xa7
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.(*loggingT).output(0x39a6240, 0xc400000003, 0xc420207340, 0x390a66c, 0x7, 0x101, 0x0)
/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:717 +0x348
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.(*loggingT).printf(0x39a6240, 0x3, 0x26e7790, 0x26, 0xc4206b7e88, 0x1, 0x1)
/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:655 +0x14f
k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog.Fatalf(0x26e7790, 0x26, 0xc4206b7e88, 0x1, 0x1)
/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.main()
/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:257 +0x48f",closed,False,2018-05-27 20:53:23,2018-11-09 16:30:29
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/899,https://api.github.com/repos/kubernetes/autoscaler/issues/899,Delete autoprovisioned node pool after all nodes are deleted.,,closed,True,2018-05-28 11:20:42,2018-05-28 17:29:18
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/900,https://api.github.com/repos/kubernetes/autoscaler/issues/900,Move some GKE-specific logic outside core,"At some point we added a whole bunch of GKE-specific code to core. This is the first part of cleaning that mess up.

No change in actual logic. Added a new NodeGroupListProcessor interface to encapsulate the existing logic. Moved PodListProcessor and refactor how it's passed around to make it consistent and easy to add more 'processor' interfaces.

I thought about placing processors in context, but gave up on that due to problems with cyclic imports. Instead I put all processors into a single struct to avoid passing multiple processors around.",closed,True,2018-05-28 11:59:28,2018-05-29 11:58:42
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/901,https://api.github.com/repos/kubernetes/autoscaler/issues/901,Execute predicates only for similar pods.,,closed,True,2018-05-28 13:12:35,2018-05-29 09:33:15
autoscaler,GFilipek,https://github.com/kubernetes/autoscaler/pull/902,https://api.github.com/repos/kubernetes/autoscaler/issues/902,Sequential recommendation processor,RecommendationProcessor that uses provided RecommendationProcessors in sequence provided on construction,closed,True,2018-05-28 17:01:45,2018-05-28 17:27:18
autoscaler,wskinner,https://github.com/kubernetes/autoscaler/issues/903,https://api.github.com/repos/kubernetes/autoscaler/issues/903,CA does not scale up from zero nodes in group,"**What happened**:
Cluster Autoscaler will not scale up from zero nodes. However, it will scale up from one node.
I have a node group whose template includes p2.xlarge GPU instances. With zero running instances in my gpu-nodes node group, I create a new Job that requests 2 pods, each with 1 GPU. The pods are unschedulable, and CA logs show:
```I0524 15:30:32.066956       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""engine"", Name:""distributed-job-2xp8n"", UID:""34fa9255-5f67-11e8-bede-068abf0075c0"", APIVersion:""v1"", ResourceVersion:""98300"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)```
The pods never get created, and no GPU instances get spun up.

**What you expected to happen**:
CA should scale up the cluster by adding two p2.xlarge instances to the gpu-nodes group.

**How to reproduce it (as minimally and precisely as possible)**:
In a kops cluster on AWS:
1. Create a node group which includes p2.xlarge as the instance type.
2. Create a Job that requests 2 containers, each requesting a single nvidia.com/gpu.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:17:39Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.7"", GitCommit:""b30876a5539f09684ff9fde266fda10b37738c9c"", GitTreeState:""clean"", BuildDate:""2018-01-16T21:52:38Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration: AWS
- OS (e.g. from /etc/os-release): k8s-1.8-debian-jessie-amd64-hvm-ebs-2018-02-08
- Kernel (e.g. `uname -a`): Linux ip-172-20-40-189 4.4.115-k8s #1 SMP Thu Feb 8 15:37:40 UTC 2018 x86_64 GNU/Linux
- Install tools: kops 1.8.1
- Others: 
CA image: gcr.io/google_containers/cluster-autoscaler:v1.0.5
",closed,False,2018-05-29 02:19:28,2019-01-26 02:46:47
autoscaler,smarterclayton,https://github.com/kubernetes/autoscaler/pull/904,https://api.github.com/repos/kubernetes/autoscaler/issues/904,Autoscaler doesn't drain nodes that have terminal pods,"Terminal pods (Succeeded or Failed phase) are drained by kubectl drain,
and autoscaler should also drain them.

I hit this while testing a cluster that created lots of

1. directly created pods with restartPolicy=Never
2. using a custom controller that creates restartPolicy=Never pods as ""jobs""

In both cases, the autoscaler left those nodes up, even though all the pods on the node were terminated, with the ""is not replicated"" error. The autoscaler should not consider terminal pods as blockers for drain.",closed,True,2018-05-29 04:22:39,2018-05-29 17:37:05
autoscaler,smarterclayton,https://github.com/kubernetes/autoscaler/pull/905,https://api.github.com/repos/kubernetes/autoscaler/issues/905,Kubemark cloud provider doesn't compile on Mac,"Signature changed, linux code was changed.",closed,True,2018-05-29 04:38:21,2018-05-29 07:47:48
autoscaler,GFilipek,https://github.com/kubernetes/autoscaler/pull/906,https://api.github.com/repos/kubernetes/autoscaler/issues/906,Fix timestamp format in ClusterAutoscaler ConfigMap,Use explicit timestamp formatting in ClusterAutoscaler ConfigMap last update annotation.,closed,True,2018-05-29 07:50:21,2018-05-29 08:28:47
autoscaler,jevin36,https://github.com/kubernetes/autoscaler/issues/907,https://api.github.com/repos/kubernetes/autoscaler/issues/907,Are there any plans on an implementation for Rancher 2.0 for autoscaling,"Hi,

is are there any plans on implementing a cloud provider that is using the Rancher 2.0 API for scaling a cluster up or down?

Regards",closed,False,2018-05-29 08:05:38,2018-10-26 18:02:15
autoscaler,thomaschaaf,https://github.com/kubernetes/autoscaler/issues/908,https://api.github.com/repos/kubernetes/autoscaler/issues/908,Spare nodes to schedule pods during deployment,"The cluster autoscaler currently works very efficiently for us and it is always utilizing the machines to their full capacity. However this is currently slowing down our deployments if the cluster does not have enough spare capacity to allow scheduling of the pods during a deployment we have to wait 5 minutes for the nodes to come up to be able to schedule the new pods.

Is there an option to always have a spare node ready to allow pods to schedule faster?",closed,False,2018-05-29 09:38:09,2018-06-01 12:13:17
autoscaler,nightsoundars,https://github.com/kubernetes/autoscaler/issues/909,https://api.github.com/repos/kubernetes/autoscaler/issues/909,New logo design for the project,"Hello! I want to contribute on your project with making logo design. What do you think about it?

",closed,False,2018-05-29 09:39:58,2018-06-01 05:45:20
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/910,https://api.github.com/repos/kubernetes/autoscaler/issues/910,Fix using taints for GPUs in NAP.,,closed,True,2018-05-29 13:35:06,2018-05-29 18:05:33
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/911,https://api.github.com/repos/kubernetes/autoscaler/issues/911,Filter out Failed/Succeeded pods in all calls,See discussion in #904 ,closed,False,2018-05-29 15:19:32,2018-11-01 10:16:16
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/912,https://api.github.com/repos/kubernetes/autoscaler/issues/912,Add post scale up status processor.,"After a scale up, process the state. This is currently implemented as and eventing processor, that emits appropriate events for pods in different states. The processor is wrapped in a chaning processor for easy extensibility.",closed,True,2018-05-30 06:55:19,2018-06-07 09:05:36
autoscaler,mansoorcheema,https://github.com/kubernetes/autoscaler/issues/913,https://api.github.com/repos/kubernetes/autoscaler/issues/913,AKS - Autoscaler Pod fails to find nodepool scaleset,"**Kubernetes**  1.9.6
**Autoscaler**  1.1.2
**Cluster Type** AKS

node pool name extracted from label agentpool from command
 _**kubectl get nodes --show-labels**_

| NAME            |           STATUS  |   ROLES |    AGE  |     VERSION  |  LABELS |
| --------------  | ---------------- | -------- | -------- | ------------- | --------- |
| aks-nodepool1-40256392-0 |   Ready |    agent  |   7d  |      v1.9.6 |   agentpool=nodepool1 . . . |
| aks-nodepool1-40256392-1 |  Ready   |  agent    | 7d      |  v1.9.6    | agentpool=nodepool1 . . .   |

I can confirm that there isn't any scaleset of name nodepool1 visible either on portal nor via rest API but it seems that it is the nodepool used by aks as shown from kubectl get nodes command. Is nodepool same as scaleset?

**Autoscaler pod (cluster-autoscaler-5768479cdd-vxwc7) Logs** 
.
I0530 06:44:07.328200       1 cloud_provider_builder.go:128] Creating Azure Manager with default configuration.
I0530 06:44:07.328386       1 azure_manager.go:137] read configuration: 89e96a87-7615-4262-8f34-84f742ec7d40
I0530 06:44:07.328603       1 azure_manager.go:154] Created scale set client with authorizer . . .
0530 06:44:07.328767       1 azure_manager.go:162] Created scale set vm client with authorizer
.
 Failed to get scaleSet with name nodepool1: compute.VirtualMachineScaleSetsClient#Get: Failure responding to request: StatusCode=404 Failure responding to request: StatusCode=404 -- Original Error: autorest/azure: Service returned an error. Status=404 Code=""ResourceNotFound"" Message=""The Resource 'Microsoft.Compute/virtualMachineScaleSets/nodepool1' under resource group 'mansoorakscluster' was not found.""
.
.
I would like to mention that the guideline about using autoscaler on Azure [https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md](url) mentions Kubernetes 1.10.X but AKS does not support any thing beyond 1.9.6.",closed,False,2018-05-30 07:27:41,2018-07-10 14:04:29
autoscaler,danilina-wsib,https://github.com/kubernetes/autoscaler/issues/914,https://api.github.com/repos/kubernetes/autoscaler/issues/914,vertical-pod-autoscaler fails on DaemonSet,"I've created a VPA for sysdig (the monitoring tool) daemon set with ""Initial"" update policy. After recommendation was generated, I've restarted the pods, and the new pods fail to start with the following error:
```
  Warning  FailedCreate  1m (x2 over 1m)   daemonset-controller  Error creating: Internal error occurred: Internal error occurred: jsonpatch add operation does not apply: doc is missing path: /spec/containers/0/resources/requests/memory
  Warning  FailedCreate  1m (x13 over 1m)  daemonset-controller  Error creating: Internal error occurred: Internal error occurred: jsonpatch add operation does not apply: doc is missing path: /spec/containers/0/resources/requests/cpu
```",closed,False,2018-05-31 14:46:10,2018-05-31 14:48:53
autoscaler,rjkernick,https://github.com/kubernetes/autoscaler/pull/915,https://api.github.com/repos/kubernetes/autoscaler/issues/915,Add metrics server requirement to VPA Readme,,closed,True,2018-05-31 16:54:12,2018-05-31 20:04:37
autoscaler,mtougeron,https://github.com/kubernetes/autoscaler/pull/916,https://api.github.com/repos/kubernetes/autoscaler/issues/916,Update auto-discovery docs to match actual behavior,"Updating the AWS auto-discovery documentation to make the actual behavior

Modify the tag names to be naming convention consistent with other areas of the documentation

Updated examples to use a newer version of the autoscaler",closed,True,2018-05-31 23:32:18,2018-06-04 15:29:21
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/917,https://api.github.com/repos/kubernetes/autoscaler/issues/917,VPA recommender initialization moved to a separate package,,closed,True,2018-06-01 15:50:41,2018-06-04 09:39:04
autoscaler,trondhindenes,https://github.com/kubernetes/autoscaler/issues/918,https://api.github.com/repos/kubernetes/autoscaler/issues/918,[cluster-autoscaler]: parameter documentation,I can't find any documentation of all parameters - forcing me to download binary and running --help. It would be awesome to have a complete list of all parameters and their use.,closed,False,2018-06-02 21:05:02,2018-09-27 12:05:04
autoscaler,MirzaSikander,https://github.com/kubernetes/autoscaler/issues/919,https://api.github.com/repos/kubernetes/autoscaler/issues/919,panic: runtime error: invalid memory address or nil pointer dereference,"ACS-engine: `0.17.1`
Kubernetes version: `1.8.13`
CA version: `v1.0.5`
Using `standard` deployment. 

Error message:
```
I0604 01:29:40.800563       1 main.go:287] Cluster Autoscaler 1.0.5
I0604 01:29:40.997507       1 leaderelection.go:174] attempting to acquire leader lease...
I0604 01:29:41.005283       1 leaderelection.go:184] successfully acquired lease kube-system/cluster-autoscaler
I0604 01:29:41.096013       1 reflector.go:202] Starting reflector *v1.Pod (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.096058       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.096018       1 reflector.go:202] Starting reflector *v1.ReplicationController (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.096196       1 reflector.go:240] Listing and watching *v1.ReplicationController from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.096489       1 reflector.go:202] Starting reflector *v1beta1.ReplicaSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.096507       1 reflector.go:240] Listing and watching *v1beta1.ReplicaSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.096742       1 reflector.go:202] Starting reflector *v1.Node (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.096800       1 reflector.go:202] Starting reflector *v1beta1.StatefulSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.096820       1 reflector.go:240] Listing and watching *v1beta1.StatefulSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.096862       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.097023       1 predicates.go:123] Using predicate PodFitsResources
I0604 01:29:41.097040       1 predicates.go:123] Using predicate GeneralPredicates
I0604 01:29:41.097045       1 predicates.go:123] Using predicate PodToleratesNodeTaints
I0604 01:29:41.097050       1 predicates.go:123] Using predicate CheckNodeDiskPressure
I0604 01:29:41.097056       1 predicates.go:123] Using predicate MaxAzureDiskVolumeCount
I0604 01:29:41.097061       1 predicates.go:123] Using predicate NoVolumeNodeConflict
I0604 01:29:41.097072       1 predicates.go:123] Using predicate NoVolumeZoneConflict
I0604 01:29:41.097077       1 predicates.go:123] Using predicate MatchInterPodAffinity
I0604 01:29:41.097082       1 predicates.go:123] Using predicate MaxGCEPDVolumeCount
I0604 01:29:41.097087       1 predicates.go:123] Using predicate ready
I0604 01:29:41.097092       1 predicates.go:123] Using predicate CheckNodeCondition
I0604 01:29:41.097097       1 predicates.go:123] Using predicate CheckNodeMemoryPressure
I0604 01:29:41.097108       1 predicates.go:123] Using predicate MaxEBSVolumeCount
I0604 01:29:41.097113       1 predicates.go:123] Using predicate NoDiskConflict
I0604 01:29:41.097245       1 reflector.go:202] Starting reflector *v1beta1.DaemonSet (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I0604 01:29:41.097263       1 reflector.go:240] Listing and watching *v1beta1.DaemonSet from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I0604 01:29:41.097336       1 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149
I0604 01:29:41.097352       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149
I0604 01:29:41.097417       1 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I0604 01:29:41.097433       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I0604 01:29:41.097510       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212
I0604 01:29:41.097526       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212
I0604 01:29:41.097585       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I0604 01:29:41.097613       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I0604 01:29:41.097753       1 reflector.go:202] Starting reflector *v1beta1.PodDisruptionBudget (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0604 01:29:41.097790       1 reflector.go:240] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0604 01:29:41.098120       1 reflector.go:202] Starting reflector *v1.PersistentVolume (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.098168       1 reflector.go:240] Listing and watching *v1.PersistentVolume from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.098516       1 reflector.go:202] Starting reflector *v1.PersistentVolumeClaim (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.098556       1 reflector.go:240] Listing and watching *v1.PersistentVolumeClaim from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.098896       1 reflector.go:202] Starting reflector *v1.Service (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:41.098957       1 reflector.go:240] Listing and watching *v1.Service from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:73
I0604 01:29:42.200068       1 main.go:222] Registered cleanup signal handler
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x60 pc=0x2805cd6]
 goroutine 61 [running]:
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc420228770, 0xed2a68d10, 0xe0bf10df1, 0x51902a0, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:99 +0x256
main.run(0xc4207175e0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:263 +0x474
main.main.func2(0xc420aaa540)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:345 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:145 +0x97
```

Yaml used to deploy:
```
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""""]
  resources: [""events"",""endpoints""]
  verbs: [""create"", ""patch""]
- apiGroups: [""""]
  resources: [""pods/eviction""]
  verbs: [""create""]
- apiGroups: [""""]
  resources: [""pods/status""]
  verbs: [""update""]
- apiGroups: [""""]
  resources: [""endpoints""]
  resourceNames: [""cluster-autoscaler""]
  verbs: [""get"",""update""]
- apiGroups: [""""]
  resources: [""nodes""]
  verbs: [""watch"",""list"",""get"",""update""]
- apiGroups: [""""]
  resources: [""pods"",""services"",""replicationcontrollers"",""persistentvolumeclaims"",""persistentvolumes""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""extensions""]
  resources: [""replicasets"",""daemonsets""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""policy""]
  resources: [""poddisruptionbudgets""]
  verbs: [""watch"",""list""]
- apiGroups: [""apps""]
  resources: [""statefulsets""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""storage.k8s.io""]
  resources: [""storageclasses""]
  verbs: [""get"", ""list"", ""watch""]

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""""]
  resources: [""configmaps""]
  verbs: [""create""]
- apiGroups: [""""]
  resources: [""configmaps""]
  resourceNames: [""cluster-autoscaler-status""]
  verbs: [""delete"",""get"",""update""]

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: v1
data:
  ClientID: <redacted>
  ClientSecret: <redacted>
  ResourceGroup: <redacted>
  SubscriptionID: <redacted>
  TenantID: <redacted>
  Deployment: <redacted>=
  VMType: c3RhbmRhcmQ=
kind: Secret
metadata:
  name: cluster-autoscaler-azure
  namespace: kube-system
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      tolerations:
      - effect: NoSchedule
        operator: ""Equal""
        value: ""true""
        key: node-role.kubernetes.io/master
      nodeSelector:
        kubernetes.io/role: master
      containers:
      - command:
        - ./cluster-autoscaler
        - --v=3
        - --logtostderr=true
        - --cloud-provider=azure
        - --skip-nodes-with-local-storage=false
        - --nodes=1:10:agentpool1
        env:
        - name: ARM_SUBSCRIPTION_ID
          valueFrom:
            secretKeyRef:
              key: SubscriptionID
              name: cluster-autoscaler-azure
        - name: ARM_RESOURCE_GROUP
          valueFrom:
            secretKeyRef:
              key: ResourceGroup
              name: cluster-autoscaler-azure
        - name: ARM_TENANT_ID
          valueFrom:
            secretKeyRef:
              key: TenantID
              name: cluster-autoscaler-azure
        - name: ARM_CLIENT_ID
          valueFrom:
            secretKeyRef:
              key: ClientID
              name: cluster-autoscaler-azure
        - name: ARM_CLIENT_SECRET
          valueFrom:
            secretKeyRef:
              key: ClientSecret
              name: cluster-autoscaler-azure
        - name: ARM_VM_TYPE
          valueFrom:
            secretKeyRef:
              key: VMType
              name: cluster-autoscaler-azure
        - name: ARM_DEPLOYMENT
          valueFrom:
            secretKeyRef:
              key: Deployment
              name: cluster-autoscaler-azure
        image: gcr.io/google_containers/cluster-autoscaler:v1.0.5
        imagePullPolicy: Always
        name: cluster-autoscaler
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 300Mi
        volumeMounts:
        - mountPath: /etc/ssl/certs/ca-certificates.crt
          name: ssl-certs
          readOnly: true
        - mountPath: /var/lib/azure/
          name: deploy-parameters
          readOnly: true
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      volumes:
      - hostPath:
          path: /etc/ssl/certs/ca-certificates.crt
          type: """"
        name: ssl-certs
      - name: deploy-parameters
        secret:
          secretName: cluster-autoscaler-azure-deploy-parameters
          items:
          - key: deploy-parameters
            path: azuredeploy.parameters.json
```",closed,False,2018-06-04 01:32:33,2018-07-09 07:19:17
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/920,https://api.github.com/repos/kubernetes/autoscaler/issues/920,VPA add dependency that missed godeps update,,closed,True,2018-06-05 12:54:33,2018-06-05 13:08:22
autoscaler,sancyx,https://github.com/kubernetes/autoscaler/pull/921,https://api.github.com/repos/kubernetes/autoscaler/issues/921,#790 - aws - autoscaler failing with Wrong id,"#790 

I've experienced the same problem ""autoscaler failing with Wrong id: expected format aws:///<zone>/<name>""  described in #790. This small patched resolved it, as it turned out that some node might not have providerId sometimes, returning an error in this case will result GetNodeInfosForGroups returning an empty map, even if other nodes have correct providerId. ",closed,True,2018-06-05 17:02:50,2018-06-12 15:46:35
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/922,https://api.github.com/repos/kubernetes/autoscaler/issues/922,Update GPU lists & settings according to June 2018 GCE/GKE offering,"Updated the settings according to https://cloud.google.com/compute/docs/gpus/#gpus-list, with a simplification about CPU range for 4-P100 machines.

Reordered existing list to match ordering from the web page, just to make the comparison of code & web page easier.",closed,True,2018-06-05 20:48:07,2018-06-07 11:19:23
autoscaler,gtie,https://github.com/kubernetes/autoscaler/issues/923,https://api.github.com/repos/kubernetes/autoscaler/issues/923,Cluster Autoscaler erroneously triggered during PVC binding,"Scaling up is currently triggered by any uschedulable pod. A pod can be unschedulable for just a few seconds, however, while it's waiting for a volume to be bound. Those few seconds are enough for Cluster Autoscaler to kick off the creation of a new instance.

Here is how the sequence looks in the event stream:
```
$ k get events --field-selector involvedObject.uid=45b82c29-6970-11e8-a9c2-02145a4f4030 -o=custom-columns=CREATED:.metadata.creationTimestamp,SOURCE:.source.component,REASON:.reason,MSG:.message
CREATED                SOURCE                    REASON                   MSG
2018-06-06T09:59:16Z   default-scheduler         FailedScheduling         pod has unbound PersistentVolumeClaims (repeated 92 times)
2018-06-06T09:59:22Z   cluster-autoscaler        TriggeredScaleUp         pod triggered scale-up: [{1.default.nodes.ASG-XXXXXX 37->38 (max: 39)}]
2018-06-06T09:59:23Z   default-scheduler         Scheduled                Successfully assigned os-strg-osq6574-2-68d64976f6-4s5j2 to ip-10-117-5-157.XXXXXX
2018-06-06T09:59:23Z   kubelet                   SuccessfulMountVolume    MountVolume.SetUp succeeded for volume ""logs-volume"" 
2018-06-06T09:59:23Z   kubelet                   SuccessfulMountVolume    MountVolume.SetUp succeeded for volume ""default-token-tm7c7"" 
2018-06-06T09:59:23Z   attachdetach-controller   FailedAttachVolume       AttachVolume.Attach failed for volume ""pvc-45afef4a-6970-11e8-96b5-0a38a7e18608"" : ""Error attaching EBS volume \""vol-0a2eddf3918053d85\"""" to instance ""i-0c6399097e297ff80"" since volume is in ""creating"" state
2018-06-06T09:59:27Z   attachdetach-controller   SuccessfulAttachVolume   AttachVolume.Attach succeeded for volume ""pvc-45afef4a-6970-11e8-96b5-0a38a7e18608"" 
2018-06-06T09:59:32Z   kubelet                   SuccessfulMountVolume    MountVolume.SetUp succeeded for volume ""pvc-45afef4a-6970-11e8-96b5-0a38a7e18608"" 
2018-06-06T09:59:35Z   kubelet                   Pulling                  pulling image ""XXXXXX""
2018-06-06T09:59:50Z   kubelet                   Pulled                   Successfully pulled image ""XXXXXX""
2018-06-06T09:59:50Z   kubelet                   Created                  Created container
2018-06-06T09:59:50Z   kubelet                   Started                  Started container
2018-06-06T09:59:50Z   kubelet                   Pulling                  pulling image ""alpine:3.6""
2018-06-06T09:59:53Z   kubelet                   Pulled                   Successfully pulled image ""alpine:3.6""
2018-06-06T09:59:54Z   kubelet                   Created                  Created container
2018-06-06T09:59:54Z   kubelet                   Started                  Started container

```

In my particular case, where the above bug is combined with a high concentration of PodDisrupttionBudget=0 Pods and a significant amount of turnover, this bug often means that the new extra node is there to stay. The combination quickly leads to very low usage density and very high server costs.

The above problem is observable in Kubernetes clusters running version 1.9 and 1.10 in AWS. Cluster autoscaled in use: `v.1.2.2`",closed,False,2018-06-06 11:56:04,2019-02-14 10:54:18
autoscaler,komljen,https://github.com/kubernetes/autoscaler/issues/924,https://api.github.com/repos/kubernetes/autoscaler/issues/924,"CA reports kube-dns pod as non-mirrored, but there are 2 replicas running on different nodes","On my cluster deployed with kops CA won't scale down a node because it reports kube-dns pod as non mirrored:

```
I0606 12:02:35.033951       1 cluster.go:92] Fast evaluation: node ip-10-2-1-178.eu-west-1.compute.internal cannot be removed: non-daemonset, non-mirrored, non-pdb-assigned kube-system pod present: kube-dns-756bfc7fdf-sphxd
```

But there are 2 replicas of that pod:
```
kube-system   kube-dns-756bfc7fdf-8n2q8                                          3/3       Running   0          1h        100.106.124.10    ip-10-2-3-201.eu-west-1.compute.internal
kube-system   kube-dns-756bfc7fdf-sphxd                                          3/3       Running   0          1h        100.102.241.203   ip-10-2-1-178.eu-west-1.compute.internal
```

Versions:
 - kops 1.9.1
 - k8s 1.9.6
 - CA 1.1.2
",closed,False,2018-06-06 12:14:46,2018-06-07 13:33:01
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/925,https://api.github.com/repos/kubernetes/autoscaler/issues/925,VPA fix for handling empty resources section in container spec.,fixes #811 ,closed,True,2018-06-06 12:43:58,2018-06-07 06:32:24
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/926,https://api.github.com/repos/kubernetes/autoscaler/issues/926,[WIP] Gpu cluster wide resource limits (w/fixups),,closed,True,2018-06-06 20:43:56,2018-06-13 09:28:23
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/927,https://api.github.com/repos/kubernetes/autoscaler/issues/927,[WIP] Gpu cluster wide resource limits (squashed),,closed,True,2018-06-06 20:48:51,2018-06-08 11:39:40
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/928,https://api.github.com/repos/kubernetes/autoscaler/issues/928,VPA refactoring - simplify recommender initialization,,closed,True,2018-06-07 09:59:17,2018-06-07 11:18:37
autoscaler,alexnederlof,https://github.com/kubernetes/autoscaler/issues/929,https://api.github.com/repos/kubernetes/autoscaler/issues/929,Autoscaler doesn't recognize nvidia.com/gpu when scaling up from 0 to n nodes on AWS.,"I setup a GPU pool, and autoscaler works fine scaling up from 1 to n nodes, but not from 0 to n nodes. The error message is:

```
I0605 11:27:29.865576       1 scale_up.go:54] Pod default/simple-gpu-test-6f48d9555d-l9822 is unschedulable
I0605 11:27:29.961051       1 scale_up.go:86] Upcoming 0 nodes
I0605 11:27:30.005163       1 scale_up.go:146] Scale-up predicate failed: PodFitsResources predicate mismatch, cannot put default/simple-gpu-test-6f48d9555d-l9822 on template-node-for-gpus.ci.k8s.local-5829202798403814789, reason: Insufficient nvidia.com/gpu
I0605 11:27:30.005262       1 scale_up.go:175] No pod can fit to gpus.ci.k8s.local
I0605 11:27:30.005324       1 scale_up.go:180] No expansion options
I0605 11:27:30.005393       1 static_autoscaler.go:299] Calculating unneeded nodes
I0605 11:27:30.008919       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""simple-gpu-test-6f48d9555d-l9822"", UID:""3416d787-68b3-11e8-8e8f-0639a6e973b0"", APIVersion:""v1"", ResourceVersion:""12429157"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0605 11:27:30.031707       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
```

This is on Kubernetes 1.9.6 with autoscaler 1.1.2.

The nodes carry the label `kops.k8s.io/instancegroup=gpus`, which is also present in the autoscaler group on AWS:

```json
{
            ""ResourceType"": ""auto-scaling-group"",
            ""ResourceId"": ""gpus.ci.k8s.local"",
            ""PropagateAtLaunch"": true,
            ""Value"": ""gpus"",
            ""Key"": ""k8s.io/cluster-autoscaler/node-template/label/kops.k8s.io/instancegroup""
        },
```

If I start a node, I see it has the required capacity:

```
Capacity:
 cpu:             4
 memory:          62884036Ki
 nvidia.com/gpu:  1
 pods:            110
```

This is the simple deployment I use to test it:

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: simple-gpu-test
spec: 
  replicas: 1
  template:
    metadata:
      labels:
        app: ""simplegputest""
    spec:
      containers: 
      - name: ""nvidia-smi-gpu""
        image: ""nvidia/cuda:8.0-cudnn5-runtime""
        resources: 
          limits: 
             nvidia.com/gpu: 1 # requesting 1 GPU
        volumeMounts:
        - mountPath: /usr/local/nvidia
          name: nvidia
        command: [ ""/bin/bash"", ""-c"", ""--"" ]
        args: [ ""while true; do nvidia-smi; sleep 5; done;"" ]
      volumes:
      - hostPath:
          path: /usr/local/nvidia
        name: nvidia
```

Related to #321 where I reported it earlier ",closed,False,2018-06-07 11:19:20,2018-06-07 11:32:48
autoscaler,kawych,https://github.com/kubernetes/autoscaler/pull/930,https://api.github.com/repos/kubernetes/autoscaler/issues/930,Add config map to example Addon Resizer deployment,,closed,True,2018-06-07 11:45:39,2018-06-12 15:45:20
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/931,https://api.github.com/repos/kubernetes/autoscaler/issues/931,Add NoOpScaleUpStatusProcessor,"This will be useful for the purposes of testing.
Also fix naming of Process() parameter.",closed,True,2018-06-07 13:19:09,2018-06-07 17:11:34
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/932,https://api.github.com/repos/kubernetes/autoscaler/issues/932,VPA refactoring - make InitialContainersAggregateState source agnostic,,closed,True,2018-06-07 14:02:31,2018-06-07 14:26:39
autoscaler,eherot,https://github.com/kubernetes/autoscaler/issues/933,https://api.github.com/repos/kubernetes/autoscaler/issues/933,Consider making UnremovableNodeRecheckTimeout configurable,When trying to debug the autoscaler it's pretty annoying when it wants to sit on a decision for 5 minutes.,closed,False,2018-06-07 23:29:53,2018-06-20 15:49:24
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/934,https://api.github.com/repos/kubernetes/autoscaler/issues/934,Cleanup how multi-string flags are handled in main(),,closed,True,2018-06-08 11:36:25,2018-06-08 12:53:13
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/935,https://api.github.com/repos/kubernetes/autoscaler/issues/935,Do not return error when getting cpu/memory capacity of node,,closed,True,2018-06-08 12:23:28,2018-06-08 13:48:13
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/936,https://api.github.com/repos/kubernetes/autoscaler/issues/936,Azure cloud provider requires dependencies not provided in kubernetes/kubernetes or _override,"After godeps update, build fails with:
```
$ go test ./...
cloudprovider/azure/azure_client.go:26:2: cannot find package ""github.com/Azure/azure-sdk-for-go/arm/disk"" in any of:
	/usr/local/google/home/aleksandram/.gvm/gos/go1.7/src/github.com/Azure/azure-sdk-for-go/arm/disk (from $GOROOT)
	/usr/local/google/home/aleksandram/fresh/src/github.com/Azure/azure-sdk-for-go/arm/disk (from $GOPATH)
cloudprovider/azure/azure_client.go:27:2: cannot find package ""github.com/Azure/azure-sdk-for-go/arm/network"" in any of:
	/usr/local/google/home/aleksandram/.gvm/gos/go1.7/src/github.com/Azure/azure-sdk-for-go/arm/network (from $GOROOT)
	/usr/local/google/home/aleksandram/fresh/src/github.com/Azure/azure-sdk-for-go/arm/network (from $GOPATH)
cloudprovider/azure/azure_agent_pool.go:26:2: cannot find package ""github.com/Azure/azure-sdk-for-go/arm/resources/resources"" in any of:
	/usr/local/google/home/aleksandram/.gvm/gos/go1.7/src/github.com/Azure/azure-sdk-for-go/arm/resources/resources (from $GOROOT)
	/usr/local/google/home/aleksandram/fresh/src/github.com/Azure/azure-sdk-for-go/arm/resources/resources (from $GOPATH)
cloudprovider/azure/azure_client.go:29:2: cannot find package ""github.com/Azure/azure-sdk-for-go/arm/storage"" in any of:
	/usr/local/google/home/aleksandram/.gvm/gos/go1.7/src/github.com/Azure/azure-sdk-for-go/arm/storage (from $GOROOT)
	/usr/local/google/home/aleksandram/fresh/src/github.com/Azure/azure-sdk-for-go/arm/storage (from $GOPATH)
cloudprovider/cloud_provider.go:28:2: cannot find package ""k8s.io/kubernetes/pkg/scheduler/schedulercache"" in any of:
	/usr/local/google/home/aleksandram/.gvm/gos/go1.7/src/k8s.io/kubernetes/pkg/scheduler/schedulercache (from $GOROOT)
	/usr/local/google/home/aleksandram/fresh/src/k8s.io/kubernetes/pkg/scheduler/schedulercache (from $GOPATH)
```

It seems the dependency in question was removed from kubernetes/kubernetes repo ~month ago: https://github.com/kubernetes/kubernetes/commit/b1b930a39b5a602611023b47fea1063acfd5bc53#diff-681659ab9abb4b4883e78e8aaa980dba

@feiskyer can you please either update cloud provider code to no longer rely on this dependency, or add it to _override?",closed,False,2018-06-08 12:31:29,2018-06-11 11:48:27
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/937,https://api.github.com/repos/kubernetes/autoscaler/issues/937,Cluster Autoscaler 1.3.0-alpha.0,Update version to 1.3.0-alpha.0,closed,True,2018-06-08 13:10:29,2018-06-08 14:04:22
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/938,https://api.github.com/repos/kubernetes/autoscaler/issues/938,VPA expose ClusterState outside the Recommender.,,closed,True,2018-06-08 13:55:57,2018-06-08 15:20:13
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/939,https://api.github.com/repos/kubernetes/autoscaler/issues/939,Cluster Autoscaler 1.3.0-beta.0,Update Cluster Autoscaler version to 1.3.0-beta.0,closed,True,2018-06-08 14:01:26,2018-06-08 14:18:55
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/940,https://api.github.com/repos/kubernetes/autoscaler/issues/940,Fix context,Fix godeps.,closed,True,2018-06-08 14:46:21,2018-06-11 12:33:18
autoscaler,r-divakaran-hrs,https://github.com/kubernetes/autoscaler/issues/941,https://api.github.com/repos/kubernetes/autoscaler/issues/941,Autoscaler is not scaling down or up,"Hello,

I have installed auto scaler using helm, the command used is as below
```
helm install stable/cluster-autoscaler \
  --namespace kube-system \
  --name cluster-autoscaler \
  --set image.tag=v1.1.2 \
  --set awsRegion=$REGION \
  --set rbac.create=true \
  --set autoscalingGroups\[0\].name=nodes.$NAME.$DNS_ZONE \
  --set autoscalingGroups\[0\].minSize=$MIN_NODES \
  --set autoscalingGroups\[0\].maxSize=$MAX_NODES \
  --set podAnnotations.""iam\.amazonaws\.com/role""=arn:aws:iam::$ACCOUNT_NUMBER:role/masters.$DNS_ZONE \
  --set nodeSelector.""node-role\.kubernetes\.io/master""="""" \
  --set tolerations\[0\].effect=NoSchedule \
  --set tolerations\[0\].key=""node-role.kubernetes.io/master""
```

Kubernetes version
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-27T00:13:02Z"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-07T11:55:20Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
```

The cluster is having 15 nodes and one master. There are no much pods installed and I wanted to see if auto scaler will scale down. From the logs.
```
  1 I0609 09:33:13.875968       1 static_autoscaler.go:332] Starting scale down
  2 I0609 09:33:13.902816       1 scale_down.go:387] ip-4-5-6-7.region.compute.internal was unneeded for 6m48.344665061s
  3 I0609 09:33:13.902839       1 scale_down.go:387] 1.2.3.region.compute.internal was unneeded for 6m58.532568459s
  4 I0609 09:33:13.902849       1 scale_down.go:446] No candidates for scale down
  5 I0609 09:33:14.243737       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
  6 I0609 09:33:16.315108       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
  7 I0609 09:33:18.345279       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
  8 I0609 09:33:20.352730       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
  9 I0609 09:33:22.360454       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
 10 I0609 09:33:23.916035       1 static_autoscaler.go:108] Starting main loop
 11 I0609 09:33:24.023676       1 static_autoscaler.go:240] Filtering out schedulables
 12 I0609 09:33:24.024058       1 static_autoscaler.go:250] No schedulable pods
 13 I0609 09:33:24.024076       1 static_autoscaler.go:257] No unschedulable pods
 14 I0609 09:33:24.024085       1 static_autoscaler.go:299] Calculating unneeded nodes
 15 I.b.c.d:%s/example.ip0609 09:33:24.059105       1 utils.go:399] Skipping ip-6-7-8-9.region.compute.internal - no node group config
 16 I0609 09:33:24.059593       1 scale_down.go:175] Scale-down calculation: ignoring 12 nodes, that were unremovable in the last 5m0s
 17 I0609 09:33:24.059612       1 scale_down.go:207] Node ip-4-5-6-7.region.compute.internal - utilization 0.162500
 18 I0609 09:33:24.059625       1 scale_down.go:207] Node 1.2.3.region.compute.internal - utilization 0.216500
 19 I0609 09:33:24.059638       1 scale_down.go:207] Node example.ip.compute.internal - utilization 0.725000
 20 I0609 09:33:24.059644       1 scale_down.go:211] Node example.ip.compute.internal is not suitable for removal - utilization too big (0.725000)
 21 I0609 09:33:24.101380       1 cluster.go:78] Fast evaluation: 1.2.3.region.compute.internal for removal
 22 I0609 09:33:24.101519       1 cluster.go:200] Pod monitoring/prometheus-operator-5d564c684d-x8shd can be moved to ip-172-20-51-4.eu-west-1.compute.internal
 23 I0609 09:33:24.101724       1 cluster.go:200] Pod monitoring/kube-prometheus-exporter-kube-state-5cd969f745-td4z9 can be moved to ip-172-20-51-4.eu-west-1.compute.internal
 24 I0609 09:33:24.102028       1 cluster.go:109] Fast evaluation: node 1.2.3.region.compute.internal may be removed
 25 I0609 09:33:24.102193       1 static_autoscaler.go:314] 1.2.3.region.compute.internal is unneeded since 2018-06-09 09:26:15.199568305 +0000 UTC duration 7m8.716444732s
 26 I0609 09:33:24.102216       1 static_autoscaler.go:314] ip-4-5-6-7.region.compute.internal is unneeded since 2018-06-09 09:26:25.387471703 +0000 UTC duration 6m58.528541334s
 27 I0609 09:33:24.102226       1 static_autoscaler.go:329] Scale down status: unneededOnly=false lastScaleUpTime=2018-06-06 21:12:31.471420721 +0000 UTC lastScaleDownDeleteTime=    2018-06-06 21:12:31.471421196 +0000 UTC lastScaleDownFailTime=2018-06-06 21:12:31.471421535 +0000 UTC schedulablePodsPresent=false isDeleteInProgress=false
 28 I0609 09:33:24.102243       1 static_autoscaler.go:332] Starting scale down
 29 I0609 09:33:24.139374       1 scale_down.go:387] ip-4-5-6-7.region.compute.internal was unneeded for 6m58.528541334s
 30 I0609 09:33:24.139397       1 scale_down.go:387] 1.2.3.region.compute.internal was unneeded for 7m8.716444732s
 31 I0609 09:33:24.139408       1 scale_down.go:446] No candidates for scale down
 32 I0609 09:33:24.368897       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
```
Can some one give me a clue !? 

I have reduced number of pods running in kube-system as I read auto scaler will not scale down a node by default if there is a pod on it which is running in kube-system.

Thanks",closed,False,2018-06-09 09:43:46,2018-06-11 10:37:40
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/942,https://api.github.com/repos/kubernetes/autoscaler/issues/942,Upgrade kubernetes vendor to v1.11.0-beta.2,"This PR upgrades kubernetes vendor to v1.11.0-beta.2 and 

- Updates Azure clients to use new SDK
- Update schedulercache to new package ""k8s.io/kubernetes/pkg/scheduler/cache""
- Set enableEquivalenceClassCache for schedulerConfigFactory (newly added params)
- Fix unit testing

Fixes #936 

/assign @aleksandra-malinowska @mwielgus",closed,True,2018-06-11 07:20:26,2018-06-11 13:47:08
autoscaler,mohmagdy,https://github.com/kubernetes/autoscaler/issues/943,https://api.github.com/repos/kubernetes/autoscaler/issues/943,autoscaller donot work after changing min and max limits,"i have cluster autoscaler installed , initially my max cluster size was 2 and then i removed the autoscaler and reinstall it with higher cluster size of 10 , however the autoscaling donot work and i am getting below error in autoscaler pod logs

`static_autoscaler.go:245] Failed to scale up: failed to increase node group size: ValidationError: New SetDesiredCapacity value 3 is above max value 2 for the AutoScalingGroup.`


`kubectl version
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.11"", GitCommit:""b13f2fd682d56eab7a6a2b5a1cab1a3d2c8bdd55"", GitTreeState:""clean"", BuildDate:""2017-11-25T18:34:52Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.11"", GitCommit:""b13f2fd682d56eab7a6a2b5a1cab1a3d2c8bdd55"", GitTreeState:""clean"", BuildDate:""2017-11-25T17:51:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
`

and autosacler yaml file
` ` ` 
  serviceAccountName: cluster-autoscaler
      containers:
        - name: cluster-autoscaler
          image: gcr.io/google_containers/cluster-autoscaler:v0.6.0
          livenessProbe:
            httpGet:
              path: /health-check
              port: 8085
          readinessProbe:
            httpGet:
              path: /health-check
              port: 8085
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --cloud-provider=aws
            - --nodes=4:10:nodes.MYDOMAIN
          env:
            - name: AWS_REGION
              value: us-east-1
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
` ` ` 

",closed,False,2018-06-11 07:56:52,2018-06-11 10:56:55
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/issues/944,https://api.github.com/repos/kubernetes/autoscaler/issues/944,Migrate leader election to use Lease objects,"As commented [by @liggitt](https://github.com/kubernetes/kubernetes/pull/64503#discussion_r194060708) and [@wojtek-t](https://github.com/kubernetes/kubernetes/pull/64503#discussion_r194326600) in [PR 64503](https://github.com/kubernetes/kubernetes/pull/64503) we should at some point migrate leader election to use newer API, preferably Lease objects.

The new API is expected in Kubernetes 1.12.
",open,False,2018-06-11 08:50:35,2019-03-13 10:27:45
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/945,https://api.github.com/repos/kubernetes/autoscaler/issues/945,Resource limits refactor in scale_down,This is preparatory work before introducing gpu limits to the codebase,closed,True,2018-06-11 08:52:39,2018-06-11 12:18:37
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/946,https://api.github.com/repos/kubernetes/autoscaler/issues/946,Proper handling of VPA resource policy,,closed,True,2018-06-11 09:46:58,2018-06-12 01:13:56
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/947,https://api.github.com/repos/kubernetes/autoscaler/issues/947,Update Go version to 1.10.2,Update Go version to 1.10.2 (required by k8s.io/kubelet),closed,True,2018-06-11 12:20:27,2018-06-11 12:40:23
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/948,https://api.github.com/repos/kubernetes/autoscaler/issues/948,Extract isNodeBeingDeleted function,,closed,True,2018-06-11 12:21:27,2018-06-11 12:32:54
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/949,https://api.github.com/repos/kubernetes/autoscaler/issues/949,Cluster Autoscaler 1.3.0-beta.1,Cluster Autoscaler 1.3.0-beta.1,closed,True,2018-06-11 13:23:05,2018-06-11 13:43:26
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/950,https://api.github.com/repos/kubernetes/autoscaler/issues/950,Respect GPU limits in scale_down,,closed,True,2018-06-11 13:43:54,2018-06-13 12:48:34
autoscaler,bob-shih,https://github.com/kubernetes/autoscaler/issues/951,https://api.github.com/repos/kubernetes/autoscaler/issues/951,CA-1.0.3 AWS - unable to scale up from 0,"Issue summary:
Although CA could scale down nodes to 0 and can scale up from 1 node and above, however, it always failed to scale up from 0.
(Note: the node is tainted and deployment is via ""nodeSelector"".)

Versions used:
CA:         1.0.3
Kube:       1.8.5
Cloud:      AWS
EC2 type:   m5.4xlarge

AWS ASG tags applied:
k8s.io/cluster-autoscaler/node-template/label/kubernetes.io/role worker-general
k8s.io/cluster-autoscaler/node-template/taint/dedicated worker-general:NoSchedule

Below are two scenarios that describe the issue.  They are reproducible.

Scenario 1:
===========
1. ASG (0,0,0,6) (running/desired/min/max)
2. CA is running
3. ""helm install"" my chart ""bobchart"" with values.yaml:
nodeSelector:
  kubernetes.io/role: ""worker-general""
tolerations:
  - key: ""dedicated""
    operator: ""Equal""
    value: ""worker-general""
    effect: ""NoSchedule""
4. CA ""panic"" and in CrashLoopBackOff state
....
I0611 12:39:33.613810       1 scale_up.go:54] Pod default/bob-test-bobchart-574ddc49cf-xfwl7 is unschedulable
W0611 12:39:33.820311       1 aws_manager.go:225] Found multiple availability zones, using eu-west-1b
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x1451bcd]

goroutine 83 [running]:
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).buildNodeFromTemplate(0xc4214825a0, 0xc4206e8960, 0xc4213fbcc0, 0xc4213fbcc0, 0x0, 0x0)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:252 +0x48d
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*Asg).TemplateNodeInfo(0xc4206e8960, 0xc4215cf0e0, 0x7ffcef946ee6, 0x24)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider.go:355 +0x78
k8s.io/autoscaler/cluster-autoscaler/core.GetNodeInfosForGroups(0xc421444400, 0x5, 0x8, 0x50bc120, 0xc4206e8660, 0x50c64e0, 0xc4208b20d0, 0xc4210fc2e0, 0x4, 0x4, ...)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/utils.go:211 +0x65a
k8s.io/autoscaler/cluster-autoscaler/core.ScaleUp(0xc420dc2680, 0xc42000fd40, 0x1, 0x1, 0xc421444400, 0x5, 0x8, 0xc4210fc2e0, 0x4, 0x4, ...)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:57 +0x28e
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc42149ea80, 0xed2b06485, 0xe13dc3040, 0x518f0a0, 0x0, 0x0)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:253 +0x13ea
main.run(0xc42071e820)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:262 +0x46f
main.main.func2(0xc420a7bda0)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:344 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:145 +0x97
.....
5. delete my chart, CA is running again 

Scenario 2:
===========
1. ASG (1,1,0,6) (running/desired/min/max)          <--- changed ""desired"" to 1, so ""1"" instance started up running
2. CA is running
3. ""helm install"" my chart ""bobchart"" with values.yaml:
nodeSelector:
  kubernetes.io/role: ""worker-general""
tolerations:
  - key: ""dedicated""
    operator: ""Equal""
    value: ""worker-general""
    effect: ""NoSchedule""
4. CA continuous to run and pod is created on correct node - ok
5. Deployed additional 16 charts so that CA will need to scale up. - ok
6. CA successfully scaled up to two nodes - ok
7. Delete all the additional 16 pods, CA scaled down the ""older"" node - ok
8. ASG (1,1,0,6) - ok
9. Delete the first (and the only remaining) pod, CA scale down all node - ok
10. ASG (0,0,0,6) - ok
11. Tried to deploy 1 chart, CA failed to scale up node from ""0"" and ""panic"" - failed

In both scenarios, CA failed to scale up from 0.  Attached please find the output for ""pod"" and ""node"" from ""kubectl get"".

To be able to scale up from 0 is crucial for us as we run certain job on demand only and does not require a node that runs all the time.

Thanks in advance.

Bob
[bob-test-bobchart_pod.txt](https://github.com/kubernetes/autoscaler/files/2090211/bob-test-bobchart_pod.txt)
[worker-general_node.txt](https://github.com/kubernetes/autoscaler/files/2090212/worker-general_node.txt)

",closed,False,2018-06-11 13:53:55,2018-06-11 16:57:24
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/952,https://api.github.com/repos/kubernetes/autoscaler/issues/952,Fix bug resulting resource limits not being enforced in scale_down,,closed,True,2018-06-11 14:39:39,2018-06-11 14:52:17
autoscaler,tjliupeng,https://github.com/kubernetes/autoscaler/issues/953,https://api.github.com/repos/kubernetes/autoscaler/issues/953,Is it possible to run k8s autoscaler on our own cluster?,"Hi, guys,

According to the autoscaler document, it should be run on GCE, GKE, Azure or AWS cluster. I am just wondering whether it can run on our own k8s cluster in our private cloud.

Thanks!",closed,False,2018-06-12 01:47:54,2018-06-13 00:56:59
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/954,https://api.github.com/repos/kubernetes/autoscaler/issues/954,VPA cleanups,"1. Move all VPA sources under pkg/ for consistency.
2. Rename 'recommender/output' to 'recommender/checkpoint'.
3. Do not use deprecated gcloud docker calls.",closed,True,2018-06-12 09:51:27,2018-06-12 15:44:03
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/955,https://api.github.com/repos/kubernetes/autoscaler/issues/955,Restructure checking resource limits in scale_up.go,Preparatory work for before introducing GPU limits,closed,True,2018-06-12 09:52:57,2018-06-13 17:29:40
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/956,https://api.github.com/repos/kubernetes/autoscaler/issues/956,Create NodeGroupManager which is responsible for creating…,…/deleting node groups.,closed,True,2018-06-12 12:13:34,2018-06-14 15:25:33
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/957,https://api.github.com/repos/kubernetes/autoscaler/issues/957,Add http timeout for GKE/GCE communication,,closed,True,2018-06-12 12:33:45,2018-06-12 13:46:27
autoscaler,bnookala,https://github.com/kubernetes/autoscaler/issues/958,https://api.github.com/repos/kubernetes/autoscaler/issues/958,ACS-engine private cluster with jumpbox enabled prevents autoscaler deployment,"I've got a cluster deployed with acs-engine v.18.1. I tried adding the cluster autoscaler 1.2.2 for acs-engine to the project, and got as far as deploying, but found this error from the logs:

```
I0613 03:50:51.308133       1 azure_client.go:305] azure: using client_id+client_secret to retrieve access token
E0613 03:50:54.308498       1 azure_util.go:251] Found 2 resources with type Microsoft.Network/networkSecurityGroups in the template. There should only be 1
F0613 03:50:54.308519       1 cloud_provider_builder.go:162] Failed to create Azure Manager: failed to parse node group spec: Found 2 resources with type Microsoft.Network/networkSecurityGroups in the template. There should only be 1
```

From the azure arm template, there are indeed two network security groups - one for the cluster, and the other for the jumpbox we provisioned with the cluster. 

Removing the jumpbox profile from the cluster manifest resolves this issue: https://github.com/Azure/acs-engine/blob/master/docs/clusterdefinition.md#jumpboxprofile, but removes the ability for a cluster to remain ""private"".",closed,False,2018-06-13 04:49:29,2018-11-10 14:52:42
autoscaler,mirake,https://github.com/kubernetes/autoscaler/pull/959,https://api.github.com/repos/kubernetes/autoscaler/issues/959,Typo fix: automaticaly -> automatically,,closed,True,2018-06-13 07:56:15,2018-06-13 15:50:24
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/960,https://api.github.com/repos/kubernetes/autoscaler/issues/960,Move backoff mechanism to utils.,,closed,True,2018-06-13 08:08:46,2018-06-14 12:35:34
autoscaler,jepp2078,https://github.com/kubernetes/autoscaler/pull/961,https://api.github.com/repos/kubernetes/autoscaler/issues/961,Scale down works. Mocked rancher api calls.,,closed,True,2018-06-13 09:43:24,2018-06-13 09:47:51
autoscaler,mirake,https://github.com/kubernetes/autoscaler/pull/962,https://api.github.com/repos/kubernetes/autoscaler/issues/962,Typo fix: doens't -> doesn't,,closed,True,2018-06-13 10:41:50,2018-06-13 11:07:38
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/963,https://api.github.com/repos/kubernetes/autoscaler/issues/963,Remove unused deployment scripts,Remove unused deployment scripts.,closed,True,2018-06-13 15:49:50,2018-06-14 11:44:09
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/964,https://api.github.com/repos/kubernetes/autoscaler/issues/964,Take into consideration nodes from not autoscaled groups when enforcing resources limits,,closed,True,2018-06-13 17:36:53,2018-06-14 14:19:18
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/965,https://api.github.com/repos/kubernetes/autoscaler/issues/965,Configuring registry authentication in e2e script.,"This is necessary after migrating from deprecated ""gcloud docker"".
See: https://cloud.google.com/container-registry/docs/advanced-authentication.",closed,True,2018-06-14 10:24:11,2018-06-14 10:25:59
autoscaler,chrissound,https://github.com/kubernetes/autoscaler/issues/966,https://api.github.com/repos/kubernetes/autoscaler/issues/966,How do I gain access to the cluster autoscaler on GKE?,"I am looking to modify some of the auto scaling options, but this does not seem to be possible on GKE?

It's not clear where to run these 'flags' mentioned in the FAQ or even where these command line flags need to be executed on.

Similar issue is brought up here:
https://stackoverflow.com/questions/48963625/where-to-config-the-kubernetes-cluster-autoscaler-on-google-cloud

",closed,False,2018-06-14 11:27:18,2019-01-07 16:45:32
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/967,https://api.github.com/repos/kubernetes/autoscaler/issues/967,Respect GPU limits in scale_up,,closed,True,2018-06-14 13:47:58,2018-06-14 14:03:43
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/968,https://api.github.com/repos/kubernetes/autoscaler/issues/968,Changes to the VPA API based on the v2beta1 API review,"See the API review: https://github.com/kubernetes/kubernetes/pull/63797
",closed,True,2018-06-14 13:50:36,2018-06-14 15:29:31
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/969,https://api.github.com/repos/kubernetes/autoscaler/issues/969,Add test to check if nodes from not autoscaled groups are used in max-nodes limit,,closed,True,2018-06-14 14:19:45,2018-06-14 14:52:49
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/970,https://api.github.com/repos/kubernetes/autoscaler/issues/970,Proper cleanup for vpa.,"Add information on how to tear down vpa.
Add script deleting webhook on teardown.",closed,True,2018-06-14 14:43:54,2018-06-15 11:34:10
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/971,https://api.github.com/repos/kubernetes/autoscaler/issues/971,Update godeps,Update godeps.,closed,True,2018-06-14 16:19:45,2018-06-14 16:35:21
autoscaler,chrissound,https://github.com/kubernetes/autoscaler/issues/972,https://api.github.com/repos/kubernetes/autoscaler/issues/972,How can I access the logs on GKE?,"As I have no access to the autoscaler pod on GKE, is there any way I can view the logs?

I'm trying to diagnose why a cluster in not scaling down - even with the a pod disruption config set.",closed,False,2018-06-15 09:48:25,2019-01-27 13:20:45
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/973,https://api.github.com/repos/kubernetes/autoscaler/issues/973,Fast forward Cluster Autoscaler release branch 1.3 to master,Fast forward Cluster Autoscaler release branch 1.3 to master,closed,True,2018-06-15 10:25:42,2018-06-15 11:40:46
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/974,https://api.github.com/repos/kubernetes/autoscaler/issues/974,Add GPU-related scaled_up & scaled_down metrics,,closed,True,2018-06-15 11:07:14,2018-07-04 09:00:37
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/975,https://api.github.com/repos/kubernetes/autoscaler/issues/975,GKE godep update,Note that after this update we could finally do a cleanup in cloudprovider and remove most copy-pasted functions and dirty hacks we added in 1.1. After talking with @mwielgus we decided to do it in a separate PR later on to keep this one minimal (as much as possible in case of godeps...).,closed,True,2018-06-15 15:16:20,2018-06-15 16:10:04
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/976,https://api.github.com/repos/kubernetes/autoscaler/issues/976,Remove unused sliding window.,,closed,True,2018-06-15 15:41:36,2018-06-15 16:22:29
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/977,https://api.github.com/repos/kubernetes/autoscaler/issues/977,Cherry-pick #975 on CA release-1.3,,closed,True,2018-06-15 16:21:35,2018-06-15 16:40:47
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/978,https://api.github.com/repos/kubernetes/autoscaler/issues/978,Cluster Autoscaler 1.3.0 beta.2,Cluster Autoscaler 1.3.0 beta.2,closed,True,2018-06-15 16:52:25,2018-06-15 17:14:04
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/979,https://api.github.com/repos/kubernetes/autoscaler/issues/979,Cluster Autoscaler 1.3.0-beta.2,Cluster Autoscaler 1.3.0-beta.2,closed,True,2018-06-15 16:54:02,2018-06-15 17:14:04
autoscaler,nicdoye,https://github.com/kubernetes/autoscaler/pull/980,https://api.github.com/repos/kubernetes/autoscaler/issues/980,issues/933 Consider making UnremovableNodeRecheckTimeout configurable,"Hi this is my first PR against the main(ish) parts of k8s, and I am unsure even how to run tests.

I'm hoping to add the feature requested in https://github.com/kubernetes/autoscaler/issues/933",closed,True,2018-06-15 22:07:35,2018-06-19 10:47:37
autoscaler,mirake,https://github.com/kubernetes/autoscaler/pull/981,https://api.github.com/repos/kubernetes/autoscaler/issues/981,Typo fix: udpates -> updates,,closed,True,2018-06-16 14:47:47,2018-09-26 14:37:24
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/982,https://api.github.com/repos/kubernetes/autoscaler/issues/982,Fix e2e tests after making certain fields nullable in the API,,closed,True,2018-06-18 10:37:00,2018-06-18 12:41:57
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/983,https://api.github.com/repos/kubernetes/autoscaler/issues/983, Add checkpoint versioning,"1. Fill the VerticalPodAutoscalerCheckpointStatus.Version field.
2. Do not load checkpoint with incompatible version.",closed,True,2018-06-18 13:59:41,2018-06-18 16:18:26
autoscaler,zonca,https://github.com/kubernetes/autoscaler/issues/984,https://api.github.com/repos/kubernetes/autoscaler/issues/984,Generic API based cluster autoscaler,"I think it would be great to have a cluster autoscaler that instead of interfacing directly with a cloud provider it would call a generic API endpoint with details about the request, for example to request a node to be removed or details about what kind of node to be created.
This would allow developers to write custom solutions that can work on their cloud platform with their backend of choice.

Hopefully this would not be too difficult to implement for someone familiar with the project starting from one of the existing supported providers.
Unfortunately I don't have the expertise to contribute this myself, but I would be happy to provide testing and feedback.",closed,False,2018-06-18 18:02:44,2018-07-08 01:50:35
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/985,https://api.github.com/repos/kubernetes/autoscaler/issues/985,Improve histogram exponential buckets.,"Currently the bucket boundaries are calculated as: FirstBucketSize * ExpFactor^n.
For example for FirstBucketSize=100M and ExpFactor=1.05, the bucket boundaries will be:
0:  [0...100M)
1:  [100M...105M)
2:  [105M...110.25M)
etc..
Natural and more reasonable boundaries would instead be such that the second bucket is 5% larger than the first bucket, so:
0:  [0...100M)
1:  [100M...205M)
2:  [205M...315.25M)
etc.",closed,True,2018-06-19 09:10:47,2018-06-21 07:32:01
autoscaler,rdsubhas,https://github.com/kubernetes/autoscaler/issues/986,https://api.github.com/repos/kubernetes/autoscaler/issues/986,"GKE cluster-autoscaler stuck in ""Initializing"" state in Regional cluster","- `cluster-autoscaler` is found to be stuck in `Initializing` state in a GKE Regional cluster
- Nodes are not scaling up and stuck in Pending state

### Cluster info

```
Kubernetes master version: 1.10.2-gke.3
Node pool version: 1.10.2-gke.3
Node image: cos
Node zones:
europe-west1-d
europe-west1-c
europe-west1-b
```

### GKE-Autoscaler status

```
kubectl describe configmap cluster-autoscaler-status -n kube-system
Data
====
status:
----
Cluster-autoscaler status at 2018-06-19 09:17:36.680001758 +0000 UTC:
Initializing
Events:  <none>
```

### Other notes

- We run around 10 GKE clusters. Only this one is a Regional cluster. All others are regular/zonal, and do not have this issue
- We will try to upgrade to 1.10.4 to see if it initializes
- We will try to turn off autoscaling, turn on again, and see if it initializes
- Will keep the thread updated",closed,False,2018-06-19 09:21:54,2018-06-19 09:47:05
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/issues/987,https://api.github.com/repos/kubernetes/autoscaler/issues/987,Do not use gcloud docker to operate with docker from build scripts,"Build scripts (i.e. `make dev-release`) use `gcloud docker` command.

It will be not supported soonish.
We should migrate to `gcloud auth configure-docker` and then directly use `docker`",closed,False,2018-06-19 11:18:38,2018-12-07 15:26:33
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/988,https://api.github.com/repos/kubernetes/autoscaler/issues/988,Cluster Autoscaler 1.3.0,Update Cluster Autoscaler version to 1.3.0,closed,True,2018-06-19 11:24:23,2018-06-19 12:07:10
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/989,https://api.github.com/repos/kubernetes/autoscaler/issues/989,"Revert ""Add checkpoint versioning.""","This was added by mistake. The checkpoint versioning was implemented in
a layer below (in the aggregate_container_state).
This reverts commit 1a120bb803feb71d62237bf2c00447cfc257a26a.",closed,True,2018-06-19 13:32:45,2018-06-19 13:50:13
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/990,https://api.github.com/repos/kubernetes/autoscaler/issues/990,Add VPA binaries to .gitignore.,,closed,True,2018-06-19 13:59:02,2018-06-19 15:27:26
autoscaler,mindprince,https://github.com/kubernetes/autoscaler/pull/991,https://api.github.com/repos/kubernetes/autoscaler/issues/991,GCE VMs with 2x and 4x V100 GPUs can now be created.,"/cc @kgolab 
/assign @mwielgus",closed,True,2018-06-20 02:43:33,2018-06-20 10:14:34
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/992,https://api.github.com/repos/kubernetes/autoscaler/issues/992,Vertical Pod Autoscaler version 0.2.0,,closed,True,2018-06-20 09:15:53,2018-06-20 10:12:45
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/993,https://api.github.com/repos/kubernetes/autoscaler/issues/993,Update Cluster Autoscaler version to 1.3.1-alpha,Update Cluster Autoscaler version to 1.3.1-alpha,closed,True,2018-06-20 11:11:28,2018-06-20 15:46:36
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/994,https://api.github.com/repos/kubernetes/autoscaler/issues/994,GCE VMs with 2x and 4x V100 GPUs can now be created.,Cherry-pick of #991 ,closed,True,2018-06-20 11:14:33,2018-06-20 15:46:52
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/995,https://api.github.com/repos/kubernetes/autoscaler/issues/995,Add information about backwards compatibility to VPA documentation.,,closed,True,2018-06-20 13:01:59,2018-06-20 17:13:19
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/996,https://api.github.com/repos/kubernetes/autoscaler/issues/996,VPA Allow evicting pods that have crashlooping container,,closed,True,2018-06-20 15:25:12,2018-06-20 17:14:26
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/997,https://api.github.com/repos/kubernetes/autoscaler/issues/997,VPA README formatting fix,,closed,True,2018-06-20 16:03:47,2018-06-20 17:13:44
autoscaler,jaxxstorm,https://github.com/kubernetes/autoscaler/issues/998,https://api.github.com/repos/kubernetes/autoscaler/issues/998,cluster autoscaler is panicing with blank node taints added,"I added some node taints like so:

```
k8s.io/cluster-autoscaler/node-template/taint/app |   |  
k8s.io/cluster-autoscaler/node-template/taint/group |  |
```

This is essentially a blank taint tag, for nodes/ASGs that aren't tainted.

I would expect the cluster autoscaler to handle this, but it panics and dies:

```
panic: runtime error: index out of range

goroutine 87 [running]:
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.extractTaintsFromAsg(0xc4219234a0, 0xc, 0xc, 0xc42196a180, 0xc42196a150, 0xc42021eaa0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:314 +0x2e8
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).buildNodeFromTemplate(0xc4210b1200, 0xc420cda810, 0xc4215e7240, 0xc4215e7240, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:264 +0xb36
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*Asg).TemplateNodeInfo(0xc420cda810, 0xc4217a3c20, 0xc421125ad0, 0x22)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider.go:355 +0x78
k8s.io/autoscaler/cluster-autoscaler/core.GetNodeInfosForGroups(0xc4215e7d80, 0x7, 0x8, 0x50bc120, 0xc420cda7b0, 0x50c64e0, 0xc420700ea0, 0xc4217a5e80, 0x4, 0x4, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/utils.go:211 +0x65a
k8s.io/autoscaler/cluster-autoscaler/core.ScaleUp(0xc420e36b60, 0xc42179dbf0, 0x2, 0x2, 0xc4215e7d80, 0x7, 0x8, 0xc4217a5e80, 0x4, 0x4, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:57 +0x28e
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc421547f10, 0xed2bc7ed7, 0xe056a939a, 0x518f0a0, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:253 +0x13ea
k8s.io/autoscaler/cluster-autoscaler/core.(*PollingAutoscaler).RunOnce(0xc420d03300, 0xed2bc7ed7, 0xe056a939a, 0x518f0a0, 0x518f0a0, 0x4e200)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/polling_autoscaler.go:72 +0x146
main.run(0xc420a6c5a0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:262 +0x46f
main.main.func2(0xc4203509c0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:344 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:145 +0x97
```",closed,False,2018-06-20 17:05:54,2018-06-28 09:46:21
autoscaler,mtougeron,https://github.com/kubernetes/autoscaler/pull/999,https://api.github.com/repos/kubernetes/autoscaler/issues/999,update the aws ec2 instance types,Ran `make generate` to update the available ec2 instance types in aws.,closed,True,2018-06-20 21:00:21,2018-06-21 15:13:31
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1000,https://api.github.com/repos/kubernetes/autoscaler/issues/1000,Move VPA default tag to 0.2.0,,closed,True,2018-06-21 07:03:53,2018-06-21 08:58:49
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1001,https://api.github.com/repos/kubernetes/autoscaler/issues/1001,Add pod preprocessor to admission controller.,,closed,True,2018-06-21 10:42:13,2018-06-21 14:29:54
autoscaler,GFilipek,https://github.com/kubernetes/autoscaler/pull/1002,https://api.github.com/repos/kubernetes/autoscaler/issues/1002,Add PodEvictionAdmission interface,"PodEvicitonAdmission adds ability to controll pod evicion based on pod
recommendation.",closed,True,2018-06-21 11:07:55,2018-06-21 15:25:21
autoscaler,dd-caleb,https://github.com/kubernetes/autoscaler/issues/1003,https://api.github.com/repos/kubernetes/autoscaler/issues/1003,GCE Regional Instance Groups Supported by Cluster Autoscaler?,"We've been successfully using the cluster autoscaler in AWS for a couple months and have recently tried to port our work to GCE as well. Unfortunately we haven't been able to get the cluster autoscaler to discover our **regional** managed instance groups.

Are regional managed instance groups currently supported?

It's possible I've simply misconfigured something. Here's our deployment (names slightly altered):

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
      - command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --cloud-provider=gce
        - --node-group-auto-discovery=mig:namePrefix=staging-main-testcluster,min=0,max=10
        - --regional
        image: k8s.gcr.io/cluster-autoscaler:v1.2.2
        imagePullPolicy: Always
        name: cluster-autoscaler
        volumeMounts:
        - mountPath: /etc/ssl/certs/ca-certificates.crt
          name: ssl-certs
          readOnly: true
      serviceAccount: cluster-autoscaler
      serviceAccountName: cluster-autoscaler
      volumes:
      - hostPath:
          path: /etc/ssl/certs/ca-certificates.crt
          type: """"
        name: ssl-certs
```

The logs return:

```
I0621 10:50:49.338327       1 utils.go:404] Skipping staging-main-testcluster-nodepool-general-73p2 - no node group config
I0621 10:50:49.338331       1 utils.go:404] Skipping staging-main-testcluster-zookeeper-1-sxnx - no node group config
I0621 10:50:49.338335       1 utils.go:404] Skipping staging-main-testcluster-webapp-r84t - no node group config
...
```

And we have regional instance groups like:

```
staging-main-testcluster-nodepool-general
staging-main-testcluster-zookeeper-1-mig
staging-main-testcluster-webapp-mig
```

Looking at the code I see things like:

```go
// SetMigSize sets MIG size.
func (m *gceManagerImpl) SetMigSize(mig *Mig, size int64) error {
	glog.V(0).Infof(""Setting mig size %s to %d"", mig.Id(), size)
	op, err := m.gceService.InstanceGroupManagers.Resize(mig.Project, mig.Zone, mig.Name, size).Do()
	if err != nil {
		return err
	}
	return m.waitForOp(op, mig.Project, mig.Zone)
}
```

But I believe the Google API requires using the `RegionInstanceGroupManagers` field instead, which is why I'm wondering if this is supported yet. Apologies if this was documented somewhere and I simply missed it. 

Is support for this on the roadmap? If not we may take a stab at it if we can find the resources.",closed,False,2018-06-21 11:59:27,2018-07-02 13:57:42
autoscaler,josepplaltg,https://github.com/kubernetes/autoscaler/issues/1004,https://api.github.com/repos/kubernetes/autoscaler/issues/1004,EKS cluster-autoscaler 1.3.0 downscaling after reaching minimum limit,"We've detected the cluster-autoscaler 1.3.0 running on EKS cluster is not stopping the downscaling after reaching the mimimum asg size declared on the Deployment. 

`            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --nodes=4:10:eks-jenkins-workers-m5xlarge-NodeGroup-XXXXXXXXXX
            - --skip-nodes-with-system-pods=false
            - --scale-down-delay-after-add=5m
            - --scale-down-delay-after-delete=5m
            - --balance-similar-node-groups
`

We stopped the autoscaler at 2 instances in the ASG",closed,False,2018-06-21 16:05:31,2018-11-22 16:34:38
autoscaler,jaxxstorm,https://github.com/kubernetes/autoscaler/pull/1005,https://api.github.com/repos/kubernetes/autoscaler/issues/1005,Validate taint key with regex,"Before attempting to perform a split on the tag value, validate it
matches the format <tag>:NoSchedule

This should prevent invalid tag keys from causing the autoscaler to
panic and crash

Fixes #998 ",closed,True,2018-06-21 17:48:15,2018-06-28 09:46:21
autoscaler,vantuvt,https://github.com/kubernetes/autoscaler/pull/1006,https://api.github.com/repos/kubernetes/autoscaler/issues/1006,Fix NPEs bugs,"NPEs can be encountered when specifying max/min resource policies for
containers.",closed,True,2018-06-22 03:11:20,2018-06-22 06:26:31
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1007,https://api.github.com/repos/kubernetes/autoscaler/issues/1007,VPA split min resources across containers in a pod,,closed,True,2018-06-22 11:38:45,2018-06-22 22:30:31
autoscaler,krzysztof-jastrzebski,https://github.com/kubernetes/autoscaler/pull/1008,https://api.github.com/repos/kubernetes/autoscaler/issues/1008,Move removing unneeded autoprovisioned node groups to node group manager,,closed,True,2018-06-22 12:27:52,2018-06-22 19:01:36
autoscaler,kgrygiel,https://github.com/kubernetes/autoscaler/pull/1009,https://api.github.com/repos/kubernetes/autoscaler/issues/1009,Add a high-level comment about AggregateContainerStates.,,closed,True,2018-06-22 12:59:11,2018-06-22 13:39:10
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1010,https://api.github.com/repos/kubernetes/autoscaler/issues/1010,Use Warningf instead Warning,,closed,True,2018-06-22 13:11:30,2018-06-22 13:38:39
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1011,https://api.github.com/repos/kubernetes/autoscaler/issues/1011,Add gpus to supportedResources map in GCE cloud provider,,closed,True,2018-06-22 14:00:04,2018-06-22 15:16:14
autoscaler,vantuvt,https://github.com/kubernetes/autoscaler/pull/1012,https://api.github.com/repos/kubernetes/autoscaler/issues/1012,Fix construction of the pollPeriod duration,"The construction of the pollPeriod duration was done in a manner that
did not use the unit of the poll-period flag correctly.",closed,True,2018-06-22 19:59:53,2018-06-27 13:32:47
autoscaler,trondhindenes,https://github.com/kubernetes/autoscaler/issues/1013,https://api.github.com/repos/kubernetes/autoscaler/issues/1013,AWS Autoscaling: How to specify min/max nodes with autodiscovery?,"From what I can see in the docs, I can either specify a ""node group"" (which takes min/max nodes) or use autodiscovery tags. It's not clear from the documentation how to specify min/max nodes when autodiscovery is used. It would be awesome if that was clarified.",closed,False,2018-06-22 21:50:18,2018-07-04 17:27:43
autoscaler,trondhindenes,https://github.com/kubernetes/autoscaler/pull/1014,https://api.github.com/repos/kubernetes/autoscaler/issues/1014,Added output of --help to separate readme file to improve searchability,This PR adds a new page contianing the full `--help` output in order to improve searchability for possible parameters.,closed,True,2018-06-22 22:16:18,2018-07-16 22:57:55
autoscaler,maauso,https://github.com/kubernetes/autoscaler/issues/1015,https://api.github.com/repos/kubernetes/autoscaler/issues/1015, Failed to create AWS Manager: NoCredentialProviders,"Hi guys, 

I'm trying to use autoscaler in EKS, I found an error when I want to update autoscaler yaml, for example  adding a new ASG o something like that

Steps to reproduce it: 
1.- Create a EKS cluster
2.- Run cluster-autoscaler, version 1.2.2 or 1.3, it's the same
3.- Update cluster-autoscaler yaml adding new ASG

Then

```bash
I0623 11:37:46.710465       1 reflector.go:202] Starting reflector *v1beta1.PodDisruptionBudget (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0623 11:37:46.710473       1 reflector.go:240] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0623 11:37:47.407943       1 request.go:481] Throttling request took 696.787298ms, request: PUT:https://172.20.0.1:443/api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status
I0623 11:37:47.408128       1 request.go:481] Throttling request took 298.758142ms, request: GET:https://172.20.0.1:443/api/v1/nodes?limit=500&resourceVersion=0
I0623 11:37:48.009609       1 cloud_provider_builder.go:72] Building aws cloud provider.
I0623 11:37:48.108061       1 auto_scaling_groups.go:78] Registering ASG eks-dev-NodeGroup-1HW5XCKEG8OPJ
I0623 11:37:48.108083       1 auto_scaling_groups.go:139] Invalidating unowned instance cache
I0623 11:37:48.108093       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [eks-dev-NodeGroup-1HW5XCKEG8OPJ]
F0623 11:37:48.207999       1 cloud_provider_builder.go:137] Failed to create AWS Manager: NoCredentialProviders: no valid providers in chain. Deprecated.
        For verbose messaging see aws.Config.CredentialsChainVerboseErrors
```
At this moment I didn't find the way to solve. 

Take in consideration that :
1- EKS use amazon-vpc-cni
2.- I'm using kube2iam, but I'm not using it to cluster-autoscaler
3.- I'm using Amazon Linux 2 and I modified the cert path

```
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
          imagePullPolicy: ""Always""
      volumes:
        - name: ssl-certs
          hostPath:
            path: ""/etc/ssl/certs/ca-bundle.crt""
```

Any idea? If I can give you more info let me know

",closed,False,2018-06-23 11:49:36,2018-06-27 13:36:49
autoscaler,jbartosik,https://github.com/kubernetes/autoscaler/pull/1016,https://api.github.com/repos/kubernetes/autoscaler/issues/1016,Support labels in TestCloudProvider.,,closed,True,2018-06-25 16:48:32,2018-10-31 09:12:22
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1017,https://api.github.com/repos/kubernetes/autoscaler/issues/1017,Make GetGpuTypeForMetrics more robust,"As discussed I've made GetGpuTypeForMetrics more in line with GetNodeTargetGpus - but looking at the amount of plumbing needed I'm no longer fully convinced it's really worth the effort.

Updated metrics.md for a good measure.",closed,True,2018-06-26 14:49:02,2018-07-05 09:48:44
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1018,https://api.github.com/repos/kubernetes/autoscaler/issues/1018,VPA add flag switch for setting memory limit in admission-controller,,closed,True,2018-06-27 09:00:13,2018-06-27 13:19:15
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1019,https://api.github.com/repos/kubernetes/autoscaler/issues/1019,VPA make quick OOM threshold flag-configurable.,,closed,True,2018-06-27 09:51:36,2018-06-27 10:19:01
autoscaler,vantuvt,https://github.com/kubernetes/autoscaler/pull/1020,https://api.github.com/repos/kubernetes/autoscaler/issues/1020,Add comments and renamed a function for improved clarity,,closed,True,2018-06-27 14:15:52,2018-07-05 13:04:45
autoscaler,aermakov-zalando,https://github.com/kubernetes/autoscaler/pull/1021,https://api.github.com/repos/kubernetes/autoscaler/issues/1021,Scale up: ignore existing nodes,"Add a new flag, `--scale-up-cloud-provider-template`, that forces the autoscaler to always construct the template node from the cloud provider data (e.g. AWS launch configuration) instead of a random existing node. This fixes both the trivial issues where instance type change is not taken into account until the user gets lucky with node ordering and more complicated ones where pods using node affinities sometimes don't trigger scale up (see comments in #789).

I think that this should be the default behaviour, because the current one is non-intuitive and hard to debug. However, since the users might rely on existing logic and use node selectors without the corresponding tags on the ASGs, the flag defaults to `false`.

One more change I've made selects a random AZ when constructing a template node for a multi-AZ ASG in the AWS cloud provider. Previously, the autoscaler always used the first AZ, but since this was masked by the default logic of using a random existing node it *sort of* worked when existing nodes got added or removed. Using a random AZ allows the autoscaler to progress even in these non-ideal setups.",closed,True,2018-06-28 13:32:02,2019-03-13 16:21:45
autoscaler,jeruane,https://github.com/kubernetes/autoscaler/pull/1022,https://api.github.com/repos/kubernetes/autoscaler/issues/1022,fix link to AZ PR discussion,,closed,True,2018-06-28 19:58:05,2018-07-02 15:38:14
autoscaler,ofcourseican,https://github.com/kubernetes/autoscaler/issues/1023,https://api.github.com/repos/kubernetes/autoscaler/issues/1023,Automatic node scaling on AKS leads to broken RBAC?!,"I run a version 1.10.3 on AKS and I use this autoscaler image: gcr.io/google_containers/cluster-autoscaler:v1.3.0

The cluster was created with the following command:

`az aks create --resource-group aks-testcluster --admin-username ouradmin --location eastus --name ourtestcluster --node-count 1 --enable-rbac --node-vm-size Stand
ard_D2_v2 --aad-server-app-id xxxx --aad-client-app-id xxxx --aad-server-app-secret xxxx --aad-xxxx`

After that I upgraded to Kubernetes 1.10.3:
`az aks upgrade --resource-group aks-testcluster --name ourtestcluster --kubernetes-version 1.10.3`

This is my autoscaler YAML, without the needed azure secrets:
```
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""""]
  resources: [""events"",""endpoints""]
  verbs: [""create"", ""patch""]
- apiGroups: [""""]
  resources: [""pods/eviction""]
  verbs: [""create""]
- apiGroups: [""""]
  resources: [""pods/status""]
  verbs: [""update""]
- apiGroups: [""""]
  resources: [""endpoints""]
  resourceNames: [""cluster-autoscaler""]
  verbs: [""get"",""update""]
- apiGroups: [""""]
  resources: [""nodes""]
  verbs: [""watch"",""list"",""get"",""update""]
- apiGroups: [""""]
  resources: [""pods"",""services"",""replicationcontrollers"",""persistentvolumeclaims"",""persistentvolumes""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""extensions""]
  resources: [""replicasets"",""daemonsets""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""policy""]
  resources: [""poddisruptionbudgets""]
  verbs: [""watch"",""list""]
- apiGroups: [""apps""]
  resources: [""statefulsets""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""storage.k8s.io""]
  resources: [""storageclasses""]
  verbs: [""get"", ""list"", ""watch""]

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""""]
  resources: [""configmaps""]
  verbs: [""create""]
- apiGroups: [""""]
  resources: [""configmaps""]
  resourceNames: [""cluster-autoscaler-status""]
  verbs: [""delete"",""get"",""update""]

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - image: gcr.io/google_containers/cluster-autoscaler:v1.3.0
        imagePullPolicy: Always
        name: cluster-autoscaler
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 300Mi
        command:
        - ./cluster-autoscaler
        - --v=3
        - --logtostderr=true
        - --cloud-provider=azure
        - --skip-nodes-with-local-storage=false
        - --nodes=1:20:nodepool1
        env:
        - name: ARM_SUBSCRIPTION_ID
          valueFrom:
            secretKeyRef:
              key: SubscriptionID
              name: cluster-autoscaler-azure
        - name: ARM_RESOURCE_GROUP
          valueFrom:
            secretKeyRef:
              key: ResourceGroup
              name: cluster-autoscaler-azure
        - name: ARM_TENANT_ID
          valueFrom:
            secretKeyRef:
              key: TenantID
              name: cluster-autoscaler-azure
        - name: ARM_CLIENT_ID
          valueFrom:
            secretKeyRef:
              key: ClientID
              name: cluster-autoscaler-azure
        - name: ARM_CLIENT_SECRET
          valueFrom:
            secretKeyRef:
              key: ClientSecret
              name: cluster-autoscaler-azure
        - name: ARM_VM_TYPE
          valueFrom:
            secretKeyRef:
              key: VMType
              name: cluster-autoscaler-azure
        - name: AZURE_CLUSTER_NAME
          valueFrom:
            secretKeyRef:
              key: ClusterName
              name: cluster-autoscaler-azure
        - name: AZURE_NODE_RESOURCE_GROUP
          valueFrom:
            secretKeyRef:
              key: NodeResourceGroup
              name: cluster-autoscaler-azure
      restartPolicy: Always
```

Scaling up seems to work fine at first, but then I realized some very strange behaviours:

1. I'm not able to read any pod logs anymore:

```
k logs -l app=cluster-autoscaler --namespace kube-system          
error: You must be logged in to the server (the server has asked for the client to provide credentials ( pods/log cluster-autoscaler-5bd5c7d9b8-fczbw))
```
This worked fine before the first autoscaling.

2. When trying to manually scale the cluster through the azure portal or through the cli I get this error message:

> Operation failed with status: 'Bad Request'. Details: An error has occurred in subscription c5726056-7b48-45cd-8f1c-6519e57ad64d, resourceGroup: aks-buildserver request: RBAC must be enabled for AAD to be enabled.

Which is really strange because it worked before and the cluster was created with RBAC enabled.



",closed,False,2018-06-29 12:02:45,2018-07-10 14:06:32
autoscaler,rioriost,https://github.com/kubernetes/autoscaler/pull/1024,https://api.github.com/repos/kubernetes/autoscaler/issues/1024,Update README.md,"Found an error during deploying, but I couldn't decide whether it was a bug to replace a placeholder with version string automatically or just lack of documentation.",closed,True,2018-06-29 16:01:41,2018-07-10 16:17:19
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1025,https://api.github.com/repos/kubernetes/autoscaler/issues/1025,Handle nil recommendation in admission controller.,,closed,True,2018-07-02 08:07:17,2018-07-02 11:28:37
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1026,https://api.github.com/repos/kubernetes/autoscaler/issues/1026,Fix potential panic for aks,"Virtual machines not managed by AKS may have no tags. In such case, CA will panic.

Fixes #919 

Note: there's no such issue for 1.3 because its Azure client has been upgraded and `map[string]*string` is used there (instead of `*map[string]*string`).",closed,True,2018-07-02 08:27:22,2018-07-09 06:48:39
autoscaler,dmlemos,https://github.com/kubernetes/autoscaler/issues/1027,https://api.github.com/repos/kubernetes/autoscaler/issues/1027,[Azure] Error master DNS is already used by another public IP,"When the cluster-autoscaling kicks-in it fails with this error:

## Logs
```
Scale-up: setting group agent size to 10
I0702 10:51:21.076589       1 azure_agent_pool.go:213] Waiting for deploymentsClient.CreateOrUpdate(<redacted>, cluster-autoscaler-1643426482, {%!s(*resources.DeploymentProperties=&{0xc421376268 <nil> 0xc421376270 <nil> Incremental <nil>})})
W0702 10:51:21.076690       1 clusterstate.go:223] Disabling scale-up for node group agent until 2018-07-02 11:01:21.076684398 +0000 UTC m=+970.179937661
E0702 10:51:21.077203       1 static_autoscaler.go:283] Failed to scale up: failed to increase node group size: Long running operation terminated with status 'Failed': Code=""DeploymentFailed"" Message=""At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/arm-debug for usage details."" Details=[{""code"":""BadRequest"",""message"":""{\r\n  \""error\"": {\r\n    \""code\"": \""DnsRecordInUse\"",\r\n    \""message\"": \""DNS record <redacted>.westeurope.cloudapp.azure.com is already used by another public IP.\"",\r\n    \""details\"": []\r\n  }\r\n}""}]
I0702 10:51:26.229145       1 azure_manager.go:261] Refreshed ASG list, next refresh after 2018-07-02 10:52:26.229136361 +0000 UTC m=+435.332389624
```

## Azure error
```json
{
  ""error"": {
    ""code"": ""DnsRecordInUse"",
    ""message"": ""DNS record <redacted>.westeurope.cloudapp.azure.com is already used by another public IP."",
    ""details"": []
  }
}
```

## Versions tested
Autoscaler `1.3.0` `1.2.2`
Kubernetes `1.9.7`

Deployed standard with `acs-engine`.",closed,False,2018-07-02 10:50:47,2018-12-30 15:26:34
autoscaler,DirectXMan12,https://github.com/kubernetes/autoscaler/pull/1028,https://api.github.com/repos/kubernetes/autoscaler/issues/1028,Remove the meeting information from the README,"This removes the meeting information from the README, in favor of
pointing at the community repo.  Among other things, pointing at the
community repo makes it easier to update meeting information in one
place, instead of multiple.",closed,True,2018-07-02 14:52:22,2018-07-02 15:05:17
autoscaler,sakthi-vetrivel,https://github.com/kubernetes/autoscaler/issues/1029,https://api.github.com/repos/kubernetes/autoscaler/issues/1029,[Azure AKS] not logging scale-up events in autoscaler ,"@kkmsft 
 I'm using the cluster-autoscaler for a one-node AKS cluster (rbac-enabled, k8's 1.10.3) and I noticed that while my cluster did scale up to three nodes, the cluster-autoscaler did not log any ScaleUp events. However, when it scaled back down to two nodes, it did log a ScaleDown event.
![capture](https://user-images.githubusercontent.com/17627350/42187850-702e37a2-7e06-11e8-9c0f-7396df2e0b5a.PNG)
![capture2](https://user-images.githubusercontent.com/17627350/42187851-704ce666-7e06-11e8-8afb-34f46ff83c4f.PNG)
![capture3](https://user-images.githubusercontent.com/17627350/42187853-708be276-7e06-11e8-8c96-35a96bd5743c.PNG)
",closed,False,2018-07-02 21:45:00,2019-03-06 12:24:34
autoscaler,tongda,https://github.com/kubernetes/autoscaler/issues/1030,https://api.github.com/repos/kubernetes/autoscaler/issues/1030,Turning down reschedule for Job pods,"We are running a lot of Kubernetes BatchJobs every day and hope CA could help to scale up the cluster when jobs are submitted and scale down the cluster when jobs are finished. But I found that in some cases, the pods of a job will be killed and started on another node during scaling up/down. I guess that CA will try to optimize the pods so that it could use a minimum size of the cluster.

But for BatchJob pods, CA should not stop them. Since BatchJob are designed to finish, so CA should wait for those BatchJob finishing and then start scaling up/down.",closed,False,2018-07-03 01:59:59,2018-07-08 10:23:50
autoscaler,vantuvt,https://github.com/kubernetes/autoscaler/pull/1031,https://api.github.com/repos/kubernetes/autoscaler/issues/1031,Fix the parsing of resource values,"When parsing a resource value such as ""0.5m"" the unit information was
being extracted as ""00u"" instead of ""u"".",closed,True,2018-07-03 20:42:46,2018-07-05 13:07:27
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1032,https://api.github.com/repos/kubernetes/autoscaler/issues/1032,Cherry-pick of Move removing unneeded autoprovisioned node groups to node group manager #1008,Cherry-pick of #1008,closed,True,2018-07-04 11:23:16,2018-07-04 13:13:35
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1033,https://api.github.com/repos/kubernetes/autoscaler/issues/1033,Cherry-pick of Validate taint key with regex #1005,Cherry-pick of #1005 ,closed,True,2018-07-04 11:24:30,2018-07-04 13:14:29
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1034,https://api.github.com/repos/kubernetes/autoscaler/issues/1034,Cherry-pick of update the aws ec2 instance types #999,Cherry-pick of #999,closed,True,2018-07-04 11:27:23,2018-07-04 13:14:52
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1035,https://api.github.com/repos/kubernetes/autoscaler/issues/1035,Cherry-pick of Use Warningf instead Warning #1010,Cherry-pick of #1010,closed,True,2018-07-04 11:29:02,2018-07-04 13:15:43
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1036,https://api.github.com/repos/kubernetes/autoscaler/issues/1036,Cherry-pick of Add gpus to supportedResources map in GCE cloud provider #1011,Cherry-pick of #1011,closed,True,2018-07-04 11:32:05,2018-07-04 13:16:23
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1037,https://api.github.com/repos/kubernetes/autoscaler/issues/1037,Lower default expendable pod priority cutoff to -10,"Lower default expendable pod priority cutoff to -10. 

With pod priority and preemption enabled, users may want to create pause pods for overprovisioning with priority lower than default (0 for pods with unspecified priority), but high enough that they still trigger scale-up. 

```release-note:
Default value for expendable pod priority cutoff changed from 0 to -10.

action required: users deploying workloads with priority lower than 0 may want to use priority lower than -10 to avoid triggering scale-up.
```",closed,True,2018-07-04 11:55:40,2018-07-04 13:22:27
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1038,https://api.github.com/repos/kubernetes/autoscaler/issues/1038,Cluster Autoscaler 1.3.1-beta.1,Update Cluster Autoscaler version to 1.3.1-beta.1,closed,True,2018-07-04 13:05:55,2018-07-04 13:24:06
autoscaler,wojtek-t,https://github.com/kubernetes/autoscaler/pull/1039,https://api.github.com/repos/kubernetes/autoscaler/issues/1039,Use protobufs in pod-nanny,/assign @piosz @mwielgus ,closed,True,2018-07-04 13:24:07,2018-07-04 14:16:52
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1040,https://api.github.com/repos/kubernetes/autoscaler/issues/1040,Cherry-pick of Add GPU-related scaled_up & scaled_down metrics #974,Cherry-pick of #974,closed,True,2018-07-04 13:26:10,2018-07-05 09:52:12
autoscaler,wojtek-t,https://github.com/kubernetes/autoscaler/pull/1041,https://api.github.com/repos/kubernetes/autoscaler/issues/1041,"Cherrypick: ""Use protobufs in pod-nanny"" to 1.8 branch of addon-resizer",/assign @piosz @x13n @MaciekPytel ,closed,True,2018-07-04 14:16:43,2018-07-04 14:41:54
autoscaler,mf-lit,https://github.com/kubernetes/autoscaler/pull/1042,https://api.github.com/repos/kubernetes/autoscaler/issues/1042,Add a note regarding scaling values when using auto-discovery,As per information I found here https://github.com/kubernetes/autoscaler/issues/1013#issuecomment-401399643,closed,True,2018-07-04 15:47:50,2018-07-04 17:27:18
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1043,https://api.github.com/repos/kubernetes/autoscaler/issues/1043,Refactor and extend GPU metrics error types,Refactor and extend GPU metrics error types to better track missing GPUs.,closed,True,2018-07-05 10:57:28,2018-07-05 11:46:01
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1044,https://api.github.com/repos/kubernetes/autoscaler/issues/1044,VPA treat pod evictions due to low memory at node as OOMs.,Signed-off-by: Slawomir Chylek <schylek@google.com>,closed,True,2018-07-05 11:20:41,2018-07-05 12:37:55
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1045,https://api.github.com/repos/kubernetes/autoscaler/issues/1045,Cherry pick of GPU metrics fixes in #1017 and #1043,Cherry pick of #1017 and #1043,closed,True,2018-07-05 11:48:37,2018-07-05 12:06:22
autoscaler,wojtek-t,https://github.com/kubernetes/autoscaler/pull/1046,https://api.github.com/repos/kubernetes/autoscaler/issues/1046,Build 1.8.2 image od addon resizer,/assign @x13n ,closed,True,2018-07-05 12:33:19,2018-07-05 12:54:43
autoscaler,vantuvt,https://github.com/kubernetes/autoscaler/pull/1047,https://api.github.com/repos/kubernetes/autoscaler/issues/1047,Add support for custom min cluster sizes,"Add a command line flag allowing the use of an ExponentialEstimator with
a custom minimum cluster size. The default minimum cluster size is 16 to
retain compatibility with existing behaviour. Also tweak some unit test
error logging to make it easier to diagnose failures.",closed,True,2018-07-05 18:29:23,2018-07-12 14:41:01
autoscaler,pauloancheta,https://github.com/kubernetes/autoscaler/pull/1048,https://api.github.com/repos/kubernetes/autoscaler/issues/1048,fix spelling and indentation,Small fix for spelling and indentation,closed,True,2018-07-05 21:08:01,2018-07-10 16:16:51
autoscaler,gustavoatt,https://github.com/kubernetes/autoscaler/issues/1049,https://api.github.com/repos/kubernetes/autoscaler/issues/1049,Autoscaler fails to scale up nodes with pending pods,"We are currently running a cluster-autoscaler in AWS, on a kops cluster. We had autoscaling enabled for an ASG where each node would have a taint and a label applied. We would then only schedule pods onto it that had a toleration for that taint.

Our setup was working correctly, until a few days ago, when scale up stopped happening (scale downs were working correctly). Most of the logs we saw were like this:

```
16:25:53.494 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.493718       1 utils.go:130] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-26tbkn marked as unschedulable can be scheduled on ip-10-79-150-148.ec2.internal. Ignoring in scale up.
16:25:53.494 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.494043       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-24mkp5 marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.494 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.494225       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2xqhmb marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.494 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.494391       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2lb9tj marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.494 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.494558       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2grt2g marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.494 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.494747       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2pkg4w marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.494 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.494908       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2l5dk8 marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.495 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.495072       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-22wjf9 marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.495 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.495244       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2kv4xp marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.495 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.495404       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-25q2bq marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.495 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.495565       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2nqk9j marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.495 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.495733       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-26v2sv marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.496 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.495904       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2dd65s marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.496 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.496062       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2252x4 marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.496 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.496226       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2znsb4 marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.496 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.496390       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2d6xbs marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.496 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.496553       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-24b289 marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.496 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.496723       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-29b6sc marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.497 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.496923       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2j5bkz marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.497 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.497095       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-28rfln marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.497 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.497295       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2pbfxx marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.497 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.497470       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2g46tk marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.497 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.497668       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-22fg6q marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.590 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.590031       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2xcdsf marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.690 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.689812       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2zqwmh marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.690 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.690170       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2dbksl marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.690 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.690435       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2z9lv9 marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.690 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.690673       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-265h6w marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.691 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.690949       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2l8cvk marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
16:25:53.691 kops-k8s-cluster.services.opendoor.com I0703 23:25:53.691222       1 utils.go:125] Pod spark-worker-test-pyspark-notebook-jupyter-signals-spark-2l2klb marked as unschedulable can be scheduled (based on simulation run for other pod owned by the same controller). Ignoring in scale up.
```

It is unclear why autoscaler thinks these pods are schedulable, since when we disabled autoscaling and manually increase node count those pods were scheduled successfully.

We are running kubernetes 1.9.9 and the autoscaler with the following flags:

```shell
./cluster-autoscaler --cloud-provider=aws --namespace=kube-system --nodes=1:50:honeycomb-worker.kops-k8s-cluster.services.opendoor.com --v=4 --alsologtostderr=true
```

It is running using this docker image `k8s.gcr.io/cluster-autoscaler:v1.1.2`. Any help in how to fix this would be greatly appreciated since we would love to re-enable cluster autoscaler.

## Context

We are using kubernetes and cluster autoscaler to schedule periodically our cronjobs through the k8s API. This means that we create and delete hundreds of pods during ~30min period. Some of these pods are Spark pods, which might require large amounts of memory (~30Gb plus a couple of cores).",closed,False,2018-07-05 21:45:34,2019-04-02 11:50:26
autoscaler,nikhilkhanna,https://github.com/kubernetes/autoscaler/issues/1050,https://api.github.com/repos/kubernetes/autoscaler/issues/1050,Generic API based cluster autoscaler ,"Despite being brought up in other issues #984  and #191 including with example implementations, I haven't seen this issue properly addressed. It would be awesome to have a cluster autoscaler that instead of interfacing directly with a cloud provider it would call a generic API endpoint with details about the request, for example to request a node to be removed or details about what kind of node to be created.
",closed,False,2018-07-08 01:53:57,2019-01-14 02:30:28
autoscaler,tkubica12,https://github.com/kubernetes/autoscaler/issues/1051,https://api.github.com/repos/kubernetes/autoscaler/issues/1051,"[Azure AKS] Scaling fails with ""Changing property 'networkProfile.networkPlugin' is not allowed when using Advanced Networking","v1.2.2 fails with AKS when configured with Advanced Networking:

```
E0708 09:51:50.493997       1 azure_container_service_pool.go:262] containerservice.ManagedClustersClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code=""PropertyChangeNotAllowed"" Message=""Changing property 'networkProfile.networkPlugin' is not allowed.""
W0708 09:51:50.494266       1 clusterstate.go:249] Disabling scale-up for node group nodepool1 until 2018-07-08 09:56:50.494249582 +0000 UTC
E0708 09:51:50.506118       1 static_autoscaler.go:304] Failed to scale up: failed to increase node group size: containerservice.ManagedClustersClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code=""PropertyChangeNotAllowed"" Message=""Changing property 'networkProfile.networkPlugin' is not allowed.""
```

I rembember this was issue with old AZ CLI version as well so I assume there is change in API call needed for latest AKS clusters.

",closed,False,2018-07-08 10:44:37,2018-07-30 13:25:34
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1052,https://api.github.com/repos/kubernetes/autoscaler/issues/1052,Add more events for CA,"CA current only events after scale-up or before scale-down.

This PR add a event before scale-up and event after scale-down, so that we could track the scaling status from events.",closed,True,2018-07-09 07:44:46,2018-07-11 03:30:09
autoscaler,sheldonkwok,https://github.com/kubernetes/autoscaler/pull/1053,https://api.github.com/repos/kubernetes/autoscaler/issues/1053,Feature/add aws launch template asg support,"I'm currently creating my AWS Auto Scaling Groups with Launch Templates instead of Launch Configurations. Launch Templates are extensible to launching single EC2 instances and Spot Fleets as well. This PR covers fetching the instance type with Launch Templates since it currently only checks for Launch Configurations.

I tested by fetching instance types on ASGs created with Launch Templates and Launch Configs. ",closed,True,2018-07-09 18:11:43,2018-07-17 11:24:56
autoscaler,vainu-arto,https://github.com/kubernetes/autoscaler/pull/1054,https://api.github.com/repos/kubernetes/autoscaler/issues/1054,Add annotation to mark pods as unsafe to evict,"Allow using the cluster-autoscaler.kubernetes.io/safe-to-evict annotation also to mark the pod as not safe to evict, this fixes issue #510.",closed,True,2018-07-11 06:37:08,2018-11-05 20:36:55
autoscaler,vainu-arto,https://github.com/kubernetes/autoscaler/pull/1055,https://api.github.com/repos/kubernetes/autoscaler/issues/1055,Cherry pick pod unsafe annotations for 1.1,Cherry pick PR #1054 for release 1.1.,closed,True,2018-07-12 06:36:34,2018-07-12 09:47:08
autoscaler,vainu-arto,https://github.com/kubernetes/autoscaler/pull/1056,https://api.github.com/repos/kubernetes/autoscaler/issues/1056,Cherry pick pod unsafe annotations for 1.2,Cherry pick PR #1054 for release 1.2.,closed,True,2018-07-12 06:37:22,2018-07-12 09:47:33
autoscaler,vainu-arto,https://github.com/kubernetes/autoscaler/pull/1057,https://api.github.com/repos/kubernetes/autoscaler/issues/1057,Cherry pick pod unsafe annotations for 1.3,Cherry pick PR #1054 for release 1.3.,closed,True,2018-07-12 06:37:56,2018-07-12 06:40:52
autoscaler,vainu-arto,https://github.com/kubernetes/autoscaler/pull/1058,https://api.github.com/repos/kubernetes/autoscaler/issues/1058,Cherry pick pod unsafe annotations for 1.3,Cherry pick PR #1054 for release 1.3.,closed,True,2018-07-12 06:41:48,2018-07-12 13:03:43
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1059,https://api.github.com/repos/kubernetes/autoscaler/issues/1059,Cluster Autoscaler 1.3.1,Update Cluster Autoscaler version to 1.3.1,closed,True,2018-07-12 09:43:30,2018-07-12 09:55:57
autoscaler,MasterScrat,https://github.com/kubernetes/autoscaler/issues/1060,https://api.github.com/repos/kubernetes/autoscaler/issues/1060,Autoscaling bare metal clusters,"I am running my own bare metal cluster. I would like to autoscale it by turning physical nodes on and off.

I imagine this would be doable using IPMI.",closed,False,2018-07-12 10:02:13,2018-12-09 13:11:11
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1061,https://api.github.com/repos/kubernetes/autoscaler/issues/1061,Cluster Autoscaler 1.3.2-alpha.0,Update Cluster Autoscaler version to 1.3.2-alpha.0,closed,True,2018-07-12 10:59:16,2018-07-12 11:37:00
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1062,https://api.github.com/repos/kubernetes/autoscaler/issues/1062,VPA Garbage collect old AggregateContainerStates,,closed,True,2018-07-12 12:55:30,2018-07-12 14:16:53
autoscaler,guitmz,https://github.com/kubernetes/autoscaler/issues/1063,https://api.github.com/repos/kubernetes/autoscaler/issues/1063,[Azure AKS] Can't scale-up nodes,"I have deployed cluster-autoscaler (v1.3.1) in AKS (with k8s 1.10.3) yesterday and to test, I created a pod with a large CPU/RAM request. I was expecting a new node to be created but that didn't happened and all I can see is the output below:

```
I0713 10:34:43.451225       1 azure_manager.go:261] Refreshed ASG list, next refresh after 2018-07-13 10:35:43.451205157 +0000 UTC m=+577.450491985
I0713 10:34:43.706164       1 utils.go:503] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0713 10:34:43.706875       1 scale_up.go:249] Pod default/frontend is unschedulable
I0713 10:34:43.926322       1 scale_up.go:376] No expansion options
```

I manually scaled up my cluster with 1 extra node and the pod was scheduled and it is running fine. Interestingly, I still see the same output as above in cluster-autoscaler log even after the pod is  running. In every ASG refresh, cluster-autoscaler still repeats the same log output. 

Before scaling up the nodes manually, the following message was present in CA logs (it is not anymore):

`Node group agentpool is not ready for scaleup`

Am I missing something here? I didn't changed any default options in CA other than the required values.

Thanks!",closed,False,2018-07-13 10:38:44,2018-11-20 07:36:27
autoscaler,sc250024,https://github.com/kubernetes/autoscaler/issues/1064,https://api.github.com/repos/kubernetes/autoscaler/issues/1064,[AWS Kops] Cluster autoscaler timeout when trying to run on master node,"Greetings,

I'm running `cluster-autoscaler` as part of a Kops cluster running on AWS. The problem is simple: when I run `cluster-autoscaler` on a regular node, it works fine, but when it's running on a master node, the pod times out, receives a `CrashLoopBackOff`, and then tries again to no avail. The following log message is the most applicable

```
F0715 16:55:30.458743       1 main.go:319] Failed to get nodes from apiserver: Get https://100.64.0.1:443/api/v1/nodes: dial tcp 100.64.0.1:443: i/o timeout
goroutine 1 [running]:
```

Kops version:  **1.9.1**
Kubernetes version: **1.9.6**
Cloud provider: **AWS**
Commands ran: **Followed the commands in the [Kops cluster-autoscaler documention](https://github.com/kubernetes/kops/tree/master/addons/cluster-autoscaler)**
What happened after the commands executed?: **Pod starts, and then goes into a `CrashLoopBackOff` loops**
What did you expect to happen?: **The `cluster-autoscaler` app goes into its normal loop of checking if the ASG is in it's target node size.**

Extra notes:
- I'm using the Calico networking stack
- I'm using the [Kube2IAM](https://github.com/jtblin/kube2iam) project. Both the master and node IAM roles have the proper permissions to assume the role I provide in the annotation like so:

```yaml
annotations:
  iam.amazonaws.com/role: CompanyIamRole-ClusterAutoscalerEc2
```

Cluster manifest and full log output: https://gist.github.com/sc250024/81525c85f3cdfc60349b3bfdcce755af",closed,False,2018-07-15 16:58:04,2018-07-17 18:40:20
autoscaler,aojea,https://github.com/kubernetes/autoscaler/issues/1065,https://api.github.com/repos/kubernetes/autoscaler/issues/1065,Missing CONTRIBUTING.md file,"All K8s subrepositories should have a CONTRIBUTING.md file, which at the minimum should point to https://github.com/kubernetes/community/blob/master/contributors/guide/README.md. Care should be taken that all information is in sync with the contributor guide.

Subrepositories may also have contributing guidelines specific to that repository. They should be explicitly documented and explained in the CONTRIBUTING.md

Ref:  https://github.com/kubernetes/community/issues/1832",closed,False,2018-07-16 09:19:36,2018-07-18 09:23:52
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1066,https://api.github.com/repos/kubernetes/autoscaler/issues/1066,Add updating kubernetes.sync to Godeps update instructions,Add updating kubernetes.sync to Godeps update instructions following discussion in #1053,closed,True,2018-07-16 12:50:34,2018-07-16 22:55:22
autoscaler,x13n,https://github.com/kubernetes/autoscaler/pull/1067,https://api.github.com/repos/kubernetes/autoscaler/issues/1067,bump addon resizer tag to 1.8.3,/assign @MaciekPytel ,closed,True,2018-07-17 07:13:44,2018-07-17 08:20:49
autoscaler,xtroncode,https://github.com/kubernetes/autoscaler/issues/1068,https://api.github.com/repos/kubernetes/autoscaler/issues/1068,"[Cluster-autoscaler, aws] Node not being deleted from kubernetes cluster","`kubernetes-version: 1.10.5`
`cluster-autoscaler version: 1.2.2`

I am experimenting with `cluster-autoscaler` on aws, while the desired capacity is being properly adjusted and the scale-in and scale-out works well, the nodes that are terminated are not deleted by the cluster autoscaler from k8s i.e when I run `kubectl get nodes` I can still see the terminated nodes in `NotReady` state and this causes issues in updating `DaemonSets`   https://github.com/kubernetes/kubernetes/issues/44458   

Ideally one would expect the node to be deleted from the cluster as well since the instance is terminated. 
I am running cluster-autoscaler with the following params 
```
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --nodes=1:10:KubernetesStaging
```

Am I missing something or this is the expected behaviour ?",closed,False,2018-07-17 07:29:00,2018-09-24 10:14:21
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1069,https://api.github.com/repos/kubernetes/autoscaler/issues/1069,Adding CONTRIBUTING.md file,"Adding the missing Contribution guide.
fixes #1065 ",closed,True,2018-07-17 11:13:00,2018-07-18 09:23:52
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/issues/1070,https://api.github.com/repos/kubernetes/autoscaler/issues/1070,Consider nodes for scaled down in order which avoid unnecessary pod disruptions,"Currently scale down logic does not do much (if anything) to prevent multiple disruptions of pods as nodes are being removed in scale down process.
It seems that giving preference for scale down to nodes with less utilization would be better. Current code will give same preference to scale down to any node with utilization less than threshold (default 50%).",open,False,2018-07-17 15:33:05,2019-01-25 23:58:58
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1071,https://api.github.com/repos/kubernetes/autoscaler/issues/1071,Upgrade AKS API to 2018-03-31,"This PR upgrades AKS API to new 2018-03-31 (which is required for clusters with custom VNET) together with `go-autorest` v10.12.0.

It also updates AKS/ACS agent pools to use new versioned APIs.

Fix #1051 #1063",closed,True,2018-07-19 06:55:31,2018-07-30 13:26:37
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1072,https://api.github.com/repos/kubernetes/autoscaler/issues/1072,Set RecommendationProvided if recommendation not empty.,,closed,True,2018-07-19 10:01:43,2018-07-20 16:25:26
autoscaler,MirzaSikander,https://github.com/kubernetes/autoscaler/issues/1073,https://api.github.com/repos/kubernetes/autoscaler/issues/1073,Azure: New nodes as part of scale up are not being setup properly,"Kuberentes version: `v1.11.0`
Autoscale version: `v1.3.0`

New NC6 nodes, being added by auto-scaler, are missing some dependencies which prevent the following **kube-system** pods from being deployed. 

```
azure-cni-networkmonitor-j85gh
kube-proxy-xhskd
nvidia-device-plugin-drhtd
```

And they all have the same error:

```
Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""azure-cni-networkmonitor-j85gh"": Error response from daemon: shim error: fork/exec /usr/bin/nvidia-container-runtime: no such file or directory

Pod sandbox changed, it will be killed and re-created.
```

Further details about node:
```
Kernel Version: 4.15.0-1013-azure
OS Image: Ubuntu 16.04.4 LTS
Container Runtime Version: docker://1.13.1
Kubelet Version: v1.11.0
Kube-Proxy Version: v1.11.0
```",closed,False,2018-07-19 22:12:17,2018-08-02 02:39:31
autoscaler,zjfroot,https://github.com/kubernetes/autoscaler/pull/1074,https://api.github.com/repos/kubernetes/autoscaler/issues/1074,The CA file on Amazon Linux 2 is at a different location,,closed,True,2018-07-20 10:03:23,2018-07-21 11:32:02
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1075,https://api.github.com/repos/kubernetes/autoscaler/issues/1075,Clean up last remains of dynamic autoscaler,Remove unused code and no longer needed builder pattern.,closed,True,2018-07-20 14:16:34,2018-07-21 11:32:55
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1076,https://api.github.com/repos/kubernetes/autoscaler/issues/1076,Fix potential panic,"err may be nil some cases, so err.Error() would panic CA.

Fix https://github.com/kubernetes/autoscaler/issues/863#issuecomment-406857408",closed,True,2018-07-23 03:19:48,2018-07-23 13:07:31
autoscaler,omerlh,https://github.com/kubernetes/autoscaler/issues/1077,https://api.github.com/repos/kubernetes/autoscaler/issues/1077,[Azure AKS] Failed to scale up nodes,"When trying to scale up, I've encountered the following error:
```
E0723 05:15:10.999472       1 static_autoscaler.go:283] Failed to scale up: Could not compute total resources: No node info for: agentpool&lt;nil&gt;
```
My deployment:
* Azure AKS
* Kuberentes 1.10.5
* CA latest version, using @feiskyer temporary build to solve #863 (`feisky/cluster-autoscaler:1076`).

This is the CA command line arguments:
```
 command:
        - ./cluster-autoscaler
        - --v=3
        - --logtostderr=true
        - --cloud-provider=azure
        - --skip-nodes-with-local-storage=false
        - --nodes=3:10:agentpool
```
And this is the output of `kubectl get nodes --show-labels`:
```
NAME                       STATUS    ROLES     AGE       VERSION   LABELS
aks-agentpool-32249617-0   Ready     agent     19h       v1.10.5   agentpool=agentpool,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=Standard_DS1_v2,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=westus,failure-domain.beta.kubernetes.io/zone=0,kubernetes.azure.com/cluster=MC_test-aks_test-aks_westus,kubernetes.io/hostname=aks-agentpool-32249617-0,kubernetes.io/role=agent,storageprofile=managed,storagetier=Premium_LRS
aks-agentpool-32249617-1   Ready     agent     19h       v1.10.5   agentpool=agentpool,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=Standard_DS1_v2,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=westus,failure-domain.beta.kubernetes.io/zone=0,kubernetes.azure.com/cluster=MC_test-aks_test-aks_westus,kubernetes.io/hostname=aks-agentpool-32249617-1,kubernetes.io/role=agent,storageprofile=managed,storagetier=Premium_LRS
aks-agentpool-32249617-2   Ready     agent     19h       v1.10.5   agentpool=agentpool,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=Standard_DS1_v2,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=westus,failure-domain.beta.kubernetes.io/zone=1,kubernetes.azure.com/cluster=MC_test-aks_test-aks_westus,kubernetes.io/hostname=aks-agentpool-32249617-2,kubernetes.io/role=agent,storageprofile=managed,storagetier=Premium_LRS
```
So from what I can tell, it looks like I set up everything correctly. Can you help me figure what I'm missing?",closed,False,2018-07-23 05:26:22,2018-07-23 08:13:13
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1078,https://api.github.com/repos/kubernetes/autoscaler/issues/1078,Fix error on node info not found for group,"Instead of using unrelated (and always nil) error variable, create a new error from scratch. Fixes potential cases for #1076 ",closed,True,2018-07-23 09:17:36,2018-07-23 13:08:43
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1079,https://api.github.com/repos/kubernetes/autoscaler/issues/1079,Simplify building cloud provider,"Pass the entire options object to build cloud provider.
Remove no longer necessary builder pattern for cloud provider (it's never re-constructed after initialization.)",closed,True,2018-07-23 09:23:56,2018-07-25 11:02:59
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1080,https://api.github.com/repos/kubernetes/autoscaler/issues/1080,Remove not-so-useful type check test,"This test will always pass since it only checks the returned type, which is guaranteed by the function signature anyway.

Whether we can write a better test for it, or if we even need one, is another question.",closed,True,2018-07-23 09:43:16,2018-07-23 10:00:11
autoscaler,cabrinoob,https://github.com/kubernetes/autoscaler/issues/1081,https://api.github.com/repos/kubernetes/autoscaler/issues/1081,EKS and tag k8s.io/cluster-autoscaler/node-template/label is not assigned to worker nodes,"Hi,
I'am using an EKS cluster and I'd like to use cluster-autoscaler with it.

So I have a brand new EKS cluster (with existing nodes) and I'd like to add new worker nodes with the CloudFormation script (as explained [here](https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html)).

This script provided 3 new t2.small EC2 into an auto-scaling group. Because I'd like to use nodeSelector, I have tagged the ASG as explained [here](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#scaling-a-node-group-to-0) : 

_If you are using nodeSelector you need to tag the ASG with a node-template key ""k8s.io/cluster-autoscaler/node-template/label/""_

In my AWS console I see my tag on my ASGlike that : 

![tag](https://i.imgur.com/wzZltrG.png)

So, my problem is that I see news nodes in kubernetes, but the tag _k8s.io/cluster-autoscaler/node-template/label/project_ has not been assigned to the labels of these nodes .... I don't know what I have missed.

The only tags I see are : 

```
Labels:         beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=t2.small
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=us-east-1
                    failure-domain.beta.kubernetes.io/zone=us-east-1c
                    kubernetes.io/hostname=ip-xxx-xxx-xxx-xxx.ec2.internal
```

Here is the launch command of the cluster-autoscaler pod : 
```json
""command"": [
              ""./cluster-autoscaler"",
              ""--v=4"",
              ""--stderrthreshold=info"",
              ""--cloud-provider=aws"",
              ""--skip-nodes-with-local-storage=false"",
              ""--nodes=1:3:at-eks-worker-nodes-asg2-NodeGroup-1QOBK4RZ42IZI""
            ]
```
What I've missed?

Thank you for your help
",closed,False,2018-07-23 10:02:59,2018-09-12 09:20:32
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1082,https://api.github.com/repos/kubernetes/autoscaler/issues/1082,VPA fix conditions to trigger garbage collection.,,closed,True,2018-07-23 11:51:21,2018-07-23 12:13:55
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1083,https://api.github.com/repos/kubernetes/autoscaler/issues/1083,Do not garbage collect newly created ContainerStates.,"Do not garbage collect newly created ContainerStates (LastSampleStart.IsZero() == true). 

Also fix typo. ",closed,True,2018-07-23 17:11:27,2018-07-23 17:27:22
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1084,https://api.github.com/repos/kubernetes/autoscaler/issues/1084,Fix recommendation present condition in recommender e2e.,,closed,True,2018-07-24 07:32:37,2018-07-24 08:53:27
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1085,https://api.github.com/repos/kubernetes/autoscaler/issues/1085,Reinstate removing non-nexisting pods from the model.,,closed,True,2018-07-24 14:38:49,2018-07-24 14:57:19
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/issues/1086,https://api.github.com/repos/kubernetes/autoscaler/issues/1086,Add unit tests for cluster_feeder.,We really need tests for https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/recommender/input/cluster_feeder.go ,open,False,2018-07-24 14:56:32,2019-03-13 18:21:09
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1087,https://api.github.com/repos/kubernetes/autoscaler/issues/1087,Move creating cloud provider out of context,"Move creating cloud provider out of context.
Move creating helper structs with dynamic config into cloud provider constructor.",closed,True,2018-07-25 11:45:58,2018-07-26 11:30:40
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/1088,https://api.github.com/repos/kubernetes/autoscaler/issues/1088,Fix node memory prediction in GCP cloudprovider,,closed,True,2018-07-25 15:42:31,2018-07-26 15:45:11
autoscaler,sakthivetrivel,https://github.com/kubernetes/autoscaler/pull/1089,https://api.github.com/repos/kubernetes/autoscaler/issues/1089,changing link to microsoft docs,,closed,True,2018-07-25 22:34:35,2018-08-01 07:25:10
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1090,https://api.github.com/repos/kubernetes/autoscaler/issues/1090,Use proxy for container state aggregator.,"When a ContainerAggregatedState of a live container is garbage collected due to lack of samples, is will be recreated once the samples start appearing again. This prevents us from not providing recommendations for containers that are live but didn't get samples for 8 days.",closed,True,2018-07-26 08:06:43,2018-07-27 16:15:56
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1091,https://api.github.com/repos/kubernetes/autoscaler/issues/1091,VPA PodBuilder introduced to improve readability.,,closed,True,2018-07-26 09:55:57,2018-07-26 10:17:32
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1092,https://api.github.com/repos/kubernetes/autoscaler/issues/1092,Group all Kubernetes API clients,"Move all clients used for communicating with Kubernetes API server to a separate struct within context. This doesn't affect current usage as the struct is embedded, the only benefit is cleaner structs and logically divided responsibility. Before, most clients lived in AutoscalingContext, except for ListerRegistry, which was embedded directly in StaticAutoscaler.

Delta except for tests is 32 lines. The rest is reducing test setup verbosity.",closed,True,2018-07-26 11:42:09,2018-07-26 15:19:20
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1093,https://api.github.com/repos/kubernetes/autoscaler/issues/1093,VPA fix handling of pending pods.,,closed,True,2018-07-26 12:58:32,2018-07-27 08:58:20
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1094,https://api.github.com/repos/kubernetes/autoscaler/issues/1094,Remove not-so-useful constructor test,This is mostly testing test setup (i.e. that `expander.RandomExpanderName` is a valid expander strategy name),closed,True,2018-07-26 15:37:01,2018-07-31 10:13:06
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/1095,https://api.github.com/repos/kubernetes/autoscaler/issues/1095,Cherry-pick #1088 on CA 1.3,,closed,True,2018-07-26 15:50:39,2018-07-26 17:00:26
autoscaler,bob-shih,https://github.com/kubernetes/autoscaler/issues/1096,https://api.github.com/repos/kubernetes/autoscaler/issues/1096,"Support for new AWS EC2 instance types (R5, R5d, and z1d)","Please add the new AWS EC2 instance types (R5, R5d, and z1d) support in https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/ec2_instance_types.go

(See https://aws.amazon.com/blogs/aws/now-available-r5-r5d-and-z1d-instances/)

Thanks",closed,False,2018-07-27 09:52:31,2018-09-12 09:19:36
autoscaler,satlank,https://github.com/kubernetes/autoscaler/pull/1097,https://api.github.com/repos/kubernetes/autoscaler/issues/1097,Re-run gen to pick up new AWS instance types,"This adds r5, r5d and z1d types. Addresses #1096.",closed,True,2018-07-27 19:08:35,2018-10-27 01:30:49
autoscaler,chrisduong,https://github.com/kubernetes/autoscaler/issues/1098,https://api.github.com/repos/kubernetes/autoscaler/issues/1098,Version 1.1.3 | Auto Discovery not working,"Hi,

I'm using Version 1.1.3. I'm using Kubernetes version 1.9.x, I cannot use the version 1.3.x as the WARNing about compatibility.

I followed this guide https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws. But I've got the Error log

```
I0730 05:35:42.909028       1 auto_scaling.go:79] Failed to describe ASGs: Must specify at least one ASG name.
F0730 05:35:42.909055       1 cloud_provider_builder.go:112] Failed to create AWS cloud provider: Failed to get ASGs: List of ASG names was empty. Must specify at least one ASG name
```

My Deployment config is:

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""1""
  labels:
    app: aws-cluster-autoscaler
    chart: cluster-autoscaler-0.7.0
    heritage: Tiller
    release: cluster-ac
  name: cluster-ac-aws-cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: aws-cluster-autoscaler
      release: cluster-ac
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/port: ""8085""
        prometheus.io/scrape: ""true""
        scheduler.alpha.kubernetes.io/tolerations: '[{""key"":""dedicated"", ""value"":""master""}]'
      creationTimestamp: null
      labels:
        app: aws-cluster-autoscaler
        release: cluster-ac
    spec:
      containers:
      - command:
        - ./cluster-autoscaler
        - --cloud-provider=aws
        - --namespace=kube-system
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/devops-kops.k8s.local
        - --logtostderr=true
        - --skip-nodes-with-local-storage
        - --stderrthreshold=info
        - --v=4
        env:
        - name: AWS_REGION
          value: ap-southeast-1
        image: k8s.gcr.io/cluster-autoscaler:v1.1.3
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health-check
            port: 8085
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: aws-cluster-autoscaler
        ports:
        - containerPort: 8085
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/ssl/certs/ca-certificates.crt
          name: ssl-certs
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/role: master
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: cluster-ac-aws-cluster-autoscaler
      serviceAccountName: cluster-ac-aws-cluster-autoscaler
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      volumes:
      - hostPath:
          path: /etc/ssl/certs/ca-certificates.crt
          type: """"
        name: ssl-certs
```",closed,False,2018-07-30 05:58:40,2018-07-30 07:59:07
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1099,https://api.github.com/repos/kubernetes/autoscaler/issues/1099,Cherry pick of #1071: Upgrade AKS API to 2018-03-31,Cherry pick of #1071: Upgrade AKS API to 2018-03-31 to CA release-1.3,closed,True,2018-07-31 04:50:40,2018-08-14 14:01:02
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1100,https://api.github.com/repos/kubernetes/autoscaler/issues/1100,Cherry pick of #1071: Upgrade AKS API to 2018-03-31,Cherry pick of #1071: Upgrade AKS API to 2018-03-31 to CA release-1.2.,closed,True,2018-07-31 04:51:38,2018-08-14 14:00:34
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1101,https://api.github.com/repos/kubernetes/autoscaler/issues/1101,Initialization package renamed to routines.,,closed,True,2018-07-31 08:40:16,2018-07-31 08:52:35
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1102,https://api.github.com/repos/kubernetes/autoscaler/issues/1102,Move initializing defaults out of main,Move initializing defaults out of main. Extract building autoscaler from run(). Extract constructing auxiliary kube clients to a separate function.,closed,True,2018-07-31 11:26:47,2018-08-02 12:26:50
autoscaler,wmuizelaar,https://github.com/kubernetes/autoscaler/issues/1103,https://api.github.com/repos/kubernetes/autoscaler/issues/1103,"VPA replaces annotations of pods, in stead of updating. This breaks PSP.","Hi,

Testing with the vertical-pod-autoscaler, we figured out that the annotations on a pod are completely replaced by the admission controller, in stead of merged/added.

Used version of the containers:
```
k8s.gcr.io/vpa-admission-controller:0.2.0
k8s.gcr.io/vpa-recommender:0.2.0
k8s.gcr.io/vpa-updater:0.2.0
```

What I see is that containers that are run from a deployment, and I set a custom annotation in my specfile, these will be overwritten by the vpa-admission-controller:

Before:
```
kubectl describe pod nginx-deployment-5d78d59c86-dxg7h
Name:           nginx-deployment-5d78d59c86-dxg7h
Namespace:      default
Node:           gke-bolcom-sbx-4wu-internal-4e3396ab-b2lj/10.16.4.18
Start Time:     Tue, 31 Jul 2018 13:37:25 +0200
Labels:         app=nginx
                pod-template-hash=1834815742
Annotations:    bol.io/test=test
                kubernetes.io/limit-ranger=LimitRanger plugin set: cpu request for container nginx
```

After:
```
kubectl describe pod nginx-deployment-5d78d59c86-lr2wv
Name:           nginx-deployment-5d78d59c86-lr2wv
Namespace:      default
Node:           <none>
Labels:         app=nginx
                pod-template-hash=1834815742
Annotations:    vpaUpdates=Pod resources updated by nginx-vpa: container 0: cpu request, memory request, memory limit
```

When using Pod Security Policies in a cluster, this will also set annotations, and since those are overwritten by the admission controller, those pods won't get scheduled again, since there cannot be found a valid podsecuritypolicy.",closed,False,2018-07-31 12:15:34,2018-08-01 14:02:35
autoscaler,maartenvandenbogaard,https://github.com/kubernetes/autoscaler/pull/1104,https://api.github.com/repos/kubernetes/autoscaler/issues/1104,VPA: don't overwrite existing annotations when patching pod,Fixes: https://github.com/kubernetes/autoscaler/issues/1103,closed,True,2018-07-31 20:48:46,2018-08-01 14:02:35
autoscaler,abizake,https://github.com/kubernetes/autoscaler/issues/1105,https://api.github.com/repos/kubernetes/autoscaler/issues/1105,"[Azure] [AKS] Cluster not Scaling up with error ""Node group agentpool is not ready for scaleup"""," Failed to scale up: failed to increase node group size: containerservice.ManagedClustersClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code=""PropertyChangeNotAllowed"" Message=""Changing property 'enableRBAC' is not allowed.""

Additional info 
There are currently 2 nodes in the cluster.
I created the Cluster Auto Scaler with Min=3 and Max =5
I also scaled the pods because of which there is one pods unscheduled.

Kubernetes Version : 1.10.5
Cluster Autoscaler version : V1.2.2
Cloud Provider :- Azure (AKS)

Please Advise.",closed,False,2018-08-01 14:57:31,2018-11-20 06:35:00
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1106,https://api.github.com/repos/kubernetes/autoscaler/issues/1106,Remove VPA admission-controller option to set limit.,,closed,True,2018-08-01 19:38:09,2018-08-02 10:15:10
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1107,https://api.github.com/repos/kubernetes/autoscaler/issues/1107,Don't use typed errors during initialization,"We don't check or report errors at this stage, just fail.",closed,True,2018-08-02 12:31:00,2018-08-16 13:08:52
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1108,https://api.github.com/repos/kubernetes/autoscaler/issues/1108,Capping to limit or min/max allowed leaves annotation,,closed,True,2018-08-02 13:33:02,2018-08-06 10:11:03
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1109,https://api.github.com/repos/kubernetes/autoscaler/issues/1109,Clean up estimators,"Hide estimators behind interface. Check flag only once, during initialization. Deprecate BasicEstimator.",closed,True,2018-08-02 14:35:51,2018-11-06 13:30:36
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1110,https://api.github.com/repos/kubernetes/autoscaler/issues/1110,Extract autoscaling GCE client,Extract GCE calls and hide them behind a new interface. This will allow for easier mocking (and getting rid of jsons from gke_manager_test.go) in the near future.,closed,True,2018-08-03 09:50:32,2018-08-06 15:44:25
autoscaler,spiffxp,https://github.com/kubernetes/autoscaler/issues/1111,https://api.github.com/repos/kubernetes/autoscaler/issues/1111,Create an OWNERS file,"ref: https://github.com/kubernetes/community/issues/1721

This repo is listed in sigs.yaml as belonging to sig-autoscaling, but it lacks an OWNERS file.  Can you please add one with the appropriate list of approvers and reviewers? eg:
- [robust example: approvers, reviewers, aliases](https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/master/OWNERS)
- [simple example: only approvers, no aliases](https://github.com/kubernetes-incubator/external-storage/blob/master/OWNERS)

For more information see the [OWNERS docs](https://go.k8s.io/owners)

/sig autoscaling

/assign @mwielgus @DirectXMan12 
sig-autoscaling chairs

/assign @aleksandra-malinowska @bskiba @MaciekPytel 
as folks who've merged PR's here recently",closed,False,2018-08-04 03:06:29,2018-08-21 20:45:05
autoscaler,directionless,https://github.com/kubernetes/autoscaler/issues/1112,https://api.github.com/repos/kubernetes/autoscaler/issues/1112,addon-resizer scale based on number of namespaces (feature request),"Hi! I have a couple of small GKE clusters, and many of the tools I interact with use the addon-resizer. In my environment, it usually underprovisions and results in endless cycles of OOMKilling and number tweaking. 

I _suspect_ this is because I have a n large number of namespaces:
```
$ kubectl get ns | wc -l
     238

$ kubectl get nodes | wc -l
      47

$ kubectl get pods --all-namespaces | wc -l
    4008
```

Most recently, I've run into this with kube-state-metrics, heapster, and metrics-server. The latter two are mostly google provisioned, so it's extra awkward working with them.

How do y'all feel about adding a namespace scaling option?",closed,False,2018-08-04 06:23:46,2019-02-09 10:56:18
autoscaler,wangxy518,https://github.com/kubernetes/autoscaler/pull/1113,https://api.github.com/repos/kubernetes/autoscaler/issues/1113,Update container-gen.go,update some urls.,closed,True,2018-08-06 07:01:15,2018-08-06 09:52:05
autoscaler,towca,https://github.com/kubernetes/autoscaler/pull/1114,https://api.github.com/repos/kubernetes/autoscaler/issues/1114,Add AutoscalingStatusProcessor,,closed,True,2018-08-06 11:53:06,2018-08-07 13:03:22
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1115,https://api.github.com/repos/kubernetes/autoscaler/issues/1115,Add metrics to VPA,"Added some basic metrics to all 3 VPA components.

@bskiba , please have a look.",closed,True,2018-08-06 13:20:08,2018-08-07 12:25:10
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1116,https://api.github.com/repos/kubernetes/autoscaler/issues/1116,Extract autoscaling GKE client,Extract autoscaling GKE client from GceManager.,closed,True,2018-08-06 16:27:13,2018-08-08 12:56:45
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1117,https://api.github.com/repos/kubernetes/autoscaler/issues/1117,VPA tolerate nil annotation map returned from recommendation processor,,closed,True,2018-08-06 18:36:47,2018-08-06 18:50:24
autoscaler,MirzaSikander,https://github.com/kubernetes/autoscaler/issues/1118,https://api.github.com/repos/kubernetes/autoscaler/issues/1118,[Azure] [ACS-engine] Scale-up failing - Provided value '110' is not valid.,"ACS-engine Version: v0.20.2
Kubernetes version: 1.11
Auto-scaler version: v1.3.0

From the error below it seems that there is a cap on the number of nodes you can have in a cluster, which is unexpected in itself, but we haven't reached the cap. There have been multiple (successful) scale-ups and scale-downs, and right now there are only 11 nodes in the cluster.

```
I0806 19:19:32.705974       1 utils.go:196] Pod athens-ppo-a-copy-lr0.001-tf1000-1 can't be scheduled on linuxpool1, predicate failed: PodFitsResources predicate mismatch, cannot put default/athens-ppo-a-copy-lr0.001-tf1000-1 on template-node-for-linuxpool1-8985657718976750180, reason: Insufficient alpha.kubernetes.io/nvidia-gpu
I0806 19:19:32.706150       1 scale_up.go:383] Best option to resize: linuxpool1
I0806 19:19:32.706169       1 scale_up.go:387] Estimated 6 nodes needed in linuxpool1
I0806 19:19:32.706184       1 scale_up.go:466] Final scale-up plan: [{linuxpool1 10-&gt;16 (max: 100)}]
I0806 19:19:32.706204       1 scale_up.go:536] Scale-up: setting group linuxpool1 size to 16
I0806 19:19:33.569099       1 azure_agent_pool.go:213] Waiting for deploymentsClient.CreateOrUpdate(k8s-internal-linux-02-5b5a78e0, cluster-autoscaler-1498246002, {%!s(*resources.DeploymentProperties=&amp;{0xc420f813a8 &lt;nil&gt; 0xc420f813b0 &lt;nil&gt; Incremental &lt;nil&gt;})})
W0806 19:19:33.569186       1 clusterstate.go:223] Disabling scale-up for node group linuxpool1 until 2018-08-06 19:49:33.569180802 +0000 UTC m=+876033.409897693
E0806 19:19:33.569271       1 static_autoscaler.go:283] Failed to scale up: failed to increase node group size: resources.DeploymentsClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code=""InvalidTemplate"" Message=""Deployment template validation failed: 'The provided value '106' for the template parameter 'linuxpool1Count' at line '1' and column '14753' is not valid. The parameter value is not part of the allowed value(s): '1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100'.'.""
```",closed,False,2018-08-06 19:21:57,2018-10-29 06:05:28
autoscaler,wangxy518,https://github.com/kubernetes/autoscaler/pull/1119,https://api.github.com/repos/kubernetes/autoscaler/issues/1119,Update external_docs.go,update a url,closed,True,2018-08-07 01:40:08,2018-08-07 09:39:29
autoscaler,wangxy518,https://github.com/kubernetes/autoscaler/pull/1120,https://api.github.com/repos/kubernetes/autoscaler/issues/1120,Update CHANGELOG.md,"""https://github.com/magiconair/properties/tags/v1.7.0""   is an unvalid url.
So I changed it .",closed,True,2018-08-07 04:40:16,2018-08-07 09:39:55
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1121,https://api.github.com/repos/kubernetes/autoscaler/issues/1121,VPA list of conditions extended.,,closed,True,2018-08-07 11:41:52,2018-08-07 12:23:55
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1122,https://api.github.com/repos/kubernetes/autoscaler/issues/1122,Add metrics for execution time of RunOnce,"Covers Recommender's and Updater's RunOnce.

@bskiba, please have a look.",closed,True,2018-08-07 13:17:08,2018-08-07 14:52:16
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1123,https://api.github.com/repos/kubernetes/autoscaler/issues/1123,Remove faulty validation for VPA,"The CRD creation doesn't work at the moment. Validation fails for enums and lists. This should fix e2e tests for now. 
Will add validation to admission webhook in a followup pr.",closed,True,2018-08-07 13:30:28,2018-08-07 14:49:41
autoscaler,Netdoc41,https://github.com/kubernetes/autoscaler/issues/1124,https://api.github.com/repos/kubernetes/autoscaler/issues/1124,readiness for agent pool,"I have been working on implementing the cluster autoscaler in Azure. i have gotten past most of my issues. (most were simple mistakes by me) the one i cant find a fix for or enough information on is 

""Failed to find readiness information for agentpool""

Its in the logs over and over.

2018-08-07T14:15:04.475086315Z I0807 14:15:04.474856       1 scale_down.go:446] No candidates for scale down
2018-08-07T14:15:04.475122415Z W0807 14:15:04.474891       1 clusterstate.go:347] Failed to find readiness information for agentpool
2018-08-07T14:15:04.475125715Z W0807 14:15:04.474901       1 clusterstate.go:404] Failed to find readiness information for agentpool
2018-08-07T14:15:04.475128915Z W0807 14:15:04.474904       1 clusterstate.go:347] Failed to find readiness information for agentpool

Nothing else in there of concern. What did i mis-configure?",closed,False,2018-08-07 14:15:56,2019-01-04 19:28:34
autoscaler,nhoughto,https://github.com/kubernetes/autoscaler/issues/1125,https://api.github.com/repos/kubernetes/autoscaler/issues/1125,Support AWS Spot block instances as pseudo ASG,"We use autoscaler on AWS quite successfully for real time workloads, but would like to use spot instances for our CI build workload. Since spot instances themselves are too disruptive for non-preemptive workloads like CI jobs we want to use Spot Blocks (defined duration instances) where they are guaranteed for N hours.

Autoscaler could provision these instances and then replace them when they expire, as well as manage the condoning and draining as required, could obv also managing scaling them up too.

We currently use some bash scripts to manage this but would love a first party / more flexible answer.

Our current bash scripts, for anyone else with this problem:

https://github.com/bsycorp/spot-keeper",open,False,2018-08-08 10:01:23,2019-01-09 22:02:03
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1126,https://api.github.com/repos/kubernetes/autoscaler/issues/1126,Refactor VPA utilities,,closed,True,2018-08-08 12:03:55,2018-08-09 08:18:14
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1127,https://api.github.com/repos/kubernetes/autoscaler/issues/1127,Make Create() cloud provider method return newly created node group,"After creating a node group, cloud provider should refresh its node group config. Node group that was created may not be the same as its blueprint with flipped ""exist"" bit. Instead of indirectly overwriting the struct at provided address, return node group object representing a group that already exists at this point.",closed,True,2018-08-08 12:57:38,2018-08-08 13:21:08
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1128,https://api.github.com/repos/kubernetes/autoscaler/issues/1128,VPA routines refactoring - making them more accessible,,closed,True,2018-08-08 14:58:20,2018-08-09 09:34:31
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1129,https://api.github.com/repos/kubernetes/autoscaler/issues/1129,Extract cache from GCE manager,"This extracts cache management part of GCE manager in order to make it:
1) easier to understand and verify it is used correctly,
2) reusable for separating GKE cloud provider.",closed,True,2018-08-08 15:54:09,2018-08-13 14:06:22
autoscaler,jeruane,https://github.com/kubernetes/autoscaler/pull/1130,https://api.github.com/repos/kubernetes/autoscaler/issues/1130,"cherry pick: ""Fix nvidia gpu resource name on AWS"" to 1.1","cherry pick of:
https://github.com/kubernetes/autoscaler/pull/648",closed,True,2018-08-08 18:53:52,2018-08-09 14:30:18
autoscaler,jeruane,https://github.com/kubernetes/autoscaler/pull/1131,https://api.github.com/repos/kubernetes/autoscaler/issues/1131,"cherry pick: ""Fix nvidia gpu resource name on AWS"" to 1.0","cherry pick of:
https://github.com/kubernetes/autoscaler/pull/648",closed,True,2018-08-08 18:53:55,2018-10-29 17:13:26
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1132,https://api.github.com/repos/kubernetes/autoscaler/issues/1132,Fix label names to be in line with Prometheus conventions,"@bskiba , please have a look.",closed,True,2018-08-08 19:26:04,2018-08-09 08:06:48
autoscaler,aermakov-zalando,https://github.com/kubernetes/autoscaler/issues/1133,https://api.github.com/repos/kubernetes/autoscaler/issues/1133,Node pool scale up timeout,"The autoscaler has a timeout for non-ready nodes which forces it to kill those nodes and potentially select a different node pool in the next iteration. However, in the situation where the node pool cannot scale up at all it'll happily wait forever, keeping pods in Pending state without trying to compensate.

For example, setting multiple AWS Spot node pools with different instance types, or setting up a Spot pool and an On Demand pool doesn't really work. We'd expect CA to scale up one of the ASGs, detect a few minutes later that there's still no nodes coming up (because the corresponding Spot pool doesn't have capacity) and fall back to another pool. What actually happens is that CA will scale up the node pool by increasing desired capacity and then not do anything at all other than printing `Upcoming 1 nodes`/`Failed to find readiness information for ...`.",open,False,2018-08-09 09:34:33,2019-04-02 07:23:34
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1134,https://api.github.com/repos/kubernetes/autoscaler/issues/1134,VPA fix merging initial checkpoint into ContainerNameToAggregateStateMap,,closed,True,2018-08-09 15:41:38,2018-08-10 08:01:43
autoscaler,ringtail,https://github.com/kubernetes/autoscaler/issues/1135,https://api.github.com/repos/kubernetes/autoscaler/issues/1135,Could GPU support move to cloud provider?,"autoscaler has GKE GPU support and I want to add alicloud GPU to autoscaler. But I found the code is outside the scope of cloud provider, Is there any plan to move those code to cloud provider?",open,False,2018-08-10 02:34:00,2019-04-05 10:13:21
autoscaler,timothyclarke,https://github.com/kubernetes/autoscaler/issues/1136,https://api.github.com/repos/kubernetes/autoscaler/issues/1136,Feature Request - Scale down should cordon nodes,"I use EC2 and have had instances where pods have been scheduled onto nodes which are in a shutting down state in EC2.
From observation it appears that when scaling down pods the node is not cordoned which would prevent new pods from being scheduled onto them.",closed,False,2018-08-10 07:38:24,2019-01-10 11:41:49
autoscaler,timothyclarke,https://github.com/kubernetes/autoscaler/issues/1137,https://api.github.com/repos/kubernetes/autoscaler/issues/1137,"Feature Request - scale-down-utilization-threshold per instance group, and from label","We utilise a cluster that contains both CI and development environments. As such there is more than one instance group that needs scaling.
We are in AWS and the instance groups use different sized nodes for the CI compared to the general environments. As such having a common `scale-down-utilization-threshold` will cause either one instance group to prematurely kill nodes to be rescheduled elsewhere (for the larger node types), or to never scale down instance types because their utilisation is too high when they are only running networking pods (or similar daemon sets).

It would be really useful to set the `scale-down-utilization-threshold` on a per instance group basis, and be able to set that via cloud labels so it can be autodiscovered.",closed,False,2018-08-10 08:08:02,2018-11-27 09:14:16
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1138,https://api.github.com/repos/kubernetes/autoscaler/issues/1138, Add a sequential implementation of PodEvictionAdmission,"Adds sequentialPodEvictionAdmission for chaining a number of PEAs.
Also fixes a typo, in a separate commit not to pollute the diff of the real change.

@bskiba , please have a look",closed,True,2018-08-10 13:28:59,2018-08-10 14:19:24
autoscaler,nithya-krish,https://github.com/kubernetes/autoscaler/issues/1139,https://api.github.com/repos/kubernetes/autoscaler/issues/1139,metrics-server,"I am facing the problem that my metrics-server container ends up with the crashloop backoff state. Did anyone facing the same problem. Please help me to solve this issue.

",closed,False,2018-08-10 14:42:21,2018-08-10 14:45:00
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1140,https://api.github.com/repos/kubernetes/autoscaler/issues/1140,VPA shouldn't store checkpoint if history is being loaded.,,closed,True,2018-08-13 07:57:51,2018-08-13 09:35:37
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1141,https://api.github.com/repos/kubernetes/autoscaler/issues/1141,Sync godeps to kubernetes/kubernetes 524a814,Split into two commits to separate manual changes.,closed,True,2018-08-13 10:21:07,2018-08-13 12:34:50
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1142,https://api.github.com/repos/kubernetes/autoscaler/issues/1142,Create GKE cloud provider,"For now, GKE cloud provider is just copied GCE cloud provider. Next PRs will prune redundant functionality (GCE-only stuff from GKE and vice versa.)

Also contains the last (hopefully) of necessary refactors - mostly exporting stuff so that it can be used from another package.",closed,True,2018-08-13 16:28:34,2018-08-14 11:52:13
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1143,https://api.github.com/repos/kubernetes/autoscaler/issues/1143,Fix e2e tests,,closed,True,2018-08-14 08:48:53,2018-08-14 09:22:25
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1144,https://api.github.com/repos/kubernetes/autoscaler/issues/1144,Seperate e2e test run from deployment.,,closed,True,2018-08-14 08:56:10,2018-08-14 13:06:18
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1145,https://api.github.com/repos/kubernetes/autoscaler/issues/1145,Remover redundant parts of GKE and GCE cloud providers ,"Remove GKE-specific code and types from GCE cloud providers.
Remove GCE-specific code from GKE cloud provider.",closed,True,2018-08-14 11:55:34,2018-08-14 13:19:24
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1146,https://api.github.com/repos/kubernetes/autoscaler/issues/1146,Cluster Autoscaler 1.3.2 beta.1,Update Cluster Autoscaler version to 1.3.2 beta.1,closed,True,2018-08-14 13:58:23,2018-08-14 15:04:33
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1147,https://api.github.com/repos/kubernetes/autoscaler/issues/1147,Cluster Autoscaler 1.2.3-beta.1,Update Cluster Autoscaler version to 1.2.3-beta.1,closed,True,2018-08-14 14:03:39,2018-08-14 15:04:18
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1148,https://api.github.com/repos/kubernetes/autoscaler/issues/1148,Clean up GceManager interface,Export and include in interface only methods used in gce_cloud_provider.go. Add comments to exported methods.,closed,True,2018-08-14 14:28:40,2018-08-14 14:55:20
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1149,https://api.github.com/repos/kubernetes/autoscaler/issues/1149,Clean up GkeManager interface,Export and include in interface only methods used in gke_cloud_provider.go. Add comments to exported methods.,closed,True,2018-08-14 14:56:46,2018-11-06 15:35:46
autoscaler,tomkerkhove,https://github.com/kubernetes/autoscaler/issues/1150,https://api.github.com/repos/kubernetes/autoscaler/issues/1150,Feature Request - Trigger webhook when scaling is performed,"_Disclaimer - If this is already possible, sorry for creating this issue but I couldn't find anything related to this_

Would be great to be able to configure a webhook which is called for every scaling operation (up or down).
This would allow users to gain insights on their scaling actions if they wish to do so.

A great example is how Azure Monitor autoscale provides this on their component for Azure scaling ([docs](https://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/insights-autoscale-to-webhook-email)).

[I wrote a blog post](https://blog.tomkerkhove.be/2017/10/16/autoscaling-your-platform-with-azure-monitor/) a while back why I love this and why it's important.",closed,False,2018-08-14 16:57:32,2018-08-16 14:06:58
autoscaler,phsiao,https://github.com/kubernetes/autoscaler/pull/1151,https://api.github.com/repos/kubernetes/autoscaler/issues/1151,Improve getMatchingVPA logging to be more clear (with pod name) and using logging level,"Currently in getMatchingVPA, it can generate a lot of lines of ""Let's choose from x configs"" without any context if you set v=1.  This PR sets the logging level to be identical to surrounding logging level, as well as add pod name to the log line for clarity.",closed,True,2018-08-15 01:41:38,2018-08-16 15:11:07
autoscaler,phsiao,https://github.com/kubernetes/autoscaler/pull/1152,https://api.github.com/repos/kubernetes/autoscaler/issues/1152,VPA admission controller should stop review Pod Update event,"
I might be wrong, but it is my understanding that pod Update review has no impact through VPA admission controller.   It would be doing work that will have no effect at the end.

There can be a lot of pod updates events, for example, updating pod annotations, and would be creating a lot of unnecessary work.  I'd like to propose we remove the review of pod Update.",closed,True,2018-08-15 14:28:00,2018-08-20 18:56:05
autoscaler,phsiao,https://github.com/kubernetes/autoscaler/pull/1153,https://api.github.com/repos/kubernetes/autoscaler/issues/1153,Add support for customizing qps and burst for recommender via --kubeapi-qps and --kube-api-burst,"
For a cluster with a large number of vpa objects, the default qps is too low (5 I believe) for it to finish processing quickly and you end up getting a lot of noise related to throttling.

This PR adds basic support for specifying and customizing qps and burst, and will be applied the setting to both checkpoint writer and vpcClient.

",closed,True,2018-08-16 20:58:58,2018-08-31 19:23:36
autoscaler,devkid,https://github.com/kubernetes/autoscaler/pull/1154,https://api.github.com/repos/kubernetes/autoscaler/issues/1154,Mention --maxGracefulTerminationSec in FAQ.md,"The FAQ was wrong in a way that it said that pods are given at most 10 minutes to gracefully terminate, but the maximum time is actually configurable.",closed,True,2018-08-17 13:33:05,2018-08-20 09:56:08
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1155,https://api.github.com/repos/kubernetes/autoscaler/issues/1155,Update Cluster Autoscaler base image,Update Cluster Autoscaler base image to debian-base-amd:0.3.2,closed,True,2018-08-17 16:07:20,2018-08-20 08:54:21
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1156,https://api.github.com/repos/kubernetes/autoscaler/issues/1156,Cherry pick of #1155: Update Cluster Autoscaler base image,Cherry pick of #1155: Update Cluster Autoscaler base image to debian-base-amd:0.3.2,closed,True,2018-08-17 16:10:50,2018-08-20 09:43:19
autoscaler,consideRatio,https://github.com/kubernetes/autoscaler/issues/1157,https://api.github.com/repos/kubernetes/autoscaler/issues/1157,Autoscaler docs - priority cutoff confusion,"While reading the [Cluster Autoscaler FAQ](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-cluster-autoscaler-work-with-pod-priority-and-preemption), I find the following statement saying that pods with priority lower than 0 don't trigger scale up.

**SCREENSHOT 1**
![image](https://user-images.githubusercontent.com/3837114/44282710-3bdf9700-a25c-11e8-84b7-795c4c35136d.png)

---

Later, in this section a lot of confusion is created:

1. The example suggest changing the cutoff which isn't recommended, without motivation.
2. The cutoff is claimed to be -1, while it was claimed to be 0 above.


**SCREENSHOT 2**
![image](https://user-images.githubusercontent.com/3837114/44282825-95e05c80-a25c-11e8-9bca-7f347610603a.png)
",closed,False,2018-08-17 18:32:01,2019-03-23 10:23:48
autoscaler,phsiao,https://github.com/kubernetes/autoscaler/pull/1158,https://api.github.com/repos/kubernetes/autoscaler/issues/1158,Update vertical-pod-autoscaler base image to debian-base-amd:0.3.2,"
Following #1155 to update base image to address CVEs, but only for VPA.",closed,True,2018-08-17 18:34:29,2018-08-20 13:14:29
autoscaler,wangxy518,https://github.com/kubernetes/autoscaler/pull/1159,https://api.github.com/repos/kubernetes/autoscaler/issues/1159,Update README.md,these urls have been changed.,closed,True,2018-08-20 08:14:33,2018-08-20 10:59:26
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1160,https://api.github.com/repos/kubernetes/autoscaler/issues/1160,Remove discoveryOptions from GKE manager constructor,Remove discoveryOptions from GKE manager constructor,closed,True,2018-08-20 09:19:43,2018-08-20 09:57:23
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/issues/1161,https://api.github.com/repos/kubernetes/autoscaler/issues/1161,Expose culprit code path which resulted in an error in metrics,"Currently we are reporting errors counts grouping them just by one axis. Each reported error is one of:
* cloudProviderError
* apiCallError
* internalError
* transientError

as defined in: https://github.com/kubernetes/autoscaler/blob/5c4693f95f37e37dc135cc4c3f69978a22545104/cluster-autoscaler/utils/errors/errors.go#L48

This does not give us information which code path resulted in an error. To address that we could extend the errors counts metrics with another label. Let's call ""operation"" and fill it in with values which marks code path which was source of an error.",closed,False,2018-08-20 11:05:05,2019-02-09 10:56:19
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1162,https://api.github.com/repos/kubernetes/autoscaler/issues/1162,Cherry pick of #1155: Update Cluster Autoscaler base image,"Cherry pick of #1155: Update Cluster Autoscaler base image to debian-base-amd:0.3.2

",closed,True,2018-08-20 11:11:51,2018-08-20 11:48:47
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1163,https://api.github.com/repos/kubernetes/autoscaler/issues/1163,Cherry pick of #1155: Update Cluster Autoscaler base image,"Cherry pick of #1155: Update Cluster Autoscaler base image to debian-base-amd:0.3.2

",closed,True,2018-08-20 11:12:51,2018-08-20 11:48:55
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/issues/1164,https://api.github.com/repos/kubernetes/autoscaler/issues/1164,Use informer to list VPA checkpoints.,"We call API server directly to list VPA checkpoints. We should switch to using informer:
https://github.com/kubernetes/autoscaler/blob/9feb612e2200e04c2ac8f7dd7cc8e7adbe300add/vertical-pod-autoscaler/pkg/recommender/input/cluster_feeder.go#L190-L192",closed,False,2018-08-20 14:19:11,2018-09-10 19:46:37
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/issues/1165,https://api.github.com/repos/kubernetes/autoscaler/issues/1165,Check if VPA object update is needed.,"Object updates are costly. We should check if the object has changed before trying to perform an update. 
We can also skip updates if recommendation delta is small and conditions haven't changed.
https://github.com/kubernetes/autoscaler/blob/9feb612e2200e04c2ac8f7dd7cc8e7adbe300add/vertical-pod-autoscaler/pkg/utils/vpa/api.go#L58:6",closed,False,2018-08-20 14:21:39,2018-11-09 12:23:18
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1166,https://api.github.com/repos/kubernetes/autoscaler/issues/1166,Add authorization to vpa e2e tests.,@MaciekPytel ,closed,True,2018-08-20 18:07:27,2018-08-20 18:38:03
autoscaler,phsiao,https://github.com/kubernetes/autoscaler/issues/1167,https://api.github.com/repos/kubernetes/autoscaler/issues/1167,vpa kubernetes client should initialize version for better logging and auditing,"
Looks like vpa clients are not initialized with proper client version information so it shows up like this

```
I0820 19:48:18.095862       7 round_trippers.go:386] curl -k -v -XPATCH  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json-patch+json"" -H ""User-Agent: recommender/v0.0.0 (linux/amd64) kubernetes/$Format""
```

where version is shown as v0.0.0, and with the $Format.",closed,False,2018-08-20 19:52:10,2019-02-09 10:56:18
autoscaler,yguo0905,https://github.com/kubernetes/autoscaler/pull/1168,https://api.github.com/repos/kubernetes/autoscaler/issues/1168,Ignore resources with Cloud TPU prefix,"In this way, we don't need to change this code to support
1. Preemptible Cloud TPU (e.g., `cloud-tpus.google.com/preemptible-v2`).
2. New TPU generations (e.g., `cloud-tpus.google.com/v3`).",closed,True,2018-08-21 00:34:32,2018-08-21 08:56:15
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1169,https://api.github.com/repos/kubernetes/autoscaler/issues/1169,Fix GKE operation wait timeout,Accidentally used GCE timeout for GKE operations.,closed,True,2018-08-21 09:28:42,2018-08-21 09:49:19
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1170,https://api.github.com/repos/kubernetes/autoscaler/issues/1170,VPA Godeps update,,closed,True,2018-08-21 10:44:27,2018-08-21 10:56:29
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1171,https://api.github.com/repos/kubernetes/autoscaler/issues/1171,Use Go 1.10.3,Use Go 1.10.3 as older version causes test-in-docker to fail.,closed,True,2018-08-21 11:21:06,2018-08-21 20:46:08
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1172,https://api.github.com/repos/kubernetes/autoscaler/issues/1172,Cherry-pick of #791: Ignore TPU resource in simulations,Cherry-pick of #791: Ignore TPU resource in simulations,closed,True,2018-08-21 11:22:15,2018-08-22 12:04:32
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1173,https://api.github.com/repos/kubernetes/autoscaler/issues/1173,Cherry-pick of #1168: Ignore resources with Cloud TPU prefix,"Cherry-pick of #1168: Ignore resources with Cloud TPU prefix

cc @yguo0905 ",closed,True,2018-08-21 11:35:48,2018-08-21 14:34:12
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1174,https://api.github.com/repos/kubernetes/autoscaler/issues/1174,Update owners files for autoscaling,fixes #1111,closed,True,2018-08-21 14:00:09,2018-08-21 20:45:05
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1175,https://api.github.com/repos/kubernetes/autoscaler/issues/1175,Fix fetching resource limits on GKE,Add checking limiter is not nil. Log warnings and errors if it is.,closed,True,2018-08-21 14:05:08,2018-08-22 11:01:56
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1176,https://api.github.com/repos/kubernetes/autoscaler/issues/1176,Cluster Autoscaler 1.3.2-beta.2,Update Cluster Autoscaler version to 1.3.2-beta.2,closed,True,2018-08-21 14:36:57,2018-08-21 14:55:37
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1177,https://api.github.com/repos/kubernetes/autoscaler/issues/1177,Cherry-pick of #1168: Ignore resources with Cloud TPU prefix,Cherry-pick of #1168: Ignore resources with Cloud TPU prefix,closed,True,2018-08-22 12:09:05,2018-08-22 12:32:16
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1178,https://api.github.com/repos/kubernetes/autoscaler/issues/1178,Cluster Autoscaler 1.2.3-beta.2,Update Cluster Autoscaler version to 1.2.3-beta.2,closed,True,2018-08-22 12:34:30,2018-08-22 14:10:27
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1179,https://api.github.com/repos/kubernetes/autoscaler/issues/1179,Handle channel closes from pod event watcher,"Issue: After ~30 minutes pod eviction event watch channel is closed. We are reading from a closed channel in an infinite loop that burns cpu. Causes recommender to start eating way too much cpu ~30 mins after start.
Fix: Watch for channel closure and restart the watch.",closed,True,2018-08-22 13:36:01,2018-08-22 13:50:10
autoscaler,romanlutz,https://github.com/kubernetes/autoscaler/pull/1180,https://api.github.com/repos/kubernetes/autoscaler/issues/1180,Add batch jobs and cronjobs to Azure cluster-autoscaler,"Jobs and CronJobs were both missing so far which makes the cluster-autoscaler complain that it doesn't have access to ""batch"".",closed,True,2018-08-22 22:55:48,2018-08-26 22:56:03
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/issues/1181,https://api.github.com/repos/kubernetes/autoscaler/issues/1181,VPA object should have condition to signalize no matching pods ,"When the label selector on VPA doesn't match any pods in the cluster, the recomender should set a condition on VPA object to indicate this as it will cause us not to provide recommendations.",open,False,2018-08-23 09:23:18,2019-03-20 17:18:50
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/issues/1182,https://api.github.com/repos/kubernetes/autoscaler/issues/1182,Cap recommendation upperBound to some sane value.,"When we start providing recommendation for a VPA object, initially the upperBound is ridiculously high. This is confusing to the users and provides no information. We should cap this value to sth sane.

Some ideas are to cap upperBound to target recommendation +X cores or target recommendation + XX% of cores (analogous for memory).",closed,False,2018-08-23 09:27:46,2019-02-01 16:16:17
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1183,https://api.github.com/repos/kubernetes/autoscaler/issues/1183,Initialize eviction restriction with pods controlled by VPA.,"Additionally, remove failed and succeeded pods from live pods list.",closed,True,2018-08-23 13:57:45,2018-08-27 15:04:53
autoscaler,fdornberger,https://github.com/kubernetes/autoscaler/issues/1184,https://api.github.com/repos/kubernetes/autoscaler/issues/1184,"Cluster-autoscaler not considering the resource ""pod"" to scale upwards","I'm using the CA on an AWS EKS cluster. 

In the dev environment, I use t2.small instances which serve the purpose. The resource footprint of my apps in this environment is fairly low, which is why my pods could easily fit on 2 t2.small instances but AWS has a limit with regards to how many pods can be installed on a given instance type. This list can be found here: https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt

With 2 t2.small instances, I can thus host up to 16 pods before AWS no longer allows new pods to be scheduled.

Since I have more than 16 pods that should be deployed, some apps show this status:

```
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  1m (x330 over 1h)  default-scheduler  0/2 nodes are available: 1 Insufficient memory, 2 Insufficient pods.
```

The cluster auto-scaler determines he can't satisfy the apps needs even by scaling up the nodes by 1 as it doesn't seem to take ""pods"" as an allowed resource based on each instance. Understood that I ran out of memory to serve all the pods as of now, but scaling would solve this issue. The auto-scaler doesn't seem to know that scaling up instances will also allow the max-amount of pods that can be run on a cluster.

Manually provisioning an new EC2 instance does help but that is not the goal of using the cluster-autoscaler.

Did anyone solve that problem already?",closed,False,2018-08-23 14:38:35,2018-08-24 15:57:39
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1185,https://api.github.com/repos/kubernetes/autoscaler/issues/1185,Report reason why pod didn't trigger scale-up,"Report reason why pod didn't trigger scale-up in NotTriggeredScaleUp event. Format is similar to FailedScheduling, i.e. list of ""\<failure reason\> (\<number of affected node groups\>)"".

This can be further improved by:
- detailing reason why node group was skipped (e.g. max size reached, not healthy),
- communicating why NAP couldn't create a node group,
- passing raw predicate errors plus the above information to scale up status processor, so it can decide the message format.",closed,True,2018-08-23 15:50:59,2018-08-28 12:25:49
autoscaler,javier-b-perez,https://github.com/kubernetes/autoscaler/pull/1186,https://api.github.com/repos/kubernetes/autoscaler/issues/1186,cluster-autoscaler: modify dockerfile and makefile to support args,Add new target to only build docker image using docker-in-docker.,closed,True,2018-08-23 22:52:05,2018-08-31 14:22:43
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1187,https://api.github.com/repos/kubernetes/autoscaler/issues/1187,Compare in overflow-safe way,"This also allows using math.MaxInt64 as ""no upper bound"".",closed,True,2018-08-24 10:46:41,2018-08-24 11:01:56
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1188,https://api.github.com/repos/kubernetes/autoscaler/issues/1188,Pass NoScaleUpInfo to ScaleUpStatus processor,Follow-up to #1185.,closed,True,2018-08-24 15:18:16,2018-08-28 16:36:16
autoscaler,romanlutz,https://github.com/kubernetes/autoscaler/pull/1189,https://api.github.com/repos/kubernetes/autoscaler/issues/1189,Add batch jobs and cronjobs to Azure cluster-autoscaler,"Jobs and CronJobs were both missing so far which makes the cluster-autoscaler complain that it doesn't have access to ""batch"".",closed,True,2018-08-26 23:06:12,2018-08-27 09:14:02
autoscaler,koooge,https://github.com/kubernetes/autoscaler/pull/1190,https://api.github.com/repos/kubernetes/autoscaler/issues/1190,[doc] Replace to link,Replace duplicated doc to a link,closed,True,2018-08-27 02:42:22,2018-08-27 13:41:34
autoscaler,koooge,https://github.com/kubernetes/autoscaler/pull/1191,https://api.github.com/repos/kubernetes/autoscaler/issues/1191,[examples] Update resource version,"Hi. 
Use v1 version in examples. apps/v1 may be just alias of exensions/v1beta1 though.
",closed,True,2018-08-27 03:20:33,2018-08-31 17:00:59
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1192,https://api.github.com/repos/kubernetes/autoscaler/issues/1192,Update README,Add 1.3 / K8s 1.11 to recommended release combinations. Add link to release notes for recent releases. Rephrase a couple other sentences.,closed,True,2018-08-27 12:21:31,2018-08-29 19:24:34
autoscaler,towca,https://github.com/kubernetes/autoscaler/pull/1193,https://api.github.com/repos/kubernetes/autoscaler/issues/1193,Fix GetClusterSize to return actual size in line with the rest of CSR,"It returned the number of registered nodes, but should return the number
of provisioned nodes instead.",closed,True,2018-08-27 13:01:13,2018-08-27 13:31:02
autoscaler,towca,https://github.com/kubernetes/autoscaler/pull/1194,https://api.github.com/repos/kubernetes/autoscaler/issues/1194,"Make the GKE Mig struct public, add a method to retrieve its name",,closed,True,2018-08-27 13:12:13,2018-08-27 15:30:24
autoscaler,danbeaulieu,https://github.com/kubernetes/autoscaler/issues/1195,https://api.github.com/repos/kubernetes/autoscaler/issues/1195,Several puzzles with AWS cluster autoscaler,"Hi I am prototyping a cluster on AWS using k8s 1.11.z. I get similar results for CA 1.2.2 and the 1.3 beta.

I have a 3 node control plane running t2 mediums created via kubeadm. I am NOT using cloud-provider=aws for any of the control plane services as we found it introduces a lot of magic that complicates our architecture. We do however set cloud-provider=aws for CA. 

I have a worker node ASG (danb-test) with the correct tags and it is scaled to 0. So a 3 node control plane with no worker nodes and cluster auto scaler is running on the control plane.

I started a pod to test CA. I'd expect it to identify my worker node ASG (it does), figure out no nodes are running in the ASG, and scale it up.

```
kubectl run testpod --image=ubuntu:16.04 bash
```

Below is the log that illustrates some of my puzzles.

```
I0827 13:01:50.249429       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0827 13:01:51.120193       1 static_autoscaler.go:131] Starting main loop
I0827 13:01:51.343324       1 auto_scaling_groups.go:245] Regenerating instance to ASG map for ASGs: [danb-test]
I0827 13:01:51.420342       1 aws_manager.go:125] Refreshed ASG list, next refresh after 2018-08-27 13:02:01.420334282 +0000 UTC m=+228.570627650
W0827 13:01:51.509220       1 clusterstate.go:517] Failed to get nodegroup for i-0253e1488de4b1f8c: Wrong id: expected format aws:///<zone>/<name>, got 
W0827 13:01:51.509266       1 clusterstate.go:517] Failed to get nodegroup for i-0ecf671292db11881: Wrong id: expected format aws:///<zone>/<name>, got 
W0827 13:01:51.509274       1 clusterstate.go:517] Failed to get nodegroup for i-065d19525b16ec0b4: Wrong id: expected format aws:///<zone>/<name>, got 
I0827 13:01:51.509363       1 utils.go:486] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0827 13:01:51.509378       1 static_autoscaler.go:244] Filtering out schedulables
I0827 13:01:51.509517       1 static_autoscaler.go:254] No schedulable pods
I0827 13:01:51.509540       1 scale_up.go:60] Pod default/testpod-74cdb45bf5-rwc72 is unschedulable
E0827 13:01:51.509560       1 static_autoscaler.go:283] Failed to scale up: failed to build node infos for node groups: Wrong id: expected format aws:///<zone>/<name>, got 
```

1) i-0253e1488de4b1f8c, i-0ecf671292db11881, and i-065d19525b16ec0b4 are all control plane nodes. They are in an ASG but that ASG is not tagged for discovery by CA. Why is CA doing anything with them?
2) The log indicates the testpod pod is unschedulable, why is this? Do I need to annotate my pod, etc?
3) Since I don't set cloud-provider=aws for the control plane services my nodes don't have a providerID. Is there a reason why the providerID needs to be set? 
",closed,False,2018-08-27 13:14:31,2019-02-07 22:18:30
autoscaler,ingvagabund,https://github.com/kubernetes/autoscaler/issues/1196,https://api.github.com/repos/kubernetes/autoscaler/issues/1196,godep restore complains about non-existing revisions,"When running `godep restore` over a clean `$GOPATH/src` (export GOPATH=/home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler) of the master HEAD (https://github.com/kubernetes/autoscaler/commit/9ef1336fc0b9ed110c3e2b1a12a0233f7592b3a5), I get the following error:

```
$ time godep restore
godep: [WARNING]: godep should only be used inside a valid go package directory and
godep: [WARNING]: may not function correctly. You are probably outside of your $GOPATH.
godep: [WARNING]:	Current Directory: /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler
godep: [WARNING]:	$GOPATH: /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/Azure/azure-sdk-for-go; git checkout 62308f62165c69a28170b0392a20c9c72dbe7379
fatal: reference is not a tree: 62308f62165c69a28170b0392a20c9c72dbe7379
godep: error downloading dep (github.com/Azure/azure-sdk-for-go/services/compute/mgmt/2017-12-01/compute): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/Azure/go-autorest; git checkout d4ecd8a0def06a087762abe233fd6a5039dbba86
fatal: reference is not a tree: d4ecd8a0def06a087762abe233fd6a5039dbba86
godep: error downloading dep (github.com/Azure/go-autorest/autorest): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/aws/aws-sdk-go; git checkout b333379714ba5da996341d0e657c3761d9810fc5
fatal: reference is not a tree: b333379714ba5da996341d0e657c3761d9810fc5
godep: error downloading dep (github.com/aws/aws-sdk-go/aws): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/containerd/containerd; git checkout d652f2da42f57ad81cede93aace6507273ef3891
fatal: reference is not a tree: d652f2da42f57ad81cede93aace6507273ef3891
godep: error downloading dep (github.com/containerd/containerd/api/services/containers/v1): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/containernetworking/cni; git checkout ae9c5b045bbd6b1f463c9d0740da02975ce59500
fatal: reference is not a tree: ae9c5b045bbd6b1f463c9d0740da02975ce59500
godep: error downloading dep (github.com/containernetworking/cni/libcni): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/coreos/etcd; git checkout 4dd89663c21a4cd3467bee9854a8dd51e680d742
fatal: reference is not a tree: 4dd89663c21a4cd3467bee9854a8dd51e680d742
godep: error downloading dep (github.com/coreos/etcd/client): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/coreos/rkt; git checkout 7d7e2f5917503f71eb1dd6b463be2e28ee4f6b9e
fatal: reference is not a tree: 7d7e2f5917503f71eb1dd6b463be2e28ee4f6b9e
godep: error downloading dep (github.com/coreos/rkt/api/v1alpha): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/cyphar/filepath-securejoin; git checkout ee18807fcd75e3884022d37c779d99ad5c50d24b
fatal: reference is not a tree: ee18807fcd75e3884022d37c779d99ad5c50d24b
godep: error downloading dep (github.com/cyphar/filepath-securejoin): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/docker/distribution; git checkout 5418b72312947138a4ea6c72d79525d5cabcf6a8
fatal: reference is not a tree: 5418b72312947138a4ea6c72d79525d5cabcf6a8
godep: error downloading dep (github.com/docker/distribution/digestset): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/docker/docker; git checkout 15fd5902a7fb1a4ae8c7599dc09d4c6cdee96b57
fatal: reference is not a tree: 15fd5902a7fb1a4ae8c7599dc09d4c6cdee96b57
godep: error downloading dep (github.com/docker/docker/api): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/docker/libnetwork; git checkout 3731ca901b630c88ec4e24e1b43e7a29e70f6a8c
fatal: reference is not a tree: 3731ca901b630c88ec4e24e1b43e7a29e70f6a8c
godep: error downloading dep (github.com/docker/libnetwork/ipvs): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/google/cadvisor; git checkout c28fac629f75bc6d86fa75e81f906553496a0843
fatal: reference is not a tree: c28fac629f75bc6d86fa75e81f906553496a0843
godep: error downloading dep (github.com/google/cadvisor/accelerators): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/libopenstorage/openstorage; git checkout 57514627b074af023bf6bcf4d6408229c78cc73d
fatal: reference is not a tree: 57514627b074af023bf6bcf4d6408229c78cc73d
godep: error downloading dep (github.com/libopenstorage/openstorage/api): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/opencontainers/runc; git checkout 8a234707cf8e8adbd7fcb330b3bba294e81af1f1
fatal: reference is not a tree: 8a234707cf8e8adbd7fcb330b3bba294e81af1f1
godep: error downloading dep (github.com/opencontainers/runc/libcontainer): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/rancher/go-rancher; git checkout 6d1b1e585b2bdb49b30e0dcb3139f301336fbff3
fatal: reference is not a tree: 6d1b1e585b2bdb49b30e0dcb3139f301336fbff3
godep: error downloading dep (github.com/rancher/go-rancher/client): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/rubiojr/go-vhd; git checkout 262778108aaf0fcbb1f872e99c9c5dfbd3e4ddc5
fatal: reference is not a tree: 262778108aaf0fcbb1f872e99c9c5dfbd3e4ddc5
godep: error downloading dep (github.com/rubiojr/go-vhd/vhd): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/stretchr/testify; git checkout 043b0b2c152ad47e708135af8e4b98165d4e7353
fatal: reference is not a tree: 043b0b2c152ad47e708135af8e4b98165d4e7353
godep: error downloading dep (github.com/stretchr/testify/assert): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/github.com/vmware/govmomi; git checkout 6dbb3df5fa3cd978f11822008b876e745cb352ad
fatal: reference is not a tree: 6dbb3df5fa3cd978f11822008b876e745cb352ad
godep: error downloading dep (github.com/vmware/govmomi/find): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/golang.org/x/exp; git checkout b0e94b3113450c88b8d7558f006bb7a007fb0419
fatal: reference is not a tree: b0e94b3113450c88b8d7558f006bb7a007fb0419
godep: error downloading dep (golang.org/x/exp/inotify): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/google.golang.org/api; git checkout 1c014d4a0194dbca282b44f3c2a9b6461cad529a
fatal: reference is not a tree: 1c014d4a0194dbca282b44f3c2a9b6461cad529a
godep: error downloading dep (google.golang.org/api/compute/v0.alpha): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/k8s.io/api; git checkout bb34df88c8239da7694e94a4fe3e8f3d4d31553f
fatal: reference is not a tree: bb34df88c8239da7694e94a4fe3e8f3d4d31553f
godep: error downloading dep (k8s.io/api/admission/v1beta1): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/k8s.io/apiextensions-apiserver; git checkout 590ad7a610968e905328179c7903d118f604d5bb
fatal: reference is not a tree: 590ad7a610968e905328179c7903d118f604d5bb
godep: error downloading dep (k8s.io/apiextensions-apiserver/pkg/features): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/k8s.io/apimachinery; git checkout 5e83b0fbb51d8a7f0185f1677fd4690d2ea8563d
fatal: reference is not a tree: 5e83b0fbb51d8a7f0185f1677fd4690d2ea8563d
godep: error downloading dep (k8s.io/apimachinery/pkg/api/equality): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/k8s.io/apiserver; git checkout 57306a8a847197d422e50e7f001094cf48b609aa
fatal: reference is not a tree: 57306a8a847197d422e50e7f001094cf48b609aa
godep: error downloading dep (k8s.io/apiserver/pkg/admission): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/k8s.io/client-go; git checkout 4d3b54463bd5ed422684015c99e806f571e4968b
fatal: reference is not a tree: 4d3b54463bd5ed422684015c99e806f571e4968b
godep: error downloading dep (k8s.io/client-go/discovery): exit status 128
# cd /home/jchaloup/Projects/src/k8s.io/autoscaler/cluster-autoscaler/src/k8s.io/kubernetes; git checkout b39f3669c29ea206ada649a237354d42095ae55a
fatal: reference is not a tree: b39f3669c29ea206ada649a237354d42095ae55a
godep: error downloading dep (k8s.io/kubernetes/cmd/kube-proxy/app): exit status 128
godep: Error downloading some deps. Aborting restore and check.

real	15m21.319s
user	3m43.330s
sys	1m2.677s
```

That should not happen no matter how you generate the Godeps.json file.",closed,False,2018-08-28 10:44:22,2018-09-10 19:44:03
autoscaler,hello2mao,https://github.com/kubernetes/autoscaler/pull/1197,https://api.github.com/repos/kubernetes/autoscaler/issues/1197,Fix bad syntax for struct tag value in prometheus_util.go,This PR fixes bad syntax for struct tag value in prometheus_util.go.,closed,True,2018-08-29 11:19:17,2018-08-29 13:14:25
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1198,https://api.github.com/repos/kubernetes/autoscaler/issues/1198,Clean up eviction admission no VPAs in the cluster ,"We need to make sure, that even if there are no VPAs, Eviction Admission will correctly initialize its state. ",closed,True,2018-08-30 08:58:36,2018-08-30 11:04:13
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1199,https://api.github.com/repos/kubernetes/autoscaler/issues/1199,Check for ready condition not true,"Check for ready condition not true instead of just false.

/cc @losipiuk ",closed,True,2018-08-30 10:24:56,2018-08-30 12:39:24
autoscaler,kkmsft,https://github.com/kubernetes/autoscaler/pull/1200,https://api.github.com/repos/kubernetes/autoscaler/issues/1200,Code cleanup - use const from the package.,,closed,True,2018-08-31 06:04:47,2018-08-31 10:25:19
autoscaler,phsiao,https://github.com/kubernetes/autoscaler/pull/1201,https://api.github.com/repos/kubernetes/autoscaler/issues/1201,WIP: VPA object status reflects no matching pods,"
Working on solution for #1181 ",closed,True,2018-08-31 20:16:29,2018-10-17 11:28:07
autoscaler,AdamDang,https://github.com/kubernetes/autoscaler/pull/1202,https://api.github.com/repos/kubernetes/autoscaler/issues/1202,Correct the returned message,Line 179: checkpotint->checkpoint,closed,True,2018-09-01 07:39:03,2018-09-03 08:47:15
autoscaler,mikeweiwei,https://github.com/kubernetes/autoscaler/pull/1203,https://api.github.com/repos/kubernetes/autoscaler/issues/1203,update resource version,update resource version ,closed,True,2018-09-03 03:15:25,2018-09-03 14:40:30
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1204,https://api.github.com/repos/kubernetes/autoscaler/issues/1204,Update VPA status only when needed.,"Adds ObservedVpas to cluster state as a baseline for status comparison.
Removes some excessive logging (Each VPA per loop, each Pod per loop).",closed,True,2018-09-03 14:48:10,2018-09-04 10:36:09
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1205,https://api.github.com/repos/kubernetes/autoscaler/issues/1205,Add caching node locations in GKE cloud provider,Refresh node locations when refreshing other GKE resources. Add a method for getting it from cloud provider. Use zone from system labels when constructing new node group in GKE cloud provider.,closed,True,2018-09-03 16:28:11,2018-09-04 14:43:23
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1206,https://api.github.com/repos/kubernetes/autoscaler/issues/1206,Fix setting last GC time,@schylek,closed,True,2018-09-03 17:13:42,2018-09-03 17:45:16
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1207,https://api.github.com/repos/kubernetes/autoscaler/issues/1207,Updates to VPA readme,"Add info on possible use with HPA on custom metrics.
Fix some nits.",closed,True,2018-09-03 17:44:56,2018-09-04 09:45:03
autoscaler,frittentheke,https://github.com/kubernetes/autoscaler/issues/1208,https://api.github.com/repos/kubernetes/autoscaler/issues/1208,Cluster-Autoscaler should query AWS meta-data to determine AWS_REGION if not set,"Currently, the cluster-autoscaler requires the AWS_REGION variable to be defined. Surely this can be done statically in the YAML or via some init container calling the meta-data service from AWS or even by using the downwardAPI at some point in the future (https://github.com/kubernetes/kubernetes/issues/40610).

But with auto-discovery of the node ASG by tags and the whole scaling up and down depending on access to the AWS API anyways, why not use the ec2metadata (https://docs.aws.amazon.com/sdk-for-go/api/aws/ec2metadata/) to determine in which region we are running? Sure it should be possible to still configure this variable statically, but I believe this is then one less thing to configure when starting with cluster-autoscaling on AWS and allows the configuration to be even simpler, no matter in what region the Kubernetes Cluster lives ...",closed,False,2018-09-04 07:34:05,2019-02-04 21:33:54
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1209,https://api.github.com/repos/kubernetes/autoscaler/issues/1209,Cluster Autoscaler 1.3.2,Update Cluster Autoscaler version to 1.3.2,closed,True,2018-09-04 08:35:57,2018-09-04 08:51:54
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1210,https://api.github.com/repos/kubernetes/autoscaler/issues/1210,Remove obsolete dead code,,closed,True,2018-09-04 11:44:35,2018-09-05 13:32:58
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1211,https://api.github.com/repos/kubernetes/autoscaler/issues/1211,Add HasMinLimitSet and HasMaxLimitSet to methods ResourceLimiter,,closed,True,2018-09-04 12:11:39,2018-09-04 12:28:04
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1212,https://api.github.com/repos/kubernetes/autoscaler/issues/1212,Reduce GKE API calls,"Replace 2 GET cluster and 1 LIST node pools call with a single GET cluster on forceRefresh().

This also changes LIST node pools call to GET cluster call after creating/deleting node pool. I don't think it should have much performance impact - the object is larger, but it counts as a full refresh, so it reduces the total number of calls. @MaciekPytel WDYT?",closed,True,2018-09-04 14:55:17,2018-09-04 15:27:41
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1213,https://api.github.com/repos/kubernetes/autoscaler/issues/1213,Allow running hack/verify-spelling.sh from any directory,"Previously if script was not run from root repository directory it
was failing with following error:

```
can't load package: package k8s.io/github.com/client9/misspell/cmd/misspell: cannot find package ""k8s.io/github.com/client9/misspell/cmd/misspell"" in any of:
  ...GPATH ENTRIES...
```",closed,True,2018-09-04 19:29:25,2018-09-07 12:35:00
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1214,https://api.github.com/repos/kubernetes/autoscaler/issues/1214,Add a method for inserting custom test node group to test cloud provider,Add a method for inserting real node group to test cloud provider.,closed,True,2018-09-05 10:36:59,2018-09-05 13:57:45
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1215,https://api.github.com/repos/kubernetes/autoscaler/issues/1215,Update godeps updating recipe.,,closed,True,2018-09-05 14:10:52,2018-09-05 14:56:33
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1216,https://api.github.com/repos/kubernetes/autoscaler/issues/1216,Update godeps,,closed,True,2018-09-05 14:18:55,2018-09-06 08:54:10
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1217,https://api.github.com/repos/kubernetes/autoscaler/issues/1217,Cluster Autoscaler 1.2.3,,closed,True,2018-09-05 15:35:50,2018-09-06 09:09:41
autoscaler,mateusz,https://github.com/kubernetes/autoscaler/issues/1218,https://api.github.com/repos/kubernetes/autoscaler/issues/1218,[AWS/EKS] node group not ready for scale up / pod wouldn't fit,"I've set the autoscaler from the helm chart on AWS EKS:

```
helm install --namespace kube-system stable/cluster-autoscaler --name autoscaler \
--set sslCertPath=/etc/ssl/certs/ca-bundle.crt \
--set rbac.create=true \
--set awsRegion=us-west-2 \
--set image.tag=v1.3.1 \
--set autoDiscovery.clusterName=plague-ssdemo-eks2
```

Then I stressed the cluster to run out of resources - the error in kube on the pod is `0/2 nodes are available: 1 Insufficient memory, 2 Insufficient pods.` . IPs have ran out on both nodes, and memory on one:

![screen shot 2018-09-06 at 12 18 06 pm](https://user-images.githubusercontent.com/118653/45127913-f238fb00-b1ce-11e8-9a62-219ba1fa7a48.png)

The autoscaler is not triggering scale up though. In short the logs say:

* `Node group ... is not ready for scaleup`
* `pod didn't trigger scale-up (it wouldn't fit if a new node is added)`

Note when I add the new node manually via AWS's ASG (set desired to 3), the new pod comes up right away - so the ""it wouldn't fit if a new node is added"" part is definitely untrue.

The log on the autoscaler pod is as follows, and just repeats:

```
I0906 00:12:32.502615       1 flags.go:52] FLAG: --address="":8085""
I0906 00:12:32.502666       1 flags.go:52] FLAG: --alsologtostderr=""false""
I0906 00:12:32.502672       1 flags.go:52] FLAG: --application-metrics-count-limit=""100""
I0906 00:12:32.502676       1 flags.go:52] FLAG: --azure-container-registry-config=""""
I0906 00:12:32.502690       1 flags.go:52] FLAG: --balance-similar-node-groups=""false""
I0906 00:12:32.502694       1 flags.go:52] FLAG: --boot-id-file=""/proc/sys/kernel/random/boot_id""
I0906 00:12:32.502699       1 flags.go:52] FLAG: --cloud-config=""""
I0906 00:12:32.502703       1 flags.go:52] FLAG: --cloud-provider=""aws""
I0906 00:12:32.502707       1 flags.go:52] FLAG: --cloud-provider-gce-lb-src-cidrs=""130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16""
I0906 00:12:32.502718       1 flags.go:52] FLAG: --cluster-name=""""
I0906 00:12:32.502722       1 flags.go:52] FLAG: --container-hints=""/etc/cadvisor/container_hints.json""
I0906 00:12:32.502727       1 flags.go:52] FLAG: --containerd=""unix:///var/run/containerd.sock""
I0906 00:12:32.502731       1 flags.go:52] FLAG: --cores-total=""0:320000""
I0906 00:12:32.502735       1 flags.go:52] FLAG: --docker=""unix:///var/run/docker.sock""
I0906 00:12:32.502740       1 flags.go:52] FLAG: --docker-env-metadata-whitelist=""""
I0906 00:12:32.502744       1 flags.go:52] FLAG: --docker-only=""false""
I0906 00:12:32.502748       1 flags.go:52] FLAG: --docker-root=""/var/lib/docker""
I0906 00:12:32.502752       1 flags.go:52] FLAG: --docker-tls=""false""
I0906 00:12:32.502756       1 flags.go:52] FLAG: --docker-tls-ca=""ca.pem""
I0906 00:12:32.502760       1 flags.go:52] FLAG: --docker-tls-cert=""cert.pem""
I0906 00:12:32.502765       1 flags.go:52] FLAG: --docker-tls-key=""key.pem""
I0906 00:12:32.502769       1 flags.go:52] FLAG: --enable-load-reader=""false""
I0906 00:12:32.502773       1 flags.go:52] FLAG: --estimator=""binpacking""
I0906 00:12:32.502778       1 flags.go:52] FLAG: --event-storage-age-limit=""default=0""
I0906 00:12:32.502783       1 flags.go:52] FLAG: --event-storage-event-limit=""default=0""
I0906 00:12:32.502788       1 flags.go:52] FLAG: --expander=""random""
I0906 00:12:32.502792       1 flags.go:52] FLAG: --expendable-pods-priority-cutoff=""0""
I0906 00:12:32.502797       1 flags.go:52] FLAG: --gke-api-endpoint=""""
I0906 00:12:32.502803       1 flags.go:52] FLAG: --global-housekeeping-interval=""1m0s""
I0906 00:12:32.502809       1 flags.go:52] FLAG: --google-json-key=""""
I0906 00:12:32.502815       1 flags.go:52] FLAG: --gpu-total=""[]""
I0906 00:12:32.502821       1 flags.go:52] FLAG: --housekeeping-interval=""10s""
I0906 00:12:32.502827       1 flags.go:52] FLAG: --httptest.serve=""""
I0906 00:12:32.502832       1 flags.go:52] FLAG: --kubeconfig=""""
I0906 00:12:32.502838       1 flags.go:52] FLAG: --kubernetes=""""
I0906 00:12:32.502844       1 flags.go:52] FLAG: --leader-elect=""true""
I0906 00:12:32.502852       1 flags.go:52] FLAG: --leader-elect-lease-duration=""15s""
I0906 00:12:32.502857       1 flags.go:52] FLAG: --leader-elect-renew-deadline=""10s""
I0906 00:12:32.502863       1 flags.go:52] FLAG: --leader-elect-resource-lock=""endpoints""
I0906 00:12:32.502869       1 flags.go:52] FLAG: --leader-elect-retry-period=""2s""
I0906 00:12:32.502875       1 flags.go:52] FLAG: --log-backtrace-at="":0""
I0906 00:12:32.502883       1 flags.go:52] FLAG: --log-cadvisor-usage=""false""
I0906 00:12:32.502888       1 flags.go:52] FLAG: --log-dir=""""
I0906 00:12:32.502893       1 flags.go:52] FLAG: --log-flush-frequency=""5s""
I0906 00:12:32.502899       1 flags.go:52] FLAG: --logtostderr=""true""
I0906 00:12:32.502905       1 flags.go:52] FLAG: --machine-id-file=""/etc/machine-id,/var/lib/dbus/machine-id""
I0906 00:12:32.502913       1 flags.go:52] FLAG: --max-autoprovisioned-node-group-count=""15""
I0906 00:12:32.502917       1 flags.go:52] FLAG: --max-empty-bulk-delete=""10""
I0906 00:12:32.502921       1 flags.go:52] FLAG: --max-failing-time=""15m0s""
I0906 00:12:32.502925       1 flags.go:52] FLAG: --max-graceful-termination-sec=""600""
I0906 00:12:32.502930       1 flags.go:52] FLAG: --max-inactivity=""10m0s""
I0906 00:12:32.502935       1 flags.go:52] FLAG: --max-node-provision-time=""15m0s""
I0906 00:12:32.502940       1 flags.go:52] FLAG: --max-nodes-total=""0""
I0906 00:12:32.502943       1 flags.go:52] FLAG: --max-total-unready-percentage=""45""
I0906 00:12:32.502948       1 flags.go:52] FLAG: --memory-total=""0:6400000""
I0906 00:12:32.502954       1 flags.go:52] FLAG: --min-replica-count=""0""
I0906 00:12:32.502958       1 flags.go:52] FLAG: --namespace=""kube-system""
I0906 00:12:32.502962       1 flags.go:52] FLAG: --node-autoprovisioning-enabled=""false""
I0906 00:12:32.502966       1 flags.go:52] FLAG: --node-group-auto-discovery=""[asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/plague-ssdemo-eks2]""
I0906 00:12:32.502978       1 flags.go:52] FLAG: --nodes=""[]""
I0906 00:12:32.502982       1 flags.go:52] FLAG: --ok-total-unready-count=""3""
I0906 00:12:32.502986       1 flags.go:52] FLAG: --regional=""false""
I0906 00:12:32.502990       1 flags.go:52] FLAG: --scale-down-candidates-pool-min-count=""50""
I0906 00:12:32.502994       1 flags.go:52] FLAG: --scale-down-candidates-pool-ratio=""0.1""
I0906 00:12:32.502999       1 flags.go:52] FLAG: --scale-down-delay-after-add=""10m0s""
I0906 00:12:32.503003       1 flags.go:52] FLAG: --scale-down-delay-after-delete=""10s""
I0906 00:12:32.503007       1 flags.go:52] FLAG: --scale-down-delay-after-failure=""3m0s""
I0906 00:12:32.503011       1 flags.go:52] FLAG: --scale-down-enabled=""true""
I0906 00:12:32.503015       1 flags.go:52] FLAG: --scale-down-non-empty-candidates-count=""30""
I0906 00:12:32.503019       1 flags.go:52] FLAG: --scale-down-unneeded-time=""10m0s""
I0906 00:12:32.503023       1 flags.go:52] FLAG: --scale-down-unready-time=""20m0s""
I0906 00:12:32.503027       1 flags.go:52] FLAG: --scale-down-utilization-threshold=""0.5""
I0906 00:12:32.503031       1 flags.go:52] FLAG: --scan-interval=""10s""
I0906 00:12:32.503036       1 flags.go:52] FLAG: --skip-nodes-with-local-storage=""true""
I0906 00:12:32.503039       1 flags.go:52] FLAG: --skip-nodes-with-system-pods=""true""
I0906 00:12:32.503043       1 flags.go:52] FLAG: --stderrthreshold=""0""
I0906 00:12:32.503047       1 flags.go:52] FLAG: --storage-driver-buffer-duration=""1m0s""
I0906 00:12:32.503052       1 flags.go:52] FLAG: --storage-driver-db=""cadvisor""
I0906 00:12:32.503056       1 flags.go:52] FLAG: --storage-driver-host=""localhost:8086""
I0906 00:12:32.503060       1 flags.go:52] FLAG: --storage-driver-password=""root""
I0906 00:12:32.503064       1 flags.go:52] FLAG: --storage-driver-secure=""false""
I0906 00:12:32.503068       1 flags.go:52] FLAG: --storage-driver-table=""stats""
I0906 00:12:32.503072       1 flags.go:52] FLAG: --storage-driver-user=""root""
I0906 00:12:32.503076       1 flags.go:52] FLAG: --test.bench=""""
I0906 00:12:32.503080       1 flags.go:52] FLAG: --test.benchmem=""false""
I0906 00:12:32.503084       1 flags.go:52] FLAG: --test.benchtime=""1s""
I0906 00:12:32.503089       1 flags.go:52] FLAG: --test.blockprofile=""""
I0906 00:12:32.503092       1 flags.go:52] FLAG: --test.blockprofilerate=""1""
I0906 00:12:32.503096       1 flags.go:52] FLAG: --test.count=""1""
I0906 00:12:32.503101       1 flags.go:52] FLAG: --test.coverprofile=""""
I0906 00:12:32.503104       1 flags.go:52] FLAG: --test.cpu=""""
I0906 00:12:32.503108       1 flags.go:52] FLAG: --test.cpuprofile=""""
I0906 00:12:32.503112       1 flags.go:52] FLAG: --test.failfast=""false""
I0906 00:12:32.503116       1 flags.go:52] FLAG: --test.list=""""
I0906 00:12:32.503120       1 flags.go:52] FLAG: --test.memprofile=""""
I0906 00:12:32.503124       1 flags.go:52] FLAG: --test.memprofilerate=""0""
I0906 00:12:32.503128       1 flags.go:52] FLAG: --test.mutexprofile=""""
I0906 00:12:32.503132       1 flags.go:52] FLAG: --test.mutexprofilefraction=""1""
I0906 00:12:32.503135       1 flags.go:52] FLAG: --test.outputdir=""""
I0906 00:12:32.503139       1 flags.go:52] FLAG: --test.parallel=""2""
I0906 00:12:32.503143       1 flags.go:52] FLAG: --test.run=""""
I0906 00:12:32.503147       1 flags.go:52] FLAG: --test.short=""false""
I0906 00:12:32.503151       1 flags.go:52] FLAG: --test.testlogfile=""""
I0906 00:12:32.503155       1 flags.go:52] FLAG: --test.timeout=""0s""
I0906 00:12:32.503159       1 flags.go:52] FLAG: --test.trace=""""
I0906 00:12:32.503163       1 flags.go:52] FLAG: --test.v=""false""
I0906 00:12:32.503167       1 flags.go:52] FLAG: --v=""4""
I0906 00:12:32.503171       1 flags.go:52] FLAG: --version=""false""
I0906 00:12:32.503177       1 flags.go:52] FLAG: --vmodule=""""
I0906 00:12:32.503182       1 flags.go:52] FLAG: --write-status-configmap=""true""
I0906 00:12:32.503188       1 main.go:311] Cluster Autoscaler 1.3.1
I0906 00:12:32.522597       1 leaderelection.go:185] attempting to acquire leader lease  kube-system/cluster-autoscaler...
I0906 00:12:32.527183       1 leaderelection.go:253] lock is held by autoscaler-aws-cluster-autoscaler-7cb5bc8ff5-j9466 and has not yet expired
I0906 00:12:32.527203       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0906 00:12:36.654538       1 leaderelection.go:253] lock is held by autoscaler-aws-cluster-autoscaler-7cb5bc8ff5-j9466 and has not yet expired
I0906 00:12:36.654559       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0906 00:12:39.474450       1 leaderelection.go:253] lock is held by autoscaler-aws-cluster-autoscaler-7cb5bc8ff5-j9466 and has not yet expired
I0906 00:12:39.474485       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0906 00:12:42.719939       1 leaderelection.go:253] lock is held by autoscaler-aws-cluster-autoscaler-7cb5bc8ff5-j9466 and has not yet expired
I0906 00:12:42.719961       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0906 00:12:45.131001       1 leaderelection.go:253] lock is held by autoscaler-aws-cluster-autoscaler-7cb5bc8ff5-j9466 and has not yet expired
I0906 00:12:45.131020       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0906 00:12:49.123319       1 leaderelection.go:194] successfully acquired lease kube-system/cluster-autoscaler
I0906 00:12:49.123893       1 factory.go:33] Event(v1.ObjectReference{Kind:""Endpoints"", Namespace:""kube-system"", Name:""cluster-autoscaler"", UID:""4d93a17a-b15b-11e8-a631-06f452ccda64"", APIVersion:""v1"", ResourceVersion:""1143257"", FieldPath:""""}): type: 'Normal' reason: 'LeaderElection' autoscaler-aws-cluster-autoscaler-65bd75b9f-sgn57 became leader
I0906 00:12:49.124731       1 predicates.go:126] Using predicate PodFitsResources
I0906 00:12:49.124763       1 predicates.go:126] Using predicate GeneralPredicates
I0906 00:12:49.124774       1 predicates.go:126] Using predicate PodToleratesNodeTaints
I0906 00:12:49.124783       1 predicates.go:126] Using predicate NoDiskConflict
I0906 00:12:49.124857       1 predicates.go:126] Using predicate CheckNodeCondition
I0906 00:12:49.124873       1 predicates.go:126] Using predicate CheckNodeDiskPressure
I0906 00:12:49.124895       1 predicates.go:126] Using predicate CheckNodePIDPressure
I0906 00:12:49.124918       1 predicates.go:126] Using predicate CheckVolumeBinding
I0906 00:12:49.124938       1 predicates.go:126] Using predicate MaxGCEPDVolumeCount
I0906 00:12:49.124968       1 predicates.go:126] Using predicate NoVolumeZoneConflict
I0906 00:12:49.124991       1 predicates.go:126] Using predicate CheckNodeMemoryPressure
I0906 00:12:49.125002       1 predicates.go:126] Using predicate MatchInterPodAffinity
I0906 00:12:49.125006       1 predicates.go:126] Using predicate MaxEBSVolumeCount
I0906 00:12:49.125010       1 predicates.go:126] Using predicate ready
I0906 00:12:49.125014       1 predicates.go:126] Using predicate MaxAzureDiskVolumeCount
I0906 00:12:49.125205       1 reflector.go:202] Starting reflector *v1beta1.DaemonSet (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I0906 00:12:49.125233       1 reflector.go:240] Listing and watching *v1beta1.DaemonSet from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I0906 00:12:49.125613       1 reflector.go:202] Starting reflector *v1.Node (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.125630       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.125714       1 reflector.go:202] Starting reflector *v1.StorageClass (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.125807       1 reflector.go:240] Listing and watching *v1.StorageClass from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.125871       1 reflector.go:202] Starting reflector *v1.Service (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.125887       1 reflector.go:240] Listing and watching *v1.Service from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.126121       1 reflector.go:202] Starting reflector *v1.ReplicationController (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.126138       1 reflector.go:240] Listing and watching *v1.ReplicationController from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.126341       1 reflector.go:202] Starting reflector *v1.PersistentVolume (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.126381       1 reflector.go:240] Listing and watching *v1.PersistentVolume from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.126465       1 reflector.go:202] Starting reflector *v1beta1.ReplicaSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.126480       1 reflector.go:240] Listing and watching *v1beta1.ReplicaSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.126359       1 reflector.go:202] Starting reflector *v1beta1.PodDisruptionBudget (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.126662       1 reflector.go:240] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.126772       1 reflector.go:202] Starting reflector *v1beta1.StatefulSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.126784       1 reflector.go:240] Listing and watching *v1beta1.StatefulSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.127011       1 reflector.go:202] Starting reflector *v1.Pod (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.127027       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.127320       1 reflector.go:202] Starting reflector *v1.PersistentVolumeClaim (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.127361       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I0906 00:12:49.127375       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I0906 00:12:49.127362       1 reflector.go:240] Listing and watching *v1.PersistentVolumeClaim from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0906 00:12:49.127474       1 reflector.go:202] Starting reflector *v1beta1.PodDisruptionBudget (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0906 00:12:49.127489       1 reflector.go:240] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0906 00:12:49.127324       1 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149
I0906 00:12:49.128362       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149
I0906 00:12:49.127336       1 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I0906 00:12:49.128525       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I0906 00:12:49.127352       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212
I0906 00:12:49.128579       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212
I0906 00:12:49.158904       1 cloud_provider_builder.go:72] Building aws cloud provider.
I0906 00:12:49.325337       1 request.go:485] Throttling request took 196.732097ms, request: GET:https://172.20.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded&limit=500&resourceVersion=0
I0906 00:12:49.392829       1 auto_scaling_groups.go:245] Regenerating instance to ASG map for ASGs: [plague-ssdemo-eks2workers-NodeGroup-LHS1EG3DDRK4]
I0906 00:12:49.501034       1 auto_scaling_groups.go:119] Registering ASG plague-ssdemo-eks2workers-NodeGroup-LHS1EG3DDRK4
I0906 00:12:49.501060       1 aws_manager.go:126] Refreshed ASG list, next refresh after 2018-09-06 00:12:59.50105597 +0000 UTC m=+27.101545956
I0906 00:12:49.501137       1 main.go:239] Registered cleanup signal handler
I0906 00:12:49.525325       1 request.go:485] Throttling request took 396.692565ms, request: GET:https://172.20.0.1:443/api/v1/nodes?limit=500&resourceVersion=0
I0906 00:12:51.133021       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0906 00:12:53.147202       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0906 00:12:55.157866       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0906 00:12:57.167687       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0906 00:12:59.199073       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0906 00:12:59.509845       1 static_autoscaler.go:131] Starting main loop
I0906 00:12:59.631327       1 auto_scaling_groups.go:245] Regenerating instance to ASG map for ASGs: [plague-ssdemo-eks2workers-NodeGroup-LHS1EG3DDRK4]
I0906 00:12:59.755840       1 aws_manager.go:126] Refreshed ASG list, next refresh after 2018-09-06 00:13:09.755828867 +0000 UTC m=+37.356319213
I0906 00:12:59.755943       1 utils.go:503] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0906 00:12:59.755957       1 static_autoscaler.go:244] Filtering out schedulables
I0906 00:12:59.756058       1 static_autoscaler.go:254] No schedulable pods
I0906 00:12:59.756072       1 static_autoscaler.go:258] No unschedulable pods
I0906 00:12:59.756082       1 static_autoscaler.go:305] Calculating unneeded nodes
I0906 00:12:59.756161       1 scale_down.go:404] Node ip-10-15-5-57.us-west-2.compute.internal - utilization 0.876906
I0906 00:12:59.756177       1 scale_down.go:408] Node ip-10-15-5-57.us-west-2.compute.internal is not suitable for removal - utilization too big (0.876906)
I0906 00:12:59.756189       1 scale_down.go:404] Node ip-10-15-4-111.us-west-2.compute.internal - utilization 0.745318
I0906 00:12:59.756195       1 scale_down.go:408] Node ip-10-15-4-111.us-west-2.compute.internal is not suitable for removal - utilization too big (0.745318)
I0906 00:12:59.756273       1 static_autoscaler.go:331] Scale down status: unneededOnly=true lastScaleUpTime=2018-09-06 00:12:49.501092639 +0000 UTC m=+17.101582590 lastScaleDownDeleteTime=2018-09-06 00:12:49.501093446 +0000 UTC m=+17.101583394 lastScaleDownFailTime=2018-09-06 00:12:49.501094253 +0000 UTC m=+17.101584203 scaleDownForbidden=false isDeleteInProgress=false
...
I0906 00:15:33.470810       1 static_autoscaler.go:131] Starting main loop
I0906 00:15:33.586095       1 auto_scaling_groups.go:245] Regenerating instance to ASG map for ASGs: [plague-ssdemo-eks2workers-NodeGroup-LHS1EG3DDRK4]
I0906 00:15:33.662305       1 aws_manager.go:126] Refreshed ASG list, next refresh after 2018-09-06 00:15:43.66229647 +0000 UTC m=+191.262786852
I0906 00:15:33.662393       1 utils.go:503] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0906 00:15:33.662406       1 static_autoscaler.go:244] Filtering out schedulables
I0906 00:15:33.662516       1 static_autoscaler.go:254] No schedulable pods
I0906 00:15:33.662539       1 scale_up.go:249] Pod default/mateuszdemo121-596fb94bcf-2q5bc is unschedulable
I0906 00:15:33.662547       1 scale_up.go:249] Pod default/mateuszdemo121-configure-silverstripe-wkjng is unschedulable
I0906 00:15:33.674821       1 scale_up.go:291] Upcoming 0 nodes
W0906 00:15:33.674846       1 scale_up.go:307] Node group plague-ssdemo-eks2workers-NodeGroup-LHS1EG3DDRK4 is not ready for scaleup
I0906 00:15:33.674854       1 scale_up.go:376] No expansion options
I0906 00:15:33.674890       1 static_autoscaler.go:305] Calculating unneeded nodes
I0906 00:15:33.674988       1 scale_down.go:404] Node ip-10-15-5-57.us-west-2.compute.internal - utilization 0.876906
I0906 00:15:33.675005       1 scale_down.go:408] Node ip-10-15-5-57.us-west-2.compute.internal is not suitable for removal - utilization too big (0.876906)
I0906 00:15:33.675016       1 scale_down.go:404] Node ip-10-15-4-111.us-west-2.compute.internal - utilization 0.745318
I0906 00:15:33.675024       1 scale_down.go:408] Node ip-10-15-4-111.us-west-2.compute.internal is not suitable for removal - utilization too big (0.745318)
I0906 00:15:33.675102       1 static_autoscaler.go:331] Scale down status: unneededOnly=true lastScaleUpTime=2018-09-06 00:12:49.501092639 +0000 UTC m=+17.101582590 lastScaleDownDeleteTime=2018-09-06 00:12:49.501093446 +0000 UTC m=+17.101583394 lastScaleDownFailTime=2018-09-06 00:12:49.501094253 +0000 UTC m=+17.101584203 scaleDownForbidden=false isDeleteInProgress=false
I0906 00:15:33.675564       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""mateuszdemo121-configure-silverstripe-wkjng"", UID:""d897f081-b169-11e8-a769-02dbba053656"", APIVersion:""v1"", ResourceVersion:""1143618"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0906 00:15:33.675584       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""mateuszdemo121-596fb94bcf-2q5bc"", UID:""d8906e8b-b169-11e8-a769-02dbba053656"", APIVersion:""v1"", ResourceVersion:""1143605"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```

The status ConfigMap is:

```
    Cluster-wide:
      Health:      Healthy (ready=2 unready=0 notStarted=0 longNotStarted=0 registered=2 longUnregistered=0)
                   LastProbeTime:      2018-09-06 00:19:18.667442302 +0000 UTC m=+406.267932387
                   LastTransitionTime: 2018-09-06 00:12:59.501320425 +0000 UTC m=+27.101810494
      ScaleUp:     NoActivity (ready=2 registered=2)
                   LastProbeTime:      2018-09-06 00:19:18.667442302 +0000 UTC m=+406.267932387
                   LastTransitionTime: 2018-09-06 00:12:59.501320425 +0000 UTC m=+27.101810494
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2018-09-06 00:19:18.667442302 +0000 UTC m=+406.267932387
                   LastTransitionTime: 2018-09-06 00:12:59.501320425 +0000 UTC m=+27.101810494

    NodeGroups:
      Name:        plague-ssdemo-eks2workers-NodeGroup-LHS1EG3DDRK4
      Health:      Healthy (ready=2 unready=0 notStarted=0 longNotStarted=0 registered=2 longUnregistered=0 cloudProviderTarget=2 (minSize=1, maxSize=8))
                   LastProbeTime:      2018-09-06 00:19:18.667442302 +0000 UTC m=+406.267932387
                   LastTransitionTime: 2018-09-06 00:12:59.501320425 +0000 UTC m=+27.101810494
      ScaleUp:     Backoff (ready=2 cloudProviderTarget=2)
                   LastProbeTime:      2018-09-06 00:19:18.667442302 +0000 UTC m=+406.267932387
                   LastTransitionTime: 2018-09-06 00:14:41.945208724 +0000 UTC m=+129.545698778
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2018-09-06 00:19:18.667442302 +0000 UTC m=+406.267932387
                   LastTransitionTime: 2018-09-06 00:12:59.501320425 +0000 UTC m=+27.101810494
```

My ASG is set to desired/min/max = 2/1/8

The app that is trying to come up follows this layout more or less:

```
kind: Deployment
...
      containers:
      - name: solr
	...
        resources:
          requests:
            memory: ""256Mi""
            cpu: ""0m""
          limits:
            memory: ""384Mi""
            cpu: ""1000m""
      - name: web
	...
        resources:
          requests:
            memory: ""256Mi""
            cpu: ""0m""
          limits:
            memory: ""384Mi""
            cpu: ""1000m""

kind: Job
...
      containers:
      - name: web
      <no limits set>
```

We also use some NFS mounts and some EBS mounts.

I've ran out of ideas and things to check short of deploying my own version of the autoscaler with heaps of Printfs spread around the code. Any suggestions what am I doing wrong?",closed,False,2018-09-06 00:39:46,2018-09-06 23:41:35
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1219,https://api.github.com/repos/kubernetes/autoscaler/issues/1219,Add client-go metrics to VPA,"Import k/k/pkg/client/metrics/prometheus to register rest_client_request_* metrics.

Commits are separated to split the real change from godeps update.",closed,True,2018-09-06 07:59:16,2018-09-06 11:57:04
autoscaler,Whitespirit0,https://github.com/kubernetes/autoscaler/issues/1220,https://api.github.com/repos/kubernetes/autoscaler/issues/1220,Release date of CA 1.2.3 and 1.3.2,"Hello,

Do you have an estimated release date for Cluster Autoscaler 1.2.3 and 1.3.2?

Thanks!",closed,False,2018-09-06 08:54:34,2018-09-06 14:26:45
autoscaler,mikeweiwei,https://github.com/kubernetes/autoscaler/pull/1221,https://api.github.com/repos/kubernetes/autoscaler/issues/1221,update ca version,update ca version ,closed,True,2018-09-06 09:26:59,2018-09-06 09:31:00
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1222,https://api.github.com/repos/kubernetes/autoscaler/issues/1222,Add client-go metrics to CA,"Import k/k/pkg/client/metrics/prometheus to register rest_client_request_* metrics.

Commits are separated to split the real change from godeps update.",closed,True,2018-09-06 10:41:54,2018-09-06 11:53:47
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1223,https://api.github.com/repos/kubernetes/autoscaler/issues/1223,Fast-forward cluster-autoscaler-release-1.12 branch,,closed,True,2018-09-06 12:24:00,2018-09-06 13:35:41
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1224,https://api.github.com/repos/kubernetes/autoscaler/issues/1224,Cluster Autoscaler v1.12-beta.1,,closed,True,2018-09-06 13:38:29,2018-09-06 14:01:25
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1225,https://api.github.com/repos/kubernetes/autoscaler/issues/1225,VPA increase CPU recommendation resolution for low values,,closed,True,2018-09-06 14:28:07,2018-09-07 12:33:46
autoscaler,ricolin,https://github.com/kubernetes/autoscaler/pull/1226,https://api.github.com/repos/kubernetes/autoscaler/issues/1226,[WIP]Support openstack cluster autoscaler,WIP for #734 ,closed,True,2018-09-06 14:31:02,2019-02-14 17:37:43
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1227,https://api.github.com/repos/kubernetes/autoscaler/issues/1227,VPA fix logging calls,,closed,True,2018-09-06 14:38:03,2018-09-07 12:32:49
autoscaler,downeast,https://github.com/kubernetes/autoscaler/issues/1228,https://api.github.com/repos/kubernetes/autoscaler/issues/1228,Failure to create AWS manager: limited to 50 ASG names,"Hello, we are running the cluster autoscaler on a cluster with many different scaling groups, and hit the following error on startup:

```
E0906 20:27:17.672470       1 aws_manager.go:122] Failed to regenerate ASG cache: ValidationError: The number of group names that may be passed in is limited to 50
	status code: 400, request id: 3f77f9d5-b213-11e8-9faf-6919af635e71
F0906 20:27:17.672503       1 cloud_provider_builder.go:137] Failed to create AWS Manager: ValidationError: The number of group names that may be passed in is limited to 50
	status code: 400, request id: 3f77f9d5-b213-11e8-9faf-6919af635e71
```


Seems like we need some batching logic here?  This is version 1.3.1.",closed,False,2018-09-06 20:37:06,2018-09-20 15:33:46
autoscaler,mikeweiwei,https://github.com/kubernetes/autoscaler/pull/1229,https://api.github.com/repos/kubernetes/autoscaler/issues/1229,fix event,"
Add more event.When node is deleted and then add envet ",closed,True,2018-09-07 07:35:00,2018-09-07 12:32:20
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1230,https://api.github.com/repos/kubernetes/autoscaler/issues/1230,Create an Event when Pod is evicted by VPA Updater,"The newEventRecorder function is based on CA code.

/assign @bskiba , please have a look.",closed,True,2018-09-07 09:54:37,2018-09-07 12:30:05
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1231,https://api.github.com/repos/kubernetes/autoscaler/issues/1231,VPA histogram percentile should return end of the bucket instead of the middle point,,closed,True,2018-09-07 12:42:07,2018-09-10 20:29:23
autoscaler,towca,https://github.com/kubernetes/autoscaler/pull/1232,https://api.github.com/repos/kubernetes/autoscaler/issues/1232,Add a method to determine if a node group is at its its target size to CSR,,closed,True,2018-09-07 18:27:47,2018-09-10 14:24:51
autoscaler,towca,https://github.com/kubernetes/autoscaler/pull/1233,https://api.github.com/repos/kubernetes/autoscaler/issues/1233,"Add a scale down status processor, refactor so that there's more scal…",…e down info available to it,closed,True,2018-09-07 18:49:40,2018-09-13 14:54:13
autoscaler,delgod,https://github.com/kubernetes/autoscaler/pull/1234,https://api.github.com/repos/kubernetes/autoscaler/issues/1234,add AWS t3 instance types,,closed,True,2018-09-09 12:46:35,2018-09-25 07:41:29
autoscaler,sunlintong,https://github.com/kubernetes/autoscaler/pull/1235,https://api.github.com/repos/kubernetes/autoscaler/issues/1235,fix 3 spell,maybe I'm too boring this weekend :),closed,True,2018-09-09 13:48:29,2018-09-12 06:28:19
autoscaler,mikeweiwei,https://github.com/kubernetes/autoscaler/pull/1236,https://api.github.com/repos/kubernetes/autoscaler/issues/1236,warningf instead warning,warningf instead warning,closed,True,2018-09-10 08:17:31,2018-09-11 09:46:13
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1237,https://api.github.com/repos/kubernetes/autoscaler/issues/1237,Remove unused test code,Remove no longer used JSON responses.,closed,True,2018-09-10 09:23:08,2018-09-10 13:57:17
autoscaler,choury,https://github.com/kubernetes/autoscaler/pull/1238,https://api.github.com/repos/kubernetes/autoscaler/issues/1238,Use the same package name in main.go,Should not import one package twice.,closed,True,2018-09-10 09:50:41,2018-09-12 09:16:35
autoscaler,spiffxp,https://github.com/kubernetes/autoscaler/issues/1239,https://api.github.com/repos/kubernetes/autoscaler/issues/1239,Use tide for PR merging,"This is a [core repository](https://github.com/kubernetes/community/blob/master/github-management/kubernetes-repositories.md#core-repositories).  As such, it [needs to use the same merge automation as the rest of the project](https://github.com/kubernetes/community/blob/master/github-management/kubernetes-repositories.md#rules-1)

I have a PR open that will address this at an org-wide level: https://github.com/kubernetes/test-infra/pull/9342.  It will:
- enable the approve plugin, to allow use of the `/approve` plugin
- enable the blunderbuss plugin, to assign reviews based on OWNERS files
- add all repos in the kubernetes org to tide's query

Can one of the repo maintainers here drop an LGTM (or objections) on the linked PR?  Alternatively, if I hear no objections by Monday 10am PT of next week, I will merge the PR.",closed,False,2018-09-11 22:23:39,2018-09-17 17:11:16
autoscaler,zeelichsheng,https://github.com/kubernetes/autoscaler/issues/1240,https://api.github.com/repos/kubernetes/autoscaler/issues/1240,Autoscaler scaling down worker node with more pods than other candidate,"When autoscaler has multiple candidates for scaling down, it always picks the first candidate. It does not consider whether the cost of scaling down the first candidate is actually more than the second candidate.

For example, this is the log of autoscaler choosing a scaling-down candidate of two worker nodes. Worker node A has much more pods than node B. However, worker node A was chosen to be removed. This causes more disruption than scaling down node B:

> I0911 22:08:43.688940       1 cluster.go:80] Fast evaluation: worker-A for removal
> I0911 22:08:43.689041       1 cluster.go:206] Pod default/server-foo can be moved to worker-B
> I0911 22:08:43.779866       1 cluster.go:206] Pod default/server-bar can be moved to worker-B
> I0911 22:08:43.780039       1 cluster.go:206] Pod default/vsx-foo can be moved to worker-B
> I0911 22:08:43.780084       1 cluster.go:206] Pod default/db-tunnel can be moved to worker-B
> I0911 22:08:43.780133       1 cluster.go:206] Pod default/vsx-bar can be moved to worker-B
> I0911 22:08:43.780172       1 cluster.go:206] Pod default/publisher-foo can be moved to worker-B
> I0911 22:08:43.780206       1 cluster.go:206] Pod default/publisher-bar can be moved to worker-B
> I0911 22:08:43.780276       1 cluster.go:206] Pod default/es can be moved to worker-B
> I0911 22:08:43.780343       1 cluster.go:111] Fast evaluation: node worker-A may be removed
> I0911 22:08:43.780380       1 cluster.go:80] Fast evaluation: worker-B for removal
> I0911 22:08:43.780414       1 cluster.go:206] Pod default/sync-to-tap can be moved to worker-A
> I0911 22:08:43.780472       1 cluster.go:206] Pod default/bitnami-sync can be moved to worker-A
> I0911 22:08:43.780489       1 cluster.go:111] Fast evaluation: node worker-B may be removed

> I0911 22:08:43.780532       1 static_autoscaler.go:335] worker-A is unneeded since 2018-09-11 21:58:40.585339203 +0000 UTC m=+3106664.381171299 duration 10m3.100336453s
> I0911 22:08:43.780575       1 static_autoscaler.go:335] worker-B is unneeded since 2018-09-11 21:57:50.289226383 +0000 UTC m=+3106614.085058506 duration 10m53.396449246s
> I0911 22:08:43.803429       1 scale_down.go:488] Scale-down: removing node worker-A, utilization: 0.125, pods to reschedule: default/server-foo,default/server-bar,default/vsx-foo,default/db-tunnel,default/vsx-bar,default/publisher-foo,default/publisher-bar,default/es

Can we improve autoscaler to make a better decision to avoid such situation?

K8S version: 1.10.2
Autoscaler version: 1.2.2",closed,False,2018-09-12 01:26:54,2018-09-12 10:45:02
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1241,https://api.github.com/repos/kubernetes/autoscaler/issues/1241,Regenerate godeps for VPA to drop unnecessary dependencies.,"Thanks to updates in upstream grpc some tools (lint, misspell, staticcheck) are no longer required as transitive dependencies.

@bskiba , please have a look.",closed,True,2018-09-12 07:34:26,2018-09-12 09:12:28
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1242,https://api.github.com/repos/kubernetes/autoscaler/issues/1242,Export GKE API endpoint flag,Temporary workaround for issues with testing setup in GKE.,closed,True,2018-09-12 09:23:42,2018-09-12 09:59:08
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1243,https://api.github.com/repos/kubernetes/autoscaler/issues/1243,Add bindata.go which got mistakenly removed by PR #1241.,,closed,True,2018-09-13 11:56:11,2018-09-13 12:17:41
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1244,https://api.github.com/repos/kubernetes/autoscaler/issues/1244,Call CheckPodsSchedulableOnNode in scale_up.go via caching layer,,closed,True,2018-09-13 14:32:43,2018-09-18 09:21:04
autoscaler,hercynium,https://github.com/kubernetes/autoscaler/pull/1245,https://api.github.com/repos/kubernetes/autoscaler/issues/1245,Add configurable delay for pod age before considering for scale-up,"  - This is intended to address the issue described in https://github.com/kubernetes/autoscaler/issues/923
  - the delay is configurable via a CLI option
  - in production (on AWS) we set this to a value of 2m
  - the delay could possibly be set as low as 30s and still be effective depending on your workload and environment
  - the default of 0 for the CLI option results in no change to the CA's behavior from defaults.",closed,True,2018-09-14 18:01:22,2018-09-17 18:48:14
autoscaler,mikeweiwei,https://github.com/kubernetes/autoscaler/pull/1246,https://api.github.com/repos/kubernetes/autoscaler/issues/1246,Warning instead Warningf,Warning instead Warningf,closed,True,2018-09-17 06:09:16,2018-09-17 09:00:44
autoscaler,mikeweiwei,https://github.com/kubernetes/autoscaler/pull/1247,https://api.github.com/repos/kubernetes/autoscaler/issues/1247,Add operator for the  pod tolerations ,,closed,True,2018-09-17 10:34:14,2018-09-18 07:14:18
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1248,https://api.github.com/repos/kubernetes/autoscaler/issues/1248,Cluster Autoscaler 1.12.0,,closed,True,2018-09-17 11:56:35,2018-09-17 12:42:00
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1249,https://api.github.com/repos/kubernetes/autoscaler/issues/1249,Update GCB config for Cluster Autoscaler,"@rvkubiak - follow-up to #665, PTAL if it makes sense",closed,True,2018-09-17 13:37:27,2019-02-01 16:47:34
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1250,https://api.github.com/repos/kubernetes/autoscaler/issues/1250,Update Cluster Autoscaler releases recommendation,Update Cluster Autoscaler releases recommendation with 1.12.,closed,True,2018-09-17 18:53:56,2018-09-18 07:15:17
autoscaler,nburn42,https://github.com/kubernetes/autoscaler/pull/1251,https://api.github.com/repos/kubernetes/autoscaler/issues/1251,Added r5 instance type,https://aws.amazon.com/ec2/instance-types/r5/,closed,True,2018-09-17 19:39:48,2018-09-17 19:41:30
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1252,https://api.github.com/repos/kubernetes/autoscaler/issues/1252,Update OWNERS,Accidentally my Slack username was used instead of Github one. Fixing also @losipiuk's username.,closed,True,2018-09-18 09:05:32,2019-03-20 15:39:11
autoscaler,mikeweiwei,https://github.com/kubernetes/autoscaler/pull/1253,https://api.github.com/repos/kubernetes/autoscaler/issues/1253,fix logging calls(azure_container_service_pool.go),,closed,True,2018-09-18 11:25:40,2018-09-18 11:38:08
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1254,https://api.github.com/repos/kubernetes/autoscaler/issues/1254,NodeGroupManager.CreateNodeGroup can return extra created node groups.,,closed,True,2018-09-18 12:19:17,2018-09-19 14:35:53
autoscaler,towca,https://github.com/kubernetes/autoscaler/pull/1255,https://api.github.com/repos/kubernetes/autoscaler/issues/1255,Add the ability to retrieve the original reasons from a PredicateError,,closed,True,2018-09-18 20:03:30,2018-09-20 14:12:38
autoscaler,towca,https://github.com/kubernetes/autoscaler/pull/1256,https://api.github.com/repos/kubernetes/autoscaler/issues/1256,Refactor the scale-(up|down) status processors so that they have more…,"… info available

Replace the simple boolean ScaledUp property of ScaleUpStatus with a more
comprehensive ScaleUpResult. Add more possible values to ScaleDownResult.
Refactor the processors execution so that they are always executed every
iteration, even if RunOnce exits earlier.",closed,True,2018-09-18 20:16:31,2018-09-20 16:05:48
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1257,https://api.github.com/repos/kubernetes/autoscaler/issues/1257,Add CRD validation.,"Formatting rules are checked via deserialization from JSON.
Logical rules are checked explicitly.

Enums are checked in go code to avoid problems with old kubectl
which don't support enums nor arrays in CRD validation.",closed,True,2018-09-18 21:42:50,2018-09-19 08:24:30
autoscaler,phyllisstein,https://github.com/kubernetes/autoscaler/issues/1258,https://api.github.com/repos/kubernetes/autoscaler/issues/1258,"VPA updater pod crashing with ""invalid memory address or nil pointer dereference.""","👋 Hey folks! Just wanted to flag an issue I'm seeing with the VPA updater pod. It seems to crash every five minutes or so with the following error message:

```
I0919 04:43:55.060264       9 flags.go:52] FLAG: --alsologtostderr=""false""
I0919 04:43:55.060515       9 flags.go:52] FLAG: --eviction-tolerance=""0.5""
I0919 04:43:55.060526       9 flags.go:52] FLAG: --httptest.serve=""""
I0919 04:43:55.060530       9 flags.go:52] FLAG: --log-backtrace-at="":0""
I0919 04:43:55.060535       9 flags.go:52] FLAG: --log-dir=""""
I0919 04:43:55.060540       9 flags.go:52] FLAG: --logtostderr=""false""
I0919 04:43:55.060543       9 flags.go:52] FLAG: --min-replicas=""2""
I0919 04:43:55.060546       9 flags.go:52] FLAG: --stderrthreshold=""0""
I0919 04:43:55.060550       9 flags.go:52] FLAG: --updater-interval=""1m0s""
I0919 04:43:55.060553       9 flags.go:52] FLAG: --v=""4""
I0919 04:43:55.060556       9 flags.go:52] FLAG: --vmodule=""""
I0919 04:43:55.060561       9 main.go:45] Vertical Pod Autoscaler 0.2.0 Updater
I0919 04:43:55.061818       9 reflector.go:202] Starting reflector *v1alpha1.VerticalPodAutoscaler (1h0m0s) from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/utils/vpa/api.go:83
I0919 04:43:55.061847       9 reflector.go:240] Listing and watching *v1alpha1.VerticalPodAutoscaler from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/utils/vpa/api.go:83
I0919 04:43:55.161615       9 shared_informer.go:123] caches populated
I0919 04:43:55.161638       9 api.go:87] Initial VPA synced successfully
I0919 04:43:55.161733       9 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/logic/updater.go:155
I0919 04:43:55.161758       9 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/logic/updater.go:155
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0xe572b3]
 goroutine 1 [running]:
k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/priority.(*UpdatePriorityCalculator).getUpdatePriority(0xc42077fb00, 0xc4203d66a0, 0xc4208416e0, 0xc4203d66a0, 0xc4208416e0, 0x0)
	/usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/priority/update_priority_calculator.go:121 +0x7b3
k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/priority.(*UpdatePriorityCalculator).AddPod(0xc42077fb00, 0xc4203d66a0, 0xc42040db40, 0xbee092f1cd7ad123, 0xe0271a439, 0x17bb6e0)
	/usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/priority/update_priority_calculator.go:75 +0x1b3
k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/logic.(*updater).getPodsForUpdate(0xc4202001c0, 0xc420862710, 0x2, 0x2, 0xc42031ab40, 0xc420862710, 0x2, 0x2)
	/usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/logic/updater.go:123 +0x1d9
k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/logic.(*updater).RunOnce(0xc4202001c0)
	/usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/logic/updater.go:102 +0xa3f
main.main()
	/usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/main.go:55 +0x16e
```

Is there a stable Git commit that it makes sense to roll back to while this is being looked at? Thanks in advance for your help, and thanks for all your work on the autoscaler.",closed,False,2018-09-19 04:48:36,2018-10-09 11:33:41
autoscaler,mikeweiwei,https://github.com/kubernetes/autoscaler/pull/1259,https://api.github.com/repos/kubernetes/autoscaler/issues/1259,fix logging calls(cloud_provider_builder.go),,closed,True,2018-09-19 10:02:12,2018-09-19 11:53:12
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1260,https://api.github.com/repos/kubernetes/autoscaler/issues/1260,Add simple health-check to all VPA components.,"For updater & recommender we stop returning ""healthy"" if more than five loops passed without an update (process stuck somewhere).
For admission controller we always return ""healthy"".

@bskiba please decide whether you want to have this configurable or report ""unhealthy"" in some other situations.",closed,True,2018-09-19 11:22:51,2018-09-19 16:28:14
autoscaler,johanneswuerbach,https://github.com/kubernetes/autoscaler/pull/1261,https://api.github.com/repos/kubernetes/autoscaler/issues/1261,Fetch ASG names in chunks of 50,"Fixes https://github.com/kubernetes/autoscaler/issues/1228

While https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_DescribeAutoScalingGroups.html reports a `maximum length of 1600`, it seems that only max. 50 names are allowed as input https://github.com/cristim/autospotting/issues/199#issuecomment-355764282",closed,True,2018-09-20 00:11:59,2018-09-20 16:15:28
autoscaler,geoxanadu,https://github.com/kubernetes/autoscaler/issues/1262,https://api.github.com/repos/kubernetes/autoscaler/issues/1262,cluster-autoscaler in AzureGermanCloud,"I am trying to implement the cluster-autoscaler in a kubernetes cluster in the Azure German Cloud. While editing the yaml file I came across the value ""provider"". Typically ""azure"" is here provided. However, it seems that this does not work for the german cloud. Unfortunately I was not able to find the suitable flag for the german cloud. 
""AZUREGERMANCLOUD"", ""azuregermancloud"" does not work either. It always leads to a CrashLoopBackOff. Any ideas what I might have overseen?",closed,False,2018-09-20 11:39:36,2019-01-01 09:51:36
autoscaler,Dregathar,https://github.com/kubernetes/autoscaler/issues/1263,https://api.github.com/repos/kubernetes/autoscaler/issues/1263,"Reaction on ""failed to launch a new EC2 instance"" event","Hello,
Our application uses different types of instances as k8s nodes: c5 and i3. Pods running on c5 instances (let's name them 'core pods') use EBS volumes, so these pods are bound to a particular AZ. Pods running on i3 instances (let's name them 'compute pods') don't have EBS volumes and can run in any AZ. However, the application performs better if both core pods and compute pods run in the same AZ. The problem with i3 instances is that they might be lacking in some AZ's at some points in time (I personally saw 2 cases when i3.2xlarge instances were not available in a given AZ, and there was a recommendation in AWS console to create instances in other AZ's). Ideally we want to run compute pods in the same AZ as core pods, but in case when i3 instances are not available in the AZ, it is fine to run instances in another AZ.
To actually configure this scenario and to test how autoscaler would behave, I've created 2 node groups:

```
nodes-1a
  machineType: i3.xlarge
  maxSize: 10
  minSize: 0
  nodeLabels:
    instance_macro_group: computenodes
    kops.k8s.io/instancegroup: nodes-1a
	
nodes-1b
  machineType: i3.2xlarge
  maxSize: 10
  minSize: 0
  nodeLabels:
    instance_macro_group: computenodes
    kops.k8s.io/instancegroup: nodes-1b
```

Then I've created a deployment with the template spec containing the following:
  
```
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: kops.k8s.io/instancegroup
                operator: In
                values:
                - nodes-1a
            weight: 1
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: instance_macro_group
                operator: In
                values:
                - computenodes
```
				
and container memory has the following configuration:
```
        resources:
          requests:
            memory: 25Gi
```

nodes-1a instance group has maxSize = 10, however, the AWS account has a limit on number of running i3.xlarge instances = 6 (I'm not able to reproduce lacking of instances of a particular type, so, using instance limits fits the purpose of the test)

When the deployment is scaled to 1 replica, cluster autoscaler changes the number of nodes in nodes-1a to 1 node; when the deployment is scaled to 6 replicas, cluster autoscaler changes the number of nodes in nodes-1a to 6 nodes; when the deployment is scaled to 7 replicas, the number of nodes in nodes-1a is changed to 7, however, launching a new instance in the autoscaling group fails with the following description ""Launching a new EC2 instance. Status Reason: You have requested more instances (7) than your current instance limit of 6 allows for the specified instance type. Please visit http://aws.amazon.com/contact-us/ec2-request to request an adjustment to this limit. Launching EC2 instance failed.""

At this point the pod is stuck in 'Pending' state indefinitely.
What would be useful in this case is a mechanism to get a ""feedback"" from the Autoscaling group that launching a new EC2 instance failed, and try to find an alternative instance group (in our case - nodes-1b) to scale it out to be able to schedule the pod on it.
Does this request seem reasonable, or is there a way round the problem using some existing functionality?",open,False,2018-09-20 16:51:49,2019-02-28 22:27:45
autoscaler,MIBc,https://github.com/kubernetes/autoscaler/pull/1264,https://api.github.com/repos/kubernetes/autoscaler/issues/1264,Add kube-state-metrics which uses addon-resizer,"Can we add kube-state-metrics which also use addon-resizer to improve its performance:
https://github.com/kubernetes/kube-state-metrics/blob/master/kubernetes/kube-state-metrics-deployment.yaml#L34",closed,True,2018-09-21 09:02:20,2018-09-27 12:06:21
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1265,https://api.github.com/repos/kubernetes/autoscaler/issues/1265,Check for ready condition not true,,closed,True,2018-09-21 09:11:09,2018-09-21 09:26:13
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1266,https://api.github.com/repos/kubernetes/autoscaler/issues/1266,Update OWNERS to match master,,closed,True,2018-09-21 09:24:14,2019-03-20 15:39:11
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1267,https://api.github.com/repos/kubernetes/autoscaler/issues/1267,Update top-level OWNERS to match master,,closed,True,2018-09-21 09:26:30,2019-03-20 15:39:15
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1268,https://api.github.com/repos/kubernetes/autoscaler/issues/1268,Cluster Autoscaler version 1.3.3,,closed,True,2018-09-21 13:10:16,2018-09-21 13:11:34
autoscaler,WebSpider,https://github.com/kubernetes/autoscaler/pull/1269,https://api.github.com/repos/kubernetes/autoscaler/issues/1269,Adds a FAQ entry with all commandline parameters,"This PR adds an entry in FAQ.md with all current commandline parameters, description and default value
Fixes #918 ",closed,True,2018-09-23 07:41:49,2018-09-27 12:05:05
autoscaler,stroon2,https://github.com/kubernetes/autoscaler/issues/1270,https://api.github.com/repos/kubernetes/autoscaler/issues/1270,[AKS] [Autoscaler]Failed to find readiness information for agentpool /  segmentation violation ,"Hello everyone,

Cluster autoscaler service seems to have some problem to get nodegroup health status however our cluster works fine.

Pod scheduling works fine but when I  exceed cluster limit autoscaler don't scale the cluster an the pod panic.


Here is more information: 


**Kubectl get node:** 
''' 
_NAME                       STATUS    ROLES     AGE       VERSION   LABELS
aks-agentpool-33865844-0   Ready     agent     14d       v1.11.2   agentpool=agentpool,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/insta                    nce-type=Standard_D16s_v3,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=northeurope,failure-domain.beta.kubernetes.io/                    zone=0,kubernetes.azure.com/cluster=MC_RG-APPLI-HPROD_AKS-LHA-HPROD02_northeurope,kubernetes.io/hostname=aks-agentpool-33865844-0,                    kubernetes.io/role=agent,storageprofile=managed,storagetier=Premium_LRS
aks-agentpool-33865844-1   Ready     agent     14d       v1.11.2   agentpool=agentpool,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/insta                    nce-type=Standard_D16s_v3,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=northeurope,failure-domain.beta.kubernetes.io/                    zone=1,kubernetes.azure.com/cluster=MC_RG-APPLI-HPROD_AKS-LHA-HPROD02_northeurope,kubernetes.io/hostname=aks-agentpool-33865844-1,                    kubernetes.io/role=agent,storageprofile=managed,storagetier=Premium_LRS
'''_


**Kubernetes 1.11.2
Cluster-autoscaler: 1.3.3**


**cluster-autoscaler-status**
_'''
kubectl -n kube-system describe configmaps cluster-autoscaler-status
...
Cluster-autoscaler status at 2018-09-24 11:24:47.973812405 +0000 UTC:
Cluster-wide:
  Health:      Healthy (ready=6 unready=0 notStarted=0 longNotStarted=0 registered=6 longUnregistered=0)
               LastProbeTime:      2018-09-24 11:24:47.436701137 +0000 UTC m=+5227.476091680
               LastTransitionTime: 2018-09-24 09:58:13.148928229 +0000 UTC m=+33.188318872
  ScaleUp:     NoActivity (ready=6 registered=6)
               LastProbeTime:      2018-09-24 11:24:47.436701137 +0000 UTC m=+5227.476091680
               LastTransitionTime: 2018-09-24 09:58:13.148928229 +0000 UTC m=+33.188318872
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2018-09-24 11:24:47.436701137 +0000 UTC m=+5227.476091680
               LastTransitionTime: 2018-09-24 09:58:13.148928229 +0000 UTC m=+33.188318872

NodeGroups:
  Name:        agentpool
  **Health:      Unhealthy (ready=0 unready=0 notStarted=0 longNotStarted=0 registered=0 longUnregistered=0 cloudProviderTarget=6 (minSize=5, maxSize=10))**
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC
               LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC
  ScaleUp:     Backoff (ready=0 cloudProviderTarget=6)
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC
               LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2018-09-24 11:24:47.436701137 +0000 UTC m=+5227.476091680
               LastTransitionTime: 2018-09-24 09:58:13.148928229 +0000 UTC m=+33.188318872
'''_


**begining of autoscaler startup logs**
'''
_I0924 09:57:41.508847       1 flags.go:52] FLAG: --address="":8085""
I0924 09:57:41.508897       1 flags.go:52] FLAG: --alsologtostderr=""false""
I0924 09:57:41.508904       1 flags.go:52] FLAG: --application-metrics-count-limit=""100""
I0924 09:57:41.508912       1 flags.go:52] FLAG: --azure-container-registry-config=""""
I0924 09:57:41.508918       1 flags.go:52] FLAG: --balance-similar-node-groups=""false""
I0924 09:57:41.508922       1 flags.go:52] FLAG: --boot-id-file=""/proc/sys/kernel/random/boot_id""
I0924 09:57:41.508997       1 flags.go:52] FLAG: --cloud-config=""""
I0924 09:57:41.509005       1 flags.go:52] FLAG: --cloud-provider=""azure""
I0924 09:57:41.509011       1 flags.go:52] FLAG: --cloud-provider-gce-lb-src-cidrs=""130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16""
I0924 09:57:41.509022       1 flags.go:52] FLAG: --cluster-name=""""
I0924 09:57:41.509044       1 flags.go:52] FLAG: --container-hints=""/etc/cadvisor/container_hints.json""
I0924 09:57:41.509051       1 flags.go:52] FLAG: --containerd=""unix:///var/run/containerd.sock""
I0924 09:57:41.509067       1 flags.go:52] FLAG: --cores-total=""0:320000""
I0924 09:57:41.509073       1 flags.go:52] FLAG: --docker=""unix:///var/run/docker.sock""
I0924 09:57:41.509104       1 flags.go:52] FLAG: --docker-env-metadata-whitelist=""""
I0924 09:57:41.509110       1 flags.go:52] FLAG: --docker-only=""false""
I0924 09:57:41.509125       1 flags.go:52] FLAG: --docker-root=""/var/lib/docker""
I0924 09:57:41.509131       1 flags.go:52] FLAG: --docker-tls=""false""
I0924 09:57:41.509137       1 flags.go:52] FLAG: --docker-tls-ca=""ca.pem""
I0924 09:57:41.509157       1 flags.go:52] FLAG: --docker-tls-cert=""cert.pem""
I0924 09:57:41.509164       1 flags.go:52] FLAG: --docker-tls-key=""key.pem""
I0924 09:57:41.509169       1 flags.go:52] FLAG: --enable-load-reader=""false""
I0924 09:57:41.509175       1 flags.go:52] FLAG: --estimator=""binpacking""
I0924 09:57:41.509197       1 flags.go:52] FLAG: --event-storage-age-limit=""default=0""
I0924 09:57:41.509204       1 flags.go:52] FLAG: --event-storage-event-limit=""default=0""
I0924 09:57:41.509209       1 flags.go:52] FLAG: --expander=""random""
I0924 09:57:41.509215       1 flags.go:52] FLAG: --expendable-pods-priority-cutoff=""0""
I0924 09:57:41.509237       1 flags.go:52] FLAG: --gke-api-endpoint=""""
I0924 09:57:41.509243       1 flags.go:52] FLAG: --global-housekeeping-interval=""1m0s""
I0924 09:57:41.509264       1 flags.go:52] FLAG: --google-json-key=""""
I0924 09:57:41.509270       1 flags.go:52] FLAG: --gpu-total=""[]""
I0924 09:57:41.509292       1 flags.go:52] FLAG: --housekeeping-interval=""10s""
I0924 09:57:41.509298       1 flags.go:52] FLAG: --httptest.serve=""""
I0924 09:57:41.509304       1 flags.go:52] FLAG: --kubeconfig=""""
I0924 09:57:41.509327       1 flags.go:52] FLAG: --kubernetes=""""
I0924 09:57:41.509344       1 flags.go:52] FLAG: --leader-elect=""true""
I0924 09:57:41.509352       1 flags.go:52] FLAG: --leader-elect-lease-duration=""15s""
I0924 09:57:41.509374       1 flags.go:52] FLAG: --leader-elect-renew-deadline=""10s""
I0924 09:57:41.509394       1 flags.go:52] FLAG: --leader-elect-resource-lock=""endpoints""
I0924 09:57:41.509399       1 flags.go:52] FLAG: --leader-elect-retry-period=""2s""
I0924 09:57:41.509405       1 flags.go:52] FLAG: --log-backtrace-at="":0""
I0924 09:57:41.509425       1 flags.go:52] FLAG: --log-cadvisor-usage=""false""
I0924 09:57:41.509431       1 flags.go:52] FLAG: --log-dir=""""
I0924 09:57:41.509451       1 flags.go:52] FLAG: --log-flush-frequency=""5s""
I0924 09:57:41.509456       1 flags.go:52] FLAG: --logtostderr=""true""
I0924 09:57:41.509461       1 flags.go:52] FLAG: --machine-id-file=""/etc/machine-id,/var/lib/dbus/machine-id""
I0924 09:57:41.509467       1 flags.go:52] FLAG: --max-autoprovisioned-node-group-count=""15""
I0924 09:57:41.509486       1 flags.go:52] FLAG: --max-empty-bulk-delete=""10""
I0924 09:57:41.509491       1 flags.go:52] FLAG: --max-failing-time=""15m0s""
I0924 09:57:41.509496       1 flags.go:52] FLAG: --max-graceful-termination-sec=""600""
I0924 09:57:41.509501       1 flags.go:52] FLAG: --max-inactivity=""10m0s""
I0924 09:57:41.509520       1 flags.go:52] FLAG: --max-node-provision-time=""15m0s""
I0924 09:57:41.509525       1 flags.go:52] FLAG: --max-nodes-total=""0""
I0924 09:57:41.509530       1 flags.go:52] FLAG: --max-total-unready-percentage=""45""
I0924 09:57:41.509536       1 flags.go:52] FLAG: --memory-total=""0:6400000""
I0924 09:57:41.509556       1 flags.go:52] FLAG: --min-replica-count=""0""
I0924 09:57:41.509561       1 flags.go:52] FLAG: --namespace=""kube-system""
I0924 09:57:41.509566       1 flags.go:52] FLAG: --node-autoprovisioning-enabled=""false""
I0924 09:57:41.509570       1 flags.go:52] FLAG: --node-group-auto-discovery=""[]""
I0924 09:57:41.509590       1 flags.go:52] FLAG: --nodes=""[5:10:agentpool]""
I0924 09:57:41.509596       1 flags.go:52] FLAG: --ok-total-unready-count=""3""
I0924 09:57:41.509601       1 flags.go:52] FLAG: --regional=""false""
I0924 09:57:41.509606       1 flags.go:52] FLAG: --scale-down-candidates-pool-min-count=""50""
I0924 09:57:41.509624       1 flags.go:52] FLAG: --scale-down-candidates-pool-ratio=""0.1""
I0924 09:57:41.509630       1 flags.go:52] FLAG: --scale-down-delay-after-add=""10m0s""
I0924 09:57:41.509635       1 flags.go:52] FLAG: --scale-down-delay-after-delete=""10s""
I0924 09:57:41.509639       1 flags.go:52] FLAG: --scale-down-delay-after-failure=""3m0s""
I0924 09:57:41.509652       1 flags.go:52] FLAG: --scale-down-enabled=""true""
I0924 09:57:41.509657       1 flags.go:52] FLAG: --scale-down-non-empty-candidates-count=""30""
I0924 09:57:41.509661       1 flags.go:52] FLAG: --scale-down-unneeded-time=""10m0s""
I0924 09:57:41.509666       1 flags.go:52] FLAG: --scale-down-unready-time=""20m0s""
I0924 09:57:41.509670       1 flags.go:52] FLAG: --scale-down-utilization-threshold=""0.5""
I0924 09:57:41.509691       1 flags.go:52] FLAG: --scan-interval=""10s""
I0924 09:57:41.509696       1 flags.go:52] FLAG: --skip-nodes-with-local-storage=""false""
I0924 09:57:41.509700       1 flags.go:52] FLAG: --skip-nodes-with-system-pods=""true""
I0924 09:57:41.509705       1 flags.go:52] FLAG: --stderrthreshold=""2""
I0924 09:57:41.509725       1 flags.go:52] FLAG: --storage-driver-buffer-duration=""1m0s""
I0924 09:57:41.509730       1 flags.go:52] FLAG: --storage-driver-db=""cadvisor""
I0924 09:57:41.509735       1 flags.go:52] FLAG: --storage-driver-host=""localhost:8086""
I0924 09:57:41.509739       1 flags.go:52] FLAG: --storage-driver-password=""root""
I0924 09:57:41.509744       1 flags.go:52] FLAG: --storage-driver-secure=""false""
I0924 09:57:41.509761       1 flags.go:52] FLAG: --storage-driver-table=""stats""
I0924 09:57:41.509766       1 flags.go:52] FLAG: --storage-driver-user=""root""
I0924 09:57:41.509771       1 flags.go:52] FLAG: --test.bench=""""
I0924 09:57:41.509775       1 flags.go:52] FLAG: --test.benchmem=""false""
I0924 09:57:41.509795       1 flags.go:52] FLAG: --test.benchtime=""1s""
I0924 09:57:41.509800       1 flags.go:52] FLAG: --test.blockprofile=""""
I0924 09:57:41.509804       1 flags.go:52] FLAG: --test.blockprofilerate=""1""
I0924 09:57:41.509809       1 flags.go:52] FLAG: --test.count=""1""
I0924 09:57:41.509813       1 flags.go:52] FLAG: --test.coverprofile=""""
I0924 09:57:41.509833       1 flags.go:52] FLAG: --test.cpu=""""
I0924 09:57:41.509837       1 flags.go:52] FLAG: --test.cpuprofile=""""
I0924 09:57:41.509842       1 flags.go:52] FLAG: --test.failfast=""false""
I0924 09:57:41.509846       1 flags.go:52] FLAG: --test.list=""""
I0924 09:57:41.509868       1 flags.go:52] FLAG: --test.memprofile=""""
I0924 09:57:41.509873       1 flags.go:52] FLAG: --test.memprofilerate=""0""
I0924 09:57:41.509877       1 flags.go:52] FLAG: --test.mutexprofile=""""
I0924 09:57:41.509882       1 flags.go:52] FLAG: --test.mutexprofilefraction=""1""
I0924 09:57:41.509886       1 flags.go:52] FLAG: --test.outputdir=""""
I0924 09:57:41.509902       1 flags.go:52] FLAG: --test.parallel=""16""
I0924 09:57:41.509907       1 flags.go:52] FLAG: --test.run=""""
I0924 09:57:41.509951       1 flags.go:52] FLAG: --test.short=""false""
I0924 09:57:41.509959       1 flags.go:52] FLAG: --test.testlogfile=""""
I0924 09:57:41.509982       1 flags.go:52] FLAG: --test.timeout=""0s""
I0924 09:57:41.509988       1 flags.go:52] FLAG: --test.trace=""""
I0924 09:57:41.509994       1 flags.go:52] FLAG: --test.v=""false""
I0924 09:57:41.509999       1 flags.go:52] FLAG: --v=""6""
I0924 09:57:41.510005       1 flags.go:52] FLAG: --version=""false""
I0924 09:57:41.510025       1 flags.go:52] FLAG: --vmodule=""""
I0924 09:57:41.510031       1 flags.go:52] FLAG: --write-status-configmap=""true""
I0924 09:57:41.510054       1 main.go:311] Cluster Autoscaler 1.3.3_

'''

**other cluster-autoscaler log extract:** 
'''
_I0924 12:04:47.228074       1 azure_container_service_pool.go:80] AKS AgentPool profile name: agentpool
I0924 12:04:47.228242       1 utils.go:451] Skipping aks-agentpool-33865844-0 - no node group config
I0924 12:04:47.228300       1 utils.go:451] Skipping aks-agentpool-33865844-1 - no node group config
I0924 12:04:47.228333       1 utils.go:451] Skipping aks-agentpool-33865844-2 - no node group config
I0924 12:04:47.228406       1 utils.go:451] Skipping aks-agentpool-33865844-3 - no node group config
I0924 12:04:47.228493       1 utils.go:451] Skipping aks-agentpool-33865844-4 - no node group config
I0924 12:04:47.228559       1 utils.go:451] Skipping aks-agentpool-33865844-5 - no node group config
I0924 12:04:47.228893       1 static_autoscaler.go:331] Scale down status: unneededOnly=true lastScaleUpTime=2018-09-24 12:03:33.3416408 +0000 UTC m=+22.111044501 lastScaleDownDeleteTime=2018-09-24 12:03:33.3416409 +0000 UTC m=+22.111044701 lastScaleDownFailTime=2018-09-24 12:03:33.3416411 +0000 UTC m=+22.111044801 scaleDownForbidden=false isDeleteInProgress=false_
''''


 **Got a runtime panic: as soon as autoscaler found unshedulable pods** 

'''
_
W0924 12:13:55.585498       1 clusterstate.go:539] Readiness for node group agentpool not found
I0924 12:13:55.585718       1 utils.go:503] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0924 12:13:55.586012       1 static_autoscaler.go:244] Filtering out schedulables
I0924 12:13:55.593844       1 static_autoscaler.go:254] No schedulable pods
I0924 12:13:55.593973       1 scale_up.go:249] Pod test/testdep2-5b68d598f5-v8wwq is unschedulable
I0924 12:13:55.593989       1 scale_up.go:249] Pod test/testdep2-5b68d598f5-jf8ff is unschedulable
I0924 12:13:55.593996       1 scale_up.go:249] Pod test/testdep2-5b68d598f5-8bbbh is unschedulable
I0924 12:13:55.594001       1 scale_up.go:249] Pod test/testdep2-5b68d598f5-r25fz is unschedulable



W0924 12:13:55.766106       1 clusterstate.go:321] Failed to find readiness information for agentpool
W0924 12:13:55.766120       1 clusterstate.go:377] Failed to find readiness information for agentpool
W0924 12:13:55.766127       1 clusterstate.go:321] Failed to find readiness information for agentpool
**panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0xa056e4]**

goroutine 138 [running]:
k8s.io/autoscaler/cluster-autoscaler/utils/errors.ToAutoscalerError(0x38487c6, 0x12, 0x0, 0x0, 0x5cc1b40, 0xc42135a000)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/utils/errors/errors.go:74 +0x74
k8s.io/autoscaler/cluster-autoscaler/core.calculateScaleUpCoresMemoryTotal(0xc42135a100, 0x1, 0x1, 0xc42115b500, 0xc4207b5800, 0x6, 0x8, 0xc4204ac800, 0xc4223b0b48, 0x7f2e43f3e000, ...)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:119 +0x1a4
k8s.io/autoscaler/cluster-autoscaler/core.computeScaleUpResourcesLeftLimits(0xc42135a100, 0x1, 0x1, 0xc42115b500, 0xc4207b5800, 0x6, 0x8, 0xc4202eb730, 0x0, 0x0, ...)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:56 +0x8e
k8s.io/autoscaler/cluster-autoscaler/core.ScaleUp(0xc422d601c0, 0xc4207b5080, 0xc421902000, 0xc421582800, 0x89, 0x100, 0xc4218943c0, 0x6, 0x8, 0xc42045ed90, ...)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:273 +0x57a
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc4211b8b40, 0xbee253209e970f90, 0x3ed22bb7d, 0x5c9ef20, 0x0, 0x0)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:278 +0x1d86
main.run(0xc420843860)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:288 +0x48d
main.main.func2(0xc420054ae0)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:369 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:155 +0x92
_
'''

Does anyone have an idea ?
Thanks 

Regards
Stephane




",closed,False,2018-09-24 12:20:22,2018-10-26 12:59:00
autoscaler,mtougeron,https://github.com/kubernetes/autoscaler/issues/1271,https://api.github.com/repos/kubernetes/autoscaler/issues/1271,new aws ec2 instance types missing from 1.3.3 release,The latest 1.3.3 release is missing the new aws ec2 instance types. Is it possible to get a release cut with those new types? The new types were added 2 weeks ago with PR #1234 ,closed,False,2018-09-24 19:46:45,2019-02-22 10:02:30
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1272,https://api.github.com/repos/kubernetes/autoscaler/issues/1272,VPA - sort conditions on VPA object to avoid floating elements,,closed,True,2018-09-25 03:59:19,2018-09-25 10:21:25
autoscaler,mikeweiwei,https://github.com/kubernetes/autoscaler/pull/1273,https://api.github.com/repos/kubernetes/autoscaler/issues/1273,fix logging calls(cloud_provider_builder.go),,closed,True,2018-09-25 08:12:46,2018-09-25 08:41:36
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1274,https://api.github.com/repos/kubernetes/autoscaler/issues/1274,Cherry-pick: Fix error on node info not found for group,,closed,True,2018-09-25 13:34:48,2018-09-25 15:07:33
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1275,https://api.github.com/repos/kubernetes/autoscaler/issues/1275,Cherry pick of #1076: Fix potential panic,Cherry pick of #1076. Another thing that would've prevented #1270.,closed,True,2018-09-25 15:37:35,2018-09-25 16:57:25
autoscaler,ringtail,https://github.com/kubernetes/autoscaler/pull/1276,https://api.github.com/repos/kubernetes/autoscaler/issues/1276,Add Alibaba Cloud Provider,"Alibaba Cloud Container Service is a high-performance and scalable container application management service that enables you to use Docker and Kubernetes to manage the lifecycle of containerized applications. 

This commit add Alibaba Cloud Provider for cluster autoscaler.

Signed-off-by: ringtail zhongwei.lzw@alibaba-inc.com",closed,True,2018-09-26 09:08:44,2018-09-26 09:14:49
autoscaler,ringtail,https://github.com/kubernetes/autoscaler/pull/1277,https://api.github.com/repos/kubernetes/autoscaler/issues/1277,Add Alibaba Cloud Provider support,"Alibaba Cloud Container Service is a high-performance and scalable container application management service that enables you to use Docker and Kubernetes to manage the lifecycle of containerized applications. 

This commit add Alibaba Cloud Provider for cluster autoscaler.

Signed-off-by: ringtail zhongwei.lzw@alibaba-inc.com",closed,True,2018-09-26 12:34:56,2018-10-17 11:18:21
autoscaler,warmchang,https://github.com/kubernetes/autoscaler/issues/1278,https://api.github.com/repos/kubernetes/autoscaler/issues/1278,"As VPA evolves to beta features in release 1.12, the README and examples need to be updated.","ref: https://github.com/kubernetes/features/issues/21

I can implement these changes. :smiley: ",closed,False,2018-09-27 06:35:43,2018-10-25 01:43:43
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1279,https://api.github.com/repos/kubernetes/autoscaler/issues/1279,Fix info on Min and Max Allowed,Min/Max Allowed does not affect recommendation but actuation.,closed,True,2018-09-27 14:55:04,2018-10-24 09:57:37
autoscaler,ReillyProcentive,https://github.com/kubernetes/autoscaler/issues/1280,https://api.github.com/repos/kubernetes/autoscaler/issues/1280,node-group-auto-discovery no longer accepts multiple tags,"We're running a Kubernetes 1.10.8 cluster in AWS (provisioned via KOPS) with the following flags:
```
--namespace=k8s-autoscaler
--v=4
--stderrthreshold=info
--cloud-provider=aws
--skip-nodes-with-local-storage=false
--expander=least-waste
--node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler=enabled,KubernetesCluster=k8s.ourcluster.domain
```

This worked fine with Autoscaler 1.1.3, however when we upgrade to 1.2.3 it fails with the following error:
```
I0927 17:20:35.816328       1 cloud_provider_builder.go:72] Building aws cloud provider.
I0927 17:20:35.916130       1 request.go:481] Throttling request took 399.921532ms, request: GET:https://100.64.0.1:443/api/v1/services?limit=500&resourceVersion=0
F0927 17:20:35.916206       1 cloud_provider_builder.go:137] Failed to create AWS Manager: invalid key=value pair [tag k8s.io/cluster-autoscaler enabled, KubernetesCluster k8s.ourcluster.domain]
```

Looks like the behavior was changed by this commit:
53fc344ecabbad73c0063d065bc2223d43318033",closed,False,2018-09-27 17:31:12,2018-12-27 21:57:10
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/issues/1281,https://api.github.com/repos/kubernetes/autoscaler/issues/1281,Add Go tools,Go format tools like update-gofmt.go and verify-gofmt.go should be in repo,closed,False,2018-09-28 06:28:02,2018-10-23 06:04:16
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1282,https://api.github.com/repos/kubernetes/autoscaler/issues/1282,Adding  update_gofmt.sh,[WIP]Adding update-gofmt script as part of go format tools,closed,True,2018-09-28 06:34:34,2018-10-19 10:57:02
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1283,https://api.github.com/repos/kubernetes/autoscaler/issues/1283,Updater uses informers to fetch pod controllers.,"This includes SatefulSets, Replicasets and ReplicationContollers.",closed,True,2018-09-28 13:08:15,2018-10-01 11:30:23
autoscaler,cablespaghetti,https://github.com/kubernetes/autoscaler/pull/1284,https://api.github.com/repos/kubernetes/autoscaler/issues/1284,Adds latest EC2 Instance Types to 1.3 Branch - Fixes #1271,Fixes #1271 ,closed,True,2018-09-28 13:44:41,2018-09-28 14:24:48
autoscaler,cablespaghetti,https://github.com/kubernetes/autoscaler/pull/1285,https://api.github.com/repos/kubernetes/autoscaler/issues/1285,Adds latest EC2 Instance Types to 1.2 Branch,Fixes #1271 for 1.2 branch. Helpful for me because I'm stuck on K8S 1.10 in Amazon EKS.,closed,True,2018-09-28 13:45:50,2018-10-01 09:21:28
autoscaler,cablespaghetti,https://github.com/kubernetes/autoscaler/pull/1286,https://api.github.com/repos/kubernetes/autoscaler/issues/1286,AWS: Support selecting ASG by values - 1.2 Branch,"Fixes #1280

I've pushed a built docker container here under the v1.2-1280fix tag: https://hub.docker.com/r/cablespaghetti/cluster-autoscaler/tags/

@ReillyProcentive can you verify that this fixes your problem?",closed,True,2018-09-28 15:09:38,2018-10-01 08:57:46
autoscaler,twaugh,https://github.com/kubernetes/autoscaler/issues/1287,https://api.github.com/repos/kubernetes/autoscaler/issues/1287,autoscaler/addon-resizer/Godeps/Godeps.json has wrong ImportPath,"The ImportPath uses the old name on the master branch:
https://github.com/kubernetes/autoscaler/blob/master/addon-resizer/Godeps/Godeps.json#L2

It seems to be ok on the addon-resizer-release-1.8 branch though:
https://github.com/kubernetes/autoscaler/blob/addon-resizer-release-1.8/addon-resizer/Godeps/Godeps.json#L2
",closed,False,2018-09-28 16:22:48,2018-10-17 11:26:13
autoscaler,shatil,https://github.com/kubernetes/autoscaler/pull/1288,https://api.github.com/repos/kubernetes/autoscaler/issues/1288,Read AWS Region from EC2 Metadata,"Tackles kubernetes/autoscaler#1208. EC2 Metadata helpfully supplies AWS
Region info, so rather than _requiring_ an environment variable, this
patch enables `aws_manager.go` to retrieve that information from EC2
itself.

Example YAMLs no longer reference the vestigial `AWS_REGION` environment
variable, but supplying it to `cluster-autoscaler` still relays it to
the AWS SDK like before.",closed,True,2018-09-29 05:20:23,2018-10-02 13:09:16
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1289,https://api.github.com/repos/kubernetes/autoscaler/issues/1289,Update OWNERS,Syncing with master branch.,closed,True,2018-10-01 08:25:56,2019-03-20 15:39:14
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1290,https://api.github.com/repos/kubernetes/autoscaler/issues/1290,Create OWNERS on 1.2 branch,Syncing with master.,closed,True,2018-10-01 08:27:13,2019-03-20 15:39:16
autoscaler,okgolove,https://github.com/kubernetes/autoscaler/issues/1291,https://api.github.com/repos/kubernetes/autoscaler/issues/1291,[AWS] 'NotTriggerScaleUp' pod didn't trigger scale-up,"Common problem, but I wasn't able to find any solution.
I have ASG, have auto-scaler

I deployed statefulset with application and then I got these errors:

```
auto-scaler-aws-cluster-autoscaler-5f78688568-6q4hn aws-cluster-autoscaler I1002 11:03:26.503432       1 utils.go:196] Pod myapp-server-0 can't be scheduled on nodes.prod.example.com, predicate failed: GeneralPredicates predicate mismatch, cannot put produ
ction/sportradar-uof-server-0 on template-node-for-nodes.prod.example.com-7085033211816415489, reason: node(s) didn't match node selector
auto-scaler-aws-cluster-autoscaler-5f78688568-6q4hn aws-cluster-autoscaler I1002 11:03:26.503455       1 scale_up.go:371] No pod can fit to nodes.prod.example.com
auto-scaler-aws-cluster-autoscaler-5f78688568-6q4hn aws-cluster-autoscaler I1002 11:03:26.503465       1 scale_up.go:376] No expansion options
auto-scaler-aws-cluster-autoscaler-5f78688568-6q4hn aws-cluster-autoscaler I1002 11:03:26.504202       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""production"", Name:""myapp-server-0"", UID:""7a8d1128-c62b-11e8-9f1f-066d51fa4852"", APIVersion:""v1""
, ResourceVersion:""6206696"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```

ASG status:

```

Cluster-autoscaler status at 2018-10-02 11:13:03.741699636 +0000 UTC:
Cluster-wide:
  Health:      Healthy (ready=8 unready=0 notStarted=0 longNotStarted=0 registered=8 longUnregistered=0)
               LastProbeTime:      2018-10-02 11:13:03.611386199 +0000 UTC m=+2086.611779690
               LastTransitionTime: 2018-10-02 10:38:44.809890287 +0000 UTC m=+27.810283749
  ScaleUp:     NoActivity (ready=8 registered=8)
               LastProbeTime:      2018-10-02 11:13:03.611386199 +0000 UTC m=+2086.611779690
               LastTransitionTime: 2018-10-02 10:38:44.809890287 +0000 UTC m=+27.810283749
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2018-10-02 11:13:03.611386199 +0000 UTC m=+2086.611779690
               LastTransitionTime: 2018-10-02 10:38:44.809890287 +0000 UTC m=+27.810283749

NodeGroups:
  Name:        nodes.prod.example.com
  Health:      Healthy (ready=5 unready=0 notStarted=0 longNotStarted=0 registered=5 longUnregistered=0 cloudProviderTarget=5 (minSize=0, maxSize=10))
               LastProbeTime:      2018-10-02 11:13:03.611386199 +0000 UTC m=+2086.611779690
               LastTransitionTime: 2018-10-02 10:38:44.809890287 +0000 UTC m=+27.810283749
  ScaleUp:     NoActivity (ready=5 cloudProviderTarget=5)
               LastProbeTime:      2018-10-02 11:13:03.611386199 +0000 UTC m=+2086.611779690
               LastTransitionTime: 2018-10-02 10:38:44.809890287 +0000 UTC m=+27.810283749
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2018-10-02 11:13:03.611386199 +0000 UTC m=+2086.611779690
               LastTransitionTime: 2018-10-02 10:38:44.809890287 +0000 UTC m=+27.810283749
```",closed,False,2018-10-02 11:14:41,2018-10-02 11:47:02
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1292,https://api.github.com/repos/kubernetes/autoscaler/issues/1292,If possible use nodeInfo based on created node group,,closed,True,2018-10-02 13:06:53,2018-10-02 13:58:53
autoscaler,aermakov-zalando,https://github.com/kubernetes/autoscaler/pull/1293,https://api.github.com/repos/kubernetes/autoscaler/issues/1293,deletetaint: retry on conflicts,"Node objects are frequently updated, and the taint/untaint code doesn't handle conflicts at all, simply returning an error. This causes the rest of the scaling code to abort the downscaling and wait for 15 minutes, only to most likely fail again on the next try. We've observed it multiple times in clusters that needed to be downscaled by 10-20 nodes, and in some situations had to intervene manually because CA couldn't do anything for hours.
Fix by simply retrying the taint/untaint operation until it either succeeds or the number of retries exceeds 50.",closed,True,2018-10-02 14:16:14,2018-10-15 12:07:07
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1294,https://api.github.com/repos/kubernetes/autoscaler/issues/1294,Recalculate clusterStateRegistry after adding multiple node groups,,closed,True,2018-10-02 15:05:35,2018-10-02 15:28:08
autoscaler,mattnworb,https://github.com/kubernetes/autoscaler/issues/1295,https://api.github.com/repos/kubernetes/autoscaler/issues/1295,vpa-recommender panicking in model.(*AggregateContainerState).MergeContainerState,"At first glance this looks similar to #1258 but my panic and stacktrace looks different, also note the pod is up and running ok for some time before it panics:

```
I1002 22:24:32.480954       8 request.go:481] Throttling request took 191.220188ms, request: PATCH:https://10.178.96.1:443/apis/poc.autoscaling.k8s.io/v1alpha1/namespaces/creator-authorization/verticalpodautoscalers/authorization3dev
I1002 22:24:32.489580       8 recommender.go:66] VPA to update #{seti-test test-with-gke-repo}: &{ID:{Namespace:seti-test VpaName:test-with-gke-repo} PodSelector:app=test-with-gke-repo Conditions:map[RecommendationProvided:{Type:RecommendationProvided Status:True LastTransitionTime:2018-07-17 20:27:45 +0000 UTC Reason: Message:}] Recommendation:0xc42036e0a0 aggregateContainerStates:map[] ResourcePolicy:<nil> ContainersInitialAggregateState:map[test-with-gke-repo:0xc4200aff80]}
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0xe05646]

goroutine 1 [running]:
k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/model.(*AggregateContainerState).MergeContainerState(0x0, 0xc4200aff80)
        /usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/model/aggregate_container_state.go:71 +0x26
k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/model.(*Vpa).MergeCheckpointedState(0xc4203fd7a0, 0xc421d0f6e0)
        /usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/model/vpa.go:110 +0xd1
k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/model.(*Vpa).AggregateStateByContainerName(0xc4203fd7a0, 0x1a35958)
        /usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/model/vpa.go:118 +0x47
k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/logic.(*podResourceRecommender).GetRecommendedPodResources(0xc420605020, 0xc4203fd7a0, 0x16)
        /usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/logic/recommender.go:70 +0x40
k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/initialization.(*recommender).updateVPAs(0xc4202807e0)
        /usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/initialization/recommender.go:67 +0x630
k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/initialization.(*recommender).RunOnce(0xc4202807e0)
        /usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/initialization/recommender.go:96 +0xcc
main.main()
        /usr/local/google/home/bskiba/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/main.go:57 +0x158
```

This is with the image k8s.gcr.io/vpa-recommender:0.2.0",closed,False,2018-10-02 22:29:30,2019-02-07 15:24:13
autoscaler,stickyd,https://github.com/kubernetes/autoscaler/pull/1296,https://api.github.com/repos/kubernetes/autoscaler/issues/1296,Grammar correction in printed error message,,closed,True,2018-10-03 19:45:20,2018-10-07 23:36:28
autoscaler,hans-permana,https://github.com/kubernetes/autoscaler/issues/1297,https://api.github.com/repos/kubernetes/autoscaler/issues/1297,"Installation on cloud provider other than AWS, GCP, or Azure","Hi guys,

Is cluster autoscaler installation on a cloud platform other than AWS, GCP, or Azure supported? On the documentation it says that I have to set `cloudProvider` and I don't know what to put in my case. I am using Open Telekom Cloud (OTC).  

I tried not specifying `cloudProvider` and it defaulted to `aws` (and hence resulted in errors). Could someone give me an enlightenment?",closed,False,2018-10-04 12:59:17,2018-10-05 12:30:01
autoscaler,blandman,https://github.com/kubernetes/autoscaler/issues/1298,https://api.github.com/repos/kubernetes/autoscaler/issues/1298,Feature Request: Cluster Autoscaler - scale up Deployment if PCB doesn't meet the scale down/drain requirements,"I'm looking over the cluster-autoscaler source to find where this could be added, not sure I have the go chops to submit a PR, but wanted to share this as a feature request.

I have a very RAM heavy microservice java app (20gb no load), in prod we'll run two of every service for HA. However our dev/test/stage environments it would be too expensive to run more than 1 of each service. 

I have each microservice in a deployment, with a Horizontal Pod Autoscaler to handle usage spikes / performance testing. 

```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: service-autoscaler
  namespace: app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: service
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 700
```

In initial deployment in the dev environment, the cluster autoscaler caused significant occasional downtime as it balanced the cluster. These Java microservices sometimes take 4 minutes to become ready. So to avoid this, I applied a Pod Disruption Budget on each service.

```yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: service-budget
  namespace: app
spec:
  minAvailable: 1
  selector:
    matchLabels:
      run: service
```

So far so good. However since the Deployments have only 1 replica, this permanently blocks the cluster autoscaler from scaling down. 

There is an [issue on Kubernetes](https://github.com/kubernetes/kubernetes/issues/48307) relating to the kubectl drain in this scenario, but it's been closed due to inactivity. Seems like this use case for pdb wasn't recommended because it isn't HA with 1 replica. The argument being, Kubernetes *should* make 1 replica as available as possible were downtime even without a pdb should be avoided at all costs. 

The feature as follows.
- Check for pdb
- If a pdb is blocking a drain, scale the deployment/pod up until it meets the pdb criteria, ensuring the new pods are scheduled on ideal nodes.
- Wait for new Pods to become ready
- Drain the node

Maybe this could result in a new command argument that forces this behavior on all Deployments with 1 replica, without the need for a pdb.

As I search for a place to implement this, I found that [cluster-autoscaler/simulator/drain.go](https://github.com/kubernetes/autoscaler/blob/2908132f14d6edf362353690df6c6dd5a035ea1a/cluster-autoscaler/simulator/drain.go#L87) could be a starting place. From there I'm not sure how this could be implemented, but if there's any guidance where I could begin testing an implementation I'll gladly attempt to figure this out. 

One issue I see off the get go is determining if the pod is attached to a deployment, and if there is a horizontal autoscaler to read from to determine if scaling up is acceptable. ",open,False,2018-10-04 18:45:32,2019-03-20 20:48:41
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1299,https://api.github.com/repos/kubernetes/autoscaler/issues/1299,Addon-resizer/Godeps.json using wrong import path,Closes  #1287,closed,True,2018-10-05 05:23:14,2018-10-17 11:26:13
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1300,https://api.github.com/repos/kubernetes/autoscaler/issues/1300,Small clarification in FAQ (how scale-down works),,closed,True,2018-10-08 07:34:27,2019-03-20 15:37:11
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1301,https://api.github.com/repos/kubernetes/autoscaler/issues/1301,VPA - limit number of checkpoints written in main loop without starvation,,closed,True,2018-10-08 17:30:28,2018-10-09 14:26:38
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1302,https://api.github.com/repos/kubernetes/autoscaler/issues/1302,Don't move min/max bucket on sub-epsilon samples,"The first commit partially fixes a problem with ""zero recommendation"" appearing in some new clusters.
The next ones are unrelated minor clean-ups.

The rest of the ""zero recommendation"" fix will be part of another PR.

/assign @bskiba ",closed,True,2018-10-09 08:40:25,2018-10-09 11:09:19
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1303,https://api.github.com/repos/kubernetes/autoscaler/issues/1303,Keep small-weight samples for at least MemoryAggregationWindowLength,"Together with PR #1302 this fixes a problem with ""zero recommendation"" appearing in some new clusters.

NOTE: This PR changes how long samples are kept, in particular when restoring a checkpoint.

/assign @bskiba 
/cc @schylek ",closed,True,2018-10-09 08:54:04,2018-10-10 09:26:47
autoscaler,hadr10,https://github.com/kubernetes/autoscaler/issues/1304,https://api.github.com/repos/kubernetes/autoscaler/issues/1304,FailedToScaleUpGroup with version 1.3.3,"Hello,

when i tested new version 1.3.3 i got error message during scaling:
Events:
  Type     Reason                Age              From                Message
  ----     ------                ----             ----                -------
  Warning  FailedToScaleUpGroup  1m (x2 over 7m)  cluster-autoscaler  Scale-up failed for group agentpool: containerserv
ice.ManagedClustersClient#CreateOrUpdate: Invalid input: autorest/validation: validation failed: parameter=parameters.Ma
nagedClusterProperties.AadProfile.ServerAppSecret constraint=Null value=(*string)(nil) details: value can not be null; required parameter

With version 1.3.1 it works fine.

Thank you,
HH",closed,False,2018-10-09 10:27:41,2018-11-20 06:33:36
autoscaler,mooncak,https://github.com/kubernetes/autoscaler/pull/1305,https://api.github.com/repos/kubernetes/autoscaler/issues/1305,Fix typos issues,"Fix typos issues in below files:

1. cluster-autoscaler/cloudprovider/azure/azure_util.go
2. vertical-pod-autoscaler/pkg/recommender/model/aggregate_container_state.go",closed,True,2018-10-09 15:19:51,2018-10-10 07:45:50
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1306,https://api.github.com/repos/kubernetes/autoscaler/issues/1306,Ignore lo/fluentd-ds-ready when checking node similarity,,closed,True,2018-10-10 07:45:37,2018-10-10 10:55:58
autoscaler,ringtail,https://github.com/kubernetes/autoscaler/pull/1307,https://api.github.com/repos/kubernetes/autoscaler/issues/1307,Add alicloud cloudprovider support ,The previous PR is #1277 . This PR remove packages from vendor and extract the core of SDK to source code.,closed,True,2018-10-10 09:43:23,2018-10-10 10:54:17
autoscaler,mikeweiwei,https://github.com/kubernetes/autoscaler/pull/1308,https://api.github.com/repos/kubernetes/autoscaler/issues/1308,Copy the original styles,Copy the original styles (the scheduleDeleteEmptyNodes() function),closed,True,2018-10-10 10:57:10,2018-10-10 11:10:03
autoscaler,ringtail,https://github.com/kubernetes/autoscaler/pull/1309,https://api.github.com/repos/kubernetes/autoscaler/issues/1309,Add Alibaba Cloud Provider support with no vendor,The previous PR is #1277 . This PR remove packages from vendor and extract the core of SDK to source code.,closed,True,2018-10-10 12:10:38,2018-10-23 15:28:38
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1310,https://api.github.com/repos/kubernetes/autoscaler/issues/1310,Update generated VPA code.,"Includes getting changes to verify-booilerplate from k/k as the
boilerplate for the generated code has changed.",closed,True,2018-10-10 13:21:48,2018-10-11 11:08:07
autoscaler,gongguan,https://github.com/kubernetes/autoscaler/issues/1311,https://api.github.com/repos/kubernetes/autoscaler/issues/1311,How to add customized kubernetes predicate,"Autoscaler can't get some predicates which configured in kube-scheduler, such as PodFitsHostPorts, HostName, etc.
It seems that autoscaler only get predicate in priorityPredicates and k8s defaultPredicate(in cluster-autoscaler/simulator/predicates.go). Can predicates be customized?
`var priorityPredicates = []string{""PodFitsResources"", ""GeneralPredicates"", ""PodToleratesNodeTaints""}`
`predicateMap, err := schedulerConfigFactory.GetPredicates(provider.FitPredicateKeys)`",closed,False,2018-10-11 03:25:49,2018-10-11 10:58:48
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1312,https://api.github.com/repos/kubernetes/autoscaler/issues/1312,VPA drain context.Done() channel in checkpoint writer.,,closed,True,2018-10-11 09:17:42,2018-10-11 11:26:38
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1313,https://api.github.com/repos/kubernetes/autoscaler/issues/1313,Beta API for VPA,"Beta API for Vertical Pod Autoscaler.
Contains:
* VPA beta CRD
* Definitions of beta VPA objects
* Autogenerated client code",closed,True,2018-10-11 11:43:48,2018-10-15 14:56:00
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1314,https://api.github.com/repos/kubernetes/autoscaler/issues/1314,Add sigdescribe to VPA e2e tests.,,closed,True,2018-10-11 19:18:21,2018-10-12 07:12:19
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1315,https://api.github.com/repos/kubernetes/autoscaler/issues/1315,VPA recommender/updater main loop fixed.,+ typo fixed,closed,True,2018-10-12 06:17:00,2018-10-12 07:18:28
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1316,https://api.github.com/repos/kubernetes/autoscaler/issues/1316,Treat nodes from same GKE node pool similar,,closed,True,2018-10-15 07:02:46,2018-10-17 11:26:38
autoscaler,frittentheke,https://github.com/kubernetes/autoscaler/issues/1317,https://api.github.com/repos/kubernetes/autoscaler/issues/1317,Feature Request: Allow individual values of scale-down-utilization-threshold for CPU and memory,"When adjusting scale down limits it's far less risky to go full on Mad Max on the CPU as a resource than it is when talking about memory: When you run out of CPU everything becomes slower, when running out of memory things begin to crash.

Currently the cluster-autoscaler does not allow individual values for CPU and memory, _scale-down-utilization-threshold_ is always affecting both using the one with the higher value to determine if it can scale down more nodes. I suggest to allow for both resources to be configured to a different level, i.e. to allow a CPU headroom of only 10% while still only scaling down when there is more than 50% for memory still left. 

This would not be a breaking change, nobody is forced to move away from one value for both. I simply believe some workloads differ greatly in their required headroom of CPU and memory. A more flexible configuration allows to run those clusters more efficiently in regards to cluster scaling.",open,False,2018-10-15 09:34:07,2019-01-17 08:29:23
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1318,https://api.github.com/repos/kubernetes/autoscaler/issues/1318,Open up common utility functions for VPA e2e tests,@schylek ,closed,True,2018-10-15 14:03:24,2018-10-16 09:15:16
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1319,https://api.github.com/repos/kubernetes/autoscaler/issues/1319,Refactor cloud provider builder to allow builds for a single provider,"This PR:
- moves code for building cloud provider objects inside the respective modules,
- adds support for creating {AWS,AZURE,GCE,Kubemark}-only builds.

Binary size depending on build:
- all (default): 166MB
- AWS: 65MB
- Azure: 56MB
- GCE: 75MB
- Kubemark: 164MB ",closed,True,2018-10-15 16:31:20,2018-10-17 11:24:07
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1320,https://api.github.com/repos/kubernetes/autoscaler/issues/1320,E2e test for no PDB violations.,@kgolab ,closed,True,2018-10-16 15:21:03,2018-10-18 09:24:12
autoscaler,Office-Manager,https://github.com/kubernetes/autoscaler/issues/1321,https://api.github.com/repos/kubernetes/autoscaler/issues/1321, NoCredentialProviders: no valid providers in chain.,"Hi all ,

Running EKS 1.10 so I've been attempting to use the 1.2 release cluster autoscaler

The pod will spin up but looking at the error logs I notice
```

1 static_autoscaler.go:118] Failed to update node registry: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors
```

I can confirm all the EKS worker nodes have the IAM policy attached to the instance 
If there's any additional information I can give please let me know and I'll update this asap",closed,False,2018-10-16 19:38:56,2018-11-13 10:36:14
autoscaler,panaji,https://github.com/kubernetes/autoscaler/issues/1322,https://api.github.com/repos/kubernetes/autoscaler/issues/1322,Cluster autoscaler hang,"CA hang (there are no more logs) right after this message:

I1016 23:27:56.944166       1 scale_up.go:199] Best option to resize: agentpool1
I1016 23:27:56.944199       1 scale_up.go:203] Estimated 1 nodes needed in agentpool1
I1016 23:27:56.944213       1 scale_up.go:292] Final scale-up plan: [{agentpool1 10->11 (max: 40)}]
I1016 23:27:56.944223       1 scale_up.go:344] Scale-up: setting group agentpool1 size to 11
I1016 23:27:57.019743       1 azure_agent_pool.go:209] Waiting for deploymentsClient.CreateOrUpdate(<cluster_name>, cluster-autoscaler-1834391606, {%!s(*resources.DeploymentProperties=&{0xc4200f73a8 <nil> 0xc4200f73b0 <nil> Incremental <nil>})})
E1016 23:33:03.549684       1 azure_agent_pool.go:324] Delete virtual machine ""k8s-agentpool1-41420005-3"" failed: azure#WaitForCompletion: context has been cancelled: StatusCode=200 -- Original Error: context deadline exceeded
I1016 23:33:03.560675       1 delete.go:106] Releasing taint {Key:ToBeDeletedByClusterAutoscaler Value:1539731882 Effect:NoSchedule TimeAdded:<nil>} on node k8s-agentpool1-41420005-3
I1016 23:33:03.576884       1 delete.go:119] Successfully released toBeDeletedTaint on node k8s-agentpool1-41420005-3

Kubernetes Version : 1.10.1
Cluster Autoscaler version : v1.2.2
Cloud Provider : Azure (ACS)
VMType: standard

Please Advise.",closed,False,2018-10-17 01:36:24,2018-10-17 01:42:21
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1323,https://api.github.com/repos/kubernetes/autoscaler/issues/1323,Remove Kubemark cloud provider from default build,"From #1319, CA binary size depending on build:
- all (default): 166MB
- AWS: 65MB
- Azure: 56MB
- GCE: 75MB
- Kubemark: 164MB

Removing Kubemark from all (default) build reduces it to ~98 MB. Given that it's only useful specifically for scalability testing, we can probably remove it from default image altogether.",closed,True,2018-10-17 11:47:35,2018-10-17 11:59:07
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1324,https://api.github.com/repos/kubernetes/autoscaler/issues/1324,Bump CA version to 1.3.4,,closed,True,2018-10-17 11:57:26,2018-10-17 12:09:06
autoscaler,xichengliudui,https://github.com/kubernetes/autoscaler/pull/1325,https://api.github.com/repos/kubernetes/autoscaler/issues/1325,Remove duplicate words,Remove duplicate words,closed,True,2018-10-18 07:15:09,2018-10-18 07:39:55
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1326,https://api.github.com/repos/kubernetes/autoscaler/issues/1326,Ignore lo/fluentd-ds-ready when checking node similarity,,closed,True,2018-10-18 11:24:43,2018-10-18 11:45:50
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1327,https://api.github.com/repos/kubernetes/autoscaler/issues/1327,Start using beta VPA API,"The permissions and scripts are updated so that both apis can be used.
This way both ./hack/vpa-up.sh and ./hack/run-e2e.sh will work.",closed,True,2018-10-18 11:24:46,2018-10-18 11:43:50
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1328,https://api.github.com/repos/kubernetes/autoscaler/issues/1328,Switch admission controller to v1beta1,Also adds one change to script that was missed in #1327 ,closed,True,2018-10-18 12:28:05,2018-10-18 12:46:03
autoscaler,heltondd,https://github.com/kubernetes/autoscaler/issues/1329,https://api.github.com/repos/kubernetes/autoscaler/issues/1329,Azure cluster autoscaler upgrade all virtual machines in vmss,"When image is changed in vmss, cluster autoscaler execute upgrade in all virtual machines in vmss at same time, causing indisponibilty.

**Kubernetes 1.10.5**
**Cluster-autoscaler: 1.2.2**

**luster-autoscaler log:**
....
I1017 16:52:06.296148       1 utils.go:456] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I1017 16:52:06.296471       1 scale_up.go:59] Pod default/kuard-79d85cc498-sbgs6 is unschedulable
I1017 16:52:06.315470       1 scale_up.go:199] Best option to resize: k8s-small-23489316-vmss
I1017 16:52:06.315509       1 scale_up.go:203] Estimated 1 nodes needed in k8s-small-23489316-vmss
I1017 16:52:06.315523       1 scale_up.go:292] Final scale-up plan: [{k8s-small-23489316-vmss 5->6 (max: 10)}]
I1017 16:52:06.315535       1 scale_up.go:344] Scale-up: setting group k8s-small-23489316-vmss size to 6
I1017 16:56:39.759928       1 azure_manager.go:261] Refreshed ASG list, next refresh after 2018-10-17 16:57:39.759919743 +0000 UTC
I1017 16:56:40.139878       1 utils.go:456] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I1017 16:56:40.140395       1 scale_up.go:59] Pod default/kuard-79d85cc498-tf7n6 is unschedulable
I1017 16:56:40.140411       1 scale_up.go:59] Pod default/kuard-79d85cc498-7rsd9 is unschedulable
I1017 16:56:40.140415       1 scale_up.go:59] Pod default/kuard-79d85cc498-h2cln is unschedulable
I1017 16:56:40.322798       1 scale_up.go:199] Best option to resize: k8s-small-23489316-vmss
I1017 16:56:40.322832       1 scale_up.go:203] Estimated 2 nodes needed in k8s-small-23489316-vmss
I1017 16:56:40.322847       1 scale_up.go:292] Final scale-up plan: [{k8s-small-23489316-vmss 6->8 (max: 10)}]
I1017 16:56:40.322857       1 scale_up.go:344] Scale-up: setting group k8s-small-23489316-vmss size to 8
I1017 17:06:09.556365       1 azure_manager.go:261] Refreshed ASG list, next refresh after 2018-10-17 17:07:09.556355068 +0000 UTC
I1017 17:06:10.111621       1 utils.go:456] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I1017 17:06:10.111979       1 static_autoscaler.go:280] No unschedulable pods
....
**Scale Node: ""Create or Upgrade Virtual Machine Scale Set""  initiate by clientid used by cluster autoscaler**

JSON:
....
""operationName"": {
        ""value"": ""Microsoft.Compute/virtualMachineScaleSets/write"",
        ""localizedValue"": ""Create or Update Virtual Machine Scale Set""
    },
    ""resourceGroupName"": ""k8s"",
    ""resourceProviderName"": {
        ""value"": ""Microsoft.Compute"",
        ""localizedValue"": ""Microsoft.Compute""
    },
    ""resourceType"": {
        ""value"": ""Microsoft.Compute/virtualMachineScaleSets"",
        ""localizedValue"": ""Microsoft.Compute/virtualMachineScaleSets""
    },
    ""resourceId"": ""/subscriptions/xxxx/resourcegroups/k8s/providers/Microsoft.Compute/virtualMachineScaleSets/k8s-small-23489316-vmss"",
    ""status"": {
        ""value"": ""Succeeded"",
        ""localizedValue"": ""Succeeded""
    },
....
**Next Log is ""Manual Upgrade Virtual Machine Scale Set"" initiate by clientid used by cluster autoscaler**

JSON:
.....
operationName"": {
        ""value"": ""Microsoft.Compute/virtualMachineScaleSets/manualupgrade/action"",
        ""localizedValue"": ""Manual Upgrade Virtual Machine Scale Set""
    },
    ""resourceGroupName"": ""k8s"",
    ""resourceProviderName"": {
        ""value"": ""Microsoft.Compute"",
        ""localizedValue"": ""Microsoft.Compute""
    },
    ""resourceType"": {
        ""value"": ""Microsoft.Compute/virtualMachineScaleSets"",
        ""localizedValue"": ""Microsoft.Compute/virtualMachineScaleSets""
    },
    ""resourceId"": ""/subscriptions/xxx/resourceGroups/k8s/providers/Microsoft.Compute/virtualMachineScaleSets/k8s-small-23489316-vmss"",
    ""status"": {
        ""value"": ""Succeeded"",
        ""localizedValue"": ""Succeeded""
....

**VMSS Upgrade Policy:**
az vmss show -g k8s -n k8s-small-23489316-vmss -o json | grep -i upgrade
  ""upgradePolicy"": {
    ""automaticOsUpgradePolicy"": {
      ""enableAutomaticOsUpgrade"": false
    ""rollingUpgradePolicy"": null
          ""autoUpgradeMinorVersion"": true,
          ""autoUpgradeMinorVersion"": true,",open,False,2018-10-18 14:07:40,2019-03-11 10:41:09
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1330,https://api.github.com/repos/kubernetes/autoscaler/issues/1330,Store single ScaleUpRequest per node group,,closed,True,2018-10-18 15:52:56,2018-10-19 09:04:19
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1331,https://api.github.com/repos/kubernetes/autoscaler/issues/1331,Allow updating Increase for ScaleUpRequest in ClusterStateRegistry,,closed,True,2018-10-19 09:45:54,2018-10-19 16:34:50
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1332,https://api.github.com/repos/kubernetes/autoscaler/issues/1332,[WIP] Handle scale-up quota/stockout errors ,,closed,True,2018-10-19 16:34:07,2018-12-07 10:25:03
autoscaler,xmik,https://github.com/kubernetes/autoscaler/pull/1333,https://api.github.com/repos/kubernetes/autoscaler/issues/1333,Update FAQ: remove info on not existent flag --unregistered-node-removal-time,The `--unregistered-node-removal-time` was removed in https://github.com/kubernetes/autoscaler/pull/469 but it is still mentioned in FAQ.,closed,True,2018-10-19 18:41:26,2018-10-22 18:35:18
autoscaler,rifelpet,https://github.com/kubernetes/autoscaler/pull/1334,https://api.github.com/repos/kubernetes/autoscaler/issues/1334,Update volume topological scheduling FAQ,This kubernetes feature has since been released. I also added node group setup suggestions based on my experiences with AWS ASGs but i believe it applies to other cloud providers as well.,closed,True,2018-10-19 22:30:52,2018-10-22 18:53:19
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1335,https://api.github.com/repos/kubernetes/autoscaler/issues/1335,VPA reducing memory pressure for inspecting pod updates,,closed,True,2018-10-22 09:39:12,2018-10-22 20:05:23
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1336,https://api.github.com/repos/kubernetes/autoscaler/issues/1336,Add metrics for GCP API usage,"Request counter for now (split by resource & verb).

```
# TYPE cluster_autoscaler_gce_request_count counter
cluster_autoscaler_gce_request_count{resource=""instance_group_managers"",verb=""get""} 35
cluster_autoscaler_gce_request_count{resource=""instance_group_managers"",verb=""list_managed_instances""} 18
cluster_autoscaler_gce_request_count{resource=""instance_templates"",verb=""get""} 1
cluster_autoscaler_gce_request_count{resource=""machine_types"",verb=""get""} 1
cluster_autoscaler_gce_request_count{resource=""machine_types"",verb=""list""} 1
# HELP cluster_autoscaler_gke_request_count Counter of GKE API requests for each verb and API resource.
# TYPE cluster_autoscaler_gke_request_count counter
cluster_autoscaler_gke_request_count{resource=""clusters"",verb=""get""} 3
```",closed,True,2018-10-22 10:47:45,2018-11-06 15:32:56
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1337,https://api.github.com/repos/kubernetes/autoscaler/issues/1337,Cluster Autoscaler 1.2.4,Update Cluster Autoscaler version to 1.2.4,closed,True,2018-10-22 10:59:11,2019-01-07 14:36:51
autoscaler,anshrma,https://github.com/kubernetes/autoscaler/issues/1338,https://api.github.com/repos/kubernetes/autoscaler/issues/1338,Question : Autoscaler to add two nodes (instead of 1) when scaling up on AWS,"Looking for guidence/configuration/if feasible currently, for the autoscaler to add two nodes instead of one on AWS ?",open,False,2018-10-22 23:03:12,2019-03-22 18:06:00
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1339,https://api.github.com/repos/kubernetes/autoscaler/issues/1339,Fix Go tools on Cluster Autoscaler 1.2 branch,Presubmits are broken because of golint (e.g. seen on #1337),closed,True,2018-10-23 11:03:50,2018-10-23 11:39:07
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1340,https://api.github.com/repos/kubernetes/autoscaler/issues/1340,NodeGroup.Nodes() return Instance struct instead instance name,"This is preparatory work for handling resource related
(stockout/quota-exceeded) error conditions in CA.",closed,True,2018-10-23 15:44:46,2018-10-26 13:06:25
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/1341,https://api.github.com/repos/kubernetes/autoscaler/issues/1341,"Move nodegroup balancing to processor, add GKE-specific implementation",This allows customizing the balancing logic for different use-cases. In particular this PR implements GKE specific version (only enabled if provider is gke) that considers node groups with the same gke-nodepool as similar.,closed,True,2018-10-23 15:56:13,2018-10-26 14:59:32
autoscaler,rbtcollins,https://github.com/kubernetes/autoscaler/issues/1342,https://api.github.com/repos/kubernetes/autoscaler/issues/1342,autoscaler 1.1.1 failed to detect changed ASG max size,We had a situation today where autoscaler 1.1.1 on a 1.9.6 k8s cluster refused to scale up a node group because it was at its limit. This puzzled us as we'd just added 30 nodes to the limit. After a couple of hours of this we restarted the autoscaler and it correctly resized the ASG at that point.,open,False,2018-10-24 03:37:57,2019-04-02 06:15:13
autoscaler,TinySong,https://github.com/kubernetes/autoscaler/pull/1343,https://api.github.com/repos/kubernetes/autoscaler/issues/1343,fix typo,,closed,True,2018-10-24 09:45:48,2018-10-24 10:18:41
autoscaler,anjensan,https://github.com/kubernetes/autoscaler/pull/1344,https://api.github.com/repos/kubernetes/autoscaler/issues/1344,Refactor - add factories for Recommender & ClusterFeedProvider,"PR allows to pass custom implementations of components / clients to Recommender & ClusterFeedProvider structs. It might be used to shared some components (kube clients etc).
",closed,True,2018-10-24 10:52:10,2018-10-25 11:25:20
autoscaler,andrewsykim,https://github.com/kubernetes/autoscaler/issues/1345,https://api.github.com/repos/kubernetes/autoscaler/issues/1345,"Does cluster-autoscaler need to support ""out-of-tree"" cloud providers? ","To be consistent with kubernetes/kubernetes, maybe it's time to consider an ""out-of-tree"" cloud provider model. Or at the very least, have each cloud-provider's implementation of the autoscaler in their respective repos (https://github.com/kubernetes/cloud-provider-gcp for GCE, https://github.com/kubernetes/cloud-provider-aws for AWS, etc) and have this repo import them. 

cc @mwielgus ",open,False,2018-10-24 13:09:14,2019-03-22 18:05:59
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/1346,https://api.github.com/repos/kubernetes/autoscaler/issues/1346,AWS unit tests flaking on 1.2 branch,"Failure: https://travis-ci.org/kubernetes/autoscaler/jobs/445110392

Most likely a flake caused by assuming the order of iteration over keys in a map (which is randomized in Go):

```
DescribeTagsPages(*autoscaling.DescribeTagsInput,func(*autoscaling.DescribeTagsOutput, bool) bool)
		0: {
  Filters: [{
      Name: ""key"",
      Values: [""anothertag""]
    },{
      Name: ""key"",
      Values: [""tag""]
    }],
  MaxRecords: 100
}
		1: (func(*autoscaling.DescribeTagsOutput, bool) bool)(0x11b2ca0)
The closest call I have is: 
DescribeTagsPages(*autoscaling.DescribeTagsInput,mock.AnythingOfTypeArgument)
		0: {
  Filters: [{
      Name: ""key"",
      Values: [""tag""]
    },{
      Name: ""key"",
      Values: [""anothertag""]
    }],
  MaxRecords: 100
}
		1: ""func(*autoscaling.DescribeTagsOutput, bool) bool""
Difference found in argument 0:
--- Expected
+++ Actual
@@ -3,6 +3,6 @@
       Name: ""key"",
-      Values: [""anothertag""]
+      Values: [""tag""]
     },{
       Name: ""key"",
-      Values: [""tag""]
+      Values: [""anothertag""]
     }],
```

cc @cablespaghetti ",closed,False,2018-10-24 13:24:18,2019-01-07 16:54:50
autoscaler,gjtempleton,https://github.com/kubernetes/autoscaler/pull/1347,https://api.github.com/repos/kubernetes/autoscaler/issues/1347,Add 2 missing CA flags to param docs,"Simply adds 2 CA flags that are defined but not documented outside of the code currently to the relevant FAQ docs.

Maintained the ordering from `main.go`'s parsing",closed,True,2018-10-24 19:53:49,2018-10-25 13:47:43
autoscaler,damienwebdev,https://github.com/kubernetes/autoscaler/issues/1348,https://api.github.com/repos/kubernetes/autoscaler/issues/1348,"Azure Fails to scale up nodegroup after setting new ""Min""","I've updated the ""min"" on a deployment of cluster autoscaler on AKS from ""4"" to ""6"".

Not only did it not immediately start to scale up `2` new nodes to accomodate the request, now, when I try to apply pod cpu pressure to attempt to scale up nodes I get

```
Warning  FailedToScaleUpGroup  15s   cluster-autoscaler  Scale-up failed for group agentpool: Target size 5 requested outside Max: 10, Min: 6
```

It seems that it won't scale up to ""min"" even if told to. I've tried cluster-autoscaler v1.3.3 and cluster-autoscaler v1.2.2",open,False,2018-10-25 02:50:29,2019-02-22 15:19:44
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1349,https://api.github.com/repos/kubernetes/autoscaler/issues/1349,Add informational UncappedTarged field to VPA api.,,closed,True,2018-10-25 10:56:19,2018-10-25 13:03:43
autoscaler,SleepyBrett,https://github.com/kubernetes/autoscaler/issues/1350,https://api.github.com/repos/kubernetes/autoscaler/issues/1350,[Cluster Autoscaler] --IgnoreTaint=sometaint proposal.,"For our clusters we use Immutable nodes and therefore certain changes require us to roll over our node pools. We currently use the following system.

1) We apply the new node config to the cloud provider, assuring that new nodes that appear will have the new configuration.

2) We taint all the existing nodes in the pool with `recycle=true:NoSchedule` assuring that any pods that get drained don't just end up on another old node leading to constant leapfrogging.

3) We have a pod that looks for the recycle taint, when it finds one it drains the node and calls the cloud provider to terminate it.

The problem we are finding is that the cluster autoscale in this scenario will not actually scale up these node pools. My assumption is that since it sees all the nodes in the pool as tainted with `recycle=true:NoSchedule` it has decided that any node that comes from scaling that pool up will result in the taint. This makes a certain amount of sense.

I'm proposing a flag that allows us to pass taints that should be ignored by the cluster autoscaler for purposes of scaleup.

Our current workaround is to manually go into the cloud provider and scale the group up by 1 so that it will have one non-tainted ""new configuration"" to disrupt the behavior.
",open,False,2018-10-25 14:59:06,2019-04-03 17:49:55
autoscaler,justinsb,https://github.com/kubernetes/autoscaler/issues/1351,https://api.github.com/repos/kubernetes/autoscaler/issues/1351,vertical-pod-autoscaler Godeps.json referencing wrong version (?),"While trying out go modules with vertical-pod-autoscaler, I encountered hundreds (?) of errors like this:

```
...
go: converting Godeps/Godeps.json: stat k8s.io/apimachinery/pkg/fields@bd0b8021277293fc71a2f800f9ec5901cc676269: unknown revision bd0b8021277293fc71a2f800f9ec5901cc676269
...
```

And indeed, it seems to be a bad revision: https://github.com/kubernetes/apimachinery/commit/bd0b8021277293fc71a2f800f9ec5901cc676269

It's not a k/k revision either, so I'm not entirely sure what's going on...",closed,False,2018-10-25 15:33:23,2019-02-01 16:15:04
autoscaler,justinsb,https://github.com/kubernetes/autoscaler/pull/1352,https://api.github.com/repos/kubernetes/autoscaler/issues/1352,Fix Fatalf format string,The error argument was omitted.,closed,True,2018-10-25 16:45:23,2018-10-25 16:59:22
autoscaler,justinsb,https://github.com/kubernetes/autoscaler/pull/1353,https://api.github.com/repos/kubernetes/autoscaler/issues/1353,Remove vertical-pod-autoscaler dependencies on k/k,"* SelfLink was unused
* Build Schemas as needed
* Repoint imports to k8s.io/api",closed,True,2018-10-25 16:46:36,2019-02-01 16:39:00
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1354,https://api.github.com/repos/kubernetes/autoscaler/issues/1354,Recommender capps recommendation according to policy.,,closed,True,2018-10-25 16:54:21,2018-10-26 16:25:51
autoscaler,MrSaints,https://github.com/kubernetes/autoscaler/pull/1355,https://api.github.com/repos/kubernetes/autoscaler/issues/1355,Update FAQ on overprovisioning to account for k8s 1.11,"In k8s 1.11, pod priority, and preemption is enabled by default.
The API is under `v1beta`, and they do not need to be enabled.",closed,True,2018-10-26 10:56:56,2018-10-26 11:12:32
autoscaler,frobware,https://github.com/kubernetes/autoscaler/pull/1356,https://api.github.com/repos/kubernetes/autoscaler/issues/1356,gce: increase test timeout in TestWaitForOp,"On local hardware I have not seen this test fail using the current
50ms timeout. On AWS/CI I see this fail occasionally; I built and
copied this test to an AWS node and run it repeatedly for ~1 hour. The
min was 5ms and the max was 268ms, so bumping the timeout to 500ms.

Signed-off-by: Andrew McDermott <amcdermo@redhat.com>",closed,True,2018-10-26 14:34:16,2019-03-22 06:21:35
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1357,https://api.github.com/repos/kubernetes/autoscaler/issues/1357,Modify execution_latency_seconds buckets,,closed,True,2018-10-26 16:56:48,2018-10-26 18:15:51
autoscaler,rayl15,https://github.com/kubernetes/autoscaler/issues/1358,https://api.github.com/repos/kubernetes/autoscaler/issues/1358,Cluster is not scaling down,"Testing the cluster-autoscaler configured as per https://eksworkshop.com/scaling/deploy_ca.files/cluster_autoscaler.yml.

Testing scenario 1:
At first when manually scaled up pods with increase in number of replicas things were good. Cluster's node scaled up and on reducing replica it went down. 

Testing scenario 2:
Created ingress controller, service, deployment and configured resource and limits with HPA to scale on memory and cpu. Metric server configured.

Load tested using siege tool. The cluster scaled up to 8 nodes. The  autoscaling configuration on aws autoscaling is min 3 max 10.

Issue:
Nodes are not scaling down. On checking autoscaler logs the nodes are being ignored. After 10 hours nodes are still getting ignored to scale down. 



```I1027 09:28:35.464736       1 scale_down.go:175] Scale-down calculation: ignoring 8 nodes, that were unremovable in the last 5m0s
I1027 09:28:35.464940       1 static_autoscaler.go:352] Scale down status: unneededOnly=false lastScaleUpTime=2018-10-26 22:27:42.99390411 +0000 UTC lastScaleDownDeleteTime=2018-10-26 21:38:38.941042585 +0000 UTC lastScaleDownFailTime=2018-10-26 21:38:38.941042981 +0000 UTC schedulablePodsPresent=false isDeleteInProgress=false
I1027 09:28:35.464960       1 static_autoscaler.go:355] Starting scale down
I1027 09:28:35.610526       1 scale_down.go:446] No candidates for scale down
I1027 09:28:36.499784       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1027 09:28:38.509926       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1027 09:28:40.585753       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1027 09:28:42.679449       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1027 09:28:44.689159       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1027 09:28:45.620706       1 static_autoscaler.go:114] Starting main loop
I1027 09:28:45.620732       1 aws_manager.go:241] Refreshed ASG list, next refresh after 2018-10-27 09:29:45.62072788 +0000 UTC
I1027 09:28:45.865477       1 utils.go:456] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I1027 09:28:45.865500       1 static_autoscaler.go:263] Filtering out schedulables
I1027 09:28:45.865875       1 static_autoscaler.go:273] No schedulable pods
I1027 09:28:45.865896       1 static_autoscaler.go:280] No unschedulable pods
I1027 09:28:45.865909       1 static_autoscaler.go:322] Calculating unneeded nodes
I1027 09:28:45.984170       1 scale_down.go:175] Scale-down calculation: ignoring 8 nodes, that were unremovable in the last 5m0s
I1027 09:28:45.984368       1 static_autoscaler.go:352] Scale down status: unneededOnly=false lastScaleUpTime=2018-10-26 22:27:42.99390411 +0000 UTC lastScaleDownDeleteTime=2018-10-26 21:38:38.941042585 +0000 UTC lastScaleDownFailTime=2018-10-26 21:38:38.941042981 +0000 UTC schedulablePodsPresent=false isDeleteInProgress=false
I1027 09:28:45.984389       1 static_autoscaler.go:355] Starting scale down
I1027 09:28:46.095594       1 scale_down.go:446] No candidates for scale down
I1027 09:28:46.769568       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1027 09:28:48.779873       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1027 09:28:49.376320       1 reflector.go:428] k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87: Watch close - *v1.Service total 0 items received
I1027 09:28:50.975226       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1027 09:28:52.986106       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1027 09:28:55.069521       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1027 09:28:56.106729       1 static_autoscaler.go:114] Starting main loop
I1027 09:28:56.434579       1 utils.go:456] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I1027 09:28:56.434601       1 static_autoscaler.go:263] Filtering out schedulables
I1027 09:28:56.434809       1 static_autoscaler.go:273] No schedulable pods
I1027 09:28:56.434825       1 static_autoscaler.go:280] No unschedulable pods
I1027 09:28:56.434834       1 static_autoscaler.go:322] Calculating unneeded nodes
I1027 09:28:56.552143       1 scale_down.go:175] Scale-down calculation: ignoring 8 nodes, that were unremovable in the last 5m0s
I1027 09:28:56.552341       1 static_autoscaler.go:352] Scale down status: unneededOnly=false lastScaleUpTime=2018-10-26 22:27:42.99390411 +0000 UTC lastScaleDownDeleteTime=2018-10-26 21:38:38.941042585 +0000 UTC lastScaleDownFailTime=2018-10-26 21:38:38.941042981 +0000 UTC schedulablePodsPresent=false isDeleteInProgress=false
I1027 09:28:56.552365       1 static_autoscaler.go:355] Starting scale down
I1027 09:28:56.688899       1 scale_down.go:446] No candidates for scale down``` 
",open,False,2018-10-27 09:41:51,2019-03-22 18:05:58
autoscaler,ringtail,https://github.com/kubernetes/autoscaler/issues/1359,https://api.github.com/repos/kubernetes/autoscaler/issues/1359,Reservation mode in autoscaler.,"I try to design the reservation mode in autoscaler to guarantee the speed of scaling up nodes. I think the key words of reservation is how to describe the workloads in the proper way. Maybe I can add a options like `--workloads-reservation` and pass an array such as `[""4core8mem1gpu*10"" ,""1core2mem0gpu""].  And we will try to simulate the pod to cluster in autoscaler. If the pod is unscheduled then we will follow the scale up logic below.

What's your opinion about this?",closed,False,2018-10-29 04:03:48,2018-10-31 02:15:11
autoscaler,keithlayne,https://github.com/kubernetes/autoscaler/pull/1360,https://api.github.com/repos/kubernetes/autoscaler/issues/1360,Fix broken link to VPA Admission Webhook readme,,closed,True,2018-10-29 16:43:44,2018-10-29 17:14:50
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1361,https://api.github.com/repos/kubernetes/autoscaler/issues/1361,Pass on-event oomInfo without creating a new goroutine.,Can be seen as a continuation of #1335 - now using buffered channel in OnEvent too.,closed,True,2018-10-30 10:44:33,2018-10-30 10:58:02
autoscaler,ringtail,https://github.com/kubernetes/autoscaler/pull/1362,https://api.github.com/repos/kubernetes/autoscaler/issues/1362,add alibaba cloud doc link,add alibaba cloud provider doc link in readme.,closed,True,2018-10-30 12:25:32,2018-10-30 15:08:50
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1363,https://api.github.com/repos/kubernetes/autoscaler/issues/1363,Use real-usage sample to estimate memory usage after OOM,"Use real-usage sample (exclude previous OOMs) to estimate memory usage after OOM.

This should fix the problem reported by @davidquarles under #1182 - but *not* the initial issue reported by @bskiba there.

Note for a reviewer: I've unexported memoryPeak & oomPeak to decrease chance of invalid usage.",closed,True,2018-10-30 13:59:30,2018-10-30 15:23:15
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1364,https://api.github.com/repos/kubernetes/autoscaler/issues/1364,Protect against negative totalWeight values,,closed,True,2018-10-30 13:59:35,2018-10-30 14:33:48
autoscaler,multi-io,https://github.com/kubernetes/autoscaler/pull/1365,https://api.github.com/repos/kubernetes/autoscaler/issues/1365,hack/verify-all.sh: missing negation,A missing negation made that scripts output less stuff if we pass -v (i.e. verbose).,closed,True,2018-10-30 17:33:13,2018-10-31 12:03:05
autoscaler,okgolove,https://github.com/kubernetes/autoscaler/issues/1366,https://api.github.com/repos/kubernetes/autoscaler/issues/1366,EKS IP-addresses limits,"Hello. EKS uses AWS CNI to assign private IP to every pod https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI
So, if you haven't free IPs your pod won't be schedulded.

Can somehow autoscaler implements autoscaling based on IP limits?",closed,False,2018-10-31 11:59:43,2019-03-26 12:36:48
autoscaler,ringtail,https://github.com/kubernetes/autoscaler/issues/1367,https://api.github.com/repos/kubernetes/autoscaler/issues/1367,GPU node should only calculate gpu utilization,"GPU instance is quite different from other instance. It's much more expansive and not sensitive to cpu or memory. If this instance has GPU,Should we have different utilization formula for it. 

```
I1031 20:38:32.303631   31733 scale_down.go:407] Node cn-shenzhen.i-wz9ex1vq9wpu4sxvyy8o - utilization 0.600000
I1031 20:38:32.303670   31733 scale_down.go:411] Node cn-shenzhen.i-wz9ex1vq9wpu4sxvyy8o is not suitable for removal - utilization too big (0.600000)
I1031 20:38:32.303694   31733 scale_down.go:407] Node cn-shenzhen.i-wz91n8ywxxvcmqa2jxxh - utilization 0.600000
I1031 20:38:32.303711   31733 scale_down.go:411] Node cn-shenzhen.i-wz91n8ywxxvcmqa2jxxh is not suitable for removal - utilization too big (0.600000)
```
Here is a 2C8G1GPU instance and  nvidia-device-plugin will request at least 1 core so this instance can't scale down forever even no one use this GPU.

",open,False,2018-10-31 12:46:51,2019-03-21 15:40:33
autoscaler,anshrma,https://github.com/kubernetes/autoscaler/issues/1368,https://api.github.com/repos/kubernetes/autoscaler/issues/1368,Question : Configuration for Autoscaling in two Availability zones - AWS and EKS,"Here is my scenario (on AWS and on EKS)

Have two ASGs, one creates nodes in us-east-1a and another in us-east-1b.
Here is how i install the cluster autoscaler

helm install --set autoDiscovery.clusterName=${CLUSTER_NAME} --set awsRegion=us-east-1  --set sslCertPath=/etc/kubernetes/pki/ca.crt --set rbac.create=true --set extraArgs.balance-similar-node-groups=false  --set cloud-provider=aws --name cluster-autoscaler stable/cluster-autoscaler --namespace kube-system

I have the following tags on both of the ASGs

k8s.io/cluster-autoscaler/enabled = kubernetes.io/cluster/${CLUSTER_NAME} 

Goal : Pods have two replicas and with necessary affinity in place, i am able to indeed spread the two replicas across the two ASGs (and hence two AZs).

Scenario :  When there is a scale up event due to addition of more application, only one node is added in either of the ASGs and now the two replicas of the application go on the same newly added node.

Question : Is there any configuration i can change, which ensures that nodes are added to both the ASGs instead of only one ???",open,False,2018-11-01 00:15:16,2019-04-02 18:18:51
autoscaler,hadr10,https://github.com/kubernetes/autoscaler/issues/1369,https://api.github.com/repos/kubernetes/autoscaler/issues/1369,Autoscaling doesn't work with new versions Cluster Autoscaler 1.3.3 and 1.3.4,"Hello,

azure aks doesn't work with new cluster autoscaler 1.3.3 and 1.3.4. With 1.3.1 it works good. https://github.com/kubernetes/autoscaler/releases
Error from autoscaler:
Warning FailedToScaleUpGroup 2m cluster-autoscaler Scale-up failed for group agentpool: containerservice.Managed
ClustersClient#CreateOrUpdate: Invalid input: autorest/validation: validation failed: parameter=parameters.ManagedCluste
rProperties.AadProfile.ServerAppSecret constraint=Null value=(*string)(nil) details: value can not be null; required parameter

Thank you,
Honza",closed,False,2018-11-01 07:44:32,2018-11-20 06:33:54
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1370,https://api.github.com/repos/kubernetes/autoscaler/issues/1370,"Changing permissions on ""update-gofmt"" script","Changed permissions and added script run instruction in ""verify-gofmt.sh""",closed,True,2018-11-02 05:48:21,2018-11-16 06:54:12
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/issues/1371,https://api.github.com/repos/kubernetes/autoscaler/issues/1371,Switch to github.com/pkg/errors,"Currently Autoscaler is using fmt.Errorf both for wrapping errors and for creating new errors, but there is a general consensus on moving toward github.com/pkg/errors.

Ref:
https://github.com/kubernetes/kubeadm/issues/1183#issue-372862070
https://github.com/kubernetes/kubernetes/pull/70271#issue-225967737

If Owner/Approver agrees  to this I would like to work on it.",closed,False,2018-11-02 06:23:47,2018-11-12 04:37:08
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1372,https://api.github.com/repos/kubernetes/autoscaler/issues/1372,Add e2e tests for updater with different controllers.,,closed,True,2018-11-02 13:28:20,2018-11-05 13:57:14
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1373,https://api.github.com/repos/kubernetes/autoscaler/issues/1373,Vertical Pod Autoscaler version 0.3.0,,closed,True,2018-11-02 14:37:13,2018-11-02 15:13:08
autoscaler,mooncak,https://github.com/kubernetes/autoscaler/pull/1374,https://api.github.com/repos/kubernetes/autoscaler/issues/1374,Fix typos: alredy -> already,"Signed-off-by: mooncake <xcoder@tenxcloud.com>

Fix typos: alredy -> already",closed,True,2018-11-04 13:15:29,2018-11-04 15:57:36
autoscaler,multi-io,https://github.com/kubernetes/autoscaler/pull/1375,https://api.github.com/repos/kubernetes/autoscaler/issues/1375,hack/verify-all.sh -v output corrected,"https://github.com/kubernetes/autoscaler/pull/1365 fixed a missing negation in `hack/verify-all.sh` so we now get proper headers when `-v` is passed, but it forgot to also change whether or not the verifier script output was suppressed or not depending on `-v` -- that setting was correct before https://github.com/kubernetes/autoscaler/pull/1365 and is now wrong (so the PR was sort of a disimprovement). This PR fixes that.

Sorry for the confusion.",closed,True,2018-11-04 16:09:30,2018-11-06 10:37:21
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1376,https://api.github.com/repos/kubernetes/autoscaler/issues/1376,Build VPA releases in docker,@MaciekPytel ,closed,True,2018-11-05 15:56:18,2018-11-07 16:03:10
autoscaler,rajatjindal,https://github.com/kubernetes/autoscaler/issues/1377,https://api.github.com/repos/kubernetes/autoscaler/issues/1377,question: is it possible to run cluster-autoscaler in dry-run mode,"Hi, Its more of a question if we can run cluster-autoscaler in dry-run mode. We want to evaluate when autoscaler would have triggered the scale-down/up actions without actually taking the action (to avoid disruption during the evaluation)

Thanks
Rajat Jindal",closed,False,2018-11-05 17:50:19,2019-04-04 20:14:56
autoscaler,johanneswuerbach,https://github.com/kubernetes/autoscaler/pull/1378,https://api.github.com/repos/kubernetes/autoscaler/issues/1378,AWS: Improved balancing,"Various ClusterAutoscaler components like the BalancingProcessor act based on the current `TargetSize` of a `NodeGroup`. https://github.com/kubernetes/autoscaler/blob/f341d8a68a6aad996964cda5db67e2f254b71c8d/cluster-autoscaler/processors/nodegroupset/balancing_processor.go#L89

As the current AWS implementation only updates `TargetSize` when refreshing the ASG list every minutes (less when being rate-limited), the cached `TargetSize` might quickly mismatch the actual value.

While each ASG change could trigger a cache refresh, I don't think this is optimal as it introduces again further requests.

Instead I decided to pre-calculate the `TargetSize` change for AWS on AutoScalingGroup changes and Node deletion to make better balancing decisions.

Any wrongly pre-calculated `TargetSize` will be auto-corrected on the next successful ASG list refresh.",closed,True,2018-11-05 23:08:12,2018-11-19 11:01:51
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1379,https://api.github.com/repos/kubernetes/autoscaler/issues/1379,Relax list of OWNERS in hack directory,,closed,True,2018-11-06 10:00:13,2018-11-06 10:12:45
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1380,https://api.github.com/repos/kubernetes/autoscaler/issues/1380,Make Backoff interface,,closed,True,2018-11-06 10:38:38,2018-11-07 13:13:44
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1381,https://api.github.com/repos/kubernetes/autoscaler/issues/1381,Fix formatted log messages,"Go 1.11.1 complains about malformatted log messages:

```
# k8s.io/autoscaler/cluster-autoscaler/cloudprovider/azure
cloudprovider/azure/azure_agent_pool.go:213: Verbose.Infof format %s has arg newDeployment of wrong type k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/Azure/azure-sdk-for-go/services/resources/mgmt/2017-05-10/resources.Deployment
cloudprovider/azure/azure_client.go:122: Verbose.Infof format %q reads arg #2, but call has 1 arg
cloudprovider/azure/azure_client.go:124: Verbose.Infof format %q reads arg #2, but call has 1 arg
cloudprovider/azure/azure_client.go:145: Verbose.Infof format %q has arg vmInstanceIDs of wrong type k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/Azure/azure-sdk-for-go/services/compute/mgmt/2018-04-01/compute.VirtualMachineScaleSetVMInstanceRequiredIDs
cloudprovider/azure/azure_client.go:147: Verbose.Infof format %q has arg vmInstanceIDs of wrong type k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/Azure/azure-sdk-for-go/services/compute/mgmt/2018-04-01/compute.VirtualMachineScaleSetVMInstanceRequiredIDs
cloudprovider/azure/azure_container_service_pool.go:300: Errorf format %d arg agentPool.MaxSize is a func value, not called
# k8s.io/autoscaler/cluster-autoscaler/cloudprovider/alicloud
cloudprovider/alicloud/alicloud_auto_scaling_group.go:183: Errorf format %s has arg template.InstanceType of wrong type *alicloud.instanceType
```",closed,True,2018-11-06 13:53:10,2018-11-06 14:06:32
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1382,https://api.github.com/repos/kubernetes/autoscaler/issues/1382,Update godeps,,closed,True,2018-11-06 14:54:19,2018-11-06 15:51:18
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1383,https://api.github.com/repos/kubernetes/autoscaler/issues/1383,Add test checking Initial and Off are handled correctly,Extract some common functionality,closed,True,2018-11-06 15:59:04,2018-11-08 09:35:19
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1384,https://api.github.com/repos/kubernetes/autoscaler/issues/1384,Cluster Autoscaler 1.12.1,,closed,True,2018-11-06 16:08:54,2018-11-06 17:58:23
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1385,https://api.github.com/repos/kubernetes/autoscaler/issues/1385,Update godeps,,closed,True,2018-11-06 16:29:33,2018-11-19 09:34:15
autoscaler,zegl,https://github.com/kubernetes/autoscaler/pull/1386,https://api.github.com/repos/kubernetes/autoscaler/issues/1386,Update RBAC example to include replicasets in the apps apigroup,This is needed as of Kubernetes 1.12,closed,True,2018-11-06 17:50:03,2018-11-08 10:03:22
autoscaler,gwohletz,https://github.com/kubernetes/autoscaler/issues/1387,https://api.github.com/repos/kubernetes/autoscaler/issues/1387,Please release 1.2.4 with r5 support,"Recently 1.3.4 was released and includes support for a couple new aws instance types (r5, z1), it would be greatly appreciated if a similar release 1.2.4 could be made for those of us that will be still using kubernetes 1.10 for at least another couple months.",closed,False,2018-11-06 18:14:08,2019-01-07 18:46:39
autoscaler,lachlancooper,https://github.com/kubernetes/autoscaler/pull/1388,https://api.github.com/repos/kubernetes/autoscaler/issues/1388,Correct flag for node group auto-discovery,This fixes the `--node-group-auto-discovery` flag in the aws README.,closed,True,2018-11-07 01:15:21,2018-11-07 12:05:24
autoscaler,zegl,https://github.com/kubernetes/autoscaler/issues/1389,https://api.github.com/repos/kubernetes/autoscaler/issues/1389,Node is not deleted after scaledown on 1.12,"After upgrading from Kubernetes v1.11 / cluster-autoscaler v1.3 to 1.12, nodes are not correctly deleted from the Kubernetes API when scaling down.

I've extracted the relevant log lines from cluster-autoscaler.

```
I1107 08:45:56.702249       1 scale_down.go:582] ip-10-21-54-103.eu-west-1.compute.internal was unneeded for 7m32.152950271s
I1107 08:45:56.702027       1 cluster.go:112] Fast evaluation: node ip-10-21-54-103.eu-west-1.compute.internal may be removed
I1107 08:48:46.305050       1 scale_down.go:977] All pods removed from ip-10-21-54-103.eu-west-1.compute.internal
I1107 08:48:46.391409       1 aws_manager.go:201] Terminating EC2 instance: i-06b751146b9b80e17
I1107 08:48:46.391726       1 factory.go:33] Event(v1.ObjectReference{Kind:""Node"", Namespace:"""", Name:""ip-10-21-54-103.eu-west-1.compute.internal"", UID:""a8e01d4b-e1d2-11e8-9283-022197af68f0"", APIVersion:""v1"", ResourceVersion:""68195502"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' node removed by cluster autoscaler
I1107 08:48:51.849215       1 scale_down.go:382] Skipping ip-10-21-54-103.eu-west-1.compute.internal from delete considerations - the node is currently being deleted
I1107 08:49:43.140636       1 utils.go:462] Skipping ip-10-21-54-103.eu-west-1.compute.internal - no node group config
I1107 08:49:53.549440       1 utils.go:462] Skipping ip-10-21-54-103.eu-west-1.compute.internal - no node group config
```

The EC2 instance is successfully terminated, but the node object is never deleted from the cluster.

Current details from the node, a few hours after the deletion:

```
CreationTimestamp:  Tue, 06 Nov 2018 15:45:52 +0100
Taints:             ToBeDeletedByClusterAutoscaler=1541580510:NoSchedule
                    node.cloudprovider.kubernetes.io/shutdown:NoSchedule
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      false
Conditions:
  Type             Status    LastHeartbeatTime                 LastTransitionTime                Reason                    Message
  ----             ------    -----------------                 ------------------                ------                    -------
  OutOfDisk        Unknown   Wed, 07 Nov 2018 09:48:37 +0100   Wed, 07 Nov 2018 09:49:18 +0100   NodeStatusUnknown         Kubelet stopped posting node status.
  MemoryPressure   Unknown   Wed, 07 Nov 2018 09:48:37 +0100   Wed, 07 Nov 2018 09:49:18 +0100   NodeStatusUnknown         Kubelet stopped posting node status.
  DiskPressure     Unknown   Wed, 07 Nov 2018 09:48:37 +0100   Wed, 07 Nov 2018 09:49:18 +0100   NodeStatusUnknown         Kubelet stopped posting node status.
  PIDPressure      False     Wed, 07 Nov 2018 09:48:37 +0100   Tue, 06 Nov 2018 15:45:52 +0100   KubeletHasSufficientPID   kubelet has sufficient PID available
  Ready            Unknown   Wed, 07 Nov 2018 09:48:37 +0100   Wed, 07 Nov 2018 09:49:18 +0100   NodeStatusUnknown         Kubelet stopped posting node status.
```

Is this a bug in cluster-autoscaler, or should this be reported to another Kubernetes project?


",closed,False,2018-11-07 09:18:01,2018-11-07 09:32:03
autoscaler,omerlh,https://github.com/kubernetes/autoscaler/issues/1390,https://api.github.com/repos/kubernetes/autoscaler/issues/1390,Missing metrics - errors_total,"I installed cluster autoscaler on a cluster running on Azure. I'm now adding some metrics to monitor it, and I noticed `errors_total`. This looks like an important metrics to monitor, but it's not returning from the autoscaler. I looked on the code, and it seems this metrics exist - so maybe I missing something.
Autoscaler version: `k8s.gcr.io/cluster-autoscaler:v1.3.0`.

Metrics returned:

```
# HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.
# TYPE apiserver_audit_event_total counter
apiserver_audit_event_total 0
# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le=""0""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""21600""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""43200""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""86400""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""172800""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""345600""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""604800""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""2.592e+06""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""7.776e+06""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""1.5552e+07""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""3.1104e+07""} 0
apiserver_client_certificate_expiration_seconds_bucket{le=""+Inf""} 0
apiserver_client_certificate_expiration_seconds_sum 0
apiserver_client_certificate_expiration_seconds_count 0
# HELP cluster_autoscaler_cluster_safe_to_autoscale Whether or not cluster is healthy enough for autoscaling. 1 if it is, 0 otherwise.
# TYPE cluster_autoscaler_cluster_safe_to_autoscale gauge
cluster_autoscaler_cluster_safe_to_autoscale 1
# HELP cluster_autoscaler_created_node_groups_total Number of node groups created by Node Autoprovisioning.
# TYPE cluster_autoscaler_created_node_groups_total counter
cluster_autoscaler_created_node_groups_total 0
# HELP cluster_autoscaler_deleted_node_groups_total Number of node groups deleted by Node Autoprovisioning.
# TYPE cluster_autoscaler_deleted_node_groups_total counter
cluster_autoscaler_deleted_node_groups_total 0
# HELP cluster_autoscaler_evicted_pods_total Number of pods evicted by CA
# TYPE cluster_autoscaler_evicted_pods_total counter
cluster_autoscaler_evicted_pods_total 0
# HELP cluster_autoscaler_function_duration_seconds Time taken by various parts of CA main loop.
# TYPE cluster_autoscaler_function_duration_seconds histogram
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""0.01""} 6279
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""0.05""} 6331
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""0.1""} 6347
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""0.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""1""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""2.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""7.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""10""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""12.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""15""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""17.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""20""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""22.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""25""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""27.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""30""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""50""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""75""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""100""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""1000""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""filterOutSchedulable"",le=""+Inf""} 6349
cluster_autoscaler_function_duration_seconds_sum{function=""filterOutSchedulable""} 4.676553542999989
cluster_autoscaler_function_duration_seconds_count{function=""filterOutSchedulable""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""0.01""} 6320
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""0.05""} 6337
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""0.1""} 6348
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""0.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""1""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""2.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""7.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""10""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""12.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""15""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""17.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""20""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""22.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""25""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""27.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""30""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""50""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""75""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""100""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""1000""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""findUnneeded"",le=""+Inf""} 6349
cluster_autoscaler_function_duration_seconds_sum{function=""findUnneeded""} 4.3102938329999825
cluster_autoscaler_function_duration_seconds_count{function=""findUnneeded""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""0.01""} 0
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""0.05""} 0
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""0.1""} 65
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""0.5""} 6327
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""1""} 6341
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""2.5""} 6344
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""5""} 6346
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""7.5""} 6346
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""10""} 6346
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""12.5""} 6348
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""15""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""17.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""20""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""22.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""25""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""27.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""30""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""50""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""75""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""100""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""1000""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""main"",le=""+Inf""} 6349
cluster_autoscaler_function_duration_seconds_sum{function=""main""} 1131.597676045
cluster_autoscaler_function_duration_seconds_count{function=""main""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""0.01""} 6281
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""0.05""} 6286
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""0.1""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""0.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""1""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""2.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""7.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""10""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""12.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""15""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""17.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""20""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""22.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""25""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""27.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""30""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""50""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""75""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""100""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""1000""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown"",le=""+Inf""} 6291
cluster_autoscaler_function_duration_seconds_sum{function=""scaleDown""} 0.8830251510000011
cluster_autoscaler_function_duration_seconds_count{function=""scaleDown""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""0.01""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""0.05""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""0.1""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""0.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""1""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""2.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""7.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""10""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""12.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""15""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""17.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""20""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""22.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""25""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""27.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""30""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""50""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""75""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""100""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""1000""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:findNodesToRemove"",le=""+Inf""} 6291
cluster_autoscaler_function_duration_seconds_sum{function=""scaleDown:findNodesToRemove""} 0
cluster_autoscaler_function_duration_seconds_count{function=""scaleDown:findNodesToRemove""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""0.01""} 6282
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""0.05""} 6286
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""0.1""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""0.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""1""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""2.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""7.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""10""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""12.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""15""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""17.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""20""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""22.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""25""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""27.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""30""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""50""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""75""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""100""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""1000""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:miscOperations"",le=""+Inf""} 6291
cluster_autoscaler_function_duration_seconds_sum{function=""scaleDown:miscOperations""} 0.8420665120000006
cluster_autoscaler_function_duration_seconds_count{function=""scaleDown:miscOperations""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""0.01""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""0.05""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""0.1""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""0.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""1""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""2.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""7.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""10""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""12.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""15""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""17.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""20""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""22.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""25""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""27.5""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""30""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""50""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""75""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""100""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""1000""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""scaleDown:nodeDeletion"",le=""+Inf""} 6291
cluster_autoscaler_function_duration_seconds_sum{function=""scaleDown:nodeDeletion""} 0
cluster_autoscaler_function_duration_seconds_count{function=""scaleDown:nodeDeletion""} 6291
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""0.01""} 0
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""0.05""} 0
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""0.1""} 1948
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""0.5""} 6334
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""1""} 6343
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""2.5""} 6344
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""5""} 6346
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""7.5""} 6346
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""10""} 6346
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""12.5""} 6348
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""15""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""17.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""20""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""22.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""25""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""27.5""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""30""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""50""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""75""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""100""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""1000""} 6349
cluster_autoscaler_function_duration_seconds_bucket{function=""updateClusterState"",le=""+Inf""} 6349
cluster_autoscaler_function_duration_seconds_sum{function=""updateClusterState""} 842.1254474549997
cluster_autoscaler_function_duration_seconds_count{function=""updateClusterState""} 6349
# HELP cluster_autoscaler_last_activity Last time certain part of CA logic executed.
# TYPE cluster_autoscaler_last_activity gauge
cluster_autoscaler_last_activity{activity=""autoscaling""} 1.541661162e+09
cluster_autoscaler_last_activity{activity=""main""} 1.541661162e+09
cluster_autoscaler_last_activity{activity=""scaleDown""} 1.541661162e+09
# HELP cluster_autoscaler_nap_enabled Whether or not Node Autoprovisioning is enabled. 1 if it is, 0 otherwise.
# TYPE cluster_autoscaler_nap_enabled gauge
cluster_autoscaler_nap_enabled 0
# HELP cluster_autoscaler_node_groups_count Number of node groups managed by CA.
# TYPE cluster_autoscaler_node_groups_count gauge
cluster_autoscaler_node_groups_count{node_group_type=""autoprovisioned""} 0
cluster_autoscaler_node_groups_count{node_group_type=""autoscaled""} 1
# HELP cluster_autoscaler_nodes_count Number of nodes in cluster.
# TYPE cluster_autoscaler_nodes_count gauge
cluster_autoscaler_nodes_count{state=""longUnregistered""} 0
cluster_autoscaler_nodes_count{state=""notStarted""} 0
cluster_autoscaler_nodes_count{state=""ready""} 7
cluster_autoscaler_nodes_count{state=""unready""} 0
cluster_autoscaler_nodes_count{state=""unregistered""} 0
# HELP cluster_autoscaler_scaled_up_nodes_total Number of nodes added by CA.
# TYPE cluster_autoscaler_scaled_up_nodes_total counter
cluster_autoscaler_scaled_up_nodes_total 0
# HELP cluster_autoscaler_unneeded_nodes_count Number of nodes currently considered unneeded by CA.
# TYPE cluster_autoscaler_unneeded_nodes_count gauge
cluster_autoscaler_unneeded_nodes_count 0
# HELP cluster_autoscaler_unschedulable_pods_count Number of unschedulable pods in the cluster.
# TYPE cluster_autoscaler_unschedulable_pods_count gauge
cluster_autoscaler_unschedulable_pods_count 0
# HELP get_token_count Counter of total Token() requests to the alternate token source
# TYPE get_token_count counter
get_token_count 0
# HELP get_token_fail_count Counter of failed Token() requests to the alternate token source
# TYPE get_token_fail_count counter
get_token_fail_count 0
# HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile=""0""} 3.41e-05
go_gc_duration_seconds{quantile=""0.25""} 8.2902e-05
go_gc_duration_seconds{quantile=""0.5""} 0.000122703
go_gc_duration_seconds{quantile=""0.75""} 0.000389809
go_gc_duration_seconds{quantile=""1""} 0.10032829
go_gc_duration_seconds_sum 9.918545783999999
go_gc_duration_seconds_count 770
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 157
# HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.
# TYPE go_memstats_alloc_bytes gauge
go_memstats_alloc_bytes 1.7837136e+07
# HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed.
# TYPE go_memstats_alloc_bytes_total counter
go_memstats_alloc_bytes_total 1.1434824528e+10
# HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table.
# TYPE go_memstats_buck_hash_sys_bytes gauge
go_memstats_buck_hash_sys_bytes 2.025664e+06
# HELP go_memstats_frees_total Total number of frees.
# TYPE go_memstats_frees_total counter
go_memstats_frees_total 1.34192099e+08
# HELP go_memstats_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started.
# TYPE go_memstats_gc_cpu_fraction gauge
go_memstats_gc_cpu_fraction 0.00048300586568055177
# HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata.
# TYPE go_memstats_gc_sys_bytes gauge
go_memstats_gc_sys_bytes 1.671168e+06
# HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use.
# TYPE go_memstats_heap_alloc_bytes gauge
go_memstats_heap_alloc_bytes 1.7837136e+07
# HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used.
# TYPE go_memstats_heap_idle_bytes gauge
go_memstats_heap_idle_bytes 1.2288e+07
# HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use.
# TYPE go_memstats_heap_inuse_bytes gauge
go_memstats_heap_inuse_bytes 2.752512e+07
# HELP go_memstats_heap_objects Number of allocated objects.
# TYPE go_memstats_heap_objects gauge
go_memstats_heap_objects 104157
# HELP go_memstats_heap_released_bytes Number of heap bytes released to OS.
# TYPE go_memstats_heap_released_bytes gauge
go_memstats_heap_released_bytes 925696
# HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system.
# TYPE go_memstats_heap_sys_bytes gauge
go_memstats_heap_sys_bytes 3.981312e+07
# HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection.
# TYPE go_memstats_last_gc_time_seconds gauge
go_memstats_last_gc_time_seconds 1.5416611629152594e+09
# HELP go_memstats_lookups_total Total number of pointer lookups.
# TYPE go_memstats_lookups_total counter
go_memstats_lookups_total 2064
# HELP go_memstats_mallocs_total Total number of mallocs.
# TYPE go_memstats_mallocs_total counter
go_memstats_mallocs_total 1.34296256e+08
# HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures.
# TYPE go_memstats_mcache_inuse_bytes gauge
go_memstats_mcache_inuse_bytes 3472
# HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system.
# TYPE go_memstats_mcache_sys_bytes gauge
go_memstats_mcache_sys_bytes 16384
# HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures.
# TYPE go_memstats_mspan_inuse_bytes gauge
go_memstats_mspan_inuse_bytes 397176
# HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system.
# TYPE go_memstats_mspan_sys_bytes gauge
go_memstats_mspan_sys_bytes 491520
# HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place.
# TYPE go_memstats_next_gc_bytes gauge
go_memstats_next_gc_bytes 3.3430912e+07
# HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations.
# TYPE go_memstats_other_sys_bytes gauge
go_memstats_other_sys_bytes 790584
# HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator.
# TYPE go_memstats_stack_inuse_bytes gauge
go_memstats_stack_inuse_bytes 1.277952e+06
# HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator.
# TYPE go_memstats_stack_sys_bytes gauge
go_memstats_stack_sys_bytes 1.277952e+06
# HELP go_memstats_sys_bytes Number of bytes obtained from system.
# TYPE go_memstats_sys_bytes gauge
go_memstats_sys_bytes 4.6086392e+07
# HELP go_threads Number of OS threads created
# TYPE go_threads gauge
go_threads 13
# HELP http_request_duration_microseconds The HTTP request latencies in microseconds.
# TYPE http_request_duration_microseconds summary
http_request_duration_microseconds{handler=""prometheus"",quantile=""0.5""} 2054.559
http_request_duration_microseconds{handler=""prometheus"",quantile=""0.9""} 6993.911
http_request_duration_microseconds{handler=""prometheus"",quantile=""0.99""} 6993.911
http_request_duration_microseconds_sum{handler=""prometheus""} 215700.27600000004
http_request_duration_microseconds_count{handler=""prometheus""} 19
# HELP http_request_size_bytes The HTTP request sizes in bytes.
# TYPE http_request_size_bytes summary
http_request_size_bytes{handler=""prometheus"",quantile=""0.5""} 167
http_request_size_bytes{handler=""prometheus"",quantile=""0.9""} 167
http_request_size_bytes{handler=""prometheus"",quantile=""0.99""} 167
http_request_size_bytes_sum{handler=""prometheus""} 3508
http_request_size_bytes_count{handler=""prometheus""} 19
# HELP http_requests_total Total number of HTTP requests made.
# TYPE http_requests_total counter
http_requests_total{code=""200"",handler=""prometheus"",method=""get""} 19
# HELP http_response_size_bytes The HTTP response sizes in bytes.
# TYPE http_response_size_bytes summary
http_response_size_bytes{handler=""prometheus"",quantile=""0.5""} 3355
http_response_size_bytes{handler=""prometheus"",quantile=""0.9""} 3371
http_response_size_bytes{handler=""prometheus"",quantile=""0.99""} 3371
http_response_size_bytes_sum{handler=""prometheus""} 63263
http_response_size_bytes_count{handler=""prometheus""} 19
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 210.06
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 1.048576e+06
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 11
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 9.545728e+07
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.54159649472e+09
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 5.14985984e+08
```",closed,False,2018-11-08 07:32:44,2018-11-11 05:45:17
autoscaler,mborsz,https://github.com/kubernetes/autoscaler/pull/1391,https://api.github.com/repos/kubernetes/autoscaler/issues/1391,Migrate pod_nanny to fetch only nodes metadata.,"This pod requires only number of nodes so we can fetch only nodes
metadata to reduce resource consumption and latency.
Tests on 5000-node cluster show that this change reduces 'GET /api/v1/nodes' latency from ~5s to ~100ms.
",closed,True,2018-11-08 10:16:37,2018-11-08 13:08:50
autoscaler,mborsz,https://github.com/kubernetes/autoscaler/pull/1392,https://api.github.com/repos/kubernetes/autoscaler/issues/1392,Migrate to go1.10,Go 1.8 is no longer supported and travis is not able to run presubmit with it.,closed,True,2018-11-08 10:44:01,2018-11-08 12:19:16
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1393,https://api.github.com/repos/kubernetes/autoscaler/issues/1393,Distinguish between eviction and successful restart in e2e,,closed,True,2018-11-08 16:10:06,2018-11-09 10:18:36
autoscaler,mooncak,https://github.com/kubernetes/autoscaler/pull/1394,https://api.github.com/repos/kubernetes/autoscaler/issues/1394,Fix typos: checkponits -> checkpoints,Fix typos: checkponits -> checkpoints,closed,True,2018-11-08 16:40:47,2018-11-09 08:08:52
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1395,https://api.github.com/repos/kubernetes/autoscaler/issues/1395,Build VPA releases in docker,,closed,True,2018-11-09 08:01:45,2018-11-09 09:48:32
autoscaler,mooncak,https://github.com/kubernetes/autoscaler/pull/1396,https://api.github.com/repos/kubernetes/autoscaler/issues/1396,"Fix typos: reqest->request, approporiate->appropriate","Fix typos: reqest->request, approporiate->appropriate",closed,True,2018-11-10 12:32:02,2018-11-13 08:38:13
autoscaler,OndrejKu,https://github.com/kubernetes/autoscaler/issues/1397,https://api.github.com/repos/kubernetes/autoscaler/issues/1397,"""autoscaler-aws-cluster-autoscaler"" is forbidden: attempt to grant extra privileges","I'm having troubles with installing autocluster using helm. This is the command I'm using:
```
$ helm install --name autoscaler \                                                                                                        
    --namespace kube-system \
    --set image.tag=v1.1.10 \
    --set autoDiscovery.clusterName=myClusterName \
    --set extraArgs.balance-similar-node-groups=false \
    --set extraArgs.expander=random \
    --set rbac.create=true \
    --set rbac.pspEnabled=true \
    --set awsRegion=us-east-1 \
    --set nodeSelector.""node-role\.kubernetes\.io/master""="""" \
    --set tolerations[0].effect=NoSchedule \
    --set tolerations[0].key=node-role.kubernetes.io/master \
    --set cloudProvider=aws \
    stable/cluster-autoscaler

```
And this was produced: 
```
Error: release autoscaler failed: clusterroles.rbac.authorization.k8s.io ""autoscaler-aws-cluster-autoscaler"" is forbidden: attempt to grant extra privileges: 
[PolicyRule{Resources:[""events""], APIGroups:[""""], Verbs:[""create""]} 
PolicyRule{Resources:[""events""], APIGroups:[""""], Verbs:[""patch""]}
PolicyRule{Resources:[""endpoints""], APIGroups:[""""], Verbs:[""create""]} 
PolicyRule{Resources:[""endpoints""], APIGroups:[""""], Verbs:[""patch""]} 
PolicyRule{Resources:[""pods/eviction""], APIGroups:[""""], Verbs:[""create""]} 
PolicyRule{Resources:[""pods/status""], APIGroups:[""""], Verbs:[""update""]} 
PolicyRule{Resources:[""endpoints""], ResourceNames:[""cluster-autoscaler""], APIGroups:[""""], Verbs:[""get""]} 
PolicyRule{Resources:[""endpoints""], ResourceNames:[""cluster-autoscaler""], APIGroups:[""""], Verbs:[""update""]} 
PolicyRule{Resources:[""nodes""], APIGroups:[""""], Verbs:[""watch""]} 
PolicyRule{Resources:[""nodes""], APIGroups:[""""], Verbs:[""list""]} 
PolicyRule{Resources:[""nodes""], APIGroups:[""""], Verbs:[""get""]} 
PolicyRule{Resources:[""nodes""], APIGroups:[""""], Verbs:[""update""]} 
PolicyRule{Resources:[""pods""], APIGroups:[""""], Verbs:[""watch""]} 
PolicyRule{Resources:[""pods""], APIGroups:[""""], Verbs:[""list""]} 
PolicyRule{Resources:[""pods""], APIGroups:[""""], Verbs:[""get""]} 
PolicyRule{Resources:[""services""], APIGroups:[""""], Verbs:[""watch""]} 
PolicyRule{Resources:[""services""], APIGroups:[""""], Verbs:[""list""]} 
PolicyRule{Resources:[""services""], APIGroups:[""""], Verbs:[""get""]} 
PolicyRule{Resources:[""replicationcontrollers""], APIGroups:[""""], Verbs:[""watch""]} 
PolicyRule{Resources:[""replicationcontrollers""], APIGroups:[""""], Verbs:[""list""]} 
PolicyRule{Resources:[""replicationcontrollers""], APIGroups:[""""], Verbs:[""get""]} 
PolicyRule{Resources:[""persistentvolumeclaims""], APIGroups:[""""], Verbs:[""watch""]} 
PolicyRule{Resources:[""persistentvolumeclaims""], APIGroups:[""""], Verbs:[""list""]} 
PolicyRule{Resources:[""persistentvolumeclaims""], APIGroups:[""""], Verbs:[""get""]} 
PolicyRule{Resources:[""persistentvolumes""], APIGroups:[""""], Verbs:[""watch""]} 
PolicyRule{Resources:[""persistentvolumes""], APIGroups:[""""], Verbs:[""list""]} 
PolicyRule{Resources:[""persistentvolumes""], APIGroups:[""""], Verbs:[""get""]} 
PolicyRule{Resources:[""replicasets""], APIGroups:[""extensions""], Verbs:[""watch""]} 
PolicyRule{Resources:[""replicasets""], APIGroups:[""extensions""], Verbs:[""list""]} 
PolicyRule{Resources:[""replicasets""], APIGroups:[""extensions""], Verbs:[""get""]} 
PolicyRule{Resources:[""daemonsets""], APIGroups:[""extensions""], Verbs:[""watch""]} 
PolicyRule{Resources:[""daemonsets""], APIGroups:[""extensions""], Verbs:[""list""]} 
PolicyRule{Resources:[""daemonsets""], APIGroups:[""extensions""], Verbs:[""get""]} 
PolicyRule{Resources:[""poddisruptionbudgets""], APIGroups:[""policy""], Verbs:[""watch""]} 
PolicyRule{Resources:[""poddisruptionbudgets""], APIGroups:[""policy""], Verbs:[""list""]} 
PolicyRule{Resources:[""replicasets""], APIGroups:[""apps""], Verbs:[""watch""]} 
PolicyRule{Resources:[""replicasets""], APIGroups:[""apps""], Verbs:[""list""]} 
PolicyRule{Resources:[""replicasets""], APIGroups:[""apps""], Verbs:[""get""]} 
PolicyRule{Resources:[""statefulsets""], APIGroups:[""apps""], Verbs:[""watch""]} 
PolicyRule{Resources:[""statefulsets""], APIGroups:[""apps""], Verbs:[""list""]} 
PolicyRule{Resources:[""statefulsets""], APIGroups:[""apps""], Verbs:[""get""]} 
PolicyRule{Resources:[""storageclasses""], APIGroups:[""storage.k8s.io""], Verbs:[""watch""]} 
PolicyRule{Resources:[""storageclasses""], APIGroups:[""storage.k8s.io""], Verbs:[""list""]} 
PolicyRule{Resources:[""storageclasses""], APIGroups:[""storage.k8s.io""], Verbs:[""get""]} 
PolicyRule{Resources:[""podsecuritypolicies""], 
ResourceNames:[""privileged-autoscaler-aws-cluster-autoscaler""], APIGroups:[""extensions""], Verbs:[""use""]}] 
user=&{system:serviceaccount:kube-system:default a46837a7-6659-11e7-8aee-0a5e020b5188 [system:serviceaccounts system:serviceaccounts:kube-system system:authenticated] map[]} ownerrules=[] ruleResolutionErrors=[]
````
```
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.0"", GitCommit:""0ed33881dc4355495f623c6f22e7dd0b7632b7c0"", GitTreeState:""clean"", BuildDate:""2018-09-27T17:05:32Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.10"", GitCommit:""098570796b32895c38a9a1c9286425fb1ececa18"", GitTreeState:""clean"", BuildDate:""2018-08-02T17:11:51Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Client: &version.Version{SemVer:""v2.11.0"", GitCommit:""2e55dbe1fdb5fdb96b75ff144a339489417b146b"", GitTreeState:""clean""}
Server: &version.Version{SemVer:""v2.11.0"", GitCommit:""2e55dbe1fdb5fdb96b75ff144a339489417b146b"", GitTreeState:""clean""}
```
Any idea how kube-system service account can request all these policies?",closed,False,2018-11-12 18:38:40,2018-11-12 21:18:38
autoscaler,arjanschaaf,https://github.com/kubernetes/autoscaler/issues/1398,https://api.github.com/repos/kubernetes/autoscaler/issues/1398,Cluster Autoscaler 1.12.0 doesn't respect explicit minSize for nodegroup as Cluster Autoscaler 1.2.2 did.,"I'm using the helm chart to deploy the cluster autoscaler into AWS based (EKS) kubernetes clusters: https://github.com/helm/charts/tree/master/stable/cluster-autoscaler

The previous release of the Helm chart (0.7.0) used this cluster-autoscaler image version: **1.2.2**
The current release of the Helm chart (0.8.0) uses a new version of the cluster-autoscaler image: **1.12.0**

My setup is as follows: I created an AWS auto scaling group with a minSize of **0** and a maxSize of **20**
My default configuration for the autoscaler is:
```
    Command:
      ./cluster-autoscaler
      --cloud-provider=aws
      --namespace=kube-system
      --nodes=1:20:<AMAZON_AUTOSCALING_GROUP_NAME>
      --logtostderr=true
      --stderrthreshold=info
      --v=4
```

And night I update this configuration through a cronjob so that the configuration for the cluster-autoscaler looks like:
```
      --nodes=0:5:<AMAZON_AUTOSCALING_GROUP_NAME>
```
This autoscaling group is used for a build farm and at night I don't want to pay for idling nodes, while during the day I like to have at least 1 node readily available.

This works as expected with version **1.2.2** but when updating to **1.12.0** the explicit minSize of **1** is not respected and the cluster-autoscaler will always scale back to **0** nodes. It looks to me as if the cluster-autoscaler only looks at the minSize of the underlying AWS autoscaling group. When chancing back the cluster-autoscaler deployment to an image version of **1.2.2** it instantly works as expected: so I don't think it has to do with any other possible changes when switching between the Helm chart version 0.7.0 and 0.8.0.

Looking at the log with cluster-autoscaler version **1.12.0** I see:
```
I1112 19:59:59.715879       1 scale_down.go:402] Node <AWS node DNS name> - utilization 0.027500
```
and after that the cluster-autoscaler issues a scale down command and the node is shutdown

Switching back to version **1.2.2** is see this (in the same exact situation):
```
I1112 21:22:13.289037       1 utils.go:471] Skipping <AWS node DNS name> - node group min size reached
```",open,False,2018-11-13 10:08:15,2019-01-18 16:58:54
autoscaler,nikopen,https://github.com/kubernetes/autoscaler/pull/1399,https://api.github.com/repos/kubernetes/autoscaler/issues/1399,Clarify down-scale capabilities,"Wasn't clear by initially reading the readme.

I'd also like to add specifics on the time intervals (configurable or not? etc), please let me know how that works.",closed,True,2018-11-13 11:10:29,2018-11-13 14:33:41
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1400,https://api.github.com/repos/kubernetes/autoscaler/issues/1400,Inject Backoff instance to ClusterStateRegistry on creation,,closed,True,2018-11-13 13:22:53,2018-11-13 13:55:15
autoscaler,gtie,https://github.com/kubernetes/autoscaler/issues/1401,https://api.github.com/repos/kubernetes/autoscaler/issues/1401,1.12 branch is missing pull/1245,"Looks like 1.12 branch (and 1.12.1 by extension) got cut out *just before* merging https://github.com/kubernetes/autoscaler/pull/1245. 

Since that PR is quite useful (and the corresponding capability is in the current docs!), it would be great if we can have another patch release that includes it.",closed,False,2018-11-13 17:24:07,2019-03-25 11:04:02
autoscaler,ppruthi,https://github.com/kubernetes/autoscaler/issues/1402,https://api.github.com/repos/kubernetes/autoscaler/issues/1402,Cluster Autoscalar does not show ASG Scaling Activity History,"version: 1.2.3

### CloudProvider: AWS

### Description

ASG Scaling Activity History failure messages like this one:
```
Launching a new EC2 instance. Status Reason: We currently do not have sufficient xx.large capacity in the Availability Zone you requested (xx-xxxx-xx). Our system will be working on provisioning additional capacity. You can currently get xx.large capacity by not specifying an Availability Zone in your request or choosing xx-xxxx-xx. Launching EC2 instance failed.
```

are not surfaced on cluster autoscalar logs to expose why the scale up didn't happen.
Instead the cluster autoscalar logs messages like:

```
pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```

which does not completely reflect whats wrong with the cloud provider.",open,False,2018-11-13 18:06:57,2019-03-22 16:04:12
autoscaler,hercynium,https://github.com/kubernetes/autoscaler/pull/1403,https://api.github.com/repos/kubernetes/autoscaler/issues/1403,Merge pull request #1245 from hercynium/HEAD-PodDelayPatch,"Add configurable delay for pod age before considering for scale-up

This is in response to https://github.com/kubernetes/autoscaler/issues/1401",closed,True,2018-11-14 21:39:03,2018-11-19 16:14:20
autoscaler,SataQiu,https://github.com/kubernetes/autoscaler/pull/1404,https://api.github.com/repos/kubernetes/autoscaler/issues/1404,Fix typo: posistive -> positive,Fix typo: posistive -> positive,closed,True,2018-11-15 07:49:45,2018-11-15 08:44:57
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1405,https://api.github.com/repos/kubernetes/autoscaler/issues/1405,Add info on forking,,closed,True,2018-11-15 10:59:27,2018-11-22 10:24:44
autoscaler,consideRatio,https://github.com/kubernetes/autoscaler/issues/1406,https://api.github.com/repos/kubernetes/autoscaler/issues/1406,Getting the CA to play well with a custom scheduler,"I opened this issue: https://github.com/kubernetes/kubernetes/issues/71070

I understand it as that the Cluster Autoscaler is utilizing hardcoded logic from the default scheduler. Perhaps it would it be possible in the future to avoid hardcoding this but instead cooperating with the scheduler for the pod?",closed,False,2018-11-15 14:22:05,2018-11-15 14:58:35
autoscaler,awprice,https://github.com/kubernetes/autoscaler/pull/1407,https://api.github.com/repos/kubernetes/autoscaler/issues/1407,add flag to ignore daemonsets when calculating resource utilization of a node,"Adds the flag `--ignore-daemon-set-pods-in-utilization` (defaults to false) and when enabled, factors DaemonSet pods out when calculating the resource utilization of a node. This is useful when there are only system or metrics DaemonSets on the node that are not user workloads.

Not sure how to test this as there don't seem to be any other existing tests that test the functionality of the flags.",closed,True,2018-11-16 00:19:43,2018-11-23 09:12:17
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1408,https://api.github.com/repos/kubernetes/autoscaler/issues/1408,"Changing permissions on ""update-gofmt"" script","Changed permissions and added script run instruction in ""verify-gofmt.sh""",closed,True,2018-11-16 05:15:00,2018-11-16 09:00:28
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1409,https://api.github.com/repos/kubernetes/autoscaler/issues/1409,Making code Go compliant,,closed,True,2018-11-16 06:57:52,2018-11-22 11:38:58
autoscaler,mmingorance-dh,https://github.com/kubernetes/autoscaler/issues/1410,https://api.github.com/repos/kubernetes/autoscaler/issues/1410,Pending pod triggers new node instead of evict a pod with lower priority,"Hi all,
Yesterday we started to test cluster-autoscaler with priorityClasses and podPriority to always have some available extra capacity in our cluster, however whenever a new pod comes up and is in pending state, this one triggers a new node with cluster-autoscaler instead of replacing any pod from the ""paused"" deployment running with lower priorityClass.

This is configuration I added to my cluster:
```kubeAPIServer:
    authorizationMode: RBAC
    authorizationRbacSuperUser: admin
    runtimeConfig:
      scheduling.k8s.io/v1alpha1: ""true""
      admissionregistration.k8s.io/v1beta1: ""true""
      autoscaling/v2beta1: ""true""

kubelet:
    featureGates:
      PodPriority: ""true""
 
 
kubeAPIServer:
    featureGates:
      PodPriority: ""true""

kubeAPIServer:
   admissionControl:
   - Priority
   - NamespaceLifecycle
    - LimitRanger
    - ServiceAccount
    - PersistentVolumeLabel
    - DefaultStorageClass
    - ResourceQuota
    - DefaultTolerationSeconds

kubeControllerManager:
    horizontalPodAutoscalerDownscaleDelay: 1h0m0s
    horizontalPodAutoscalerUseRestClients: true
```


As I could see in my masters, these features seems to be enabled:
`--feature-gates=PodPriority=true`
`--enable-admission-plugins=Priority,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,ValidatingAdmissionWebhook,NodeRestriction,ResourceQuota`

Is there anything else that I'm missing in my config?
The overscaling deployment is the same one you can find in cluster-autoscaler FAQ.",closed,False,2018-11-16 13:53:55,2019-03-12 23:16:33
autoscaler,gtie,https://github.com/kubernetes/autoscaler/issues/1411,https://api.github.com/repos/kubernetes/autoscaler/issues/1411,Crash when Autoscaler starts and AutoscalinGroups have 0 instances,"We are using Cluster Autoscaler (CA) to right-size several AutoScalingGroups in AWS (each featuring different AZ/taint combination). When the leader encouters such an autoscaling group, it crashes with SIGSEGV (and then, next leader does the same, and so on).

Here is the traceback:
```
I1116 18:30:35.877019       1 request.go:485] Throttling request took 90.934181ms, request: GET:https://100.64.0.1:443/api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status
I1116 18:30:36.077059       1 request.go:485] Throttling request took 196.99294ms, request: PUT:https://100.64.0.1:443/api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x134867f]

goroutine 119 [running]:
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).buildNodeFromTemplate(0xc421932bd0, 0xc42359f6c0, 0xc422d9a7c0, 0x0, 0x0, 0xc4206fe750)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:243 +0x3df
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsNodeGroup).TemplateNodeInfo(0xc4208737e0, 0xc421784e70, 0xc423623c00, 0x40)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider.go:303 +0x68
k8s.io/autoscaler/cluster-autoscaler/core.GetNodeInfosForGroups(0xc421b52b00, 0xa, 0x10, 0x3b71440, 0xc4235f9c10, 0x3b932e0, 0xc420372000, 0xc422578940, 0x5, 0x8, ...)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/utils.go:258 +0x296
k8s.io/autoscaler/cluster-autoscaler/core.ScaleUp(0xc4212bcb40, 0xc421023500, 0xc42376a280, 0xc42286b200, 0x1f, 0x20, 0xc421b52b00, 0xa, 0x10, 0xc422578940, ...)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:253 +0x3ef
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc4235cd720, 0xbef3e112c492bf84, 0x28bfbca95, 0x5c8ef20, 0x0, 0x0)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:281 +0x1df4
main.run(0xc4209bc870)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:291 +0x4c8
main.main.func2(0xc4201ac360)
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:372 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
        /gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:155 +0x92
```

Above happens with version 1.3.1 and 1.12.1.",closed,False,2018-11-16 18:34:14,2018-11-20 19:28:02
autoscaler,brantb,https://github.com/kubernetes/autoscaler/issues/1412,https://api.github.com/repos/kubernetes/autoscaler/issues/1412,[Azure AKS] Autoscaler can't set agent pool size on an AKS cluster with AzureAD integration enabled,"Kubernetes version: 1.11.4 (Latest available on AKS as of writing)
cluster-autoscaler version: 1.3.4

After scaling down the cluster and deleting a VM, the Azure provider attempts to update the node count in the AKS agent pool. When the AKS cluster has the AzureAD integration enabled, this fails with the following errors:

```
I1116 20:38:15.436203       1 utils.go:422] Decreasing size of agentpool, expected=5 current=3 delta=-2
I1116 20:38:15.873178       1 azure_container_service_pool.go:304] Setting size for cluster (""development-001"") with new count (3)
I1116 20:38:16.322277       1 azure_container_service_pool.go:184] Current size: 5, Target size requested: 3
E1116 20:38:16.322644       1 azure_container_service_pool.go:193] Failed to update AKS cluster (%q): %vdevelopment-001containerservice.ManagedClustersClient#CreateOrUpdate: Invalid input: autorest/validation: validation failed: parameter=parameters.ManagedClusterProperties.AadProfile.ServerAppSecret constraint=Null value=(*string)(nil) details: value can not be null; required parameter
E1116 20:38:16.322736       1 static_autoscaler.go:190] Failed to fix node group sizes: Failed to decrease agentpool: containerservice.ManagedClustersClient#CreateOrUpdate: Invalid input: autorest/validation: validation failed: parameter=parameters.ManagedClusterProperties.AadProfile.ServerAppSecret constraint=Null value=(*string)(nil) details: value can not be null; required parameter
```

I have a feeling this is happening because `AadProfile.ServerAppSecret` is a write-only property in the ARM API so it isn't being round-tripped when `managedContainerServicesClient.Get()` [is called earlier in the function](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/azure_container_service_pool.go#L121-L123).",closed,False,2018-11-16 20:53:02,2018-11-20 06:45:04
autoscaler,multi-io,https://github.com/kubernetes/autoscaler/pull/1413,https://api.github.com/repos/kubernetes/autoscaler/issues/1413,pass kubeconfig to cloudprovider builder,"We have an internal cloud provider here (which we're planning to publish in a separate PR later) that uses the K8s cluster-api for creating nodes and thus needs to access to the cluster itself (its CloudConfig is just the cluster kubeconfig). As a preparation for that, this PR changes AutoscalerOptions and the cloud provider builder so we pass the kubeconfig down to the `buildCloudProvider` in case any actual cloud provider needs it.",closed,True,2018-11-18 18:41:22,2018-12-03 07:11:48
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1414,https://api.github.com/repos/kubernetes/autoscaler/issues/1414,Cluster Autoscaler 1.12: fix ServerAppSecret issues for AKS clusters,"Without this change, CA would report following errors when using on AKS clusters:

```
containerservice.Managed
ClustersClient#CreateOrUpdate: Invalid input: autorest/validation: validation failed: parameter=parameters.ManagedCluste
rProperties.AadProfile.ServerAppSecret constraint=Null value=(*string)(nil) details: value can not be null; required parameter`
```

Fixes #1369 #1105 #1304
",closed,True,2018-11-19 02:34:36,2018-11-21 09:06:34
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1415,https://api.github.com/repos/kubernetes/autoscaler/issues/1415,Cluster Autoscaler 1.3: fix ServerAppSecret issues for AKS clusters,"Without this change, CA would report following errors when using on AKS clusters:

```
containerservice.Managed
ClustersClient#CreateOrUpdate: Invalid input: autorest/validation: validation failed: parameter=parameters.ManagedCluste
rProperties.AadProfile.ServerAppSecret constraint=Null value=(*string)(nil) details: value can not be null; required parameter`
```

Fixes #1369 #1105 #1304
",closed,True,2018-11-19 02:59:14,2018-12-18 07:08:13
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1416,https://api.github.com/repos/kubernetes/autoscaler/issues/1416,Cluster Autoscaler 1.2: fix ServerAppSecret issues for AKS clusters,"Without this change, CA would report following errors when using on AKS clusters:

```
containerservice.Managed
ClustersClient#CreateOrUpdate: Invalid input: autorest/validation: validation failed: parameter=parameters.ManagedCluste
rProperties.AadProfile.ServerAppSecret constraint=Null value=(*string)(nil) details: value can not be null; required parameter`
```

Fixes #1369 #1105 #1304
",closed,True,2018-11-19 03:19:51,2018-11-19 09:17:54
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1417,https://api.github.com/repos/kubernetes/autoscaler/issues/1417,Cluster Autoscaler: fix ServerAppSecret issues for AKS clusters,"Without this change, CA would report following errors when using on AKS clusters:

```
containerservice.Managed
ClustersClient#CreateOrUpdate: Invalid input: autorest/validation: validation failed: parameter=parameters.ManagedCluste
rProperties.AadProfile.ServerAppSecret constraint=Null value=(*string)(nil) details: value can not be null; required parameter`
```

Fixes #1369 #1105 #1304

This PR also updates kubernetes version to v1.13.0-beta.1.",closed,True,2018-11-19 05:43:47,2018-11-19 09:18:12
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1418,https://api.github.com/repos/kubernetes/autoscaler/issues/1418,Add replicasets in the apps apigroup,This is required for kubernetes v1.12.x.,closed,True,2018-11-19 06:16:23,2018-11-19 09:20:00
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1419,https://api.github.com/repos/kubernetes/autoscaler/issues/1419,Update cluster-autoscaler version to 1.13.0-alpha.1,,closed,True,2018-11-19 15:18:14,2018-11-19 15:31:42
autoscaler,hercynium,https://github.com/kubernetes/autoscaler/pull/1420,https://api.github.com/repos/kubernetes/autoscaler/issues/1420,Merge pull request #1245 from hercynium/HEAD-PodDelayPatch,"Add configurable delay for pod age before considering for scale-up

This is in response to https://github.com/kubernetes/autoscaler/issues/1401",closed,True,2018-11-19 16:13:52,2019-02-01 16:54:30
autoscaler,jbartosik,https://github.com/kubernetes/autoscaler/pull/1421,https://api.github.com/repos/kubernetes/autoscaler/issues/1421,Don't load CA key.,We don't use it for anything.,closed,True,2018-11-19 16:41:23,2018-11-20 09:22:47
autoscaler,seh,https://github.com/kubernetes/autoscaler/pull/1422,https://api.github.com/repos/kubernetes/autoscaler/issues/1422,Update AWS EC2 instance type catalog,"Run _make generate_ to pull and translate a fresh catalog of [AWS's available EC2 instance types](https://aws.amazon.com/ec2/instance-types/) used by the cluster autoscaler.

This addresses #1411 but isn't a complete solution, as [we should also trap these instance type lookup failures](https://github.com/kubernetes/autoscaler/issues/1411#issuecomment-439999208).",closed,True,2018-11-19 21:38:44,2018-11-20 18:42:19
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1423,https://api.github.com/repos/kubernetes/autoscaler/issues/1423,Run feature gates based logic to fix consistency of CA and scheduler,,closed,True,2018-11-20 13:07:39,2018-11-20 13:23:41
autoscaler,dharmab,https://github.com/kubernetes/autoscaler/issues/1424,https://api.github.com/repos/kubernetes/autoscaler/issues/1424,Support Azure Managed Identity from Instance Metadata instead of VM extension,"[The Azure Managed Service Identity VM extension is being deprecated in January.](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview)

![image](https://user-images.githubusercontent.com/4450708/48782398-c9f96c80-ece6-11e8-8130-e0f5d4136181.png)

This issue is to request support for the Azure Instance Metadata managed identity API, which is the replacement for the VM extension.",closed,False,2018-11-20 15:08:35,2018-11-20 15:40:24
autoscaler,seh,https://github.com/kubernetes/autoscaler/pull/1425,https://api.github.com/repos/kubernetes/autoscaler/issues/1425,Report unknown AWS EC2 instance types requested by ASGs,"As [noted in diagnosing](https://github.com/kubernetes/autoscaler/issues/1411#issuecomment-439999208) #1411, the cluster autoscaler fails in AWS when it attempts to deduce the EC2 instance type used by an ASG, but fails to find the nominated instance type in its internal catalog. The  `(*cloudprovider/aws/AwsManager).getAsgTemplatelookup` method does not anticipate such a lookup failure, and panics shortly afterward by attempting to dereference the resulting null pointer as a would-be instance type descriptor.

Instead, trap such lookup failures and return an indicative failure, noting which ASG is trying to use which instance type that the autoscaler has never heard of.

Note that the recent #1422 freshens the autoscaler's instance type catalog, making such lookup failures less likely for the next EC2 product cycle or two, but this patch makes it easier for operators to diagnose such a problem without losing the autoscaler entirely.

Fixes #1411.",closed,True,2018-11-20 18:28:54,2018-11-20 19:28:02
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1426,https://api.github.com/repos/kubernetes/autoscaler/issues/1426,Update OWNERS,Yet another branch where wrong usernames persisted.,closed,True,2018-11-21 08:57:41,2019-03-20 15:39:17
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1427,https://api.github.com/repos/kubernetes/autoscaler/issues/1427,Start listing known beta limitations,,closed,True,2018-11-21 12:15:22,2018-11-21 12:28:29
autoscaler,jbartosik,https://github.com/kubernetes/autoscaler/pull/1428,https://api.github.com/repos/kubernetes/autoscaler/issues/1428,Don't load CA key.,"We don't use it for anything.

Cherrypick 057743565cf88bfe58af2d4f284cf81a789c49f1 tovpa-release-0.3 branch",closed,True,2018-11-21 13:52:34,2018-11-21 14:10:11
autoscaler,OndrejKu,https://github.com/kubernetes/autoscaler/issues/1429,https://api.github.com/repos/kubernetes/autoscaler/issues/1429,AWS autoscaler (possibly) doesn't update records that are accessed by nginx-lego,"Hey guys, I'm having troubles with nginx-lego (I know it's deprecated) and node autoscaler. I had to scale up manually through hpa and patching temporarily minReplicas to high number. All scaled well, new nodes were added because of pod increase. After the traffic spike I set the number back to normal (which is really low) and I can see a lot of bad gateway 502 errors. After I examinated nginx-lego pod's log, I was able to see that plenty of requests were going to pods that aren't there anymore (connection refused or No route to host). Any idea on what could be wrong?",closed,False,2018-11-21 17:58:09,2018-11-22 14:41:35
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/issues/1430,https://api.github.com/repos/kubernetes/autoscaler/issues/1430,Updater crashed after creating VPA with example/hamster deployment ,"Updater showing ""CrashLoopBackOff"" status after I created vpa and pods with ""example/hamster.yaml"".
Updater Logs:
 I1121 05:51:38.583230       7 flags.go:52] FLAG: --alsologtostderr=""false""
I1121 05:51:38.583999       7 flags.go:52] FLAG: --eviction-tolerance=""0.5""
I1121 05:51:38.584039       7 flags.go:52] FLAG: --httptest.serve=""""
I1121 05:51:38.584063       7 flags.go:52] FLAG: --log-backtrace-at="":0""
I1121 05:51:38.584093       7 flags.go:52] FLAG: --log-dir=""""
I1121 05:51:38.584117       7 flags.go:52] FLAG: --logtostderr=""false""
I1121 05:51:38.584147       7 flags.go:52] FLAG: --min-replicas=""2""
I1121 05:51:38.584170       7 flags.go:52] FLAG: --stderrthreshold=""0""
I1121 05:51:38.584192       7 flags.go:52] FLAG: --updater-interval=""1m0s""
I1121 05:51:38.584224       7 flags.go:52] FLAG: --v=""4""
I1121 05:51:38.584248       7 flags.go:52] FLAG: --vmodule=""""
I1121 05:51:38.584278       7 main.go:45] Vertical Pod Autoscaler 0.2.0 Updater
I1121 05:51:38.669950       7 reflector.go:202] Starting reflector *v1alpha1.VerticalPodAutoscaler (1h0m0s) from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/utils/vpa/api.go:83
I1121 05:51:38.670031       7 reflector.go:240] Listing and watching *v1alpha1.VerticalPodAutoscaler from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/utils/vpa/api.go:83
I1121 05:51:38.869940       7 shared_informer.go:123] caches populated
I1121 05:51:38.869984       7 api.go:87] Initial VPA synced successfully
I1121 05:51:38.870168       7 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/logic/updater.go:155
I1121 05:51:38.870220       7 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/logic/updater.go:155
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0xe572b3]
",closed,False,2018-11-22 05:36:04,2019-01-07 10:15:41
autoscaler,garo,https://github.com/kubernetes/autoscaler/issues/1431,https://api.github.com/repos/kubernetes/autoscaler/issues/1431,Unable to scale AWS autoscaling array up to satisfy pod EBS requirement in particular zone (NoVolumeZoneConflict),"Running Kubernetes v1.10.3-eks in Amazon EKS. Cluster has one AWS Autoscaling Array with three different availability zone / subnets defined. At the time of the problem the cluster has two nodes, one in us-east-1a and one in us-east-1c.

There is a pod with an PVC attached, which is backed by an EBS PV in us-east-1d. Because there isn't any node running in us-east-1d the pod cannot start.

The problem is that cluster-autoscaler isn't able to scale up the autoscaling array so that a new worker would appear in us-east-1d to satisfy the zone requirement. Manually increasing the autoscaling size does give a new node in the correct region.

Cluster-autoscaler is installed with helm: chart version cluster-autoscaler-0.7.0, App version: 1.2.2.
Installation command:
```
helm install --name cluster-autoscaler --namespace kube-system \
   --version 0.7.0 \ # 0.7.0 is meant for kubernetes 1.10.x, see https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler
   --set autoDiscovery.clusterName=$CLUSTER_NAME \
   --set awsRegion=$AWS_REGION \
   --set sslCertPath=/etc/kubernetes/pki/ca.crt \
   --set rbac.create=true \
   --set podAnnotations.""iam\.amazonaws\.com/role""=$CLUSTER_NAME-eks-worker-node \
   stable/cluster-autoscaler
```

Cluster autoscaler error log:

```
static_autoscaler.go:114] Starting main loop
utils.go:456] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
static_autoscaler.go:263] Filtering out schedulables
static_autoscaler.go:273] No schedulable pods
scale_up.go:59] Pod monitoring/prometheus-mon-prometheus-operator-prometheus-0 is unschedulable
scale_up.go:92] Upcoming 0 nodes
scale_up.go:152] Scale-up predicate failed: NoVolumeZoneConflict predicate mismatch, cannot put monitoring/prometheus-mon-prometheus-operator-prometheus-0 on template-node-for-cluster-generic-nodes-4423088653825289861, reason: node(s) had no available volume zone
scale_up.go:181] No pod can fit to cluster-generic-nodes
scale_up.go:186] No expansion options
static_autoscaler.go:322] Calculating unneeded nodes
factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""monitoring"", Name:""prometheus-mon-prometheus-operator-prometheus-0"", UID:""9473cd20-ee27-11e8-83f9-0e13418086b6"", APIVersion:""v1"", ResourceVersion:""5808447"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
scale_down.go:175] Scale-down calculation: ignoring 2 nodes, that were unremovable in the last 5m0s
static_autoscaler.go:352] Scale down status: unneededOnly=true lastScaleUpTime=2018-11-22 07:26:02.736718705 +0000 UTC lastScaleDownDeleteTime=2018-11-22 07:26:02.736719107 +0000 UTC lastScaleDownFailTime=2018-11-22 07:26:02.736719507 +0000 UTC schedulablePodsPresent=false isDeleteInProgress=false
```",closed,False,2018-11-22 07:44:30,2019-02-12 21:33:26
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1432,https://api.github.com/repos/kubernetes/autoscaler/issues/1432,Update examples and deployments for new version.,,closed,True,2018-11-22 10:20:46,2018-11-22 12:41:09
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1433,https://api.github.com/repos/kubernetes/autoscaler/issues/1433,Start listing known beta limitations.,,closed,True,2018-11-22 10:29:54,2018-11-22 11:19:06
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1434,https://api.github.com/repos/kubernetes/autoscaler/issues/1434,Run feature gates based logic to fix consistency of CA and scheduler,Cherry pick,closed,True,2018-11-22 11:49:37,2018-11-22 12:24:21
autoscaler,vishalcs05,https://github.com/kubernetes/autoscaler/issues/1435,https://api.github.com/repos/kubernetes/autoscaler/issues/1435,Cluster Autoscaler is not scaling the nodes when not using `--cloud-provider=aws` in kubelet configuration,"Kubernetes Version: `v1.11.2`  
Cluster Autoscaler Version: `v1.2.2`  

Cluster Autoscaler is not scaling the nodes when not using `--cloud-provider=aws` in kubelet configuration, getting below logs in cluster autoscaler.

```
I1122 12:36:46.798580       1 scale_up.go:59] Pod default/my-application-56dc8d4bf5-j5jzs is unschedulable
E1122 12:36:46.798600       1 static_autoscaler.go:304] Failed to scale up: failed to build node infos for node groups: Wrong id: expected format aws:///<zone>/<name>, got
I1122 12:36:48.516201       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
```
Though it recognised the autoscaling group which I passed in autoscaler deployment yaml.
```
I1122 09:26:53.892573       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [k8s-test-workers]
I1122 09:26:54.693002       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I1122 09:26:55.023658       1 aws_manager.go:241] Refreshed ASG list, next refresh after 2018-11-22 09:27:55.023651853 +0000 UTC
I1122 09:26:55.023787       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [k8s-test-workers]
I1122 09:26:55.028800       1 main.go:228] Registered cleanup signal handler
```
Options I gave in cluster autoscaler deployment are below
       
         command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=random
            - --max-node-provision-time=40m
            - --scan-interval=40s
            - --scale-down-unneeded-time=5m
            - --nodes=0:10:k8s-test-workers

Is using `--cloud-provider` necessary in kubelet config for autoscaler to work ? if not, can somebody tell me what I am missing in configuration.",closed,False,2018-11-22 12:51:43,2018-11-22 13:14:28
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1436,https://api.github.com/repos/kubernetes/autoscaler/issues/1436,Update examples and deployments for new version.,,closed,True,2018-11-22 14:57:59,2018-12-13 01:34:48
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1437,https://api.github.com/repos/kubernetes/autoscaler/issues/1437,Update overprovisioning setup,Link AWS kops setup instructions from #1410,closed,True,2018-11-22 15:04:26,2019-03-20 15:39:20
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1438,https://api.github.com/repos/kubernetes/autoscaler/issues/1438,Ignore starting nodes in node group health,"When a user (or test) manually creates new nodes, or if Cluster Autoscaler restarts and loses track of scale-ups it requested, it may consider a node group unhealthy because of upcoming nodes. This is inconsistent with how cluster state is computed and with how scale-up expansion options are calculated, and most likely not what we want to do.",closed,True,2018-11-22 19:55:29,2019-02-07 11:18:04
autoscaler,hex108,https://github.com/kubernetes/autoscaler/issues/1439,https://api.github.com/repos/kubernetes/autoscaler/issues/1439,[Proposal] Add cron pod autoscaler,"I have implemented a CronHPA which enables us to scale workloads(those support `scale` subresource) periodically using crontab scheme.  

## Use cases
A lot of game player will play games from Friday evening to Sunday evening. It will be better to provide a better experience for players if game servers(pods) could be scaled to a lager size at Friday evening and scaled to normal size at Sunday evening. And that's what game server admins do every week. 

Some other customers also do similar things because their products' usage also have peaks and valleys periodically. CronHPA is for their requirement.

## CronHPA example:

```
---
apiVersion: gs.io/v1alpha1
kind: CronHPA
metadata:
  name: example-cron-hpa
spec:
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: demo-deployment
  crons:
    - schedule: ""0 23 * * 5""  // Set replicas to 60 every Friday 23:00
      targetReplicas: 60
    - schedule: ""0 23 * * 7""  // Set replicas to 30 every Sunday 23:00
      targetReplicas: 30

---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: demo-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

BTW: Not sure it is the right place to send this proposal. If it is, I'll send a PR.",closed,False,2018-11-23 01:50:45,2018-12-09 14:45:30
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1440,https://api.github.com/repos/kubernetes/autoscaler/issues/1440,[WIP] Adding test cases for cluster feeder,Currently Adding only for LoadPod as testing purpose. I'll add remaining in next commit.,closed,True,2018-11-23 07:13:51,2019-02-01 16:58:17
autoscaler,johanneswuerbach,https://github.com/kubernetes/autoscaler/pull/1441,https://api.github.com/repos/kubernetes/autoscaler/issues/1441,1.12: Backport AWS: Improved balancing,Backport https://github.com/kubernetes/autoscaler/pull/1378 to 1.12.x,closed,True,2018-11-25 23:06:28,2018-12-07 09:21:07
autoscaler,johanneswuerbach,https://github.com/kubernetes/autoscaler/pull/1442,https://api.github.com/repos/kubernetes/autoscaler/issues/1442,1.3: Backport AWS: Improved balancing,Backport #1378 to 1.3.x,closed,True,2018-11-25 23:12:39,2018-12-07 09:22:28
autoscaler,lvjing2,https://github.com/kubernetes/autoscaler/issues/1443,https://api.github.com/repos/kubernetes/autoscaler/issues/1443,Does autoscaler support dynamic config by updating the config map?,"Just as the title, I am curious about whether autoscaler support dynamic config via updating the config map, since it seems had been removed by #851 .",closed,False,2018-11-26 10:10:41,2019-01-29 01:48:43
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1444,https://api.github.com/repos/kubernetes/autoscaler/issues/1444,Update godeps on 1.13 branch,,closed,True,2018-11-26 14:11:42,2018-11-26 17:28:31
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1445,https://api.github.com/repos/kubernetes/autoscaler/issues/1445,Update go version used from 1.10.2 to 1.11.2 to match one used by k8s,,closed,True,2018-11-26 14:27:25,2018-11-26 15:02:19
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1446,https://api.github.com/repos/kubernetes/autoscaler/issues/1446,Update cluster-autoscaler godeps on master,,closed,True,2018-11-26 16:34:18,2019-02-20 13:05:41
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1447,https://api.github.com/repos/kubernetes/autoscaler/issues/1447,Add OWNERS to /builder,"Builder is used only for Cluster Autoscaler. It'll be useful if more maintainers can e.g. update Go version there. Long-term, we should probably move it to /cluster-autoscaler though.",closed,True,2018-11-26 16:40:24,2018-11-27 10:45:26
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1448,https://api.github.com/repos/kubernetes/autoscaler/issues/1448,Cluster Autoscaler release 1.13.0-rc.1,,closed,True,2018-11-26 17:30:01,2018-11-26 17:47:50
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/issues/1449,https://api.github.com/repos/kubernetes/autoscaler/issues/1449,AWS cloud provider tests take 120 seconds,"The problem started to occur after merging 
https://github.com/kubernetes/autoscaler/commit/a36f8007afa65fcb2ff7ae291ee815be5512e01c
Each of 6 test method tries to get region from AWS api (see aws_manager.go `getRegion` method). This call times out after 20seconds.

@shatil Is there a chance that you could fix that?",closed,False,2018-11-26 17:46:00,2019-01-15 09:13:05
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1450,https://api.github.com/repos/kubernetes/autoscaler/issues/1450,CA cherry picks to 1.13 and update version,,closed,True,2018-11-26 18:47:45,2018-11-27 09:22:38
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1451,https://api.github.com/repos/kubernetes/autoscaler/issues/1451,Initialize klog,,closed,True,2018-11-26 19:28:05,2018-11-26 20:01:51
autoscaler,spinus,https://github.com/kubernetes/autoscaler/issues/1452,https://api.github.com/repos/kubernetes/autoscaler/issues/1452,Azure Scale Set - node labeling,"I have a set of specific worker nodes to scale and I have corresponding node selector on azure.

I get 
```
I1126 14:27:18.221788       1 scale_up.go:249] Pod XXX-6fbdd6b9b5-9w6tp is unschedulable
I1126 14:27:18.247845       1 utils.go:196] Pod XXX-6fbdd6b9b5-9w6tp can't be scheduled on k8sscaleset, predicate failed: GeneralPredicates predicate mismatch, cannot put XXX-6fbdd6b9b5-9w6tp on template-node-for-k8sscaleset-3970733359372029124, reason: node(s) didn't match node selector
```

I'm not sure what should I do to configure autoscaler to put labels on the nodes (azure scale set). How I do it?",open,False,2018-11-26 20:22:07,2019-04-03 08:40:22
autoscaler,c4po,https://github.com/kubernetes/autoscaler/issues/1453,https://api.github.com/repos/kubernetes/autoscaler/issues/1453,AWS: Failed to create AWS Manager,"Kubernetes: 1.11.4
CA: 1.3.4

tried the `cluster-autoscaler-one-asg.yaml` and `cluster-autoscaler-autodiscover.yaml`
getting error message like this.
```
I1127 00:45:11.243473       1 cloud_provider_builder.go:72] Building aws cloud provider.
E1127 00:45:11.243641       1 aws_manager.go:122] Failed to regenerate ASG cache: cannot autodiscover ASGs: MissingRegion: could not find region configuration
F1127 00:45:11.243667       1 cloud_provider_builder.go:137] Failed to create AWS Manager: cannot autodiscover ASGs: MissingRegion: could not find region configuration
```

did I miss anything?",closed,False,2018-11-27 00:49:13,2018-12-03 20:49:14
autoscaler,spinus,https://github.com/kubernetes/autoscaler/issues/1454,https://api.github.com/repos/kubernetes/autoscaler/issues/1454,"Azure vmss ""failed to get instance ID from cloud provider: instance not found""","I have an issue with using Azure ScaleSet. Kubelet is failing on scale set nodes with error:
```
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: I1127 09:48:34.632346   30012 kubelet.go:1760] Starting kubelet main sync loop.
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: I1127 09:48:34.632750   30012 kubelet.go:1777] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: I1127 09:48:34.633116   30012 server.go:986] Started kubelet
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: I1127 09:48:34.633665   30012 volume_manager.go:247] Starting Kubelet Volume Manager
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: I1127 09:48:34.636357   30012 desired_state_of_world_populator.go:130] Desired state populator starts to run
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: W1127 09:48:34.654094   30012 cni.go:172] Unable to update cni config: No networks found in /etc/cni/net.d
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: E1127 09:48:34.654345   30012 kubelet.go:2112] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: I1127 09:48:34.738014   30012 kubelet.go:1777] skipping pod synchronization - [container runtime is down]
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: I1127 09:48:34.738402   30012 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: I1127 09:48:34.793119   30012 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: E1127 09:48:34.848439   30012 kubelet_node_status.go:75] Unable to construct v1.Node object for kubelet: failed to get instance ID from cloud provider: instance not found
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: I1127 09:48:34.938418   30012 kubelet.go:1777] skipping pod synchronization - [container runtime is down]
Nov 27 09:48:34 k8sscales00000Q kubelet[30012]: F1127 09:48:34.941184   30012 kubelet.go:1330] Kubelet failed to get node info: failed to get instance ID from cloud provider: instance not found
```

kubernetes: 1.11
cluster-autoscaler: 1.3.4",closed,False,2018-11-27 09:54:20,2019-03-26 04:34:27
autoscaler,davidxia,https://github.com/kubernetes/autoscaler/issues/1455,https://api.github.com/repos/kubernetes/autoscaler/issues/1455,Confusing error message when applying a VerticalPodAutoscaler with boolean value for spec.updatePolicy.updateMode,"I tried to apply the VerticalPodAutoscaler YAML below.

```yaml
apiVersion: ""poc.autoscaling.k8s.io/v1alpha1""
kind: ""VerticalPodAutoscaler""
metadata:
  name: ""foo""
spec:
  selector:
    matchLabels:
      app: ""foo""
  updatePolicy:
    updateMode: Off
```

## What happened

I didn't realize an unquoted `Off` is a YAML boolean false. This lead to the following error.

```
kubectl apply -f /path/to/file/above

error: error when creating ""/path/to/file/above"": Post https://<master-IP>/apis/poc.autoscaling.k8s.io/v1alpha1/namespaces/default/verticalpodautoscalers: unexpected EOF; some request body already written
```

Everything works as expected if I quote the `Off` to `""Off""`.

## What I expected

Perhaps the error message from the API could be more helpful. Maybe some early validation on the value of `updateMode` would help.

## kube versions

```
kubectl version

Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.7"", GitCommit:""0c38c362511b20a098d7cd855f1314dad92c2780"", GitTreeState:""clean"", BuildDate:""2018-08-20T10:09:03Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10+"", GitVersion:""v1.10.6-gke.11"", GitCommit:""42df8ec7aef509caba40b6178616dcffca9d7355"", GitTreeState:""clean"", BuildDate:""2018-11-08T20:06:00Z"", GoVersion:""go1.9.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```",closed,False,2018-11-27 16:59:08,2018-11-30 12:11:27
autoscaler,frobware,https://github.com/kubernetes/autoscaler/pull/1456,https://api.github.com/repos/kubernetes/autoscaler/issues/1456,gce: increase test timeout in TestWaitForGkeOp,"This is the same change as was done in https://github.com/kubernetes/autoscaler/pull/1356 where I see
intermittent CI failures on AWS CI instances.

Signed-off-by: Andrew McDermott <amcdermo@redhat.com>",closed,True,2018-11-27 17:46:10,2019-03-22 06:21:34
autoscaler,williamhuangh,https://github.com/kubernetes/autoscaler/issues/1457,https://api.github.com/repos/kubernetes/autoscaler/issues/1457,addon resizer changes `optional: true` of volume mounts,"original yaml
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prometheus-deployment
  namespace: monitoring
  annotations:
    k8sdeploy.lyft.net/skip-rollout-monitoring: true
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      name: prometheus
      labels:
        app: prometheus
        version: ${sha}
    spec:
      serviceAccount: prometheus
      securityContext:
        runAsUser: 0 # This is required to use the EBS volume which is owned by root
      containers:
      - name: addon-resizer
        image: k8s.gcr.io/addon-resizer:1.7
        resources:
          limits:
            cpu: 100m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 30Mi
        env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        command:
          - /pod_nanny
          - --container=prometheus
          - --cpu=3
          - --extra-cpu=10m
          - --memory=2Gi
          - --extra-memory=100Mi
          - --threshold=5
          - --deployment=prometheus        
      - name: prometheus
        image: quay.io/prometheus/prometheus:v2.3.2
        args:
            - '--config.file=/etc/prometheus/prometheus.yaml'
            - --storage.tsdb.path=/prometheus
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle            
        ports:
        - name: web
          containerPort: 9090
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
        resources:
          limits:
            cpu: 8
            memory: 12000Mi
          requests:
            cpu: 3
            memory: 2000Mi
        env:
        - name: STORAGE_RETENTION
          valueFrom:
            configMapKeyRef:
              name: prometheus-env
              key: storage-retention
        - name: STORAGE_MEMORY_CHUNKS
          valueFrom:
            configMapKeyRef:
              name: prometheus-env
              key: storage-memory-chunks
        volumeMounts:
        - name: config-volume
          mountPath: /etc/prometheus
        - name: rules
          mountPath: /etc/prometheus/rules
        - name: rules-ingress
          mountPath: /etc/prometheus/rules/ingress
        - name: rules-custom
          mountPath: /etc/prometheus/rules/services
        - name: prometheus-data
          mountPath: /prometheus
      volumes:
      - name: config-volume
        configMap:
          name: prometheus-configmap
      - name: rules
        configMap:
          name: prometheus-rules
      - name: rules-ingress
        configMap:
          name: prometheus-rules-ingress
          optional: true
      - name: rules-custom
        configMap:
          name: prometheus-rules-custom
          optional: true
      - name: prometheus-data
        persistentVolumeClaim:
          claimName: prometheus
```
after addon-resizer
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prometheus-deployment
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: prometheus
        version: 7c4cf683790c010aba8698883f245fdacee706d1
      name: prometheus
    spec:
      containers:
      - command:
        - /pod_nanny
        - --container=prometheus
        - --cpu=3
        - --extra-cpu=10m
        - --memory=2Gi
        - --extra-memory=100Mi
        - --threshold=5
        - --deployment=prometheus-deployment
        env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        image: k8s.gcr.io/addon-resizer:1.7
        imagePullPolicy: IfNotPresent
        name: addon-resizer
        resources:
          limits:
            cpu: 100m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 30Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      - args:
        - --config.file=/etc/prometheus/prometheus.yaml
        - --storage.tsdb.path=/prometheus
        - --web.console.libraries=/etc/prometheus/console_libraries
        - --web.console.templates=/etc/prometheus/consoles
        - --web.enable-lifecycle
        env:
        - name: STORAGE_RETENTION
          valueFrom:
            configMapKeyRef:
              key: storage-retention
              name: prometheus-env
        - name: STORAGE_MEMORY_CHUNKS
          valueFrom:
            configMapKeyRef:
              key: storage-memory-chunks
              name: prometheus-env
        image: quay.io/prometheus/prometheus:v2.3.2
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /-/healthy
            port: 9090
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 30
        name: prometheus
        ports:
        - containerPort: 9090
          name: web
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /-/ready
            port: 9090
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 30
        resources:
          limits:
            cpu: 3110m
            memory: 3148Mi
          requests:
            cpu: 3110m
            memory: 3148Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/prometheus
          name: config-volume
        - mountPath: /etc/prometheus/rules
          name: rules
        - mountPath: /etc/prometheus/rules/ingress
          name: rules-ingress
        - mountPath: /etc/prometheus/rules/services
          name: rules-custom
        - mountPath: /prometheus
          name: prometheus-data
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        runAsUser: 0
      serviceAccount: prometheus
      serviceAccountName: prometheus
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          defaultMode: 420
          name: prometheus-configmap
        name: config-volume
      - configMap:
          defaultMode: 420
          name: prometheus-rules
        name: rules
      - configMap:
          defaultMode: 420
          name: prometheus-rules-ingress
        name: rules-ingress
      - configMap:
          defaultMode: 420
          name: prometheus-rules-custom
        name: rules-custom
      - name: prometheus-data
        persistentVolumeClaim:
          claimName: prometheus
```
notice that it drops the `optional: true` from volumes",open,False,2018-11-28 02:23:11,2019-01-23 09:11:37
autoscaler,luarx,https://github.com/kubernetes/autoscaler/issues/1458,https://api.github.com/repos/kubernetes/autoscaler/issues/1458,Vertical-pod-autoscaler is requesting more MEMORY than needed,"If I run the `hamster.yaml` example of vertical-pod-autoscaler, it schedules great the requested CPU (I have  tested it modifying the main command to see how VPA modifies the requested CPU).

The problem is that it **requests more memory than needed** (look at the picture). Do I have to configure anything else?
![image](https://user-images.githubusercontent.com/22997139/49157013-16642e00-f31f-11e8-8517-6483f532e7d4.png)


",closed,False,2018-11-28 14:04:10,2019-02-05 18:48:26
autoscaler,luarx,https://github.com/kubernetes/autoscaler/issues/1459,https://api.github.com/repos/kubernetes/autoscaler/issues/1459,Vertical-pod-autoscaler - How much time does it need to change the requested CPU/Memory?,"I wonder how much time does vertical-pod-autoscaler need to change the requested CPU/Memory.

Also...does it have an internal registry with the historical resources of each pod?",open,False,2018-11-28 14:10:10,2019-03-29 12:47:17
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1460,https://api.github.com/repos/kubernetes/autoscaler/issues/1460,Cluster Autoscaler release 1.13.0,,closed,True,2018-11-28 14:45:49,2018-11-28 14:47:01
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1461,https://api.github.com/repos/kubernetes/autoscaler/issues/1461,Cluster Autoscaler release 1.13.1,,closed,True,2018-11-28 15:52:45,2018-11-28 17:10:40
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1462,https://api.github.com/repos/kubernetes/autoscaler/issues/1462,Expose WaitForRecommendationPresent method and extract WaitForVPAMatch.,Introduced to facilitate waiting for VPA object changes.,closed,True,2018-11-29 10:22:33,2018-12-24 09:46:58
autoscaler,dmaspataud,https://github.com/kubernetes/autoscaler/pull/1463,https://api.github.com/repos/kubernetes/autoscaler/issues/1463,"Updating AWS EC2 instance type (C5N, F1, G3S, R5A, U instances) - 1.2.x",,closed,True,2018-11-29 16:53:01,2018-12-07 09:25:01
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1464,https://api.github.com/repos/kubernetes/autoscaler/issues/1464,Better quota-exceeded/stockout handling,,closed,True,2018-11-29 18:31:23,2018-12-31 13:28:09
autoscaler,JoeWrightss,https://github.com/kubernetes/autoscaler/pull/1465,https://api.github.com/repos/kubernetes/autoscaler/issues/1465,Fix some typos: reconstitures -> reconstitutes,"Fix some typos: ""reconstitures"" -> ""reconstitutes"".",closed,True,2018-12-02 07:23:47,2018-12-03 07:37:29
autoscaler,JoeWrightss,https://github.com/kubernetes/autoscaler/pull/1466,https://api.github.com/repos/kubernetes/autoscaler/issues/1466,Fix klog.Warningf error message in gen.go,Line 128: unmarshaling -> unmarshalling,closed,True,2018-12-02 09:06:26,2018-12-03 07:07:54
autoscaler,JoeWrightss,https://github.com/kubernetes/autoscaler/pull/1467,https://api.github.com/repos/kubernetes/autoscaler/issues/1467,Fix some typos,"Fix some typos: ""sheduled"" -> ""scheduled"" in nodegroup_list_processor.go and pod_list_processor.go.",closed,True,2018-12-03 08:01:55,2018-12-03 09:38:38
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1468,https://api.github.com/repos/kubernetes/autoscaler/issues/1468,Move ResourceLimiter to separate file,"Cleanup refactor.

Honestly I do not believe the implementation should be in cloudprovider package at all.",closed,True,2018-12-03 09:06:28,2018-12-04 10:18:07
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1469,https://api.github.com/repos/kubernetes/autoscaler/issues/1469,Change Error and State constants to match golang convention,,closed,True,2018-12-03 09:10:39,2018-12-03 09:54:33
autoscaler,pondohva,https://github.com/kubernetes/autoscaler/issues/1470,https://api.github.com/repos/kubernetes/autoscaler/issues/1470,"pod in pending state on GKE, but pool can be scaled up","Hello!

Master version: 1.11.3-gke.18
Nodepool version: v1.10.7-gke.2
Minimum nodes: 3
Maximum nodes: 10
Current size: 7

Node in pool has 2 CPU and 7.5Gb ram, new pod require (and limit) 500m CPU and 256Mi, but I'm getting this message

```
Events:
  Type     Reason             Age                 From                Message
  ----     ------             ----                ----                -------
  Normal   NotTriggerScaleUp  11m (x174 over 1h)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added)
  Warning  FailedScheduling   1m (x5490 over 1h)  default-scheduler   0/9 nodes are available: 2 node(s) didn't match node selector, 8 Insufficient cpu.
```

I've got 2 pods in this state, it was ok, because it appears in deployment update, but looks really strange. I decided just to remove resource requests for non critical pods.",closed,False,2018-12-03 11:40:01,2018-12-04 13:26:07
autoscaler,lvjing2,https://github.com/kubernetes/autoscaler/issues/1471,https://api.github.com/repos/kubernetes/autoscaler/issues/1471,Too much unneeded Scale up and scale down in CA,"Hi all, is cluster autoscaling safe to be used for production?  when I through the cluster-autoscaler repo, I found this maybe a problem for production. The case is that 
1. when we set the min/max range of resource utilization to 60%-80% for a NodeGroup which contains 3 node with each usage 80%, 80%, 40%. 
2. Then the third one is blow the min, so it will be delete, and all the pod will be migrate to the other two nodes. So the utilization of other node will reached  100% which is bigger than the max. 
3. Then the CA will start to scale up a new node. 

In this case, there need to one scale up and one scale down. But from my point, it is better if there should is no scale up and no scale down.",open,False,2018-12-03 15:03:58,2019-01-22 10:55:44
autoscaler,JoeWrightss,https://github.com/kubernetes/autoscaler/pull/1472,https://api.github.com/repos/kubernetes/autoscaler/issues/1472,"Typo fixed: ""Merics"" -> ""Metrics""",Fixs typo in line 76.,closed,True,2018-12-03 17:17:06,2018-12-03 18:32:48
autoscaler,errm,https://github.com/kubernetes/autoscaler/pull/1473,https://api.github.com/repos/kubernetes/autoscaler/issues/1473,cluster autoscaler: Support scaling AWS groups from 0 when using MixedInstancesPolicy,"AWS recently added the MixedInstancesPolicy as an alternative to LaunchTemplate
or LaunchConfigruration when specifying the config for the instances in an autoscaling group.

https://aws.amazon.com/blogs/aws/new-ec2-auto-scaling-groups-with-multiple-instance-types-purchase-options/

The MixedInstancesPolicy holds a reference to a LaunchTemplate... plus an number of overides
that can overide the instance type specified in the LaunchTemplate.  Thus when using this
type an autoscaling group can schedule multiple instance types.

The MixedInstancesPolicy also allows some percentage of instances in the autoscaling group to be
launched as spot instances, this is kindof othoganal to this PR, but the main reason I wan't to be
able to configure ASGs in this way + use cluster autoscaler.

MixedInstancesPolicy breaks Cluster Autoscaler a bit since there is no longer an LaunchTemplate / Configuration
to discover the instance type from, when the ASG was scaled to 0.

This change makes it so that the cluster autoscaler will work again ...

I have made a few (hopefully reasonable) assumptions:

Firstly where there are instance types specified in multiple overidees, I presume that they will all
have an equivelent amount of resources.

e.g.If I creating an ASGs that has m5.large, m4.large and m3.large overides .... and AWS is choosing
the instance type with the best spot price ... but these three instance types have (roughly) the same
level of resources... so the autoscaler can still work properly... even though we don't know exactly
which instance type will be chosen. The code change just take the instance type from the first override.",closed,True,2018-12-03 17:19:07,2019-02-07 12:43:08
autoscaler,tghartland,https://github.com/kubernetes/autoscaler/pull/1474,https://api.github.com/repos/kubernetes/autoscaler/issues/1474,Fix logged error in static autoscaler,,closed,True,2018-12-04 16:05:11,2018-12-04 16:18:48
autoscaler,hertzknight,https://github.com/kubernetes/autoscaler/issues/1475,https://api.github.com/repos/kubernetes/autoscaler/issues/1475,VPA - Installer not working,"I am trying out Vertical Pod Autoscaler and haven't had any luck with deploying it. I am using a new Cluster on Google Cloud with the following kubernetes versions:

```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:17:39Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11+"", GitVersion:""v1.11.2-gke.18"", GitCommit:""5796233393d7bc034428de15191ad3d2eaff95fb"", GitTreeState:""clean"", BuildDate:""2018-11-08T20:49:08Z"", GoVersion:""go1.10.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

However, I am seeing these as an errors and it looks like the vpa isn't working correctly as well:

```
customresourcedefinition.apiextensions.k8s.io ""verticalpodautoscalers.autoscaling.k8s.io"" created
customresourcedefinition.apiextensions.k8s.io ""verticalpodautoscalercheckpoints.autoscaling.k8s.io"" created
clusterrolebinding.rbac.authorization.k8s.io ""system:metrics-reader"" created
clusterrolebinding.rbac.authorization.k8s.io ""system:vpa-actor"" created
clusterrolebinding.rbac.authorization.k8s.io ""system:vpa-checkpoint-actor"" created
clusterrolebinding.rbac.authorization.k8s.io ""system:vpa-updater-controllers-reader-binding"" created
clusterrolebinding.rbac.authorization.k8s.io ""system:vpa-evictionter-binding"" created
serviceaccount ""vpa-admission-controller"" created
clusterrolebinding.rbac.authorization.k8s.io ""system:admission-controller"" created
Error from server (Forbidden): error when creating ""STDIN"": clusterroles.rbac.authorization.k8s.io ""system:metrics-reader"" is forbidden: attempt to grant extra privileges: [{[get] [metrics.k8s.io] [pods] [] []} {[list] [metrics.k8s.io] [pods] [] []}] user=&{118045764671172196470  [system:authenticated] map[user-assertion.cloud.google.com:[AK5xou/rOIx33lzgER/ZrKY/aW5p9nv/qrxHG/JuYiwdrC3wtLbopLBN3Orqg7z+X+2fM6E2EvdLCBnLBTaF5nSfUPavY20lr4h9zXVMvmt38e0T1UX1zTxgQS+1/69TMDR0xYIIjV1uxVw6riiEQ++04EiiOANgc+uE9R4TGlz82nuaUNs/FVKhz3Ov7AnY6aFlBc4CJrHitMfjCx5xA06x0Rwd2DG0bTvQOCyd5Q==]]} ownerrules=[{[create] [authorization.k8s.io] [selfsubjectaccessreviews selfsubjectrulesreviews] [] []} {[get] [] [] [] [/api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/]}] ruleResolutionErrors=[]
Error from server (Forbidden): error when creating ""STDIN"": clusterroles.rbac.authorization.k8s.io ""system:vpa-actor"" is forbidden: attempt to grant extra privileges: [{[get] [] [pods] [] []} {[list] [] [pods] [] []} {[watch] [] [pods] [] []} {[get] [] [nodes] [] []} {[list] [] [nodes] [] []} {[watch] [] [nodes] [] []} {[get] [] [events] [] []} {[list] [] [events] [] []} {[watch] [] [events] [] []} {[create] [] [events] [] []} {[get] [poc.autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[list] [poc.autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[watch] [poc.autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[patch] [poc.autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[get] [autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[list] [autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[watch] [autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[patch] [autoscaling.k8s.io] [verticalpodautoscalers] [] []}] user=&{118045764671172196470  [system:authenticated] map[user-assertion.cloud.google.com:[AK5xou/rOIx33lzgER/ZrKY/aW5p9nv/qrxHG/JuYiwdrC3wtLbopLBN3Orqg7z+X+2fM6E2EvdLCBnLBTaF5nSfUPavY20lr4h9zXVMvmt38e0T1UX1zTxgQS+1/69TMDR0xYIIjV1uxVw6riiEQ++04EiiOANgc+uE9R4TGlz82nuaUNs/FVKhz3Ov7AnY6aFlBc4CJrHitMfjCx5xA06x0Rwd2DG0bTvQOCyd5Q==]]} ownerrules=[{[create] [authorization.k8s.io] [selfsubjectaccessreviews selfsubjectrulesreviews] [] []} {[get] [] [] [] [/api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/]}] ruleResolutionErrors=[]
Error from server (Forbidden): error when creating ""STDIN"": clusterroles.rbac.authorization.k8s.io ""system:vpa-checkpoint-actor"" is forbidden: attempt to grant extra privileges: [{[get] [poc.autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[list] [poc.autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[watch] [poc.autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[create] [poc.autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[patch] [poc.autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[delete] [poc.autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[get] [autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[list] [autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[watch] [autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[create] [autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[patch] [autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[delete] [autoscaling.k8s.io] [verticalpodautoscalercheckpoints] [] []} {[get] [] [namespaces] [] []} {[list] [] [namespaces] [] []}] user=&{118045764671172196470  [system:authenticated] map[user-assertion.cloud.google.com:[AK5xou/rOIx33lzgER/ZrKY/aW5p9nv/qrxHG/JuYiwdrC3wtLbopLBN3Orqg7z+X+2fM6E2EvdLCBnLBTaF5nSfUPavY20lr4h9zXVMvmt38e0T1UX1zTxgQS+1/69TMDR0xYIIjV1uxVw6riiEQ++04EiiOANgc+uE9R4TGlz82nuaUNs/FVKhz3Ov7AnY6aFlBc4CJrHitMfjCx5xA06x0Rwd2DG0bTvQOCyd5Q==]]} ownerrules=[{[create] [authorization.k8s.io] [selfsubjectaccessreviews selfsubjectrulesreviews] [] []} {[get] [] [] [] [/api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/]}] ruleResolutionErrors=[]
Error from server (Forbidden): error when creating ""STDIN"": clusterroles.rbac.authorization.k8s.io ""system:evictioner"" is forbidden: attempt to grant extra privileges: [{[get] [extensions] [replicasets] [] []} {[create] [] [pods/eviction] [] []}] user=&{118045764671172196470  [system:authenticated] map[user-assertion.cloud.google.com:[AK5xou/rOIx33lzgER/ZrKY/aW5p9nv/qrxHG/JuYiwdrC3wtLbopLBN3Orqg7z+X+2fM6E2EvdLCBnLBTaF5nSfUPavY20lr4h9zXVMvmt38e0T1UX1zTxgQS+1/69TMDR0xYIIjV1uxVw6riiEQ++04EiiOANgc+uE9R4TGlz82nuaUNs/FVKhz3Ov7AnY6aFlBc4CJrHitMfjCx5xA06x0Rwd2DG0bTvQOCyd5Q==]]} ownerrules=[{[create] [authorization.k8s.io] [selfsubjectaccessreviews selfsubjectrulesreviews] [] []} {[get] [] [] [] [/api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/]}] ruleResolutionErrors=[]
Error from server (Forbidden): error when creating ""STDIN"": clusterroles.rbac.authorization.k8s.io ""system:controllers-reader"" is forbidden: attempt to grant extra privileges: [{[get] [] [replicationcontrollers] [] []} {[list] [] [replicationcontrollers] [] []} {[watch] [] [replicationcontrollers] [] []} {[get] [apps] [statefulsets] [] []} {[list] [apps] [statefulsets] [] []} {[watch] [apps] [statefulsets] [] []} {[get] [apps] [replicasets] [] []} {[list] [apps] [replicasets] [] []} {[watch] [apps] [replicasets] [] []}] user=&{118045764671172196470  [system:authenticated] map[user-assertion.cloud.google.com:[AK5xou/rOIx33lzgER/ZrKY/aW5p9nv/qrxHG/JuYiwdrC3wtLbopLBN3Orqg7z+X+2fM6E2EvdLCBnLBTaF5nSfUPavY20lr4h9zXVMvmt38e0T1UX1zTxgQS+1/69TMDR0xYIIjV1uxVw6riiEQ++04EiiOANgc+uE9R4TGlz82nuaUNs/FVKhz3Ov7AnY6aFlBc4CJrHitMfjCx5xA06x0Rwd2DG0bTvQOCyd5Q==]]} ownerrules=[{[create] [authorization.k8s.io] [selfsubjectaccessreviews selfsubjectrulesreviews] [] []} {[get] [] [] [] [/api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/]}] ruleResolutionErrors=[]
Error from server (Forbidden): error when creating ""STDIN"": clusterroles.rbac.authorization.k8s.io ""system:admission-controller"" is forbidden: attempt to grant extra privileges: [{[get] [] [pods] [] []} {[list] [] [pods] [] []} {[watch] [] [pods] [] []} {[get] [] [configmaps] [] []} {[list] [] [configmaps] [] []} {[watch] [] [configmaps] [] []} {[get] [] [nodes] [] []} {[list] [] [nodes] [] []} {[watch] [] [nodes] [] []} {[create] [admissionregistration.k8s.io] [mutatingwebhookconfigurations] [] []} {[delete] [admissionregistration.k8s.io] [mutatingwebhookconfigurations] [] []} {[get] [admissionregistration.k8s.io] [mutatingwebhookconfigurations] [] []} {[list] [admissionregistration.k8s.io] [mutatingwebhookconfigurations] [] []} {[get] [poc.autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[list] [poc.autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[watch] [poc.autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[get] [autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[list] [autoscaling.k8s.io] [verticalpodautoscalers] [] []} {[watch] [autoscaling.k8s.io] [verticalpodautoscalers] [] []}] user=&{118045764671172196470  [system:authenticated] map[user-assertion.cloud.google.com:[AK5xou/rOIx33lzgER/ZrKY/aW5p9nv/qrxHG/JuYiwdrC3wtLbopLBN3Orqg7z+X+2fM6E2EvdLCBnLBTaF5nSfUPavY20lr4h9zXVMvmt38e0T1UX1zTxgQS+1/69TMDR0xYIIjV1uxVw6riiEQ++04EiiOANgc+uE9R4TGlz82nuaUNs/FVKhz3Ov7AnY6aFlBc4CJrHitMfjCx5xA06x0Rwd2DG0bTvQOCyd5Q==]]} ownerrules=[{[create] [authorization.k8s.io] [selfsubjectaccessreviews selfsubjectrulesreviews] [] []} {[get] [] [] [] [/api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/]}] ruleResolutionErrors=[]
serviceaccount ""vpa-updater"" created
deployment.extensions ""vpa-updater"" created
serviceaccount ""vpa-recommender"" created
deployment.extensions ""vpa-recommender"" created
Generating certs for the VPA Admission Controller in /tmp/vpa-certs.
Generating RSA private key, 2048 bit long modulus
```

I also noticed these log errors on the vpa pod:

```
E1204 16:30:17.361649      10 reflector.go:134] k8s.io/autoscaler/vertical-pod-autoscaler/pkg/utils/vpa/api.go:89: Failed to list *v1beta1.VerticalPodAutoscaler: verticalpodautoscalers.autoscaling.k8s.io is forbidden: User ""system:serviceaccount:kube-system:vpa-admission-controller"" cannot list verticalpodautoscalers.autoscaling.k8s.io at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io ""system:admission-controller"" not found
I1204 16:30:18.362369      10 reflector.go:169] Listing and watching *v1beta1.VerticalPodAutoscaler from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/utils/vpa/api.go:89
```
",closed,False,2018-12-04 16:39:23,2018-12-04 18:02:35
autoscaler,jeruane,https://github.com/kubernetes/autoscaler/pull/1476,https://api.github.com/repos/kubernetes/autoscaler/issues/1476,ec2_instance_types CA cherry picks to 1.12,,closed,True,2018-12-04 22:26:26,2018-12-05 15:30:34
autoscaler,jeruane,https://github.com/kubernetes/autoscaler/pull/1477,https://api.github.com/repos/kubernetes/autoscaler/issues/1477,ec2_instance_types CA cherry picks to 1.3,,closed,True,2018-12-04 22:29:32,2018-12-05 15:30:39
autoscaler,jeruane,https://github.com/kubernetes/autoscaler/pull/1478,https://api.github.com/repos/kubernetes/autoscaler/issues/1478,ec2_instance_types CA cherry picks to 1.2,,closed,True,2018-12-04 22:31:25,2018-12-06 20:14:25
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1479,https://api.github.com/repos/kubernetes/autoscaler/issues/1479,Update base debian image for Cluster Autoscaler,,closed,True,2018-12-05 15:21:06,2018-12-05 15:34:09
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1480,https://api.github.com/repos/kubernetes/autoscaler/issues/1480,Update base debian image for Cluster Autoscaler,,closed,True,2018-12-05 15:34:49,2018-12-05 15:49:13
autoscaler,multi-io,https://github.com/kubernetes/autoscaler/pull/1481,https://api.github.com/repos/kubernetes/autoscaler/issues/1481,cloud provider for cluster-api,"This is an initial implementation of a new cloud provider (named ""clusterapi"") that uses cluster-api (https://github.com/kubernetes-sigs/cluster-api/) for creating and managing nodes via the K8s API itself rather than any particular cloud API. We've tested this in clusters with the Kubermatic machine-controller (https://github.com/kubermatic/machine-controller). The way it works it that cluster-api defines CRDs for ""Machines"" -- which represent machines in the backing cloud (which become nodes after successful initialization; see below), and MachineSets and MachineDeployments, which are to machines what ReplicaSets and Deployments are to pods: A MachineSet or MachineDeployment declares a replica count, and a corresponding controller (part of cluster-api) will create and (in the case of MachineDeployments) rolling-upgrade (if needed) a corresponding set of machine objects. Machine-controller, which has its own cloud provider layer, will then create and provision Kubernetes machines in a cloud environment, which will boot up, register with the API server and eventually show up as nodes.

What this autoscaler cloud provider does is expose all the MachineDeployments as node groups, and simply manipulate the replica count when asked to scale a node group.

The cloud provider works nicely in our setup (we're running on an OpenStack cloud). It's not feature-complete yet though; there are a couple of caveats:

- we've vendored a slightly older version of cluster-api because the current master contains a breaking change (https://github.com/kubernetes-sigs/cluster-api/pull/548) that machine-controller's version of cluster-api doesn't yet have

- downscaling doesn't work currently -- https://github.com/kubernetes-sigs/cluster-api/pull/558 is needed for this and wasn't merged until very recently -- we'd implement this next, wanted to get some feedback on the PR first

- ClusterapiNodeGroup.TemplateNodeInfo(), which is used to estimate new nodes' resources when scaling up empty MachineDeployments, is entirely Openstack-specific right now (because that's what we use) and won't work anywhere else. It extracts the information from the MachineDeployment's providerConfig, which contains the cloud-specific configuration to use for new machines in the deployment. We're not sure how to implement this for other clouds except manually.

- we might want to support scaling of MachineSets, not just MachineDeployments

The PR contains two required changes outside the new cloud provider itself, currently provided as separate commits in the PR to make  the review easier:

- since this cloud provider's cloud-config is just the cluster's kubeconfig, we had to change `AutoscalerOptions` and the cloud provider builder so we pass the kubeconfig down to `buildCloudProvider` (https://github.com/kubernetes/autoscaler/commit/fdb83cde659f011c1d6f1dd84ef6e3050899a402). This change was previously provided as a separate PR (https://github.com/kubernetes/autoscaler/pull/1413).

- cluster-api currently still uses glog, while autoscaler uses klog. In the current upstream, this causes a runtime panic (""redefined: log_dir"") at https://github.com/kubernetes/autoscaler/blob/d53928a11d422256ee452a8aa5e397b9e4823216/cluster-autoscaler/main.go#L324 because `glog.init()` adds a log_dir flag to the global `flag.CommandLine`, and then `klog.InitFlags(nil)` does the same, hence the error. https://github.com/kubernetes/autoscaler/commit/f4f7986862f953e9aa0d4497189dc5175186fec6 fixes this by registering klog's flags into a local flagset, then merging that and `flag.CommandLine` into `pflag.CommandLine`, which is then parsed. The fix will be needed not just for cluster-api but also with any other dependency that uses glog, so this should be of general interest.
",open,True,2018-12-05 16:54:29,2019-02-14 19:11:53
autoscaler,frobware,https://github.com/kubernetes/autoscaler/pull/1482,https://api.github.com/repos/kubernetes/autoscaler/issues/1482,handle nil nodeGroup in calculateScaleDownGpusTotal,"Explicitly handle nil as a return value for nodeGroup in
`calculateScaleDownGpusTotal()` when `NodeGroupForNode()` is called
for GPU nodes that don't exist. The current logic generates a runtime
exception:

    ""reflect: call of reflect.Value.IsNil on zero Value""

Looking through the rest of the tree all the other places that use
this pattern additionally and explicitly check whether `nodeGroup ==
nil` first.

This change now completes the pattern in
`calculateScaleDownGpusTotal()`.

Looking at the other occurrences of this pattern we see:

```
File: clusterstate/clusterstate.go
488:26:		if nodeGroup == nil || reflect.ValueOf(nodeGroup).IsNil() {

File: core/utils.go
231:26:		if nodeGroup == nil || reflect.ValueOf(nodeGroup).IsNil() {
322:26:		if nodeGroup == nil || reflect.ValueOf(nodeGroup).IsNil() {
394:27:			if nodeGroup == nil || reflect.ValueOf(nodeGroup).IsNil() {
461:26:		if nodeGroup == nil || reflect.ValueOf(nodeGroup).IsNil() {

File: core/scale_down.go
185:6:		if reflect.ValueOf(nodeGroup).IsNil() {
608:27:			if nodeGroup == nil || reflect.ValueOf(nodeGroup).IsNil() {
747:26:		if nodeGroup == nil || reflect.ValueOf(nodeGroup).IsNil() {
1010:25:	if nodeGroup == nil || reflect.ValueOf(nodeGroup).IsNil() {
```

with the notable exception at core/scale_down.go:185 which is
`calculateScaleDownGpusTotal()`.

With this change, and invoking the autoscaler with:

```
...
      --max-nodes-total=24 \
      --cores-total=8:128 \
      --memory-total=4:256 \
      --gpu-total=nvidia.com/gpu:0:16 \
      --gpu-total=amd.com/gpu:0:4 \
...
```

I no longer see a runtime exception.",closed,True,2018-12-05 18:57:55,2018-12-06 12:22:31
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1483,https://api.github.com/repos/kubernetes/autoscaler/issues/1483,Return stockouts/quota exceeded information from GCE cloud provider,,closed,True,2018-12-06 19:28:44,2018-12-07 15:45:47
autoscaler,gaozhenhai,https://github.com/kubernetes/autoscaler/pull/1484,https://api.github.com/repos/kubernetes/autoscaler/issues/1484,update log output detailed warning info,Signed-off-by: Zhenhai Gao <gaozh1988@live.com>,closed,True,2018-12-07 02:52:52,2018-12-21 14:03:15
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1485,https://api.github.com/repos/kubernetes/autoscaler/issues/1485,Fix aws flaking Unit Tests,"Looks like  this Unit Test blocks several tasks. I notice someone comment this test and I think we can use `MatchedBy` to customize comparison logic.

This will resolve #1346 and other dependent issues for 1.2

~~I run hack/verify-gofmt.sh and found several files are not formatted and then run `gofmt -s -w ${filename}` Not sure if that's needed. if not, I will remove second commit~~
",closed,True,2018-12-07 07:26:31,2018-12-08 18:50:44
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1486,https://api.github.com/repos/kubernetes/autoscaler/issues/1486,Update Cluster Autoscaler version to 1.13.1,,closed,True,2018-12-07 10:38:53,2018-12-07 10:54:43
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1487,https://api.github.com/repos/kubernetes/autoscaler/issues/1487,Update Debian base image version to 0.4.0 in Makefile,,closed,True,2018-12-07 11:31:27,2018-12-07 11:50:17
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1488,https://api.github.com/repos/kubernetes/autoscaler/issues/1488,Keep one place where default base image for Cluster Austoscaler is defined,,closed,True,2018-12-07 11:45:55,2018-12-07 11:58:01
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1489,https://api.github.com/repos/kubernetes/autoscaler/issues/1489,Keep one place where default base image for Cluster Austoscaler is defined,,closed,True,2018-12-07 11:50:09,2018-12-07 11:54:37
autoscaler,shatil,https://github.com/kubernetes/autoscaler/pull/1490,https://api.github.com/repos/kubernetes/autoscaler/issues/1490,Set AWS_REGION so tests don't wait for SDK timeout #1449,"This patch sets and then resets the `AWS_REGION` environment variable in
tests that call `createAWSManagerInternal`, which calls `getRegion`.

`getRegion` introduced `AWS_REGION` lookup at runtime, but tests that
don't (a) run inside AWS or (b) set `AWS_REGION` environment variable
encountered AWS SDK's EC2 Metadata lookup timeout. Tests that indirectly
invoked `getRegion`, e.g., via `createAWSManagerInternal`, saw at least
a 5-second timeout per invocation, ballooning test times.",closed,True,2018-12-07 17:10:48,2019-01-11 14:51:33
autoscaler,JoeWrightss,https://github.com/kubernetes/autoscaler/pull/1491,https://api.github.com/repos/kubernetes/autoscaler/issues/1491,Fix some spelling errors,"Fixs some small typos in [balance_similar.md](https://github.com/kubernetes/autoscaler/compare/master...JoeWrightss:patch-5#diff-e17723f54b49a53ce2943f71ef4ea0d1).
1. ""labalSelector"" to ""labelSelector"".
2. ""antiaffinity"" to ""anti-affinity"".",closed,True,2018-12-07 17:13:24,2018-12-08 17:12:14
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1492,https://api.github.com/repos/kubernetes/autoscaler/issues/1492,Cherry-pick of #1485 to 1.12 : Fix aws flaking Unit Tests,,closed,True,2018-12-08 21:04:32,2018-12-10 11:55:13
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1493,https://api.github.com/repos/kubernetes/autoscaler/issues/1493,Cherry-pick of #1485 to 1.13 : Fix aws flaking Unit Tests,,closed,True,2018-12-08 21:04:57,2018-12-10 11:56:47
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1494,https://api.github.com/repos/kubernetes/autoscaler/issues/1494,Cherry-pick of #1485 to 1.3 : Fix aws flaking Unit Tests,,closed,True,2018-12-08 21:05:24,2018-12-10 23:53:07
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1495,https://api.github.com/repos/kubernetes/autoscaler/issues/1495,Cherry-pick of #1485 to master : Fix aws flaking Unit Tests,,closed,True,2018-12-08 21:05:49,2018-12-10 23:53:11
autoscaler,lvjing2,https://github.com/kubernetes/autoscaler/issues/1496,https://api.github.com/repos/kubernetes/autoscaler/issues/1496,Consider add a switcher to skip calculating the utilization of slot low priority pause pods which be used to support overprovisioning? ,"If i want to scale down the node which it's utilization is below 0.1. There are only few user pods which resource usage is blow 0.1, so in fact this node should be considered to scale down. But if there are many other slot pod that make the node's utilization is above 0.1, then the node will not be consider to be scaled down. I think this is not reasonable, so I am thinking how about add a switcher here to skip calculating the utilization of slot low priority pause pods which be used to support overprovisioning? 
https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/simulator/cluster.go#L178",open,False,2018-12-09 05:26:52,2019-03-13 13:30:37
autoscaler,mhvijay,https://github.com/kubernetes/autoscaler/issues/1497,https://api.github.com/repos/kubernetes/autoscaler/issues/1497,"Auatoscaler killing pods launched by deployment/Jobs even though ""cluster-autoscaler.kubernetes.io/safe-to-evict"": ""true"" is given","Auatoscaler killing pods launched by deployment/Jobs even though ""cluster-autoscaler.kubernetes.io/safe-to-evict"": ""true"" is given
Below is the yaml used.
---
apiVersion: batch/v1
kind: Job
metadata:
  name: autoscaling-test
  namespace: test
  annotations:
    cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
  labels:
    cluster_name: test
spec:
  backoffLimit: 0
  template:
    spec:
      serviceAccountName: test
      containers:
      - name: autoscaling-test
        image: <IMAGE_NAME>
        command:
        - sh
        - ""-c""
        - tail -f /dev/null
      restartPolicy: Never
",open,False,2018-12-10 21:56:50,2019-03-11 21:51:36
autoscaler,CodeLingoBot,https://github.com/kubernetes/autoscaler/pull/1498,https://api.github.com/repos/kubernetes/autoscaler/issues/1498,Fix error format strings according to best practices from CodeReviewComments,"Use [CodeLingo](https://codelingo.io) to automatically fix error format strings following the
[Go Code Review Comments guidelines](https://github.com/golang/go/wiki/CodeReviewComments) in [CONTRIBUTING.md](https://github.com/kubernetes/autoscaler/blob/master/CONTRIBUTING.md).

This patch was generated by running the CodeLingo Rewrite Flow over the ""[go-error-fmt](https://github.com/codelingo/codelingo/blob/master/tenets/codelingo/code-review-comments/go-error-fmt/codelingo.yaml)"" Tenet. Note: the same Tenet can be used to automate PR reviews and generate contributor docs by [Installing](https://github.com/apps/codelingo) the CodeLingo GitHub app. Learn more at [codelingo.io](https://codelingo.io).",closed,True,2018-12-11 02:40:03,2019-01-06 21:21:01
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1499,https://api.github.com/repos/kubernetes/autoscaler/issues/1499,Add cache for resource IDs of vmss instances,"This PR adds caches for resource IDs of vmss instances. Those IDs won't change after creation, so it's safe for CA to keep them in memory.

Without this, cluster-autoscaler would be Throttled by Azure API: 

```json
""details"": [
    {
      ""code"": ""TooManyRequests"",
      ""message"": ""{\""operationGroup\"":\""GetVMScaleSetVM30Min\"",\""startTime\"":\""2018-11-29T20:49:14.3565951+00:00\"",\""endTime\"":\""2018-11-29T21:04:14.3565951+00:00\"",\""allowedRequestCount\"":2500,\""measuredRequestCount\"":2596}"",
      ""target"": ""GetVMScaleSetVM30Min""
    }
  ],
  ""innererror"": {
    ""internalErrorCode"": ""TooManyRequestsReceived""
  },
  ""code"": ""OperationNotAllowed"",
  ""message"": ""The server rejected the request because too many requests have been received for this subscription.""
}
```

cc @kkmsft ",closed,True,2018-12-11 04:36:01,2018-12-17 14:54:07
autoscaler,sylr,https://github.com/kubernetes/autoscaler/issues/1500,https://api.github.com/repos/kubernetes/autoscaler/issues/1500,Azure VMSS + Availability Zones = NoVolumeZoneConflict,"Hi,

Here is my setup:

 - 3 masters, 1 in each availability zone of westeurope
 - 3 agent pools (1 VMSS), 1 by availability zone of westeurope

With this setup I wanted to workaround the fact that there is no Azure API allowing to spawn an instance in a VMSS in a particular availability zone in order to access a PV which has been created in that availability zone.

So instead of having 1  VMSS in several zones I create 3, 1 in each zone so that the cluster-autoscaler could be able to spawn instances in the desired zone.

Unfortunately it does not seem to work.

```
I1211 10:31:49.359147       1 utils.go:514] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I1211 10:31:49.359308       1 scale_up.go:263] Pod default/prometheus-alertmanager-d9dcffbf8-sspb9 is unschedulable
I1211 10:31:49.359315       1 scale_up.go:263] Pod default/prometheus-server-dd5759489-8smwv is unschedulable
I1211 10:31:49.438834       1 utils.go:208] Pod prometheus-alertmanager-d9dcffbf8-sspb9 can't be scheduled on k8s1-euw-sandbox-agentzone1-vmss, predicate failed: NoVolumeZoneConflict predicate mismatch, reason: node(s) had no available volume zone
I1211 10:31:49.438893       1 utils.go:208] Pod prometheus-server-dd5759489-8smwv can't be scheduled on k8s1-euw-sandbox-agentzone1-vmss, predicate failed: NoVolumeZoneConflict predicate mismatch, reason: node(s) had no available volume zone
I1211 10:31:49.438914       1 utils.go:208] Pod prometheus-alertmanager-d9dcffbf8-sspb9 can't be scheduled on k8s1-euw-sandbox-agentzone2-vmss, predicate failed: NoVolumeZoneConflict predicate mismatch, reason: node(s) had no available volume zone
I1211 10:31:49.438925       1 utils.go:208] Pod prometheus-server-dd5759489-8smwv can't be scheduled on k8s1-euw-sandbox-agentzone2-vmss, predicate failed: NoVolumeZoneConflict predicate mismatch, reason: node(s) had no available volume zone
I1211 10:31:49.438948       1 utils.go:208] Pod prometheus-alertmanager-d9dcffbf8-sspb9 can't be scheduled on k8s1-euw-sandbox-agentzone3-vmss, predicate failed: NoVolumeZoneConflict predicate mismatch, reason: node(s) had no available volume zone
I1211 10:31:49.438959       1 utils.go:208] Pod prometheus-server-dd5759489-8smwv can't be scheduled on k8s1-euw-sandbox-agentzone3-vmss, predicate failed: NoVolumeZoneConflict predicate mismatch, reason: node(s) had no available volume zone
I1211 10:31:49.438976       1 scale_up.go:416] No expansion options
```

Cheers.",open,False,2018-12-11 10:42:29,2019-03-11 12:43:14
autoscaler,patrickshan,https://github.com/kubernetes/autoscaler/issues/1501,https://api.github.com/repos/kubernetes/autoscaler/issues/1501,vertical-pod-autoscaler - the way vpa recommender uses prometheus history cpu data,"when using prometheus as history data provider, how does VPA recommender calculate CPU recoomendation ? 

It seems currently it uses `container_cpu_usage_seconds_total` which is a counter but it doesn't seem to do any rate calculation based on it ? is this correct ? while for memory it uses a gauge metric `container_memory_usage_bytes` which makes sense",open,False,2018-12-11 10:58:48,2019-03-22 08:41:12
autoscaler,sylr,https://github.com/kubernetes/autoscaler/pull/1502,https://api.github.com/repos/kubernetes/autoscaler/issues/1502,Azure: Generate meaningful failure-domain.beta.kubernetes.io/zone,Maybe this could help with https://github.com/kubernetes/autoscaler/issues/1500,closed,True,2018-12-11 11:49:39,2018-12-20 02:08:51
autoscaler,george-miller,https://github.com/kubernetes/autoscaler/issues/1503,https://api.github.com/repos/kubernetes/autoscaler/issues/1503,[cluster-autoscaler] Scale up based on Node Utilization,"Hello, firstly I want to thank you for making this awesome autoscaler, it works so well and makes Kubernetes an even better platform (even though it was already the best).

The problem I am having is that it takes too long to provision new resources because CA waits until there is an unschedulable pod to trigger a scale up.  If CA scaled the cluster based on node resource utilization, it could preemptively scale the cluster and no pod would be left Pending in an unschedulable state unless a huge mass of pods arrived at the same time.  

This paradigm would make sense in a cluster like mine, where every pod must have resource requests and limits, because node utilization percent would be accurate.  In this way it would work a lot like a [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details), where the current metric value would be node utilization and the desired metric value would be a utilization percent that the admin sets, like 80%.  The equation would look like:
```
desiredNodes = ceil[currentNodes * ( currentUtilization / desiredUtilization )]
```
Maybe CA could implement both strategies, where it scales up either when a pod is unschedulable and a new node would help or when utilization is not at the desired percent.  

What do you think?  Is it a crazy idea? ",open,False,2018-12-11 20:54:41,2019-02-14 09:27:23
autoscaler,koooge,https://github.com/kubernetes/autoscaler/pull/1504,https://api.github.com/repos/kubernetes/autoscaler/issues/1504,[doc] Add 1.13,,closed,True,2018-12-12 04:39:54,2018-12-13 12:58:53
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1505,https://api.github.com/repos/kubernetes/autoscaler/issues/1505,Add a tool to convert alpha VPA objects to beta API,"Based on bskiba:to-beta-docs so @bskiba , please have a look.",closed,True,2018-12-12 15:18:20,2018-12-13 20:45:03
autoscaler,thealmightygrant,https://github.com/kubernetes/autoscaler/pull/1506,https://api.github.com/repos/kubernetes/autoscaler/issues/1506,Remove namespace from hamster example.,"Very minor change, but in order to ensure that the vpa ends up in the same namespace as the deployment, the default namespace needs to be removed here. This fixes any issues with the current context have a default namespace that is not named ""default"".",closed,True,2018-12-12 16:31:37,2018-12-13 01:36:27
autoscaler,pkelleratwork,https://github.com/kubernetes/autoscaler/issues/1507,https://api.github.com/repos/kubernetes/autoscaler/issues/1507,AWS EKS - Failed to get nodes from apiserver: nodes is forbidden:,"followed directions from here - https://medium.com/@alejandro.millan.frias/cluster-autoscaler-in-amazon-eks-d9f787176519

getting error
```
F1212 21:30:29.408142       1 main.go:345] Failed to get nodes from apiserver: nodes is forbidden: User ""system:serviceaccount:default:default"" cannot list nodes at the cluster scope
```

is this the correct default path? 
`sslCertPath: /etc/kubernetes/pki/ca.crt`

What are the tags that need to be added? 


",closed,False,2018-12-12 21:40:25,2019-01-10 22:10:24
autoscaler,Deepak1100,https://github.com/kubernetes/autoscaler/issues/1508,https://api.github.com/repos/kubernetes/autoscaler/issues/1508,scaling down node with running bare pods and nodes with safe-to-evict false pods as well,"The whole Infra is running on AWS
I have tried setting annotation safe-to-evict = false and PDB as well. I have task schedule by airflow on Kube which is not controlled by aby Kube controller(airflow scheduler control them) and there nodes also getting scale down even though they are running there with above annotation and PDB as well and they are just bare pods but still. 
I can see in logs that CA is checking them and it is ignoring those nodes but when scale down happen some other nodes getting scale down.
and for me it is not even respecting --min-replica flag as well
I have launched the one ASG which spans instance in 2 AZs. it's in mumbai region.
scale up happen properly and its problem with scale down.
The termination policy of ASG is Default",closed,False,2018-12-13 08:06:31,2019-01-23 16:53:26
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1509,https://api.github.com/repos/kubernetes/autoscaler/issues/1509,Update docs as part of beta release.,,closed,True,2018-12-13 21:03:59,2018-12-17 12:13:48
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1510,https://api.github.com/repos/kubernetes/autoscaler/issues/1510,Add a tool to convert alpha VPA objects to beta API,,closed,True,2018-12-17 14:38:47,2018-12-17 14:52:35
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1511,https://api.github.com/repos/kubernetes/autoscaler/issues/1511,Update docs as part of beta release.,,closed,True,2018-12-17 15:20:58,2018-12-18 08:48:44
autoscaler,awly,https://github.com/kubernetes/autoscaler/issues/1512,https://api.github.com/repos/kubernetes/autoscaler/issues/1512,Rebuild cluster-autoscaler to pick up fixes for openssl CVEs,"CVE-2018-15686 and CVE-2018-0734 are fixed in the latest openssl version.
These images don't have the fix currently:
  - gcr.io/google-containers/cluster-autoscaler:v1.12.1
  - gcr.io/google-containers/cluster-autoscaler:v1.13.0
  - gcr.io/google-containers/cluster-autoscaler:v1.3.4

Please rebuild them or cut a new version.",open,False,2018-12-17 22:33:05,2019-03-18 09:23:48
autoscaler,hadr10,https://github.com/kubernetes/autoscaler/issues/1513,https://api.github.com/repos/kubernetes/autoscaler/issues/1513,Failed to decrease agentpool,"Hi,

we had got issue with autoscaler on AKS. 
AKS version: 1.11.3 (with RBAC)
Autoscaler version: 1.3.1

error message:
E1217 13:31:34.867548       1 static_autoscaler.go:190] Failed to fix node group sizes: Failed to decrease agentpool: attempt to delete existing nodes targetSize:3 delta:-1 existingNodes: 3 

After deleting pod on AKS, the new one autoscaler was created and scale up started working.

Thank you,
Honza",open,False,2018-12-18 08:56:53,2019-03-24 13:48:16
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1514,https://api.github.com/repos/kubernetes/autoscaler/issues/1514,Make WatchEvictionEventsWithRetries public.,,closed,True,2018-12-18 14:06:52,2018-12-18 14:21:08
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/issues/1515,https://api.github.com/repos/kubernetes/autoscaler/issues/1515,Incorrect handling of condition based taints if TaintNodesByCondition is used,"According to @aermakov-zalando in https://github.com/kubernetes/kubernetes/issues/72129
CA misbehaves if TainNodesByCondition is in use:

> The autoscaler itself doesn't treat these conditions in a special way, from what I understand by looking at the source code. This means that if the tolerations are not there, it'll spin up a new node, determine that the pods > can't run on it because of taints and happily create a new node instead of waiting for the node to be ready.

That needs to be verified and fixed if needed.",open,False,2018-12-18 16:00:34,2019-03-14 15:34:35
autoscaler,frobware,https://github.com/kubernetes/autoscaler/pull/1516,https://api.github.com/repos/kubernetes/autoscaler/issues/1516,fix calculation of max cluster size,"When scaling up, the calculation for the maximum size of the cluster
based on `--max-nodes-total` doesn't take into account any nodes that
are in the process of coming up. This allows the cluster to grow
beyond the size specified.

With this change I now see:

scale_up.go:266] 21 other pods are also unschedulable
scale_up.go:423] Best option to resize: openshift-cluster-api/amcdermo-ca-worker-us-east-2b
scale_up.go:427] Estimated 18 nodes needed in openshift-cluster-api/amcdermo-ca-worker-us-east-2b
scale_up.go:432] Capping size to max cluster total size (23)
static_autoscaler.go:275] Failed to scale up: max node total count already reached",closed,True,2018-12-18 17:06:14,2019-01-03 18:07:45
autoscaler,JoeWrightss,https://github.com/kubernetes/autoscaler/pull/1517,https://api.github.com/repos/kubernetes/autoscaler/issues/1517,Fix klog.Errorf() error message,Fix a small typo in [alicloud_auto_scaling.go](https://github.com/kubernetes/autoscaler/compare/master...JoeWrightss:patch-6?expand=1#diff-a325b69429b1a9ce1fcd7afa381803ed) at line 168.,closed,True,2018-12-19 01:56:55,2018-12-21 14:01:13
autoscaler,lsytj0413,https://github.com/kubernetes/autoscaler/pull/1518,https://api.github.com/repos/kubernetes/autoscaler/issues/1518,refactor(*): fix golint warning,fix a goling warning.,closed,True,2018-12-19 02:04:54,2018-12-22 03:19:53
autoscaler,aly-aurea,https://github.com/kubernetes/autoscaler/issues/1519,https://api.github.com/repos/kubernetes/autoscaler/issues/1519,Question: Is it safe to run CAS 1.2.3 with AWS ASG with multi-type instances?,"I am going to replace my AWS EKS production cluster workers ASG that uses a single instance type with the new ASG launch template that uses multiple types of EC2 instances.
I am already running a cluster in the test environment with this new setup, but I want to make sure that there are no concerns to run it before moving to production.

I use CAS v1.2.3 and EKS with k8s v1.10.3",closed,False,2018-12-20 01:15:13,2018-12-20 13:54:39
autoscaler,max-radin,https://github.com/kubernetes/autoscaler/issues/1520,https://api.github.com/repos/kubernetes/autoscaler/issues/1520,affinity for daemonset pod blocks scale-up,"I'd like to add a pod affinity rule that requires my pod to run on a node that already is running a fluentd logging pod. (The fluentd pods are launched by a daemonset.) My affinity rule looks like:
```yaml
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: k8s-app
            operator: In
            values:
            - fluentd-logging
        topologyKey: kubernetes.io/hostname
```
Unfortunately this prevents the autoscalerfrom scaling up – see log below. 
```
I1220 23:52:25.618948       1 scale_up.go:152] Scale-up predicate failed: MatchInterPodAffinity predicate mismatch, cannot put default/h2-zwq56-1824281309 on template-node-for-m4_2xlarge-7056460638582648182, reason: node(s) didn't match pod affinity/anti-affinity,node(s) didn't match pod affinity rules
```
It would be great if the autoscaler would scale-up in this situation; i.e. recognize that if a new node was created, then a daemonset would launch a pod satisfying the interpod affinity rules for a pod that currently has no node to run on.

One workaround would be to tell the autoscaler to ignore interpod affinity rules – is there an easy way to do this?",open,False,2018-12-21 00:16:35,2019-01-18 19:44:47
autoscaler,aerostitch,https://github.com/kubernetes/autoscaler/pull/1521,https://api.github.com/repos/kubernetes/autoscaler/issues/1521,Doc: scheduler.alpha.kubernetes.io/critical-pod annotations are deprecated. Use priorityClassName instead.,"Hi guys,

Thanks for this awesome work!
I noticed in the documentation the use of the `scheduler.alpha.kubernetes.io/critical-pod` annotation that has been deprecated as of version 1.13 and will be removed in 1.14.

So this PR changes the doc to use `priorityClassName: system-cluster-critical`.

Let me know  if I need to change some wording.

Thanks for your help,
Joseph",closed,True,2018-12-21 01:39:45,2019-01-03 18:37:24
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1522,https://api.github.com/repos/kubernetes/autoscaler/issues/1522,Cherry-pick of #1499 to 1.12: Add cache for resource IDs of vmss instances,/assign @losipiuk @kkmsft ,closed,True,2018-12-21 05:15:29,2018-12-23 14:42:09
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1523,https://api.github.com/repos/kubernetes/autoscaler/issues/1523,Cherry-pick of #1499 to 1.13: Add cache for resource IDs of vmss instances,"/assign @losipiuk 

cc @kkmsft 

",closed,True,2018-12-21 05:16:03,2018-12-23 14:42:04
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1524,https://api.github.com/repos/kubernetes/autoscaler/issues/1524,Cherry-pick of #1499 to 1.2: Add cache for resource IDs of vmss instances,"/assign @losipiuk

cc @kkmsft",closed,True,2018-12-21 05:17:13,2018-12-23 14:42:00
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1525,https://api.github.com/repos/kubernetes/autoscaler/issues/1525,Cherry-pick of #1499 to 1.3: Add cache for resource IDs of vmss instances,"/assign @losipiuk

cc @kkmsft",closed,True,2018-12-21 05:17:57,2018-12-23 14:42:33
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1526,https://api.github.com/repos/kubernetes/autoscaler/issues/1526,Use k8s.io/klog instead github.com/golang/glog,,closed,True,2018-12-21 08:55:19,2019-01-24 18:39:37
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1527,https://api.github.com/repos/kubernetes/autoscaler/issues/1527,cherry pick node balancing 1.12,,closed,True,2018-12-21 13:38:48,2018-12-21 13:53:16
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1528,https://api.github.com/repos/kubernetes/autoscaler/issues/1528,cherry pick node balancing 1.3,,closed,True,2018-12-21 13:38:56,2018-12-21 13:55:09
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1529,https://api.github.com/repos/kubernetes/autoscaler/issues/1529,Treat nodes from same GKE node pool similar,,closed,True,2018-12-21 13:39:10,2018-12-21 14:01:22
autoscaler,raffaelespazzoli,https://github.com/kubernetes/autoscaler/issues/1530,https://api.github.com/repos/kubernetes/autoscaler/issues/1530,VPA - increase flexibility on reading the certificates ,"I am trying to install the admission controller using a different method of providing certificates.
If I understand correctly the code right now expects certificates in the following locations:
```
/etc/tls-certs/caKey.pem
/etc/tls-certs/caCert.pem
/etc/tls-certs/serverKey.pem
/etc/tls-certs/serverCert.pem
```
It is possible to change the directory where the certs are, but not the name of the files.
Can you add a way to specify the name and location of each file independently?

Also something must be escaping me, but why is the caKey there? That is unusual and doesn't look right. Just to be sure, who is going to call this endpoint and how is it going to validate the certificate?
",closed,False,2018-12-22 20:07:48,2018-12-27 11:22:04
autoscaler,lsytj0413,https://github.com/kubernetes/autoscaler/pull/1531,https://api.github.com/repos/kubernetes/autoscaler/issues/1531,refactor(*): fix some golint warning,"fix some golint warning.

- add some export function/type comment
- replace some increment with ++
- omit some will be inferred type name",closed,True,2018-12-24 03:13:09,2018-12-24 10:08:45
autoscaler,zhaoar92,https://github.com/kubernetes/autoscaler/issues/1532,https://api.github.com/repos/kubernetes/autoscaler/issues/1532,Question:  what's the difference betwen Recorder and LogRecorder?  ,"I have a question about the code below, what's the difference betwen Recorder and LogRecorder?  

I can find the event log in configmap by using LogRecorder.Eventf().  But where's the event log by  Recorder.Eventf() ?  

```
type AutoscalingKubeClients struct {
	// Listers.
	kube_util.ListerRegistry
	// ClientSet interface.
	ClientSet kube_client.Interface
	// Recorder for recording events.
	Recorder kube_record.EventRecorder
	// LogRecorder can be used to collect log messages to expose via Events on some central object.
	LogRecorder *utils.LogEventRecorder
}
```",closed,False,2018-12-24 13:04:20,2018-12-25 02:02:57
autoscaler,petr-k,https://github.com/kubernetes/autoscaler/pull/1533,https://api.github.com/repos/kubernetes/autoscaler/issues/1533,VPA: fix README link to API definition,,closed,True,2018-12-25 13:52:31,2018-12-27 09:03:51
autoscaler,petr-k,https://github.com/kubernetes/autoscaler/pull/1534,https://api.github.com/repos/kubernetes/autoscaler/issues/1534,More flexible VPA certificate configuration,This resolves #1530 by adding separate flags for cert-related file paths.,closed,True,2018-12-25 15:42:51,2018-12-27 12:57:28
autoscaler,petr-k,https://github.com/kubernetes/autoscaler/issues/1535,https://api.github.com/repos/kubernetes/autoscaler/issues/1535,Run e2e tests during CI,"End-to-end tests do not seem to be run during CI. I think it would be beneficial to put up the necessary plumbing to allow tests to be run in Travis. Also, autoscaling components could be tested against multiple Kubernetes versions (if that makes sense?).

[kubeadm-dind-cluster](https://github.com/kubernetes-sigs/kubeadm-dind-cluster)  or [kind](https://github.com/kubernetes-sigs/kind) could be used to spin up the testing cluster.",open,False,2018-12-25 22:12:55,2019-02-22 16:39:27
autoscaler,hello2mao,https://github.com/kubernetes/autoscaler/pull/1536,https://api.github.com/repos/kubernetes/autoscaler/issues/1536,Support Baiducloud Cloud Provider for cluster-autoscaler,"**What this PR does / why we need it:**
BaiduCloud Container Engine is a high-performance and scalable container application management service using Kubernetes.
As the gold member of Kubernetes community, we create this PR to support Baiducloud Cloud Provider for cluster-autoscaler.",closed,True,2018-12-26 03:28:30,2019-01-11 00:32:58
autoscaler,tmwtp,https://github.com/kubernetes/autoscaler/issues/1537,https://api.github.com/repos/kubernetes/autoscaler/issues/1537,Skipping node group ... max size reached,"Environment details:

K8s version - 1.10.7
Installer - Kops
cluster auto-scaler image - gcr.io/google-containers/cluster-autoscaler
Image version - v1.2.3 

Issue:

On heavy load on the cluster there was a need for new nodes to be ran but i got the following error in the auto-scaler logs:

> I1227 10:48:22.745330       1 scale_up.go:61] 30 other pods are also unschedulable
I1227 10:48:22.915535       1 scale_up.go:92] Upcoming 0 nodes
I1227 10:48:23.118126       1 scale_up.go:116] Skipping node group nodes.XXXXXXXX - max size reached
I1227 10:48:23.118148       1 scale_up.go:186] No expansion options

At the same time there were 10 nodes running in the ASG (nodes.XXXXXXXX) and the maximum value was 50 (40 more nodes can be started in the ASG) yet per the log the auto-scaler says it has reached the maximum nodes in the ASG.

I had no issue going to the AWS console and increase the number of desired nodes but the auto scaler did not take care of that by itself.

Any help will be appreciated

Thank you

Roiy 

",closed,False,2018-12-27 14:27:26,2019-01-03 13:12:43
autoscaler,Shnatsel,https://github.com/kubernetes/autoscaler/pull/1538,https://api.github.com/repos/kubernetes/autoscaler/issues/1538,"Less technical, more verbose PDB reference in FAQ","Since it is easy to get to the part referencing PDB through a link from ""Troubleshooting"" without reading the entire document, reference PDB by full name and link to an explanation of what it is in ""What types of pods can prevent CA from removing a node?""",closed,True,2018-12-27 14:47:08,2018-12-27 16:01:48
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/1539,https://api.github.com/repos/kubernetes/autoscaler/issues/1539,Add listers for more controllers,"Currently we call the API many times per loop to get various controllers. Turns out it impacts CA performance quite a bit in large clusters (and it puts an extra load on API server). 

This is preparatory work for migrating those API calls to use watch cache instead.

I decided to use upstream listers directly, rather than wrap them in our own interface. Partly because we will actually need to replace Get() calls which the upstream lister already provides (and our interfaces don't), partly because all we did in our interface was forward the call to upstream interface anyway and using upstream interface feels more consistent with k8s ecosystem.
I've migrated existing daemonSetLister to use upstream interface as well to be consistent with other controllers (and migrated it to use apps/v1.DaemonSet instead of old extensions/v1beta1). I've left other listers as they were, because they actually have custom logic and we won't need Get calls for those. ",closed,True,2018-12-28 13:06:38,2018-12-28 15:01:22
autoscaler,raffaelespazzoli,https://github.com/kubernetes/autoscaler/issues/1540,https://api.github.com/repos/kubernetes/autoscaler/issues/1540,Add the ability to better pass additional arguments to containers,I suggest using `ENTRYPOINT` instead of `CMD` to set the default command of the containers that are used by VPA. This allows to use the `args` field in Kubernetes pods to customize the arguments to be passed to the containers. ,open,False,2018-12-28 13:55:11,2019-03-28 14:25:14
autoscaler,sylr,https://github.com/kubernetes/autoscaler/pull/1541,https://api.github.com/repos/kubernetes/autoscaler/issues/1541,Add multi stage Dockerfile,Allows to build & package image inside docker.,closed,True,2018-12-29 16:33:19,2019-02-01 15:00:32
autoscaler,JoeWrightss,https://github.com/kubernetes/autoscaler/pull/1542,https://api.github.com/repos/kubernetes/autoscaler/issues/1542,Fix typo in comment,"Fix typo in comment:  ""cloud proivder"" to ""cloud provider"" in line 360.",closed,True,2018-12-30 09:47:45,2019-01-02 13:24:15
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/1543,https://api.github.com/repos/kubernetes/autoscaler/issues/1543,Add functions for testing new listers,,closed,True,2018-12-31 10:39:14,2018-12-31 10:52:58
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/1544,https://api.github.com/repos/kubernetes/autoscaler/issues/1544,Use listers in scale-down,I'll migrate scale-up in follow up PR,closed,True,2018-12-31 13:38:17,2018-12-31 16:20:50
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1545,https://api.github.com/repos/kubernetes/autoscaler/issues/1545,Add cloudprovider.GetNodeResources method,,closed,True,2018-12-31 13:42:16,2019-01-02 14:21:33
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1546,https://api.github.com/repos/kubernetes/autoscaler/issues/1546,Rename nodeGroupBackoffInfo to backoff in ClusterStateRegistry,,closed,True,2018-12-31 17:00:15,2018-12-31 17:21:55
autoscaler,piontec,https://github.com/kubernetes/autoscaler/issues/1547,https://api.github.com/repos/kubernetes/autoscaler/issues/1547,vertical-pod-autoscaler 0.3.0 on AWS EKS - admission controller doesn't kick in,"HI!
I'm running VPA on EKS cluster in AWS. It supports mutating webhooks, as claimed by AWS. Now, I have the following configuration (to test ""hamster""  deployment in ""Initial"" mode):
```
$ ksdev get deploy,pod | grep vpa
deployment.extensions/vpa-admission-controller             1         1         1            1           1d
deployment.extensions/vpa-recommender                      1         1         1            1           1d
deployment.extensions/vpa-updater                          1         1         1            1           1d

pod/vpa-admission-controller-58977d995f-knwrr             1/1       Running     0          29m
pod/vpa-recommender-6bf9f87f85-6zz86                      1/1       Running     0          29m
pod/vpa-updater-6df84c89dd-pfb29                          1/1       Running     0          28m
```
Webhook is registered and seems in place:
```
$ ksdev get mutatingwebhookconfiguration.v1beta1.admissionregistration.k8s.io -o yaml       
apiVersion: v1
items:
- apiVersion: admissionregistration.k8s.io/v1beta1
  kind: MutatingWebhookConfiguration
  metadata:
    creationTimestamp: 2019-01-02T09:39:29Z
    generation: 1
    name: vpa-webhook-config
    namespace: """"
    resourceVersion: ""39148745""
    selfLink: /apis/admissionregistration.k8s.io/v1beta1/mutatingwebhookconfigurations/vpa-webhook-config
    uid: 4cec7d97-0e72-11e9-889f-127fc02963b2
  webhooks:
  - clientConfig:
      caBundle: [CUT]
      service:
        name: vpa-webhook
        namespace: kube-system
    failurePolicy: Ignore
    name: vpa.k8s.io
    namespaceSelector: {}
    rules:
    - apiGroups:
      - """"
      apiVersions:
      - v1
      operations:
      - CREATE
      resources:
      - pods
    - apiGroups:
      - autoscaling.k8s.io
      apiVersions:
      - v1beta1
      operations:
      - CREATE
      - UPDATE
      resources:
      - verticalpodautoscalers
kind: List
metadata:
  resourceVersion: """"
  selfLink: """"
```
Hamster pods are running, VPA is created and successfully updated by Recommender:
```
$ kdev get verticalpodautoscalers.autoscaling.k8s.io -o yaml
apiVersion: v1
items:
- apiVersion: autoscaling.k8s.io/v1beta1
  kind: VerticalPodAutoscaler
  metadata:   
    clusterName: """"
    creationTimestamp: 2019-01-02T09:50:32Z
    generation: 1
    name: hamster-vpa
    namespace: default
    resourceVersion: ""39155158""
    selfLink: /apis/autoscaling.k8s.io/v1beta1/namespaces/default/verticalpodautoscalers/hamster-vpa
    uid: d8733e00-0e73-11e9-9516-0a01d9a5380e
  spec:
    selector:
      matchLabels:
        app: hamster
    updatePolicy:
      updateMode: Initial
  status:
    conditions:
    - lastTransitionTime: 2019-01-02T09:51:05Z
      status: ""True""
      type: RecommendationProvided
    recommendation:
      containerRecommendations:
      - containerName: hamster
        lowerBound:
          cpu: 560m
          memory: 262144k
        target:
          cpu: 587m
          memory: 262144k
        uncappedTarget:
          cpu: 587m
          memory: 262144k
        upperBound:
          cpu: 15428m
          memory: ""282975409""
```

But actual admission controller seems to do nothing: the only logs I get are (repeated over and over):
```
I0102 10:05:21.020578       1 reflector.go:357] k8s.io/autoscaler/vertical-pod-autoscaler/pkg/utils/vpa/api.go:89: Watch close - *v1beta1.VerticalPodAutoscaler total 8 items received
I0102 10:05:21.020906       1 round_trippers.go:383] GET https://172.20.0.1:443/apis/autoscaling.k8s.io/v1beta1/verticalpodautoscalers?resourceVersion=39153926&timeoutSeconds=431&watch=true
I0102 10:05:21.021195       1 round_trippers.go:390] Request Headers:
I0102 10:05:21.021284       1 round_trippers.go:393]     Accept: application/json, */*
I0102 10:05:21.021460       1 round_trippers.go:393]     User-Agent: admission-controller/v0.0.0 (linux/amd64) kubernetes/$Format
I0102 10:05:21.021566       1 round_trippers.go:393]     Authorization: Bearer [XXX]
I0102 10:05:21.029808       1 round_trippers.go:408] Response Status: 200 OK in 8 milliseconds
I0102 10:05:21.029968       1 round_trippers.go:411] Response Headers:
I0102 10:05:21.030170       1 round_trippers.go:414]     Audit-Id: a812bbcd-1c26-49a2-9f7e-da07a60b7d51
I0102 10:05:21.030291       1 round_trippers.go:414]     Content-Type: application/json
I0102 10:05:21.030468       1 round_trippers.go:414]     Date: Wed, 02 Jan 2019 10:05:21 GMT
```

When new pods matching the selector are created, their default resurces are not changes nor there's anything showing up in logs. HOw can I investigate this problem?",open,False,2019-01-02 10:13:26,2019-02-28 08:06:07
autoscaler,arjunrn,https://github.com/kubernetes/autoscaler/issues/1548,https://api.github.com/repos/kubernetes/autoscaler/issues/1548,Recommender memory grows linearly with the number of different label sets in the cluster.,"The  [aggregateContainerStateMap](https://github.com/kubernetes/autoscaler/blob/0945a8fe9987a086223532ba60a3d1ffcf4e24fa/vertical-pod-autoscaler/pkg/recommender/model/cluster.go#L45) in the `cluster.go` is a map which stores container states where the keys of the map are a combination of the namespace, container name and pod labels. This works fine when the number of pods with unique labels does not grow a lot in an 8 day interval, after which the container state gets garbage collected. 

But this doesn't work well when there are a lot of CronJobs running in the cluster. The cronjob controller creates jobs for every execution and each job has a unique suffix. This job name is part of the pod label when the job runs. As a result an entry is created in the `aggregateContainerStateMap` every time the cronjob runs causing the memory to grow linearly and subsequently the container gets OOM killed. This is what it looks like in our production clusters:

![recommender_oomkills](https://user-images.githubusercontent.com/626536/50594510-2d0d2000-0e9d-11e9-86df-10620c4b4c9f.png)

I propose that the `job-name` be blacklisted and there be a flag on the recommender which accepts a list of labels which should not be recorded.",open,False,2019-01-02 13:50:34,2019-04-02 11:58:48
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/1549,https://api.github.com/repos/kubernetes/autoscaler/issues/1549,Use lister in GetNodeInfosForGroups,This significantly improves performance on very large clusters (3x faster in my tests).,closed,True,2019-01-02 15:01:20,2019-01-02 16:27:06
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1550,https://api.github.com/repos/kubernetes/autoscaler/issues/1550,Pass nodeGroup->NodeInfo map to ClusterStateRegistry,,closed,True,2019-01-02 15:23:27,2019-01-08 16:14:42
autoscaler,piontec,https://github.com/kubernetes/autoscaler/issues/1551,https://api.github.com/repos/kubernetes/autoscaler/issues/1551,vertiacal-pod-autoscaler: how to configure for integration with prometheus?,"I'm running VPA 0.3.0 on k8s 1.10.11. I'm trying to find some docs that explain how VPA fetches metrics' history from prometheus. Which metrics and labels it expects to be present in prometheus? Currently, no matter if I give it a correct or incorrect prometheus URL in `--prometheus-address`, nothing is logged about prometheus queries or connections, even with `--v=8`.",closed,False,2019-01-03 11:03:27,2019-02-28 08:07:10
autoscaler,bibryam,https://github.com/kubernetes/autoscaler/pull/1552,https://api.github.com/repos/kubernetes/autoscaler/issues/1552,Update FAQ.md,Typo,closed,True,2019-01-03 13:55:33,2019-01-10 21:06:15
autoscaler,MaciekPytel,https://github.com/kubernetes/autoscaler/pull/1553,https://api.github.com/repos/kubernetes/autoscaler/issues/1553,Cluster Autoscaler 1.3.5,,closed,True,2019-01-04 15:34:25,2019-01-04 15:48:51
autoscaler,thereverendtom,https://github.com/kubernetes/autoscaler/issues/1554,https://api.github.com/repos/kubernetes/autoscaler/issues/1554,Issue with Vertical Pod Autoscaler in Helm chart,"I have added the Vertical Pod Autoscaler to my k8s cluster (version 1.10.3), and I added several VPAs to one of my Helm charts. An example looks like this:

```
apiVersion: autoscaling.k8s.io/v1beta1
kind: VerticalPodAutoscaler
metadata:
  name: web-vpa
  namespace: zeus-dev
spec:
  selector:
    matchLabels:
      app: web
  updatePolicy:
    updateMode: ""Off""
```

My first Helm deployment worked without issue and the VPA appears to be functioning as intended. However, upon my next Helm deployment (no change to the VPA - just an unrelated code change), I get the following error:

`Error: UPGRADE FAILED: no VerticalPodAutoscaler with the name ""web-vpa"" found`

But when I query the cluster, the VPA is there:

```
kubectl get verticalpodautoscaler -n zeus-dev
NAME                                     CREATED AT
web-vpa                                  7d
```

I am running the latest version of Helm (2.12.1) and the most recent version of the VPA as of Dec 28. When I remove the VPA definitions from my chart, the Helm deployment succeeds.

Has anyone else encountered this? ",closed,False,2019-01-04 22:40:01,2019-01-07 18:33:43
autoscaler,hobbsh,https://github.com/kubernetes/autoscaler/issues/1555,https://api.github.com/repos/kubernetes/autoscaler/issues/1555,cluster-autoscaler 1.2.2 not scaling AWS ASG to zero,"Hi,

I have been unable to get the cluster-autoscaler to scale one of my two autoscaling groups to zero. In this scenario, the `blue` worker group should be scaled to zero. It's very likely I missed something but have been unable to track down what that might be based on the documentation/information available on the internet.

I have tagged both ASGs with `k8s.io/cluster-autoscaler/node-template/label/eks_worker_group: [blue|green]` and nodes have labels `eks_worker_group: [blue|green]`. Nodes are also tagged with the same tag on the EC2 side as well. 

Running with the following options:
```
        - ./cluster-autoscaler
        - --cloud-provider=aws
        - --namespace=default
        - --nodes=0:0:staging-k8s-worker-blue20180814211923516200000002
        - --nodes=3:7:staging-k8s-worker-green20181212202045835900000004
        - --logtostderr=true
        - --scale-down-delay-after-add=5m
        - --scale-down-delay-after-delete=5m
        - --skip-nodes-with-system-pods=false
        - --stderrthreshold=info
        - --v=4
```

Here is status:
```
Cluster-wide:
  Health:      Healthy (ready=6 unready=0 notStarted=0 longNotStarted=0 registered=6 longUnregistered=0)
               LastProbeTime:      2019-01-05 01:26:23.670947407 +0000 UTC
               LastTransitionTime: 2019-01-05 01:26:23.670947407 +0000 UTC
  ScaleUp:     NoActivity (ready=6 registered=6)
               LastProbeTime:      2019-01-05 01:26:23.670947407 +0000 UTC
               LastTransitionTime: 2019-01-05 01:26:23.670947407 +0000 UTC
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2019-01-05 01:26:23.670947407 +0000 UTC
               LastTransitionTime: 2019-01-05 01:26:23.670947407 +0000 UTC

NodeGroups:
  Name:        staging-k8s-worker-blue20180814211923516200000002
  Health:      Healthy (ready=3 unready=0 notStarted=0 longNotStarted=0 registered=3 longUnregistered=0 cloudProviderTarget=0 (minSize=0, maxSize=0))
               LastProbeTime:      2019-01-05 01:26:23.670947407 +0000 UTC
               LastTransitionTime: 2019-01-05 01:26:23.670947407 +0000 UTC
  ScaleUp:     NoActivity (ready=3 cloudProviderTarget=0)
               LastProbeTime:      2019-01-05 01:26:23.670947407 +0000 UTC
               LastTransitionTime: 2019-01-05 01:26:23.670947407 +0000 UTC
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2019-01-05 01:26:23.670947407 +0000 UTC
               LastTransitionTime: 2019-01-05 01:26:23.670947407 +0000 UTC

  Name:        staging-k8s-worker-green20181212202045835900000004
  Health:      Healthy (ready=3 unready=0 notStarted=0 longNotStarted=0 registered=3 longUnregistered=0 cloudProviderTarget=3 (minSize=3, maxSize=7))
               LastProbeTime:      2019-01-05 01:26:23.670947407 +0000 UTC
               LastTransitionTime: 2019-01-05 01:26:23.670947407 +0000 UTC
  ScaleUp:     NoActivity (ready=3 cloudProviderTarget=3)
               LastProbeTime:      2019-01-05 01:26:23.670947407 +0000 UTC
               LastTransitionTime: 2019-01-05 01:26:23.670947407 +0000 UTC
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2019-01-05 01:26:23.670947407 +0000 UTC
               LastTransitionTime: 2019-01-05 01:26:23.670947407 +0000 UTC
```

And this is what I see in the logs, no smoking gun that I can see:
```
I0105 01:24:33.384719       1 static_autoscaler.go:114] Starting main loop
I0105 01:24:33.384753       1 aws_manager.go:241] Refreshed ASG list, next refresh after 2019-01-05 01:25:33.384747177 +0000 UTC
I0105 01:24:33.757020       1 leaderelection.go:199] successfully renewed lease default/cluster-autoscaler
I0105 01:24:34.099184       1 utils.go:456] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0105 01:24:34.099232       1 static_autoscaler.go:263] Filtering out schedulables
I0105 01:24:34.099609       1 static_autoscaler.go:273] No schedulable pods
I0105 01:24:34.099644       1 static_autoscaler.go:280] No unschedulable pods
I0105 01:24:34.099662       1 static_autoscaler.go:322] Calculating unneeded nodes
I0105 01:24:34.395094       1 utils.go:413] Skipping ip-10-99-62-151.us-west-2.compute.internal - node group min size reached
I0105 01:24:34.395127       1 utils.go:413] Skipping ip-10-99-60-164.us-west-2.compute.internal - node group min size reached
I0105 01:24:34.395139       1 utils.go:413] Skipping ip-10-99-61-85.us-west-2.compute.internal - node group min size reached
I0105 01:24:34.395149       1 utils.go:413] Skipping ip-10-99-61-86.us-west-2.compute.internal - node group min size reached
I0105 01:24:34.395159       1 utils.go:413] Skipping ip-10-99-62-166.us-west-2.compute.internal - node group min size reached
I0105 01:24:34.395169       1 utils.go:413] Skipping ip-10-99-60-139.us-west-2.compute.internal - node group min size reached
I0105 01:24:34.395482       1 static_autoscaler.go:352] Scale down status: unneededOnly=false lastScaleUpTime=2019-01-05 01:01:11.941954422 +0000 UTC lastScaleDownDeleteTime=2019-01-05 01:01:11.941954962 +0000 UTC lastScaleDownFailTime=2019-01-05 01:01:11.941955491 +0000 UTC schedulablePodsPresent=false isDeleteInProgress=false
I0105 01:24:34.395513       1 static_autoscaler.go:355] Starting scale down
I0105 01:24:34.674506       1 scale_down.go:446] No candidates for scale down
```


",open,False,2019-01-05 01:31:03,2019-01-08 21:06:18
autoscaler,amitlt,https://github.com/kubernetes/autoscaler/issues/1556,https://api.github.com/repos/kubernetes/autoscaler/issues/1556,"cluster-autoscaler AWS ""runtime error: invalid memory address or nil pointer dereference""","**kubernete version: 1.10.11**
**ca version: 1.2.2**

**instance type**: r5.large

[this issue](https://github.com/kubernetes/autoscaler/issues/1411) describes the problem being that the specific instance type isn't in the configuration, but looking into the `1.2.2` release i see that the instance type [does indeed exist](https://github.com/kubernetes/autoscaler/blob/cluster-autoscaler-release-1.2/cluster-autoscaler/cloudprovider/aws/ec2_instance_types.go#L894)

furthermore, from my testing, scaling up to 1 instance and scaling back down to 0 instances worked multiple times, so I'm not entirely sure what triggered this issue.


**values.yaml**
```yaml
autoDiscovery:
# Only cloudProvider `aws` and `gce` are supported by auto-discovery at this time
# AWS: Set tags as described in https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup
  clusterName:

autoscalingGroups:
# At least one element is required if not using autoDiscovery
  - name: jobs-nodes.k8s.test.site.com
    maxSize: 20
    minSize: 0
  # - name: asg2
  #   maxSize: 2
  #   minSize: 1

autoscalingGroupsnamePrefix: []
# At least one element is required if not using autoDiscovery
  # - name: ig01
  #   maxSize: 10
  #   minSize: 0
  # - name: ig02
  #   maxSize: 10
  #   minSize: 0

# Required if cloudProvider=aws
awsRegion: us-east-1

# Currently only `gce`, `aws` & `spotinst` are supported
cloudProvider: aws

sslCertPath: /etc/ssl/certs/ca-certificates.crt

# Configuration file for cloud provider
cloudConfigPath: /etc/gce.conf

image:
  repository: gcr.io/google_containers/cluster-autoscaler
  tag: v1.2.2
  pullPolicy: Always

tolerations: []

extraArgs:
  v: 4
  stderrthreshold: info
  logtostderr: true
  skip-nodes-with-system-pods: ""false""
  # write-status-configmap: true
  # leader-elect: true
  # skip-nodes-with-local-storage: false
  # expander: least-waste
  # scale-down-enabled: true
  # balance-similar-node-groups: true
  # min-replica-count: 2
  # scale-down-utilization-threshold: 0.5
  # scale-down-non-empty-candidates-count: 5
  # max-node-provision-time: 15m0s
  # scan-interval: 10s
  # scale-down-delay: 10m
  # scale-down-unneeded-time: 10m
  # skip-nodes-with-local-storage: false


## Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## affinity: {}

podDisruptionBudget: |
  maxUnavailable: 1
  # minAvailable: 2

## Node labels for pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

podAnnotations: {
  ""iam.amazonaws.com/role"": ""arn:aws:iam::671587110562:role/system/k8s-autoscaler-test-role""
}
podLabels: {}
replicaCount: 1

rbac:
  ## If true, create & use RBAC resources
  ##
  create: true
  ## If true, create & use Pod Security Policy resources
  ## https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  pspEnabled: true
  ## Ignored if rbac.create is true
  ##
  serviceAccountName: default

resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 300Mi
  # requests:
  #   cpu: 100m
  #   memory: 300Mi

priorityClassName: """"

service:
  annotations: {}
  clusterIP: """"

  ## List of IP addresses at which the service is available
  ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
  ##
  externalIPs: []

  loadBalancerIP: """"
  loadBalancerSourceRanges: []
  servicePort: 8085
  portName: http
  type: ClusterIP

spotinst:
  account: """"
  token: """"
  image:
    repository: spotinst/kubernetes-cluster-autoscaler
    tag: 0.6.0
    pullPolicy: IfNotPresent
```

**ca starts with the following flags:**
```yaml
spec:
  containers:
  - command:
    - ./cluster-autoscaler
    - --cloud-provider=aws
    - --namespace=kube-system
    - --nodes=0:20:jobs-nodes.k8s.test.site.com
    - --logtostderr=true
    - --skip-nodes-with-system-pods=false
    - --stderrthreshold=info
    - --v=4
```



**ca logs:**
```
I0106 08:13:18.701677       1 flags.go:52] FLAG: --address="":8085""
I0106 08:13:18.701711       1 flags.go:52] FLAG: --alsologtostderr=""false""
I0106 08:13:18.701719       1 flags.go:52] FLAG: --application-metrics-count-limit=""100""
I0106 08:13:18.701722       1 flags.go:52] FLAG: --azure-container-registry-config=""""
I0106 08:13:18.701727       1 flags.go:52] FLAG: --balance-similar-node-groups=""false""
I0106 08:13:18.701731       1 flags.go:52] FLAG: --boot-id-file=""/proc/sys/kernel/random/boot_id""
I0106 08:13:18.701735       1 flags.go:52] FLAG: --cloud-config=""""
I0106 08:13:18.701786       1 flags.go:52] FLAG: --cloud-provider=""aws""
I0106 08:13:18.701793       1 flags.go:52] FLAG: --cloud-provider-gce-lb-src-cidrs=""130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16""
I0106 08:13:18.701800       1 flags.go:52] FLAG: --cluster-name=""""
I0106 08:13:18.701804       1 flags.go:52] FLAG: --configmap=""""
I0106 08:13:18.701807       1 flags.go:52] FLAG: --container-hints=""/etc/cadvisor/container_hints.json""
I0106 08:13:18.701811       1 flags.go:52] FLAG: --containerd=""unix:///var/run/containerd.sock""
I0106 08:13:18.701816       1 flags.go:52] FLAG: --cores-total=""0:320000""
I0106 08:13:18.701819       1 flags.go:52] FLAG: --docker=""unix:///var/run/docker.sock""
I0106 08:13:18.701824       1 flags.go:52] FLAG: --docker-env-metadata-whitelist=""""
I0106 08:13:18.701827       1 flags.go:52] FLAG: --docker-only=""false""
I0106 08:13:18.701831       1 flags.go:52] FLAG: --docker-root=""/var/lib/docker""
I0106 08:13:18.701835       1 flags.go:52] FLAG: --docker-tls=""false""
I0106 08:13:18.701839       1 flags.go:52] FLAG: --docker-tls-ca=""ca.pem""
I0106 08:13:18.701842       1 flags.go:52] FLAG: --docker-tls-cert=""cert.pem""
I0106 08:13:18.701846       1 flags.go:52] FLAG: --docker-tls-key=""key.pem""
I0106 08:13:18.701849       1 flags.go:52] FLAG: --enable-load-reader=""false""
I0106 08:13:18.701852       1 flags.go:52] FLAG: --estimator=""binpacking""
I0106 08:13:18.701856       1 flags.go:52] FLAG: --event-storage-age-limit=""default=0""
I0106 08:13:18.701860       1 flags.go:52] FLAG: --event-storage-event-limit=""default=0""
I0106 08:13:18.701864       1 flags.go:52] FLAG: --expander=""random""
I0106 08:13:18.701868       1 flags.go:52] FLAG: --expendable-pods-priority-cutoff=""0""
I0106 08:13:18.701872       1 flags.go:52] FLAG: --gke-api-endpoint=""""
I0106 08:13:18.701875       1 flags.go:52] FLAG: --global-housekeeping-interval=""1m0s""
I0106 08:13:18.701878       1 flags.go:52] FLAG: --google-json-key=""""
I0106 08:13:18.701882       1 flags.go:52] FLAG: --housekeeping-interval=""10s""
I0106 08:13:18.701886       1 flags.go:52] FLAG: --httptest.serve=""""
I0106 08:13:18.701889       1 flags.go:52] FLAG: --kubeconfig=""""
I0106 08:13:18.701893       1 flags.go:52] FLAG: --kubernetes=""""
I0106 08:13:18.701896       1 flags.go:52] FLAG: --leader-elect=""true""
I0106 08:13:18.701902       1 flags.go:52] FLAG: --leader-elect-lease-duration=""15s""
I0106 08:13:18.701908       1 flags.go:52] FLAG: --leader-elect-renew-deadline=""10s""
I0106 08:13:18.701912       1 flags.go:52] FLAG: --leader-elect-resource-lock=""endpoints""
I0106 08:13:18.701916       1 flags.go:52] FLAG: --leader-elect-retry-period=""2s""
I0106 08:13:18.701919       1 flags.go:52] FLAG: --log-backtrace-at="":0""
I0106 08:13:18.701926       1 flags.go:52] FLAG: --log-cadvisor-usage=""false""
I0106 08:13:18.701930       1 flags.go:52] FLAG: --log-dir=""""
I0106 08:13:18.701933       1 flags.go:52] FLAG: --log-flush-frequency=""5s""
I0106 08:13:18.701937       1 flags.go:52] FLAG: --logtostderr=""true""
I0106 08:13:18.701941       1 flags.go:52] FLAG: --machine-id-file=""/etc/machine-id,/var/lib/dbus/machine-id""
I0106 08:13:18.701946       1 flags.go:52] FLAG: --max-autoprovisioned-node-group-count=""15""
I0106 08:13:18.701949       1 flags.go:52] FLAG: --max-empty-bulk-delete=""10""
I0106 08:13:18.701954       1 flags.go:52] FLAG: --max-failing-time=""15m0s""
I0106 08:13:18.701957       1 flags.go:52] FLAG: --max-graceful-termination-sec=""600""
I0106 08:13:18.701961       1 flags.go:52] FLAG: --max-inactivity=""10m0s""
I0106 08:13:18.701965       1 flags.go:52] FLAG: --max-node-provision-time=""15m0s""
I0106 08:13:18.701968       1 flags.go:52] FLAG: --max-nodes-total=""0""
I0106 08:13:18.701971       1 flags.go:52] FLAG: --max-total-unready-percentage=""45""
I0106 08:13:18.701976       1 flags.go:52] FLAG: --memory-total=""0:6400000""
I0106 08:13:18.701979       1 flags.go:52] FLAG: --min-replica-count=""0""
I0106 08:13:18.701982       1 flags.go:52] FLAG: --namespace=""kube-system""
I0106 08:13:18.701986       1 flags.go:52] FLAG: --node-autoprovisioning-enabled=""false""
I0106 08:13:18.701990       1 flags.go:52] FLAG: --node-group-auto-discovery=""[]""
I0106 08:13:18.701994       1 flags.go:52] FLAG: --nodes=""[0:20:jobs-nodes.k8s.test.site.com]""
I0106 08:13:18.701999       1 flags.go:52] FLAG: --ok-total-unready-count=""3""
I0106 08:13:18.702002       1 flags.go:52] FLAG: --regional=""false""
I0106 08:13:18.702007       1 flags.go:52] FLAG: --scale-down-candidates-pool-min-count=""50""
I0106 08:13:18.702010       1 flags.go:52] FLAG: --scale-down-candidates-pool-ratio=""0.1""
I0106 08:13:18.702014       1 flags.go:52] FLAG: --scale-down-delay-after-add=""10m0s""
I0106 08:13:18.702017       1 flags.go:52] FLAG: --scale-down-delay-after-delete=""10s""
I0106 08:13:18.702021       1 flags.go:52] FLAG: --scale-down-delay-after-failure=""3m0s""
I0106 08:13:18.702025       1 flags.go:52] FLAG: --scale-down-enabled=""true""
I0106 08:13:18.702029       1 flags.go:52] FLAG: --scale-down-non-empty-candidates-count=""30""
I0106 08:13:18.702032       1 flags.go:52] FLAG: --scale-down-unneeded-time=""10m0s""
I0106 08:13:18.702036       1 flags.go:52] FLAG: --scale-down-unready-time=""20m0s""
I0106 08:13:18.702040       1 flags.go:52] FLAG: --scale-down-utilization-threshold=""0.5""
I0106 08:13:18.702043       1 flags.go:52] FLAG: --scan-interval=""10s""
I0106 08:13:18.702048       1 flags.go:52] FLAG: --skip-nodes-with-local-storage=""true""
I0106 08:13:18.702051       1 flags.go:52] FLAG: --skip-nodes-with-system-pods=""false""
I0106 08:13:18.702054       1 flags.go:52] FLAG: --stderrthreshold=""0""
I0106 08:13:18.702059       1 flags.go:52] FLAG: --storage-driver-buffer-duration=""1m0s""
I0106 08:13:18.702062       1 flags.go:52] FLAG: --storage-driver-db=""cadvisor""
I0106 08:13:18.702066       1 flags.go:52] FLAG: --storage-driver-host=""localhost:8086""
I0106 08:13:18.702070       1 flags.go:52] FLAG: --storage-driver-password=""root""
I0106 08:13:18.702073       1 flags.go:52] FLAG: --storage-driver-secure=""false""
I0106 08:13:18.702077       1 flags.go:52] FLAG: --storage-driver-table=""stats""
I0106 08:13:18.702080       1 flags.go:52] FLAG: --storage-driver-user=""root""
I0106 08:13:18.702084       1 flags.go:52] FLAG: --test.bench=""""
I0106 08:13:18.702088       1 flags.go:52] FLAG: --test.benchmem=""false""
I0106 08:13:18.702091       1 flags.go:52] FLAG: --test.benchtime=""1s""
I0106 08:13:18.702094       1 flags.go:52] FLAG: --test.blockprofile=""""
I0106 08:13:18.702098       1 flags.go:52] FLAG: --test.blockprofilerate=""1""
I0106 08:13:18.702101       1 flags.go:52] FLAG: --test.count=""1""
I0106 08:13:18.702105       1 flags.go:52] FLAG: --test.coverprofile=""""
I0106 08:13:18.702108       1 flags.go:52] FLAG: --test.cpu=""""
I0106 08:13:18.702111       1 flags.go:52] FLAG: --test.cpuprofile=""""
I0106 08:13:18.702116       1 flags.go:52] FLAG: --test.memprofile=""""
I0106 08:13:18.702119       1 flags.go:52] FLAG: --test.memprofilerate=""0""
I0106 08:13:18.702122       1 flags.go:52] FLAG: --test.mutexprofile=""""
I0106 08:13:18.702127       1 flags.go:52] FLAG: --test.mutexprofilefraction=""1""
I0106 08:13:18.702130       1 flags.go:52] FLAG: --test.outputdir=""""
I0106 08:13:18.702133       1 flags.go:52] FLAG: --test.parallel=""2""
I0106 08:13:18.702137       1 flags.go:52] FLAG: --test.run=""""
I0106 08:13:18.702141       1 flags.go:52] FLAG: --test.short=""false""
I0106 08:13:18.702144       1 flags.go:52] FLAG: --test.timeout=""0s""
I0106 08:13:18.702148       1 flags.go:52] FLAG: --test.trace=""""
I0106 08:13:18.702151       1 flags.go:52] FLAG: --test.v=""false""
I0106 08:13:18.702155       1 flags.go:52] FLAG: --v=""4""
I0106 08:13:18.702161       1 flags.go:52] FLAG: --version=""false""
I0106 08:13:18.702169       1 flags.go:52] FLAG: --vmodule=""""
I0106 08:13:18.702172       1 flags.go:52] FLAG: --write-status-configmap=""true""
I0106 08:13:18.702178       1 main.go:298] Cluster Autoscaler 1.2.2
I0106 08:13:18.734980       1 leaderelection.go:175] attempting to acquire leader lease  kube-system/cluster-autoscaler...
I0106 08:13:18.746266       1 leaderelection.go:184] successfully acquired lease kube-system/cluster-autoscaler
I0106 08:13:18.746698       1 factory.go:33] Event(v1.ObjectReference{Kind:""Endpoints"", Namespace:""kube-system"", Name:""cluster-autoscaler"", UID:""38678a25-0f52-11e9-baa4-0e4b20da8fb0"", APIVersion:""v1"", ResourceVersion:""36472506"", FieldPath:""""}): type: 'Normal' reason: 'LeaderElection' k8s-autoscaler-aws-cluster-autoscaler-d6bd46fc9-j42v8 became leader
I0106 08:13:18.747459       1 predicates.go:125] Using predicate PodFitsResources
I0106 08:13:18.747471       1 predicates.go:125] Using predicate GeneralPredicates
I0106 08:13:18.747476       1 predicates.go:125] Using predicate PodToleratesNodeTaints
I0106 08:13:18.747481       1 predicates.go:125] Using predicate CheckNodeDiskPressure
I0106 08:13:18.747485       1 predicates.go:125] Using predicate NoDiskConflict
I0106 08:13:18.747491       1 predicates.go:125] Using predicate NoVolumeZoneConflict
I0106 08:13:18.747494       1 predicates.go:125] Using predicate CheckNodeCondition
I0106 08:13:18.747498       1 predicates.go:125] Using predicate CheckNodeMemoryPressure
I0106 08:13:18.747502       1 predicates.go:125] Using predicate MaxAzureDiskVolumeCount
I0106 08:13:18.747507       1 predicates.go:125] Using predicate ready
I0106 08:13:18.747512       1 predicates.go:125] Using predicate MatchInterPodAffinity
I0106 08:13:18.747517       1 predicates.go:125] Using predicate MaxEBSVolumeCount
I0106 08:13:18.747521       1 predicates.go:125] Using predicate CheckVolumeBinding
I0106 08:13:18.747525       1 predicates.go:125] Using predicate MaxGCEPDVolumeCount
I0106 08:13:18.747761       1 reflector.go:202] Starting reflector *v1beta1.DaemonSet (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I0106 08:13:18.747789       1 reflector.go:240] Listing and watching *v1beta1.DaemonSet from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I0106 08:13:18.747984       1 reflector.go:202] Starting reflector *v1beta1.PodDisruptionBudget (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.748025       1 reflector.go:240] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.748180       1 reflector.go:202] Starting reflector *v1.Pod (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.748191       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.748452       1 reflector.go:202] Starting reflector *v1.StorageClass (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.748464       1 reflector.go:240] Listing and watching *v1.StorageClass from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.748687       1 reflector.go:202] Starting reflector *v1.PersistentVolumeClaim (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.748725       1 reflector.go:240] Listing and watching *v1.PersistentVolumeClaim from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.748787       1 reflector.go:202] Starting reflector *v1.PersistentVolume (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.748794       1 reflector.go:240] Listing and watching *v1.PersistentVolume from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.749082       1 reflector.go:202] Starting reflector *v1.Service (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.749093       1 reflector.go:240] Listing and watching *v1.Service from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.749311       1 reflector.go:202] Starting reflector *v1.ReplicationController (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.749349       1 reflector.go:240] Listing and watching *v1.ReplicationController from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.749407       1 reflector.go:202] Starting reflector *v1.Node (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.749414       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.749734       1 reflector.go:202] Starting reflector *v1beta1.ReplicaSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.749796       1 reflector.go:240] Listing and watching *v1beta1.ReplicaSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.750085       1 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149
I0106 08:13:18.750131       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149
I0106 08:13:18.750241       1 reflector.go:202] Starting reflector *v1beta1.StatefulSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.750272       1 reflector.go:240] Listing and watching *v1beta1.StatefulSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:87
I0106 08:13:18.750359       1 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I0106 08:13:18.750370       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I0106 08:13:18.750444       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212
I0106 08:13:18.750453       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212
I0106 08:13:18.750513       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I0106 08:13:18.750555       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I0106 08:13:18.750607       1 reflector.go:202] Starting reflector *v1beta1.PodDisruptionBudget (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0106 08:13:18.750612       1 reflector.go:240] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0106 08:13:18.947752       1 request.go:481] Throttling request took 197.141227ms, request: GET:https://100.64.0.1:443/api/v1/nodes?limit=500&resourceVersion=0
I0106 08:13:19.147770       1 request.go:481] Throttling request took 396.132055ms, request: PUT:https://100.64.0.1:443/api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status
I0106 08:13:19.155802       1 cloud_provider_builder.go:72] Building aws cloud provider.
I0106 08:13:19.155902       1 auto_scaling_groups.go:78] Registering ASG jobs-nodes.k8s.test.site.com
I0106 08:13:19.155913       1 auto_scaling_groups.go:139] Invalidating unowned instance cache
I0106 08:13:19.155919       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:19.345600       1 aws_manager.go:241] Refreshed ASG list, next refresh after 2019-01-06 08:14:19.345591774 +0000 UTC
I0106 08:13:19.345724       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:19.547761       1 request.go:481] Throttling request took 196.79672ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-63-118.ec2.internal
I0106 08:13:19.747763       1 request.go:481] Throttling request took 196.744941ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-61-191.ec2.internal
I0106 08:13:19.947758       1 request.go:481] Throttling request took 196.622592ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-63-16.ec2.internal
I0106 08:13:20.147755       1 request.go:481] Throttling request took 197.004768ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-63-153.ec2.internal
I0106 08:13:20.347759       1 request.go:481] Throttling request took 196.74824ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-61-151.ec2.internal
I0106 08:13:20.547762       1 request.go:481] Throttling request took 197.238579ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-63-86.ec2.internal
I0106 08:13:20.747735       1 request.go:481] Throttling request took 197.072671ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-62-182.ec2.internal
I0106 08:13:20.757579       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0106 08:13:20.947758       1 request.go:481] Throttling request took 196.767459ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-62-168.ec2.internal
I0106 08:13:21.147761       1 request.go:481] Throttling request took 196.564581ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-62-233.ec2.internal
I0106 08:13:21.347777       1 request.go:481] Throttling request took 196.680603ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-61-134.ec2.internal
I0106 08:13:21.547756       1 request.go:481] Throttling request took 196.794005ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-62-155.ec2.internal
I0106 08:13:21.747790       1 request.go:481] Throttling request took 196.72087ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-61-94.ec2.internal
I0106 08:13:21.947758       1 request.go:481] Throttling request took 196.413705ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-62-138.ec2.internal
I0106 08:13:22.147756       1 request.go:481] Throttling request took 196.80818ms, request: GET:https://100.64.0.1:443/api/v1/nodes/ip-10-0-62-74.ec2.internal
I0106 08:13:22.151032       1 main.go:228] Registered cleanup signal handler
I0106 08:13:22.767123       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0106 08:13:24.777707       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0106 08:13:26.787862       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0106 08:13:28.800381       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0106 08:13:30.811593       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0106 08:13:32.151181       1 static_autoscaler.go:114] Starting main loop
I0106 08:13:32.460952       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:32.580709       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:32.685289       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:32.821598       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0106 08:13:32.839600       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:33.012626       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:33.145119       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:33.260650       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:33.405939       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:33.572608       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:33.724017       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:33.843445       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:33.964336       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:34.076162       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:34.165768       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:34.323805       1 auto_scaling_groups.go:153] Regenerating instance to ASG map for ASGs: [jobs-nodes.k8s.test.site.com]
I0106 08:13:34.475068       1 static_autoscaler.go:263] Filtering out schedulables
I0106 08:13:34.475464       1 static_autoscaler.go:273] No schedulable pods
I0106 08:13:34.475485       1 scale_up.go:59] Pod default/ai-service-job-1546732800-m2ckm is unschedulable
W0106 08:13:34.665288       1 aws_manager.go:380] Found multiple availability zones, using us-east-1b
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x145dccd]

goroutine 74 [running]:
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).buildNodeFromTemplate(0xc421782b90, 0xc420e7ba40, 0xc42162d200, 0xc42162d200, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:407 +0x48d
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*Asg).TemplateNodeInfo(0xc420e7ba40, 0xc42129fc80, 0x7ffdd5459730, 0x1f)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider.go:294 +0x78
k8s.io/autoscaler/cluster-autoscaler/core.GetNodeInfosForGroups(0xc421ce2300, 0xf, 0x10, 0x5a07ca0, 0xc4217da1d0, 0x5a14620, 0xc4200d83c0, 0xc421123b60, 0x3, 0x4, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/utils.go:228 +0x65a
k8s.io/autoscaler/cluster-autoscaler/core.ScaleUp(0xc420dba700, 0xc421ef1368, 0x1, 0x1, 0xc421ce2300, 0xf, 0x10, 0xc421123b60, 0x3, 0x4, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:63 +0x393
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc421092980, 0xed3c3afac, 0xe090276c9, 0x5bf5180, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:299 +0x296a
main.run(0xc420a93400)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:269 +0x474
main.main.func2(0xc420b894a0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:356 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:145 +0x97
```",closed,False,2019-01-06 08:34:13,2019-01-07 18:44:02
autoscaler,CodeLingoBot,https://github.com/kubernetes/autoscaler/pull/1557,https://api.github.com/repos/kubernetes/autoscaler/issues/1557, Fix error format strings according to best practices from CodeReviewComments,"Use [CodeLingo](https://codelingo.io) to automatically fix error format strings following the
[Go Code Review Comments guidelines](https://github.com/golang/go/wiki/CodeReviewComments) in [CONTRIBUTING.md](https://github.com/kubernetes/autoscaler/blob/master/CONTRIBUTING.md).

This patch was generated by running the CodeLingo Rewrite Flow over the ""[go-error-fmt](https://github.com/codelingo/codelingo/blob/master/tenets/codelingo/code-review-comments/go-error-fmt/codelingo.yaml)"" Tenet. Note: the same Tenet can be used to automate PR reviews and generate contributor docs by [Installing](https://github.com/apps/codelingo) the CodeLingo GitHub app. Learn more at [codelingo.io](https://codelingo.io).",closed,True,2019-01-06 23:07:47,2019-02-01 17:13:21
autoscaler,tsu1980,https://github.com/kubernetes/autoscaler/issues/1558,https://api.github.com/repos/kubernetes/autoscaler/issues/1558,Cannot configure expander strategy in GKE,"### Description
I've created two node pools in GKE, preemptible(pool1) and non-preemptible(pool2) node pools. poo1 is used for stateless workload like web server, and pool2 is used for used for stateful workloads and fallback of the preemptible node. So I can make a more cost-efficient cluster.

But I struggled to configure to set high priority to pool1 that used when node pool scaling up.
AFAIK, its handled by CA expander strategy setting. And should be `price` strategy in my case.
But I don't know what GKE uses. and GKE is not provided to configure this setting.

### Settings
#### pool1 - preemptible node pool
`gcloud container node-pools create pool2 --enable-autoscaling --num-nodes=1 --min-nodes=1 --max-nodes=10 --preemptible`
used for stateless workload like web server.

#### pool2 - non preemptible node pool
`gcloud container node-pools create pool1 --enable-autoscaling --num-nodes=1 --min-nodes=1 --max-nodes=10`
used for stateful workloads and fallback of the preemptible node.

Sorry for my poor english!
",closed,False,2019-01-07 03:27:23,2019-01-08 00:23:53
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/issues/1559,https://api.github.com/repos/kubernetes/autoscaler/issues/1559,Nodegroup min/max should respect both explicit setting and ASG api return,"Cluster scaler supports explicit nodeGroup min/max setting from user like following,
```
--nodes=1:10:k8s-worker-asg-1
```
The problem is user can try arbitrary min/max combination here. I think we should consider ASG API return as well at the time register asg. But it brings another problem once ASG min/max is updated on cloud provider side, explicitlyConfigured might be wiped. Could someone help verify if this is by design or should we fix this issue? If that's reasonable, I can submit a PR to address this issue.

```go
if !m.explicitlyConfigured[asg.AwsRef] {
  existing.minSize = asg.minSize
  existing.maxSize = asg.maxSize
} else {
  minSize := cloudprovider.MaxOf(asg.minSize, existing.minSize)
  maxSize := cloudprovider.MinOf(asg.maxSize, existing.maxSize)

  if minSize <= maxSize {
    existing.minSize = minSize
    existing.maxSize = maxSize
  } else {
    klog.V(4).Infof(""Min/Max %d/%d doesn't make sense"", minSize, maxSize)
  }
}
```


Let's assume ASG min/max setting is 3/3, desired instance 3. 

Option 1. --nodes=3:4:k8s-worker-asg-1. CA fails scaling up because it reach max of ASG. 
```
I0106 22:04:07.483292   51532 scale_up.go:262] Pod default/gpu-deployment-687f9486f-zcz9d is unschedulable
I0106 22:04:07.483454   51532 scale_up.go:304] Upcoming 0 nodes
I0106 22:04:07.483551   51532 scale_up.go:427] Best option to resize: eksctl-eks-test-nodegroup-0-NodeGroup-3OK0XRHS0SA4
I0106 22:04:07.483564   51532 scale_up.go:431] Estimated 1 nodes needed in eksctl-eks-test-nodegroup-0-NodeGroup-3OK0XRHS0SA4
I0106 22:04:07.483582   51532 scale_up.go:533] Final scale-up plan: [{eksctl-eks-test-nodegroup-0-NodeGroup-3OK0XRHS0SA4 3->4 (max: 4)}]
I0106 22:04:07.483610   51532 scale_up.go:690] Scale-up: setting group eksctl-eks-test-nodegroup-0-NodeGroup-3OK0XRHS0SA4 size to 4
I0106 22:04:07.483649   51532 auto_scaling_groups.go:203] Setting asg eksctl-eks-test-nodegroup-0-NodeGroup-3OK0XRHS0SA4 size to 4
I0106 22:04:07.483727   51532 factory.go:33] Event(v1.ObjectReference{Kind:""ConfigMap"", Namespace:""kube-system"", Name:""cluster-autoscaler-status"", UID:""f8eb720c-1241-11e9-8127-0233a42c3342"", APIVersion:""v1"", ResourceVersion:""48396"", FieldPath:""""}): type: 'Normal' reason: 'ScaledUpGroup' Scale-up: setting group eksctl-eks-test-nodegroup-0-NodeGroup-3OK0XRHS0SA4 size to 4
W0106 22:04:07.709530   51532 clusterstate.go:252] Disabling scale-up for node group eksctl-eks-test-nodegroup-0-NodeGroup-3OK0XRHS0SA4 until 2019-01-06 22:09:07.483285 -0800 PST m=+332.011590914
I0106 22:04:07.709545   51532 factory.go:33] Event(v1.ObjectReference{Kind:""ConfigMap"", Namespace:""kube-system"", Name:""cluster-autoscaler-status"", UID:""f8eb720c-1241-11e9-8127-0233a42c3342"", APIVersion:""v1"", ResourceVersion:""48396"", FieldPath:""""}): type: 'Warning' reason: 'FailedToScaleUpGroup' Scale-up failed for group eksctl-eks-test-nodegroup-0-NodeGroup-3OK0XRHS0SA4: ValidationError: New SetDesiredCapacity value 4 is above max value 3 for the AutoScalingGroup.
    status code: 400, request id: 0bc126e5-1242-11e9-bdaa-31a801fb61d3
E0106 22:04:07.710168   51532 static_autoscaler.go:316] Failed to scale up: failed to increase node group size: ValidationError: New SetDesiredCapacity value 4 is above max value 3 for the AutoScalingGroup.
    status code: 400, request id: 0bc126e5-1242-11e9-bdaa-31a801fb61d3
```
 
Option 1. --nodes=2:3:k8s-worker-asg-1. CA fails scaling down because it reach min of ASG. 
```
I0106 21:55:41.350524   50789 static_autoscaler.go:353] ip-192-168-89-16.us-west-2.compute.internal is unneeded since 2019-01-06 21:45:37.952451 -0800 PST m=+10.924306654 duration 10m3.171756787s
I0106 21:55:41.350633   50789 static_autoscaler.go:353] ip-192-168-63-220.us-west-2.compute.internal is unneeded since 2019-01-06 21:45:37.952451 -0800 PST m=+10.924306654 duration 10m3.171756787s
I0106 21:55:41.350653   50789 static_autoscaler.go:364] Scale down status: unneededOnly=false lastScaleUpTime=2019-01-06 21:45:27.948718 -0800 PST m=+0.920658372 lastScaleDownDeleteTime=2019-01-06 21:45:27.948718 -0800 PST m=+0.920658474 lastScaleDownFailTime=2019-01-06 21:45:27.948718 -0800 PST m=+0.920658593 scaleDownForbidden=false isDeleteInProgress=false
I0106 21:55:41.350675   50789 static_autoscaler.go:374] Starting scale down
I0106 21:55:41.350705   50789 scale_down.go:600] ip-192-168-63-220.us-west-2.compute.internal was unneeded for 10m3.171756787s
I0106 21:55:41.350727   50789 scale_down.go:600] ip-192-168-89-16.us-west-2.compute.internal was unneeded for 10m3.171756787s
I0106 21:55:41.350776   50789 scale_down.go:819] Scale-down: removing empty node ip-192-168-63-220.us-west-2.compute.internal
I0106 21:55:41.350896   50789 factory.go:33] Event(v1.ObjectReference{Kind:""ConfigMap"", Namespace:""kube-system"", Name:""cluster-autoscaler-status"", UID:""704067f2-123f-11e9-bbab-06ef894b7ba6"", APIVersion:""v1"", ResourceVersion:""47291"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDownEmpty' Scale-down: removing empty node ip-192-168-63-220.us-west-2.compute.internal
I0106 21:55:41.411900   50789 delete.go:64] Successfully added toBeDeletedTaint on node ip-192-168-63-220.us-west-2.compute.internal
E0106 21:55:41.830057   50789 scale_down.go:869] Problem with empty node deletion: failed to delete ip-192-168-63-220.us-west-2.compute.internal: ValidationError: Currently, desiredSize equals minSize (3). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
    status code: 400, request id: de1b03cd-1240-11e9-9b00-739c82dfa8b4
E0106 21:55:41.832861   50789 static_autoscaler.go:396] Failed to scale down: failed to delete at least one empty node: failed to delete ip-192-168-63-220.us-west-2.compute.internal: ValidationError: Currently, desiredSize equals minSize (3). Terminating instance without replacement will violate group's min size constraint. Either set shouldDecrementDesiredCapacity flag to false or lower group's min size.
    status code: 400, request id: de1b03cd-1240-11e9-9b00-739c82dfa8b4
I0106 21:55:41.871769   50789 delete.go:120] Releasing taint {Key:ToBeDeletedByClusterAutoscaler Value:1546840541 Effect:NoSchedule TimeAdded:<nil>} on node ip-192-168-63-220.us-west-2.compute.internal
I0106 21:55:41.901328   50789 delete.go:139] Successfully released toBeDeletedTaint on node ip-192-168-63-220.us-west-2.compute.internal
```

Option 3.  --nodes=10:20:k8s-worker-asg-1.  min >  ASG.MaxSize   
Option 4. --nodes=1:2:k8s-worker-asg-1.   max < ASG.MinSize
Have above issues on scaling up & scaling down. 




",closed,False,2019-01-07 06:58:31,2019-04-03 20:40:17
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1560,https://api.github.com/repos/kubernetes/autoscaler/issues/1560,Add interface for OOM observer.,,closed,True,2019-01-07 15:41:21,2019-01-08 09:00:27
autoscaler,raffaelespazzoli,https://github.com/kubernetes/autoscaler/issues/1561,https://api.github.com/repos/kubernetes/autoscaler/issues/1561,vertical-pod-autoscaler: make the admission controller self registration optional,"currently the `vpa-admission-controller` self register itself as an admission controller to the master API.
As a consequence the vpa-admission-controller service account must have enough permissions to do so.
Security aware organization may have a problem with this set-up as the `vpa-admission-controller` main duty is not to register new admission controller it is just to set correct resources for pod being created. 
I suggest to make it optional to have the `vpa-admission-controller` self-registering  and allow a path where the vpa-admission-controller is registered as a part of the installation process. 
This would allow for better separation of duties and least privilege, making the entire deployment more secure. ",open,False,2019-01-07 15:54:05,2019-03-05 11:18:31
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1562,https://api.github.com/repos/kubernetes/autoscaler/issues/1562,Add GPU Optimization Proposal,"Proposal for GPU Optimization in Cluster Autoscaler. 

I noticed there're several issues #1367  #1135 talking about problems to support GPU and I'd like to make some change to better support this use case. The issues have been discussed in the original issue ticket but lack of implementation. 

Please feel free to leave comments and let me know if this makes sense. ",closed,True,2019-01-07 18:31:16,2019-04-05 17:00:28
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1563,https://api.github.com/repos/kubernetes/autoscaler/issues/1563,Extend backoff interface with NodeInfo and error information,,closed,True,2019-01-08 17:09:33,2019-01-09 12:27:54
autoscaler,elithrar,https://github.com/kubernetes/autoscaler/pull/1564,https://api.github.com/repos/kubernetes/autoscaler/issues/1564,GCE CA: use public selfLink URL format.,"- gcloud commands & APIs return ""www.googleapis.com"", not
""content.googleapis.com"" as the selfLink. This documentation & error message
change makes this more obvious.
- Improve checks & tests around valid expectedResource names.

---

Example output that end-users see:

```
➜  gcloud compute instance-groups describe gke-your-first-cluster-1-pool-1-4f8729cc-grp --zone=us-central1-a
creationTimestamp: '2019-01-08T10:05:47.710-08:00'
description: ""This instance group is controlled by Instance Group Manager 'gke-your-first-cluster-1-pool-1-4f8729cc-grp'.\
  \ To modify instances in this group, use the Instance Group Manager API: https://cloud.google.com/compute/docs/reference/latest/instanceGroupManagers""
fingerprint: 42WmSpB8rSM=
id: '5710522466650642580'
kind: compute#instanceGroup
name: gke-your-first-cluster-1-pool-1-4f8729cc-grp
network: https://www.googleapis.com/compute/v1/projects/silverlock-sandbox/global/networks/default
selfLink: https://www.googleapis.com/compute/v1/projects/silverlock-sandbox/zones/us-central1-a/instanceGroups/gke-your-first-cluster-1-pool-1-4f8729cc-grp
size: 1
subnetwork: https://www.googleapis.com/compute/v1/projects/silverlock-sandbox/regions/us-central1/subnetworks/default
zone: https://www.googleapis.com/compute/v1/projects/silverlock-sandbox/zones/us-central1-a
```",closed,True,2019-01-08 22:03:38,2019-02-04 10:46:56
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/issues/1565,https://api.github.com/repos/kubernetes/autoscaler/issues/1565,Do not store NodeGroup object in ScaleUpRequest/ScaleDownRequest,"We recently changed the `ScaleUp/DownRequest` objects to hold reference to `NodeGroup`.
This is not a good idea as we do not guarantee that `NodeGroup` objects created by cloud provider are valid after alogorithm loop completes. We should get back to storing `NodeGroup` id instead and gracefully handle situation when node group for given id is no longer available.",open,False,2019-01-09 12:12:57,2019-01-09 12:13:12
autoscaler,DeliangFan,https://github.com/kubernetes/autoscaler/issues/1566,https://api.github.com/repos/kubernetes/autoscaler/issues/1566,Add enableEvict flag for cluster autoscaler,"For scale down progress, we may need to disable evict for some condition. When enableEvict is disable, CA only need to taint the node. As long as the task finish, delete the empty taint node. ",open,False,2019-01-09 12:19:58,2019-01-11 11:00:52
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1567,https://api.github.com/repos/kubernetes/autoscaler/issues/1567,Use model abstractions in OomInfo,,closed,True,2019-01-09 12:58:46,2019-01-10 09:23:01
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1568,https://api.github.com/repos/kubernetes/autoscaler/issues/1568,OOM observer implements ResourceEventHandler,@schylek ,closed,True,2019-01-10 09:54:46,2019-01-10 10:06:58
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/1569,https://api.github.com/repos/kubernetes/autoscaler/issues/1569,Release Cluster Autoscaler 1.12.2,"To go with Kubernetes 1.12.5, we'll need to cut it on Monday, January 14th.

If you believe something should be included that isn't yet merged to cluster-autoscaler-release-1.12 branch, please link a PR with cherry-pick here. Short description of why it's needed would be welcome.

cc @feiskyer @ringtail @johanneswuerbach @Jeffwan ",closed,False,2019-01-10 10:40:48,2019-01-25 03:26:18
autoscaler,hello2mao,https://github.com/kubernetes/autoscaler/pull/1570,https://api.github.com/repos/kubernetes/autoscaler/issues/1570,Add some contact details for Baiducloud cluster-autoscaler,"**What this PR does / why we need it:**
Add some contact details for Baiducloud.",closed,True,2019-01-11 01:11:18,2019-01-11 13:40:40
autoscaler,prageethw,https://github.com/kubernetes/autoscaler/issues/1571,https://api.github.com/repos/kubernetes/autoscaler/issues/1571,EKS CA spin up too many instances than it really required and unable to scale in once scaled out,"1. I'm currently running 5 applications, installed using helm

```
helm list

aws-cluster-autoscaler	1       	Fri Jan 11 23:59:58 2019	DEPLOYED	cluster-autoscaler-0.11.0  	1.13.1     	aws-cluster-autoscaler
external-dns          	1       	Sat Jan 12 00:00:12 2019	DEPLOYED	external-dns-1.3.0         	0.5.9      	external-dns
grafana               	1       	Sat Jan 12 00:03:44 2019	DEPLOYED	grafana-1.21.4             	5.4.2      	metrics
kubernetes-dashboard  	1       	Sat Jan 12 00:00:24 2019	DEPLOYED	kubernetes-dashboard-0.10.2	1.10.1     	kube-system
metrics-server        	1       	Fri Jan 11 23:59:47 2019	DEPLOYED	metrics-server-2.0.4       	0.3.1      	metrics
prometheus            	1       	Sat Jan 12 00:00:43 2019	DEPLOYED	prometheus-8.4.1           	2.6.0      	metrics
```

2. I can run all these in KOPS cluster with CA barely with 2 t2.small instances with CA enabled.

3. When I run this in EKS it claims for at least 5 t2.small instances.

here are the logs from CA.

```bash
 1 scale_down.go:376] Scale-down calculation: ignoring 2 nodes unremovable in the last 5m0s
I0111 15:11:58.902447       1 scale_down.go:407] Node ip-192-168-75-121.us-east-2.compute.internal - utilization 0.520000
I0111 15:11:58.902457       1 scale_down.go:407] Node ip-192-168-43-198.us-east-2.compute.internal - utilization 0.420000
I0111 15:11:58.902464       1 scale_down.go:407] Node ip-192-168-3-189.us-east-2.compute.internal - utilization 0.525000
I0111 15:11:58.902563       1 scale_down.go:456] Finding additional 3 candidates for scale down.
I0111 15:11:58.902597       1 cluster.go:90] Fast evaluation: ip-192-168-75-121.us-east-2.compute.internal for removal
I0111 15:11:58.902604       1 cluster.go:104] Fast evaluation: node ip-192-168-75-121.us-east-2.compute.internal cannot be removed: pod with local storage present: aws-cluster-autoscaler-94fff4774-9lgf2
I0111 15:11:58.902610       1 cluster.go:90] Fast evaluation: ip-192-168-43-198.us-east-2.compute.internal for removal
I0111 15:11:58.902616       1 cluster.go:104] Fast evaluation: node ip-192-168-43-198.us-east-2.compute.internal cannot be removed: non-daemonset, non-mirrored, non-pdb-assigned kube-system pod present: kubernetes-dashboard-75576f5d59-99stw
I0111 15:11:58.902620       1 cluster.go:90] Fast evaluation: ip-192-168-3-189.us-east-2.compute.internal for removal
I0111 15:11:58.902626       1 cluster.go:104] Fast evaluation: node ip-192-168-3-189.us-east-2.compute.internal cannot be removed: non-daemonset, non-mirrored, non-pdb-assigned kube-system pod present: tiller-deploy-85744d9bfb-wr2bb



I0111 15:08:17.480324       1 cluster.go:222] Evaluation ip-192-168-75-121.us-east-2.compute.internal for metrics/prometheus-pushgateway-86cd6bc5d5-kqczv -> PodFitsResources predicate mismatch, reason: Insufficient pods
I0111 15:08:17.480346       1 cluster.go:225] Pod metrics/prometheus-pushgateway-86cd6bc5d5-kqczv can be moved to ip-192-168-63-133.us-east-2.compute.internal
I0111 15:08:17.480363       1 cluster.go:222] Evaluation ip-192-168-75-121.us-east-2.compute.internal for metrics/prometheus-server-858dbbd8b6-ksx96 -> PodFitsResources predicate mismatch, reason: Insufficient pods
I0111 15:08:17.480379       1 scheduler_binder.go:532] All bound volumes for Pod ""metrics/prometheus-server-858dbbd8b6-ksx96"" match with Node ""ip-192-168-63-133.us-east-2.compute.internal""
I0111 15:08:17.480460       1 cluster.go:222] Evaluation ip-192-168-63-133.us-east-2.compute.internal for metrics/prometheus-server-858dbbd8b6-ksx96 -> NoVolumeZoneConflict predicate mismatch, reason: node(s) had no available volume zone
I0111 15:08:17.480469       1 cluster.go:222] Evaluation ip-192-168-43-198.us-east-2.compute.internal for metrics/prometheus-server-858dbbd8b6-ksx96 -> PodFitsResources predicate mismatch, reason: Insufficient pods
I0111 15:08:17.480476       1 cluster.go:222] Evaluation ip-192-168-3-189.us-east-2.compute.internal for metrics/prometheus-server-858dbbd8b6-ksx96 -> PodFitsResources predicate mismatch, reason: Insufficient pods
I0111 15:08:17.480482       1 cluster.go:126] Fast evaluation: node ip-192-168-23-76.us-east-2.compute.internal is not suitable for removal: failed to find place for metrics/prometheus-server-858dbbd8b6-ksx96

```
4. my CA helm instruction.

```bash
helm install stable/cluster-autoscaler \
    --name aws-cluster-autoscaler \
    --namespace aws-cluster-autoscaler \
    --version 0.11.0 \
    --set autoDiscovery.clusterName=$NAME \
    --set awsRegion=$AWS_DEFAULT_REGION \
    --set sslCertPath=/etc/kubernetes/pki/ca.crt \
    --set rbac.create=true \
    --set autoscalingGroupsnamePrefix[0].maxSize=10 \
    --set autoscalingGroupsnamePrefix[0].minSize=2 \
    --set autoscalingGroupsnamePrefix[0].name=$ASG_NAME \
    --set extraArgs.scale-down-delay-after-add=""5m0s"" \
    --set extraArgs.scale-down-unneeded-time=""5m0s"" \
    --set extraArgs.scale-down-unready-time=""10m0s"" \
    --set extraArgs.scale-down-utilization-threshold=0.6 \
    --set extraArgs.scan-interval=""20s"" \
    --set replicaCount=2 \
    --set resources.limits.cpu=""200m"",resources.limits.memory=""100Mi""

```



",closed,False,2019-01-11 15:19:12,2019-01-12 13:35:52
autoscaler,mgoodness,https://github.com/kubernetes/autoscaler/issues/1572,https://api.github.com/repos/kubernetes/autoscaler/issues/1572,VPA: allow running outside kube-system namespace,AFAICT there's no technical reason vertical-pod-autoscaler needs to run in `kube-system`. The only impediment today seems to be that the webhook self-registration has the namespace hardwired. May I submit a PR that allows the use of a `NAMESPACE` environment variable to override?,closed,False,2019-01-11 16:15:35,2019-01-14 19:30:48
autoscaler,mgoodness,https://github.com/kubernetes/autoscaler/pull/1573,https://api.github.com/repos/kubernetes/autoscaler/issues/1573,VPA: Use downward API to set namespace for webhook service reference,Closes #1572 ,closed,True,2019-01-11 20:15:48,2019-01-14 19:30:48
autoscaler,hjacobs,https://github.com/kubernetes/autoscaler/issues/1574,https://api.github.com/repos/kubernetes/autoscaler/issues/1574,Vertical Pod Autoscaler does not recover application from OOMKill loop,"I tried the Vertical Pod Autoscaler (VPA) with default settings and `updateMode: Auto` for more than a month and now have seen some disconcerning behavior: it reduced the memory requests/limits after some time and the application (which runs as a  single pod) went into `OOMKill` crashloop. Sadly the only way to recover was to set `minAllowed` for memory in the `VerticalPodAutoscaler` CRD.

Here the memory graph (usage + requests/limits) which shows the pod unavailable with `OOMKill` (it does not show OOM, but that's what `kubectl describe pod ..` showed me):

![kubernetes-vpa-oom](https://user-images.githubusercontent.com/510328/51061264-fd73bb80-15f2-11e9-8c7d-b1088263ad5c.png)

More details and context in my blog post: https://srcco.de/posts/one-month-of-kubernetes-vertical-pod-autoscaler-vpa.html

It would be great if the VPA could detect such `OOMKill` death situations and recover from it (by slowly (?) increasing memory again).

Anybody else has seen this behavior and could explain what was happening and/or if this general problem (no automatic recovery from OOM) is known?

/cc @arjunrn",closed,False,2019-01-11 21:48:51,2019-02-01 16:12:08
autoscaler,JoeWrightss,https://github.com/kubernetes/autoscaler/pull/1575,https://api.github.com/repos/kubernetes/autoscaler/issues/1575,Fix some typos in comment,"1. ""scheduable"" to ""schedulable"".
2. ""SynchornizedBeforeSuite"" to ""SynchronizedBeforeSuite"".",closed,True,2019-01-12 16:00:35,2019-01-15 08:47:48
autoscaler,prageethw,https://github.com/kubernetes/autoscaler/issues/1576,https://api.github.com/repos/kubernetes/autoscaler/issues/1576,Insufficient pods stops scaling-in confusing message-AWS EKS,"logs below

I0113 03:43:30.105993       1 cluster.go:222] Evaluation ip-192-168-75-46.us-east-2.compute.internal for test/tiller-deploy-7b584bf689-vzbn4 -> PodFitsResources predicate mismatch, reason: Insufficient pods
I0113 03:43:30.106018       1 cluster.go:222] Evaluation ip-192-168-84-242.us-east-2.compute.internal for test/tiller-deploy-7b584bf689-vzbn4 -> PodFitsResources predicate mismatch, reason: Insufficient pods
I0113 03:43:30.106038       1 cluster.go:222] Evaluation ip-192-168-50-191.us-east-2.compute.internal for test/tiller-deploy-7b584bf689-vzbn4 -> PodFitsResources predicate mismatch, reason: Insufficient pods
I0113 03:43:30.106085       1 cluster.go:222] Evaluation ip-192-168-41-162.us-east-2.compute.internal for test/tiller-deploy-7b584bf689-vzbn4 -> PodFitsResources predicate mismatch, reason: Insufficient pods
I0113 03:43:30.106106       1 cluster.go:126] Fast evaluation: node ip-192-168-2-214.us-east-2.compute.internal is not suitable for removal: failed to find place for test/tiller-deploy-7b584bf689-vzbn4",open,False,2019-01-13 03:49:48,2019-01-29 10:10:02
autoscaler,AmitDaniel-wsc,https://github.com/kubernetes/autoscaler/issues/1577,https://api.github.com/repos/kubernetes/autoscaler/issues/1577,Autoscaler can't scale down node that is running heapster pod,"Hi all . 

I'm using AKS version : 1.11.5
I installed Autoscaler version 1.2.2 and i can see that the scaling down always keep 1 extra node cause the ""heapster"" pod is running on this node . 

**More info :** 

This is all the pods that are running on this node :
```
kubectl get all -o wide --all-namespaces | grep aks-agentpool-25462656-13


kube-system   pod/heapster-5d6f9b846c-5chxb               2/2     Running   0          4h    10.244.12.3    aks-agentpool-25462656-13   <none>
kube-system   pod/kube-proxy-4pp4d                        1/1     Running   0          4h    10.240.0.5     aks-agentpool-25462656-13   <none>
kube-system   pod/kube-svc-redirect-nlfvw                 2/2     Running   0          4h    10.240.0.5     aks-agentpool-25462656-13   <none>
kube-system   pod/omsagent-jqb68                          1/1     Running   0          4h    10.244.12.2    aks-agentpool-25462656-13   <none>
```

**Autoscaler logs :** 

```
I0114 07:36:30.071132       1 scale_down.go:257] Finding additional 1 candidates for scale down.
I0114 07:36:30.071176       1 cluster.go:80] Fast evaluation: aks-agentpool-25462656-13 for removal
I0114 07:36:30.071185       1 cluster.go:94] Fast evaluation: node aks-agentpool-25462656-13 cannot be removed: non-daemonset, non-mirrored, non-pdb-assigned kube-system pod present: heapster-5d6f9b846c-5chxb

```

**Autoscaler configmap :** 

```
kubectl get configmap cluster-autoscaler-status -n kube-system -o yaml

apiVersion: v1
data:
  status: |+
    Cluster-autoscaler status at 2019-01-14 07:41:02.124149262 +0000 UTC:
    Cluster-wide:
      Health:      Healthy (ready=2 unready=0 notStarted=0 longNotStarted=0 registered=2 longUnregistered=0)
                   LastProbeTime:      2019-01-14 07:41:00.902176609 +0000 UTC
                   LastTransitionTime: 2019-01-13 19:10:42.025144261 +0000 UTC
      ScaleUp:     NoActivity (ready=2 registered=2)
                   LastProbeTime:      2019-01-14 07:41:00.902176609 +0000 UTC
                   LastTransitionTime: 2019-01-14 02:13:58.13318201 +0000 UTC
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2019-01-14 07:41:00.902176609 +0000 UTC
                   LastTransitionTime: 2019-01-14 05:32:02.490131329 +0000 UTC

    NodeGroups:
      Name:        agentpool
      Health:      Healthy (ready=2 unready=0 notStarted=0 longNotStarted=0 registered=2 longUnregistered=0 cloudProviderTarget=2 (minSize=1, maxSize=100))
                   LastProbeTime:      2019-01-14 07:41:00.902176609 +0000 UTC
                   LastTransitionTime: 2019-01-14 03:02:29.08919697 +0000 UTC
      ScaleUp:     NoActivity (ready=2 cloudProviderTarget=2)
                   LastProbeTime:      2019-01-14 07:41:00.902176609 +0000 UTC
                   LastTransitionTime: 2019-01-14 03:02:29.08919697 +0000 UTC
      ScaleDown:   NoCandidates (candidates=0)
                   LastProbeTime:      2019-01-14 07:41:00.902176609 +0000 UTC
                   LastTransitionTime: 2019-01-14 05:32:02.490131329 +0000 UTC

kind: ConfigMap
metadata:
  annotations:
    cluster-autoscaler.kubernetes.io/last-updated: 2019-01-14 07:41:02.124149262 +0000
      UTC
  creationTimestamp: ""2019-01-02T14:32:14Z""
  name: cluster-autoscaler-status
  namespace: kube-system
  resourceVersion: ""2033346""
  selfLink: /api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status
  uid: 329f272b-0e9b-11e9-bc7f-92e377fe880f
```

Do ""Fix"" manually this issue i'm running :
`kubectl drain <NODE>`
Once the heapster pod is moving to the first node i'm running: 
` kubectl uncordon <NODE>`
 and after couple of mins the autoscaler is removing this node

Do you have any idea why it's happening ?

Thanks , 
Amit",closed,False,2019-01-14 07:53:25,2019-03-22 00:35:03
autoscaler,calixwu,https://github.com/kubernetes/autoscaler/issues/1578,https://api.github.com/repos/kubernetes/autoscaler/issues/1578,Why not to drain daemonset pods? ,"I read the source code about the scaling-down part, and I found the cluster-autoscaler never drain the daemonset pods when scaling down. Wouldn't this cause problems?",open,False,2019-01-14 09:46:10,2019-04-05 00:57:26
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1579,https://api.github.com/repos/kubernetes/autoscaler/issues/1579,Cluster Autoscaler 1.12.2,Update Cluster Autoscaler version to 1.12.2,closed,True,2019-01-14 12:18:25,2019-01-14 12:31:47
autoscaler,timv2,https://github.com/kubernetes/autoscaler/issues/1580,https://api.github.com/repos/kubernetes/autoscaler/issues/1580,Scaling up from 0 on EKS,"Hi,
I have the following setup
EKS with Kubernetes 1.11.5
Cluster autoscaler 1.3.5 with single ASG

I wonder,
since in EKS we have no access to a master node, we cannot use cluster autoscaler deployment on the master node. I'm wondering, when we scaled down to 0 and need to scale up, since there is no deployment, how will the cluster autoscaler ever know that there is reason to scale up?
I'm testing it, and it's not working.

A 2nd question, partly related, can we have multiple cluster autoscalers deployed (scaling up different node groups independently)?

Regards,
Tim",closed,False,2019-01-14 12:41:06,2019-01-15 13:49:43
autoscaler,arjunrn,https://github.com/kubernetes/autoscaler/issues/1581,https://api.github.com/repos/kubernetes/autoscaler/issues/1581,Nil deference in OOMObserver,"https://github.com/kubernetes/autoscaler/blob/7c629c1e1701ed11349c7162b0edabb755846177/vertical-pod-autoscaler/pkg/recommender/input/oom/observer.go#L151

There is a nil deference at this point with the stack trace:

```
Observed a panic: ""invalid memory address or nil pointer dereference"" (runtime error: invalid memory address or nil pointer dereference)
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:72
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:65
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:51
/usr/lib/go/src/runtime/asm_amd64.s:522
/usr/lib/go/src/runtime/panic.go:513
/usr/lib/go/src/runtime/panic.go:82
/usr/lib/go/src/runtime/signal_unix.go:390
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/pkg/recommender/input/oom/observer.go:151
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/client-go/tools/cache/controller.go:376
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/client-go/tools/cache/delta_fifo.go:444
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/client-go/tools/cache/controller.go:150
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/client-go/tools/cache/controller.go:124
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88
/home/username/go/src/k8s.io/autoscaler/vertical-pod-autoscaler/vendor/k8s.io/client-go/tools/cache/controller.go:124
/usr/lib/go/src/runtime/asm_amd64.s:1333
```

An initial look suggests that `Terminated` is a pointer type and it could not be set which causes the panic.

We see this occasionally in the logs and I could co-relate it with memory leak issues when the memory started growing immediately after this panic.",closed,False,2019-01-14 15:42:36,2019-01-16 12:42:25
autoscaler,tariq1890,https://github.com/kubernetes/autoscaler/pull/1582,https://api.github.com/repos/kubernetes/autoscaler/issues/1582,Replace the deprecated function calls of go-autorest,As per the Azure GO SDK docs `WaitForCompletion` is deprecated. This should be safe since `WaitForCompletion` calls `WaitForCompletionRef` anyway,closed,True,2019-01-15 00:38:32,2019-01-15 16:47:13
autoscaler,tariq1890,https://github.com/kubernetes/autoscaler/pull/1583,https://api.github.com/repos/kubernetes/autoscaler/issues/1583,Azure: update docs to ensure aks/aks-engine autoscaler users provide the resource group name correctly,"Amending docs as per the thread in #884 . For Cluster-Autoscaler to work, we need to ensure that resource groups match by case when provided in the secret.",closed,True,2019-01-15 05:27:23,2019-01-15 16:52:22
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1584,https://api.github.com/repos/kubernetes/autoscaler/issues/1584,Move GPULabel and GPUTypes to cloud provider,"This PR is to address #1135 

1. Add GPULabel() and GetAvailableGPUTypes() in cloudprodiver interface 
2. Caller will pass GPULabel to methods in gpu.go which original relies on GPULabel
3. Pass cloud.CloudProvider to methods in scale_up.go and scale_down.go that don't use GPULabel directly. 

Please don't merge this PR until I confirm with all cloud provider owners and check if they're ok about labels and supported GPU types. ",closed,True,2019-01-16 06:48:41,2019-03-26 02:01:18
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1585,https://api.github.com/repos/kubernetes/autoscaler/issues/1585,VPA - protection against malformed pod status in OomObserver,,closed,True,2019-01-16 11:36:54,2019-01-16 11:53:26
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1586,https://api.github.com/repos/kubernetes/autoscaler/issues/1586,Tainting unneeded nodes as PreferNoSchedule,,closed,True,2019-01-16 11:44:11,2019-01-16 12:10:59
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1587,https://api.github.com/repos/kubernetes/autoscaler/issues/1587,Tainting unneeded nodes as PreferNoSchedule,,closed,True,2019-01-16 12:09:46,2019-03-22 16:47:12
autoscaler,qw1mb0,https://github.com/kubernetes/autoscaler/issues/1588,https://api.github.com/repos/kubernetes/autoscaler/issues/1588,Updater not set resources request on pods,"I installed VPA 0.3.0 in a cluster 1.10.8 on two nodes and a master in GCE.

Have enabled MutatingAdmissionWebhook:
```
# kubectl -n kube-system get pod kube-apiserver-master-europe-west1-b-l6x1 -o yaml | grep MutatingAdmissionWebhook
      --cloud-provider=gce --enable-admission-plugins=Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,NodeRestriction,ResourceQuota
```
and enabled api:
```
# kubectl api-versions | grep admissionregistration.k8s.io
admissionregistration.k8s.io/v1beta1
```

And I wanted to check his work by applying such yaml:
```
apiVersion: ""autoscaling.k8s.io/v1beta1""
kind: VerticalPodAutoscaler
metadata:
  name: hamster-vpa
spec:
  selector:
    matchLabels:
      app: hamster
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hamster
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: hamster
    spec:
      containers:
      - name: hamster
        image: k8s.gcr.io/ubuntu-slim:0.1
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
          limits:
            cpu: 300m
            memory: 250Mi
        command: [""/bin/sh""]
        args: [""-c"", ""while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done""]
```

It's deployed to cluster:
```
# kubectl get vpa,pod
NAME                                                   AGE
verticalpodautoscaler.autoscaling.k8s.io/hamster-vpa   3h

NAME                           READY     STATUS    RESTARTS   AGE
pod/hamster-5c6f76c9bb-4lbwt   1/1       Running   0          2m
pod/hamster-5c6f76c9bb-h7tqm   1/1       Running   0          37s
```

Every 1m one of pod recreate and I see on **Updater** logs:

```
I0116 23:01:37.161863       5 update_priority_calculator.go:118] pod accepted for update hamster-5c6f76c9bb-h7tqm with priority 6
I0116 23:01:37.162488       5 update_priority_calculator.go:118] pod accepted for update hamster-5c6f76c9bb-4lbwt with priority 6
I0116 23:01:37.162567       5 updater.go:137] evicting pod hamster-5c6f76c9bb-h7tqm
I0116 23:01:37.270985       5 updater.go:195] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""hamster-5c6f76c9bb-h7tqm"", UID:""898279de-19e2-11e9-8af9-42010a840006"", APIVersion:""v1"", ResourceVersion:""32694081"", FieldPath:""""}): type: 'Normal' reason: 'EvictedByVPA' Pod was evicted by VPA Updater to apply resource recommendation.
```

See that vpa can fetch metrics seen on VPA object:
```
# kubectl describe vpa/hamster-vpa
Name:         hamster-vpa
Namespace:    default
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={""apiVersion"":""autoscaling.k8s.io/v1beta1"",""kind"":""VerticalPodAutoscaler"",""metadata"":{""annotations"":{},""name"":""hamster-vpa"",""namespace"":""default""},""spe...
API Version:  autoscaling.k8s.io/v1beta1
Kind:         VerticalPodAutoscaler
Metadata:
  Cluster Name:        
  Creation Timestamp:  2019-01-16T19:57:50Z
  Generation:          1
  Resource Version:    32694279
  Self Link:           /apis/autoscaling.k8s.io/v1beta1/namespaces/default/verticalpodautoscalers/hamster-vpa
  UID:                 011af3da-19c9-11e9-8af9-42010a840006
Spec:
  Selector:
    Match Labels:
      App:  hamster
Status:
  Conditions:
    Last Transition Time:  2019-01-16T19:59:11Z
    Status:                True
    Type:                  RecommendationProvided
  Recommendation:
    Container Recommendations:
      Container Name:  hamster
      Lower Bound:
        Cpu:     503m
        Memory:  262144k
      Target:
        Cpu:     671m
        Memory:  262144k
      Uncapped Target:
        Cpu:     671m
        Memory:  262144k
      Upper Bound:
        Cpu:     6200m
        Memory:  262144k
Events:          <none>

```

But every new pod have old resource request: 

```
# kubectl get pod
kNAME                       READY     STATUS    RESTARTS   AGE
hamster-5c6f76c9bb-4lbwt   1/1       Running   0          4m
hamster-5c6f76c9bb-7rmcz   1/1       Running   0          46s

# kubectl describe pod hamster-5c6f76c9bb-7rmcz
Name:           hamster-5c6f76c9bb-7rmcz
Namespace:      default
Node:           nodes-z27n/10.132.0.5
Start Time:     Wed, 16 Jan 2019 23:02:37 +0000
Labels:         app=hamster
                pod-template-hash=1729327566
Annotations:    <none>
Status:         Running
IP:             100.96.2.97
Controlled By:  ReplicaSet/hamster-5c6f76c9bb
Containers:
  hamster:
    Container ID:  docker://140bf6968b8cd041bea1587f0daf9a245e211d60b635d347fbcb038dade85429
    Image:         k8s.gcr.io/ubuntu-slim:0.1
    Image ID:      docker-pullable://k8s.gcr.io/ubuntu-slim@sha256:b6f8c3885f5880a4f1a7cf717c07242eb4858fdd5a84b5ffe35b1cf680ea17b1
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
    Args:
      -c
      while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done
    State:          Running
      Started:      Wed, 16 Jan 2019 23:02:38 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     300m
      memory:  250Mi
    Requests:
      cpu:        100m
      memory:     50Mi
```

And i not see some annotations on these pods.

Why VPA not working for me? Helm please.",closed,False,2019-01-16 23:05:33,2019-01-17 16:31:31
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1589,https://api.github.com/repos/kubernetes/autoscaler/issues/1589,Consider GPU utilization in scaling down,"Address #1367 

1. Consider GPU resource in utilization calculation. 
2. Add separate flag for GPU Utilization Threshold. 
3. Add UTs and also cover the case that annotate GPU pods to prevent eviction. (Training use case)",closed,True,2019-01-17 00:24:58,2019-04-04 21:44:27
autoscaler,calixwu,https://github.com/kubernetes/autoscaler/issues/1590,https://api.github.com/repos/kubernetes/autoscaler/issues/1590,Masters should be considered for reschedulable pod when scaling down,"I read the source codes about the scaling-down part. I found that the autoscaler filter out masters when it ""FindNodesToRemove"" at ""TryScaleDown"" call:
```
nodesToRemove, _, _, err := simulator.FindNodesToRemove(candidates, nodesWithoutMaster, nonExpendablePods, sd.context.ListerRegistry,
		sd.context.PredicateChecker, 1, false,
		sd.podLocationHints, sd.usageTracker, time.Now(), pdbs)
```

I think it should pass ""allNodes"" instead of ""nodesWithoutMaster"", as some people may allow master node to be schedulable for normal pods.",open,False,2019-01-17 10:05:35,2019-01-18 02:20:42
autoscaler,gjasny,https://github.com/kubernetes/autoscaler/issues/1591,https://api.github.com/repos/kubernetes/autoscaler/issues/1591,Binary in docker image of cluster-autoscaler:v1.3.5 does not match source code,"I tried to use cluster-autoscaler version 1.3.5 with the Docker image at `gcr.io/google-containers/cluster-autoscaler:v1.3.5` on AWS. I got strange error messages saying it tried to call `DescribeLaunchConfigurations` and the `LaunchConfigurations` parameter was an empty list (or list of an empty string). This is weird because the auto scaling groups it complained about use LaunchTemplates, not LaunchConfigurations.

I used gdb to break at the error and saw that the `type aws` did only contain the `LaunchConfiguration` field, but none of the `LaunchTemplate*` fields that are present in this structure in the source code at tag `cluster-autoscaler-1.3.5`. That means the binary in the v1.3.5 Docker image was not built from the code at tag 1.3.5. Is it possible that an old version of the cluster-autoscaler binary was used to build the v.1.3.5 Docker image by mistake?

I manually built the binary and container from tag `cluster-autoscaler-1.3.5` and this works.",open,False,2019-01-17 13:28:48,2019-01-18 17:34:34
autoscaler,ankrause,https://github.com/kubernetes/autoscaler/issues/1592,https://api.github.com/repos/kubernetes/autoscaler/issues/1592,Azure VMSS scale fails when the scale set includes user-assigned identities,"CloudProvider: Azure (VMSS)
Kubernetes version: 1.13.1
CA Version: v1.13.1

When running the Cluster Autoscaler in a K8S cluster using VMSS, scale operations fail if the target VMSS instance contains user-assigned identities. AAD Pod Identity will add this type of identity on behalf of the user if configured, but there may also be other scenarios.

```
I0118 01:29:49.048745       1 scale_up.go:427] Best option to resize: k8s-linuxpool1-xxxxxx-vmss
I0118 01:29:49.048839       1 scale_up.go:431] Estimated 2 nodes needed in k8s-linuxpool1-xxxxxx-vmss
I0118 01:29:49.048925       1 scale_up.go:533] Final scale-up plan: [{k8s-linuxpool1-xxxxxx-vmss 2->4 (max: 20)}]
I0118 01:29:49.049095       1 scale_up.go:690] Scale-up: setting group k8s-linuxpool1-xxxxxx-vmss size to 4
E0118 01:29:49.367722       1 azure_scale_set.go:153] virtualMachineScaleSetsClient.CreateOrUpdate for scale set ""k8s-linuxpool1-xxxxxx-vmss"" failed: compute.VirtualMachineScaleSetsClient#CreateOrUpdate: Failure sending request: StatusCode=0 -- Original Error: autorest/azure: Service returned an error. Status=400 Code=""InvalidIdentityValues"" Message=""Invalid value for the identities '/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.ManagedIdentity/userAssignedIdentities/<identity1-name>,/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.ManagedIdentity/userAssignedIdentities/<identity2-name>'. The 'UserAssignedIdentities' property keys should only be empty json objects or null.""
W0118 01:29:49.367813       1 clusterstate.go:221] Disabling scale-up for node group k8s-linuxpool1-xxxxxx-vmss until 2019-01-18 01:34:49.367807633 +0000 UTC m=+2939.586049944
E0118 01:29:49.367858       1 static_autoscaler.go:312] Failed to scale up: failed to increase node group size: compute.VirtualMachineScaleSetsClient#CreateOrUpdate: Failure sending request: StatusCode=0 -- Original Error: autorest/azure: Service returned an error. Status=400 Code=""InvalidIdentityValues"" Message=""Invalid value for the identities '/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.ManagedIdentity/userAssignedIdentities/<identity1-name>,/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.ManagedIdentity/userAssignedIdentities/<identity2-name>'. The 'UserAssignedIdentities' property keys should only be empty json objects or null.""
```

This can be resolved by nulling the identities property of the VMSS object before sending the CreateOrUpdate request in azure_scale_set.go, similar to the ProvisioningState in latest source.

```
op.Sku.Capacity = &size
op.Identity = nil
op.VirtualMachineScaleSetProperties.ProvisioningState = nil
updateCtx, updateCancel := getContextWithCancel()
defer updateCancel()
_, err = scaleSet.manager.azClient.virtualMachineScaleSetsClient.CreateOrUpdate(updateCtx, resourceGroup, scaleSet.Name, op)
if err != nil {
    klog.Errorf(""virtualMachineScaleSetsClient.CreateOrUpdate for scale set %q failed: %v"", scaleSet.Name, err)
    return err
}
```

I will try to send a patch PR referencing this issue today.",closed,False,2019-01-18 01:54:01,2019-01-27 08:32:11
autoscaler,ankrause,https://github.com/kubernetes/autoscaler/issues/1593,https://api.github.com/repos/kubernetes/autoscaler/issues/1593,Cluster Autoscaler cannot parse Windows VMAS agent pool created by AKS-Engine,"CloudProvider: Azure
Kubernetes version: 1.13.1
Cluster Autoscaler version: 1.13.1
AKS-Engine version: 0.28.1 and 0.29.1

I deployed a hybrid cluster to Azure using AKS-Engine with VMAS-based agent pools and small initial node counts (2 and 2). The resulting VM names followed the format {4 digits}k8s{3 digits}, where the last digit was the instance number which could presumably grow beyond a single digit. When evaluating scale options, the Cluster Autoscaler attempted to parse those machine names using the regex:

```
k8sWindowsVMNamingFormat               = ""^([a-fA-F0-9]{5})([0-9a-zA-Z]{3})([a-zA-Z0-9]{4,6})$""
```

Since the regex doesn't match against the generated VM name, it returned an error indicating the resource was missing an identifier:

```
vmNameParts := vmnameWindowsRegexp.FindStringSubmatch(vmName)
if len(vmNameParts) != 4 {
    return """", """", -1, -1, fmt.Errorf(""resource name was missing from identifier"")
}
```

Hopefully this is the right repository for this issue.",closed,False,2019-01-18 03:58:24,2019-02-01 12:20:29
autoscaler,devkid,https://github.com/kubernetes/autoscaler/pull/1594,https://api.github.com/repos/kubernetes/autoscaler/issues/1594,Cherry pick to backport support for AWS EC2 Launch Templates onto 1.3 release branch to support Launch Templates with Kubernetes 1.11,This feature is included in cluster-autoscaler starting from 1.12.,closed,True,2019-01-18 13:36:50,2019-03-22 17:14:43
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1595,https://api.github.com/repos/kubernetes/autoscaler/issues/1595,Add VPA FAQ,TLA FTW,closed,True,2019-01-18 16:01:20,2019-01-21 11:25:49
autoscaler,vivekbagade,https://github.com/kubernetes/autoscaler/pull/1596,https://api.github.com/repos/kubernetes/autoscaler/issues/1596,Added better checks for filterSchedulablePods and added a tunable fla…,"1) Modified the filterSchedulablePods logic.
2) Added a flag exclude-schedulable-pods to the logic.

WIP:
1) Correct one failing test cases affected by new logic. (https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/core/static_autoscaler_test.go#L561)
2) Add a test case or two for the new logic.",closed,True,2019-01-18 19:03:14,2019-01-27 21:00:32
autoscaler,netiperher,https://github.com/kubernetes/autoscaler/issues/1597,https://api.github.com/repos/kubernetes/autoscaler/issues/1597,Azure AKS cluster autoscaler doesn't support bulk removal of empty nodes,"CloudProvider: Azure AKS
Kubernetes version: 1.11.5
Cluster Autoscaler version: 1.3.5 and current master
ACS-Engine version: v0.26.3-aks

In our setup we run an Azure AKS cluster with multiple nodes that each run a single pod requiring all resources on the node. The pods are part of a statefulset that we scale quite aggressively from 2 to 20 and then back to 2 replicas at regular intervals.

When scaling down we observe unexpected behaviours in the cluster autoscaler, where initially nodes are removed quickly but then mysteriously are added back a few minutes later.

I believe this is caused by `scheduleDeleteEmptyNodes` bulk removing nodes in parallel. And `DeleteNodesInternal` in `azure_container_service_pool.go` decreasing the AKS cluster size with only one instead of actual number of nodes in the bulk set, since the function is called multiple times in parallel with a list of only a single VM each time.

The effect is that scaling down from 20 to 2 nodes takes about 1.5 to 2 hours for us. A workaround is setting `--max-empty-bulk-delete=1` (default: 10) since this decreases the scale down to 30 minutes, but running in parallel should be much faster.",closed,False,2019-01-20 10:38:41,2019-03-11 21:20:38
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1598,https://api.github.com/repos/kubernetes/autoscaler/issues/1598,"Tainting nodes - update first, refresh on conflict",Depend on https://github.com/kubernetes/autoscaler/pull/1587,closed,True,2019-01-21 17:40:57,2019-03-22 16:47:27
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1599,https://api.github.com/repos/kubernetes/autoscaler/issues/1599,Refactor tests of tainting,"Depends on https://github.com/kubernetes/autoscaler/pull/1598

/assign @losipiuk
/assign @aleksandra-malinowska
/assign @MaciekPytel

Please review only last commit but do not merge as it depend on other PR
/hold",closed,True,2019-01-22 09:17:50,2019-03-22 16:47:17
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1600,https://api.github.com/repos/kubernetes/autoscaler/issues/1600,Cherry pick: More flexible VPA certificate configuration #1534,,closed,True,2019-01-22 12:24:10,2019-01-23 12:20:39
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1601,https://api.github.com/repos/kubernetes/autoscaler/issues/1601,Update default priority cutoff value in FAQ.md,"/assign MaciekPytel
/assign aleksandra-malinowska",closed,True,2019-01-22 15:27:11,2019-03-22 16:47:00
autoscaler,anoopl,https://github.com/kubernetes/autoscaler/issues/1602,https://api.github.com/repos/kubernetes/autoscaler/issues/1602,[Azure AKS] Not scaling up with error autoscaler.go:283] Failed to scale up: Could not compute total resources: No node info for: nodepool1,"AKS Kubernetes Version: 1.11.5
CA Vesrion: 1.3.5

Initially I had only 2 nodes and the minimum size was 3. It scaled up to 3 nodes successfully.
But it's not scaling up after this. I have maximum nodes 10

Yaml used:
https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/cluster-autoscaler-containerservice.yaml
Few log lines:

I0122 22:54:44.226319       1 utils.go:503] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0122 22:54:44.226736       1 scale_up.go:249] Pod default/nginx-759fc54bbb-pj4zh is unschedulable
I0122 22:54:44.226745       1 scale_up.go:249] Pod default/nginx-759fc54bbb-b468p is unschedulable
I0122 22:54:44.226748       1 scale_up.go:249] Pod default/nginx-759fc54bbb-49j2g is unschedulable
E0122 22:54:44.558783       1 static_autoscaler.go:283] Failed to scale up: Could not compute total resources: No node info for: nodepool1
W0122 22:54:44.559048       1 clusterstate.go:321] Failed to find readiness information for nodepool1
W0122 22:54:44.559061       1 clusterstate.go:377] Failed to find readiness information for nodepool1
W0122 22:54:44.559066       1 clusterstate.go:321] Failed to find readiness information for nodepool1
I0122 22:54:54.597354       1 azure_manager.go:261] Refreshed ASG list, next refresh after 2019-01-22 22:55:54.597343724 +0000 UTC m=+1344.575177577
W0122 22:54:55.047510       1 clusterstate.go:377] Failed to find readiness information for nodepool1
W0122 22:54:55.047546       1 clusterstate.go:539] Readiness for node group nodepool1 not found
I0122 22:54:55.047592       1 utils.go:503] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0122 22:54:55.047962       1 scale_up.go:249] Pod default/nginx-759fc54bbb-pj4zh is unschedulable
I0122 22:54:55.047970       1 scale_up.go:249] Pod default/nginx-759fc54bbb-49j2g is unschedulable
I0122 22:54:55.047985       1 scale_up.go:249] Pod default/nginx-759fc54bbb-b468p is unschedulable
E0122 22:54:55.450115       1 static_autoscaler.go:283] Failed to scale up: Could not compute total resources: No node info for: nodepool1
W0122 22:54:55.450426       1 clusterstate.go:321] Failed to find readiness information for nodepool1
W0122 22:54:55.452424       1 clusterstate.go:377] Failed to find readiness information for nodepool1
W0122 22:54:55.452434       1 clusterstate.go:321] Failed to find readiness information for nodepool1
",closed,False,2019-01-22 23:05:40,2019-02-07 05:23:02
autoscaler,kawych,https://github.com/kubernetes/autoscaler/pull/1603,https://api.github.com/repos/kubernetes/autoscaler/issues/1603,"Remove kawych, piosz and x13n from Addon Resizer OWNERS",,closed,True,2019-01-23 09:09:04,2019-02-01 14:46:35
autoscaler,ringtail,https://github.com/kubernetes/autoscaler/issues/1604,https://api.github.com/repos/kubernetes/autoscaler/issues/1604,Support recycling mode node group ,"In Alibaba Cloud. There is a kind of node group that can recycle nodes without deleting them. These nodes can be stopped without any payment and start at any time you need. But if the ECS inventory is empty. These nodes will be deleted from node group and replace by a new one. So if we use recycling mode node group with CA. Some nodes will be never deleted  because they are `notReady` and not in any node group.  So is there any similar scene in other clouds.If it is common, I'd like to make a pr to support it.",closed,False,2019-01-23 09:11:31,2019-01-24 02:56:34
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1605,https://api.github.com/repos/kubernetes/autoscaler/issues/1605,Cherry pick: Use downward API to set namespace for webhook service reference #1573,,closed,True,2019-01-23 12:24:23,2019-01-24 15:32:06
autoscaler,mrIncompetent,https://github.com/kubernetes/autoscaler/pull/1606,https://api.github.com/repos/kubernetes/autoscaler/issues/1606,Add flag to configure the prometheus job name to be used when fetching the history data,,closed,True,2019-01-23 13:25:07,2019-01-29 14:59:41
autoscaler,glasser,https://github.com/kubernetes/autoscaler/issues/1607,https://api.github.com/repos/kubernetes/autoscaler/issues/1607,"""it wouldn't fit if a new node is added"" scale-up failure message can be misleading","As the FAQ says, there are several reasons that scale-up might not happen:

>CA doesn't add nodes to the cluster if it wouldn't make a pod schedulable. It will only consider adding nodes to node groups for which it was configured. So one of the reasons it doesn't scale up the cluster may be that the pod has too large (e.g. 100 CPUs), or too specific requests (like node selector), and wouldn't fit on any of the available node types. Another possible reason is that all suitable node groups are already at their maximum size.

When the ""all suitable node groups are already at their maximum size"" case occurs, the failure message (eg, in the NotTriggerScaleUp event on the pod) says `pod didn't trigger scale-up (it wouldn't fit if a new node is added)`.  (This is based on having just experienced this on GKE with a pod that wants a certain type of node that's only in one node pool which is at its maximum size.)

This appears to be working as intended according to the FAQ, but I'd argue that the failure message is misleading — unless you've read the FAQ, it leads you in the direction of ""make the pod smaller"", not ""raise the limit on the node group"".

If there is at least one group at its maximum size, the message could say ""a node group is at its maximum size, and the pod wouldn't fit if a new node is added to other node groups"".  (For even better results, it could give a count of maxed-out node groups, and only include the second half of the phrase if there are any non-maxed-out-node groups.)

If the code structure is such that this information is difficult to surface when determining the message, the message could just always say ""it wouldn't fit if a new node is added to any node group not at its maximum size"" or something, which I think would still be a bit of an improvement.",closed,False,2019-01-23 18:49:25,2019-01-23 23:39:10
autoscaler,suker200,https://github.com/kubernetes/autoscaler/issues/1608,https://api.github.com/repos/kubernetes/autoscaler/issues/1608,Azure InstanceType case sensitive,"Hi, 
I met an issue relate to case sensitive instance type in Azure response. 

https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/azure_scale_set.go#L429
https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/azure_instance_types.go#L640

The value of ""*template.Sku.Name"" is : Standard_D8_V3 
But the key of ""InstanceTypes"" is: Standard_D8_v3

It made cluster autoscaler failed. 

I have workaround add 
```
	""Standard_D8_v3"": {
		InstanceType: ""Standard_D8_v3"",
		VCPU:         8,
		MemoryMb:     32768,
		GPU:          0,
	},
	""Standard_D8_V3"": {
		InstanceType: ""Standard_D8_V3"",
		VCPU:         8,
		MemoryMb:     32768,
		GPU:          0,
	},
```
to ""InstanceTypes"" and cluster-autoscaler works as normal
",open,False,2019-01-24 08:58:30,2019-02-01 05:17:30
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1609,https://api.github.com/repos/kubernetes/autoscaler/issues/1609,Cherry-pick: VPA - protection against malformed pod status in OomObserver #1585,,closed,True,2019-01-24 15:57:26,2019-01-30 10:09:09
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1610,https://api.github.com/repos/kubernetes/autoscaler/issues/1610,Use k8s.io/klog instead github.com/golang/glog,,closed,True,2019-01-24 21:38:32,2019-02-04 10:53:44
autoscaler,JinAirsOs,https://github.com/kubernetes/autoscaler/issues/1611,https://api.github.com/repos/kubernetes/autoscaler/issues/1611,cluster-autoscaler-release-1.12 make build failed,"when i run it on mac os,
$make build
rm -f cluster-autoscaler
go get github.com/tools/godep
GOOS=linux godep go build ./...
vendor/github.com/google/cadvisor/accelerators/nvidia.go:30:2: build constraints exclude all Go files in /Users/kerry/go/src/k8s.io/autoscaler/cluster-autoscaler/vendor/github.com/mindprince/gonvml
godep: go exit status 1
make: *** [build] Error 1",open,False,2019-01-25 03:22:56,2019-01-25 11:52:17
autoscaler,yuriwoof,https://github.com/kubernetes/autoscaler/issues/1612,https://api.github.com/repos/kubernetes/autoscaler/issues/1612,"[Azure,AKS] All agent node deleted from autoscaler using v1.3.4","Hi, my customer found all agent node in AKS cluster (k8s version is 1.11.5) was deleted.
I confirm that last node was delete from autoscaler though operation log in Azure side.

```
Go/go1.10.2 (amd64-linux) go-autorest/v10.12.0 Azure-SDK-For-Go/v14.6.0 compute/2017-12-01; cluster-autoscaler/v0.0.0-master+$Format:%h$
```

I cannot reproduce this issue, but could I ask how to mitigate this issue (e.g. use another version, etc)?
Following is manifest file.

```yaml
---
apiVersion: v1
kind: Secret
metadata:
    name: cluster-autoscaler-azure
    namespace: kube-system
data:
    ClientID: xxx
    ClientSecret: xxx
    ResourceGroup: xxx
    SubscriptionID: xxx
    TenantID: xxx
    VMType: xxx
    ClusterName: xxx
    NodeResourceGroup: xxx
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""""]
  resources: [""events"",""endpoints""]
  verbs: [""create"", ""patch""]
- apiGroups: [""""]
  resources: [""pods/eviction""]
  verbs: [""create""]
- apiGroups: [""""]
  resources: [""pods/status""]
  verbs: [""update""]
- apiGroups: [""""]
  resources: [""endpoints""]
  resourceNames: [""cluster-autoscaler""]
  verbs: [""get"",""update""]
- apiGroups: [""""]
  resources: [""nodes""]
  verbs: [""watch"",""list"",""get"",""update""]
- apiGroups: [""""]
  resources: [""pods"",""services"",""replicationcontrollers"",""persistentvolumeclaims"",""persistentvolumes""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""extensions""]
  resources: [""replicasets"",""daemonsets""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""policy""]
  resources: [""poddisruptionbudgets""]
  verbs: [""watch"",""list""]
- apiGroups: [""apps""]
  resources: [""statefulsets""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""storage.k8s.io""]
  resources: [""storageclasses""]
  verbs: [""get"", ""list"", ""watch""]
 
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""""]
  resources: [""configmaps""]
  verbs: [""create""]
- apiGroups: [""""]
  resources: [""configmaps""]
  resourceNames: [""cluster-autoscaler-status""]
  verbs: [""delete"",""get"",""update""]
 
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system
 
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system
 
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - image: gcr.io/google-containers/cluster-autoscaler:v1.3.4
        imagePullPolicy: Always
        name: cluster-autoscaler
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 300Mi
        command:
        - ./cluster-autoscaler
        - --v=3
        - --logtostderr=true
        - --cloud-provider=azure
        - --skip-nodes-with-local-storage=false
        - --nodes=1:32:nodepool1
        env:
        - name: ARM_SUBSCRIPTION_ID
          valueFrom:
            secretKeyRef:
              key: SubscriptionID
              name: cluster-autoscaler-azure
        - name: ARM_RESOURCE_GROUP
          valueFrom:
            secretKeyRef:
              key: ResourceGroup
              name: cluster-autoscaler-azure
        - name: ARM_TENANT_ID
          valueFrom:
            secretKeyRef:
              key: TenantID
              name: cluster-autoscaler-azure
        - name: ARM_CLIENT_ID
          valueFrom:
            secretKeyRef:
              key: ClientID
              name: cluster-autoscaler-azure
        - name: ARM_CLIENT_SECRET
          valueFrom:
            secretKeyRef:
              key: ClientSecret
              name: cluster-autoscaler-azure
        - name: ARM_VM_TYPE
          valueFrom:
            secretKeyRef:
              key: VMType
              name: cluster-autoscaler-azure
        - name: AZURE_CLUSTER_NAME
          valueFrom:
            secretKeyRef:
              key: ClusterName
              name: cluster-autoscaler-azure
        - name: AZURE_NODE_RESOURCE_GROUP
          valueFrom:
            secretKeyRef:
              key: NodeResourceGroup
              name: cluster-autoscaler-azure
      restartPolicy: Always
---
```





 ",closed,False,2019-01-25 06:53:29,2019-02-15 08:06:14
autoscaler,safanaj,https://github.com/kubernetes/autoscaler/pull/1613,https://api.github.com/repos/kubernetes/autoscaler/issues/1613,Some configurable parameters,"To use VPA on AWS EKS I needed to change the port for the admission controller webhook, so I would propose to have a flag for it.

For the recommender to be able to get historical metrics from prometheus I need to tune the metrics and labels to query prometheus properly, so I would add a bunch of flags for that.

This PR is just to share the ideas.",closed,True,2019-01-25 07:18:47,2019-02-26 08:46:16
autoscaler,drewhemm,https://github.com/kubernetes/autoscaler/issues/1614,https://api.github.com/repos/kubernetes/autoscaler/issues/1614,cluster-autoscaler.kubernetes.io/safe-to-evict=true annotation not working,"I am using the `cluster-autoscaler.kubernetes.io/safe-to-evict=true` annotation, but I am still seeing messages like this in the autoscaler logs:

```
I0125 11:23:10.132575       1 cluster.go:89] Fast evaluation: node ip-192-168-##-##.eu-west-1.compute.internal cannot be removed: kube-system/kube-proxy-rbpck is not replicated
```

```
$ kubectl describe pod kube-proxy-6smv5 -n kube-system                           
Name:               kube-proxy-6smv5
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               ip-192-168-###-###.eu-west-1.compute.internal/192.168.###.###
Start Time:         Fri, 25 Jan 2019 10:56:01 +0000
Labels:             controller-revision-hash=165802934
                    k8s-app=kube-proxy
                    pod-template-generation=2
Annotations:        cluster-autoscaler.kubernetes.io/safe-to-evict=true
Status:             Running
```

Am I doing something wrong?

_Originally posted by @drewhemm in https://github.com/kubernetes/autoscaler/issues/351#issuecomment-457542906_",closed,False,2019-01-25 11:28:25,2019-01-25 16:14:21
autoscaler,BenBlaisdell,https://github.com/kubernetes/autoscaler/issues/1615,https://api.github.com/repos/kubernetes/autoscaler/issues/1615,Invalid memory address error after moving to EC2 Launch Templates from Launch Configurations,"CA version: 1.13.1
CA helm chart version: 0.11.2
Kubernetes version: 1.11.5

Error happens when trying to scale up with autoscaling groups using launch templates

Error:
```
I0124 20:55:33.619234       1 flags.go:52] FLAG: --address="":8085""
I0124 20:55:33.619269       1 flags.go:52] FLAG: --alsologtostderr=""false""
I0124 20:55:33.619275       1 flags.go:52] FLAG: --application-metrics-count-limit=""100""
I0124 20:55:33.619280       1 flags.go:52] FLAG: --azure-container-registry-config=""""
I0124 20:55:33.619329       1 flags.go:52] FLAG: --balance-similar-node-groups=""false""
I0124 20:55:33.619337       1 flags.go:52] FLAG: --boot-id-file=""/proc/sys/kernel/random/boot_id""
I0124 20:55:33.619351       1 flags.go:52] FLAG: --cloud-config=""""
I0124 20:55:33.619358       1 flags.go:52] FLAG: --cloud-provider=""aws""
I0124 20:55:33.619380       1 flags.go:52] FLAG: --cloud-provider-gce-lb-src-cidrs=""130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16""
I0124 20:55:33.619394       1 flags.go:52] FLAG: --cluster-name=""""
I0124 20:55:33.619398       1 flags.go:52] FLAG: --container-hints=""/etc/cadvisor/container_hints.json""
I0124 20:55:33.619402       1 flags.go:52] FLAG: --containerd=""unix:///var/run/containerd.sock""
I0124 20:55:33.619407       1 flags.go:52] FLAG: --cores-total=""0:320000""
I0124 20:55:33.619411       1 flags.go:52] FLAG: --docker=""unix:///var/run/docker.sock""
I0124 20:55:33.619416       1 flags.go:52] FLAG: --docker-env-metadata-whitelist=""""
I0124 20:55:33.619420       1 flags.go:52] FLAG: --docker-only=""false""
I0124 20:55:33.619424       1 flags.go:52] FLAG: --docker-root=""/var/lib/docker""
I0124 20:55:33.619428       1 flags.go:52] FLAG: --docker-tls=""false""
I0124 20:55:33.619432       1 flags.go:52] FLAG: --docker-tls-ca=""ca.pem""
I0124 20:55:33.619436       1 flags.go:52] FLAG: --docker-tls-cert=""cert.pem""
I0124 20:55:33.619440       1 flags.go:52] FLAG: --docker-tls-key=""key.pem""
I0124 20:55:33.619444       1 flags.go:52] FLAG: --enable-load-reader=""false""
I0124 20:55:33.619448       1 flags.go:52] FLAG: --estimator=""binpacking""
I0124 20:55:33.619453       1 flags.go:52] FLAG: --event-storage-age-limit=""default=0""
I0124 20:55:33.619458       1 flags.go:52] FLAG: --event-storage-event-limit=""default=0""
I0124 20:55:33.619462       1 flags.go:52] FLAG: --expander=""random""
I0124 20:55:33.619467       1 flags.go:52] FLAG: --expendable-pods-priority-cutoff=""0""
I0124 20:55:33.619472       1 flags.go:52] FLAG: --gke-api-endpoint=""""
I0124 20:55:33.619476       1 flags.go:52] FLAG: --global-housekeeping-interval=""1m0s""
I0124 20:55:33.619481       1 flags.go:52] FLAG: --google-json-key=""""
I0124 20:55:33.619486       1 flags.go:52] FLAG: --gpu-total=""[]""
I0124 20:55:33.619491       1 flags.go:52] FLAG: --housekeeping-interval=""10s""
I0124 20:55:33.619495       1 flags.go:52] FLAG: --httptest.serve=""""
I0124 20:55:33.619520       1 flags.go:52] FLAG: --kubeconfig=""""
I0124 20:55:33.619537       1 flags.go:52] FLAG: --kubernetes=""""
I0124 20:55:33.619547       1 flags.go:52] FLAG: --leader-elect=""true""
I0124 20:55:33.619555       1 flags.go:52] FLAG: --leader-elect-lease-duration=""15s""
I0124 20:55:33.619571       1 flags.go:52] FLAG: --leader-elect-renew-deadline=""10s""
I0124 20:55:33.619582       1 flags.go:52] FLAG: --leader-elect-resource-lock=""endpoints""
I0124 20:55:33.619588       1 flags.go:52] FLAG: --leader-elect-retry-period=""2s""
I0124 20:55:33.619592       1 flags.go:52] FLAG: --log-backtrace-at="":0""
I0124 20:55:33.619599       1 flags.go:52] FLAG: --log-cadvisor-usage=""false""
I0124 20:55:33.619605       1 flags.go:52] FLAG: --log-dir=""""
I0124 20:55:33.619609       1 flags.go:52] FLAG: --log-flush-frequency=""5s""
I0124 20:55:33.619614       1 flags.go:52] FLAG: --logtostderr=""true""
I0124 20:55:33.619619       1 flags.go:52] FLAG: --machine-id-file=""/etc/machine-id,/var/lib/dbus/machine-id""
I0124 20:55:33.619627       1 flags.go:52] FLAG: --max-autoprovisioned-node-group-count=""15""
I0124 20:55:33.619632       1 flags.go:52] FLAG: --max-empty-bulk-delete=""10""
I0124 20:55:33.619636       1 flags.go:52] FLAG: --max-failing-time=""15m0s""
I0124 20:55:33.619641       1 flags.go:52] FLAG: --max-graceful-termination-sec=""600""
I0124 20:55:33.619646       1 flags.go:52] FLAG: --max-inactivity=""10m0s""
I0124 20:55:33.619650       1 flags.go:52] FLAG: --max-node-provision-time=""15m0s""
I0124 20:55:33.619655       1 flags.go:52] FLAG: --max-nodes-total=""0""
I0124 20:55:33.619660       1 flags.go:52] FLAG: --max-total-unready-percentage=""45""
I0124 20:55:33.619665       1 flags.go:52] FLAG: --memory-total=""0:6400000""
I0124 20:55:33.619670       1 flags.go:52] FLAG: --min-replica-count=""0""
I0124 20:55:33.619675       1 flags.go:52] FLAG: --namespace=""kube-system""
I0124 20:55:33.619679       1 flags.go:52] FLAG: --node-autoprovisioning-enabled=""false""
I0124 20:55:33.619684       1 flags.go:52] FLAG: --node-group-auto-discovery=""[asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/my-super-cool-eks-cluster]""
I0124 20:55:33.619694       1 flags.go:52] FLAG: --nodes=""[]""
I0124 20:55:33.619699       1 flags.go:52] FLAG: --ok-total-unready-count=""3""
I0124 20:55:33.619703       1 flags.go:52] FLAG: --regional=""false""
I0124 20:55:33.619708       1 flags.go:52] FLAG: --scale-down-candidates-pool-min-count=""50""
I0124 20:55:33.619730       1 flags.go:52] FLAG: --scale-down-candidates-pool-ratio=""0.1""
I0124 20:55:33.619737       1 flags.go:52] FLAG: --scale-down-delay-after-add=""10m0s""
I0124 20:55:33.619748       1 flags.go:52] FLAG: --scale-down-delay-after-delete=""10s""
I0124 20:55:33.619754       1 flags.go:52] FLAG: --scale-down-delay-after-failure=""3m0s""
I0124 20:55:33.619764       1 flags.go:52] FLAG: --scale-down-enabled=""true""
I0124 20:55:33.619770       1 flags.go:52] FLAG: --scale-down-non-empty-candidates-count=""30""
I0124 20:55:33.619782       1 flags.go:52] FLAG: --scale-down-unneeded-time=""10m0s""
I0124 20:55:33.619793       1 flags.go:52] FLAG: --scale-down-unready-time=""20m0s""
I0124 20:55:33.619803       1 flags.go:52] FLAG: --scale-down-utilization-threshold=""0.5""
I0124 20:55:33.619810       1 flags.go:52] FLAG: --scan-interval=""10s""
I0124 20:55:33.619815       1 flags.go:52] FLAG: --skip-nodes-with-local-storage=""true""
I0124 20:55:33.619819       1 flags.go:52] FLAG: --skip-nodes-with-system-pods=""true""
I0124 20:55:33.619824       1 flags.go:52] FLAG: --stderrthreshold=""0""
I0124 20:55:33.619829       1 flags.go:52] FLAG: --storage-driver-buffer-duration=""1m0s""
I0124 20:55:33.619834       1 flags.go:52] FLAG: --storage-driver-db=""cadvisor""
I0124 20:55:33.619839       1 flags.go:52] FLAG: --storage-driver-host=""localhost:8086""
I0124 20:55:33.619844       1 flags.go:52] FLAG: --storage-driver-password=""root""
I0124 20:55:33.619848       1 flags.go:52] FLAG: --storage-driver-secure=""false""
I0124 20:55:33.619853       1 flags.go:52] FLAG: --storage-driver-table=""stats""
I0124 20:55:33.619857       1 flags.go:52] FLAG: --storage-driver-user=""root""
I0124 20:55:33.619862       1 flags.go:52] FLAG: --test.bench=""""
I0124 20:55:33.619866       1 flags.go:52] FLAG: --test.benchmem=""false""
I0124 20:55:33.619871       1 flags.go:52] FLAG: --test.benchtime=""1s""
I0124 20:55:33.619876       1 flags.go:52] FLAG: --test.blockprofile=""""
I0124 20:55:33.619880       1 flags.go:52] FLAG: --test.blockprofilerate=""1""
I0124 20:55:33.619885       1 flags.go:52] FLAG: --test.count=""1""
I0124 20:55:33.619889       1 flags.go:52] FLAG: --test.coverprofile=""""
I0124 20:55:33.619894       1 flags.go:52] FLAG: --test.cpu=""""
I0124 20:55:33.619898       1 flags.go:52] FLAG: --test.cpuprofile=""""
I0124 20:55:33.619903       1 flags.go:52] FLAG: --test.failfast=""false""
I0124 20:55:33.619907       1 flags.go:52] FLAG: --test.list=""""
I0124 20:55:33.619912       1 flags.go:52] FLAG: --test.memprofile=""""
I0124 20:55:33.619916       1 flags.go:52] FLAG: --test.memprofilerate=""0""
I0124 20:55:33.619921       1 flags.go:52] FLAG: --test.mutexprofile=""""
I0124 20:55:33.619925       1 flags.go:52] FLAG: --test.mutexprofilefraction=""1""
I0124 20:55:33.619929       1 flags.go:52] FLAG: --test.outputdir=""""
I0124 20:55:33.619933       1 flags.go:52] FLAG: --test.parallel=""2""
I0124 20:55:33.619949       1 flags.go:52] FLAG: --test.run=""""
I0124 20:55:33.619955       1 flags.go:52] FLAG: --test.short=""false""
I0124 20:55:33.619966       1 flags.go:52] FLAG: --test.testlogfile=""""
I0124 20:55:33.619973       1 flags.go:52] FLAG: --test.timeout=""0s""
I0124 20:55:33.619982       1 flags.go:52] FLAG: --test.trace=""""
I0124 20:55:33.619988       1 flags.go:52] FLAG: --test.v=""false""
I0124 20:55:33.619999       1 flags.go:52] FLAG: --v=""4""
I0124 20:55:33.620007       1 flags.go:52] FLAG: --version=""false""
I0124 20:55:33.620014       1 flags.go:52] FLAG: --vmodule=""""
I0124 20:55:33.620025       1 flags.go:52] FLAG: --write-status-configmap=""true""
I0124 20:55:33.620033       1 main.go:311] Cluster Autoscaler 1.3.2-beta.2
I0124 20:55:33.636324       1 leaderelection.go:185] attempting to acquire leader lease  kube-system/cluster-autoscaler...
I0124 20:55:33.639574       1 leaderelection.go:253] lock is held by cluster-autoscaler-aws-cluster-autoscaler-67b754646-wrxk5 and has not yet expired
I0124 20:55:33.639586       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0124 20:55:36.640531       1 leaderelection.go:253] lock is held by cluster-autoscaler-aws-cluster-autoscaler-67b754646-wrxk5 and has not yet expired
I0124 20:55:36.640546       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0124 20:55:39.694661       1 leaderelection.go:253] lock is held by cluster-autoscaler-aws-cluster-autoscaler-67b754646-wrxk5 and has not yet expired
I0124 20:55:39.694676       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0124 20:55:41.802660       1 leaderelection.go:253] lock is held by cluster-autoscaler-aws-cluster-autoscaler-67b754646-wrxk5 and has not yet expired
I0124 20:55:41.802675       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0124 20:55:44.016743       1 leaderelection.go:253] lock is held by cluster-autoscaler-aws-cluster-autoscaler-67b754646-wrxk5 and has not yet expired
I0124 20:55:44.016759       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0124 20:55:46.318392       1 leaderelection.go:253] lock is held by cluster-autoscaler-aws-cluster-autoscaler-67b754646-wrxk5 and has not yet expired
I0124 20:55:46.318407       1 leaderelection.go:190] failed to acquire lease kube-system/cluster-autoscaler
I0124 20:55:48.877539       1 leaderelection.go:194] successfully acquired lease kube-system/cluster-autoscaler
I0124 20:55:48.877596       1 factory.go:33] Event(v1.ObjectReference{Kind:""Endpoints"", Namespace:""kube-system"", Name:""cluster-autoscaler"", UID:""60df55e6-b09a-11e8-a23b-0a4c9a97d754"", APIVersion:""v1"", ResourceVersion:""43771684"", FieldPath:""""}): type: 'Normal' reason: 'LeaderElection' cluster-autoscaler-aws-cluster-autoscaler-67b754646-cqn8b became leader
I0124 20:55:48.881314       1 predicates.go:126] Using predicate PodFitsResources
I0124 20:55:48.881327       1 predicates.go:126] Using predicate GeneralPredicates
I0124 20:55:48.881332       1 predicates.go:126] Using predicate PodToleratesNodeTaints
I0124 20:55:48.881336       1 predicates.go:126] Using predicate CheckNodeCondition
I0124 20:55:48.881341       1 predicates.go:126] Using predicate MatchInterPodAffinity
I0124 20:55:48.881350       1 predicates.go:126] Using predicate MaxAzureDiskVolumeCount
I0124 20:55:48.881354       1 predicates.go:126] Using predicate CheckNodeDiskPressure
I0124 20:55:48.881359       1 predicates.go:126] Using predicate MaxEBSVolumeCount
I0124 20:55:48.881363       1 predicates.go:126] Using predicate MaxGCEPDVolumeCount
I0124 20:55:48.881368       1 predicates.go:126] Using predicate NoVolumeZoneConflict
I0124 20:55:48.881372       1 predicates.go:126] Using predicate CheckNodePIDPressure
I0124 20:55:48.881376       1 predicates.go:126] Using predicate CheckVolumeBinding
I0124 20:55:48.881383       1 predicates.go:126] Using predicate NoDiskConflict
I0124 20:55:48.881389       1 predicates.go:126] Using predicate ready
I0124 20:55:48.881394       1 predicates.go:126] Using predicate CheckNodeMemoryPressure
I0124 20:55:48.881490       1 reflector.go:202] Starting reflector *v1.Node (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.881512       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.882199       1 reflector.go:202] Starting reflector *v1.PersistentVolumeClaim (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.882209       1 reflector.go:240] Listing and watching *v1.PersistentVolumeClaim from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.882468       1 reflector.go:202] Starting reflector *v1.ReplicationController (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.882478       1 reflector.go:240] Listing and watching *v1.ReplicationController from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.882728       1 reflector.go:202] Starting reflector *v1.Service (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.882736       1 reflector.go:240] Listing and watching *v1.Service from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.883395       1 reflector.go:202] Starting reflector *v1beta1.ReplicaSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.883406       1 reflector.go:240] Listing and watching *v1beta1.ReplicaSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.883749       1 reflector.go:202] Starting reflector *v1beta1.StatefulSet (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.883760       1 reflector.go:240] Listing and watching *v1beta1.StatefulSet from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.884060       1 reflector.go:202] Starting reflector *v1beta1.PodDisruptionBudget (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.884067       1 reflector.go:240] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.884343       1 reflector.go:202] Starting reflector *v1.StorageClass (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.884352       1 reflector.go:240] Listing and watching *v1.StorageClass from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.884598       1 reflector.go:202] Starting reflector *v1.PersistentVolume (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.884608       1 reflector.go:240] Listing and watching *v1.PersistentVolume from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.885311       1 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149
I0124 20:55:48.885321       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149
I0124 20:55:48.885495       1 reflector.go:202] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I0124 20:55:48.885510       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:174
I0124 20:55:48.885623       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212
I0124 20:55:48.885629       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:212
I0124 20:55:48.885713       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I0124 20:55:48.885723       1 reflector.go:240] Listing and watching *v1.Node from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:239
I0124 20:55:48.885809       1 reflector.go:202] Starting reflector *v1beta1.PodDisruptionBudget (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0124 20:55:48.885814       1 reflector.go:240] Listing and watching *v1beta1.PodDisruptionBudget from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:266
I0124 20:55:48.885937       1 reflector.go:202] Starting reflector *v1beta1.DaemonSet (1h0m0s) from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I0124 20:55:48.885953       1 reflector.go:240] Listing and watching *v1beta1.DaemonSet from k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:293
I0124 20:55:48.887522       1 reflector.go:202] Starting reflector *v1.Pod (0s) from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:48.887541       1 reflector.go:240] Listing and watching *v1.Pod from k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/informers/factory.go:130
I0124 20:55:49.081671       1 request.go:485] Throttling request took 194.063405ms, request: GET:https://172.20.0.1:443/api/v1/pods?limit=500&resourceVersion=0
I0124 20:55:49.286305       1 request.go:485] Throttling request took 380.764299ms, request: POST:https://172.20.0.1:443/api/v1/namespaces/kube-system/configmaps
I0124 20:55:49.296553       1 cloud_provider_builder.go:72] Building aws cloud provider.
I0124 20:55:49.513499       1 auto_scaling_groups.go:245] Regenerating instance to ASG map for ASGs: [my-super-cool-eks-cluster-kiam-agent20190124194811783200000011 my-super-cool-eks-cluster-kiam-server20190124194811791100000014 my-super-cool-eks-cluster-kiam2019012317025967540000000d my-super-cool-eks-cluster-private2019012317025967450000000c my-super-cool-eks-cluster-private20190124194811787700000012 my-super-cool-eks-cluster-public2019012317025967410000000b my-super-cool-eks-cluster-public20190124194811791000000013 my-super-cool-eks-cluster-spot2019012317024925190000000a my-super-cool-eks-cluster-spot20190124194811779900000010]
I0124 20:55:49.724510       1 auto_scaling_groups.go:119] Registering ASG my-super-cool-eks-cluster-kiam-agent20190124194811783200000011
I0124 20:55:49.724541       1 auto_scaling_groups.go:119] Registering ASG my-super-cool-eks-cluster-kiam-server20190124194811791100000014
I0124 20:55:49.724549       1 auto_scaling_groups.go:119] Registering ASG my-super-cool-eks-cluster-kiam2019012317025967540000000d
I0124 20:55:49.724561       1 auto_scaling_groups.go:119] Registering ASG my-super-cool-eks-cluster-private2019012317025967450000000c
I0124 20:55:49.724569       1 auto_scaling_groups.go:119] Registering ASG my-super-cool-eks-cluster-private20190124194811787700000012
I0124 20:55:49.724575       1 auto_scaling_groups.go:119] Registering ASG my-super-cool-eks-cluster-public2019012317025967410000000b
I0124 20:55:49.724590       1 auto_scaling_groups.go:119] Registering ASG my-super-cool-eks-cluster-public20190124194811791000000013
I0124 20:55:49.724603       1 auto_scaling_groups.go:119] Registering ASG my-super-cool-eks-cluster-spot2019012317024925190000000a
I0124 20:55:49.724610       1 auto_scaling_groups.go:119] Registering ASG my-super-cool-eks-cluster-spot20190124194811779900000010
I0124 20:55:49.724618       1 aws_manager.go:126] Refreshed ASG list, next refresh after 2019-01-24 20:55:59.724615039 +0000 UTC m=+26.282444809
I0124 20:55:49.728200       1 main.go:239] Registered cleanup signal handler
I0124 20:55:50.885138       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0124 20:55:52.900143       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0124 20:55:54.908035       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0124 20:55:56.915693       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0124 20:55:58.923435       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0124 20:55:59.756500       1 static_autoscaler.go:131] Starting main loop
I0124 20:55:59.920298       1 auto_scaling_groups.go:245] Regenerating instance to ASG map for ASGs: [my-super-cool-eks-cluster-kiam-agent20190124194811783200000011 my-super-cool-eks-cluster-kiam-server20190124194811791100000014 my-super-cool-eks-cluster-kiam2019012317025967540000000d my-super-cool-eks-cluster-private2019012317025967450000000c my-super-cool-eks-cluster-private20190124194811787700000012 my-super-cool-eks-cluster-public2019012317025967410000000b my-super-cool-eks-cluster-public20190124194811791000000013 my-super-cool-eks-cluster-spot2019012317024925190000000a my-super-cool-eks-cluster-spot20190124194811779900000010]
I0124 20:56:00.070165       1 aws_manager.go:126] Refreshed ASG list, next refresh after 2019-01-24 20:56:10.070158526 +0000 UTC m=+36.627988323
I0124 20:56:00.070379       1 utils.go:503] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0124 20:56:00.070393       1 static_autoscaler.go:244] Filtering out schedulables
I0124 20:56:00.070887       1 static_autoscaler.go:254] No schedulable pods
I0124 20:56:00.070913       1 scale_up.go:249] Pod tst/dummy-pod-79f9499b8f-mkctr is unschedulable
I0124 20:56:00.070922       1 scale_up.go:249] Pod stg/dummy-pod-769f9f67c4-kf9dj is unschedulable
W0124 20:56:00.175214       1 aws_manager.go:216] Found multiple availability zones, using us-west-2a
I0124 20:56:00.328916       1 request.go:485] Throttling request took 147.769745ms, request: PUT:https://172.20.0.1:443/api/v1/namespaces/kube-system/configmaps/cluster-autoscaler-status
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x134867f]

goroutine 58 [running]:
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).buildNodeFromTemplate(0xc421863140, 0xc4221a7260, 0xc42216c600, 0x0, 0x0, 0x9)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:243 +0x3df
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsNodeGroup).TemplateNodeInfo(0xc420acb340, 0xc4211f4300, 0xc4221d3980, 0x35)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider.go:303 +0x68
k8s.io/autoscaler/cluster-autoscaler/core.GetNodeInfosForGroups(0xc421510e40, 0x7, 0x8, 0x3b70c40, 0xc4221ee870, 0x3b92fe0, 0xc4209400f0, 0xc42218a400, 0x7, 0x8, ...)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/utils.go:258 +0x296
k8s.io/autoscaler/cluster-autoscaler/core.ScaleUp(0xc4220f96c0, 0xc421062d00, 0xc422322a00, 0xc4209527d0, 0x2, 0x2, 0xc421510e40, 0x7, 0x8, 0xc42218a400, ...)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:253 +0x3ef
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc4221a5cc0, 0xbf0aa777eb6f75c5, 0x61eccfe88, 0x5c9ef20, 0x0, 0x0)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:278 +0x1d86
main.run(0xc420b0ae10)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:288 +0x48d
main.main.func2(0xc4200aacc0)
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:369 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
    /gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:155 +0x92
```

Deployed using Helm
Manifests:
```
---
# Source: cluster-autoscaler/templates/podsecuritypolicy.yaml
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged-cluster-autoscaler-aws-cluster-autoscaler
spec:
  allowedCapabilities:
  - '*'
  fsGroup:
    rule: RunAsAny
  privileged: true
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - '*'
  hostPID: true
  hostIPC: true
  hostNetwork: true
  hostPorts:
  - min: 1
    max: 65536
---
# Source: cluster-autoscaler/templates/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  labels:
    app: aws-cluster-autoscaler
    chart: cluster-autoscaler-0.11.2
    heritage: Tiller
    release: cluster-autoscaler
  name: cluster-autoscaler-aws-cluster-autoscaler
spec:
  selector:
    matchLabels:
      app: aws-cluster-autoscaler
      release: cluster-autoscaler
  maxUnavailable: 1
  # minAvailable: 2
---
# Source: cluster-autoscaler/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: aws-cluster-autoscaler
    chart: cluster-autoscaler-0.11.2
    heritage: Tiller
    release: cluster-autoscaler
  name: cluster-autoscaler-aws-cluster-autoscaler
---
# Source: cluster-autoscaler/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    app: aws-cluster-autoscaler
    chart: cluster-autoscaler-0.11.2
    heritage: Tiller
    release: cluster-autoscaler
  name: cluster-autoscaler-aws-cluster-autoscaler
rules:
  - apiGroups:
      - """"
    resources:
      - events
      - endpoints
    verbs:
      - create
      - patch
  - apiGroups:
    - """"
    resources:
    - pods/eviction
    verbs:
    - create
  - apiGroups:
      - """"
    resources:
      - pods/status
    verbs:
      - update
  - apiGroups:
      - """"
    resources:
      - endpoints
    resourceNames:
      - cluster-autoscaler
    verbs:
      - get
      - update
  - apiGroups:
      - """"
    resources:
      - nodes
    verbs:
    - watch
    - list
    - get
    - update
  - apiGroups:
    - """"
    resources:
      - pods
      - services
      - replicationcontrollers
      - persistentvolumeclaims
      - persistentvolumes
    verbs:
      - watch
      - list
      - get
  - apiGroups:
    - batch
    resources:
      - jobs
      - cronjobs
    verbs:
      - watch
      - list
      - get
  - apiGroups:
      - extensions
    resources:
      - replicasets
      - daemonsets
    verbs:
      - watch
      - list
      - get
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - watch
      - list
  - apiGroups:
    - apps
    resources:
    - replicasets
    - statefulsets
    verbs:
    - watch
    - list
    - get
  - apiGroups:
    - storage.k8s.io
    resources:
    - storageclasses
    verbs:
    - watch
    - list
    - get
  - apiGroups:
    - extensions
    resources:
    - podsecuritypolicies
    resourceNames:
    - privileged-cluster-autoscaler-aws-cluster-autoscaler
    verbs:
    - use
---
# Source: cluster-autoscaler/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    app: aws-cluster-autoscaler
    chart: cluster-autoscaler-0.11.2
    heritage: Tiller
    release: cluster-autoscaler
  name: cluster-autoscaler-aws-cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler-aws-cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler-aws-cluster-autoscaler
    namespace: kube-system
---
# Source: cluster-autoscaler/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  labels:
    app: aws-cluster-autoscaler
    chart: cluster-autoscaler-0.11.2
    heritage: Tiller
    release: cluster-autoscaler
  name: cluster-autoscaler-aws-cluster-autoscaler
rules:
  - apiGroups:
      - """"
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - """"
    resources:
      - configmaps
    resourceNames:
      - cluster-autoscaler-status
    verbs:
      - delete
      - get
      - update
---
# Source: cluster-autoscaler/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  labels:
    app: aws-cluster-autoscaler
    chart: cluster-autoscaler-0.11.2
    heritage: Tiller
    release: cluster-autoscaler
  name: cluster-autoscaler-aws-cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler-aws-cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler-aws-cluster-autoscaler
    namespace: kube-system
---
# Source: cluster-autoscaler/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: aws-cluster-autoscaler
    chart: cluster-autoscaler-0.11.2
    heritage: Tiller
    release: cluster-autoscaler
  name: cluster-autoscaler-aws-cluster-autoscaler
spec:
  clusterIP: """"
  ports:
    - port: 8085
      protocol: TCP
      targetPort: 8085
      name: http
  selector:
    app: aws-cluster-autoscaler
    release: cluster-autoscaler
  type: ""ClusterIP""
---
# Source: cluster-autoscaler/templates/deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: aws-cluster-autoscaler
    chart: cluster-autoscaler-0.11.2
    heritage: Tiller
    release: cluster-autoscaler
  name: cluster-autoscaler-aws-cluster-autoscaler
spec:
  replicas: 1
  template:
    metadata:
      annotations:
        iam.amazonaws.com/role: cluster-autoscaler

      labels:
        app: aws-cluster-autoscaler
        release: cluster-autoscaler
    spec:
      containers:
        - name: aws-cluster-autoscaler
          image: ""k8s.gcr.io/cluster-autoscaler:v1.3.2-beta.2""
          imagePullPolicy: ""IfNotPresent""
          command:
            - ./cluster-autoscaler
            - --cloud-provider=aws
            - --namespace=kube-system
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/my-super-cool-eks-cluster
            - --logtostderr=true
            - --stderrthreshold=info
            - --v=4

          env:
            - name: AWS_REGION
              value: ""us-west-2""
          livenessProbe:
            httpGet:
              path: /health-check
              port: 8085
          ports:
            - containerPort: 8085
          resources:
            {}

          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/pki/ca-trust/extracted/pem
              readOnly: true
      serviceAccountName: cluster-autoscaler-aws-cluster-autoscaler
      tolerations:
        []

      volumes:
        - name: ssl-certs
          hostPath:
            path: /etc/pki/ca-trust/extracted/pem
```
",open,False,2019-01-25 18:12:14,2019-04-05 05:52:16
autoscaler,resouer,https://github.com/kubernetes/autoscaler/issues/1616,https://api.github.com/repos/kubernetes/autoscaler/issues/1616,Considering Making Cluster Autoscaler Extensible,"Currently the Cluster Autoscaler is well designed and organized. 

While the the scaling policy (e.g. expander) requirements from cloud providers vary a lot, and even for different customers on same cloud. There're also use cases that for specific cloud workload, specific scaling policies are preferred as well. For instance, [the AWS cloud autoscaler of OpenAI](https://github.com/openai/kubernetes-ec2-autoscaler) is focusing on batch jobs.

The issue is an initial discussion about making Cluster Autoscaler policy extensible, the custom requirements for expander could be a good start point. In the whole picture, we can actually set up multiple extension point for every phase of Cluster Autoscaler.

The extension point can be implemented by standard [Go plugin](https://golang.org/pkg/plugin/) so we can have native performance comparing to RPC call.

cc @losipiuk @aleksandra-malinowska Would like to know your advices, if there're already plan/roadmap, would be happy to be involved.
",open,False,2019-01-25 23:30:05,2019-01-29 18:31:39
autoscaler,ankrause,https://github.com/kubernetes/autoscaler/pull/1617,https://api.github.com/repos/kubernetes/autoscaler/issues/1617,Remove identity from Azure VMSS scale request as it is not allowed to be set,"Resolve #1592 by omitting the Identities property from the VirtualMachineScaleSet object when sending the scale request. If set, Azure will return 400 BadRequest.",closed,True,2019-01-26 05:16:36,2019-01-27 08:32:11
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1618,https://api.github.com/repos/kubernetes/autoscaler/issues/1618,Remove GkeApiEndpoint,,closed,True,2019-01-28 21:32:46,2019-01-28 21:47:48
autoscaler,immarvin,https://github.com/kubernetes/autoscaler/issues/1619,https://api.github.com/repos/kubernetes/autoscaler/issues/1619,is there any Doc on how to create a customized cloud provider?,"I tried to search any tutorials or Doc on ""how to create a customized cloud provider?"" in google, the autoscaler Docs and K8S Docs, but got nothing.

How can I create a customized cloud provider? is there any material on this?   

thanks",open,False,2019-01-29 08:58:12,2019-02-14 11:08:59
autoscaler,dnutels,https://github.com/kubernetes/autoscaler/issues/1620,https://api.github.com/repos/kubernetes/autoscaler/issues/1620,Deploy cluster-autoscaler to a different namespace,"__What__

Probably documentation bug/request

__Description__

It seems that it should be possible to deploy `cluster-autoscaler` to a different namespace and is, in fact, working, as far as deployment go.

However, when inspecting the `cluster-autoscaler` (deployed to `system` namespace) logs the following error is found:

```
...   1 status.go:120] Failed to retrieve status configmap for update: configmaps ""cluster-autoscaler-status"" is forbidden: User ""system:serviceaccount:system:cluster-autoscaler"" cannot get configmaps in the namespace ""kube-system""
```

And , indeed, the configmap isn't created, and then... isn't accessed.

Before I dive into configuring namespaces for Roles/Bindings and such, I'd like to verify whether this is normal? 

If it is not recommended, this should probably be documented.

__Reasoning__

`kube-system` AFAIK isn't the best namespace to put ""external"" stuff into, due to possible elevated permissions, certs or whatnot.

Theoretically, I'd like to separate (may be even on security group level) what is accessible for what node groups, where `cluster-autoscaler` isn't in the `kube-system` one.

Perhaps, however, this is an overkill.

__Side Note__

Setting RoleBinding and ClusterRoleBinding to be `kube-syste` seems to work properly, where `configmap/cluster-autoscaler-status` is created and updated.",open,False,2019-01-29 11:57:27,2019-02-28 20:33:01
autoscaler,wyb1,https://github.com/kubernetes/autoscaler/pull/1621,https://api.github.com/repos/kubernetes/autoscaler/issues/1621,add prometheus as history provider to FAQ,"Added how to set prometheus as the history provider to the FAQ. 
@bskiba ",closed,True,2019-01-29 14:08:29,2019-01-30 15:28:35
autoscaler,justinsb,https://github.com/kubernetes/autoscaler/pull/1622,https://api.github.com/repos/kubernetes/autoscaler/issues/1622,WIP: Support CPVPA through duck-typing,WIP to enable discussion,closed,True,2019-01-29 14:42:49,2019-02-07 11:20:27
autoscaler,justinsb,https://github.com/kubernetes/autoscaler/pull/1623,https://api.github.com/repos/kubernetes/autoscaler/issues/1623,CPVPA workstream: Create ScalingPolicy duck-type interface,"As the first step towards support other VPA policies via duck-typing,
we abstract out a ScalingPolicy interface.

The API type directly implements this interface.",closed,True,2019-01-29 15:05:04,2019-03-01 11:35:07
autoscaler,mossuchida,https://github.com/kubernetes/autoscaler/issues/1624,https://api.github.com/repos/kubernetes/autoscaler/issues/1624,Need CPU & memory update for Deployment,"Currently I am testing VPA with HPA.  HPA creates new pods based on resource requests in Deployment.  VPA only updates currently created pods resource requests, when the new pods are scaled out by HPA, they use default hardcoded resource requests.  I would like newly scaled out pods to use the current resource utilization of the pod - scheduler uses resource request to determine which node to be deployed, it is important for HPA.
Please add an option to also update Deployment just the same as Pods.",closed,False,2019-01-29 18:12:31,2019-01-30 17:13:12
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1625,https://api.github.com/repos/kubernetes/autoscaler/issues/1625,Allow vpa-admission-controler to installl on specific path,#1561 ,closed,True,2019-01-30 00:39:40,2019-02-27 08:42:17
autoscaler,hello2mao,https://github.com/kubernetes/autoscaler/pull/1626,https://api.github.com/repos/kubernetes/autoscaler/issues/1626,Use klog instead of log,"**What this PR does / why we need it:**
Use klog instead of log for BaiduCloud debug mode log.",closed,True,2019-01-30 02:55:12,2019-02-02 15:39:51
autoscaler,rcleere,https://github.com/kubernetes/autoscaler/issues/1627,https://api.github.com/repos/kubernetes/autoscaler/issues/1627,Pods left unschedulable ,"Kubelet version: 1.13.0
CA version: v1.13.0
Cloud: GCE

Hello,
We run a couple of custom deployed Kubernets clusters in GCE and each cluster has several zonal MIGs with different types of nodes for different workloads. For our general purpose compute pools we have 4 zonal MIGs, and are configured in CA with a minimum of 0 and a max node count of 200. 

One of our clusters with smallish deployments, with pods of less than 20 per deployment, the CA works perfectly. A different cluster the autoscaler normally works but during high scale events stalls, which I will describe later. 

The not perfect cluster has a few deployments that can each scale, with HPA, from 200 to 500 pods each, and on rolling updates a deployment can actually double in size to 1000 pods in a single deployment. So what we have observed when one of these large deployments scales is the pods get created and get marked as Pending because they are unschedulable, as to be expected, and for about 80-90% of those pods the autoscaler scales the general compute MIGs a few nodes at a time and gets them running. However at the tail end of scaling the MIGs the CA seems to ""stall"", meaning those pods that didn't find homes while the other 80-90% of the pods were scheduled sit and the CA ignores them and the CA no longer produces any log messages for unschedulable pods of those deployments. When the stall happens it does still seem to scale other workload specific MIGs, but those MIGs don't have the same volume of pods. 

To get past the CA stall we manually scale the 4 zonal general compute MIGs with ~5 nodes per MIG, this brings up enough capacity for the remaining unschedulable pods to run on. Like I said this usually happens during a deployment update and so after it doubles it size it is finally able start deleting the old pods, we then have excess capacity and the CA scales-down as expected. 

When we first started experiencing this problem we finally figured out that the Kubernetes scheduler had a self imposed client API rate limit and it wasn't able to walk all the newly created pods to actually mark them as unschedulable. We raised the client API rate limit of the scheduler and that helped significantly, and after a couple of rounds of doubling the api rate limit we thought we had it beet. But the problem persisted. 

I should note that also during the time we were working with the client api rate limit we implemented node overprovisioning with a single pod taking all resources of a node and letting it get preempted by node priority. This is actually how we get notified of this CA stall, because those nodes will get preempted but never find a node to run on and we get paged in that situation. We only run 1 overprovision pod per zonal MIG. It really helps with some of the smaller bursty load.

I am really at a loss as to what to look for, I would think if we were hitting a GCE quota limit I wouldn't be able to scale the MIGs up manually from the console. I did check that we do not seem to be over the GCE API limits from what I can tell. I fear turning on debug level logging of the CA, but I can if necessary, at normal log levels it is pretty chatty.

I would love to hear any other thoughts on what to investigate with regards to the stall. 

Thanks
Ryan",closed,False,2019-01-30 04:00:43,2019-01-31 23:10:27
autoscaler,VinayVanama,https://github.com/kubernetes/autoscaler/pull/1628,https://api.github.com/repos/kubernetes/autoscaler/issues/1628,change file name to ca-bundle.crt,,closed,True,2019-01-30 05:49:21,2019-02-04 11:14:32
autoscaler,Globegitter,https://github.com/kubernetes/autoscaler/issues/1629,https://api.github.com/repos/kubernetes/autoscaler/issues/1629,Cluster autoscaler: multiple node utilization,We are making use of the kube downscaler (https://github.com/hjacobs/kube-downscaler) to reduce our cluster size over night and I noticed that we have 50% of our node with <60% utilization. It would be great if there was an additional setting that one could turn on to take the utilization of multiple nodes together into account. So e.g. if there are at least 3 nodes with <60% utilization that it will scale down one of the nodes because the other nodes should easily be able to handle the extra pods and we could save further costs.,open,False,2019-01-30 07:58:05,2019-02-05 08:46:47
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1630,https://api.github.com/repos/kubernetes/autoscaler/issues/1630,VPA release 0.3.1,,closed,True,2019-01-30 10:37:22,2019-01-30 15:25:35
autoscaler,wyb1,https://github.com/kubernetes/autoscaler/issues/1631,https://api.github.com/repos/kubernetes/autoscaler/issues/1631,Add Metrics to VPA Recommender,"Hello, 
we would like to use the vertical pod autoscaler and it would help if we could monitor the recommendations that the VPA recommender gives with prometheus. Would it be possible to expose these metrics in the VPA recommender?

I'm hoping that the metric could look something like this:

`vpa_recommender_recommendation{container=""<container>"",job=""vpa"",pod=""<pod>"",recommendation_type=""lower_bound"",resource_name=""cpu"",vpa_name=""<vpa>""}`

I'd be willing to open a PR for this if it is accepted.",closed,False,2019-01-30 13:30:49,2019-02-08 15:07:24
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1632,https://api.github.com/repos/kubernetes/autoscaler/issues/1632,Update base Debian image for VPA,,closed,True,2019-01-31 10:20:43,2019-01-31 10:34:26
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1633,https://api.github.com/repos/kubernetes/autoscaler/issues/1633,Update base Debian image for VPA,,closed,True,2019-01-31 10:29:41,2019-01-31 10:45:23
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1634,https://api.github.com/repos/kubernetes/autoscaler/issues/1634,Cherrypick of #1293: deletetaint retry on conflicts 1.3,Cherry-pick of #1293. ,closed,True,2019-01-31 17:02:50,2019-01-31 17:35:16
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1635,https://api.github.com/repos/kubernetes/autoscaler/issues/1635,Cherry-pick of #1293: deletetaint: retry on conflicts,Cherry-pick of #1293. ,closed,True,2019-01-31 17:02:52,2019-01-31 17:40:44
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1636,https://api.github.com/repos/kubernetes/autoscaler/issues/1636,Cherry-pick of #1474: Fix logged error in static autoscaler,Cherry-pick of #1474.,closed,True,2019-01-31 17:31:47,2019-02-01 11:53:06
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1637,https://api.github.com/repos/kubernetes/autoscaler/issues/1637,Cherry-pick of #1474: Fix logged error in static autoscaler,Cherry-pick of #1474.,closed,True,2019-01-31 17:32:02,2019-02-01 11:53:02
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1638,https://api.github.com/repos/kubernetes/autoscaler/issues/1638,Default VPA version 0.3.1,,closed,True,2019-01-31 18:06:39,2019-02-01 14:33:41
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1639,https://api.github.com/repos/kubernetes/autoscaler/issues/1639,Default VPA version 0.3.1,,closed,True,2019-01-31 18:06:56,2019-02-01 14:30:09
autoscaler,khteh,https://github.com/kubernetes/autoscaler/issues/1640,https://api.github.com/repos/kubernetes/autoscaler/issues/1640,AWS EKS CloudFormation NodeAutoScalingGroupMinSize  NodeAutoScalingGroupMaxSize ,"Hi, I deploy k8s on AWS EKS using cloud formation which lets me speficy NodeAutoScalingGroupMinSize and NodeAutoScalingGroupMaxSize. How does the aws cluster auto-scaler wok with these 2 parameters? Which one takes precedence? Thanks.",closed,False,2019-02-01 02:27:37,2019-02-04 11:17:05
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1641,https://api.github.com/repos/kubernetes/autoscaler/issues/1641, Fix windows name parsing for Azure VMAS nodes,"The PR fixes windows name parsing for Azure VMAS nodes. It also moves yaml to examples and add missing daemonsets.

Fixes #1593",closed,True,2019-02-01 08:20:42,2019-02-01 12:43:04
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1642,https://api.github.com/repos/kubernetes/autoscaler/issues/1642,[WIP] Storing store NodeGroupName object instead of NodeGroup.id,,closed,True,2019-02-01 09:07:58,2019-02-11 08:59:40
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1643,https://api.github.com/repos/kubernetes/autoscaler/issues/1643,Capacity prediction based on physical memory,,closed,True,2019-02-01 10:16:20,2019-03-22 16:46:40
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1644,https://api.github.com/repos/kubernetes/autoscaler/issues/1644,Cluster Autoscaler 1.3.6,Update Cluster Autoscaler version to 1.3.6.,closed,True,2019-02-01 12:16:15,2019-02-11 10:30:56
autoscaler,carlosedp,https://github.com/kubernetes/autoscaler/issues/1645,https://api.github.com/repos/kubernetes/autoscaler/issues/1645,"addon-resizer changed parameter, examples still with ""threshold""","Since commit https://github.com/kubernetes/autoscaler/commit/93942efb7530a7d266dd7d7a0c83b0417b049471, addon-resizer changed the --threshold parameter to --acceptance-offset but the documentation wasn't updated.

> Usage of pod_nanny:
>       --container=""pod-nanny"": The name of the container to watch. This defaults to the nanny itself.
>       --cpu=""MISSING"": The base CPU resource requirement.
>       --deployment="""": The name of the deployment being monitored. This is required.
>       --extra-cpu=""0"": The amount of CPU to add per node.
>       --extra-memory=""0Mi"": The amount of memory to add per node.
>       --extra-storage=""0Gi"": The amount of storage to add per node.
>       --log-flush-frequency=5s: Maximum number of seconds between log flushes
>       --memory=""MISSING"": The base memory resource requirement.
>       --namespace=$MY_POD_NAMESPACE: The namespace of the ward. This defaults to the nanny pod's own namespace.
>       --pod=$MY_POD_NAME: The name of the pod to watch. This defaults to the nanny's own pod.
>       --poll-period=10000: The time, in milliseconds, to poll the dependent container.
>       --storage=""MISSING"": The base storage resource requirement.
>       --threshold=0: A number between 0-100. The dependent's resources are rewritten when they deviate from expected by more than threshold.",closed,False,2019-02-01 12:47:49,2019-02-18 10:38:39
autoscaler,khteh,https://github.com/kubernetes/autoscaler/issues/1646,https://api.github.com/repos/kubernetes/autoscaler/issues/1646,AWS latest source code cluster-autoscaler-autodiscover.yaml error,"```
E0201 08:59:01.510099       1 aws_manager.go:237] Failed to fetch ASGs: cannot autodiscover ASGs: MissingRegion: could not find region configuration
F0201 08:59:01.510169       1 cloud_provider_builder.go:137] Failed to create AWS Manager: cannot autodiscover ASGs: MissingRegion: could not find region configuration
```",closed,False,2019-02-01 13:52:59,2019-02-04 03:56:22
autoscaler,koooge,https://github.com/kubernetes/autoscaler/issues/1647,https://api.github.com/repos/kubernetes/autoscaler/issues/1647,cluster-autoscaler/aws: Unable to get instance type,"Hi there ✋ 
I tried to set new ASG's launch template option to ""Combine purchase options and instances"". But it failed to scale up due to the missing instance type. Isn't this option supported yet?

```
E0202 02:39:04.817953       1 utils.go:274] Unable to build proper template node for foo-bar: Unable to get instance type from launch config or launch template
E0202 02:39:04.817967       1 static_autoscaler.go:312] Failed to scale up: failed to build node infos for node groups: Unable to get instance type from launch config or launch template
```

https://aws.amazon.com/about-aws/whats-new/2018/11/scale-instances-across-purchase-options-in-a-single-ASG/?nc1=h_ls",closed,False,2019-02-02 04:07:09,2019-02-04 07:43:38
autoscaler,dnutels,https://github.com/kubernetes/autoscaler/pull/1648,https://api.github.com/repos/kubernetes/autoscaler/issues/1648,Add explanation for deployment into non-default namespace,In continuation to: https://github.com/kubernetes/autoscaler/issues/1620,closed,True,2019-02-02 08:38:33,2019-02-04 10:36:01
autoscaler,khteh,https://github.com/kubernetes/autoscaler/issues/1649,https://api.github.com/repos/kubernetes/autoscaler/issues/1649,AWS EKS autodiscover autoscaling Pod scheduling failed,"Failed scheduling pod. Any advice and insight is appreciated.
```
I0204 07:31:14.597199       1 scheduler_binder.go:338] All volumes for Pod ""default/iconverse-elasticsearch-0"" match with Node ""template-node-for-iconverse-worker-nodes-NodeGroup-1B0XF3VJS2O0J-1244933151533827896""
I0204 07:31:14.597296       1 scale_up.go:152] Scale-up predicate failed: NoVolumeZoneConflict predicate mismatch, cannot put default/iconverse-elasticsearch-0 on template-node-for-iconverse-worker-nodes-NodeGroup-1B0XF3VJS2O0J-1244933151533827896, reason: node(s) had no available volume zone
I0204 07:31:14.597335       1 scheduler_binder.go:338] All volumes for Pod ""default/iconverse-mysql-1"" match with Node ""template-node-for-iconverse-worker-nodes-NodeGroup-1B0XF3VJS2O0J-1244933151533827896""
I0204 07:31:14.597434       1 scale_up.go:152] Scale-up predicate failed: NoVolumeZoneConflict predicate mismatch, cannot put default/iconverse-mysql-1 on template-node-for-iconverse-worker-nodes-NodeGroup-1B0XF3VJS2O0J-1244933151533827896, reason: node(s) had no available volume zone
I0204 07:31:14.597456       1 scale_up.go:181] No pod can fit to iconverse-worker-nodes-NodeGroup-1B0XF3VJS2O0J
I0204 07:31:14.597468       1 scale_up.go:186] No expansion options
I0204 07:31:14.597516       1 static_autoscaler.go:322] Calculating unneeded nodes
I0204 07:31:14.597863       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""iconverse-elasticsearch-0"", UID:""11cdf1a9-2849-11e9-b3fc-06f1be837e28"", APIVersion:""v1"", ResourceVersion:""6211123"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0204 07:31:14.597881       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""iconverse-mysql-1"", UID:""11e05b55-2849-11e9-b3fc-06f1be837e28"", APIVersion:""v1"", ResourceVersion:""6211125"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
I0204 07:31:14.603999       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0204 07:31:14.685488       1 scale_down.go:175] Scale-down calculation: ignoring 2 nodes, that were unremovable in the last 5m0s
I0204 07:31:14.685582       1 static_autoscaler.go:352] Scale down status: unneededOnly=false lastScaleUpTime=2019-02-04 04:01:00.233513434 +0000 UTC lastScaleDownDeleteTime=2019-02-04 06:49:34.102920447 +0000 UTC lastScaleDownFailTime=2019-02-04 03:24:50.790531631 +0000 UTC schedulablePodsPresent=false isDeleteInProgress=false
I0204 07:31:14.685618       1 static_autoscaler.go:355] Starting scale down
I0204 07:31:14.825312       1 scale_down.go:446] No candidates for scale down
I0204 07:31:16.615359       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0204 07:31:18.626796       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0204 07:31:20.646423       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0204 07:31:22.658910       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0204 07:31:24.670532       1 leaderelection.go:199] successfully renewed lease kube-system/cluster-autoscaler
I0204 07:31:24.840315       1 static_autoscaler.go:114] Starting main loop
I0204 07:31:25.076214       1 utils.go:456] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0204 07:31:25.076235       1 static_autoscaler.go:263] Filtering out schedulables
I0204 07:31:25.076343       1 scheduler_binder.go:338] All volumes for Pod ""default/iconverse-elasticsearch-0"" match with Node ""ip-10-0-2-236.ap-southeast-1.compute.internal""
I0204 07:31:25.076398       1 scheduler_binder.go:338] All volumes for Pod ""default/iconverse-mysql-1"" match with Node ""ip-10-0-2-236.ap-southeast-1.compute.internal""
I0204 07:31:25.076433       1 static_autoscaler.go:273] No schedulable pods
I0204 07:31:25.076446       1 scale_up.go:59] Pod default/iconverse-elasticsearch-0 is unschedulable
I0204 07:31:25.076451       1 scale_up.go:59] Pod default/iconverse-mysql-1 is unschedulable
I0204 07:31:25.193514       1 scale_up.go:92] Upcoming 0 nodes
I0204 07:31:25.337999       1 scheduler_binder.go:338] All volumes for Pod ""default/iconverse-elasticsearch-0"" match with Node ""template-node-for-iconverse-worker-nodes-NodeGroup-1B0XF3VJS2O0J-4078860463823378974""
I0204 07:31:25.338056       1 scale_up.go:152] Scale-up predicate failed: NoVolumeZoneConflict predicate mismatch, cannot put default/iconverse-elasticsearch-0 on template-node-for-iconverse-worker-nodes-NodeGroup-1B0XF3VJS2O0J-4078860463823378974, reason: node(s) had no available volume zone
I0204 07:31:25.338073       1 scheduler_binder.go:338] All volumes for Pod ""default/iconverse-mysql-1"" match with Node ""template-node-for-iconverse-worker-nodes-NodeGroup-1B0XF3VJS2O0J-4078860463823378974""
I0204 07:31:25.338105       1 scale_up.go:152] Scale-up predicate failed: NoVolumeZoneConflict predicate mismatch, cannot put default/iconverse-mysql-1 on template-node-for-iconverse-worker-nodes-NodeGroup-1B0XF3VJS2O0J-4078860463823378974, reason: node(s) had no available volume zone
I0204 07:31:25.338118       1 scale_up.go:181] No pod can fit to iconverse-worker-nodes-NodeGroup-1B0XF3VJS2O0J
I0204 07:31:25.338130       1 scale_up.go:186] No expansion options
I0204 07:31:25.338177       1 static_autoscaler.go:322] Calculating unneeded nodes
I0204 07:31:25.338369       1 factory.go:33] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""iconverse-elasticsearch-0"", UID:""11cdf1a9-2849-11e9-b3fc-06f1be837e28"", APIVersion:""v1"", ResourceVersion:""6211123"", FieldPath:""""}): type: 'Normal' reason: 'NotTriggerScaleUp' pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```",closed,False,2019-02-04 09:10:56,2019-02-05 08:59:31
autoscaler,nielsole,https://github.com/kubernetes/autoscaler/issues/1650,https://api.github.com/repos/kubernetes/autoscaler/issues/1650,Cluster Autoscaler unaware of ephemeral storage of ASGs without nodes,"When an ASG has 0 nodes, the cluster autoscaler does not scale up the nodes for pods which have `ephemeral-storage` requests with the error message: `reason: Insufficient ephemeral-storage`

When a node of the same ASG already exist, the scaling works fine. 

This seems to be similar to the issue with taints and labels of nodes, which can be set via tags on the ASG. I could not find a similar functionality for indicating allocatable resources of a node.

In my eyes this is clearly missing functionality / bug. What is the best way to go about it? 
Maybe also do it via tags on the ASG or can we autodetect it via describing the vanilla ASG.
Is there something I overlooked? I can't be the first one to run into this issue!?

Image version: k8s.gcr.io/cluster-autoscaler:v1.13.1
Flags used:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/${CLUSTER}

Link to the same question in #sig-autoscaling
https://kubernetes.slack.com/archives/C09R1LV8S/p1549291479482900
",closed,False,2019-02-05 08:59:20,2019-02-11 19:23:22
autoscaler,carlosedp,https://github.com/kubernetes/autoscaler/pull/1651,https://api.github.com/repos/kubernetes/autoscaler/issues/1651,Fix readme to reflect addon-resizer version 2.1.,Updated readme.md to reflect latest parameters from addon-resizer 2.1. Also added references to the images pushed into the registry.,closed,True,2019-02-05 13:30:41,2019-02-18 13:38:15
autoscaler,aerostitch,https://github.com/kubernetes/autoscaler/issues/1652,https://api.github.com/repos/kubernetes/autoscaler/issues/1652,Cluster-autoscaler minimum number for nodes,"Hi,

We're using the cluster-autoscaler 1.3.5 with k8s 1.11.6 on latest coreos in AWS.
During the night we have some jobs that do sudden spikes of cpu usage, which we handled in the past by setting the minimum size of the autoscaling group via the nodes flag: `--nodes=40:120:services-k8s-worker` as we are fine keeping some provisioning overhead.
We are on AWS and have been running k8s way before EKS so haven't switched to EKS yet.

Question 1:
Recently I noticed that the minimum size of our autoscaling group was way bellow the 40 nodes we set in the flags (it was set to 30 manually at some point and hasn't moved since then), so I'm wondering if setting it this way has been deprecated. (We've done the upgrade to k8s from 1.9 to 1.10 and then 1.11 over the last few months and upgrade the cluster-autoscaler too so not sure exactly when it stopped working).

Question 2:
I tried to set the `cores-total` flag to a minimum too `--cores-total=500:1920` but it doesn't seem to change anything on my cluster size. Wondering if that could be due to the fact that we're still using heapster (preparing to move to the metrics server soon but still not there yet).

Note that there's nothing in the logs that indicate an issue:
```
$ kubectl logs --namespace kube-system -l app=cluster-autoscaler
I0205 00:15:21.037178       1 leaderelection.go:185] attempting to acquire leader lease  kube-system/cluster-autoscaler...
I0205 00:15:36.996668       1 leaderelection.go:194] successfully acquired lease kube-system/cluster-autoscaler
W0205 17:39:27.344999       1 reflector.go:341] k8s.io/autoscaler/cluster-autoscaler/utils/kubernetes/listers.go:149: watch of *v1.Pod ended with: too old resource version: 473097260 (473105247)
```

Thanks for your help,
Joseph",closed,False,2019-02-05 22:30:11,2019-02-06 01:00:05
autoscaler,suneeta-mall,https://github.com/kubernetes/autoscaler/issues/1653,https://api.github.com/repos/kubernetes/autoscaler/issues/1653,failed to renew lease kube-system/cluster-autoscaler: failed to tryAcquireOrRenew ,"I am running on Kubernetes 12.5 with etcd3 with cluster-autoscaler `v1.2.2` (on AWS) and my cluster is running healthy with everything operation. After some scaling activity. cluster autoscaler goes into crash loop with error as following:

```I0205 23:32:52.241463       1 leaderelection.go:249] failed to renew lease kube-system/cluster-autoscaler: failed to tryAcquireOrRenew context deadline exceeded
F0205 23:32:52.241542       1 main.go:384] lost master
goroutine 1 [running]:
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/klog.stacks(0xc000022100, 0xc000574000, 0x37, 0xee)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/klog/klog.go:828 +0xd4
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/klog.(*loggingT).output(0x4333560, 0xc000000003, 0xc00056e000, 0x429c819, 0x7, 0x180, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/klog/klog.go:779 +0x306
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/klog.(*loggingT).printf(0x4333560, 0x3, 0x26f2036, 0xb, 0x0, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/klog/klog.go:678 +0x14b
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/klog.Fatalf(0x26f2036, 0xb, 0x0, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/klog/klog.go:1207 +0x67
main.main.func3()
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:384 +0x47
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run.func1(0xc000668000)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:163 +0x40
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run(0xc000668000, 0x29c4b00, 0xc000591dc0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:172 +0x112
k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.RunOrDie(0x29c4b40, 0xc000046040, 0x29cbd20, 0xc0001e6a20, 0x37e11d600, 0x2540be400, 0x77359400, 0xc00001f030, 0x27baac0, 0x0, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:184 +0x99
main.main()
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:372 +0x5cf
I0205 23:32:52.241724       1 factory.go:33] Event(v1.ObjectReference{Kind:""Endpoints"", Namespace:""kube-system"", Name:""cluster-autoscaler"", UID:""e78ccdca-2440-11e9-8514-0a1153ba0cc4"", APIVersion:""v1"", ResourceVersion:""6949892"", FieldPath:""""}): type: 'Normal' reason: 'LeaderElection' cluster-autoscaler-57f79874cf-c45xb stopped leading
I0205 23:32:52.745013       1 auto_scaling_groups.go:124] Registering ASG XXXX
```

Everything in cluster seem to work perfectly find and masters, cluster and etcd are all healthy.  
Is there a way any way to resurrect/resolve this issue? ",open,False,2019-02-06 01:22:01,2019-02-17 22:02:23
autoscaler,VinayVanama,https://github.com/kubernetes/autoscaler/pull/1654,https://api.github.com/repos/kubernetes/autoscaler/issues/1654,ssl-certs path mismatch,,closed,True,2019-02-06 17:12:16,2019-02-07 18:21:21
autoscaler,maximerihouey,https://github.com/kubernetes/autoscaler/pull/1655,https://api.github.com/repos/kubernetes/autoscaler/issues/1655,Correct apiVersion in dynamic provisionning 1.11,scheduling.k8s.io/v1beta -> scheduling.k8s.io/v1beta1,closed,True,2019-02-07 14:31:06,2019-02-07 15:54:33
autoscaler,nielsole,https://github.com/kubernetes/autoscaler/pull/1656,https://api.github.com/repos/kubernetes/autoscaler/issues/1656,Adding ability to override allocatable resources via ASG tags. ,Fixes #1650,closed,True,2019-02-08 08:34:42,2019-02-11 19:23:23
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1657,https://api.github.com/repos/kubernetes/autoscaler/issues/1657,Vpa v1beta2 api,"v1beta2 api changes the way VPA references groups of pods to scale. We are switching from label selectors to a mechanism used by other autoscaling component - Horizontal Pod Autoscaler. 

VPA objects will reference a resource via CrossVersionObjectReference. This object will need to implement the scale subresource, but only to the extent that it is possible to fetch the resource's label selector to identify pods grouped by this resource.

The most common use case will be referencing a controller such as Deployment or StatefulSet.

This change also adds new conditions for VPA to signal deprecated or unsupported configurations.",closed,True,2019-02-08 10:10:46,2019-02-11 18:41:58
autoscaler,dvianello,https://github.com/kubernetes/autoscaler/issues/1658,https://api.github.com/repos/kubernetes/autoscaler/issues/1658,Cluster-autoscaler and WaitForFirstConsumer binding mode,"Hello all,

we're running a multi-AZ k8s cluster in AWS (kops-)configured with the recommended 1 ags/per AZ setup to avoid imbalances and issues with PVs.

Unfortunately, some of the instance types are _not_ available in all AZs, so while the rest of the nodes are across 3 AZs (`eu-west-1{a,b,c}`), the ""special"" nodes are only configured with ASGs in two of them (`eu-west-1{b,c}`). However, this brings about another issue: if the pv for a pod requiring the special nodes gets created in eu-west-1a, then `cluster autoscaler` will refuse to spin them up as they won't be able to be bound to the PV. Fair enough! 

We had hoped that `volumeBindingMode: WaitForFirstConsumer` would have helped delaying the PV creation to _after_ scheduling of the pod happended, so we replaced the standard `gp2` `storageclass` that kops creates with one with `WaitForFirstConsumer` enabled. However, in this case cluster autoscaler seems to refuse to spin up any instance in any AZs as it's waiting for a PV to be created:
  
```
I0208 12:41:51.882717       1 scheduler_binder.go:438] No matching volumes for Pod ""kubeflow/jupyter-dario"", PVC ""kubeflow/dario-workspace"" on node ""template-node-for-cpu-thick-a.k8s-upgrade.k8s.local-3780623283797772889""
I0208 12:41:51.882748       1 utils.go:196] Pod jupyter-dario can't be scheduled on cpu-thick-a.k8s-upgrade.k8s.local, predicate failed: CheckVolumeBinding predicate mismatch, cannot put kubeflow/jupyter-dario on template-node-for-cpu-thick-a.k8s-upgrade.k8s.local-3780623283797772889, reason: node(s) didn't find available persistent volumes to bind
I0208 12:41:51.882778       1 scale_up.go:371] No pod can fit to cpu-thick-a.k8s-upgrade.k8s.local
I0208 12:41:51.882805       1 scheduler_binder.go:438] No matching volumes for Pod ""kubeflow/jupyter-dario"", PVC ""kubeflow/dario-workspace"" on node ""template-node-for-cpu-thick-b.k8s-upgrade.k8s.local-6949585462030478254""
I0208 12:41:51.882818       1 utils.go:196] Pod jupyter-dario can't be scheduled on cpu-thick-b.k8s-upgrade.k8s.local, predicate failed: CheckVolumeBinding predicate mismatch, cannot put kubeflow/jupyter-dario on template-node-for-cpu-thick-b.k8s-upgrade.k8s.local-6949585462030478254, reason: node(s) didn't find available persistent volumes to bind
I0208 12:41:51.882826       1 scale_up.go:371] No pod can fit to cpu-thick-b.k8s-upgrade.k8s.local
I0208 12:41:51.882865       1 scheduler_binder.go:438] No matching volumes for Pod ""kubeflow/jupyter-dario"", PVC ""kubeflow/dario-workspace"" on node ""template-node-for-cpu-thick-c.k8s-upgrade.k8s.local-4646905604613009770""
I0208 12:41:51.882881       1 utils.go:196] Pod jupyter-dario can't be scheduled on cpu-thick-c.k8s-upgrade.k8s.local, predicate failed: CheckVolumeBinding predicate mismatch, cannot put kubeflow/jupyter-dario on template-node-for-cpu-thick-c.k8s-upgrade.k8s.local-4646905604613009770, reason: node(s) didn't find available persistent volumes to bind
I0208 12:41:51.882889       1 scale_up.go:371] No pod can fit to cpu-thick-c.k8s-upgrade.k8s.local
```
Is this expected behaviour, or am I missing a flag/config somewhere?

Thanks!",open,False,2019-02-08 12:48:41,2019-02-13 13:53:27
autoscaler,dvianello,https://github.com/kubernetes/autoscaler/issues/1659,https://api.github.com/repos/kubernetes/autoscaler/issues/1659,Cluster autoscaler scaling too much with GPU nodes,"Hello there,

playing with GPU instances & autoscaler on AWS, we're seeing something strange. 

We've added the ` cloud.google.com/gke-accelerator` label to the GPU nodes to help autoscaler recognizing that we're bringing up a GPU node,  and we do see
```
Overriding status of node xxxx.xxxx.compute.internal, which seems to have unready GPU
```
popping around in the logs.

Sometimes (I suspect race condition here) everything works fine and only _one_ node gets provisioned.  In other cases, the just-provisioned node quickly flips from `Ready` to `NotReady` and back, causing autoscaler to add another node to the pool. It's a transient status that I believe is due to the kops device plugin hook we use that switches everything over to `nvidia-docker` thus causing the few seconds of `NotReady`.

Any tunable in CA that can make it a bit more forgiving of these fluctuations & thus avoid spinning up GPUs instances that we don't need?

Thanks for the help!
",closed,False,2019-02-08 15:09:59,2019-02-08 16:33:32
autoscaler,jayanthpurushothaman,https://github.com/kubernetes/autoscaler/issues/1660,https://api.github.com/repos/kubernetes/autoscaler/issues/1660,Cluster Autoscaler 1.3.5 doesn't remove additional disks when scaling down nodes,"**Cluster Autoscaler version: 1.3.5
Kubernetes: 1.11.6
OS: Ubuntu 16.04 LTS
Disk Type: Premium SSD (Managed)
VM Type: Standard E8s V3
Cloud: Microsoft Azure**


When Cluster Autoscaler scales down the Kubernetes Cluster, it removes only the OS disk attached to the VMs but ignores all other additional Managed disks. We have to manually clean up afterwards. Can this bug be looked at and fixed? Logs below:

```
I0204 09:14:15.628312       1 scale_down.go:384] Skipping k8s-sandbox02n01-96008520-5 from delete considerations - the node is currently being deleted
I0204 09:14:15.840400       1 azure_agent_pool.go:458] found nic name for VM (<RESOURCE GROUP>/k8s-sandbox02n01-96008520-5): k8s-sandbox02n01-96008520-nic-5
I0204 09:14:15.840430       1 azure_agent_pool.go:461] deleting VM: <RESOURCE GROUP>/k8s-sandbox02n01-96008520-5
I0204 09:14:15.840454       1 azure_agent_pool.go:465] waiting for VirtualMachine deletion: <RESOURCE GROUP>/k8s-sandbox02n01-96008520-5
I0204 09:14:25.889043       1 utils.go:503] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0204 09:14:25.889312       1 static_autoscaler.go:258] No unschedulable pods
I0204 09:14:25.889523       1 scale_down.go:373] Scale-down calculation: ignoring 1 nodes unremovable in the last 5m0s
I0204 09:14:25.889552       1 scale_down.go:384] Skipping k8s-sandbox02n01-96008520-5 from delete considerations - the node is currently being deleted
I0204 09:14:36.378622       1 utils.go:503] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0204 09:14:36.378970       1 static_autoscaler.go:258] No unschedulable pods
I0204 09:14:36.379177       1 scale_down.go:373] Scale-down calculation: ignoring 1 nodes unremovable in the last 5m0s
I0204 09:14:36.379209       1 scale_down.go:384] Skipping k8s-sandbox02n01-96008520-5 from delete considerations - the node is currently being deleted
I0204 09:14:46.385253       1 azure_manager.go:261] Refreshed ASG list, next refresh after 2019-02-04 09:15:46.38524274 +0000 UTC m=+146321.245059181
I0204 09:15:03.219262       1 azure_agent_pool.go:471] VirtualMachine <RESOURCE GROUP>/k8s-sandbox02n01-96008520-5 removed
I0204 09:15:03.219287       1 azure_agent_pool.go:474] deleting nic: <RESOURCE GROUP>/k8s-sandbox02n01-96008520-nic-5
I0204 09:15:04.041472       1 azure_agent_pool.go:478] waiting for nic deletion: <RESOURCE GROUP>/k8s-sandbox02n01-96008520-nic-5
I0204 09:15:04.041515       1 azure_agent_pool.go:483] interface <RESOURCE GROUP>/k8s-sandbox02n01-96008520-nic-5 removed
I0204 09:15:04.041540       1 azure_agent_pool.go:506] deleting managed disk: <RESOURCE GROUP>/k8s-sandbox02n01-96008520-5_OsDisk_1_bb8b6735705b4cf69dfcfceebf0edd1c
I0204 09:15:09.928561       1 azure_agent_pool.go:514] disk <RESOURCE GROUP>/k8s-sandbox02n01-96008520-5_OsDisk_1_bb8b6735705b4cf69dfcfceebf0edd1c removed
```
",open,False,2019-02-09 09:01:38,2019-02-11 09:44:09
autoscaler,netiperher,https://github.com/kubernetes/autoscaler/issues/1661,https://api.github.com/repos/kubernetes/autoscaler/issues/1661,Azure AKS cluster autoscaler hangs during up scaling,"CloudProvider: Azure AKS
Kubernetes version: 1.11.5
Cluster Autoscaler version: 1.3.5
ACS-Engine version: v0.26.3-aks

In our setup we run an Azure AKS cluster with multiple nodes that each run a single pod requiring all resources on the node. The pods are part of a statefulset that we scale quite aggressively from 2 to 20 and then back to 2 replicas at regular intervals.

Sometimes (<10%) when scaling up nodes the whole process takes considerable longer time than usual and this is often associated with the autoscaler pod crashing. When everything works normally the average time for us is 21 minutes when going from 2 to 20 nodes and when not working scaling up can take as long as 75 minutes.

Attached logs and screenshot of the `cluster_autoscaler_nodes_count` metric for when the upscaling stopped working after scaling up to 16 nodes and then nothing happens for 30 minutes.

![screenshot 2019-02-07 at 18 40 29](https://user-images.githubusercontent.com/45091747/52519612-3fc60200-2c5e-11e9-8c36-93b5d8002723.png)

Logs before crash (16:30 to 17:15)
[autoscaler-azure-cluster-autoscaler-7c8df96664-h8fs4-before.log](https://github.com/kubernetes/autoscaler/files/2847708/autoscaler-azure-cluster-autoscaler-7c8df96664-h8fs4-before.log)

Logs after crash (17:15 to 17:45)
[autoscaler-azure-cluster-autoscaler-7c8df96664-h8fs4-after.log](https://github.com/kubernetes/autoscaler/files/2847710/autoscaler-azure-cluster-autoscaler-7c8df96664-h8fs4-after.log)

Relevant log lines are around this lines
```
E0207 16:49:23.319809       1 static_autoscaler.go:283] Failed to scale up: failed to increase node group size: Code="""" Message=""""
```

cc @feiskyer ",closed,False,2019-02-09 10:43:50,2019-03-08 11:34:42
autoscaler,miry,https://github.com/kubernetes/autoscaler/pull/1662,https://api.github.com/repos/kubernetes/autoscaler/issues/1662,Don't panic on unknown aws instance type,"Cluster Autoscaler crashes if there are some AWS ASG that use instances
types and was not updated in the build.
    
Update tests for case when there is unknown instance type and return
error if there is one.

Sample of excepttion:  #483


Covers changes for 1.3 branch only",closed,True,2019-02-10 13:11:23,2019-02-11 18:28:05
autoscaler,miry,https://github.com/kubernetes/autoscaler/pull/1663,https://api.github.com/repos/kubernetes/autoscaler/issues/1663,Updated aws instance types actual for 2019.02.10,Sync instance type for aws via `make generate`,closed,True,2019-02-10 13:20:32,2019-02-12 12:20:50
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1664,https://api.github.com/repos/kubernetes/autoscaler/issues/1664,Add the ability to better pass additional arguments to containers,#1540 ,closed,True,2019-02-10 20:40:16,2019-03-16 00:11:54
autoscaler,suhas184,https://github.com/kubernetes/autoscaler/issues/1665,https://api.github.com/repos/kubernetes/autoscaler/issues/1665,"vpa ""auto"" mode does not work with replicas: 1","The vpa hamster example in the git repo does not work if the replica count is decreased to 1 . 

The deployment object definition is as below:
_apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hamster
spec:
  replicas: **2**_

if we  change the replicas to , it doesn't evict the pod and creare a new one with recommended cpu and memory. The recommendation is provided but it does not automatically recreate a pod with new specifications under ""Auto"" or ""Recreate"" mode.

_apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hamster
spec:
  replicas: **1**_

has anyone faced this issue before or is it a known behavior?",closed,False,2019-02-11 11:26:33,2019-02-18 10:37:42
autoscaler,c3mb0,https://github.com/kubernetes/autoscaler/issues/1666,https://api.github.com/repos/kubernetes/autoscaler/issues/1666,Allow honoring AWS ASG cooldown,"EKS does not allow direct interaction with `kube-controller-manager`, hence it is impossible to set the `--horizontal-pod-autoscaler-upscale-delay` flag, which leads to [thrashing](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-cooldown-delay). One way to tackle this situation could be to tell `cluster-autoscaler` to honor ASG cooldowns. Right now, it is a hardcoded [false](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/auto_scaling_groups.go#L201). It would be great if the behaviour could be controlled via a flag.",closed,False,2019-02-11 14:55:20,2019-02-13 11:38:04
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1667,https://api.github.com/repos/kubernetes/autoscaler/issues/1667,Allow passing EXCLUDE variable for verify-all.sh,,closed,True,2019-02-11 15:59:04,2019-02-11 18:19:06
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1668,https://api.github.com/repos/kubernetes/autoscaler/issues/1668,Vpa v1beta2 api ,"This PR is a replacement of #1657 to address minor comments.

Original note:

v1beta2 api changes the way VPA references groups of pods to scale. We are switching from label selectors to a mechanism used by other autoscaling component - Horizontal Pod Autoscaler.

VPA objects will reference a resource via CrossVersionObjectReference. This object will need to implement the scale subresource, but only to the extent that it is possible to fetch the resource's label selector to identify pods grouped by this resource.

The most common use case will be referencing a controller such as Deployment or StatefulSet.

This change also adds new conditions for VPA to signal deprecated or unsupported configurations.",closed,True,2019-02-11 15:59:19,2019-02-11 18:42:48
autoscaler,mabushey,https://github.com/kubernetes/autoscaler/issues/1669,https://api.github.com/repos/kubernetes/autoscaler/issues/1669,cluster-autoscaler-run-on-master.yaml: error converting YAML to JSON,"```
➜  kubectl version
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.0"", GitCommit:""91e7b4fd31fcd3d5f436da26c980becec37ceefe"", GitTreeState:""clean"", BuildDate:""2018-06-27T20:17:28Z"", GoVersion:""go1.10.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.7"", GitCommit:""65ecaf0671341311ce6aea0edab46ee69f65d59e"", GitTreeState:""clean"", BuildDate:""2019-01-24T19:22:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
✗ kubectl apply -f autoscaler-cluster-autoscaler-1.3.6/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-run-on-master.yaml 
serviceaccount/cluster-autoscaler created
clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created
role.rbac.authorization.k8s.io/cluster-autoscaler created
clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
error: error parsing autoscaler-cluster-autoscaler-1.3.6/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-run-on-master.yaml: error converting YAML to JSON: yaml: invalid map key: map[interface {}]interface {}{""region"":interface {}(nil)}

```",open,False,2019-02-11 23:57:58,2019-03-07 09:14:40
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1670,https://api.github.com/repos/kubernetes/autoscaler/issues/1670,Update AWS documentation and CA version in examples,"1. v1.10 is going to sunset, upgrade to 1.3.6 which is compatible with v1.11 in examples.
2. Remind users to replace config in examples. #1669
3. Make clarification on min/max settings for one/multiple node groups. #1559 #1555 #1640
4. Recommend users to adopt Auto-Discovery option. #1640",closed,True,2019-02-12 07:06:54,2019-02-12 11:55:38
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1671,https://api.github.com/repos/kubernetes/autoscaler/issues/1671,Cherry-pick of #1288: Read AWS Region from EC2 Metadata,cherry-pick of #1288 ,closed,True,2019-02-12 07:08:42,2019-02-12 10:54:12
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1672,https://api.github.com/repos/kubernetes/autoscaler/issues/1672,"VPA v1beta2 API - selector removed, checkpoints included in v1beta2.",,closed,True,2019-02-12 09:51:31,2019-02-12 11:57:43
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1673,https://api.github.com/repos/kubernetes/autoscaler/issues/1673,Cherry-pick of #1288: Read AWS Region from EC2 Metadata,Cherry-pick of #1288,closed,True,2019-02-12 10:24:12,2019-02-12 18:19:07
autoscaler,danmassie,https://github.com/kubernetes/autoscaler/issues/1674,https://api.github.com/repos/kubernetes/autoscaler/issues/1674,[Azure AKS] 403s received from Azure when listing VMSS instances,"aks-engine 0.27.0
kubernetes 1.13.1
cluster autoscaler v1.13.1

The cluster autoscaler periodically receives 403s from Azure which results in the pod restarting where it resumes normal behaviour until receiving a further 403. 

`E0208 08:54:43.057787       1 azure_scale_set.go:199] VirtualMachineScaleSetVMsClient.List failed for k8s-agentpool1-39472669-vmss: compute.VirtualMachineScaleSetVMsClient#List: Failure responding to request: StatusCode=403 -- Original Error: autorest/azure: Serv
ice returned an error. Status=403 Code=""AuthorizationFailed"" Message=""The client '6053ad54-5340-4d95-8842-9f2ac12c4566' with object id '6053ad54-5340-4d95-8842-9f2ac12c4566' does not have authorization to perform action 'Microsoft.Compute/virtualMachineScaleSets/
virtualMachines/read' over scope '/subscriptions/xxx-xxx-xxx-xxx/resourceGroups/k8s/providers/Microsoft.Compute/virtualMachineScaleSets/k8s-agentpool1-39472669-vmss'.""
F0208 08:54:43.057854       1 azure_cloud_provider.go:139] Failed to create Azure Manager: compute.VirtualMachineScaleSetVMsClient#List: Failure responding to request: StatusCode=403 -- Original Error: autorest/azure: Service returned an error. Status=403 Code=""A
uthorizationFailed"" Message=""The client 'xxx-xxx-xxx-xxx' with object id 'xxx-xxx-xxx-xxx' does not have authorization to perform action 'Microsoft.Compute/virtualMachineScaleSets/virtualMachines/read' over scope '/subscr
iptions/xxx-xxx-xxx-xxx/resourceGroups/k8s/providers/Microsoft.Compute/virtualMachineScaleSets/k8s-agentpool1-39472669-vmss'.""
`",open,False,2019-02-12 11:50:43,2019-02-19 06:59:48
autoscaler,khteh,https://github.com/kubernetes/autoscaler/issues/1675,https://api.github.com/repos/kubernetes/autoscaler/issues/1675,Scale-out latency?,"Hi, I am using AWS EKS with multi-ASG in multi-zones and I have deployed the autodiscover autoscaler. Is there any measured data regarding node scale-out latency? What measures does the autoscaler take to prevent production downtime due to the node scale-out latency when there is a sudden surge of load? Thanks.",closed,False,2019-02-12 12:15:50,2019-02-13 00:59:12
autoscaler,leonsodhi-lf,https://github.com/kubernetes/autoscaler/issues/1676,https://api.github.com/repos/kubernetes/autoscaler/issues/1676,AWS EKS cluster autoscaler only using ASGs that have at least one node,"CloudProvider: AWS EKS
Kubernetes version: 1.11.5
Cluster Autoscaler version: 1.3.6

With the autoscaler options shown below and multiple identical ASGs (other than AZ), new nodes always end up being created using an ASG that has at least 1 node. Looking at the logs, autoscaler considers these ASGs to have the least memory wastage:

```
waste.go:57] Expanding Node Group m5_large_az1 would waste 95.00% CPU, 99.09% Memory, 97.04% Blended
waste.go:57] Expanding Node Group m5_large_az2 would waste 95.00% CPU, 99.15% Memory, 97.07% Blended
waste.go:57] Expanding Node Group m5_large_az3 would waste 95.00% CPU, 99.15% Memory, 97.07% Blended
waste.go:57] Expanding Node Group m5_large_az4 would waste 95.00% CPU, 99.15% Memory, 97.07% Blended
```

If I manually set the desired count on the `m5_large_az2` ASG to 1, I then see:
```
I0212 14:25:26.668623 1 waste.go:57] Expanding Node Group m5_large_az1 would waste 95.00% CPU, 99.09% Memory, 97.04% Blended
I0212 14:25:26.668656 1 waste.go:57] Expanding Node Group m5_large_az2 would waste 95.00% CPU, 99.09% Memory, 97.04% Blended
I0212 14:25:26.668668 1 waste.go:57] Expanding Node Group m5_large_az3 would waste 95.00% CPU, 99.15% Memory, 97.07% Blended
I0212 14:25:26.668678 1 waste.go:57] Expanding Node Group m5_large_az4 would waste 95.00% CPU, 99.15% Memory, 97.07% Blended
```
I believe this has started happening since I added taints to the nodes, and tolerations and nodeselectors to the pods.

## Autoscaler options
```
--v=4
- --stderrthreshold=info
- --cloud-provider=aws
- --skip-nodes-with-local-storage=false
- --skip-nodes-with-system-pods=false
- --balance-similar-node-groups=true
- --expander=least-waste     
",closed,False,2019-02-12 16:36:47,2019-02-12 19:00:01
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1677,https://api.github.com/repos/kubernetes/autoscaler/issues/1677,Cherry-pick of #1490: Set AWS_REGION so tests don't wait for SDK timeout #1449,cherry-pick of #1490 ,closed,True,2019-02-12 18:47:53,2019-02-12 20:36:53
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1678,https://api.github.com/repos/kubernetes/autoscaler/issues/1678,Cherry-pick of #1490: Set AWS_REGION so tests don't wait for SDK timeout #1449,Cherry-pick of #1490 ,closed,True,2019-02-12 18:49:10,2019-02-12 20:36:47
autoscaler,lsytj0413,https://github.com/kubernetes/autoscaler/pull/1679,https://api.github.com/repos/kubernetes/autoscaler/issues/1679,fix(expander): avoid panic when random expander,"when the random-expander is used, if the options length is zero there will be a panic cause. this pr try to fix it.",closed,True,2019-02-13 03:46:31,2019-02-13 10:02:52
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1680,https://api.github.com/repos/kubernetes/autoscaler/issues/1680,Use k8s.io/klog instead of glog,,closed,True,2019-02-13 09:57:39,2019-02-27 10:27:22
autoscaler,MoJo2600,https://github.com/kubernetes/autoscaler/issues/1681,https://api.github.com/repos/kubernetes/autoscaler/issues/1681,[AWS] Autoscaling not working with podAntiAffinity and topologyKey failure-domain.beta.kubernetes.io/zone,"Hello,

I'm not sure if this is an issue or I am using the autoscaler wrong. So any advice is highly appreciated. I have a cluster set up with kops which is currently running in 1 AZ, but the Autoscaling Group is set up to use multiple AZs. On pod is already running on this node. The second node has the following podAntiAffinity configured:

```
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - theotherpod
        topologyKey: failure-domain.beta.kubernetes.io/zone
```
This pod is now all the time in Pending state. The message is:
`NotTriggerScaleUp  37s (x3 over 3m39s)   cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 node(s) didn't match pod anti-affinity rules, 1 node(s) didn't match pod affinity/anti-affinity`

The autoscaling does work with when the topologyKey is set to `kubernetes.io/hostname`

I think the autoscaler has no idea, that a new node would be started in another region an it would then fit. Can this be somehow configured?

Best regards

Christian",closed,False,2019-02-13 14:23:16,2019-02-13 14:43:00
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1682,https://api.github.com/repos/kubernetes/autoscaler/issues/1682,Cherry-pick of #1643: Capacity prediction based on physical memory,"Cherry-pick of https://github.com/kubernetes/autoscaler/pull/1643
with dependency on https://github.com/kubernetes/autoscaler/pull/1550

/cc aleksandra-malinowska
/cc MaciekPytel
/cc losipiuk

/hold",closed,True,2019-02-13 15:43:38,2019-03-22 16:46:45
autoscaler,forkbomber,https://github.com/kubernetes/autoscaler/pull/1683,https://api.github.com/repos/kubernetes/autoscaler/issues/1683,VPA: add YAML start of a document marks,"Some workflows assume concatenated YAML to be as valid & applicable as a bunch of separate YAML files.

This commit make make it safe & valid to merge all VPA files in deploy/ prior to execution.

Fixes syntax error resulting from merging VPA files without explicit initial start-of-a-document marks:

`error: error validating ""vpa.yaml"": error validating data: ValidationError(ServiceAccount): unknown field ""spec"" in io.k8s.api.core.v1.ServiceAccount; if you choose to ignore these errors, turn validation off with --validate=false`",closed,True,2019-02-13 18:06:19,2019-02-14 11:11:52
autoscaler,CodeLingoBot,https://github.com/kubernetes/autoscaler/pull/1684,https://api.github.com/repos/kubernetes/autoscaler/issues/1684,Code lingo setup,"Add the Effective Go and Code Review Comments Tenet Bundles from https://golang.org/doc/effective_go.html and https://github.com/golang/go/wiki/CodeReviewComments. You need to [install](https://github.com/apps/codelingo) the CodeLingo GitHub App to automate code reviews, bug fixes and contributor docs with these Tenets. Find more Tenet bundles at [www.codelingo.io/tenets](link)",closed,True,2019-02-13 23:38:51,2019-02-14 11:06:47
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1685,https://api.github.com/repos/kubernetes/autoscaler/issues/1685,VPA - Introduce scale reader role,Original author kgolab,closed,True,2019-02-14 10:18:06,2019-02-14 10:37:37
autoscaler,mwielgus,https://github.com/kubernetes/autoscaler/pull/1686,https://api.github.com/repos/kubernetes/autoscaler/issues/1686,Remove codelingo configs,"On the second thought, we don't want config files for bots not officially accepted.",closed,True,2019-02-14 11:15:20,2019-02-14 12:10:46
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1687,https://api.github.com/repos/kubernetes/autoscaler/issues/1687,VPA - Introduce VpaTargetSelectorFetcher,"Following implementations are provided:
- one for v1beta1 VPAs that fetches label selector
- one for v1beta2 that fetches from targetRef (contributed by kgolab)
- mock",closed,True,2019-02-14 12:42:07,2019-02-14 13:59:18
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1688,https://api.github.com/repos/kubernetes/autoscaler/issues/1688,VPA beta2 CRD - version that supports v1beta1 too.,+ fix in v1beta2 register.go,closed,True,2019-02-14 13:11:30,2019-02-14 14:01:39
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1689,https://api.github.com/repos/kubernetes/autoscaler/issues/1689,Cherry-pick of #1643: Capacity prediction based on physical memory,"Cherry-pick of #1643
with dependency on #1550

/cc aleksandra-malinowska
/cc MaciekPytel
/cc losipiuk

/hold",closed,True,2019-02-14 13:37:20,2019-03-22 16:46:55
autoscaler,tghartland,https://github.com/kubernetes/autoscaler/pull/1690,https://api.github.com/repos/kubernetes/autoscaler/issues/1690,Add OpenStack cloud provider,"This pull request adds a cloud provider for OpenStack. This requires a more up to date version of gophercloud. The specific newer commit of gophercloud is chosen because the commit following it [changes a function signature](https://github.com/gophercloud/gophercloud/commit/7bd760e8eb1d7c94108db464cc992df1620278d1#diff-18ac4c04e89cf251c4c250ccfa91b7f8R84), and this leads to [problems with k8s.io/kubernetes itself](https://gist.github.com/tghartland/e564a2e57b09e7008a82b4ad18b5c4da).

As of right now this autoscaler only supports OpenStack clusters using Magnum and Heat, but this is done through an interface OpenstackManager. I will be adding another implementation of this interface to support Magnum clusters with proper nodegroups when that is available, but this could also be extended to support any other kind of OpenStack cluster. There is only the Magnum/Heat manager now but it is already set up to support more via changing an environment variable.

All nodegroups managed by the OpenstackCloudProvider interact with openstack through a single instance of the manager.

Since Magnum does not yet have nodegroups, only a single nodegroup is currently allowed and this represents the entire cluster.",closed,True,2019-02-14 13:55:28,2019-03-15 13:06:01
autoscaler,ricolin,https://github.com/kubernetes/autoscaler/pull/1691,https://api.github.com/repos/kubernetes/autoscaler/issues/1691,Support openstack cluster autoscaler,"#734

This is for update status and plan to co-work with https://github.com/kubernetes/autoscaler/pull/1690/commits/a37e287e1b5727987896e5df63f256c1de77195b",closed,True,2019-02-14 17:35:58,2019-02-21 13:47:03
autoscaler,forkbomber,https://github.com/kubernetes/autoscaler/pull/1692,https://api.github.com/repos/kubernetes/autoscaler/issues/1692,VPA: Remove TLS client cert lookup logic,"Actual verification was turned off from the initial commit and is still [not enabled](../blob/6b1a97c16001a98607424568faf7239b5c364d11/vertical-pod-autoscaler/pkg/admission-controller/config.go#L79), so the whole construct does not add value.

Also depending on `requestheader-client-ca-file` by itself renders VPA incompatible with apiserver installations which does not use such certificate with VPA Admission Controller crash-looping with:
`vpa-admission-controller-X-Y:admission-controller F0214 18:15:09.448204       9 config.go:60] cannot find the ca.crt in the configmap, configMap.Data is map[string]string{""client-ca-file"":""...skipped...""}`

In addition to the above it worth noting that `requestheader-client-ca-file` appears to be just a wrong place to look for _possible_ apiserver client certificate (I don't think there is any).",closed,True,2019-02-14 18:18:57,2019-02-22 18:24:05
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1693,https://api.github.com/repos/kubernetes/autoscaler/issues/1693,VPA - Implement obtaining selector from well known controllers,(partial contribution from kgolab),closed,True,2019-02-15 05:18:28,2019-02-15 08:09:16
autoscaler,khteh,https://github.com/kubernetes/autoscaler/issues/1694,https://api.github.com/repos/kubernetes/autoscaler/issues/1694,Multi-AZ Multi-ASG on AWS EKS does not scale down?,"I use auto-discover autoscaler on AWS EKS with multi-az and multi-asg with nodes-min=1, ndoes-max=5 on every ASG. Somehow it has scaled up to 5 nodes but `kubectl top nodes` does not show such high resource demands and it doesn't scale down. This worries as costs concerns. Any advice and insight is appreciated!
All my statefulsets resources specs:
```
iconverse-admin.yml:75:        resources:
iconverse-admin.yml-76-          limits:
iconverse-admin.yml-77-            cpu: 250m
iconverse-admin.yml-78-            memory: 1024Mi
--
iconverse-admin.yml:93:        resources:
iconverse-admin.yml-94-          limits:
iconverse-admin.yml-95-            cpu: 250m
iconverse-admin.yml-96-            memory: 1024Mi
--
iconverse-blazegraph.yml:57:      resources:
iconverse-blazegraph.yml-58-        requests:
iconverse-blazegraph.yml-59-          storage: 1Gi
--
iconverse-connector.yml:65:        resources:
iconverse-connector.yml-66-          limits:
iconverse-connector.yml-67-            cpu: 250m
iconverse-connector.yml-68-            memory: 512Mi
--
iconverse-connector.yml:83:        resources:
iconverse-connector.yml-84-          limits:
iconverse-connector.yml-85-            cpu: 250m
iconverse-connector.yml-86-            memory: 512Mi
--
iconverse-converse.yml:65:        resources:
iconverse-converse.yml-66-          limits:
iconverse-converse.yml-67-            cpu: 250m
iconverse-converse.yml-68-            memory: 512Mi
--
iconverse-converse.yml:83:        resources:
iconverse-converse.yml-84-          limits:
iconverse-converse.yml-85-            cpu: 250m
iconverse-converse.yml-86-            memory: 512Mi
--
iconverse-elasticsearch.yml:53:        resources:
iconverse-elasticsearch.yml-54-          limits:
iconverse-elasticsearch.yml-55-            cpu: 250m
iconverse-elasticsearch.yml-56-            memory: 2048Mi
--
iconverse-elasticsearch.yml:66:      resources:
iconverse-elasticsearch.yml-67-        requests:
iconverse-elasticsearch.yml-68-          storage: 5Gi
--
iconverse-kibana.yml:58:        resources:
iconverse-kibana.yml-59-          limits:
iconverse-kibana.yml-60-            cpu: 500m
iconverse-kibana.yml-61-            memory: 1024Mi
--
iconverse-mysqlset.yml:76:      resources:
iconverse-mysqlset.yml-77-        requests:
iconverse-mysqlset.yml-78-          storage: 2Gi
--
iconverse-nlp.yml:65:        resources:
iconverse-nlp.yml-66-          limits:
iconverse-nlp.yml-67-            cpu: 250m
iconverse-nlp.yml-68-            memory: 512Mi
--
iconverse-nlp.yml:83:        resources:
iconverse-nlp.yml-84-          limits:
iconverse-nlp.yml-85-            cpu: 250m
iconverse-nlp.yml-86-            memory: 512Mi
---
iconverse-ui.yml:39:        resources:
iconverse-ui.yml-40-          limits:
iconverse-ui.yml-41-            cpu: 500m
iconverse-ui.yml-42-            memory: 512Mi

```
```
NAME                                                CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-foo1.ap-southeast-1.compute.internal     85m          4%     3663Mi          46%       
ip-foo2.ap-southeast-1.compute.internal   55m          2%     2647Mi          33%       
ip-foo3.ap-southeast-1.compute.internal    57m          2%     4609Mi          58%       
ip-foo4.ap-southeast-1.compute.internal   54m          2%     3191Mi          40%       
ip-foo5.ap-southeast-1.compute.internal   69m          3%     4934Mi          62% 
```
```
k top pods
NAME                            CPU(cores)   MEMORY(bytes)   
blazegraph-0                    1m           192Mi           
blazegraph-1                    1m           336Mi           
external-dns-7fc6cf67cf-jk2rd   0m           15Mi            
iconverse-admin-0               3m           567Mi           
iconverse-admin-1               3m           508Mi           
iconverse-connector-0           3m           297Mi           
iconverse-connector-1           3m           297Mi           
iconverse-converse-0            3m           453Mi           
iconverse-converse-1            3m           457Mi           
iconverse-daemonset-gkn8t       0m           2Mi             
iconverse-daemonset-mrrsw       0m           2Mi             
iconverse-daemonset-pqfx5       0m           1Mi             
iconverse-daemonset-px4pg       1m           1Mi             
iconverse-daemonset-qrpfk       1m           2Mi             
iconverse-elasticsearch-0       9m           1462Mi          
iconverse-kibana-0              21m          227Mi           
iconverse-kibana-1              9m           217Mi           
iconverse-mysql-0               3m           458Mi           
iconverse-nlp-0                 3m           488Mi           
iconverse-nlp-1                 3m           440Mi           
iconverse-ui-0                  2m           115Mi           
iconverse-ui-1                  1m           115Mi 
```",closed,False,2019-02-15 07:22:07,2019-02-15 13:43:00
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1695,https://api.github.com/repos/kubernetes/autoscaler/issues/1695,Cherry-pick of #1643: Capacity prediction based on physical memory,"Cherry-pick of #1643
with dependency on #1550

/cc aleksandra-malinowska
/cc MaciekPytel
/cc losipiuk

/hold",closed,True,2019-02-15 08:55:35,2019-03-22 16:46:50
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1696,https://api.github.com/repos/kubernetes/autoscaler/issues/1696,VPA - Switch to v1beta2 api,,closed,True,2019-02-15 14:00:00,2019-02-15 15:45:56
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1697,https://api.github.com/repos/kubernetes/autoscaler/issues/1697,Cluster Autoscaler 1.3.7,"/cc aleksandra-malinowska
/cc MaciekPytel
/cc losipiuk",closed,True,2019-02-15 14:06:34,2019-03-22 16:47:32
autoscaler,delirius325,https://github.com/kubernetes/autoscaler/pull/1698,https://api.github.com/repos/kubernetes/autoscaler/issues/1698,Fixed typo,Fixed a simple typo in the README 😄,closed,True,2019-02-15 14:30:08,2019-02-21 11:00:53
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1699,https://api.github.com/repos/kubernetes/autoscaler/issues/1699,VPA - e2e tests for v1beta2 API,+ support for Jobs and ReplicationControllers in SelectorFetcher,closed,True,2019-02-16 12:42:35,2019-02-18 07:18:34
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1700,https://api.github.com/repos/kubernetes/autoscaler/issues/1700,VPA - fix names of files with junit test results,,closed,True,2019-02-18 11:59:17,2019-02-18 12:13:41
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1701,https://api.github.com/repos/kubernetes/autoscaler/issues/1701,Vertical Pod Autoscaler version 0.4.0,,closed,True,2019-02-18 12:38:33,2019-02-18 14:04:46
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1702,https://api.github.com/repos/kubernetes/autoscaler/issues/1702,VPA - disable errexit in run e2e scripts,Since we want to run 2 e2e test scripts we don't want to exit on failure of the first one.,closed,True,2019-02-18 18:10:02,2019-02-19 08:44:26
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1703,https://api.github.com/repos/kubernetes/autoscaler/issues/1703,Fix error message for long-waiting operations,"Get HTTP responses together with error, so that we could get full error
messages. It also fixes some edge cases for success responses.

Fix #1661.",closed,True,2019-02-19 09:04:52,2019-02-21 12:20:45
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1704,https://api.github.com/repos/kubernetes/autoscaler/issues/1704,Update VPA to beta in main README,,closed,True,2019-02-20 10:36:58,2019-02-20 11:07:50
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1705,https://api.github.com/repos/kubernetes/autoscaler/issues/1705,VPA - check ConfigDeprecated condition for v1beta1 e2e tests,,closed,True,2019-02-20 12:08:24,2019-02-20 12:52:32
autoscaler,tomislater,https://github.com/kubernetes/autoscaler/pull/1706,https://api.github.com/repos/kubernetes/autoscaler/issues/1706,Add version field to spec,"According to [this document](https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definition-versioning/#overview) we should also set `version` field.
Without doing this, it is impossible to deploy VPA on Kubernetes 1.10.

Before:
```
./hack/vpa-up.sh
Error from server (Invalid): error when creating ""STDIN"": CustomResourceDefinition.apiextensions.k8s.io ""verticalpodautoscalers.autoscaling.k8s.io"" is invalid: spec.version: Required value
Error from server (Invalid): error when creating ""STDIN"": CustomResourceDefinition.apiextensions.k8s.io ""verticalpodautoscalercheckpoints.autoscaling.k8s.io"" is invalid: spec.version: Required value
clusterrole.rbac.authorization.k8s.io/system:metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:vpa-actor created
clusterrole.rbac.authorization.k8s.io/system:vpa-checkpoint-actor created
clusterrole.rbac.authorization.k8s.io/system:evictioner created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-actor created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-checkpoint-actor created
clusterrole.rbac.authorization.k8s.io/system:vpa-target-reader created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-vpa-target-reader-binding created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-evictionter-binding created
serviceaccount/vpa-admission-controller created
clusterrole.rbac.authorization.k8s.io/system:admission-controller created
clusterrolebinding.rbac.authorization.k8s.io/system:admission-controller created
serviceaccount/vpa-updater created
deployment.extensions/vpa-updater created
serviceaccount/vpa-recommender created
deployment.extensions/vpa-recommender created
Generating certs for the VPA Admission Controller in /tmp/vpa-certs.
Generating RSA private key, 2048 bit long modulus
................................+++++
............................................................................+++++
e is 65537 (0x10001)
Generating RSA private key, 2048 bit long modulus
.............................+++++
........................+++++
e is 65537 (0x10001)
Signature ok
subject=/CN=vpa-webhook.kube-system.svc
Getting CA Private Key
Uploading certs to the cluster.
secret/vpa-tls-certs created
Deleting /tmp/vpa-certs.
deployment.extensions/vpa-admission-controller created
service/vpa-webhook created
```

After:
```
./hack/vpa-up.sh
customresourcedefinition.apiextensions.k8s.io/verticalpodautoscalers.autoscaling.k8s.io created
customresourcedefinition.apiextensions.k8s.io/verticalpodautoscalercheckpoints.autoscaling.k8s.io created
clusterrole.rbac.authorization.k8s.io/system:metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:vpa-actor created
clusterrole.rbac.authorization.k8s.io/system:vpa-checkpoint-actor created
clusterrole.rbac.authorization.k8s.io/system:evictioner created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-actor created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-checkpoint-actor created
clusterrole.rbac.authorization.k8s.io/system:vpa-target-reader created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-vpa-target-reader-binding created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-evictionter-binding created
serviceaccount/vpa-admission-controller created
clusterrole.rbac.authorization.k8s.io/system:admission-controller created
clusterrolebinding.rbac.authorization.k8s.io/system:admission-controller created
serviceaccount/vpa-updater created
deployment.extensions/vpa-updater created
serviceaccount/vpa-recommender created
deployment.extensions/vpa-recommender created
Generating certs for the VPA Admission Controller in /tmp/vpa-certs.
Generating RSA private key, 2048 bit long modulus
..............................................................................................................................................+++++
.........................................................................................+++++
e is 65537 (0x10001)
Generating RSA private key, 2048 bit long modulus
..........................................................................................................+++++
............................+++++
e is 65537 (0x10001)
Signature ok
subject=/CN=vpa-webhook.kube-system.svc
Getting CA Private Key
Uploading certs to the cluster.
secret/vpa-tls-certs created
Deleting /tmp/vpa-certs.
deployment.extensions/vpa-admission-controller created
service/vpa-webhook created
```",closed,True,2019-02-20 12:08:35,2019-02-20 12:47:00
autoscaler,mbach04,https://github.com/kubernetes/autoscaler/issues/1707,https://api.github.com/repos/kubernetes/autoscaler/issues/1707,Allow custom AWS region overrides,"Unique regional endpoints exist in disconnected AWS environments. Currently the aws-go-sdk that is utilized heavily in k8s hard codes these regions in json [1] and doesn't appear to expose an easy method to override that would flow down to a repo like the k8s/autoscaler. A similar issue [2] with a merged PR  [3] was filed against the k8s repo.

The previous solution was to read a file that specified the custom endpoints and associated services. This is something that would benefit those same environments tremendously and all for the use of the auto-scaler. I'm happy to take a crack at this patch, but it's taking me a while to find the right place in this repo to inject the reading of such a file or env var.

Because the hostnames and certs for these custom regions are private, they will never see the light of day in the aws-go-sdk models json file defining all available services. Being able to override these hostnames dynamically would be extremely helpful.

1. [AWS endpoints models](https://github.com/aws/aws-sdk-go/blob/182cda27d0921b14139ff6d352c09e0cb20e4578/models/endpoints/endpoints.json)
2. [RFE Allow to override default AWS endpoint](https://github.com/kubernetes/kubernetes/issues/70588)
3. [Add AWS Custom Endpoint capability #70588](https://github.com/kubernetes/kubernetes/pull/72245)",closed,False,2019-02-21 04:10:53,2019-03-12 23:34:39
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1708,https://api.github.com/repos/kubernetes/autoscaler/issues/1708,Update CA godeps to Kubernetes v1.15.0-alpha.0,"Update CA godeps to Kubernetes v1.15.0-alpha.0, and also change some Go libraries to their new versions.

Refer https://github.com/kubernetes/autoscaler/pull/1446#issuecomment-465564053.

/assign @losipiuk ",closed,True,2019-02-21 06:36:49,2019-02-21 15:20:03
autoscaler,garo,https://github.com/kubernetes/autoscaler/issues/1709,https://api.github.com/repos/kubernetes/autoscaler/issues/1709,Ability to tag priorities to different ASG's for preferring particular array. ,"I wish there would be a way to set priorities to different autoscaling groups so that the cluster-autoscaler would prefer to scale one group instead of another based on a pod's nodeSelector.

My use case is that I'm running batch loads which can use any worker node, but I want to mainly use cheap AWS Spot instances. The spot instances have a taint set so that only suitable workloads (pods that can be killed at any moment without affecting the whole batch operation) are scheduled there and my batch load has an appropriate toleration.

With an appropriate toleration and a nodeSelector I can force my pods to fit only to the spot instances and cluster-autoscaler will scale those up and this works. However what does not currently work is that I don't have any meaningful way to utilise free resources on my non-spot (permanent) worker nodes while still preventing cluster-autoscaler from spinning up permanent nodes instead of spot nodes.
",open,False,2019-02-21 08:50:49,2019-04-05 01:11:18
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/issues/1710,https://api.github.com/repos/kubernetes/autoscaler/issues/1710,Update Godeps on cluster-autoscaler-release-1.14,We'll need to sync them to Kubernetes release-1.14 closer to the release.,open,False,2019-02-21 10:45:31,2019-02-26 06:58:49
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1711,https://api.github.com/repos/kubernetes/autoscaler/issues/1711,Fix vmss scaling issues when node's providerIDs are in different cases,"Fix VMSS scaling issues when node's providerIDs are in different cases.

For VMSS node's providerIDs, they may be in different cases depending user's PUT requests to the resources. This would cause autoscaler to delete all newly provisioned nodes. And more seriously, CA would scale up/down again and again.

This PR adds a workaround for this issue, so that different cased providerIDs are processed as same node.

Cross refer https://github.com/kubernetes/kubernetes/issues/71994.

/assign @aleksandra-malinowska @losipiuk 

cc @andyzhangx @ritazh ",closed,True,2019-02-22 03:26:45,2019-02-27 13:44:49
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1712,https://api.github.com/repos/kubernetes/autoscaler/issues/1712,VPA switches to 0.4.0,"Change default version
Update docs",closed,True,2019-02-22 10:13:09,2019-02-22 16:59:12
autoscaler,forkbomber,https://github.com/kubernetes/autoscaler/pull/1713,https://api.github.com/repos/kubernetes/autoscaler/issues/1713,Rename vpa-target-reader ClusterRoleBinding name,"This looks like unintended effect of renaming VPA ClusterRole `scale-reader` to `vpa-target-reader` in 0bc25d5.

Repetitive `vpa-vpa-` while being technically correct and in accordance with naming convention in a surrounding blocks just looks wrong.",closed,True,2019-02-22 12:19:44,2019-02-25 10:27:05
autoscaler,fog1985,https://github.com/kubernetes/autoscaler/issues/1714,https://api.github.com/repos/kubernetes/autoscaler/issues/1714,Add scale up threshold instead of scaling up only when new pods already can't be run. For upfront scaling.,"It seems to me a little bit illogical scaling up only when there are no resources for starting containers at all.
I didn't find any parameters for making this value E.G. 70% as for scaling down which is 50% by default.
Please correct me if I am wrong and we can tell `CA` to scale upfront and not only when there are no more resources. Which would be very useful for highly loaded production systems.",closed,False,2019-02-22 15:02:51,2019-03-29 13:01:51
autoscaler,anoora17,https://github.com/kubernetes/autoscaler/issues/1715,https://api.github.com/repos/kubernetes/autoscaler/issues/1715,AWS Autodiscovery ,"
I have problem applying Autoscaler https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml I got the error below:

I am using EKS with  Kubernetes version 1.11, changed the tags as the documentation recommended 
to k8s.io/cluster-autoscaler/enabled and   added this tag k8s.io/cluster-autoscaler/my_eks =1

 command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/my_eks

also changed the cert path in
` volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-bundle.crt
              readOnly: true
          imagePullPolicy: ""Always""
      volumes:
        - name: ssl-certs
          hostPath:
            path: ""/etc/ssl/certs/ca-bundle.crt""`
 This is the error am getting, even I exported all the AWS env for region and keys
`I0222 21:20:27.282690       1 cloud_provider_builder.go:72] Building aws cloud provider.
E0222 21:20:27.282851       1 aws_manager.go:121] Failed to regenerate ASG cache: cannot autodiscover ASGs: MissingRegion: could not find region configuration
F0222 21:20:27.282869       1 cloud_provider_builder.go:137] Failed to create AWS Manager: cannot autodiscover ASGs: MissingRegion: could not find region configuration`",open,False,2019-02-22 21:30:02,2019-04-05 00:54:32
autoscaler,openstacker,https://github.com/kubernetes/autoscaler/issues/1716,https://api.github.com/repos/kubernetes/autoscaler/issues/1716,Prevent Cluster Autoscaler from scaling down a particular node by label,"Hi @mwielgus @MaciekPytel , Is it possible to use lable instead annotation to prevent CA scaling down a particular node? Thanks.",open,False,2019-02-22 23:12:12,2019-03-04 23:07:49
autoscaler,jottofar,https://github.com/kubernetes/autoscaler/pull/1717,https://api.github.com/repos/kubernetes/autoscaler/issues/1717,Allow custom AWS region overrides #1707,"Replicated changes from kubernetes ""Add AWS Custom Endpoint capability #70588"" into cluster-autoscaler:
Modified k8s cloudprovider aws like kubernetes but exposed key type/methods for use by cluster-autoscaler aws_manager.

Closes https://github.com/kubernetes/autoscaler/issues/1707

**Which issue(s) this PR fixes:**

Fixes https://github.com/kubernetes/autoscaler/issues/1707",closed,True,2019-02-23 17:16:47,2019-03-03 23:45:07
autoscaler,justaugustus,https://github.com/kubernetes/autoscaler/pull/1718,https://api.github.com/repos/kubernetes/autoscaler/issues/1718,Add SECURITY_CONTACTS,"Closes: #893 

Signed-off-by: Stephen Augustus <saugustus@vmware.com>

I pulled the approvers from OWNERS and added them as security contacts, but let me know if I need to adjust them.

/assign @bskiba @MaciekPytel @mwielgus ",closed,True,2019-02-24 05:34:47,2019-02-24 23:22:49
autoscaler,lookstar,https://github.com/kubernetes/autoscaler/pull/1719,https://api.github.com/repos/kubernetes/autoscaler/issues/1719,AliCloud - Add asg configuration's tag to node,"This code allows labels of node to be set from cluster-autoscaler based on tags in asg of Alicloud. 
The tag format follows AWS's convention -  k8s.io/cluster-autoscaler/node-template/label/",closed,True,2019-02-25 07:16:37,2019-02-25 15:59:24
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1720,https://api.github.com/repos/kubernetes/autoscaler/issues/1720,Use separate client for events,"Using separate Kubernetes client for events is recommended practice. Also, throttling is especially painful with bulk scale down of empty nodes.",closed,True,2019-02-25 12:33:34,2019-02-26 20:00:20
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1721,https://api.github.com/repos/kubernetes/autoscaler/issues/1721,Only soft taint nodes if there's no scale down to do,"Soft tainting nodes when there are nodes we can delete immediately uses up quota, increasing the chance of delete failure. Moving it to trigger only as ""best effort"" - i.e. when there's nothing else to do and no errors.

cc @jkaniuk ",closed,True,2019-02-25 14:17:12,2019-02-26 17:18:53
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1722,https://api.github.com/repos/kubernetes/autoscaler/issues/1722,Control deprecation of v1beta1 api,,closed,True,2019-02-25 14:23:21,2019-02-26 10:33:44
autoscaler,ringtail,https://github.com/kubernetes/autoscaler/pull/1723,https://api.github.com/repos/kubernetes/autoscaler/issues/1723,Add labels support for Alibaba Cloud Provider,"link to the pr (https://github.com/kubernetes/autoscaler/pull/1719) . @lookstar Hi, If you use autoscaler in Alibaba Cloud Kubernetes,it has supported labels right now. And I am sorry to forget merge these code from internal branch. The difference is we will attach every ess label as node label instead of the value under a specific key. because the other node labels are also plain tag.",closed,True,2019-02-25 15:39:29,2019-02-25 16:05:13
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1724,https://api.github.com/repos/kubernetes/autoscaler/issues/1724,Use patch request to taint nodes,Tainting by update frequently fails due to conflicts unrelated to taints.,closed,True,2019-02-25 16:08:44,2019-03-06 13:30:21
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1725,https://api.github.com/repos/kubernetes/autoscaler/issues/1725,Godeps update to pull in klog,"Prerequisite for #1626 

@Rajat-0 ",closed,True,2019-02-25 16:32:51,2019-02-26 10:54:44
autoscaler,mossuchida,https://github.com/kubernetes/autoscaler/issues/1726,https://api.github.com/repos/kubernetes/autoscaler/issues/1726,HPA & VPA are not compatible by design,"Objective:
- Autoscale both vertically & horizontally to achieve the best resource utilization for kubernetes cluster by using HPA & VPA

Issue:
- Default Scheduler uses ""resource request"" to determine which pod to be deployed on which node, however, hardcoded ""resource request"" value might be wrong. VPA is capable of changing the ""resource request"" dynamically so that each pods will be routed to a node which has more resources.  However, HPA also uses ""resource request"" value to determine the scale up/down & it needs fixed value, there is a conflict between HPA v.s. VPA & Default Scheduler combination.

Proposed Solution
- Current HPA users need ""resource request"" for HPA, by default, HPA should use ""resource request"", however, to support HPA + VPA, add an option to change the HPA base value from ""resource request"" to ""resource limit""

Benefit:
- User is able to reschedule over-utilized pods to other nodes using VPA & Default scheduler. When resource utilization is reached to maximum performance of the pod & there is no more performance gained by the single pod, HPA will deploy additional pod.  Default scheduler uses current pod resource request (real time resource request by VPA) to accurately schedule additional pod to unutilized node, thus, we are able to achieve the maximum resource utilization automatically. [Scheduler-v2.pptx](https://github.com/kubernetes/autoscaler/files/2903133/Scheduler-v2.pptx)",closed,False,2019-02-26 00:01:40,2019-02-27 17:22:52
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1727,https://api.github.com/repos/kubernetes/autoscaler/issues/1727,VPA - add label api to vpa_objects_count metric,,closed,True,2019-02-26 09:18:48,2019-02-26 10:51:29
autoscaler,nielsole,https://github.com/kubernetes/autoscaler/issues/1728,https://api.github.com/repos/kubernetes/autoscaler/issues/1728,Continue operation if parsing some ASG fails,"One of our ASGs uses mixed instance types and has 0 instances.

Expected Behaviour:
Autoscaler behaves like described in: https://github.com/kubernetes/autoscaler/issues/1647.
The autoscaler fails to parse the mixed ASG with
`Unable to build proper template node for {{ASG_NAME}}: Unable to get instance type from launch config or launch template`
 and continues with the existing others.

Actual Behaviour:
The autoscaler fails to parse the mixed ASG with
`Unable to build proper template node for {{ASG_NAME}}: Unable to get instance type from launch config or launch template` and stops processing ASGs with the following message:
`static_autoscaler.go:312] Failed to scale up: failed to build node infos for node groups: Unable to get instance type from launch config or launch template`

I just had to remove the ASG and everything was fine, but a single invalid ASG shouldn't incapacitate the Autoscaler IMO.",open,False,2019-02-26 12:03:16,2019-04-02 18:16:46
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1729,https://api.github.com/repos/kubernetes/autoscaler/issues/1729,"Cluster Autoscaler 1.2: cherry-picks of #1641, #1703 and #1708","Cluster Autoscaler 1.2: cherry-picks of #1641, #1703 and #1708

- [#1641](https://github.com/kubernetes/autoscaler/pull/1641): Fix windows name parsing for Azure VMAS nodes 
- [#1703](https://github.com/kubernetes/autoscaler/pull/1703): Fix error message for long-waiting operations
- [#1708](https://github.com/kubernetes/autoscaler/pull/1708) (partially): Update overrides for go-autorest
",closed,True,2019-02-26 14:20:00,2019-02-26 14:50:32
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1730,https://api.github.com/repos/kubernetes/autoscaler/issues/1730,"Cluster Autoscaler 1.3: cherry-picks of #1641, #1703 and #1708","Cluster Autoscaler 1.3: cherry-picks of #1641, #1703 and #1708

- [#1641](https://github.com/kubernetes/autoscaler/pull/1641): Fix windows name parsing for Azure VMAS nodes 
- [#1703](https://github.com/kubernetes/autoscaler/pull/1703): Fix error message for long-waiting operations
- [#1708](https://github.com/kubernetes/autoscaler/pull/1708) (partially): Update overrides for go-autorest
",closed,True,2019-02-26 14:20:40,2019-02-26 15:54:52
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1731,https://api.github.com/repos/kubernetes/autoscaler/issues/1731,"Cluster Autoscaler 1.13: cherry-picks of #1641, #1703 and #1708","Cluster Autoscaler 1.13: cherry-picks of #1641, #1703 and #1708

- [#1641](https://github.com/kubernetes/autoscaler/pull/1641): Fix windows name parsing for Azure VMAS nodes 
- [#1703](https://github.com/kubernetes/autoscaler/pull/1703): Fix error message for long-waiting operations
- [#1708](https://github.com/kubernetes/autoscaler/pull/1708) (partially): Update overrides for go-autorest
",closed,True,2019-02-26 14:21:10,2019-02-26 15:54:39
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1732,https://api.github.com/repos/kubernetes/autoscaler/issues/1732,"Cluster Autoscaler 1.12: cherry-picks of #1641, #1703 and #1708","Cluster Autoscaler 1.12: cherry-picks of #1641, #1703 and #1708

- [#1641](https://github.com/kubernetes/autoscaler/pull/1641): Fix windows name parsing for Azure VMAS nodes 
- [#1703](https://github.com/kubernetes/autoscaler/pull/1703): Fix error message for long-waiting operations
- [#1708](https://github.com/kubernetes/autoscaler/pull/1708) (partially): Update overrides for go-autorest
",closed,True,2019-02-26 14:23:53,2019-02-26 16:25:27
autoscaler,schylek,https://github.com/kubernetes/autoscaler/pull/1733,https://api.github.com/repos/kubernetes/autoscaler/issues/1733,VPA - Expose TargetRef in VPA model struct.,,closed,True,2019-02-27 04:46:20,2019-02-27 08:33:53
autoscaler,hongli-my,https://github.com/kubernetes/autoscaler/issues/1734,https://api.github.com/repos/kubernetes/autoscaler/issues/1734,"vpa recommendation only have containerName, can not distinguish which pod?","if  two pod  have same label and also have same container name,  like :`my-rec-container3`;
can not find  which pod's container will use vpa recommendation.

```
apiVersion: v1
items:
- apiVersion: autoscaling.k8s.io/v1beta1
  kind: VerticalPodAutoscaler
  metadata:
    clusterName: """"
    creationTimestamp: 2019-02-25T07:39:52Z
    generation: 0
    name: my-rec-vpa1
    namespace: tdocker-vm
    resourceVersion: ""1560610""
    selfLink: /apis/autoscaling.k8s.io/v1beta1/namespaces/tdocker-vm/verticalpodautoscalers/my-rec-vpa1
    uid: 89a5d2d8-38d0-11e9-9ab5-000c292a834a
  spec:
    selector:
      matchLabels:
        purpose: try-recommend1
    updatePolicy:
      updateMode: Update
  status:
    conditions:
    - lastTransitionTime: 2019-02-25T07:40:31Z
      status: ""True""
      type: RecommendationProvided
    recommendation:
      containerRecommendations:
      - containerName: my-rec-container3
        lowerBound:
          cpu: 586m
          memory: 262144k
        target:
          cpu: 587m
          memory: 262144k
        uncappedTarget:
          cpu: 587m
          memory: 262144k
        upperBound:
          cpu: 936m
          memory: 262144k
kind: List
metadata:
  resourceVersion: """"
  selfLink: """"
```",closed,False,2019-02-27 06:44:02,2019-02-27 10:03:53
autoscaler,vivekbagade,https://github.com/kubernetes/autoscaler/pull/1735,https://api.github.com/repos/kubernetes/autoscaler/issues/1735,Added target size cache to gceCache for GKEMig,"Added migTargetSizeCache in GceCache to cache TargetSize calls for
each GKEMig for each loop of the CA. The cache is invalidated during
refresh that occurs during the start of each loop and during
CreateNodeGroup and DeleteNodeGroup.",closed,True,2019-02-27 13:36:02,2019-03-01 17:54:44
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1736,https://api.github.com/repos/kubernetes/autoscaler/issues/1736,Fix nil reference in admission controller registration.,,closed,True,2019-02-27 13:47:23,2019-02-27 14:01:56
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1737,https://api.github.com/repos/kubernetes/autoscaler/issues/1737,Missing ginkgo import,,closed,True,2019-02-27 13:57:35,2019-02-27 14:27:26
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1738,https://api.github.com/repos/kubernetes/autoscaler/issues/1738,Cluster Autoscaler:  add GetInstanceID() for cloudprovider interface,"This PR adds a new `GetInstanceID()` for cloud provider interface, which is required for fixing the VMSS cases different issues.

For VMSS node's providerIDs, they may be in different cases depending user's PUT requests to the resources. This would cause autoscaler to delete all newly provisioned nodes. And more seriously, CA would scale up/down again and again.

This PR adds a workaround for this issue, so that they are handled in lower cases in CA.

/assign @mwielgus  @losipiuk

cc @andyzhangx @ritazh",closed,True,2019-02-27 14:38:44,2019-03-02 04:47:48
autoscaler,stefansedich,https://github.com/kubernetes/autoscaler/pull/1739,https://api.github.com/repos/kubernetes/autoscaler/issues/1739,Add missing daemonsets to AWS example cluster role,"The AWS examples appear to be missing the daemonsets resource under the apps apiGroup, looks like this was updated for the Azure examples recently so adding the same to the AWS examples.",closed,True,2019-02-28 18:37:54,2019-03-01 19:24:20
autoscaler,stefansedich,https://github.com/kubernetes/autoscaler/pull/1740,https://api.github.com/repos/kubernetes/autoscaler/issues/1740,Regenerate the ec2 instance types using latest metadata,"Originally I found that the g3s.xlarge instance type was not reporting a GPU so after much back and forth I finally got AWS to add it to the metadata endpoint.

The changes were generated by running `gen.go`, it looks to have fixed up the g3s.xlarge instance but also added a bunch of new instance types, if I ran it wrong let me know.",closed,True,2019-03-01 00:24:18,2019-03-04 10:23:20
autoscaler,khteh,https://github.com/kubernetes/autoscaler/issues/1741,https://api.github.com/repos/kubernetes/autoscaler/issues/1741,It does NOT scale in,"Here is my HPA definition:
```
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: my-hpa
  namespace: default 
spec:
  minReplicas: 2
  maxReplicas: 5
  scaleTargetRef:
    apiVersion: apps/v1 
    kind: StatefulSet
    name: myapp
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageUtilization: 50
  - type: Resource
    resource:
      name: memory
      targetAverageUtilization: 50
```
`kubectl top pods`:
```
myapp-0            3m           499Mi           
myapp-1            3m           454Mi           
myapp-2            3m           469Mi     
```
`kubectl get hpa`:
```
my-hpa    StatefulSet/myapp    46%/50%, 0%/50%   2         5         3          16d
```
I expect it to have only 2 replicas (minReplicas) since the metrics suggest that the CPU and RAM utilization are well below the water marks.",closed,False,2019-03-01 02:19:22,2019-03-01 14:10:11
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1742,https://api.github.com/repos/kubernetes/autoscaler/issues/1742,Add Index for VPA Docs,,closed,True,2019-03-01 06:02:33,2019-03-01 10:17:28
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1743,https://api.github.com/repos/kubernetes/autoscaler/issues/1743,Correctly set condition when recommendation is not present,,closed,True,2019-03-01 09:48:52,2019-03-01 11:36:22
autoscaler,cablespaghetti,https://github.com/kubernetes/autoscaler/issues/1744,https://api.github.com/repos/kubernetes/autoscaler/issues/1744,AWS AZ Rebalancing Unexpectedly Terminates Nodes,"Not strictly speaking an autoscaler issue, but I will raise a PR to at least mention this in the docs if nothing else.

We couldn't work out why some of our nodes were getting _surprise_ terminated without being drained first. Our apps need to get a nice SIGTERM and shutdown nicely or things get a bit wobbly.

I've now traced it to a feature of AWS Autoscaling Groups, which is designed to keep an even number of instances in each AZ. It will automatically spin up a new instance in the AZ with less instances and terminate an instance in the AZ with too many. Which is of course not very helpful in our use case. https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html#arch-AutoScalingMultiAZ

Here's the docs of how to disable or ""Suspend"" this feature for your Autoscaling group, which I imagine anyone using this autoscaler on AWS will probably want to do: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html",closed,False,2019-03-01 17:01:02,2019-03-11 16:18:30
autoscaler,jottofar,https://github.com/kubernetes/autoscaler/pull/1745,https://api.github.com/repos/kubernetes/autoscaler/issues/1745,Allow custom AWS region overrides #1707,"Replicated changes from kubernetes ""Add AWS Custom Endpoint capability #70588"" into cluster-autoscaler:
- Modified aws_manager similar to kubernetes aws.
- Modified kubernetes aws exposing type/methods used by cluster-autoscaler aws_manager.

Closes #1707

Fixes #1707",closed,True,2019-03-03 23:46:40,2019-03-12 23:34:39
autoscaler,cablespaghetti,https://github.com/kubernetes/autoscaler/pull/1746,https://api.github.com/repos/kubernetes/autoscaler/issues/1746,"Improve AWS ""gotchas"" list and discoverability of cloud-provider READMEs.","Better document the caveats of running a multi-az autoscaling group with the cluster autoscaler on AWS. Also help people find the cloud provider specific documentation rather than just the FAQ.

Fixes #1744.",closed,True,2019-03-04 11:43:17,2019-03-05 14:50:43
autoscaler,sylr,https://github.com/kubernetes/autoscaler/issues/1747,https://api.github.com/repos/kubernetes/autoscaler/issues/1747,Autoscaler scales down even if there is no more nodes to schedule removed pods,"Here's my config,

```
      - command:
        - ./cluster-autoscaler
        - --v=3
        - --logtostderr=true
        - --cloud-provider=azure
        - --skip-nodes-with-local-storage=false
        - --nodes=0:2:k8s11-euw-dev-agentmntr1-vmss
        - --nodes=0:4:k8s11-euw-dev-agentzone1-vmss
        - --nodes=0:4:k8s11-euw-dev-agentzone2-vmss
        - --nodes=0:4:k8s11-euw-dev-agentzone3-vmss
        - --scale-down-delay-after-add=30m0s
        - --scale-down-delay-after-delete=2m0s
        - --scale-down-delay-after-failure=3m0s
        - --scale-down-unneeded-time=1m0s
        - --scale-down-unready-time=5m0s
        - --kubeconfig=/etc/config/kubeconfig
        - --kubernetes=https://127.0.0.1:443
```

In a newly created k8s cluster where not much workload has been deployed the cluster autoscaler scales down all the VMSS to 0 even if there are a few pods that needs to be schedules.

```
I0305 07:09:11.070594       1 utils.go:541] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0305 07:09:11.071067       1 static_autoscaler.go:274] No unschedulable pods
I0305 07:09:11.071347       1 scale_down.go:373] Scale-down calculation: ignoring 1 nodes unremovable in the last 5m0s
I0305 07:09:11.071582       1 cluster.go:81] Fast evaluation: k8s11-euw-dev-agentzone1-vmss-000007 for removal
I0305 07:09:11.072234       1 cluster.go:112] Fast evaluation: node k8s11-euw-dev-agentzone1-vmss-000007 may be removed
I0305 07:09:21.534996       1 utils.go:541] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0305 07:09:21.535390       1 static_autoscaler.go:274] No unschedulable pods
I0305 07:09:21.535663       1 scale_down.go:373] Scale-down calculation: ignoring 1 nodes unremovable in the last 5m0s
I0305 07:09:21.536010       1 cluster.go:81] Fast evaluation: k8s11-euw-dev-agentzone1-vmss-000007 for removal
I0305 07:09:21.536760       1 cluster.go:112] Fast evaluation: node k8s11-euw-dev-agentzone1-vmss-000007 may be removed
I0305 07:09:31.765819       1 utils.go:541] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0305 07:09:31.766205       1 static_autoscaler.go:274] No unschedulable pods
I0305 07:09:31.766466       1 scale_down.go:373] Scale-down calculation: ignoring 1 nodes unremovable in the last 5m0s
I0305 07:09:31.766754       1 cluster.go:81] Fast evaluation: k8s11-euw-dev-agentzone1-vmss-000007 for removal
I0305 07:09:31.767467       1 cluster.go:112] Fast evaluation: node k8s11-euw-dev-agentzone1-vmss-000007 may be removed
I0305 07:09:31.767532       1 scale_down.go:584] k8s11-euw-dev-agentzone1-vmss-000007 was unneeded for 1m2.1797981s
I0305 07:09:31.767762       1 cluster.go:81] Detailed evaluation: k8s11-euw-dev-agentzone1-vmss-000007 for removal
I0305 07:09:33.571572       1 cluster.go:112] Detailed evaluation: node k8s11-euw-dev-agentzone1-vmss-000007 may be removed
I0305 07:09:33.571606       1 scale_down.go:685] Scale-down: removing node k8s11-euw-dev-agentzone1-vmss-000007, utilization: 0.365, pods to reschedule: kube-system/calico-typha-horizontal-autoscaler-847fc7bc8d-v9shk,kube-system/metrics-server-69b44566d5-chw6p,kube-system/calico-typha-c7774584-rcft9,kube-system/coredns-d5b7bc49d-lxlwj,logging/tiller-deploy-fbb5cff99-dqpqr,kube-system/coredns-d5b7bc49d-4x7h9,kube-system/kubernetes-dashboard-7947fffdf5-8w5k4,kube-system/coredns-d5b7bc49d-vljmh,logging/collectorforkubernetes-addon-6bf9c8b7fb-ndrnk,kube-system/metrics-server-69b44566d5-fsssr,monitoring/tiller-deploy-7844556cb8-tpwgb
I0305 07:09:33.610947       1 delete.go:64] Successfully added toBeDeletedTaint on node k8s11-euw-dev-agentzone1-vmss-000007
```

After this all the VMSS where at 0 so it triggered a scale up right after.",closed,False,2019-03-05 08:51:26,2019-03-05 10:30:11
autoscaler,sylr,https://github.com/kubernetes/autoscaler/issues/1748,https://api.github.com/repos/kubernetes/autoscaler/issues/1748,Azure: implement tainted VMSS,"In the event a VMSS can be scaled to 0 the cluster autoscaler might become unaware a VMSS spawns tainted nodes so it would be nice to implement a way for the cluster autoscaler to discover the taints a VMSS applies to the nodes it spawns.

AWS implemetation does this by reading ASG (equivalent of Azure VMSS ?) tags: https://github.com/kubernetes/autoscaler/blob/d9ad9f438a42afe7a7abf9a3ea4db1d603b9367a/cluster-autoscaler/cloudprovider/aws/README.md#scaling-a-node-group-to-0

",open,False,2019-03-05 09:46:56,2019-03-05 13:41:57
autoscaler,krishpavan,https://github.com/kubernetes/autoscaler/issues/1749,https://api.github.com/repos/kubernetes/autoscaler/issues/1749,k8s in azure autoscaler with vmss,"I have built k8s with kubeadm with vmss and autocaler. The autoscaler work with job submission and add worker nodes join k8s. I have two ASG groups  vmss1, vmss0 with low priority vms.  
The issue is vm are not registered with ASG group and make the cluster unhealthy.  Attached the logs. 

Regards
Krish


azureuser@kubevanmaster:~$ kubectl get nodes --show-labels
NAME              STATUS   ROLES    AGE     VERSION   LABELS
kubevanmaster     Ready    master   3d19h   v1.13.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=kubevanmaster,kubernetes.io/role=master,node-role.kubernetes.io/master=
vmss05cd9000004   Ready    agent    7h22m   v1.13.4   agentpool=vmss0,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=Standard_D1_v2,beta.kubernetes.io/os=linux,kubernetes.io/agent=,kubernetes.io/hostname=vmss05cd9000004,kubernetes.io/role=agent
vmss17819000007   Ready    agent    7h23m   v1.13.4   agentpool=vmss1,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=Standard_D1_v2,beta.kubernetes.io/os=linux,kubernetes.io/agent=,kubernetes.io/hostname=vmss17819000007,kubernetes.io/role=agent,spotfleet=true vmss17819000007   Ready    agent    7h22m   v1.13.4   agentpool=vmss1,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=Standard_D1_v2,beta.kubernetes.io/os=linux,kubernetes.io/agent=,kubernetes.io/hostname=vmss17819000007,kubernetes.io/role=agent,

azureuser@kubevanmaster:~$ kubectl describe  configmap cluster-autoscaler-status -n kube-system
Name:         cluster-autoscaler-status
Namespace:    kube-system
Labels:       <none>
Annotations:  cluster-autoscaler.kubernetes.io/last-updated: 2019-03-05 13:42:38.5694064 +0000 UTC

Data
====
status:
----
Cluster-autoscaler status at 2019-03-05 13:42:38.5694064 +0000 UTC:
Cluster-wide:
  Health:      Healthy (ready=3 unready=0 notStarted=0 longNotStarted=0 registered=3 longUnregistered=2)
               LastProbeTime:      2019-03-05 13:42:37.9341032 +0000 UTC m=+26566.679524601
               LastTransitionTime: 2019-03-05 06:20:22.3535484 +0000 UTC m=+31.098969801
  ScaleUp:     NoActivity (ready=3 registered=3)
               LastProbeTime:      2019-03-05 13:42:37.9341032 +0000 UTC m=+26566.679524601
               LastTransitionTime: 2019-03-05 06:20:22.3535484 +0000 UTC m=+31.098969801
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2019-03-05 13:42:37.9341032 +0000 UTC m=+26566.679524601
               LastTransitionTime: 2019-03-05 06:20:22.3535484 +0000 UTC m=+31.098969801

NodeGroups:
  Name:        vmss0
  Health:      Healthy (ready=0 unready=0 notStarted=0 longNotStarted=0 registered=0 longUnregistered=1 cloudProviderTarget=1 (minSize=1, maxSize=10))
               LastProbeTime:      2019-03-05 13:42:37.9341032 +0000 UTC m=+26566.679524601
               LastTransitionTime: 2019-03-05 06:20:22.3535484 +0000 UTC m=+31.098969801
  ScaleUp:     NoActivity (ready=0 cloudProviderTarget=1)
               LastProbeTime:      2019-03-05 13:42:37.9341032 +0000 UTC m=+26566.679524601
               LastTransitionTime: 2019-03-05 06:20:22.3535484 +0000 UTC m=+31.098969801
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2019-03-05 13:42:37.9341032 +0000 UTC m=+26566.679524601
               LastTransitionTime: 2019-03-05 06:20:22.3535484 +0000 UTC m=+31.098969801

  Name:        vmss1
  Health:      Healthy (ready=0 unready=0 notStarted=0 longNotStarted=0 registered=0 longUnregistered=1 cloudProviderTarget=1 (minSize=1, maxSize=10))
               LastProbeTime:      2019-03-05 13:42:37.9341032 +0000 UTC m=+26566.679524601
               LastTransitionTime: 2019-03-05 06:20:22.3535484 +0000 UTC m=+31.098969801
  ScaleUp:     NoActivity (ready=0 cloudProviderTarget=1)
               LastProbeTime:      2019-03-05 13:42:37.9341032 +0000 UTC m=+26566.679524601
               LastTransitionTime: 2019-03-05 06:20:22.3535484 +0000 UTC m=+31.098969801
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2019-03-05 13:42:37.9341032 +0000 UTC m=+26566.679524601
               LastTransitionTime: 2019-03-05 06:20:22.3535484 +0000 UTC m=+31.098969801


Events:  <none>

The relevant error in logs , I have attached full logs of autoscaler pod.

Warning  ScaleUpTimedOut     13m                  cluster-autoscaler  Nodes added to group vmss1 failed to register within 22m3.133659s
  Warning  ScaleUpTimedOut     13m                  cluster-autoscaler  Nodes added to group vmss0 failed to register within 23m51.4120586s
&
I0305 05:31:57.371400       1 factory.go:33] Event(v1.ObjectReference{Kind:""ConfigMap"", Namespace:""kube-system"", Name:""cluster-autoscal                      er-status"", UID:""445789ff-3f05-11e9-99d8-000d3a1205c5"", APIVersion:""v1"", ResourceVersion:""1932043"", FieldPath:""""}): type: 'Warning' rea                      son: 'ScaleUpTimedOut' Nodes added to group vmss1 failed to register within 15m5.1236732s
",closed,False,2019-03-05 14:38:20,2019-03-06 18:10:58
autoscaler,mattnworb,https://github.com/kubernetes/autoscaler/issues/1750,https://api.github.com/repos/kubernetes/autoscaler/issues/1750,'vertical-pod-autoscaler-0.4.0' tag points to 0.3.1 images,"For example:

https://github.com/kubernetes/autoscaler/blob/vertical-pod-autoscaler-0.4.0/vertical-pod-autoscaler/deploy/recommender-deployment.yaml#L23

https://github.com/kubernetes/autoscaler/blob/7f77136aeea401ca5199292c4e5eb17eb1597e55/vertical-pod-autoscaler/deploy/recommender-deployment.yaml#L23",closed,False,2019-03-05 16:01:33,2019-03-06 11:18:27
autoscaler,mfilotto,https://github.com/kubernetes/autoscaler/issues/1751,https://api.github.com/repos/kubernetes/autoscaler/issues/1751,Add ephemeral storage requests recommendation and update to vpa,It would be nice if vpa could also take care of ephemeral storage usage and set request accordinngly,open,False,2019-03-05 23:14:53,2019-03-09 20:21:54
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1752,https://api.github.com/repos/kubernetes/autoscaler/issues/1752,Update to debian-base-0.4.1,,closed,True,2019-03-06 11:00:55,2019-03-06 11:14:08
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1753,https://api.github.com/repos/kubernetes/autoscaler/issues/1753,Cluster Autoscaler 1.2.5,,closed,True,2019-03-06 11:31:05,2019-03-07 01:14:11
autoscaler,lcasassa,https://github.com/kubernetes/autoscaler/issues/1754,https://api.github.com/repos/kubernetes/autoscaler/issues/1754,AWS: Failed to look for node info on ASG Launch Template,"I have several ASG with current size 0. All of the ASG are created with ￼Launch Template. I'm using gcr.io/google-containers/cluster-autoscaler:v1.3.7 with the command:

        - ./cluster-autoscaler
        - --cloud-provider=aws
        - --namespace=kube-system
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/eks-tf-mi-playground-1-cluster-01
        - --expander=least-waste
        - --logtostderr=true
        - --stderrthreshold=info
        - --v=4

And somehow it is detecting them as Launch Configuration but it is not. How can I tell autoscaler to look for Launch Template on all ASG instead of Launch Configuration?

This are some logs:

```
I0306 13:16:06.427091       1 auto_scaling.go:48] Failed LaunchConfiguration info request for : ValidationError: 1 validation error detected: Value '[]' at 'launchConfigurationNames' failed to satisfy constraint: Member must satisfy constraint: [Member must have length less than or equal to 1600, Member must have length greater than or equal to 1, Member must satisfy regular expression pattern: [\u0020-\uD7FF\uE000-\uFFFD\uD800\uDC00-\uDBFF\uDFFF\r\n\t]*]
	status code: 400, request id: ffed3ce5-4011-11e9-a015-cb400eaf9394
E0306 13:16:06.427123       1 utils.go:280] Unable to build proper template node for eks-worker-tf-eks-01-m5-2xl-2: ValidationError: 1 validation error detected: Value '[]' at 'launchConfigurationNames' failed to satisfy constraint: Member must satisfy constraint: [Member must have length less than or equal to 1600, Member must have length greater than or equal to 1, Member must satisfy regular expression pattern: [\u0020-\uD7FF\uE000-\uFFFD\uD800\uDC00-\uDBFF\uDFFF\r\n\t]*]
	status code: 400, request id: ffed3ce5-4011-11e9-a015-cb400eaf9394
I0306 13:16:06.513070       1 auto_scaling_groups.go:316] Regenerating instance to ASG map for ASGs: [eks-worker-tf-eks-01-m5-2xl-0 eks-worker-tf-eks-01-m5-2xl-1 eks-worker-tf-eks-01-m5-2xl-2 eks-worker-tf-eks-01-m5-4xl-0 eks-worker-tf-eks-01-m5-4xl-1 eks-worker-tf-eks-01-m5-4xl-2 eks-worker-tf-eks-01-m5-l-0 eks-worker-tf-eks-01-m5-l-1 eks-worker-tf-eks-01-m5-l-2 eks-worker-tf-eks-01-p2-xl-0 eks-worker-tf-eks-01-p2-xl-1 eks-worker-tf-eks-01-p2-xl-2]
I0306 13:16:06.626631       1 aws_manager.go:148] Refreshed ASG list, next refresh after 2019-03-06 13:16:16.626624752 +0000 UTC m=+715.550545349
I0306 13:16:06.626757       1 utils.go:541] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
I0306 13:16:06.626771       1 static_autoscaler.go:260] Filtering out schedulables
I0306 13:16:06.627040       1 static_autoscaler.go:270] No schedulable pods
I0306 13:16:06.627055       1 scale_up.go:249] Pod ml-dev/fuad-neural-training-job-pznxl is unschedulable
I0306 13:16:06.627059       1 scale_up.go:249] Pod ml-dev/ali-changeme-0 is unschedulable
E0306 13:16:06.627091       1 static_autoscaler.go:293] Failed to scale up: Could not compute total resources: No node info for: eks-worker-tf-eks-01-m5-2xl-0
I0306 13:16:08.094772       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler
I0306 13:16:10.104128       1 leaderelection.go:209] successfully renewed lease kube-system/cluster-autoscaler

```",open,False,2019-03-06 13:23:12,2019-04-03 07:14:42
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1755,https://api.github.com/repos/kubernetes/autoscaler/issues/1755,Cluster Autoscaler 1.13 : cherry pick of #1738,Cluster Autoscaler 1.13 : cherry pick of #1738: add GetInstanceID() for cloudprovider interface,closed,True,2019-03-06 14:21:39,2019-03-07 01:14:06
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1756,https://api.github.com/repos/kubernetes/autoscaler/issues/1756,Cluster Autoscaler 1.12 : cherry pick of #1738,Cluster Autoscaler 1.12 : cherry pick of #1738: add GetInstanceID() for cloudprovider interface,closed,True,2019-03-06 14:22:36,2019-03-07 01:13:55
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1757,https://api.github.com/repos/kubernetes/autoscaler/issues/1757,Cluster Autoscaler 1.2 : cherry pick of #1738,Cluster Autoscaler 1.2 : cherry pick of #1738: add GetInstanceID() for cloudprovider interface,closed,True,2019-03-06 14:23:58,2019-03-07 01:13:47
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1758,https://api.github.com/repos/kubernetes/autoscaler/issues/1758,Cluster Autoscaler 1.3 : cherry pick of #1738,Cluster Autoscaler 1.3 : cherry pick of #1738: add GetInstanceID() for cloudprovider interface,closed,True,2019-03-06 14:24:42,2019-03-07 01:14:02
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1759,https://api.github.com/repos/kubernetes/autoscaler/issues/1759,Update debian-base image to 0.4.1,,closed,True,2019-03-06 20:25:42,2019-03-06 23:34:59
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1760,https://api.github.com/repos/kubernetes/autoscaler/issues/1760,Update debian-base image to 0.4.1,,closed,True,2019-03-06 20:30:36,2019-03-06 23:35:00
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1761,https://api.github.com/repos/kubernetes/autoscaler/issues/1761,Update debian-base image to 0.4.1,,closed,True,2019-03-06 20:31:58,2019-03-06 23:34:59
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1762,https://api.github.com/repos/kubernetes/autoscaler/issues/1762,Update debian-base image to 0.4.1,,closed,True,2019-03-06 20:32:59,2019-03-06 23:34:59
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1763,https://api.github.com/repos/kubernetes/autoscaler/issues/1763,Use docker in gofmt verification,"gofmt changes between golang versions.
That PR ensures that we validate accordingly to golang version used for particular release.",closed,True,2019-03-07 13:07:49,2019-03-22 16:47:07
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1764,https://api.github.com/repos/kubernetes/autoscaler/issues/1764,Cluster Autoscaler 1.3.8,,closed,True,2019-03-07 14:53:35,2019-03-07 15:09:03
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1765,https://api.github.com/repos/kubernetes/autoscaler/issues/1765,Cluster Autoscaler 1.12.3,,closed,True,2019-03-07 14:55:20,2019-03-07 15:09:03
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1766,https://api.github.com/repos/kubernetes/autoscaler/issues/1766,Cluster Autoscaler 1.13.2,,closed,True,2019-03-07 14:56:37,2019-03-11 10:52:31
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1767,https://api.github.com/repos/kubernetes/autoscaler/issues/1767,Update godeps based on k8s.io/kubernetes release-1.14 ,As preparation for CA 1.14.0-beta1,closed,True,2019-03-07 20:32:16,2019-03-07 21:06:07
autoscaler,uruddarraju,https://github.com/kubernetes/autoscaler/pull/1768,https://api.github.com/repos/kubernetes/autoscaler/issues/1768,Fixing minor error handling bug in static autoscaler,The previous code was checking an invalid error condition that no longer is valid in the statement's scope.,closed,True,2019-03-07 23:17:19,2019-03-29 22:40:17
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1769,https://api.github.com/repos/kubernetes/autoscaler/issues/1769,Cluster Autoscaler: Cleanup GetInstanceID() interface,"Follow up of https://github.com/kubernetes/autoscaler/pull/1738.

As we agreed, #1738 is a temporary workaround for fixing the isues in stable releases. For the following v1.14, https://github.com/kubernetes/kubernetes/pull/74882 has made a change in kubernetes repo, so that resourceGroupName in providerID would be always in lower cases.

This PR reverts the changes in #1738 and also aligned the providerID in same format from #74882.

Note that with this change, because the providerID is in different format, master branch (or following v1.14 release) of CA won't work again with kubernetes <=1.14.0-beta.1. This is ok because beta version won't be run in production and CA should always be used with matched version of kubernetes according to the [version matrix ](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#releases).

/assign @MaciekPytel @mwielgus @losipiuk 
/cc @ritazh @andyzhangx ",closed,True,2019-03-08 04:32:00,2019-03-08 23:04:41
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1770,https://api.github.com/repos/kubernetes/autoscaler/issues/1770,Cluster Autoscaler 1.14.0-beta.1,,closed,True,2019-03-08 12:07:58,2019-03-08 12:23:26
autoscaler,frobware,https://github.com/kubernetes/autoscaler/pull/1771,https://api.github.com/repos/kubernetes/autoscaler/issues/1771,UPSTREAM: <carry>: fix max cluster size calculation on scale up,"When scaling up the calculation for computing the maximum cluster size
does not take into account the number of any upcoming nodes and it is
possible to grow the cluster beyond the cluster
size (--max-nodes-total).

Fixes: https://bugzilla.redhat.com/show_bug.cgi?id=1670695

See also: 5bc77f051cbce1fbe77d99cc6b0670128f330585",closed,True,2019-03-08 13:37:27,2019-03-08 14:33:39
autoscaler,viggeh,https://github.com/kubernetes/autoscaler/issues/1772,https://api.github.com/repos/kubernetes/autoscaler/issues/1772,[Feature Idea] A simple expander based on static precedence,"This is similar to https://github.com/kubernetes/autoscaler/issues/1709

When using cluster-autoscaler there is no expander available for `price`. There seems to have been some effort put into implementing one but it looks like it has stalled https://github.com/kubernetes/autoscaler/pull/486

There is no simple mechanism currently to have multiple ASGs with different lifecycle types. For instance if you have two identical ASGs, one for spot and another for on-demand, cluster-autoscaler just picks one by random. There is no weighting between them. The same goes for the scenario of having two ASGs with different instance types but one having reserved instances, thus being cheaper.

We could solve this by adding a new expander based on `precedence`. Nodegroups would be created with a `precedence` value, this could be autodiscovered through tags. When picking through available nodegroups, the expander would simply pick the one with the lowest precedence. 

This would solve a lot of problems running in AWS, one could simply set the `precedence` value of the nodegroup to the spot market price being used, or the hourly RI price.

It seems like a quick solution while implementing a price expander for AWS, although being a better solution, seems like a much bigger task.",open,False,2019-03-08 14:42:12,2019-03-22 15:14:43
autoscaler,joelsmith,https://github.com/kubernetes/autoscaler/pull/1773,https://api.github.com/repos/kubernetes/autoscaler/issues/1773,Update embargo doc link in SECURITY_CONTACTS and change PST to PSC,See https://github.com/kubernetes/security/issues/8 for more information,closed,True,2019-03-08 17:52:11,2019-03-09 00:15:13
autoscaler,naseemkullah,https://github.com/kubernetes/autoscaler/issues/1774,https://api.github.com/repos/kubernetes/autoscaler/issues/1774,"QQ: Using VPA in Auto mode, whats the relationship with controller's (deployment's) requests/limits?","when we assign a VPA in Auto mode to a controller, whats the point of having requests/limits defined in the controller and what should they even be?

I notice the VPA edits the pods resources directly but leaves the controller's resources in tact.

TIA
",closed,False,2019-03-09 15:44:02,2019-03-10 17:05:12
autoscaler,naseemkullah,https://github.com/kubernetes/autoscaler/issues/1775,https://api.github.com/repos/kubernetes/autoscaler/issues/1775,QQ: How to override MinAllowed ?,"I notice many pods getting 
```
          cpu: 12m
          memory: 131072k
```

But they actually require less. I would like to override the MinAllowed...

TIA",closed,False,2019-03-10 18:05:39,2019-03-29 12:34:08
autoscaler,nhooyr,https://github.com/kubernetes/autoscaler/issues/1776,https://api.github.com/repos/kubernetes/autoscaler/issues/1776,how does the cluster autoscaler move pods?,"Does it start the new pod first and then shutdown the old one?

Or shutdown the old one and then start the new one?

Would be good for the FAQ.",closed,False,2019-03-11 02:45:13,2019-03-11 14:54:30
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/pull/1777,https://api.github.com/repos/kubernetes/autoscaler/issues/1777,Fix bulk removal issues of AKS scaling,"Fix bulk removal issues of AKS scaling:

> When scaling down we observe unexpected behaviors in the cluster autoscaler, where initially nodes are removed quickly but then mysteriously are added back a few minutes later.

The issue happens because after the virtual machines are removed, CA would also tries to correct the node count by invoking the AKS PUT API. Unfortunately, for bulk removal, multiple virtual machines are deleted in different Go routines, and hence their calculated new counts are wrong.

This PR fixed this issue by adding a lock for ContainerServiceAgentPool. It also adds a cache for getting current node count, so as to reduce the API calls.

Fix #1597 
",closed,True,2019-03-11 03:11:35,2019-03-12 01:25:28
autoscaler,naseemkullah,https://github.com/kubernetes/autoscaler/issues/1778,https://api.github.com/repos/kubernetes/autoscaler/issues/1778,QQ: Can targetref value be a label?,"Hi,
I am dealing with ~900 single pod deployments, as the example used the deployment name for value of targetRef, i am creating 1 vpa per deployment. However all ~900 of these deployments share a common label, which makes me wonder if a single vpa can recommend/adjust resources for all of their pods by targeting with this label. 
TIA",open,False,2019-03-11 12:00:13,2019-03-11 13:58:44
autoscaler,viggeh,https://github.com/kubernetes/autoscaler/pull/1779,https://api.github.com/repos/kubernetes/autoscaler/issues/1779,Ignoring upcoming nodes from unhealthy nodegroups.,"This is a fix for https://github.com/kubernetes/autoscaler/issues/1133

I set the base branch as the 1.13 release branch. Please let me know if that is not the way to go.

If a nodegroup is in an Unhealthy state but has upcoming nodes, it is still considered to be ready to serve a pod that cannot be scheduled.
This means that other nodegroups that might be able to host the pod will not be expanded.

This happens for instance on AWS when you have an ASG with spot instances and the your price is either too high or the market does not have any spot instances available. The autoscaling group will be marked as `Unhealthy` and yet have upcoming nodes that will (maybe) never come. Other autoscaling groups will not get the chance to be expanded.",closed,True,2019-03-11 12:51:22,2019-03-13 13:24:27
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1780,https://api.github.com/repos/kubernetes/autoscaler/issues/1780,Soft taint when there are no candidates,"Quick fix for not applying soft taints when nodes haven't been unneeded for long enough. Long-term, I really think we should reconsider scale down statuses and what they mean (not unneeded for long enough != no unneeded)",closed,True,2019-03-11 13:06:49,2019-03-11 14:15:33
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1781,https://api.github.com/repos/kubernetes/autoscaler/issues/1781,Remove GKE Cloud provider,,closed,True,2019-03-11 16:49:45,2019-03-15 10:22:57
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1782,https://api.github.com/repos/kubernetes/autoscaler/issues/1782,Call CloudProvider.Refresh before getNodeInfosForGroups,"We need to call refresh before getNodeInfosForGroups. If we have
stale state getNodeInfosForGroups may fail and we will end up in infinite crash looping.",closed,True,2019-03-12 11:04:24,2019-03-12 11:20:40
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1783,https://api.github.com/repos/kubernetes/autoscaler/issues/1783,Delete support for v1beta1 API w/o deleting the API itself.,"This makes v1beta1 objects handled via ConfigUnsupported condition, sample status of an v1beta1 object after the change:
```
status:
  conditions:
  - lastTransitionTime: 2019-03-12T13:13:01Z
    message: Label selector is no longer supported, please migrate to targetRef
    status: ""True""
    type: ConfigUnsupported
  - lastTransitionTime: 2019-03-12T13:13:01Z
    status: ""False""
    type: RecommendationProvided
  recommendation: {}
```

**NOTE:** This change is not flag-guarded since it didn't make sense to reuse `beta1-api-deprecated` flag and I didn't wanted to introduce another one. Please let me know if you want this otherwise.

PS. I haven't touched validation in this PR so one can still create v1beta1 objects. This will be part of a separate PR.",closed,True,2019-03-12 13:20:58,2019-03-12 14:30:43
autoscaler,bysnupy,https://github.com/kubernetes/autoscaler/issues/1784,https://api.github.com/repos/kubernetes/autoscaler/issues/1784,When a host is scaling down how can unregister license attached on the host ?,"Some license is subscribed on the each running host, and counting the number through internet.
If the cluster is scaling out, we can attach the license using `user data` (`cloud-init`), 
but how can we unregister the license while the host is scaling down ? `cluster-autoscaler` have any feature to resolve ?

Thanks.",closed,False,2019-03-12 13:31:27,2019-03-14 10:37:29
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1785,https://api.github.com/repos/kubernetes/autoscaler/issues/1785,Forbid creating VPA objects without targetRef,"Follow-up to #1783 .

It's a bit of a poor man's solution since we don't have the version nor v1beta1 fields at hand so I've used (lack of) TargetRef for validation.
Moreover, the validation can be by-passed by updating the object but I guess at that point the user has been already warned.

An attempt to create v1beta1 object (or simply v1beta2 w/o TargetRef) ends up with:
```
Error from server (InternalError): error when creating ""b1.yaml"": Internal error occurred: admission webhook ""vpa.k8s.io"" denied the request: TargetRef is required
```
",closed,True,2019-03-12 16:07:52,2019-03-12 18:30:40
autoscaler,BenTheElder,https://github.com/kubernetes/autoscaler/issues/1786,https://api.github.com/repos/kubernetes/autoscaler/issues/1786,spurious error in logs due to using glog before parsing flags,"This:
https://github.com/kubernetes/autoscaler/blob/9733fe0f0fb50e4efa2c620f5584dd44e6ba356c/addon-resizer/nanny/main/pod_nanny.go#L61-L66

seems to cause spurious errors in the logs like `ERROR: logging before flag.Parse: I0312 19:30:31.381957 1 pod_nanny.go:109] `

xref: https://github.com/kubernetes/test-infra/issues/11739",closed,False,2019-03-12 20:02:21,2019-03-14 08:28:25
autoscaler,yuwenma,https://github.com/kubernetes/autoscaler/issues/1787,https://api.github.com/repos/kubernetes/autoscaler/issues/1787,Rebase the image to distroless,"We are currently working on rebasing k8s base images to distroless. Distroless image has several good features (smaller in size, fewer components requiring CVE patching, etc) but also doesn't have some dependencies autoscaler uses (shell). To use distroless as the base image, the following AIs needs to be done:
Step 1. Sync up with current k8s head.
Reason: oss kubernetes no longer uses glog which requires shell to redirect the log file. Instead, k8s is using klog which accepts a log path flag. This sync-up is necessary to remove the bash file run.sh which is used to redirect the glog.
Step 2. Replace the glog to klog in other autoscaler files.
Step 3. Update the base image to distroless and remove distroless-preinstalled packages like ca-certificate.
Step 4. After the above steps are done, require release engineers' help on monitoring the performance.   ",open,False,2019-03-12 23:28:34,2019-03-13 22:44:50
autoscaler,ismailyenigul,https://github.com/kubernetes/autoscaler/issues/1788,https://api.github.com/repos/kubernetes/autoscaler/issues/1788, Cluster Autoscaler 1.13.2  AWS example yaml ships with 1.2.2 image version,"Hi,

I just deployed  Cluster Autoscaler 1.13.2 on AWS but I got `Failed to create AWS Manager: cannot autodiscover ASGs: MissingRegion: could not find region configuration`

I fixed the problem by updating image version from 1.2.2 to 1.13.2 in cluster-autoscaler-autodiscover.yaml
Please update docker image version in the  1.13 release.
Thanks",closed,False,2019-03-13 09:10:30,2019-03-19 09:42:15
autoscaler,ismailyenigul,https://github.com/kubernetes/autoscaler/pull/1789,https://api.github.com/repos/kubernetes/autoscaler/issues/1789,Update cluster-autoscaler image to 1.13.2 in yaml files,"AWS example yaml files were shipped with image k8s.gcr.io/cluster-autoscaler:v1.2.2 which does not support autodiscovery in cluster-autoscale 1.13 release.
",closed,True,2019-03-13 09:25:00,2019-03-13 20:53:09
autoscaler,estahn,https://github.com/kubernetes/autoscaler/issues/1790,https://api.github.com/repos/kubernetes/autoscaler/issues/1790,VPA for CronJob,Would be great to have the VPA update CronJob resources.,open,False,2019-03-13 10:00:41,2019-03-14 05:03:05
autoscaler,ismailyenigul,https://github.com/kubernetes/autoscaler/pull/1791,https://api.github.com/repos/kubernetes/autoscaler/issues/1791,Notes about multiple AWS AZ  and PV,,closed,True,2019-03-13 20:07:40,2019-03-14 17:45:07
autoscaler,BenTheElder,https://github.com/kubernetes/autoscaler/pull/1792,https://api.github.com/repos/kubernetes/autoscaler/issues/1792,don't log before parsing flags in nanny,fixes #1786 ,closed,True,2019-03-14 01:26:27,2019-03-14 09:10:17
autoscaler,frankgu968,https://github.com/kubernetes/autoscaler/issues/1793,https://api.github.com/repos/kubernetes/autoscaler/issues/1793,EKS CA doesn't see nodeSelector labels,"Hi, 
I am currently using the following: 
EKS 1.11
Cluster-Autoscaler: 1.13.2 (also tried 1.3.8 and 1.12.0)

When I try to schedule new pod with a node selector, it doesn't trigger scale-up and claims that the ASG's GeneralPredicates predicate mismatch. This happens when I try to use my custom defined label with the pods (ie. app=web). However, if I use a system node selector like ""beta.kubernetes.io/instance-type=m5.large"", it works just fine. I have added the appropriate tags to the ASG according to the documentation and given the correct IAM permissions. Any ideas on what else to try? ",open,False,2019-03-14 01:58:27,2019-04-03 17:38:02
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1794,https://api.github.com/repos/kubernetes/autoscaler/issues/1794,Update scheduler initialization in predicates.go,"Update scheduler initialization in predicates.go
    
With previous code and recent changes to scheduler codebase we did not
initialize informer hooks which updated scheduler cache used by some of
the predicate function (especially MatchInterPodAffinity).
    
This change replicates how scheduler is initialized now - I.e now we
actually create Scheduler object now and do explicity call AddAllEventHandlers.
    
As a followup a discussion will be started with sig-scheduling to make
this initialization in CA simpler and with less copy-pasting of code
from scheduler codebase.

cc: @bsalamat, @misterikkit 
",closed,True,2019-03-14 12:11:28,2019-03-21 17:52:17
autoscaler,sc250024,https://github.com/kubernetes/autoscaler/issues/1795,https://api.github.com/repos/kubernetes/autoscaler/issues/1795,"AWS Ondemand not scaled up if Spot requests remain ""Open""","Greetings,

I'm running cluster-autoscaler v1.3.8 in a Kubernetes v1.11.8 cluster in AWS created using Kops. I'm not sure if this falls under feature request, a bug, or a misconfiguration on my part, so apologies in advance if it's a misconfiguration. Basically, for our pre-production cluster, I want to run spot instances as much as possible. I have the following node groups (and thus, ASGs), and they look like the following if the cluster is running normally:

```text
ASG Name                   Instances  Desired  Min  Max
nodes-ondemand-eu-west-1a  0          0        0    10
nodes-ondemand-eu-west-1b  0          0        0    10
nodes-ondemand-eu-west-1c  0          0        0    10
nodes-spot-eu-west-1a      2          2        2    10
nodes-spot-eu-west-1b      2          2        2    10
nodes-spot-eu-west-1c      2          2        2    10
```

I also have [k8s-spot-rescheduler](https://github.com/pusher/k8s-spot-rescheduler) running to take pods from any `ondemand` nodes that are provisioned, and move them to spot instances so that the `ondemand` nodes can be removed. However, lately, there always seems to be spot requests that are open, but not yet fulfilled:

<img width=""1130"" alt=""Screen Shot 2019-03-14 at 14 55 19"" src=""https://user-images.githubusercontent.com/8792243/54362601-7f22ac80-4669-11e9-963a-61d8d019c97c.png"">

This is normal behavior in AWS, but the problem is, the cluster autoscaler *does not* use the `ondemand` node groups at all. The following log output is what I'll see during this situation:

```
cluster-autoscaler aws_manager.go:148] Refreshed ASG list, next refresh after 2019-03-14 13:59:04.235594224 +0000 UTC m=+241976.799746087
cluster-autoscaler clusterstate.go:542] Readiness for node group nodes-spot-eu-west-1a.my-k8s-cluster.com not found
cluster-autoscaler utils.go:541] No pod using affinity / antiaffinity found in cluster, disabling affinity predicate for this loop
cluster-autoscaler static_autoscaler.go:274] No unschedulable pods
cluster-autoscaler utils.go:498] Skipping ip-10-YY-XXX-ZZZ.eu-west-1.compute.internal - node group min size reached
cluster-autoscaler utils.go:498] Skipping ip-10-YY-XXX-ZZZ.eu-west-1.compute.internal - node group min size reached
cluster-autoscaler utils.go:498] Skipping ip-10-YY-XXX-ZZZ.eu-west-1.compute.internal - node group min size reached
cluster-autoscaler scale_down.go:643] No candidates for scale down
cluster-autoscaler clusterstate.go:324] Failed to find readiness information for nodes-spot-eu-west-1a.my-k8s-cluster.com
cluster-autoscaler clusterstate.go:380] Failed to find readiness information for nodes-spot-eu-west-1a.my-k8s-cluster.com
cluster-autoscaler clusterstate.go:324] Failed to find readiness information for nodes-spot-eu-west-1a.my-k8s-cluster.com
```

The issue now is that there are pods that remain unscheduled because there's no capacity, but the cluster-autoscaler treats the ""open"" spot requests as having taken care of the scaling.

Any ideas on how to achieve both? I'm using the following configuration flags:

```shell
- ./cluster-autoscaler
- --cloud-provider=aws
- --namespace=kube-system
- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/my-k8s-cluster.com
- --balance-similar-node-groups
- --expander=random
- --logtostderr=true
- --scale-down-delay-after-add=5m
- --scale-down-delay-after-delete=5m
- --scale-down-delay-after-failure=5m
- --skip-nodes-with-local-storage=false
- --skip-nodes-with-system-pods=false
- --stderrthreshold=info
- --v=3
```",open,False,2019-03-14 14:06:50,2019-03-31 05:37:35
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1796,https://api.github.com/repos/kubernetes/autoscaler/issues/1796,Cluster Autoscaler 1.14.0-beta.2,,closed,True,2019-03-14 16:32:05,2019-03-14 17:44:29
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1797,https://api.github.com/repos/kubernetes/autoscaler/issues/1797,[WIP]VPA for CronJob,#1790,closed,True,2019-03-15 10:41:12,2019-04-03 06:52:49
autoscaler,jyipks,https://github.com/kubernetes/autoscaler/issues/1798,https://api.github.com/repos/kubernetes/autoscaler/issues/1798,on premise scaling to aws,How do i provide credentials to the aws cloudprovider so i can run from on premise bare metal and scale out to aws when it there are not enough resources on the node? I believe the aws cloudprovider is grabbing creds to create the new aws nodes.,closed,False,2019-03-15 20:18:41,2019-04-03 07:29:44
autoscaler,jstangroome,https://github.com/kubernetes/autoscaler/issues/1799,https://api.github.com/repos/kubernetes/autoscaler/issues/1799,[addon-resizer] deployment update drops existing nodeAffinity fields,"Seems to be a repeat of issue #188.

Using addon-resizer Docker image: `gcr.io/google-containers/addon-resizer-amd64:2.1`

Original deployment yaml contains:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  # ... truncated ...
spec:
  template:
      # ... truncated ...
      tolerations:
      - key: the-taint
        effect: NoSchedule
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: the-taint
                operator: Exists
```

And after addon-resizer logs ""Resources are not within the expected limits, updating the deployment"", the `apiVersion` has regressed to `extensions/v1beta1` and both the `tolerations` and `affinity` members have been removed.

",open,False,2019-03-17 22:16:47,2019-03-25 13:40:17
autoscaler,ccic,https://github.com/kubernetes/autoscaler/issues/1800,https://api.github.com/repos/kubernetes/autoscaler/issues/1800,Autoscaler was frequently killed for liveness proble failed,"Why the liveness probe fails so frequently?
Liveness probe failed: HTTP probe failed with statuscode: 500

![image](https://user-images.githubusercontent.com/19297121/54513402-4ef14b80-4992-11e9-8b11-95a592402c3c.png)
",open,False,2019-03-18 07:28:40,2019-03-18 09:45:03
autoscaler,piontec,https://github.com/kubernetes/autoscaler/pull/1801,https://api.github.com/repos/kubernetes/autoscaler/issues/1801,New expander: priority expander,"## The PR

This commit introduces code, tests and docs for a new expander called 'priority'. The motivation is placed in [this proposal](proposals/priority_expander.md). Docs are updated as well in [FAQ](FAQ.md#what-are-expanders).

## Testing procedure
Full autoamted kubernetes e2e tests were not done. The testing procedure was as follows:
1. Unit tests
  `go test ./expander/priority/...`

1. Manual e2e tests
    - I started a test cluster and connected the new cluster-autoscaler binary to it
    - checked for selection of the correct node group
    - check for handling of ConfigMap reloads
",closed,True,2019-03-18 12:15:21,2019-03-28 07:33:29
autoscaler,mgalgs,https://github.com/kubernetes/autoscaler/pull/1802,https://api.github.com/repos/kubernetes/autoscaler/issues/1802,AWS: Add note about suspending AZRebalance,"According to some user reports [1], you can actually run
cluster-autoscaler against an ASG that spans multiple AZs,
you just have to suspend the AZRebalance scaling process
to avoid unexpected node termination.

[1] https://kubernetes.slack.com/archives/C8SH2GSL9/p1552600210276600?thread_ts=1552420686.257000&cid=C8SH2GSL9

---

NOTE: I've only been running in this configuration for about 1 day, so I can't personally vouch for the correctness of this workaround.  As mentioned in the commit text above, other users have reported running in this configuration without issues.  Would be great to get confirmation from an expert though ;)",closed,True,2019-03-18 23:45:52,2019-03-19 20:34:22
autoscaler,vivekgarg20,https://github.com/kubernetes/autoscaler/issues/1803,https://api.github.com/repos/kubernetes/autoscaler/issues/1803,[CA] RefreshInterval is too low for auto discovering ASG for AWS,"As of now refreshInteval configuration is 10 seconds to discover ASG for AWS. 
It is causing AWS apis to be throttled. We are running 10's of clusters. 
I found same configuration for GCE is 1 minute.
Shouldn't 1 minute is sensible default. I would recommend to make it configurable.",closed,False,2019-03-19 07:09:39,2019-03-29 11:55:27
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1804,https://api.github.com/repos/kubernetes/autoscaler/issues/1804,Cluster Autoscaler 1.14.0,Initial (non-beta) release of CA 1.14.0. No changes since 1.14.0-beta.2,closed,True,2019-03-19 14:14:49,2019-03-28 17:42:04
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1805,https://api.github.com/repos/kubernetes/autoscaler/issues/1805,Disable running beta1 e2e tests,@schylek ,closed,True,2019-03-19 17:21:57,2019-03-20 09:58:30
autoscaler,viggeh,https://github.com/kubernetes/autoscaler/pull/1806,https://api.github.com/repos/kubernetes/autoscaler/issues/1806,Add information about node affinity to the FAQ,"I noticed that information about node affinity support is pretty hard to obtain. I found it eventually on Slack and have seen others looking for the information. See https://kubernetes.slack.com/messages/C09R1LV8S

So, I'm adding a Q to the FAQ with this information.",closed,True,2019-03-19 18:50:08,2019-03-20 12:19:25
autoscaler,mnothic,https://github.com/kubernetes/autoscaler/issues/1807,https://api.github.com/repos/kubernetes/autoscaler/issues/1807,AKS CA doesn't work can't communicate with apiserver,"**Error:**
```
F0319 21:03:36.597392       1 main.go:355] Failed to get nodes from apiserver: Get https://dev-pisclk8s-f30ccea9.hcp.westus2.azmk8s.io:443/api/v1/nodes: dial tcp: i/o timeout
```
**Configuration:**
```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""""]
  resources: [""events"",""endpoints""]
  verbs: [""create"", ""patch""]
- apiGroups: [""""]
  resources: [""pods/eviction""]
  verbs: [""create""]
- apiGroups: [""""]
  resources: [""pods/status""]
  verbs: [""update""]
- apiGroups: [""""]
  resources: [""endpoints""]
  resourceNames: [""cluster-autoscaler""]
  verbs: [""get"",""update""]
- apiGroups: [""""]
  resources: [""nodes""]
  verbs: [""watch"",""list"",""get"",""update""]
- apiGroups: [""""]
  resources: [""pods"",""services"",""replicationcontrollers"",""persistentvolumeclaims"",""persistentvolumes""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""extensions""]
  resources: [""replicasets"",""daemonsets""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""policy""]
  resources: [""poddisruptionbudgets""]
  verbs: [""watch"",""list""]
- apiGroups: [""apps""]
  resources: [""statefulsets""]
  verbs: [""watch"",""list"",""get""]
- apiGroups: [""storage.k8s.io""]
  resources: [""storageclasses""]
  verbs: [""get"", ""list"", ""watch""]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""""]
  resources: [""configmaps""]
  verbs: [""create""]
- apiGroups: [""""]
  resources: [""configmaps""]
  resourceNames: [""cluster-autoscaler-status""]
  verbs: [""delete"",""get"",""update""]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: v1
data:
  ClientID:<omited>
  ClientSecret: <omited>
  ResourceGroup: cGlzY2xkZXY=
  SubscriptionID: YTYwMThkZWYtNzcwZS00N2NjLThlODAtOTYyYzU4YWQ5ZjVj
  TenantID: M2JlYTQ3OGMtMTY4NC00YThjLThlODUtMDQ1ZWM1NGJhNDMw
  VMType: QUtT
  ClusterName: ZGV2LXBpc2Nsazhz
  NodeResourceGroup: TUNfcGlzY2xkZXZfZGV2LXBpc2NsazhzX3dlc3R1czI=
kind: Secret
metadata:
  name: cluster-autoscaler-azure
  namespace: kube-system
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      restartPolicy: Always
      serviceAccountName: cluster-autoscaler
      containers:
      - image: k8s.gcr.io/cluster-autoscaler:v1.13.2
        imagePullPolicy: Always
        name: cluster-autoscaler
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 300Mi
        command:
        - ./cluster-autoscaler
        - --v=3
        - --logtostderr=true
        - --cloud-provider=azure
        - --skip-nodes-with-local-storage=false
        - --nodes=3:12:devminion
        env:
        - name: ARM_SUBSCRIPTION_ID
          valueFrom:
            secretKeyRef:
              key: SubscriptionID
              name: cluster-autoscaler-azure
        - name: ARM_RESOURCE_GROUP
          valueFrom:
            secretKeyRef:
              key: ResourceGroup
              name: cluster-autoscaler-azure
        - name: ARM_TENANT_ID
          valueFrom:
            secretKeyRef:
              key: TenantID
              name: cluster-autoscaler-azure
        - name: ARM_CLIENT_ID
          valueFrom:
            secretKeyRef:
              key: ClientID
              name: cluster-autoscaler-azure
        - name: ARM_CLIENT_SECRET
          valueFrom:
            secretKeyRef:
              key: ClientSecret
              name: cluster-autoscaler-azure
        - name: ARM_VM_TYPE
          valueFrom:
            secretKeyRef:
              key: VMType
              name: cluster-autoscaler-azure
        - name: AZURE_CLUSTER_NAME
          valueFrom:
            secretKeyRef:
              key: ClusterName
              name: cluster-autoscaler-azure
        - name: AZURE_NODE_RESOURCE_GROUP
          valueFrom:
            secretKeyRef:
              key: NodeResourceGroup
              name: cluster-autoscaler-azure
```
**pod log:**
```
kubectl -n kube-system logs -f deployments/cluster-autoscaler
I0319 21:03:06.595447       1 flags.go:52] FLAG: --address="":8085""
I0319 21:03:06.595465       1 flags.go:52] FLAG: --alsologtostderr=""false""
I0319 21:03:06.595469       1 flags.go:52] FLAG: --balance-similar-node-groups=""false""
I0319 21:03:06.595473       1 flags.go:52] FLAG: --cloud-config=""""
I0319 21:03:06.595476       1 flags.go:52] FLAG: --cloud-provider=""azure""
I0319 21:03:06.595480       1 flags.go:52] FLAG: --cloud-provider-gce-lb-src-cidrs=""130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16""
I0319 21:03:06.595486       1 flags.go:52] FLAG: --cluster-name=""""
I0319 21:03:06.595490       1 flags.go:52] FLAG: --cores-total=""0:320000""
I0319 21:03:06.595493       1 flags.go:52] FLAG: --estimator=""binpacking""
I0319 21:03:06.595497       1 flags.go:52] FLAG: --expander=""random""
I0319 21:03:06.595500       1 flags.go:52] FLAG: --expendable-pods-priority-cutoff=""-10""
I0319 21:03:06.595504       1 flags.go:52] FLAG: --gke-api-endpoint=""""
I0319 21:03:06.595507       1 flags.go:52] FLAG: --gpu-total=""[]""
I0319 21:03:06.595511       1 flags.go:52] FLAG: --httptest.serve=""""
I0319 21:03:06.595514       1 flags.go:52] FLAG: --ignore-daemonsets-utilization=""false""
I0319 21:03:06.595518       1 flags.go:52] FLAG: --ignore-mirror-pods-utilization=""false""
I0319 21:03:06.595521       1 flags.go:52] FLAG: --kubeconfig=""""
I0319 21:03:06.595525       1 flags.go:52] FLAG: --kubernetes=""""
I0319 21:03:06.595528       1 flags.go:52] FLAG: --leader-elect=""true""
I0319 21:03:06.595534       1 flags.go:52] FLAG: --leader-elect-lease-duration=""15s""
I0319 21:03:06.595539       1 flags.go:52] FLAG: --leader-elect-renew-deadline=""10s""
I0319 21:03:06.595542       1 flags.go:52] FLAG: --leader-elect-resource-lock=""endpoints""
I0319 21:03:06.595547       1 flags.go:52] FLAG: --leader-elect-retry-period=""2s""
I0319 21:03:06.595551       1 flags.go:52] FLAG: --log-backtrace-at="":0""
I0319 21:03:06.595557       1 flags.go:52] FLAG: --log-dir=""""
I0319 21:03:06.595560       1 flags.go:52] FLAG: --log-file=""""
I0319 21:03:06.595564       1 flags.go:52] FLAG: --logtostderr=""true""
I0319 21:03:06.595567       1 flags.go:52] FLAG: --max-autoprovisioned-node-group-count=""15""
I0319 21:03:06.595571       1 flags.go:52] FLAG: --max-empty-bulk-delete=""10""
I0319 21:03:06.595574       1 flags.go:52] FLAG: --max-failing-time=""15m0s""
I0319 21:03:06.595578       1 flags.go:52] FLAG: --max-graceful-termination-sec=""600""
I0319 21:03:06.595582       1 flags.go:52] FLAG: --max-inactivity=""10m0s""
I0319 21:03:06.595585       1 flags.go:52] FLAG: --max-node-provision-time=""15m0s""
I0319 21:03:06.595589       1 flags.go:52] FLAG: --max-nodes-total=""0""
I0319 21:03:06.595592       1 flags.go:52] FLAG: --max-total-unready-percentage=""45""
I0319 21:03:06.595597       1 flags.go:52] FLAG: --memory-total=""0:6400000""
I0319 21:03:06.595600       1 flags.go:52] FLAG: --min-replica-count=""0""
I0319 21:03:06.595604       1 flags.go:52] FLAG: --namespace=""kube-system""
I0319 21:03:06.595607       1 flags.go:52] FLAG: --new-pod-scale-up-delay=""0s""
I0319 21:03:06.595611       1 flags.go:52] FLAG: --node-autoprovisioning-enabled=""false""
I0319 21:03:06.595614       1 flags.go:52] FLAG: --node-group-auto-discovery=""[]""
I0319 21:03:06.595618       1 flags.go:52] FLAG: --nodes=""[3:12:devminion]""
I0319 21:03:06.595622       1 flags.go:52] FLAG: --ok-total-unready-count=""3""
I0319 21:03:06.595625       1 flags.go:52] FLAG: --regional=""false""
I0319 21:03:06.595629       1 flags.go:52] FLAG: --scale-down-candidates-pool-min-count=""50""
I0319 21:03:06.595632       1 flags.go:52] FLAG: --scale-down-candidates-pool-ratio=""0.1""
I0319 21:03:06.595636       1 flags.go:52] FLAG: --scale-down-delay-after-add=""10m0s""
I0319 21:03:06.595639       1 flags.go:52] FLAG: --scale-down-delay-after-delete=""10s""
I0319 21:03:06.595643       1 flags.go:52] FLAG: --scale-down-delay-after-failure=""3m0s""
I0319 21:03:06.595646       1 flags.go:52] FLAG: --scale-down-enabled=""true""
I0319 21:03:06.595650       1 flags.go:52] FLAG: --scale-down-non-empty-candidates-count=""30""
I0319 21:03:06.595653       1 flags.go:52] FLAG: --scale-down-unneeded-time=""10m0s""
I0319 21:03:06.595657       1 flags.go:52] FLAG: --scale-down-unready-time=""20m0s""
I0319 21:03:06.595661       1 flags.go:52] FLAG: --scale-down-utilization-threshold=""0.5""
I0319 21:03:06.595665       1 flags.go:52] FLAG: --scan-interval=""10s""
I0319 21:03:06.595668       1 flags.go:52] FLAG: --skip-headers=""false""
I0319 21:03:06.595672       1 flags.go:52] FLAG: --skip-nodes-with-local-storage=""false""
I0319 21:03:06.595675       1 flags.go:52] FLAG: --skip-nodes-with-system-pods=""true""
I0319 21:03:06.595678       1 flags.go:52] FLAG: --stderrthreshold=""2""
I0319 21:03:06.595682       1 flags.go:52] FLAG: --test.bench=""""
I0319 21:03:06.595685       1 flags.go:52] FLAG: --test.benchmem=""false""
I0319 21:03:06.595689       1 flags.go:52] FLAG: --test.benchtime=""1s""
I0319 21:03:06.595692       1 flags.go:52] FLAG: --test.blockprofile=""""
I0319 21:03:06.595695       1 flags.go:52] FLAG: --test.blockprofilerate=""1""
I0319 21:03:06.595699       1 flags.go:52] FLAG: --test.count=""1""
I0319 21:03:06.595702       1 flags.go:52] FLAG: --test.coverprofile=""""
I0319 21:03:06.595705       1 flags.go:52] FLAG: --test.cpu=""""
I0319 21:03:06.595709       1 flags.go:52] FLAG: --test.cpuprofile=""""
I0319 21:03:06.595712       1 flags.go:52] FLAG: --test.failfast=""false""
I0319 21:03:06.595715       1 flags.go:52] FLAG: --test.list=""""
I0319 21:03:06.595719       1 flags.go:52] FLAG: --test.memprofile=""""
I0319 21:03:06.595722       1 flags.go:52] FLAG: --test.memprofilerate=""0""
I0319 21:03:06.595725       1 flags.go:52] FLAG: --test.mutexprofile=""""
I0319 21:03:06.595728       1 flags.go:52] FLAG: --test.mutexprofilefraction=""1""
I0319 21:03:06.595732       1 flags.go:52] FLAG: --test.outputdir=""""
I0319 21:03:06.595735       1 flags.go:52] FLAG: --test.parallel=""2""
I0319 21:03:06.595738       1 flags.go:52] FLAG: --test.run=""""
I0319 21:03:06.595742       1 flags.go:52] FLAG: --test.short=""false""
I0319 21:03:06.595783       1 flags.go:52] FLAG: --test.testlogfile=""""
I0319 21:03:06.595799       1 flags.go:52] FLAG: --test.timeout=""0s""
I0319 21:03:06.595803       1 flags.go:52] FLAG: --test.trace=""""
I0319 21:03:06.595865       1 flags.go:52] FLAG: --test.v=""false""
I0319 21:03:06.595872       1 flags.go:52] FLAG: --unremovable-node-recheck-timeout=""5m0s""
I0319 21:03:06.595876       1 flags.go:52] FLAG: --v=""3""
I0319 21:03:06.595880       1 flags.go:52] FLAG: --vmodule=""""
I0319 21:03:06.595896       1 flags.go:52] FLAG: --write-status-configmap=""true""
I0319 21:03:06.595902       1 main.go:333] Cluster Autoscaler 1.13.2
F0319 21:03:36.597392       1 main.go:355] Failed to get nodes from apiserver: Get https://dev-pisclk8s-f30ccea9.hcp.westus2.azmk8s.io:443/api/v1/nodes: dial tcp: i/o timeout
```
",open,False,2019-03-19 21:09:59,2019-03-21 13:31:32
autoscaler,stephbu,https://github.com/kubernetes/autoscaler/issues/1808,https://api.github.com/repos/kubernetes/autoscaler/issues/1808,Addon-Resizer Resist daily Hi/Lo swings in utilization ,"The systems we operate run a regular daily rollercoast of usage with very large daily variation in nodes/pods off-peak vs. at-peak hours.  We're running into an interesting issue where regular up/down resource resizing of our prometheus-metrics aggregators causes duplicate metrics during the existential overlap between current-sized and future-sized metrics pods. This looks on aggregated charts as ""spikes"" e.g. ```sum(metric)``` or ```count(metric)``` because of the double-reporting.  
- We could try to fix every query to compensate for the potential multiplicity of results from different aggregators - pretty ugly to aggregate the aggregates.  This seems like a large hammer that lots of our users are reluctant to comprehend let alone learn.
- We could make it less sensitive, suspect it would still scale up and down at least once per day, or not scale up enough.
- We could just pin large aggregators, but this can be pretty expensive in our smaller clusters (100's of pods vs 1000's of pods).

None of these seem viable, and hence we considered number of possible alternative extensions of the current addon-resizer (non-exclusive, incomplete, half-baked etc.):
-  ""damp"" changes - tracking and smoothing utilization across more statistically meaningful time-window e.g hours to days?
- ""lock"" the scaledown behavior i.e. ""upward only"" settings that would prevent scaledown?
- split config into ""scale-up and scale-down config"" with much larger thresholds (and increments?) for down than up?

Goal would be to let pod-resources ""float up"" to the largest size of the cluster then at worst ""drift downwards"" or stay pinned knowing we'll be back to peak within a meaningful evaluation period e.g. hours or days.  We'd take interruptions on the way up, but would stabilize within a few days.

Before we get started on considering some code, I'm looking for feedback, possible alternative suggestions and approaches, and of course alternative working solutions...",open,False,2019-03-20 19:30:25,2019-04-02 13:06:42
autoscaler,Skull0ne,https://github.com/kubernetes/autoscaler/issues/1809,https://api.github.com/repos/kubernetes/autoscaler/issues/1809,vertical-pod-autoscaler recommender is OOM when using prometheus as storage,"Hi

I read in another issue (https://github.com/kubernetes/autoscaler/issues/1551#issuecomment-451115856) that you have to define --storage=prometheus when you define --prometheus-url .

However when I do so, the recommender pod is always crashing by OOM without printing any log (even with --v=10).

I tried to increase the resources (a lot) and it seems to work : 
```---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: vpa-recommender
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: vpa-recommender
    spec:
      serviceAccountName: vpa-recommender
      containers:
      - name: recommender
        image: k8s.gcr.io/vpa-recommender:0.4.0
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 2
            memory: 3000Mi
          requests:
            cpu: 500m
            memory: 1500Mi
          #limits:
          #  cpu: 200m
          #  memory: 1000Mi
          #requests:
          #  cpu: 50m
          #  memory: 500Mi
        ports:
        - containerPort: 8080
        command:
        - ./recommender
        - --logtostderr=true
        - --v=10
        - --prometheus-address=http://prometheus-service.monitoring:9090/
        - --prometheus-cadvisor-job-name=cadvisor
        - --storage=prometheus
```

Do you have any recommendation for that please?

Thanks",closed,False,2019-03-21 21:06:37,2019-03-25 15:45:26
autoscaler,hongli-my,https://github.com/kubernetes/autoscaler/issues/1810,https://api.github.com/repos/kubernetes/autoscaler/issues/1810,Vertical Pod Autoscaler: load cpu usage from prometheus  is bug?,"https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/recommender/input/history/history_provider.go#L193

container_cpu_usage_seconds_total type is  counter,  use it as  container cpu usage is right?",closed,False,2019-03-22 05:01:25,2019-03-22 09:20:13
autoscaler,feiskyer,https://github.com/kubernetes/autoscaler/issues/1811,https://api.github.com/repos/kubernetes/autoscaler/issues/1811,Long running operations may fail because of Azure go-autorest bugs,"According to Azure/go-autorest#357, the timeout set in context doesn't take effect and 15min is always used for long running operations.

We should upgrade the version of go-autorest >= 11.3.2.

Depending on https://github.com/kubernetes/kubernetes/issues/75579.

/kind bug",open,False,2019-03-22 06:37:58,2019-04-05 13:26:32
autoscaler,safanaj,https://github.com/kubernetes/autoscaler/issues/1812,https://api.github.com/repos/kubernetes/autoscaler/issues/1812,Admission controller should set limit in case of LimitRange with max ratio,"In case VPA is setting requests to lower values and there is deployed a LimitRange with MaxLimitRequestRatio the pod could be not scheduled because it is not respecting the limts/requests ratio for some resource.

Admission controller should be aware of limit ranges and if needed explicitly decrease limits to respect the max ratio.",open,False,2019-03-22 09:12:50,2019-03-22 10:29:42
autoscaler,safanaj,https://github.com/kubernetes/autoscaler/pull/1813,https://api.github.com/repos/kubernetes/autoscaler/issues/1813,Ac take care of limit range,https://github.com/kubernetes/autoscaler/issues/1812,open,True,2019-03-22 09:31:08,2019-04-03 14:00:24
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1814,https://api.github.com/repos/kubernetes/autoscaler/issues/1814,Godeps update of google.golang.org/api,"Update google.golang.org/api to version supporting needed container/v1beta1 api
Remove override that is unneeded anymore

/cc aleksandra-malinowska
/cc losipiuk
/cc @MaciekPytel",closed,True,2019-03-22 17:19:29,2019-03-25 08:54:26
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1815,https://api.github.com/repos/kubernetes/autoscaler/issues/1815,Adding link to Sig-Autoscaling for better reference.,,closed,True,2019-03-24 15:22:26,2019-03-25 10:45:10
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1816,https://api.github.com/repos/kubernetes/autoscaler/issues/1816,Adding steps to register VPA-Admission-Controller on specific path.,,closed,True,2019-03-24 17:07:11,2019-03-25 10:31:12
autoscaler,achinthagunasekara,https://github.com/kubernetes/autoscaler/pull/1817,https://api.github.com/repos/kubernetes/autoscaler/issues/1817,Fixing YAML formatting issues,"## Description

Fixing YAML formatting issues.

## Testing 

agunasekara@MEL-M-AGUNASEKARA01:examples (dev-agunasekara-yamllint)$ find . -name '*.yaml' | xargs yamllint
agunasekara@MEL-M-AGUNASEKARA01:examples (dev-agunasekara-yamllint)$",closed,True,2019-03-25 00:07:43,2019-03-25 00:23:27
autoscaler,achinthagunasekara,https://github.com/kubernetes/autoscaler/pull/1818,https://api.github.com/repos/kubernetes/autoscaler/issues/1818,Fixing YAML formatting issues for AWS examples,"## Description

Fixing YAML formatting issues for AWS examples

## Testing 

agunasekara@MEL-M-AGUNASEKARA01:examples (dev-agunasekara-yamllint)$ find . -name '*.yaml' | xargs yamllint
agunasekara@MEL-M-AGUNASEKARA01:examples (dev-agunasekara-yamllint)$",closed,True,2019-03-25 00:27:37,2019-03-25 22:14:18
autoscaler,jkaniuk,https://github.com/kubernetes/autoscaler/pull/1819,https://api.github.com/repos/kubernetes/autoscaler/issues/1819,Fix Godeps.json,/cc losipiuk,closed,True,2019-03-25 10:16:19,2019-03-25 10:33:09
autoscaler,wyb1,https://github.com/kubernetes/autoscaler/issues/1820,https://api.github.com/repos/kubernetes/autoscaler/issues/1820,Mutating Webhook timeout on Private GKE Cluster,"The mutating webhook will always timeout when the VPA is deployed in a private GKE cluster (using the `vpa-up.sh` script). This means pod creation will always take more than 30s.

After debugging the issue I found that there are some firewall rules created that block the API Server from reaching the webhook backend on port 8000. The firewall only allows traffic from the API Server to the cluster over ports 10250 and 443. 

How to recreate:
1. Create a VPC
2. Create a GKE Cluster, enable private cluster and use the VPC from step 1
3. Run `vpa-up.sh`
4. Creating a pod will take more than 30s because the webhook times out.

Allowing port 8000 in addition to 10250 and 443 fixes this problem, but this is not a great solution.

To mitigate this problem I wanted to have the vpa-webhook service map from port 443 to 443 instead of 443 to 8000. To do this I needed to change the `--port` flag in the vpa-admission-controller but it appears that this change isn't in the current release. Is a new release of the VPA planned in the near future?

",closed,False,2019-03-25 12:28:03,2019-04-05 13:25:33
autoscaler,drewhemm,https://github.com/kubernetes/autoscaler/issues/1821,https://api.github.com/repos/kubernetes/autoscaler/issues/1821,More expensive instance type given priority,"Running version 1.3.7 on AWS EKS, I have multiple node groups and two in particular are i3.2xlarge and r5.2xlarge. Although they are identical in CPU cores and memory, the latter are cheaper to run and I am wondering why CA always seems to spawn i3.2xlarge instead. 

What is the determining factor here and can I influence it in any way?

I am using spot instances, but not sure that is relevant. 

If I manually set the min size on the ASG, CA will respect that and scheduled the pods onto those nodes, but I would like CA to automatically choose the cheapest instance type available when a pod can fit on either. ",open,False,2019-03-25 13:19:13,2019-03-26 13:19:45
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1822,https://api.github.com/repos/kubernetes/autoscaler/issues/1822,Vertical Pod Autoscaler version 0.5.0,,closed,True,2019-03-25 14:31:17,2019-03-25 14:51:13
autoscaler,kgolab,https://github.com/kubernetes/autoscaler/pull/1823,https://api.github.com/repos/kubernetes/autoscaler/issues/1823,Add a note for 0.5.0 release.,"@bskiba , please have a look.",closed,True,2019-03-25 14:48:32,2019-04-02 15:45:24
autoscaler,safanaj,https://github.com/kubernetes/autoscaler/pull/1824,https://api.github.com/repos/kubernetes/autoscaler/issues/1824,VPA recommender: use rate() to query CPU historical usage from promet…,"…heus

This is a proposed fix for #1501",closed,True,2019-03-25 14:55:25,2019-03-25 15:09:12
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1825,https://api.github.com/repos/kubernetes/autoscaler/issues/1825,Use debian-base-amd64:v1.0.0,,closed,True,2019-03-25 15:26:21,2019-03-26 09:39:20
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1826,https://api.github.com/repos/kubernetes/autoscaler/issues/1826,Update base debian images used by VPA to 1.0.0,,closed,True,2019-03-25 15:55:51,2019-03-26 10:17:17
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1827,https://api.github.com/repos/kubernetes/autoscaler/issues/1827,Update base debian images used by VPA to 1.0.0,,closed,True,2019-03-25 15:57:23,2019-03-26 10:17:17
autoscaler,safanaj,https://github.com/kubernetes/autoscaler/issues/1828,https://api.github.com/repos/kubernetes/autoscaler/issues/1828,VPA Updater minReplicas overridable in VPA objects,"Would be great if the VPA objects could specify a minimum number of replicas to allow the updater to evict their pods, this would be used only if set and greater than zero, if that would be missing or 0 the updater would still to respect the `--min-replicas` param.

This feature would require an minor API change adding an integer under updatePolicy in VPA Spec.
",closed,False,2019-03-25 18:40:43,2019-03-29 12:03:53
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1829,https://api.github.com/repos/kubernetes/autoscaler/issues/1829,Use debian-base-amd64:v1.0.0,Change-Id: I08d18627dd2a033f34cc6376d7c740299dac9dcb,closed,True,2019-03-26 11:04:22,2019-03-26 12:11:20
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1830,https://api.github.com/repos/kubernetes/autoscaler/issues/1830,Use debian-base-amd64:v1.0.0,Change-Id: I08d18627dd2a033f34cc6376d7c740299dac9dcb,closed,True,2019-03-26 11:04:35,2019-03-26 12:11:19
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1831,https://api.github.com/repos/kubernetes/autoscaler/issues/1831,Use debian-base-amd64:v1.0.0,Change-Id: I08d18627dd2a033f34cc6376d7c740299dac9dcb,closed,True,2019-03-26 11:04:43,2019-03-26 12:11:20
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1832,https://api.github.com/repos/kubernetes/autoscaler/issues/1832, Use debian-base-amd64:v1.0.0,,closed,True,2019-03-26 11:04:50,2019-03-26 12:11:19
autoscaler,gjtempleton,https://github.com/kubernetes/autoscaler/pull/1833,https://api.github.com/repos/kubernetes/autoscaler/issues/1833,Correct VPA Readme anchor links,Fixes a number of broken anchor links with the VPA Readme and adds one to final section (`Related Links`),closed,True,2019-03-26 12:02:04,2019-03-26 13:27:15
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1834,https://api.github.com/repos/kubernetes/autoscaler/issues/1834,Correct base image tag.,,closed,True,2019-03-27 08:41:22,2019-03-27 08:54:47
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1835,https://api.github.com/repos/kubernetes/autoscaler/issues/1835,Correct base image tag.,,closed,True,2019-03-27 08:43:57,2019-03-27 08:56:49
autoscaler,tweeks024,https://github.com/kubernetes/autoscaler/issues/1836,https://api.github.com/repos/kubernetes/autoscaler/issues/1836,cluster-autoscaler scales down to point pods go into pending,"We are running kubernetes v1.13.3 and cluster-autoscaler v1.13.2 (seems to be the latest on 1.13).  The primary constraint in this cluster is memory.

When all deployments are created a number of pods go into Pending.  The CA adds nodes as expected and the pods are scheduled.  However, after the scale-down timers expire the nodes are scaled back down and pods  go into pending.

CA is also not removing these nodes gracefully from the kubernetes cluster (not sure if this is expected).  This continues in a loop until CA see's the cluster as unhealthy due to too many nodes in the NotReady state.   I've tried updating  ```scale-down-utilization-threshold``` to as low as 0.05 with no change in behavior.

Here is my CA config:

```
    command:
    - ./cluster-autoscaler
    - --v=4
    - --stderrthreshold=info
    - --cloud-provider=aws
    - --balance-similar-node-groups
    - --skip-nodes-with-local-storage=false
    - --expander=least-waste
    - --scale-down-utilization-threshold=0.1
    - --nodes=0:5:kube_node_shared_eng_asg_a
    - --nodes=0:5:kube_node_shared_eng_asg_b
```

An example deployment is like this:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myApp
  namespace: shared-int
  labels:
    app: myApp
spec:
  replicas: 4
  selector:
    matchLabels:
      app: myApp
  template:
    metadata:
      labels:
        app: myApp
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - myApp
            topologyKey: kubernetes.io/hostname
      containers:
      - env:
        - name: JAVA_TOOL_OPTIONS
          value: -Xms512m -Xmx512m
        image: myApp
        livenessProbe:
          initialDelaySeconds: 300
          periodSeconds: 30
          tcpSocket:
            port: 9019
          timeoutSeconds: 3
        name: myApp
        ports:
        - containerPort: 9019
        - containerPort: 9020
        readinessProbe:
          httpGet:
            path: /health
            port: 9020
          initialDelaySeconds: 120
          periodSeconds: 15
          timeoutSeconds: 3
        resources:
          limits:
            cpu: '1'
            memory: 1024Mi
          requests:
            cpu: '0'
            memory: 768Mi
      restartPolicy: Always
```


*kubectl get nodes*
``` 
NAME                                            STATUS     ROLES    AGE     VERSION
kube01-shared-eng          Ready      master   40d     v1.13.3
kube02-shared-eng          Ready      master   40d     v1.13.3
kube03-shared-eng          Ready      master   40d     v1.13.3
node-shared-eng-121-159-1a NotReady   <none>   44h     v1.13.3
node-shared-eng-121-217-1a NotReady   <none>   45h     v1.13.3
node-shared-eng-122-145-1b NotReady   <none>   45h     v1.13.3
node-shared-eng-122-161-1b NotReady   <none>   4d17h   v1.13.3
node-shared-eng-122-20-1b  NotReady   <none>   44h     v1.13.3
node-shared-eng-122-251-1b NotReady   <none>   45h     v1.13.3
node01-shared-eng          Ready      <none>   33d     v1.13.3
node02-shared-eng          Ready      <none>   33d     v1.13.3
node03-shared-eng          Ready      <none>   33d     v1.13.3
```

*kubectl get pods --all-namespaces  | grep -i pending*
```
shared-int      api-gateway-shared-int-6987cdfb89-t4ckx                        0/1     Pending       0          44h
shared-int      events-classes-service-shared-int-56b598df95-6295g             0/1     Pending       0          44h
shared-int      events-classes-service-shared-int-56b598df95-tb2kz             0/1     Pending       0          44h
shared-int      platform-api-gateway-shared-int-55bbff97d5-2rlp7               0/1     Pending       0          44h
shared-int      platform-api-gateway-shared-int-55bbff97d5-789ws               0/1     Pending       0          44h
shared-int      redis-shared-int-5b4fc4658b-9zfc6                              0/1     Pending       0          44h
shared-int      template-expander-service-shared-int-6d44875745-6cvs2          0/1     Pending       0          44h
shared-int      timetrade-authserver-shared-int-5c7ffcd98-jhcdv                0/1     Pending       0          44h
shared-int      timetrade-authserver-shared-int-5c7ffcd98-pwvwg                0/1     Pending       0          44h
shared-test     events-classes-service-shared-test-99c89c574-z44pv             0/1     Pending       0          44h
shared-test     ews-notifications-shared-test-78c7cf59f4-8fjtc                 0/1     Pending       0          44h
shared-test     redis-shared-test-6dcfc6699f-n4n8c                             0/1     Pending       0          44h
shared-test     template-expander-service-shared-test-774cb6c67-84gtb          0/1     Pending       0          44h
shared-test     translation-service-shared-test-d5497dd4f-cm7j9                0/1     Pending       0          44h
```

*kubectl -n kube-system describe configmap cluster-autoscaler-status*
```
Name:         cluster-autoscaler-status
Namespace:    kube-system
Labels:       <none>
Annotations:  cluster-autoscaler.kubernetes.io/last-updated: 2019-03-27 14:16:30.080714192 +0000 UTC

Data
====
status:
----
Cluster-autoscaler status at 2019-03-27 14:16:30.080714192 +0000 UTC:
Cluster-wide:
  Health:      Unhealthy (ready=6 unready=6 notStarted=0 longNotStarted=0 registered=12 longUnregistered=0)
               LastProbeTime:      2019-03-27 14:16:29.751461837 +0000 UTC m=+163779.430960818
               LastTransitionTime: 2019-03-25 17:45:10.352154648 +0000 UTC m=+3500.031653585
  ScaleUp:     NoActivity (ready=6 registered=12)
               LastProbeTime:      2019-03-27 14:16:29.751461837 +0000 UTC m=+163779.430960818
               LastTransitionTime: 2019-03-25 17:45:41.281835145 +0000 UTC m=+3530.961334115
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC
               LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC

NodeGroups:
  Name:        kube_node_shared_eng_asg_a
  Health:      Healthy (ready=0 unready=0 notStarted=0 longNotStarted=0 registered=0 longUnregistered=0 cloudProviderTarget=0 (minSize=0, maxSize=5))
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC
               LastTransitionTime: 2019-03-25 16:47:20.002716092 +0000 UTC m=+29.682214982
  ScaleUp:     NoActivity (ready=0 cloudProviderTarget=0)
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC
               LastTransitionTime: 2019-03-25 17:50:50.554241419 +0000 UTC m=+3840.233740372
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC
               LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC

  Name:        kube_node_shared_eng_asg_b
  Health:      Healthy (ready=0 unready=0 notStarted=0 longNotStarted=0 registered=0 longUnregistered=0 cloudProviderTarget=0 (minSize=0, maxSize=5))
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC
               LastTransitionTime: 2019-03-25 16:47:20.002716092 +0000 UTC m=+29.682214982
  ScaleUp:     NoActivity (ready=0 cloudProviderTarget=0)
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC
               LastTransitionTime: 2019-03-25 17:55:50.294492045 +0000 UTC m=+4139.973990950
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC
               LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC


Events:
  Type     Reason            Age                     From                Message
  ----     ------            ----                    ----                -------
  Warning  ClusterUnhealthy  4m2s (x15591 over 44h)  cluster-autoscaler  Cluster is unhealthy
```",closed,False,2019-03-27 14:18:07,2019-03-28 20:02:16
autoscaler,achinthagunasekara,https://github.com/kubernetes/autoscaler/pull/1837,https://api.github.com/repos/kubernetes/autoscaler/issues/1837,Cleaning up the fix_gopath.sh,"# Description

* Cleaning up the `fix_gopath.sh` with shellcheck
* Cleaning up `cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-one-asg.yaml` with yamllint",closed,True,2019-03-27 23:50:52,2019-04-04 16:00:37
autoscaler,hmeerlo,https://github.com/kubernetes/autoscaler/issues/1838,https://api.github.com/repos/kubernetes/autoscaler/issues/1838,Assymetric nodepools and downscaling,"The docs state the following:

> The sum of cpu and memory requests of all pods running on this node is smaller than 50% of the node's allocatable. (Before 1.1.0, node capacity was used instead of allocatable.) Utilization threshold can be configured using --scale-down-utilization-threshold flag.

But I currently am in a situation where I have 1 small node (1vCPU, 3.75GB mem) which has 70% allocated CPU requests. Besides this I have 3 large nodes (8 vCPU, 30GB mem) with max 10% CPU and mem allocated. All pods on the small node are eligible to be moved, but according to the above rule this small node will never be deleted even though there is plenty of room for all pods on other nodes. Am I overlooking something?",open,False,2019-03-28 13:13:52,2019-04-01 09:21:48
autoscaler,davidquarles,https://github.com/kubernetes/autoscaler/issues/1839,https://api.github.com/repos/kubernetes/autoscaler/issues/1839,[feature request] Optionally allow VPA to manage pod limits,"I remember seeing this in the codebase in v0.2 IIRC and I couldn't find any real explanation for why it was removed.  If anyone can speak to that or what the long-term roadmap looks like here, I'm very curious.

The use case I would love to see supported is fairly narrow – I want VPA-managed pods with guaranteed QoS.  We *do* also use `LimitRange`s to enforce default limits, so ideally this would include a mechanism for overriding existing limits (perhaps the initial limit could function as a hard upper bound?).

Related: #1812 ",open,False,2019-03-28 18:25:12,2019-03-29 12:03:09
autoscaler,achinthagunasekara,https://github.com/kubernetes/autoscaler/pull/1840,https://api.github.com/repos/kubernetes/autoscaler/issues/1840,More shell-check clean up,More shell-check clean ups,closed,True,2019-03-28 23:51:37,2019-03-29 02:08:55
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1841,https://api.github.com/repos/kubernetes/autoscaler/issues/1841,Update aws RefreshInterval to 1 minute,"For #1803 

Increase RefreshInterval to reduce API calls.  Don't see any downside and 1 min update is acceptable. ",closed,True,2019-03-29 07:25:19,2019-03-29 16:08:11
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/issues/1842,https://api.github.com/repos/kubernetes/autoscaler/issues/1842,NoVolumeZoneConflictPred predicate checking for different node groups may race with provisioning PV to the cluster,"NoVolumeZoneConflictPred predicate checking for different node groups may race with provisioning PV to the cluster. The predicate is reading PVs in the cluster internally via informers. It is possible that  PV is not available when one node group is being considered for expansion and appears only when some other is. As a result a suboptimal (cost-wise) node-group can be chosen.

Note that similar issue may be true for some other predicates using informers internally. Ideally fix should address the problem holistically.",open,False,2019-03-29 10:31:19,2019-03-29 10:31:56
autoscaler,piontec,https://github.com/kubernetes/autoscaler/pull/1843,https://api.github.com/repos/kubernetes/autoscaler/issues/1843,Priority expander relies now on listers instead of direct watch api,"This is a rework of this PR: https://github.com/kubernetes/autoscaler/pull/1801
The changes are as follows:
* it uses lister now
* unittests rely on already existing tooling
* app is never crashed, just error is logged

Please review @MaciekPytel ",open,True,2019-03-29 11:10:36,2019-04-04 06:20:03
autoscaler,dezmodue,https://github.com/kubernetes/autoscaler/issues/1844,https://api.github.com/repos/kubernetes/autoscaler/issues/1844,Made the lock name configurable - run multiple CA in the same namespace,"At the moment the name of the lock is hardcoded to ""cluster-autoscaler"" which makes it impossible to run multiple instances of the CA in the same namespace as they all end up fighting for the same lock.

Running multiple instances of the autoscaler is already possible by deploying them to separate namespaces but this requires extra steps as explained here https://github.com/kubernetes/autoscaler/pull/1648#discussion_r253265737

Why would you want to run multiple autoscalers? 
In our setup we have groups of instances that serve different purposes and require for example different scale-down-delay-after-add or scale-down-unneeded-time or scale-down-utilization-threshold etc. We use taints so that a pod can only be scheduled on an instance group (which is managed by a single CA).

We are running the CA in multiple namespaces atm but it would be much more convenient to be able to run it in kube-system with different locks. 

I tested a simple change to implement this feature here https://github.com/kubernetes/autoscaler/compare/master...dezmodue:instance_identifier - but before going ahead with a PR I would like to understand if this is a request that would be taken into consideration.",open,False,2019-03-29 13:06:25,2019-04-01 08:54:14
autoscaler,deweysasser,https://github.com/kubernetes/autoscaler/issues/1845,https://api.github.com/repos/kubernetes/autoscaler/issues/1845,cluster-autoscaler 1.2.2 panics on scale-up with zero size ASGs on AWS,"# Summary
cluster-autoscaler 1.2.2, when a scale-up event triggers and an ASG in the set has min_size=0 and desired_size=0 panics

# Details

## Configuration

* Kubernetes running via EKS in AWS us-east-2
* [kube2iam](https://github.com/jtblin/kube2iam) in use for role assignments
* all instance types involved are t3.2xlarge
* 3 nodes exist in a different ASG with a taint to keep the workload described here away

## Procedure

* configure 2 ASGs, one of which has 0 for desired and min size:
* configure a deployment which applies memory pressure to the system
* scale the deployment to cause an autoscaling event
* cluster-autoscaler panics with:

```
I0329 19:25:36.083152       1 static_autoscaler.go:114] Starting main loop
I0329 19:25:36.224835       1 static_autoscaler.go:263] Filtering out schedulables
I0329 19:25:36.225488       1 static_autoscaler.go:273] No schedulable pods
I0329 19:25:36.225521       1 scale_up.go:59] Pod default/memory-load-6d466bcd85-wshzk is unschedulable
I0329 19:25:36.225527       1 scale_up.go:59] Pod default/memory-load-6d466bcd85-kspsd is unschedulable
I0329 19:25:36.225531       1 scale_up.go:59] Pod default/memory-load-6d466bcd85-8qm5k is unschedulable
I0329 19:25:36.225535       1 scale_up.go:59] Pod default/memory-load-6d466bcd85-6msrv is unschedulable
I0329 19:25:36.225539       1 scale_up.go:59] Pod default/memory-load-6d466bcd85-wz54m is unschedulable
I0329 19:25:36.225543       1 scale_up.go:59] Pod default/memory-load-6d466bcd85-2v72b is unschedulable
I0329 19:25:36.225547       1 scale_up.go:59] Pod default/memory-load-6d466bcd85-fbspb is unschedulable
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x145dccd]

goroutine 63 [running]:
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*AwsManager).buildNodeFromTemplate(0xc420736320, 0xc4211d0030, 0xc42190d1c0, 0xc42190d1c0, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_manager.go:407 +0x48d
k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws.(*Asg).TemplateNodeInfo(0xc4211d0030, 0xc420efab40, 0xc421882f90, 0x2d)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/cloudprovider/aws/aws_cloud_provider.go:294 +0x78
k8s.io/autoscaler/cluster-autoscaler/core.GetNodeInfosForGroups(0xc42159d4e0, 0x4, 0x4, 0x5a07ca0, 0xc42168b180, 0x5a14620, 0xc420d42000, 0xc4210ceaa0, 0x3, 0x4, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/utils.go:228 +0x65a
k8s.io/autoscaler/cluster-autoscaler/core.ScaleUp(0xc421286fc0, 0xc4218ad8c0, 0x7, 0x8, 0xc42159d4e0, 0x4, 0x4, 0xc4210ceaa0, 0x3, 0x4, ...)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/scale_up.go:63 +0x393
k8s.io/autoscaler/cluster-autoscaler/core.(*StaticAutoscaler).RunOnce(0xc420627a80, 0xed4306830, 0xe04f47b72, 0x5bf5180, 0x0, 0x0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/core/static_autoscaler.go:299 +0x296a
main.run(0xc4206fac80)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:269 +0x474
main.main.func2(0xc42005fbc0)
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/main.go:356 +0x2a
created by k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection.(*LeaderElector).Run
	/gopath/src/k8s.io/autoscaler/cluster-autoscaler/vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:145 +0x97
```
## Procedure Details

Initial ASG configuration:

<img width=""821"" alt=""Screen Shot 2019-03-29 at 3 25 12 PM"" src=""https://user-images.githubusercontent.com/1293125/55258560-918c1100-5239-11e9-9565-83f8bfa941b8.png"">

cluster autoscale definition:
```
...
     containers:
        - image: k8s.gcr.io/cluster-autoscaler:v1.2.2
          name: cluster-autoscaler
          env:
            - name: AWS_REGION
              value: us-east-2
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --skip-nodes-with-system-pods=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/sandbox-main
...
```

Memory pressure deployment created with ""kubectl create"":

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""1""
  creationTimestamp: null
  generation: 1
  labels:
    run: memory-load
  name: memory-load
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/memory-load
spec:
  progressDeadlineSeconds: 600
  replicas: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      run: memory-load
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: memory-load
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: memory-load
        resources:
          requests:
            memory: 1G
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status: {}
```

# Work Around

## Avoidance

Ensure that all ASGs have a minimum of 1 node and it works as expected, both scale-up and scale-down.  This will prevent the issue from occurring or recover from it if it has already occurred.
",open,False,2019-03-29 19:49:43,2019-04-01 08:52:45
autoscaler,achinthagunasekara,https://github.com/kubernetes/autoscaler/pull/1846,https://api.github.com/repos/kubernetes/autoscaler/issues/1846,YAML linting Azure examples,YAML linting Azure examples,open,True,2019-03-31 23:37:56,2019-04-05 09:03:31
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1847,https://api.github.com/repos/kubernetes/autoscaler/issues/1847,Only refresh instances cache for one mig on cache miss in GetMigForInstance,,closed,True,2019-04-01 15:37:11,2019-04-05 12:21:20
autoscaler,aleksandra-malinowska,https://github.com/kubernetes/autoscaler/pull/1848,https://api.github.com/repos/kubernetes/autoscaler/issues/1848,Fix default scale down delay after delete,"It defaulted to default value for --scan-interval (10s), not actual value passed to this flag. Setting it to 0 means it'll effectively be limited only be --scan-interval anyway.",closed,True,2019-04-02 10:52:11,2019-04-02 11:04:35
autoscaler,Shnatsel,https://github.com/kubernetes/autoscaler/issues/1849,https://api.github.com/repos/kubernetes/autoscaler/issues/1849,Behavior wrt min/max limits is not publicly documented,"[Cluster autoscaler FAQ](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md) does not describe how CA handles min/max limits.

People often expect min/max to be enforced by CA, and that changing the minimum number of nodes alone will trigger a scale-up ([example](https://b.corp.google.com/issues/129746085)). The actual behavior is that CA does not enforce these limits, so if someone manually resizes the cluster beyond those limits, CA will not intervene.

Since this is a common cause of misunderstanding, we need some public documentation on the expected behavior.",closed,False,2019-04-02 13:15:02,2019-04-04 10:32:52
autoscaler,bskiba,https://github.com/kubernetes/autoscaler/pull/1850,https://api.github.com/repos/kubernetes/autoscaler/issues/1850,VPA deployment switched to 0.5.0,,closed,True,2019-04-02 15:06:00,2019-04-02 15:35:25
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1851,https://api.github.com/repos/kubernetes/autoscaler/issues/1851,Add steps to use AWS Credentials for on premise case,"Resolve #1798 

User runs cluster autoscaler in on premise cluster and cluster autoscaler uses AWS cloud provider to scale up/down.

Rather than using IAM policy for autoscaler to get access to AWS services, they should use AWS credentials in Env to get AWS SDK authenticated and authorized.",closed,True,2019-04-02 22:58:49,2019-04-03 23:33:48
autoscaler,mmingorance-dh,https://github.com/kubernetes/autoscaler/issues/1852,https://api.github.com/repos/kubernetes/autoscaler/issues/1852,Annotation cluster-autoscaler.kubernetes.io/safe-to-evict=false not working,"I recently added the following annotation in some of my critical pods to avoid cluster-autoscaler to remove the nodes where these pods are running in. `cluster-autoscaler.kubernetes.io/safe-to-evict=false`
According to what I read in the documentation: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node This should prevent cluster autoscaler from removing the node, however I've just seen how one of the nodes running a pod with this annotations has been deleted by cluster autoscaler.

My Kubernetes version is 1.11 and the cluster-autoscaler version is 1.3.7
According to this PR (https://github.com/kubernetes/autoscaler/pull/1054), this annotation should work as expected.

Could someone help me to identify why cluster-autoscaler does not take this annotation in consideration?

Thanks!",open,False,2019-04-02 23:14:57,2019-04-05 17:03:53
autoscaler,zedtux,https://github.com/kubernetes/autoscaler/issues/1853,https://api.github.com/repos/kubernetes/autoscaler/issues/1853,[vertical-pod-autoscaler] The README is hard to read/understand,"Please move all the migration instructions in a Wiki page or a `MIGRATE.md` file or something like that keeping [the `README.md` file](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md) cleaner so that new comers (like me) aren't spammed, therefore lost, with useless information.",open,False,2019-04-03 04:39:35,2019-04-04 07:09:02
autoscaler,MartinEmrich,https://github.com/kubernetes/autoscaler/issues/1854,https://api.github.com/repos/kubernetes/autoscaler/issues/1854,Not discovering ASGs on AWS,"Hello!

I am trying to get autoscaler to work, I use v 1.3.7 (which should match with AWS's K8s 1.11).

I have 3 ASGs (one for each AZ), all tagged:

```
k8s.io/cluster-autoscaler/awsref | owned  
k8s.io/cluster-autoscaler/enabled | true  
```

Here's my startup parameters:
```
          command:
            - ./cluster-autoscaler
            - --v=99
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - >-
              --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,
              k8s.io/cluster-autoscaler/awsref
```

But I get this message if I try to create more pods to trigger a scale-up:

```
  Normal   NotTriggerScaleUp  2m19s (x10 over 3m57s)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added)
```

In the autoscaler logs, I see just this:
```
I0403 09:12:58.999402       1 static_autoscaler.go:135] Starting main loop
I0403 09:12:59.052834       1 auto_scaling_groups.go:316] Regenerating instance to ASG map for ASGs: []
I0403 09:12:59.052851       1 aws_manager.go:148] Refreshed ASG list, next refresh after 2019-04-03 09:13:09.052847656 +0000 UTC m=+1980.976476835
```
I would assume that if autoscaler would detect my ASGs, they would appear between the square braces?

Both the EKS cluster and the workers have the prescribed IAM permissions.",closed,False,2019-04-03 09:16:30,2019-04-05 16:57:34
autoscaler,losipiuk,https://github.com/kubernetes/autoscaler/pull/1855,https://api.github.com/repos/kubernetes/autoscaler/issues/1855,Update MIG target size cache via LIST api call,,open,True,2019-04-03 15:06:56,2019-04-04 15:39:55
autoscaler,Rajat-0,https://github.com/kubernetes/autoscaler/pull/1856,https://api.github.com/repos/kubernetes/autoscaler/issues/1856,Adding Migration Doc.,"Separating Migration doc from README to make it clean and understandable.
closes: #1853 ",open,True,2019-04-04 08:05:23,2019-04-05 13:28:36
autoscaler,Shnatsel,https://github.com/kubernetes/autoscaler/pull/1857,https://api.github.com/repos/kubernetes/autoscaler/issues/1857,Document behavior of min_nodes and max_nodes,"Initial documentation for the handling of min_nodes and max_nodes parameters. These are currently not publicly documented and have caused some confusion.

Fixes #1849",closed,True,2019-04-04 10:01:17,2019-04-04 16:09:19
autoscaler,piontec,https://github.com/kubernetes/autoscaler/issues/1858,https://api.github.com/repos/kubernetes/autoscaler/issues/1858,addon-resizer modifies different attributes than request/limit only,"We're running addon-resizer with https://github.com/kubernetes/kube-state-metrics, which is the default deployment. Unfortunately, addon-resizer changes other properties than limits/requests on the managed pod:
```
   name: kube-state-metrics
   namespace: kube-system
 spec:
-  progressDeadlineSeconds: 2147483647
+  progressDeadlineSeconds: 600
   replicas: 1
   revisionHistoryLimit: 10
   selector:
@@ -189,6 +189,7 @@
         terminationMessagePath: /dev/termination-log
         terminationMessagePolicy: File
       dnsPolicy: ClusterFirst
+      priorityClassName: system-cluster-critical
       restartPolicy: Always
       schedulerName: default-scheduler
       securityContext: {}
```

SInce the code seems to not update them anywhere, it must be a bug in underlying libs.",open,False,2019-04-04 12:38:57,2019-04-05 13:24:59
autoscaler,piontec,https://github.com/kubernetes/autoscaler/pull/1859,https://api.github.com/repos/kubernetes/autoscaler/issues/1859,rework addon-resizer to use client-go ,"The addon-resizer code doesn't modify attributes other than limits/requests, so it seems the bug is in libraries used. This is causing problems like #1858 and probably also #1799 and #1457 .  To solve that, I upgraded libraries to more recent ones. 

Changes include:
* drop `godep` in favor of `go mod` - I was becoming sick trying to do that using `godep`
* code is reworked to use `client-go` (version v8.0.0) and api libs (from 1.11 k8s release)
* go version is updated to 1.12.1
* `Makefile` builds docker image OK

Testing:
* unit tests are passing
* I manually verified that `addon-scaler` works OK running against an already provisioned cluster with `kube-state-metrics` pod.
",open,True,2019-04-04 12:52:55,2019-04-05 12:11:12
autoscaler,AESwrite,https://github.com/kubernetes/autoscaler/issues/1860,https://api.github.com/repos/kubernetes/autoscaler/issues/1860,Failed to create AWS Manager: RequestError: send request failed ( i/o timeout),"Hello, 
I'm currently using kubernetes v1.12.5 and CA v1.12.3
Cluster was created with kubespray v.2.8.3 (kubeadm enabled). 
Provider: AWS

I'm using the standart example of cluster-autoscaler-one-asg.yaml, I've modified only this lines:

>containers:
        _- image: k8s.gcr.io/cluster-autoscaler:v1.12.3_

>  _--nodes=1:4:k8s-worker-20190403143728147500000003-asg_

>   _env:
            - name: AWS_REGION
              value: us-east-2_

>   volumes:
        - name: ssl-certs
          hostPath:
            _path: ""/etc/ssl/certs/ca-bundle.crt""_

**I get this kind of error (the same in different versions of CA):**


>I0404 13:30:06.959934       1 leaderelection.go:227] successfully renewed lease kube-system/cluster-autoscaler
**E0404 13:30:07.364127       1 aws_manager.go:153] Failed to regenerate ASG cache: RequestError: send request failed
caused by: Post https://autoscaling.us-east-2.amazonaws.com/: dial tcp: i/o timeout
F0404 13:30:07.364158       1 cloud_provider_builder.go:149] 
caused by: Post https://autoscaling.us-east-2.amazonaws.com/: dial tcp: i/o timeout'**

I tried to use CA v1.3.0 on kubernetes v1.11.3 last week (the same yaml file, only different version of CA), and it worked. But today i get timeout error even on that v1.11.3 configuration (i didn't change anything in this configuration from last week).

How can i solve this issue? I will be glad to any help!

Update 1: container with autoscaler somehow can't reach internet. ",open,False,2019-04-04 14:04:33,2019-04-05 09:09:52
autoscaler,reinaldo-pinto,https://github.com/kubernetes/autoscaler/pull/1861,https://api.github.com/repos/kubernetes/autoscaler/issues/1861,Fix: failed forbidden Jobs,Adding rules needed of the Jobs on file to work.,open,True,2019-04-04 14:46:51,2019-04-04 16:17:07
autoscaler,frobware,https://github.com/kubernetes/autoscaler/issues/1862,https://api.github.com/repos/kubernetes/autoscaler/issues/1862,fix_gopath.sh is broken since the addition of quotes,"Since #1837 no globbing will take place.

For example:

```console
aim@mb:~/go/src/k8s.io/autoscaler/cluster-autoscaler
$ ./fix_gopath.sh 
Updating /Users/aim/go
Updating staging dep api
Initialized empty Git repository in /Users/aim/go/src/k8s.io/api/.git/
cp: /Users/aim/go/src/k8s.io/kubernetes/staging/src/k8s.io/api/*: No such file or directory
```

Whereas if you revert PR #1837

```console
$ ./fix_gopath.sh 
Updating /Users/aim/go
Updating staging dep api
Initialized empty Git repository in /Users/aim/go/src/k8s.io/api/.git/
[master (root-commit) da99f9a] from_staging
 321 files changed, 210434 insertions(+)
 create mode 100644 BUILD
 create mode 100644 CONTRIBUTING.md
...
```
",open,False,2019-04-04 16:25:38,2019-04-05 13:24:26
autoscaler,frobware,https://github.com/kubernetes/autoscaler/pull/1863,https://api.github.com/repos/kubernetes/autoscaler/issues/1863,cloudprovider/azure: fix formatting directive,"Switch formatting directive to `%v` when logging `instances`. This
allows `go test ./...` to work in `autoscaler/cluster-autoscaler`.

Without this running the tests currently fail with:

```console
cloudprovider/azure/azure_scale_set.go:282:2: Infof format %q has arg instances of wrong type []*k8s.io/autoscaler/cluster-autoscaler/cloudprovider/azure.azureRef
FAIL	k8s.io/autoscaler/cluster-autoscaler/cloudprovider/azure [build failed]
```

As this is trying to print `[]*azureRef` I added a `String()` method
to `azureRef` which returns `Name`. Without that it will print pointer
values which is unlikely to be useful as a default.",closed,True,2019-04-04 16:55:57,2019-04-05 11:13:48
autoscaler,Jeffwan,https://github.com/kubernetes/autoscaler/pull/1864,https://api.github.com/repos/kubernetes/autoscaler/issues/1864,Upgrade CA version in aws example and fix autodiscover example,"Resolve two issues.

1. Some typos from commit https://github.com/kubernetes/autoscaler/commit/e4fcef094ef227adeb8248669eedf2787c9246eb make Autodiscovery samples not working well. ASG can not be detected.
https://github.com/kubernetes/autoscaler/issues/1854

2. Not sure why default SSL cert path has been changed in commit which makes CA fails in EKS
https://github.com/kubernetes/autoscaler/commit/82162e555b059094c8ddd67e93bbd1f69279b351


EKS support for Kubernetes 1.12 has been released and I upgrade doc samples default to v1.12.3
A few users reports it's hard to configure CA in EKS, I make samples ssl ca path default to compatible with EKS-AMI. Ubuntu ssl ca path is moved to Notes. ",closed,True,2019-04-04 22:20:22,2019-04-05 08:17:20
autoscaler,achinthagunasekara,https://github.com/kubernetes/autoscaler/pull/1865,https://api.github.com/repos/kubernetes/autoscaler/issues/1865,Bug fix - #1837,Bug fix reported in https://github.com/kubernetes/autoscaler/issues/1862,open,True,2019-04-04 22:25:07,2019-04-05 11:03:02
autoscaler,frobware,https://github.com/kubernetes/autoscaler/pull/1866,https://api.github.com/repos/kubernetes/autoscaler/issues/1866,Add cluster-api based cloudprovider,"This is a new cloudprovider implementation based on the [cluster-api](https://github.com/kubernetes-sigs/cluster-api) project.

This PR has been cut from https://github.com/openshift/kubernetes-autoscaler/commit/b38dd11ffab3a1e8641420448ff7c5547072c836 and updated to reflect changes in autoscaler master (1.15); the openshift version is using 1.13. This implementation has been working well for many months and will scale up/down via a MachineSet or a MachineDeployment.

Known limitations:
- does not support scale from 0
- scale down is not currently atomic

Node groups are represented when a MachineSet or MachineDeployment has positive scaling values. The min/max values are encoded as annotations on the respective objects, for example:

```yaml
apiVersion: cluster.k8s.io/v1alpha1
kind: MachineSet
metadata:
  annotations:
    sigs.k8s.io/cluster-api-autoscaler-node-group-min-size: ""1""
    sigs.k8s.io/cluster-api-autoscaler-node-group-max-size: ""10""
```

To map between nodes and machines we currently depend on the following annotation getting added to the node object, for example:

```yaml
annotations:
    cluster.k8s.io/machine: ""machine-name""
```

We currently do this using a [nodelink-controller](https://github.com/openshift/machine-api-operator/blob/master/cmd/nodelink-controller/main.go#L63) but we have future plans to remove this and rely on the `node.Spec.ProviderID` value.

For scale down the cloudprovider implementation annotates the machine object with:

```yaml
annotations:
    cluster.k8s.io/cluster-api-delete-machine: <date>
```

and the machine controller will drain the node, delete the machine, then finally delete the node.

- We have future plans to address the scale from 0 limitation using MachineClasses
- We have future plans to address scale down using strategies in the machine controller ",open,True,2019-04-05 14:52:50,2019-04-05 14:53:54
