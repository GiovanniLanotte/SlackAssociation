name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/1,https://api.github.com/repos/kubernetes/contrib/issues/1,copy over unreferenced contrib to new contrib repo,"When this goes in, it should be the opposite of kubernetes/kubernetes#12693
",closed,True,2015-08-13 23:33:50,2015-08-14 00:14:00
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/2,https://api.github.com/repos/kubernetes/contrib/issues/2,copy over unreferenced contrib to new contrib repo,"When this goes in, it should be the opposite of kubernetes/kubernetes#12693
",closed,True,2015-08-13 23:35:15,2015-08-14 00:14:00
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/3,https://api.github.com/repos/kubernetes/contrib/issues/3,create travis yml and enable travis ci build,"carry over the simple checks to begin with. make sure everything compiles and that tests pass.
",closed,True,2015-08-14 00:24:02,2015-08-14 18:30:36
contrib,Arano-kai,https://github.com/kubernetes/contrib/pull/4,https://api.github.com/repos/kubernetes/contrib/issues/4,Ansible: add to cert all master ip's (dirty fix),"Allow nodes to be connected to the non-default interface
",closed,True,2015-08-14 17:07:16,2015-08-14 20:10:23
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/5,https://api.github.com/repos/kubernetes/contrib/issues/5,Move submit-queue to contrib,,closed,True,2015-08-14 20:30:40,2015-08-14 22:19:43
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/6,https://api.github.com/repos/kubernetes/contrib/issues/6,remove extra godeps that snuck in somehow when we first moved the repo over,"I am godep inept :frowning: 
",closed,True,2015-08-14 20:35:11,2015-08-14 22:30:20
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/7,https://api.github.com/repos/kubernetes/contrib/issues/7,move service-loadbalancer to contrib,,closed,True,2015-08-14 22:19:26,2015-08-14 22:22:33
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/8,https://api.github.com/repos/kubernetes/contrib/issues/8,"Fix up the submit queue for the new repo, add docker image",,closed,True,2015-08-14 23:55:40,2015-08-14 23:59:30
contrib,typophil,https://github.com/kubernetes/contrib/issues/9,https://api.github.com/repos/kubernetes/contrib/issues/9,setup.sh fails for various reasons,"Hello all,

I'm behind a proxy, so the first thing I had to solve was that `contrib/ansible/roles/kubernetes/files/make-ca-cert.sh` was not using the environment for a proxy nor a `curlrc` file. I ended up adding the `--proxy my.proxy.here:3128` to the `curl` call directly. After that, the script had a problem with `./easyrsa --batch ""--req-cn=${cert_ip}@$(date +%s)"" build-ca nopass` as `${cert_ip}@$(date +%s)` would resolve to a string longer than 64 characters. After I worked around that, the ansible run went on up until

```
TASK: [master | Copy master binaries] *****************************************
```

which in turn failed due to:

```
fatal: [kube-master.WHOOPS] => input file not found at \ 
/home/WHOOPS/kubernetes/contrib/ansible/roles/_output/local/go/bin/kube-apiserver \
or /home/WHOOPS/kubernetes/_output/local/go/bin/kube-apiserver
```

And because I'm new to ansible I was wondering, because I can see this in `roles/master/tasks/localBuildInstall.yml`

```
- name: Copy master binaries
 copy:
   src: ../../_output/local/go/bin/{{ item }}
   dest: /usr/bin/
   mode: 755
 with_items:
   - kube-apiserver
   - kube-scheduler
   - kube-controller-manager
   - kubectl
 notify: restart daemons
```

making sense but prior to that it doesn't say that it'll build something. Nothing. So I'm curious, when should building the mentioned binaries in that `_outcome` directory happen?

So I was following 

```
http://kubernetes.io/v1.0/docs/getting-started-guides/fedora/fedora_ansible_config.html
```

which already had the trouble of not being up2date (it doesn't state that the ansible stuff has moved) and that has lead so far nowhere.

Is this a _cluster.yml_ switch, to enable that build?
",closed,False,2015-08-17 14:33:12,2015-08-18 08:48:57
contrib,BenTheElder,https://github.com/kubernetes/contrib/pull/10,https://api.github.com/repos/kubernetes/contrib/issues/10,Add netperf tester tool,"Adds a tool for running netperf tests on a k8s cluster.
",closed,True,2015-08-18 03:39:49,2015-08-18 22:06:29
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/11,https://api.github.com/repos/kubernetes/contrib/issues/11,Add a pod YAML file for the submit-queue.,,closed,True,2015-08-18 22:10:06,2015-08-27 20:11:18
contrib,stevesloka,https://github.com/kubernetes/contrib/issues/12,https://api.github.com/repos/kubernetes/contrib/issues/12,F5 LoadBalancer,"I'm looking to see what the effort would be to write an F5 integration. Currently my enterprise uses RHEL Atomic hosts with an F5 load balancer. We can access the F5 API to programmatically create virtual servers. 

On AWS, I can specify the ""LoadBalancer"" service type, and it auto creates the ELB + configures. I'm looking for a way to do exactly the same thing. I am 100% cool with doing the effort, just looking for some strategy around how / where / and integration. 

Is this pluggable where I could mirror how it works on AWS? Or do I need to change approaches. It's probably important to be pluggable as I""m not building k8s myself, rather, using the RHEL packages from Atomic upgrades.  
",closed,False,2015-08-19 01:40:54,2018-01-02 01:30:42
contrib,jlebon,https://github.com/kubernetes/contrib/pull/13,https://api.github.com/repos/kubernetes/contrib/issues/13,ansible: factor out kube_cert_ip,"This will help users more easily specify the IP addresses for which the
certificate should be valid. The make-ca-cert.sh script allows for many
""magic strings"" such as '_use_aws_external_ip_', which can automatically
query the metadata service to retrieve the public IP. This modification
makes changing that setting much easier (and less likely to change).
",closed,True,2015-08-19 15:59:36,2015-08-19 17:53:36
contrib,jeffbean,https://github.com/kubernetes/contrib/issues/14,https://api.github.com/repos/kubernetes/contrib/issues/14,Ansible should use configuration and script files from local system ,"Right now we use the `get_url` to obtain most of the addon and configuration files from the Github content. 

Example:

``` yaml
- name: Make sure the system services namespace exists
  get_url:
    url=https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/saltbase/salt/kube-addons/namespace.yaml
    dest=""{{ kube_config_dir }}/addons/""
    force=yes
```

We should have a mechanism to use files from the local system. Since the contrib folder is now outside the main repository this is a little more difficult since we can not assume file locations. It would be nice to have as little logic in the `setup.sh` script as well but might be the best solution? 

I propose we keep the downloading versions of the files and have the option to use local files. We do this with `packageManager` and `localBuild` for the actual kubernetes binaries now. Using this same technique we have the config files.

Logging the bug to track ideas, suggestions and progress. 
",closed,False,2015-08-19 18:23:11,2018-02-14 03:03:10
contrib,jlebon,https://github.com/kubernetes/contrib/pull/15,https://api.github.com/repos/kubernetes/contrib/issues/15,Small iptables fixes,"The first patch ensures that we wait for the lock even when only listing iptables entries. Otherwise we may sometimes get a busy lock error.

The second patch removes an unnecessary iptables restart, which coincidentally resolves some issues it caused.
",closed,True,2015-08-19 21:16:13,2015-09-01 14:41:14
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/16,https://api.github.com/repos/kubernetes/contrib/issues/16,Add a github munger.,"Copied over from https://github.com/kubernetes/kubernetes/pull/12365
",closed,True,2015-08-21 04:13:32,2015-08-21 18:16:05
contrib,Arano-kai,https://github.com/kubernetes/contrib/pull/17,https://api.github.com/repos/kubernetes/contrib/issues/17,Ansible: fix etcd connection refuse,"Connection fail if fqdn not resolved
",closed,True,2015-08-21 09:28:37,2015-11-27 09:41:36
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/18,https://api.github.com/repos/kubernetes/contrib/issues/18,Add a docker file and pod spec for the munger.,"@mikedanese @ixdy 
",closed,True,2015-08-21 18:45:27,2015-08-27 04:30:47
contrib,ixdy,https://github.com/kubernetes/contrib/issues/19,https://api.github.com/repos/kubernetes/contrib/issues/19,mergebot shouldn't self-assign PRs,"I created kubernetes/kubernetes#12950 and didn't assign someone, and the mergebot later assigned me (probably because of the files I was touching).

It probably shouldn't self-assign PRs.
",closed,False,2015-08-21 18:59:52,2015-08-31 17:27:12
contrib,jlebon,https://github.com/kubernetes/contrib/pull/20,https://api.github.com/repos/kubernetes/contrib/issues/20,Add rules for flannel and kube-proxy traffic and re-enable support for firewalld,"Small patches to get flannel to work on Atomic.
",closed,True,2015-08-24 14:40:23,2015-09-01 14:31:06
contrib,jasonbrooks,https://github.com/kubernetes/contrib/issues/21,https://api.github.com/repos/kubernetes/contrib/issues/21,provisioning hangs at TASK: [node | Get the node token values] w/ 2 or more nodes,"When provisioning using Vagrantfile and default centos/7 box w/ NUM_NODES greater than 1, the process hangs at `TASK: [node | Get the node token values]`.

The problem appears to be the same as reported in https://bugzilla.redhat.com/show_bug.cgi?id=1242682 and https://bugzilla.redhat.com/show_bug.cgi?id=1240613.

The following change addresses the issue for me:

```
diff --git a/ansible/vagrant/Vagrantfile b/ansible/vagrant/Vagrantfile
index f6505a4..7d40fa5 100644
--- a/ansible/vagrant/Vagrantfile
+++ b/ansible/vagrant/Vagrantfile
@@ -128,6 +128,7 @@ Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
         ansible.groups = groups
         ansible.playbook = ""./vagrant-ansible.yml""
         ansible.limit = ""all"" #otherwise the metadata wont be there for ipv4?
+        ansible.raw_ssh_args = ['-o ControlMaster=no']
       end
     end

@@ -137,6 +138,7 @@ Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
       ansible.playbook = ""../cluster.yml""
       ansible.limit = ""all"" #otherwise the metadata wont be there for ipv4?
       ansible.tags = ansible_tags
+      ansible.raw_ssh_args = ['-o ControlMaster=no']
     end
   end
 end
```
",closed,False,2015-08-25 00:53:04,2015-09-01 23:42:21
contrib,sgallagher,https://github.com/kubernetes/contrib/pull/22,https://api.github.com/repos/kubernetes/contrib/issues/22,Ensure that kube-apiserver starts after etcd,"If etcd is running on the same system as kube-apiserver and is
being started as part of the same series of services (such as with a
custom systemd target), we need to ensure that it orders later in
startup than etcd or it will fail to find the local service.

Note: this does not attempt to start etcd on the local system if it
is not already part of the transaction, so it will have no effect
if someone calls `systemctl start kube-apiserver.service` directly.

This is the master-branch version of https://github.com/kubernetes/kubernetes/pull/12939/
",closed,True,2015-08-26 11:30:54,2015-08-31 17:33:46
contrib,jeffbean,https://github.com/kubernetes/contrib/pull/23,https://api.github.com/repos/kubernetes/contrib/issues/23,Change etcd firewall ports to variables from the role,"If you change the roles ports in any fashion the firewall ports would always stay the same.
",closed,True,2015-08-26 17:17:27,2015-08-27 03:59:59
contrib,thockin,https://github.com/kubernetes/contrib/pull/24,https://api.github.com/repos/kubernetes/contrib/issues/24,Add a PR-size munger for github,"This analyzes the additions/deletions stats and adds a label about the size of
the PR.  The heuristic is totally arbitratry for now.

This makes PR munging much slower because it has to get every PR to get the
stats.  C'est la vie.

@brendandburns @lavalamp 
",closed,True,2015-08-27 17:57:36,2015-08-29 05:50:11
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/25,https://api.github.com/repos/kubernetes/contrib/issues/25,Switch to rate limiting and cache,"@thockin @mikedanese 
",closed,True,2015-08-27 20:09:18,2015-08-30 14:47:38
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/26,https://api.github.com/repos/kubernetes/contrib/issues/26,Fix a port due to an incomplete push...,,closed,True,2015-08-27 20:18:32,2015-08-27 20:18:37
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/27,https://api.github.com/repos/kubernetes/contrib/issues/27,Update to version 0.2,,closed,True,2015-08-27 20:45:58,2015-08-30 14:40:18
contrib,mattma,https://github.com/kubernetes/contrib/issues/28,https://api.github.com/repos/kubernetes/contrib/issues/28,service-loadbalancer goes into an infinite loop of Restart container,"I followed the step described in [README](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer). 

I run into an issue of 

```
core@kube-node-01 ~ $ docker logs c3e318d07f32
I0827 21:43:17.144454       1 service_loadbalancer.go:383] Creating new loadbalancer: {Name:haproxy ReloadCmd:./haproxy_reload Config:/etc/haproxy/haproxy.cfg Template:template.cfg Algorithm:roundrobin}
I0827 21:43:17.144510       1 service_loadbalancer.go:426] All tcp/https services will be ignored.
F0827 21:43:17.145153       1 service_loadbalancer.go:436] Failed to create client: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
```

I was using the same command `kubectl create -f ./rc.yaml`. The state of the `pods` show Running, but it goes into an infinite loop of Restart container. 
",closed,False,2015-08-27 21:46:08,2015-08-27 22:08:48
contrib,jeffbean,https://github.com/kubernetes/contrib/pull/29,https://api.github.com/repos/kubernetes/contrib/issues/29,Ansible: Fix for etcd iptables variables in roles,"After looking at the history I forgot to push a fix to my own PR #23. Need
quotes on variables in the with items context.
",closed,True,2015-08-27 22:57:17,2015-08-28 00:09:06
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/30,https://api.github.com/repos/kubernetes/contrib/issues/30,Add some additional error checking.,,closed,True,2015-08-28 03:30:20,2015-08-30 14:38:49
contrib,vishh,https://github.com/kubernetes/contrib/pull/31,https://api.github.com/repos/kubernetes/contrib/issues/31,Updating kubelet reviewers.,,closed,True,2015-08-28 18:38:49,2015-08-31 17:25:55
contrib,eparis,https://github.com/kubernetes/contrib/pull/32,https://api.github.com/repos/kubernetes/contrib/issues/32,Do not counts Godeps changes in the sizer,"Github hides these in review, we typically don't review them closely, and hopefully the godep checker will tell us if someone isn't pushing what they say they are...
",closed,True,2015-08-29 05:44:19,2015-08-31 17:32:33
contrib,jfchevrette,https://github.com/kubernetes/contrib/pull/33,https://api.github.com/repos/kubernetes/contrib/issues/33,link to the official CONTRIBUTING.md doc,"Instead of duplicating the info (along with invalid links), refer the user to the official CONTRIBUTING.md doc
",closed,True,2015-08-29 19:03:04,2015-08-31 19:03:51
contrib,eparis,https://github.com/kubernetes/contrib/pull/34,https://api.github.com/repos/kubernetes/contrib/issues/34,mungegithub: Fix PR List options,"Sort: desc

Is not valid according to
https://godoc.org/github.com/google/go-github/github#PullRequestListOptions

So we have been sorting ascending. Continue to do that, but at least
make the options correct/obvious. If we want descending that's fine, but
this PR has NO actual change to the operation of the code
",closed,True,2015-08-29 19:52:10,2015-08-31 17:32:30
contrib,eparis,https://github.com/kubernetes/contrib/pull/35,https://api.github.com/repos/kubernetes/contrib/issues/35,mungegithub: blunderbuss: do not self-assign issues,"If I submit a PR, probably best if it is not assigned to me.
",closed,True,2015-08-30 15:06:28,2015-08-31 17:32:32
contrib,eparis,https://github.com/kubernetes/contrib/pull/36,https://api.github.com/repos/kubernetes/contrib/issues/36,mungegithub: Only do github operations once per PR,"not once per munger
",closed,True,2015-08-30 15:29:21,2015-08-31 17:32:31
contrib,eparis,https://github.com/kubernetes/contrib/pull/37,https://api.github.com/repos/kubernetes/contrib/issues/37,mungegithub: Create an options structure instead of individual variables,"Makes adding new command line args which get passed to mungers much more sane.
",closed,True,2015-08-30 15:40:32,2015-08-31 17:32:29
contrib,eparis,https://github.com/kubernetes/contrib/pull/38,https://api.github.com/repos/kubernetes/contrib/issues/38,mungegithub: Add a maximum pr to check,"Useful for debugging mungers as you can greatly reduce how long it takes to run and can look at specific pr (ranges)
",closed,True,2015-08-30 15:44:09,2015-08-31 17:32:31
contrib,eparis,https://github.com/kubernetes/contrib/pull/39,https://api.github.com/repos/kubernetes/contrib/issues/39,mungegithub: sizer: Skip generated files,"We automatically generated deep copies, conversions, docs, bash completions, swagger stuff. Humans don't look at those things closely, so stop counting them in the 'size' of a PR.

To be honest, this doesn't currently ignore the 'docs' but there is an open PR in kubernetes which redoes how we generate docs so this will hopefully start working once that lands. (currently we scatter the '.generated_docs' files all over the repo instead of just one file at the root of the repo, this expects it to be in the root)
",closed,True,2015-08-30 15:53:39,2015-08-31 17:32:33
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/40,https://api.github.com/repos/kubernetes/contrib/issues/40,Make the http cache optional and default to false,"Caching PRs seems to mess up the mergeability checks.
",closed,True,2015-08-31 19:45:00,2015-09-01 20:07:52
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/41,https://api.github.com/repos/kubernetes/contrib/issues/41,work around for issue #21,,closed,True,2015-08-31 21:17:17,2015-08-31 21:21:11
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/42,https://api.github.com/repos/kubernetes/contrib/issues/42,Add two new mungers for managing the submit-queue,"@eparis @zmerlynn 
",closed,True,2015-08-31 21:45:02,2015-08-31 22:10:26
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/43,https://api.github.com/repos/kubernetes/contrib/issues/43,Update the munger to 0.4,"@thockin @eparis 
",closed,True,2015-08-31 23:20:36,2015-09-01 03:38:25
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/44,https://api.github.com/repos/kubernetes/contrib/issues/44,Add a the start of a web UX,"@eparis since he likes to review my stuff ;)
",closed,True,2015-08-31 23:26:19,2015-09-01 21:35:40
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/45,https://api.github.com/repos/kubernetes/contrib/issues/45,Fix some bugs in LGTM detection by searching more events,"@eparis @thockin 
",closed,True,2015-08-31 23:44:59,2015-09-01 03:37:11
contrib,brendandburns,https://github.com/kubernetes/contrib/issues/46,https://api.github.com/repos/kubernetes/contrib/issues/46,Crash if an reviewer isn't found,"``` console
E0901 16:56:30.766474      98 blunderbuss.go:99] No owners found for PR 13025
panic: runtime error: invalid memory address or nil pointer dereference

k8s.io/contrib/mungegithub/pulls.(*BlunderbussMunger).MungePullRequest(0x4c208042130, 0x4c2080362c0, 0x8c15f0, 0xa, 0x8c15f0, 0xa, 0x4c208d05458, 0x4c2098454a0, 0x4c2082c0d00, 0x1, ...)
    /usr/local/google/home/bburns/brendandburns/src/k8s.io/contrib/mungegithub/pulls/blunderbuss.go:85 +0x129
k8s.io/contrib/mungegithub/pulls.mungePullRequestList(0x4c208d02000, 0x64, 0x8d, 0x4c2080362c0, 0x8c15f0, 0xa, 0x8c15f0, 0xa, 0x4c2080ced50, 0x3, ...)
    /usr/local/google/home/bburns/brendandburns/src/k8s.io/contrib/mungegithub/pulls/pulls.go:120 +0x6b4
k8s.io/contrib/mungegithub/pulls.MungePullRequests(0x4c2080362c0, 0x7ffe6e7e8def, 0x24, 0x8c15f0, 0xa, 0x8c15f0, 0xa, 0x0, 0x0, 0x0, ...)
    /usr/local/google/home/bburns/brendandburns/src/k8s.io/contrib/mungegithub/pulls/pulls.go:84 +0x3c6
main.main()
    /usr/local/google/home/bburns/brendandburns/src/k8s.io/contrib/mungegithub/mungegithub.go:68 +0x521

goroutine 5 [chan receive]:
github.com/golang/glog.(*loggingT).flushDaemon(0xadb780)
    /usr/local/google/home/bburns/brendandburns/src/k8s.io/contrib/Godeps/_workspace/src/github.com/golang/glog/glog.go:879 +0x78
created by github.com/golang/glog.init·1
    /usr/local/google/home/bburns/brendandburns/src/k8s.io/contrib/Godeps/_workspace/src/github.com/golang/glog/glog.go:410 +0x2a7

goroutine 17 [runnable]:
net/http.(*persistConn).readLoop(0x4c208036370)
    /usr/lib/google-golang/src/net/http/transport.go:928 +0x9ce
created by net/http.(*Transport).dialConn
    /usr/lib/google-golang/src/net/http/transport.go:660 +0xc9f
```
",closed,False,2015-09-01 17:16:22,2017-01-20 21:58:17
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/47,https://api.github.com/repos/kubernetes/contrib/issues/47,Move mergeability detection into the main pull loop.,"Add a munger that adds the ""needs-rebase"" label.

@jlowdermilk 
",closed,True,2015-09-01 19:44:40,2015-09-01 21:35:56
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/48,https://api.github.com/repos/kubernetes/contrib/issues/48,Add a label and a comment if we need 'ok-to-merge',"@lavalamp 
",closed,True,2015-09-01 19:50:15,2015-09-01 19:51:24
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/49,https://api.github.com/repos/kubernetes/contrib/issues/49,"Break out of the loop on errors, don't continue.","This has the net effect of retrying the PR that just failed (or any other older PR if it exists) rather than moving on to the next PR.

In particular, this means that if the Jenkins pre-condition fails, we will sit on that PR until it is green, rather than moving on.

In general, this should do a better job of prioritizing older PRs.
",closed,True,2015-09-01 21:37:28,2015-09-01 21:58:01
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/50,https://api.github.com/repos/kubernetes/contrib/issues/50,Update the munger and submit-queue,"@lavalamp 
",closed,True,2015-09-01 23:34:03,2015-09-01 23:35:41
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/51,https://api.github.com/repos/kubernetes/contrib/issues/51,Add a few more top level catch-alls so that we don't miss on some PRs,"@thockin @smarterclayton @bgrant0607 @derekwaynecarr 

Moar owners for moar issuez
",closed,True,2015-09-02 04:13:40,2015-09-02 16:46:57
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/52,https://api.github.com/repos/kubernetes/contrib/issues/52,Add support for a 'safe-to-merge' label,"@ixdy @lavalamp @quinton-hoole 
",closed,True,2015-09-02 04:27:24,2015-09-02 17:55:22
contrib,3cky,https://github.com/kubernetes/contrib/pull/53,https://api.github.com/repos/kubernetes/contrib/issues/53,fix issue kubernetes/kubernetes#12663,"podex: support Kubernetes v1 API / Docker Registry 2.0
podex: support insecure HTTP/self-signed registries
",closed,True,2015-09-02 09:03:39,2015-09-03 10:40:26
contrib,Arano-kai,https://github.com/kubernetes/contrib/pull/54,https://api.github.com/repos/kubernetes/contrib/issues/54,Ansible: Fix docker startup on btrfs,"Also, move startup of docker dependent services to handlers
",closed,True,2015-09-02 22:32:12,2015-10-04 16:21:28
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/55,https://api.github.com/repos/kubernetes/contrib/issues/55,Push a new submit queue.,"@thockin @quinton-hoole @ixdy 

Update the submit queue, and add more jenkins-jobs to monitor.
",closed,True,2015-09-03 00:23:16,2015-09-04 23:32:49
contrib,zarezadeh,https://github.com/kubernetes/contrib/pull/56,https://api.github.com/repos/kubernetes/contrib/issues/56,Ansible: install kubernetes from github releases,"This commit will enable the option of installing kubernetes components by downloading archives from github releases in ansible playbook. To enable this set `source_type: ghReleases` in `group_vars/all.yml`.
The default version of kubernetes is set to `v1.0.3` but it can be overridden by setting `kubernetes_version` variable
",closed,True,2015-09-03 06:58:37,2016-03-20 20:58:39
contrib,brendandburns,https://github.com/kubernetes/contrib/issues/57,https://api.github.com/repos/kubernetes/contrib/issues/57,[mungegithub] Add a [ROBOT] tag to comments from the robots to make them easier to filter.,,closed,False,2015-09-03 14:47:17,2017-01-20 22:02:03
contrib,jdef,https://github.com/kubernetes/contrib/pull/58,https://api.github.com/repos/kubernetes/contrib/issues/58,Initial add for mesosphere contributors,"Added myself, @karlkfi, and @sttts to the whitelist
",closed,True,2015-09-03 18:17:25,2015-09-05 02:18:50
contrib,eparis,https://github.com/kubernetes/contrib/pull/59,https://api.github.com/repos/kubernetes/contrib/issues/59,Include all users with commit access in the submit queue whitelist,"Continue to include those in the whitelist.txt
",closed,True,2015-09-03 22:50:04,2015-09-09 20:21:00
contrib,eparis,https://github.com/kubernetes/contrib/issues/60,https://api.github.com/repos/kubernetes/contrib/issues/60,mungegithub e2e re-starter in endless loop,"For example
https://github.com/kubernetes/kubernetes/pull/13393

My guess is the user in question is not in the e2e whitelist and thus e2e will only run when told to run. Open/close will do nothing.

Where is the e2e whitelist stored?
",closed,False,2015-09-04 15:38:46,2015-09-04 20:18:11
contrib,ixdy,https://github.com/kubernetes/contrib/pull/61,https://api.github.com/repos/kubernetes/contrib/issues/61,Skip CI check when a PR is not mergeable,"Travis and Shippable will always fail, so there's no point in trying to trigger them.

Should fix #60.

@eparis @brendandburns @lavalamp 
",closed,True,2015-09-04 20:08:31,2015-09-04 20:18:11
contrib,eparis,https://github.com/kubernetes/contrib/pull/62,https://api.github.com/repos/kubernetes/contrib/issues/62,Fix --once to actually run once instead of never running,,closed,True,2015-09-04 20:23:25,2015-09-09 20:20:59
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/63,https://api.github.com/repos/kubernetes/contrib/issues/63,update build and .yaml for 0.4,"Also add a service to expose the queue status.
",closed,True,2015-09-04 21:39:10,2015-09-04 23:32:02
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/64,https://api.github.com/repos/kubernetes/contrib/issues/64,clean up code; add more status messages,,closed,True,2015-09-04 22:06:24,2015-09-08 19:49:10
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/65,https://api.github.com/repos/kubernetes/contrib/issues/65,De-dup whitelist when committers list is generated,"@eparis This one is for you.
",closed,True,2015-09-08 20:34:57,2015-09-08 21:20:53
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/66,https://api.github.com/repos/kubernetes/contrib/issues/66,add weights to blunderbuss; add missing files to .yml,,closed,True,2015-09-08 22:19:33,2015-09-11 21:53:27
contrib,pedro-r-marques,https://github.com/kubernetes/contrib/pull/67,https://api.github.com/repos/kubernetes/contrib/issues/67,OpenContrail playbook,"This pull request adds support to provision OpenContrail via ansible. The support has been tested with Fedora 22.

The user is expected to define the following variables in group_vars/all.yml:

```
networking: opencontrail

opencontrail_public_subnet: <External subnet for LoadBalancer services>
opencontrail_private_subnet: <Internal subnet with connectivity to the kubelets>
opencontrail_gateway: <optional | gateway address on private_subnet>
opencontrail_interface: eth1
```

The provisioning assumes that the system has two interfaces: a management interface (e.g. on board 1G interface) and 1 data interface (e.g. 2x10G bond0 interface).

To configure the software gateway that provides connectivity between the external network and the private virtual networks a new type of host ""gateway"" is defined. Example inventory file:

```
[masters]
roque-kube-master

[etcd]
roque-kube-master

[nodes]
roque-node3 ipaddr=192.168.1.3/24
roque-node4 ipaddr=192.168.1.4/24

[gateways]
roque-gateway ipaddr=192.168.1.254/24
```
## Details
- OpenContrail uses kubelet to execute its userspace components. This implies running kubelet on the master, similarly to the approach taken by salt provisioning scripts which use kubelet to run the kubernetes api-server, etc.
- The OpenContrail kernel module is compiled on-demand for the target.
- OpenContrail control plane components are distributed as containers (via docker hub).
",closed,True,2015-09-09 00:38:41,2015-10-15 13:27:19
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/68,https://api.github.com/repos/kubernetes/contrib/issues/68,Add a simple web GUI,"@lavalamp @krousey 
",closed,True,2015-09-09 17:57:29,2015-09-09 20:05:15
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/69,https://api.github.com/repos/kubernetes/contrib/issues/69,Turn refresh on...,,closed,True,2015-09-09 18:38:08,2015-09-09 18:38:14
contrib,krousey,https://github.com/kubernetes/contrib/issues/70,https://api.github.com/repos/kubernetes/contrib/issues/70,submit queue UI looks terrible on wide screens,"Perhaps make it wider? Or at least center it?

![image](https://cloud.githubusercontent.com/assets/157083/9771180/38955ed4-56e9-11e5-8017-4730f9335559.png)
",closed,False,2015-09-09 18:52:45,2015-09-09 21:30:02
contrib,krousey,https://github.com/kubernetes/contrib/issues/71,https://api.github.com/repos/kubernetes/contrib/issues/71,submit queue ui should use github avatars for the whitelist,,closed,False,2015-09-09 18:53:11,2016-01-26 17:04:01
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/72,https://api.github.com/repos/kubernetes/contrib/issues/72,Fix some rendering problems.,"fixes #70 (and a different issue that @zmerlynn reported in chrome beta)
",closed,True,2015-09-09 21:28:27,2015-09-09 21:30:02
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/73,https://api.github.com/repos/kubernetes/contrib/issues/73,Auto-label long-latency PRs,"Proposals:
- New files in docs/proposals

Major API changes:
- Changes to pkg/api/register.go or pkg/expapi/register.go (note that the latter will move soon https://github.com/kubernetes/kubernetes/issues/13772)

Minor API changes:
- Changes to pkg/api/types.go or pkg/expapi/types.go

Will be useful if/when we start tracking PR latency
",closed,False,2015-09-10 00:20:22,2016-01-26 17:03:26
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/74,https://api.github.com/repos/kubernetes/contrib/issues/74,Fix a bug in the layout.,,closed,True,2015-09-10 03:06:19,2015-09-10 03:07:09
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/75,https://api.github.com/repos/kubernetes/contrib/issues/75,Rev the version of submit-queue to 0.9,,closed,True,2015-09-10 03:41:38,2015-09-16 22:37:54
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/76,https://api.github.com/repos/kubernetes/contrib/issues/76,Fix the unstable build label to match reality,,closed,True,2015-09-10 03:50:44,2015-09-10 22:34:56
contrib,eparis,https://github.com/kubernetes/contrib/pull/77,https://api.github.com/repos/kubernetes/contrib/issues/77,Remove unused fetchAllPRs() from submit-queue/github,,closed,True,2015-09-11 22:56:57,2015-09-17 00:59:38
contrib,mikedanese,https://github.com/kubernetes/contrib/issues/78,https://api.github.com/repos/kubernetes/contrib/issues/78,Submit queue ui is borken,"/api says

```
json: unsupported type: <-chan struct {}
```

@lavalamp 
",closed,False,2015-09-12 01:38:02,2015-09-17 21:50:30
contrib,eparis,https://github.com/kubernetes/contrib/pull/79,https://api.github.com/repos/kubernetes/contrib/issues/79,githubmunger: label PRs based on certain files touched,"Start with just proposals and changes to the API.
",closed,True,2015-09-12 17:27:41,2015-09-23 14:31:25
contrib,eparis,https://github.com/kubernetes/contrib/pull/80,https://api.github.com/repos/kubernetes/contrib/issues/80,Update spf13/cobra spf13/pflag,"Want to build on this...
",closed,True,2015-09-14 18:03:49,2015-09-16 20:30:14
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/81,https://api.github.com/repos/kubernetes/contrib/issues/81,Externalize the JSON state,"@lavalamp 

I believe that this will fix #78 
",closed,True,2015-09-14 20:16:23,2015-09-14 20:36:38
contrib,eparis,https://github.com/kubernetes/contrib/pull/82,https://api.github.com/repos/kubernetes/contrib/issues/82,Major rewrite of submitqueue and munger,"Huge rewrite to submit-queue and mungegithub
- Move ALL read/write from github into github/ package
- Move shared flags and functions between submit-queue and mungegithub
  into that github/ package
- Create 'config' objects to make function calling significantly easier
- Use cobra for the commands
",closed,True,2015-09-14 22:20:46,2015-09-16 20:30:15
contrib,eparis,https://github.com/kubernetes/contrib/pull/83,https://api.github.com/repos/kubernetes/contrib/issues/83,Blunderbuss: don't panic if less than 4 lines changed,"Basically the code does:

```
    log := math.Log10($NUM_LINES_CHANGED)
    logPlus := log + .5
    fileWeight := int64(logPlus)
```

So lets assume that $NUM_LINES_CHANGED == 3.

```
    log        == 0.4771212547196624
    logPlus    == 0.9771212547196624
    fileWeight == 0
```

This means that further on weightSum (and other things) will be calculated by
a multiple by fileWeight and thus will get the value of 0. Both

`100.0*float64(potentialOwners[cur])/float64(weightSum)`
  and
`selection := rand.Int63n(weightSum)`

Will cause a panic if weightSum == 0

I solve this by adding 1 to the fileWeight after it is truncated to
int64. So we can never get a value of 0.
",closed,True,2015-09-16 18:49:43,2015-09-17 00:59:38
contrib,aledbf,https://github.com/kubernetes/contrib/pull/84,https://api.github.com/repos/kubernetes/contrib/issues/84,Update service-loadbalance api with upstream,,closed,True,2015-09-16 20:00:58,2015-09-18 01:13:54
contrib,aledbf,https://github.com/kubernetes/contrib/pull/85,https://api.github.com/repos/kubernetes/contrib/issues/85,Make servicelb Docker image be based on alpine,"This change reduces the size of the image to 23MB

```
gcr.io/google_containers/servicelb       0.1   5fccf997c07f        3 weeks ago         201.8 MB
gcr.io/google_containers/servicelb       0.0   badbb8429e4e        5 seconds ago       23.23 MB
```
",closed,True,2015-09-16 20:21:03,2015-09-18 01:00:47
contrib,eparis,https://github.com/kubernetes/contrib/pull/86,https://api.github.com/repos/kubernetes/contrib/issues/86,Update the list of committers,"Also make sure the test files end with a newline.
",closed,True,2015-09-16 21:08:08,2015-09-17 00:59:38
contrib,eparis,https://github.com/kubernetes/contrib/pull/87,https://api.github.com/repos/kubernetes/contrib/issues/87,Path Based Labeler: Use a config file for the mapping,"Instead of putting it directly in the go code.
",closed,True,2015-09-16 21:40:31,2015-09-23 14:31:39
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/88,https://api.github.com/repos/kubernetes/contrib/issues/88,Build & start mungegithub 0.7,,closed,True,2015-09-16 22:20:29,2015-09-16 22:43:07
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/89,https://api.github.com/repos/kubernetes/contrib/issues/89,Fix stale info problems in munger,,closed,True,2015-09-16 23:43:35,2015-09-17 22:11:24
contrib,eparis,https://github.com/kubernetes/contrib/pull/90,https://api.github.com/repos/kubernetes/contrib/issues/90,Don't pause if we spent the whole time running,"Today we expect the queue/mungers to run every ""poll period"". But in
fact we spend a rather substantial amount of time runnin and then pause
for a poll period of time. This PR will instead only sleep if our work
took less than poll period and will try to start the loop every poll
period.
",closed,True,2015-09-17 00:41:07,2015-09-18 21:10:49
contrib,eparis,https://github.com/kubernetes/contrib/pull/91,https://api.github.com/repos/kubernetes/contrib/issues/91,mungegithub: Just use the pod.yml - not a run.sh,"mungegithub has it's own pause/loop so we should never have been hitting
the run.sh loop. Thus run.sh is just needless indirection.
",closed,True,2015-09-17 01:01:47,2015-09-23 14:30:20
contrib,aledbf,https://github.com/kubernetes/contrib/pull/92,https://api.github.com/repos/kubernetes/contrib/issues/92,Remove haproxy zombie processes left after a haproxy reload,,closed,True,2015-09-17 02:07:41,2016-02-20 23:28:51
contrib,aledbf,https://github.com/kubernetes/contrib/pull/93,https://api.github.com/repos/kubernetes/contrib/issues/93,Add syslog server to servicelb image to get haproxy logs,"Is not possible to use a file or device like `/dev/log`, but is possible to use a syslog server.
This allows the redirection of `haproxy` logs to stdout starting a local syslog server that just prints the content of the messages

```
Sep 15 17:45:57 backend-4 sh[25987]: servicelb [INFO] 172.17.144.1:58858 [15/Sep/2015:17:45:54.182] www app-demo/172.17.144.3:5000 3557/0/0/2/3559 200 139 - - ---- 2/2/0/1/0 0/0 ""HEAD /v3/health-check HTTP/1.1""
```
",closed,True,2015-09-17 03:07:50,2016-02-20 23:29:11
contrib,eparis,https://github.com/kubernetes/contrib/pull/94,https://api.github.com/repos/kubernetes/contrib/issues/94,Update godeps,"Basically did:

```
cd $contrib_dir
godep restore
cd $kubernetes_dir
godep restore
cd $contrib_dir
rm -rf Godeps
godep save ./...
```

So now contrib and kubernetes godeps are in sync.
",closed,True,2015-09-17 15:02:54,2015-09-18 21:10:49
contrib,stevesloka,https://github.com/kubernetes/contrib/issues/95,https://api.github.com/repos/kubernetes/contrib/issues/95,Ansible install,"I'm trying to run through the ansible setup and keep getting to the following spot and it just hangs: `Load the flannel config file into etcd`

I'm running Fedora Server 22 from a Fedora client. Are there any additional logs I can see or things I can dig into to see why it's hanging?
",closed,False,2015-09-17 15:09:06,2015-10-28 00:39:48
contrib,eparis,https://github.com/kubernetes/contrib/pull/96,https://api.github.com/repos/kubernetes/contrib/issues/96,submit-queue: use ListCollaborators to get all users with push access,"I didn't think ListCollaborators worked, but it appears it does work. So
get rid of the crazy logic where we get all teams, get those teams with
push access, and then get the users of those teams.  Just get the users
directly from github.

I also use more util.StringSet to simplify the code. Wish it came with
an intersection. I did it myself but an Intersection() would have been
nice...
",closed,True,2015-09-17 20:54:25,2015-09-23 14:31:43
contrib,eparis,https://github.com/kubernetes/contrib/pull/97,https://api.github.com/repos/kubernetes/contrib/issues/97,"submit-queue: assume that all users with explicit ""pull"" are whitelisted","We have teams, like kubernetes-collaborators, which are given explicit
""pull"" access to the repo. This change will cause those users to not
require the ""ok-to-merge"" label either.

`./submit-queue gencommiters` will append people with pull access to the
whitelist, but will never remove people from the whitelist

builds on #96 
",closed,True,2015-09-17 21:00:45,2015-09-23 14:30:16
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/98,https://api.github.com/repos/kubernetes/contrib/issues/98,Possibly fix JSON complaining about channels?,"Maybe fix #78?
",closed,True,2015-09-17 21:04:33,2015-09-17 21:50:30
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/99,https://api.github.com/repos/kubernetes/contrib/issues/99,Add more debugging and an API health check,"@lavalamp @eparis @mikedanese 

sort of fixes #78 (again)
",closed,True,2015-09-17 21:05:56,2015-09-17 21:55:28
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/100,https://api.github.com/repos/kubernetes/contrib/issues/100,Fix error check,"Getting this all over the logs.
",closed,True,2015-09-17 22:18:39,2015-09-17 22:21:17
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/101,https://api.github.com/repos/kubernetes/contrib/issues/101,Update build and makefiles for current versions.,,closed,True,2015-09-18 00:24:35,2015-09-22 14:22:42
contrib,eparis,https://github.com/kubernetes/contrib/pull/102,https://api.github.com/repos/kubernetes/contrib/issues/102,Count and output the number of apicalls we make each poll period,,closed,True,2015-09-19 22:47:22,2015-09-23 14:31:57
contrib,asim,https://github.com/kubernetes/contrib/issues/103,https://api.github.com/repos/kubernetes/contrib/issues/103,Service loadbalancer does not expose services in kube-system namespace,"The documentation for the service loadbalancer indicates that the kube-ui can be reached via the default configuration however the loadbalancer does not query the kube-system namespace so nothing is actually exposed. Please either change the documentation to reflect this or allow a way to query multiple namespaces e.g kube-system, default, etc.
",closed,False,2015-09-21 18:29:01,2018-02-12 08:21:07
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/104,https://api.github.com/repos/kubernetes/contrib/issues/104,Add namespace documentation to service loadbalancer,"Clarify documentation around cross namespace loadbalancing.
",closed,True,2015-09-21 19:01:42,2015-12-02 20:57:41
contrib,mvdan,https://github.com/kubernetes/contrib/pull/105,https://api.github.com/repos/kubernetes/contrib/issues/105,Add self to submit-queue whitelist,"CC @vishh @mikedanese 
",closed,True,2015-09-21 22:24:59,2015-09-21 22:44:03
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/106,https://api.github.com/repos/kubernetes/contrib/issues/106,Re-add build status that was mistakenly removed.,"@lavalamp @eparis
",closed,True,2015-09-21 23:16:15,2015-09-22 14:21:44
contrib,eparis,https://github.com/kubernetes/contrib/pull/107,https://api.github.com/repos/kubernetes/contrib/issues/107,All my other other PRs combined.,"So many of them stepped on each other. So make a monster.
",closed,True,2015-09-21 23:27:37,2015-09-23 14:29:08
contrib,pedro-r-marques,https://github.com/kubernetes/contrib/pull/108,https://api.github.com/repos/kubernetes/contrib/issues/108,Add ansible playbook support for Ubuntu 14.04.,"- Uses etcd from github release since it isn't available upstream.
- Upstream docker.io package is too old; override with apt repository from projectdocker.org.
- Use upstart for services since 14.04 doesn't yet support systemd.
- Ansible copy mode requires 0 prefix to be interpreted as octal. The issue seems to affect Ubuntu nodes but not Fedora.
  https://github.com/ansible/ansible-modules-core/issues/1798
",closed,True,2015-09-22 01:19:03,2015-10-15 13:27:56
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/109,https://api.github.com/repos/kubernetes/contrib/issues/109,Cut a new service_loadbalancer image,"Several useful fixes have gone in/are in flight/will go in soon:
- reaping zombie haproxy
- logging to syslog
- dry run mode talking to localhost kubectl proxy
- using alpine linux 
- url based routing (maybe?)
- liveness probe chk directly in haproxy
- increasing relist period to 10m
- proper flag parsing

Need to test/push a new image to google_containers
",closed,False,2015-09-22 02:21:09,2017-12-15 00:14:38
contrib,asim,https://github.com/kubernetes/contrib/issues/110,https://api.github.com/repos/kubernetes/contrib/issues/110,Service loadbalancer 404s after first request to http services,"reqrep and keep-alive do not play nicely. The first request to a http service works fine but subsequent requests 404 because the request is not parsed. Haproxy merely passes the data through. Using ""option http-server-close"" fixes this issue. 

Issue noted here a few years back http://permalink.gmane.org/gmane.comp.web.haproxy/7670
",closed,False,2015-09-22 10:50:40,2018-02-12 09:22:06
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/111,https://api.github.com/repos/kubernetes/contrib/issues/111,Fix a stupid bug in build stability logic.,"Fixes #112
I promise a unit test next, but I have a meeting...
",closed,True,2015-09-22 19:55:45,2015-09-23 14:38:30
contrib,quinton-hoole-zz,https://github.com/kubernetes/contrib/issues/112,https://api.github.com/repos/kubernetes/contrib/issues/112,checkBuilds() returns true even when builds are failing?,"https://github.com/kubernetes/contrib/blob/023e2c05df5899bf9734b0d6d21fad0b0617fb45/submit-queue/e2e.go#L93

As far as I can tell by looking at this code, this function returns true even if stable==false for some or all builds.

In theory this will cause PR's to be merged even if Jenkins e2e tests are failing.

@gmarek has reported this to be the case and @lavalamp has therefore disabled the merge bot.  

Need to verify that this theoretical bug is in fact manifesting.  The fix is pretty straightforward.

@brendandburns FYI.
",closed,False,2015-09-23 05:22:40,2015-09-23 14:23:53
contrib,quinton-hoole-zz,https://github.com/kubernetes/contrib/issues/113,https://api.github.com/repos/kubernetes/contrib/issues/113,[submit queue] only auto-merge when test jobs pass consistently,"#112 refers

We should only merge PR's if Jenkins e2e tests are stable across multiple runs.  If I'm reading the code correctly:

https://github.com/kubernetes/contrib/blob/023e2c05df5899bf9734b0d6d21fad0b0617fb45/submit-queue/e2e.go#L93

it deems a single successful test run per job to constitute a ""stable"" job.  In theory, a single fluky success could cause all pending PR's to auto-merge, which is bad.

In practise, we have multiple test jobs being checked, so if all of them become flaky, their intermittent successes would all have to line up correctly for PR's to merge (which is statistically unlikely).  But even so, I'd suggest adding a ""number of consecutive successes required"" config value per job.  Faster, more reliable test jobs (e.g. e2e-gce-parallel, at 20 min and 90%+ reliability) might require, say, 4 consecutive successes, while slower jobs (e.g. scalability, at multiple hours per run), might require fewer.
",closed,False,2015-09-23 06:30:22,2018-01-02 20:56:16
contrib,eparis,https://github.com/kubernetes/contrib/pull/114,https://api.github.com/repos/kubernetes/contrib/issues/114,Bring over a couple of the verify scripts,"gofmt
boilerplate
verify-flags
",closed,True,2015-09-23 14:20:03,2015-09-23 14:29:10
contrib,eparis,https://github.com/kubernetes/contrib/pull/115,https://api.github.com/repos/kubernetes/contrib/issues/115,Update boilerplate on submit-queue/e2e_test.go,,closed,True,2015-09-23 14:25:50,2015-09-23 14:48:53
contrib,eparis,https://github.com/kubernetes/contrib/pull/116,https://api.github.com/repos/kubernetes/contrib/issues/116,github: do not export unneeded function,,closed,True,2015-09-23 15:46:27,2015-10-09 15:47:25
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/117,https://api.github.com/repos/kubernetes/contrib/issues/117,improve test coverage.,"@eparis @lavalamp 
",closed,True,2015-09-23 17:04:00,2015-09-23 17:07:38
contrib,eparis,https://github.com/kubernetes/contrib/pull/118,https://api.github.com/repos/kubernetes/contrib/issues/118,Golint - REQUIRED,,closed,True,2015-09-23 19:11:37,2015-10-09 15:47:24
contrib,aledbf,https://github.com/kubernetes/contrib/pull/119,https://api.github.com/repos/kubernetes/contrib/issues/119,Improve haproxy connection defaults,,closed,True,2015-09-23 23:00:43,2015-09-24 18:53:40
contrib,aledbf,https://github.com/kubernetes/contrib/pull/120,https://api.github.com/repos/kubernetes/contrib/issues/120,Reload HAProxy without impacting server states,"~~Blocked until haproxy-1.6-rdev6~~

This will improve the time required to forward traffic after a reload [examples/seamless_reload.txt](http://git.haproxy.org/?p=haproxy.git;a=blob;f=examples/seamless_reload.txt;h=94df1bd6a8dbca9bf0f290d52b0c66a90b155009;hb=71503d32a74c65cf7aceb5db9ca216db2ccc453c)
",closed,True,2015-09-23 23:04:10,2015-10-19 23:53:47
contrib,pmorie,https://github.com/kubernetes/contrib/pull/121,https://api.github.com/repos/kubernetes/contrib/issues/121,WIP: fix binary src directory for local build install,"@timothysc @jlebon @eparis @mikedanese looks like something that didn't get changed over in the move from kubernetes.
",closed,True,2015-09-25 20:34:47,2015-09-25 21:54:56
contrib,eparis,https://github.com/kubernetes/contrib/pull/122,https://api.github.com/repos/kubernetes/contrib/issues/122,Complete rework of submit-queue and submit-queue web page,"`e2e` and `jenkins` now check the status of the jobs and return the results. They have nothing to do with merging PRs. This the merge logic all live in the submit-queue.

Given the submit-queue now knows why every PR is (or more likely is not) being merged we can display that information. Doing so means throwing out completely the submit-queue web page and starting anew.
",closed,True,2015-09-25 21:57:42,2015-10-09 15:47:25
contrib,ixdy,https://github.com/kubernetes/contrib/pull/123,https://api.github.com/repos/kubernetes/contrib/issues/123,"Use the more generic name ""jenkins/google"" for Jenkins commit status","The Google-run PR Jenkins is now starting to run verification checks
and unit/integration tests in addition to the e2e tests, so we should
make the name more generic.

This also sets precedent for other folks (like RedHat) who are
interested in setting up PR Jenkins bots.

@kubernetes/goog-testing @kubernetes/rh-cluster-infra @brendandburns 
",closed,True,2015-09-25 22:53:13,2015-11-18 00:50:23
contrib,aledbf,https://github.com/kubernetes/contrib/pull/124,https://api.github.com/repos/kubernetes/contrib/issues/124,Default page for traffic that reach the lb not matching any acl,,closed,True,2015-09-28 16:05:35,2015-09-30 19:48:56
contrib,aledbf,https://github.com/kubernetes/contrib/pull/125,https://api.github.com/repos/kubernetes/contrib/issues/125,Add acl to route traffic using Host header using metadata from service,"Example:

```
aledbf$ kubectl get services
NAME           LABELS                                    SELECTOR           IP(S)        PORT(S)
example-go     run=example-go                            run=example-go     10.3.0.36    8080/TCP
kubernetes     component=apiserver,provider=kubernetes   <none>             10.3.0.1     443/TCP
```

```
aledbf$ kubectl label services --overwrite --selector=""run=example-go"" lbHost=""www.k8s.io""
```

```
aledbf$ kubectl get services
NAME           LABELS                                    SELECTOR           IP(S)        PORT(S)
example-go     lbHost=www.k8s.io,run=example-go          run=example-go     10.3.0.36    8080/TCP
kubernetes     component=apiserver,provider=kubernetes   <none>             10.3.0.1     443/TCP
```
",closed,True,2015-09-28 23:17:00,2016-02-20 23:28:37
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/126,https://api.github.com/repos/kubernetes/contrib/issues/126,Push a new submit-queue.,,closed,True,2015-09-28 23:33:05,2015-09-28 23:33:16
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/127,https://api.github.com/repos/kubernetes/contrib/issues/127,"Disable auto-scaling e2e in merge queue, its being flaky","@fabioy @quinton-hoole 
",closed,True,2015-09-28 23:34:32,2015-09-29 04:07:58
contrib,fabioy,https://github.com/kubernetes/contrib/issues/128,https://api.github.com/repos/kubernetes/contrib/issues/128,Re-enable gce-autoscaling tests for merge-bit,"Because of its flakiness, the gce-autoscaling project was removed from the list of Jenkins projects that the merge-bot checks (PRs in waiting is growing). The failures needs to be investigated and the project added back to that list soon.
",closed,False,2015-09-28 23:39:01,2015-10-12 12:11:15
contrib,brendandburns,https://github.com/kubernetes/contrib/issues/129,https://api.github.com/repos/kubernetes/contrib/issues/129,submit-queue doesn't double-check 'lgtm' before merging,"We check LGTM before we start the merge testing process, but we don't double-check the LGTM.  So if it is removed in the meantime, we won't notice.
",closed,False,2015-09-30 03:18:43,2016-04-21 13:45:54
contrib,roboll,https://github.com/kubernetes/contrib/pull/130,https://api.github.com/repos/kubernetes/contrib/issues/130,add podmaster support for etcd tls,"add support for podmaster to consume tls secured etcd.

cc. @brendandburns (since you are listed as the maintainer in the dockerfile for podmaster)
",closed,True,2015-09-30 04:24:21,2015-12-09 19:58:15
contrib,aledbf,https://github.com/kubernetes/contrib/pull/131,https://api.github.com/repos/kubernetes/contrib/issues/131,Allow custom default algorithm in service-loadbalancer,"Allow to choose a different algorithm to the default `roundrobin` using labels in the service.

_Is possible to choose between:_
- roundrobin (default)
- leastconn
- first
- source

HAProxy provides other [algorithms](https://cbonte.github.io/haproxy-dconv/configuration-1.5.html#4.2-balance) but is required to use additional parameters (if someone wants support for those please open an issue or just a comment in this PR)

To change the default just set the value in the label serviceloadbalancer/lb.algorithm

```
kubectl annotate svc example-go serviceloadbalancer/lb.algorithm=""leastconn""
```

**If the specified string it does not exists in the list it will use the default.**
",closed,True,2015-09-30 20:51:03,2016-02-20 23:30:52
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/132,https://api.github.com/repos/kubernetes/contrib/issues/132,Gce L7s through Kubernetes Ingress,"Depends on https://github.com/kubernetes/kubernetes/pull/14459 
Probably just want to wait on the README for a Tl;Dr. 
",closed,True,2015-10-01 01:52:01,2016-01-25 02:30:19
contrib,liggitt,https://github.com/kubernetes/contrib/issues/133,https://api.github.com/repos/kubernetes/contrib/issues/133,Enable secure connections to etcd from podmaster,"All components that connect to etcd directly should support a secured etcd deployment. Podmaster only allows passing in the list of servers... it should also take a etcd client config file (like the API server)

https://github.com/kubernetes/contrib/blob/36816275fd53c7a2ef59650c80e2820fe3595584/pod-master/podmaster.go#L140
",closed,False,2015-10-01 03:51:02,2016-07-27 15:10:41
contrib,aledbf,https://github.com/kubernetes/contrib/pull/134,https://api.github.com/repos/kubernetes/contrib/issues/134,Show container ip and port instead of service name in the backends,"The reason for this change is show the ip address of the container running in the backend and help in case is required to know information about the container.

Instead of

```
backend example-go:8080
    option      httplog
    balance roundrobin
    server example-go:8080_0 10.2.21.11:8080
    server example-go:8080_1 10.2.21.8:8080
```

it use this

```
backend example-go:8080
    option      httplog
    balance roundrobin
    server 10.2.21.11:8080 10.2.21.11:8080
    server 10.2.21.8:8080 10.2.21.8:8080
```
",closed,True,2015-10-01 17:16:09,2016-02-20 23:29:26
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/135,https://api.github.com/repos/kubernetes/contrib/issues/135,Service lb doesn't play well with host header rules,"Building current servicelb and pushing it out to a cluster with the following services: https://gist.github.com/bprashanth/a1eaa27ef28f2c7fee0f using the rc.yaml in the servicelb directory, gives me the following errors in kubectl logs:

```
ALERT] 273/173127 (28) : parsing [/etc/haproxy/haproxy.cfg:87] : error detected while parsing switching rule : no such ACL : 'host_acl_kubernetes:443'.
[ALERT] 273/173127 (28) : parsing [/etc/haproxy/haproxy.cfg:92] : error detected while parsing switching rule : no such ACL : 'host_acl_echoheadersdefault'.
[ALERT] 273/173127 (28) : parsing [/etc/haproxy/haproxy.cfg:97] : error detected while parsing switching rule : no such ACL : 'host_acl_echoheadersx'.
```

And I can't reach http://lbip/echoheadersx, but I can reach the service through http://lbip:nodePort-of-echoheadersx. Guessing this is because of the latest host headers change. 

@aledbf
",closed,False,2015-10-01 17:42:07,2015-10-05 17:36:58
contrib,jayunit100,https://github.com/kubernetes/contrib/pull/136,https://api.github.com/repos/kubernetes/contrib/issues/136,sevice-loadbalancer: First take at a sv lb diagram,"Heres a first take at a diagram for describing how this works for users.
Also hoping it will help clarify details which i may be missing.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/136)

<!-- Reviewable:end -->
",closed,True,2015-10-01 18:24:16,2016-11-29 21:21:18
contrib,ArtfulCoder,https://github.com/kubernetes/contrib/issues/137,https://api.github.com/repos/kubernetes/contrib/issues/137,Create special test labels that add test-suites as a gatekeeper for merges,"We have the gce-e2e suite that acts as a gatekeeper for PR merges.

We should have the ability to add additional suite(s) to act as a gatekeeper for a PR merge.
This can be done by leveraging github labels.

Currently, we are in a mode where many tests are kicked out of gce-e2e suite because of slowness etc..

Having a custom label-driven suite gatekeeper, we can have certain PRs that need a specific test suite to be run, act as a gatekeep for the PR merges.

@quinton-hoole 
@brendandburns 
@ixdy 
@thockin
",closed,False,2015-10-01 20:01:46,2018-02-13 16:53:11
contrib,jayunit100,https://github.com/kubernetes/contrib/issues/138,https://api.github.com/repos/kubernetes/contrib/issues/138,[Possible bug] tcpServices binds to new map in newLoadBalancerController,"@beeps:  

In `newLoadBalacnerController` lines https://github.com/kubernetes/contrib/blob/master/service-loadbalancer/service_loadbalancer.go#L391-L395 

Seems a little off to me, i.e. line 391...

```
 lbc := loadBalancerController{ 
...
    tcpServices: map[string]int{},
}
```

In the above, shouldn't the loadBalancerController be binding tcpServices to the value of `cfg.tcpServices` ?  The code just below seems to expect that `tcpServices` actually has some data in it... 

```
strings.Split(*tcpServices, "","") {
```

I'm in the process of extracting more info from reading the code.

If I've missed something obvious, apologies in advance :).. 
",closed,False,2015-10-01 20:23:10,2015-10-06 02:25:49
contrib,aledbf,https://github.com/kubernetes/contrib/pull/139,https://api.github.com/repos/kubernetes/contrib/issues/139,Fix host header rules #135,"Fixes #135
",closed,True,2015-10-01 20:40:00,2015-10-05 17:36:58
contrib,mikedanese,https://github.com/kubernetes/contrib/issues/140,https://api.github.com/repos/kubernetes/contrib/issues/140,merge-bot not merging e2e-not-required PRs,"These are two e2e not required prs:

https://github.com/kubernetes/kubernetes/pull/14761
https://github.com/kubernetes/kubernetes/pull/14494

Merge bot comments (multiple times):

> Automatic merge from submit-queue

But the PRs are not merged.

cc @brendandburns @lavalamp @eparis 
",closed,False,2015-10-02 17:57:12,2018-08-16 21:06:29
contrib,eparis,https://github.com/kubernetes/contrib/pull/141,https://api.github.com/repos/kubernetes/contrib/issues/141,Recheck mergeability if github error about branch changed on merge,"We found in the logs:
 Base branch was modified. Review and try the merge again.
On some PRs that were not merged. Although the github API does not
describe this error we are assuming it is because github has not
calculated the mergeability of the PR in question. Thus the workaround
is to check mergeability and then try to merge again.
",closed,True,2015-10-02 19:46:07,2015-10-09 15:47:25
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/142,https://api.github.com/repos/kubernetes/contrib/issues/142,Update godep,"@eparis @lavalamp can one of you sanity check? Updating to grab: https://github.com/kubernetes/kubernetes/pull/14459

I essentially followed https://github.com/kubernetes/kubernetes/blob/92f21b3fe3399ae6e1c29979649ba7e64d6d096e/docs/devel/development.md#using-godep to checkout a clean kubernetes, godep restore to populate that temp GOPATH, cloned contrib, rm -rf Godeps, and godep save ./... in contrib to get the kubernetes godep I just restored.

Second commit changes go build to godep go build. 
",closed,True,2015-10-02 21:44:30,2016-01-25 02:30:17
contrib,glerchundi,https://github.com/kubernetes/contrib/pull/143,https://api.github.com/repos/kubernetes/contrib/issues/143,Add support for etcd config file,"Mimics kube-apiserver behaviour and allows to access a TLS secured etcd too.

Signed-off-by: Gorka Lerchundi Osa glertxundi@gmail.com
",closed,True,2015-10-05 11:54:16,2015-10-05 13:29:18
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/144,https://api.github.com/repos/kubernetes/contrib/issues/144,Dynamic weights of backends,"Create a sidecar container that runs in the same pod as your backend and reports it's qps to a service annotation. The servicelb will lookup these service annotations and update the weights of the backends by writing to the haproxy socket (or  https://github.com/kubernetes/contrib/pull/120). The goal here is to send less traffic to backends that have historically been slow to repond (so not exactly leastconns). I expect this to help with noisy neighbors. 

@aledbf @jayunit100 
",closed,False,2015-10-05 18:01:24,2018-02-12 10:23:08
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/145,https://api.github.com/repos/kubernetes/contrib/issues/145,Add a simple tool for calculating a CSV of perf. data from Jenkins.,"@wojtek-t 
",closed,True,2015-10-06 03:57:14,2015-10-08 18:58:25
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/146,https://api.github.com/repos/kubernetes/contrib/issues/146,Mergebot is drunk on issue pages,"From the logs right now.

```
I1006 20:33:38.846890       1 github.go:668] Fetching page 16510 of issues
I1006 20:33:40.737390       1 github.go:668] Fetching page 16511 of issues
I1006 20:33:42.847761       1 github.go:668] Fetching page 16512 of issues
I1006 20:33:44.840999       1 github.go:668] Fetching page 16513 of issues
I1006 20:33:46.855748       1 github.go:668] Fetching page 16514 of issues
I1006 20:33:48.763123       1 github.go:668] Fetching page 16515 of issues
I1006 20:33:50.816153       1 github.go:668] Fetching page 16516 of issues
I1006 20:33:52.949485       1 github.go:668] Fetching page 16517 of issues
I1006 20:33:54.800915       1 github.go:668] Fetching page 16518 of issues
I1006 20:33:56.796266       1 github.go:668] Fetching page 16519 of issues
I1006 20:33:58.795852       1 github.go:668] Fetching page 16520 of issues
I1006 20:34:00.960203       1 github.go:668] Fetching page 16521 of issues
I1006 20:34:02.973533       1 github.go:668] Fetching page 16522 of issues
I1006 20:34:05.166156       1 github.go:668] Fetching page 16523 of issues
I1006 20:34:06.867732       1 github.go:668] Fetching page 16524 of issues
I1006 20:34:08.911436       1 github.go:668] Fetching page 16525 of issues
I1006 20:34:10.828744       1 github.go:668] Fetching page 16526 of issues
I1006 20:34:12.900781       1 github.go:668] Fetching page 16527 of issues
I1006 20:34:14.876124       1 github.go:668] Fetching page 16528 of issues
I1006 20:34:16.764546       1 github.go:668] Fetching page 16529 of issues
I1006 20:34:18.790262       1 github.go:668] Fetching page 16530 of issues
I1006 20:34:20.993383       1 github.go:668] Fetching page 16531 of issues
I1006 20:34:22.817142       1 github.go:668] Fetching page 16532 of issues
I1006 20:34:24.834514       1 github.go:668] Fetching page 16533 of issues
I1006 20:34:26.800389       1 github.go:668] Fetching page 16534 of issues
I1006 20:34:28.815217       1 github.go:668] Fetching page 16535 of issues

```
",closed,False,2015-10-06 20:36:16,2015-10-06 20:57:25
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/147,https://api.github.com/repos/kubernetes/contrib/issues/147,Change a bunch of == to <= to fix infinite paging.,"Fixes #146 

@lavalamp 
",closed,True,2015-10-06 20:54:37,2015-10-06 21:00:35
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/148,https://api.github.com/repos/kubernetes/contrib/issues/148,ignore errors for nodes without a running firewalld,"If firewalld is installed on a fedora or centos system, the ansible scripts attempt to configure it, whether it's running or not. Most of the tasks for configuring firewalld are set to ignore errors, which allows the provisioning to continue on hosts that have firewalld installed, but where firewalld is not running. This PR adds that same `ignore_errors: yes` to the node firewalld tasks.

cc @eparis 
",closed,True,2015-10-08 17:30:16,2016-05-10 11:29:16
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/149,https://api.github.com/repos/kubernetes/contrib/issues/149,Add a simple HTTP server,"@quinton-hoole @wojtek-t 
",closed,True,2015-10-08 19:47:08,2015-10-09 21:49:52
contrib,eparis,https://github.com/kubernetes/contrib/pull/150,https://api.github.com/repos/kubernetes/contrib/issues/150,Automatically update the pod with Makefile tag,,closed,True,2015-10-08 20:49:20,2015-10-09 18:36:33
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/151,https://api.github.com/repos/kubernetes/contrib/issues/151,removes hostname that vagrant adds to the localhost line of /etc/hosts,"When bringing up a cluster using ansible and vagrant, the Vagrantfile sets the hostnames of the VMs to kube-master, kube-node-1, etc. As part of this hostname-setting, vagrant adds the hostname for each VM to the localhost line in `/etc/hosts`.

When the ansible script for etcd runs, it sets `ETCD_LISTEN_CLIENT_URLS` to `http://kube-master:2379`, which, when kube-master is on the localhost line of `/etc/hosts`, acts like it's setting `ETCD_LISTEN_CLIENT_URLS` to localhost, and so the nodes can't talk to etcd.

The PR takes the `vagrant-ansible.yml` file that adds an entry for each of the vagrant VMs in the `/etc/hosts` of each VM, and extends it to remove the hostname from the localhost line.

It may be that I'm not seeing the importance of this vagrant behavior, but ansible/vagrant with the Vagrantfile as written doesn't work without making this change.
",closed,True,2015-10-08 22:30:42,2015-10-14 17:00:29
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/152,https://api.github.com/repos/kubernetes/contrib/issues/152,Use the same test in perf dashboard,,closed,True,2015-10-09 11:29:13,2015-10-09 19:46:44
contrib,wojtek-t,https://github.com/kubernetes/contrib/issues/153,https://api.github.com/repos/kubernetes/contrib/issues/153,Merge bot is merging even when Critial builds are yellow,"As an example:

PR https://github.com/kubernetes/kubernetes/pull/15014
was merged at 00:51 PST

at the same time ""kubernetes-e2e-gce"" was yellow

cc @lavalamp @eparis @brendandburns 
",closed,False,2015-10-09 12:05:04,2015-10-09 18:16:42
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/154,https://api.github.com/repos/kubernetes/contrib/issues/154,config,"This is the config I used to push #122.
",closed,True,2015-10-09 14:54:30,2015-10-09 15:50:26
contrib,eparis,https://github.com/kubernetes/contrib/pull/155,https://api.github.com/repos/kubernetes/contrib/issues/155,Recheck long running e2e tests are stable before merge,"Since we rerun the github e2e tests (the fast ones) async, the time
between the check for stability of the slow google internal e2e tests
and when we are ready to merge could be quite large. Recheck the state
of the slow e2e tests before we merge.
",closed,True,2015-10-09 17:10:44,2015-10-09 17:31:35
contrib,roberthbailey,https://github.com/kubernetes/contrib/issues/156,https://api.github.com/repos/kubernetes/contrib/issues/156,Run separate merge queues for separate branches,"In particular, each release branch has it's own e2e test validating the release branch. Merges to the release branch shouldn't be blocked if e2e tests at HEAD are failing, and merges to HEAD shouldn't be blocked if tests against the release branch are failing. 

We currently have 2 release branches (release-1.0 and release-1.1) that should have separate merge-bots configured. 

/cc @lavalamp @eparis 
",closed,False,2015-10-09 18:24:12,2016-04-04 21:31:31
contrib,eparis,https://github.com/kubernetes/contrib/pull/157,https://api.github.com/repos/kubernetes/contrib/issues/157,Flush the async github e2e retester on slow e2e failure,"As soon as we notice that the slow e2e tests are failing, flush the
queue of PRs waiting to rerun the fast e2e tests.
",closed,True,2015-10-09 18:33:13,2015-10-13 18:31:58
contrib,eparis,https://github.com/kubernetes/contrib/pull/158,https://api.github.com/repos/kubernetes/contrib/issues/158,Helper program to read the blunderbuss config and alphabetize,,closed,True,2015-10-09 18:36:12,2015-10-09 18:36:35
contrib,aledbf,https://github.com/kubernetes/contrib/pull/159,https://api.github.com/repos/kubernetes/contrib/issues/159,Group all backends for a single service,"This improves the readability of the configuration file:

```
    acl url_acl_example-go:8080 path_beg /example-go:8080
    use_backend example-go:8080 if url_acl_example-go:8080

    acl url_acl_example-go-2:8080 path_beg /example-go-2:8080
    acl host_acl_example-go-2:8080 hdr(host) www.google.cl
    use_backend example-go-2:8080 if url_acl_example-go-2:8080 or host_acl_example-go-2:8080
```
",closed,True,2015-10-09 22:03:29,2016-02-20 23:29:40
contrib,gouyang,https://github.com/kubernetes/contrib/pull/160,https://api.github.com/repos/kubernetes/contrib/issues/160,Ansible: Add quotation marks to docker options,"Notice that docker-logrotate is sourcing /etc/sysconfig/docker, it
throws an error without quotation marks.

Signed-off-by: Guohua Ouyang gouyang@redhat.com
",closed,True,2015-10-12 03:22:25,2015-10-14 13:57:05
contrib,gouyang,https://github.com/kubernetes/contrib/pull/161,https://api.github.com/repos/kubernetes/contrib/issues/161,Ansible: Add http://127.0.0.1:2379 to etcd listen-client-urls,"So commands like 'etcdctl ls' can work without specify peers.

Signed-off-by: Guohua Ouyang gouyang@redhat.com
",closed,True,2015-10-12 04:36:22,2015-10-15 13:26:12
contrib,aledbf,https://github.com/kubernetes/contrib/pull/162,https://api.github.com/repos/kubernetes/contrib/issues/162,Add support for session affinity using service.spec.sessionAffinity or cookies,"Use the attribute `sessionAffinity` from the service spec to enable the use a stickiness table in haproxy
http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#stick-table

Example: 
- stick-table
  `sessionAffinity=ClientIP`
- cookies
  `sessionAffinity=ClientIP` and annotation `serviceloadbalancer/lb.cookie-sticky-session=true`
",closed,True,2015-10-12 17:45:41,2015-11-20 15:22:58
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/163,https://api.github.com/repos/kubernetes/contrib/issues/163,"munger: add ""what is it doing right now"" to dashboard","I'm getting a lot of questions about what the merge bot is doing. I think if we could make that inner goroutine publish a status, that would help a lot.

@eparis any time to make this happen? :)
",closed,False,2015-10-12 18:22:43,2016-01-26 17:59:43
contrib,eparis,https://github.com/kubernetes/contrib/pull/164,https://api.github.com/repos/kubernetes/contrib/issues/164,Bit more information about where PRs are related to E2E,,closed,True,2015-10-12 23:16:13,2015-10-13 00:01:41
contrib,eparis,https://github.com/kubernetes/contrib/pull/165,https://api.github.com/repos/kubernetes/contrib/issues/165,Start to collect and export more information about the e2e queue,"From a user PoV it should have no changes, but it should put more data in the API which can used to develop the web interface.
",closed,True,2015-10-13 00:31:16,2015-10-13 03:30:40
contrib,dalanlan,https://github.com/kubernetes/contrib/pull/166,https://api.github.com/repos/kubernetes/contrib/issues/166,self add to whitelist,"Thanks:)
/cc @mikedanese 
",closed,True,2015-10-13 01:33:31,2015-10-13 02:12:01
contrib,wojtek-t,https://github.com/kubernetes/contrib/issues/167,https://api.github.com/repos/kubernetes/contrib/issues/167,merge-bot is trying to merge already merged PR :),"https://github.com/kubernetes/kubernetes/pull/15452#issuecomment-147714499

That PR was merge and 2 hours later merge-bot decided to retest it again.

cc @eparis @lavalamp 
",closed,False,2015-10-13 13:41:32,2015-10-13 20:13:21
contrib,eparis,https://github.com/kubernetes/contrib/pull/168,https://api.github.com/repos/kubernetes/contrib/issues/168,Display github e2e queue in submit queue web page,,closed,True,2015-10-13 16:31:29,2015-10-13 18:31:56
contrib,eparis,https://github.com/kubernetes/contrib/pull/169,https://api.github.com/repos/kubernetes/contrib/issues/169,Timeout and Queue removal for Github E2E,,closed,True,2015-10-13 18:16:13,2015-10-13 21:15:14
contrib,eparis,https://github.com/kubernetes/contrib/pull/170,https://api.github.com/repos/kubernetes/contrib/issues/170,Refresh submit queue web page every 30 seconds,,closed,True,2015-10-13 21:41:22,2015-10-13 23:30:47
contrib,eparis,https://github.com/kubernetes/contrib/pull/171,https://api.github.com/repos/kubernetes/contrib/issues/171,Recheck merged and mergeability before rerunning e2e,"Because humans merge things by hand and the queue can then get hung
",closed,True,2015-10-14 00:18:07,2015-10-14 00:55:39
contrib,eparis,https://github.com/kubernetes/contrib/pull/172,https://api.github.com/repos/kubernetes/contrib/issues/172,Test the code which checks 'merged' inside the e2e queue,,closed,True,2015-10-14 00:53:36,2015-10-14 00:55:39
contrib,eparis,https://github.com/kubernetes/contrib/pull/173,https://api.github.com/repos/kubernetes/contrib/issues/173,"Move from a ""github config"" to a ""munge object""","- Label removals in one munger will be 'seen' in the next
- Locally cache github results and use conditional gets
- Significant improvements to submit-queue web interface (won't crash your browser after a while)
- More debug and visibility information for users concerning what the bot is up to.
",closed,True,2015-10-14 01:06:14,2015-10-27 23:43:00
contrib,gouyang,https://github.com/kubernetes/contrib/pull/174,https://api.github.com/repos/kubernetes/contrib/issues/174,Ansible: fix typo,"Signed-off-by: Guohua Ouyang gouyang@redhat.com
",closed,True,2015-10-14 01:47:53,2015-10-20 15:23:26
contrib,davidopp,https://github.com/kubernetes/contrib/pull/175,https://api.github.com/repos/kubernetes/contrib/issues/175,Add zmerlynn to review rotation for examples/,"@mikedanese @zmerlynn 
",closed,True,2015-10-14 06:37:26,2015-10-15 13:25:48
contrib,eparis,https://github.com/kubernetes/contrib/pull/176,https://api.github.com/repos/kubernetes/contrib/issues/176,Update Google Internal E2E Status More Often,"Currently, the submit queue web page only updates the google internal e2e
when it is processing PRs. This means if no PRs gets to that point in the
submit queue logic we will never update the web page. But even if there
are PRs which get to that point, the loop is every 30 minutes so its a
long delay.

Also, the web interface showed 'red' if it failed but never would go
back green if the tests started passing. So fix the javascript there.
",closed,True,2015-10-14 15:01:23,2015-10-15 15:35:42
contrib,eparis,https://github.com/kubernetes/contrib/pull/177,https://api.github.com/repos/kubernetes/contrib/issues/177,Clear the submit queue any time we hit a non-stable e2e,"We were clearning the submit queue when we found google internal e2e was
unstable during the 30 minute loop, but not when we found e2e failed
after a github build. Flush e2e any time we found an unstable build.
",closed,True,2015-10-14 15:27:33,2015-10-15 13:14:05
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/178,https://api.github.com/repos/kubernetes/contrib/issues/178,Upgrade service loadbalancer to haproxy 1.6,"Tracking bug to test and bump haproxy in service loadbalancer to 1.6, now that it's official (related https://github.com/kubernetes/contrib/pull/120, http://marc.info/?l=haproxy&m=144476286029747)
",closed,False,2015-10-14 15:55:03,2016-01-08 23:50:11
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/179,https://api.github.com/repos/kubernetes/contrib/issues/179,"for ansible/vagrant/pkgManager install w/ CentOS 7, get pkgs from extras repo","Currently, when running the ansible/vagrant scripts to bring up a cluster based on packages from the CentOS 7 repositories, we're installing kubernetes and docker from the [CentOS Build System](https://wiki.centos.org/HowTos/CommunityBuildSystem), rather than from the centos extras repository (which is enabled by default in CentOS). This PR would change that, pulling the packages from the extras repo.

The docker and kubernetes packages from the extras repo are updated about every six weeks, following updates to RHEL Atomic Host, so they're reasonably up-to-date. The CBS packages tend to be fairly bleeding-edge, with changes that might not match user expectations of CentOS. For instance, the current kubenetes-client package provided by the CBS repo includes a `kubectl` that's symlinked to the `openshift` executable.

There may be a reason why the packages from extras weren't acceptable in the past -- this repo didn't include kube 1.0 until August, for instance, but I think extras is the right source for our packages at this point. 

@eparis, does this sound reasonable to you?
",closed,True,2015-10-14 21:05:25,2015-10-17 22:29:26
contrib,eparis,https://github.com/kubernetes/contrib/pull/180,https://api.github.com/repos/kubernetes/contrib/issues/180,SubmitQueue: Web Page More Mobile Friendly,"We did some static offsets and uses plain html elements in a number of
places. These looked great on a big monitor but didn't scale well to a
mobile device.  Using all material design elements looks good on an
iPhone and has little visual impack on big displays.
",closed,True,2015-10-14 21:59:07,2015-10-15 15:35:42
contrib,eparis,https://github.com/kubernetes/contrib/pull/181,https://api.github.com/repos/kubernetes/contrib/issues/181,Add zmerlynn to review rotation for examples,,closed,True,2015-10-15 13:23:23,2015-10-15 15:35:43
contrib,azimut,https://github.com/kubernetes/contrib/issues/182,https://api.github.com/repos/kubernetes/contrib/issues/182,Ansible: Add kubedash addon support,"This is a feature request, feel free to close it if you don't think is needed or desired to have.

```
https://github.com/kubernetes/kubedash/blob/master/deploy/kube-config.yaml
```
",closed,False,2015-10-15 16:41:11,2015-12-18 12:42:16
contrib,cgwalters,https://github.com/kubernetes/contrib/pull/183,https://api.github.com/repos/kubernetes/contrib/issues/183,Ansible fixes,"See commits for detail.  /cc @pedro-r-marques for regressions.
",closed,True,2015-10-16 21:18:00,2015-10-20 17:01:39
contrib,pedro-r-marques,https://github.com/kubernetes/contrib/pull/184,https://api.github.com/repos/kubernetes/contrib/issues/184,Introduce setting as to whether to run the kubelet on a master.,"Kubelet should be an optional setting.
",closed,True,2015-10-19 16:45:43,2015-10-20 15:45:09
contrib,pedro-r-marques,https://github.com/kubernetes/contrib/pull/185,https://api.github.com/repos/kubernetes/contrib/issues/185,Add the always_run flag to commands that 'register' status.,"It is useful to be able to run the playbook with the option ""--check"" in order to validate different scenarios without making changes to the running system. That requires the pre-ansible and common tasks to succeed when ""--check"" is enabled. ""always_run"" on tasks that produce status is required for this to work.
",closed,True,2015-10-19 17:07:57,2015-10-19 18:24:16
contrib,eparis,https://github.com/kubernetes/contrib/pull/186,https://api.github.com/repos/kubernetes/contrib/issues/186,Update kubernetes godep,"Which in turn pulls in others

and removes all of _test.go
",closed,True,2015-10-19 21:25:25,2015-10-31 17:45:24
contrib,eparis,https://github.com/kubernetes/contrib/pull/187,https://api.github.com/repos/kubernetes/contrib/issues/187,Basic instructions for updating godeps in contrib,,closed,True,2015-10-20 16:03:39,2015-10-31 17:45:15
contrib,cgwalters,https://github.com/kubernetes/contrib/pull/188,https://api.github.com/repos/kubernetes/contrib/issues/188,ansible: Unify pkg installs for master/node,"To support static pods, also install kubernetes-node on the master.
That makes the package code identical between `master/` and `node/`
roles, so deduplicate into the dependent `kubernetes/` role.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/188)

<!-- Reviewable:end -->
",closed,True,2015-10-20 16:54:22,2016-09-09 13:58:20
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/189,https://api.github.com/repos/kubernetes/contrib/issues/189,Fix bug in queuing logic of serviceloadbalancer,"We should be marking all keys done when we're finished processing them, even the ones we requeue. Otherwise we might leave a zombie key in the processing queue that halts progress. 
",closed,True,2015-10-20 17:35:51,2016-01-25 02:30:20
contrib,rawlingsj,https://github.com/kubernetes/contrib/issues/190,https://api.github.com/repos/kubernetes/contrib/issues/190,[vagrant openstack] 'run_provisioner': Catched Error: Catched Error: uninitialized constant ,"Vagrant Installed Version: 1.7.4

Running ansible vagrant with the openstack provider I hit the error below after doing vagrant up.  A quick google suggested a workaround but not sure if thats acceptable.  I'll submit a pr just in case it is.

```
/Users/jamesrawlings/.vagrant.d/gems/gems/vagrant-openstack-provider-0.7.0/lib/vagrant-openstack-provider/action/provision.rb:34:in `run_provisioner': Catched Error: Catched Error: uninitialized constant VagrantPlugins::Shell::Provisioner (NameError)
    from /opt/vagrant/embedded/gems/gems/vagrant-1.7.4/lib/vagrant/action/warden.rb:95:in `call'
    from /opt/vagrant/embedded/gems/gems/vagrant-1.7.4/lib/vagrant/action/warden.rb:95:in `block in finalize_action'    
```

Workaround came from..
https://github.com/ggiamarchi/vagrant-openstack-provider/issues/240#issuecomment-144511022
",closed,False,2015-10-20 17:40:19,2016-03-20 21:03:06
contrib,rawlingsj,https://github.com/kubernetes/contrib/pull/191,https://api.github.com/repos/kubernetes/contrib/issues/191,add provision script inline settings to fix #190,,closed,True,2015-10-20 17:43:14,2016-03-20 21:02:52
contrib,rawlingsj,https://github.com/kubernetes/contrib/issues/192,https://api.github.com/repos/kubernetes/contrib/issues/192,[vagrant openstack] error inventory_hostname in groups['nodes'] or inventory_hostname in groups['gateways'],"I'm trying out the [openstack vagrant steps](https://github.com/kubernetes/contrib/tree/master/ansible/vagrant) which seem to be working with only two small changes my side which I'll raise pr's for but then fail with the error below.

```
TASK: [opencontrail | Interface configuration file (physical)] ****************
fatal: [kube-node-2] => One or more undefined variables: 'opencontrail_interface' is undefined
fatal: [kube-node-1] => One or more undefined variables: 'opencontrail_interface' is undefined

FATAL: all hosts have already failed -- aborting
```

but as a total guess I uncommented https://github.com/kubernetes/contrib/blob/master/ansible/group_vars/all.yml#L50 and set it to `eth0` which was an available interface - no idea if this was right at all but that error seemed to go away the next time I ran the vagrant up.

So now I'm left with the errors below which I guess are related and wondering if anyone has any suggestions 

```
TASK: [opencontrail | build tag] **********************************************
fatal: [kube-master] => error while evaluating conditional: inventory_hostname in groups['nodes'] or inventory_hostname in groups['gateways']
ok: [kube-node-1]
ok: [kube-node-2]

TASK: [opencontrail | Unpack vrouter tarball] *********************************
changed: [kube-node-1]
changed: [kube-node-2]

TASK: [opencontrail | Depmod] *************************************************
changed: [kube-node-1]
changed: [kube-node-2]

TASK: [opencontrail | Reduce memory utilization of vrouter] *******************
skipping: [kube-node-1]
skipping: [kube-node-2]

TASK: [opencontrail | Interface up/down scripts] ******************************
changed: [kube-node-2] => (item=ifup-vhost)
changed: [kube-node-1] => (item=ifup-vhost)
changed: [kube-node-2] => (item=ifdown-vhost)
changed: [kube-node-1] => (item=ifdown-vhost)

TASK: [opencontrail | Interface configuration file (physical)] ****************
changed: [kube-node-2]
changed: [kube-node-1]

TASK: [opencontrail | Interface configuration file (vhost0)] ******************
fatal: [kube-node-2] => {'msg': ""AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'ipaddr'"", 'failed': True}
fatal: [kube-node-1] => {'msg': ""AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'ipaddr'"", 'failed': True}
fatal: [kube-node-2] => {'msg': ""AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'ipaddr'"", 'failed': True}
fatal: [kube-node-1] => {'msg': ""AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'ipaddr'"", 'failed': True}

FATAL: all hosts have already failed -- aborting

PLAY RECAP ********************************************************************
           to retry, use: --limit @/Users/jamesrawlings/cluster.retry

kube-master                : ok=92   changed=40   unreachable=1    failed=0
kube-node-1                : ok=63   changed=25   unreachable=1    failed=0
kube-node-2                : ok=63   changed=25   unreachable=1    failed=0

An unknown error happened in Vagrant OpenStack provider

To easily debug what happened, we recommend to set the environment
variable VAGRANT_OPENSTACK_LOG to debug

    $ export VAGRANT_OPENSTACK_LOG=debug

If doing this does not help fixing your issue, there may be a bug
in the provider. Please submit an issue on Github at
https://github.com/ggiamarchi/vagrant-openstack-provider
with the stracktrace and the logs.

We are looking for feedback, so feel free to ask questions or
describe features you would like to see in this provider.
Catched Error: Catched Error: Ansible failed to complete successfully. Any error output should be
visible above. Please fix these errors and try again.
```
",closed,False,2015-10-20 17:45:50,2018-02-14 03:03:09
contrib,rawlingsj,https://github.com/kubernetes/contrib/issues/193,https://api.github.com/repos/kubernetes/contrib/issues/193,[vagrant openstack] docker not running before task opencontrail | Build docker container,"During the vagrant up I get an error as docker isn't running in the image during the **opencontrail | Build docker container** task, I've worked around it by adding `service: name=docker state=started` to https://github.com/kubernetes/contrib/blob/master/ansible/roles/docker/tasks/main.yml#L49 but I'm pretty sure my lack of understanding here means that's not the right answer.

Using fedora 22 cloud base image from https://download.fedoraproject.org/pub/fedora/linux/releases/22/Cloud/x86_64/Images/Fedora-Cloud-Base-22-20150521.x86_64.qcow2

```
TASK: [opencontrail | Build docker container] *********************************
skipping: [kube-node-1]
skipping: [kube-node-2]
failed: [kube-master] => {""changed"": true, ""cmd"": [""docker"", ""build"", ""-t"", ""opencontrail/kmod_fedora22-4.0.4-301.fc22.x86_64"", ""fedora22-4.0.4-301.fc22.x86_64""], ""delta"": ""0:00:00.023080"", ""end"": ""2015-10-20 18:01:26.362457"", ""rc"": 1, ""start"": ""2015-10-20 18:01:26.339377"", ""warnings"": []}
stderr: Post http:///var/run/docker.sock/v1.20/build?cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&memory=0&memswap=0&rm=1&t=opencontrail%2Fkmod_fedora22-4.0.4-301.fc22.x86_64&ulimits=null: dial unix /var/run/docker.sock: no such file or directory.
* Are you trying to connect to a TLS-enabled daemon without TLS?
* Is your docker daemon up and running?
```
",closed,False,2015-10-20 18:14:16,2018-02-14 03:03:09
contrib,aledbf,https://github.com/kubernetes/contrib/pull/194,https://api.github.com/repos/kubernetes/contrib/issues/194,Fix service-loadbalancer Makefile to be docker-1.7 compatible,,closed,True,2015-10-20 18:28:22,2015-10-28 18:44:21
contrib,timothysc,https://github.com/kubernetes/contrib/issues/195,https://api.github.com/repos/kubernetes/contrib/issues/195,ansible: package installation on RHEL ,"under roles/nodes/tasks/packageManagerInstall it doesn't setup the Extras repo or install for RHEL7. 

/cc @rrati 
",closed,False,2015-10-20 19:31:27,2016-06-08 20:31:08
contrib,timothysc,https://github.com/kubernetes/contrib/issues/196,https://api.github.com/repos/kubernetes/contrib/issues/196,ansible: slurp from master,"Under https://github.com/kubernetes/contrib/blob/master/ansible/roles/node/tasks/main.yml#L26 when you need to setup such that the client nodes can ssh back into the master node.  Not exactly clear from the docs. 

/cc @rrati 
",closed,False,2015-10-20 19:36:36,2015-10-20 19:47:35
contrib,ixdy,https://github.com/kubernetes/contrib/issues/197,https://api.github.com/repos/kubernetes/contrib/issues/197,submit queue should ensure CI results are fresh,"The submit queue automatically re-runs Jenkins before merging, but it's possible for the Shippable and Travis results to be very stale. Because Jenkins doesn't run all of the same verifications and checks currently, it's possible for things to break. 

Motivating example: https://github.com/kubernetes/kubernetes/pull/15608 was auto-merged, but it immediately broke the build, since some new docs had been added since its last CI run.

The GitHub API indicates when commit statuses were updated (see https://github.com/google/go-github/blob/master/github/repos_statuses.go#L34); it should hopefully be fairly straightforward to require these to be no more than N days old or retrigger CI if they are stale.

@brendandburns @eparis 
",closed,False,2015-10-21 00:35:05,2015-11-25 19:02:08
contrib,jayunit100,https://github.com/kubernetes/contrib/issues/198,https://api.github.com/repos/kubernetes/contrib/issues/198,Update ansible directions (1) for mac people (2) for openstack urls,"I had to make some changes to run this from my mac with a local repo build.

**first, we may want to show how to push local binaries - its tricky to get the right path, b/c you need to use the dockerized build and specify amd64 iirc**

You need to use `run.sh` to build dockerized linux binaries.  Then, you have to do something like this...

```
  3 --- a/ansible/roles/master/defaults/main.yml
  4 +++ b/ansible/roles/master/defaults/main.yml
  5 @@ -1,3 +1,3 @@
  6  kube_master_insecure_port: 8080
  7 
  8 -localBuildOutput: ../../_output/local
  9 \ No newline at end of file
 10 +localBuildOutput: ~/Development/kubernetes/_output/dockerized/bin/linux
```

and

```
 13 --- a/ansible/roles/master/tasks/localBuildInstall.yml
 14 +++ b/ansible/roles/master/tasks/localBuildInstall.yml
 16  ---
 17  - name: Copy master binaries
 18    copy:
 19 -    src: ""{{ localBuildOutput }}/go/bin/{{ item }}""
 20 +    src: ""{{ localBuildOutput }}/amd64/{{ item }}""
 21      dest: /usr/bin/
 22      mode: 0755
 23    with_items:
```

**also, i think we might want to update vagrant example file for openstack**

and finally in my main.yml, this is what it looks like.  Mainly, the os_auth_url was different... So we should update the example vagrant file 

```
 24 diff --git a/ansible/roles/node/defaults/main.yml b/ansible/roles/node/defaults/main.yml
 29 -localBuildOutput: ../../_output/local
 30 +localBuildOutput: ~/Development/kubernetes/_output/dockerized/bin/linux
 31 os_username: jvyas
 32 os_password: 123123123123
 33 os_tenant: ""ENG CTO Office""
 34 os_auth_url: ""http://os1-public.osop.rhcloud.com:5000/v2.0/tokens""
 35 os_region_name: ""OS1Public""
 36 os_ssh_key_name: ""JPeerindex""
 37 os_flavor: ""m1.small""
 38 os_image: ""_OS1_Fedora-Cloud-Base-22-20150521.x86_64.qcow2""
 39 os_security_groups:
 40   - ""default""
 41 os_floating_ip_pool: ""os1_public""
```
",closed,False,2015-10-22 15:45:16,2018-02-14 02:02:12
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/199,https://api.github.com/repos/kubernetes/contrib/issues/199,Fix the release-notes tool,"This change updates the release notes tool to:
- correctly ignore PRs that don't have the `release-notes` label;
- correctly include PRs that were merged after the last PR, but before the last PR was last updated;
- allow release-notes to be filtered by branch; and
- properly format usernames, e.g. @ihmccreery.

Fixes kubernetes/kubernetes#13343, although we still need to reconcile it with the rest of the release infra, (kubernetes/kubernetes#15483 and kubernetes/kubernetes#13339).
",closed,True,2015-10-23 00:21:36,2015-10-28 17:12:30
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/200,https://api.github.com/repos/kubernetes/contrib/issues/200,Simple 404 server,"A dumb 404 server for use as a loadbalancer controllers default backend.
",closed,True,2015-10-24 01:17:54,2015-10-26 03:38:18
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/201,https://api.github.com/repos/kubernetes/contrib/issues/201,Glbc fixes round 2,"~~Still a work in progress.~~
",closed,True,2015-10-24 02:49:11,2015-10-31 00:08:17
contrib,janetkuo,https://github.com/kubernetes/contrib/pull/202,https://api.github.com/repos/kubernetes/contrib/issues/202,Update kubernetes godep,"cc @bprashanth 
",closed,True,2015-10-26 18:44:08,2015-10-26 21:34:47
contrib,aledbf,https://github.com/kubernetes/contrib/issues/203,https://api.github.com/repos/kubernetes/contrib/issues/203,Service loadbalancer should support multiple namespaces,,closed,False,2015-10-26 20:06:30,2016-08-05 02:49:11
contrib,aledbf,https://github.com/kubernetes/contrib/issues/204,https://api.github.com/repos/kubernetes/contrib/issues/204,Service loadbalancer does not have SSL support,"Currently is not possible to expose a service using HTTPS with a custom SSL certificate.
",closed,False,2015-10-26 20:10:12,2016-08-05 02:48:54
contrib,aledbf,https://github.com/kubernetes/contrib/issues/205,https://api.github.com/repos/kubernetes/contrib/issues/205,Expose metrics for service loadbalancer,"maybe to prometheus? using something like https://github.com/prometheus/haproxy_exporter?
",closed,False,2015-10-26 20:12:29,2016-08-05 02:49:04
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/206,https://api.github.com/repos/kubernetes/contrib/issues/206,[wip] Haproxy ingress,"See first commit for actual changes to existing code, everything else is diff confusion. This pr:
- Adds a --use-ingress flag to use ingress
- Adds a --watch-all flag to watch all namespaces instead of just one. This only makes sense with ingress, as services might have name collisions across namespace.
- Move service-loadbalancer to Ingress/controller

@aledbf I'm totally OK with you taking over this pr if you have the bandwidth ;)
",closed,True,2015-10-26 20:15:47,2016-08-03 20:32:17
contrib,aledbf,https://github.com/kubernetes/contrib/issues/207,https://api.github.com/repos/kubernetes/contrib/issues/207,Provide a step by step guide to configure the service loadbalancer,"The idea is to start from scratch guiding the user from 0 to k8s with the service loadbalancer configured 
Something like https://gist.github.com/aledbf/2985931789f3c182f22b
",closed,False,2015-10-26 20:16:34,2016-08-05 02:48:15
contrib,jlowdermilk,https://github.com/kubernetes/contrib/pull/208,https://api.github.com/repos/kubernetes/contrib/issues/208,Allow submitqueue to merge on either jenkins ci or shippable green,"cc @brendandburns, @lavalamp 
",closed,True,2015-10-27 06:37:40,2015-10-27 16:59:50
contrib,eparis,https://github.com/kubernetes/contrib/pull/209,https://api.github.com/repos/kubernetes/contrib/issues/209,submit queue: Do not update 'status' if PR already in queue,"If a PR is on the merge queue and the munger runs again, it will
(likely) determine that the PR needs to be added to the queue. The
problem is that we also put this determination in the history. So we log
every 10 minutes that a PR is being 'enqueued' even though it was always
on the queue. This makes the log less useful.
",closed,True,2015-10-28 04:35:33,2015-10-31 17:45:17
contrib,aledbf,https://github.com/kubernetes/contrib/pull/210,https://api.github.com/repos/kubernetes/contrib/issues/210,Add multiple namespace support in service loadbalancer,"Closes #203
",closed,True,2015-10-28 20:17:49,2016-01-29 16:02:14
contrib,morganwu277,https://github.com/kubernetes/contrib/pull/211,https://api.github.com/repos/kubernetes/contrib/issues/211,Avoid the listening port conflict of etcd.,"Usually we use `2379` as the default port for communicating with other etcd instances, we should use `4001` port to let local etcdctl access  etcd contents.
",closed,True,2015-10-29 05:54:55,2015-11-05 08:10:50
contrib,eparis,https://github.com/kubernetes/contrib/pull/212,https://api.github.com/repos/kubernetes/contrib/issues/212,We were trying to assign issues to robertbailey instead of roberthbailey,,closed,True,2015-10-30 21:44:35,2015-10-31 17:45:13
contrib,eparis,https://github.com/kubernetes/contrib/pull/213,https://api.github.com/repos/kubernetes/contrib/issues/213,Handle paging when getting commits,"The change after LGTM hook has been trigger incorrectly. See:

https://github.com/kubernetes/kubernetes/pull/13146
https://github.com/kubernetes/kubernetes/pull/15994

This handles paging on commits (which we shouldn't have more than 100
commits, but that is a code path this munger uses) and adds a touch more
debug info.
",closed,True,2015-10-31 23:00:32,2015-11-11 18:57:03
contrib,eparis,https://github.com/kubernetes/contrib/pull/214,https://api.github.com/repos/kubernetes/contrib/issues/214,Update list of generated files for sizing,"Should make some PRs smaller.
",closed,True,2015-11-02 18:50:10,2015-11-11 17:19:58
contrib,emaildanwilson,https://github.com/kubernetes/contrib/pull/215,https://api.github.com/repos/kubernetes/contrib/issues/215,Added scripts for Continuous Delivery using CircleCI and Jenkins,"Please review and consider adding this to the contrib repo for others to leverage. Several style choices where made here to make the scripts work well for different CI systems and without much customization to the example kubernetes spec schema provided.
",closed,True,2015-11-02 19:06:32,2015-11-17 04:02:13
contrib,pmorie,https://github.com/kubernetes/contrib/pull/216,https://api.github.com/repos/kubernetes/contrib/issues/216,Remove erroneous apostrophe from submit queue,"@eparis
",closed,True,2015-11-03 22:12:08,2015-11-11 19:11:29
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/217,https://api.github.com/repos/kubernetes/contrib/issues/217,How to write an Ingress controller docs,"Does what each of the commits say, main goal is to describe how one writes an Ingress controller with a simple example.
",closed,True,2015-11-05 02:54:34,2015-11-07 23:03:16
contrib,morganwu277,https://github.com/kubernetes/contrib/pull/218,https://api.github.com/repos/kubernetes/contrib/issues/218,1) Avoid the listening port conflict of etcd ; 2) kube-controller-manager.service should be started after kube-apiserver.service,"1. Usually we use 2379 as the default port for communicating with other etcd instances, we should use 4001 port to let local etcdctl access etcd contents for preventing port conflicts.
2. kube-controller-manager.service should be started after kube-apiserver.service. Otherwize, we'll meet

```
Nov 04 01:31:12 centos7-node-221 kube-controller-manager[963]: F1104 01:31:12.757305     963 controllermanager.go:292] Failed to get api versions from server: Get https://centos7-node-221:443/api: dial tcp 192.168.1.221:443: connection refused
Nov 04 01:31:12 centos7-node-221 kube-controller-manager[963]: E1104 01:31:12.759007     963 nodecontroller.go:154] Error monitoring node status: Get https://centos7-node-221:443/api/v1/nodes: dial tcp 192.168.1.221:443: connection refused
```

after minions reboot.
",closed,True,2015-11-05 08:56:37,2016-03-12 00:40:47
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/219,https://api.github.com/repos/kubernetes/contrib/issues/219,"Increasity verbosity of l7 controller, improve /healthz","Add a bunch of events, surfaced broken cloud client via /healthz
",closed,True,2015-11-08 01:31:42,2015-11-19 01:45:21
contrib,philips,https://github.com/kubernetes/contrib/pull/220,https://api.github.com/repos/kubernetes/contrib/issues/220,demo: hugo: README: add missing backtick,"broken README due to missing backtick
",closed,True,2015-11-08 14:36:13,2015-11-16 22:33:30
contrib,ghost,https://github.com/kubernetes/contrib/pull/221,https://api.github.com/repos/kubernetes/contrib/issues/221,Adds a flag to service_loadbalancer which allows to add entries in ha…,"…proxy config file only for services with a given label.
",closed,True,2015-11-09 10:45:38,2015-11-10 01:24:47
contrib,ghost,https://github.com/kubernetes/contrib/pull/222,https://api.github.com/repos/kubernetes/contrib/issues/222,Removing last service from service-loadbalancer,"Small change in service_loadbalancer. Now after last service is removed from Kubernetes it will be also removed from configuration file.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/222)

<!-- Reviewable:end -->
",closed,True,2015-11-09 11:14:05,2018-02-17 11:21:01
contrib,aledbf,https://github.com/kubernetes/contrib/pull/223,https://api.github.com/repos/kubernetes/contrib/issues/223,Haproxy ingress,"replaces #206 
",closed,True,2015-11-09 18:41:54,2016-05-03 12:31:06
contrib,aledbf,https://github.com/kubernetes/contrib/pull/224,https://api.github.com/repos/kubernetes/contrib/issues/224,Create custom haproxy image to publish in gcr.io,"This will allow `FROM gcr.io/google_containers/haproxy` in #223 
",closed,True,2015-11-10 02:11:20,2016-02-20 23:31:11
contrib,piersharding,https://github.com/kubernetes/contrib/issues/225,https://api.github.com/repos/kubernetes/contrib/issues/225,Debugging problems with service-loadbalancer,"Hi -

I've built and launched the servicelb container and launched it using the rc.yml example.  I can connect to the stats port on 1936, and haproxy appears to be listening ok on 80.  I've launched a test container to get it picked up by the loadbalancer but when I curl -v http://10.100.41.6/test I get 'Not Found'.  If I query the logs for the loadbalancer I get the messages below.  From here, I'm not sure where to go to try and further diagnose why it's not working but I'm guessing that the launched test container has not been detected so the haproxy.cfg has not been automatically updated (if that's correct?) - any help on how to try and get to bottom of this would be greatly appreciated.

Thanks,
Piers.

Icore@master ~ $ kubectl logs service-loadbalancer-1t2qm  

I1110 05:58:36.959150       1 service_loadbalancer.go:550] Creating new loadbalancer: {Name:haproxy ReloadCmd:./haproxy_reload Config:/etc/haproxy/haproxy.cfg Template:template.cfg Algorithm: startSyslog:false lbDefAlgorithm:roundrobin}
E1110 05:58:36.959519       1 service_loadbalancer.go:241] Get : unsupported protocol scheme """"
I1110 05:58:36.988153       1 service_loadbalancer.go:294] haproxy -- cat: can't open '/var/run/haproxy.pid': No such file or directory
",closed,False,2015-11-10 06:34:23,2018-02-12 13:26:09
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/226,https://api.github.com/repos/kubernetes/contrib/issues/226,Initial add of the scale-demo,"@bgrant0607 for api object style
@kelseyhightower for demo guide
",closed,True,2015-11-10 23:06:45,2015-11-12 04:07:05
contrib,cgwalters,https://github.com/kubernetes/contrib/pull/227,https://api.github.com/repos/kubernetes/contrib/issues/227,ansible/kubernetes: Fix typo in script dir for is_atomic,,closed,True,2015-11-11 03:14:40,2015-11-11 14:03:49
contrib,andrejvanderzee,https://github.com/kubernetes/contrib/issues/228,https://api.github.com/repos/kubernetes/contrib/issues/228,Nodes have no External IP,"Hi,

I am trying to run the example, but I get stuck in ""Expose services""; none of my nodes have any external IP. The following command gives me nothing:

kubectl get nodes  -o json | grep -i externalip -A 1

How can I activate the external IP for the role=loadbalancer nodes? 

Thank you
",closed,False,2015-11-11 13:38:43,2017-11-01 15:25:37
contrib,paralin,https://github.com/kubernetes/contrib/pull/229,https://api.github.com/repos/kubernetes/contrib/issues/229,Use phusion:baseimage for glbc image,"Minimizes the size of the image without really sacrificing anything.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/229)

<!-- Reviewable:end -->
",closed,True,2015-11-11 23:13:48,2016-10-02 22:32:23
contrib,gouyang,https://github.com/kubernetes/contrib/pull/230,https://api.github.com/repos/kubernetes/contrib/issues/230,Ansible: add proxy to make_ca_cert,"Ensure curl works if the host is behind proxy.

Signed-off-by: Guohua Ouyang gouyang@redhat.com
",closed,True,2015-11-12 08:10:42,2015-11-12 13:52:49
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/231,https://api.github.com/repos/kubernetes/contrib/issues/231,Tidy up l7 controller,"1. Make https://github.com/kubernetes/contrib/blob/master/Ingress/controllers/gce/rc.yaml look like https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/cluster-loadbalancing/glbc. 
2. Lower case the G on: https://github.com/kubernetes/contrib/blob/master/Ingress/controllers/gce/utils.go#L146 and https://github.com/kubernetes/contrib/blob/master/Ingress/controllers/gce/utils.go#L250 
3. Change the default cluster name to ""default-cluster-name"" (https://github.com/kubernetes/contrib/blob/master/Ingress/controllers/gce/main.go#L59). 
",closed,False,2015-11-12 18:25:42,2015-11-17 18:12:32
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/232,https://api.github.com/repos/kubernetes/contrib/issues/232,fixes to skip pkg installation if host is atomic,,closed,True,2015-11-13 00:06:47,2015-11-17 13:46:41
contrib,gouyang,https://github.com/kubernetes/contrib/pull/233,https://api.github.com/repos/kubernetes/contrib/issues/233,Ansible: add proxy to get_url,"get_url is failed when the host is using proxy, need to add proxy support to get_url.

There is ansible issue that get_url does not work with HTTPS proxy, see issue [#10941](https://github.com/ansible/ansible/issues/10941) in ansible project. It works by specifying both the proxy variables in environment and validate_certs=False.

Signed-off-by: Guohua Ouyang gouyang@redhat.com
",closed,True,2015-11-13 00:37:10,2015-11-13 23:50:31
contrib,gouyang,https://github.com/kubernetes/contrib/pull/234,https://api.github.com/repos/kubernetes/contrib/issues/234,fix the syntax error,"Signed-off-by: Guohua Ouyang gouyang@redhat.com
",closed,True,2015-11-13 01:52:38,2015-11-13 23:50:00
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/235,https://api.github.com/repos/kubernetes/contrib/issues/235,Disable tip on travis,"Was changed in https://github.com/kubernetes/contrib/pull/226 and I suspect it's what's breaking Travis. This pr is a test to see if it fixes things.
",closed,True,2015-11-13 16:47:48,2015-11-13 19:39:32
contrib,zmerlynn,https://github.com/kubernetes/contrib/issues/236,https://api.github.com/repos/kubernetes/contrib/issues/236,"Add a ""cla-ok"" label","I propose a ""CLA affirmed by human"" label that @googlebot won't flip, so that the submit queue can have two positive CLA signals to go off of. (It's frustrating to fight @googlebot on cherry-picks and known-good committers.)

cc @brendandburns @jlowdermilk 
",closed,False,2015-11-13 18:24:20,2015-11-25 00:09:29
contrib,freehan,https://github.com/kubernetes/contrib/pull/237,https://api.github.com/repos/kubernetes/contrib/issues/237,Tidy up l7 controller,"https://github.com/kubernetes/contrib/issues/231
",closed,True,2015-11-13 18:37:07,2015-11-14 01:43:43
contrib,eparis,https://github.com/kubernetes/contrib/pull/238,https://api.github.com/repos/kubernetes/contrib/issues/238,Submit Queue: Merge on either 'cla: yes' or 'cla: human-approved',"Stop paying attention to the github 'status' altogether (as it was
the same as the label, excluding races)
",closed,True,2015-11-13 22:19:12,2015-11-23 15:14:55
contrib,cgwalters,https://github.com/kubernetes/contrib/pull/239,https://api.github.com/repos/kubernetes/contrib/issues/239,"ansible/make-ca-cert: Clean up verbosity, error on no-public-ip","In a configuration I was testing in OpenStack, there was no
public floating IP attached to the master.  This caused `make-ca-cert.sh`
to fail in a very obscure way while doing a `cp` of a nonexistent
cert, with `stderr` redirected to `/dev/null`.

Now apparently there's a reason for directing `easyrsa` `stderr`
to `/dev/null` - OpenSSL is just verbose.

But for the other things, e.g. curl has a `-s` flag.  And AFAICS
there was never any reason to hide errors from `cp`.

Explicitly using the `v` flag to tar but also redirecting to
`/dev/null` was clearly a left hand not knowing what the right hand is
doing sort of thing.  Just drop both.

Anyways, we fix the real problem here by cleanly erroring out if we
fail to get the public IP from any of the cluod providers, before we
even try to make a cert.
",closed,True,2015-11-14 02:58:32,2015-11-14 18:43:37
contrib,cgwalters,https://github.com/kubernetes/contrib/pull/240,https://api.github.com/repos/kubernetes/contrib/issues/240,ansible/gen_certs: Fix regression when HTTP_PROXY is not set,"Regression from 46131210fc99fe84e121f24987f0c1669f277710 when
`http_proxy` is _not_ defined.

Ansible tried to quote the undefined variable, and it ended up breaking like:

```
Could not resolve proxy: {# https_proxy #}; Name or service not known
```
",closed,True,2015-11-14 16:09:53,2015-11-14 18:42:51
contrib,eparis,https://github.com/kubernetes/contrib/pull/241,https://api.github.com/repos/kubernetes/contrib/issues/241,SubmitQueue: Add ability to filter on PRs and Historic decisions in URL,"http://submit-queue.k8s.io/#?prDisplay=eparis&historyDisplay=17235

will open the web page already filtered with the PR tab showing those
oppened by eparis and the hsitory tab will show only pr 17235 (no matter
who openned it)
",closed,True,2015-11-14 21:19:27,2015-11-18 22:10:39
contrib,gouyang,https://github.com/kubernetes/contrib/pull/242,https://api.github.com/repos/kubernetes/contrib/issues/242,ansible: make apiserver port easy to configure,"Atomic host cannot listen to port 443 currently, so it should be able to configure the api port.

Signed-off-by: Guohua Ouyang gouyang@redhat.com
",closed,True,2015-11-15 00:57:12,2016-03-20 15:30:56
contrib,gouyang,https://github.com/kubernetes/contrib/issues/243,https://api.github.com/repos/kubernetes/contrib/issues/243,ansible: failed load flannel config file if use IP in inventory,"TASK: [flannel | Load the flannel config file into etcd] *********************\* 
failed: [10.66.15.133 -> 10.66.15.133] => {""changed"": true, ""cmd"": ""/usr/bin/etcdctl --no-sync --peers=http://10.66.15.133:2379 set /cluster.local/network/config < /tmp/flannel-conf.json"", ""delta"": ""0:00:00.214135"", ""end"": ""2015-11-15 01:33:17.547932"", ""rc"": 4, ""start"": ""2015-11-15 01:33:17.333797"", ""warnings"": []}
stderr: Error:  501: All the given peers are not reachable (failed to propose on members [http://10.66.15.133:2379] twice [last error: Put http://10.66.15.133:2379/v2/keys/cluster.local/network/config: dial tcp 10.66.15.133:2379: connection refused]) [0]

FATAL: all hosts have already failed -- aborting
",closed,False,2015-11-15 02:00:18,2016-06-01 05:13:46
contrib,gouyang,https://github.com/kubernetes/contrib/pull/244,https://api.github.com/repos/kubernetes/contrib/issues/244,ansible: use hostname instead of IP for flannel config,"fixes #243

Signed-off-by: Guohua Ouyang gouyang@redhat.com
",closed,True,2015-11-15 02:08:33,2016-03-21 16:58:44
contrib,jSherz,https://github.com/kubernetes/contrib/pull/245,https://api.github.com/repos/kubernetes/contrib/issues/245,kube-ui.yml references a hard coded proxy,"The other tasks in this role have proxy environment variables passed through from ansible. This file has hard coded values for a proxy (Squid?) at RedHat. Should this be the case?
",closed,True,2015-11-15 16:20:44,2015-11-16 13:32:15
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/246,https://api.github.com/repos/kubernetes/contrib/issues/246,Increase poll interval for getting Node Port of default backend.,"No point burning through a core if the default backend isn't up yet
",closed,True,2015-11-16 00:28:54,2015-11-16 22:37:21
contrib,shaylevi2,https://github.com/kubernetes/contrib/issues/247,https://api.github.com/repos/kubernetes/contrib/issues/247,What's the point of Ingress Controller?,"The nginx pod can land on any node and get one of many possible IPs, but its supposed to be my access to the internet, what do you expect me to use in my domain A record? Why would I not just use a separate machine, outside of the cluster, for proxying traffic to my pods. Maybe a better idea would be to allow me to **find the IPs of the pods when I'm outside the cluster** as well.
",closed,False,2015-11-16 10:39:04,2015-12-02 21:47:44
contrib,gmarek,https://github.com/kubernetes/contrib/pull/248,https://api.github.com/repos/kubernetes/contrib/issues/248,Add kubernetes-kubemark-gce as a blocker to the submit-queue,"cc @quinton-hoole @fgrzadkowski @davidopp 

It looks completely stable now, so I believe we can add a 100-node kubemark test as a blocker to the submit queue.
",closed,True,2015-11-16 12:24:51,2015-11-16 22:36:31
contrib,maklemenz,https://github.com/kubernetes/contrib/issues/249,https://api.github.com/repos/kubernetes/contrib/issues/249,Ingress: resolver option in nginx-alpha controller causes bad gateway,"[controller.go](https://github.com/kubernetes/contrib/blob/60008fc849d04582e35d12edcecf025fa6ac07e7/Ingress/controllers/nginx-alpha/controller.go#L45) is setting the dns resolver to localhost. I don't know how this is handled in GCE, but on my kubernetes-cluster installed on openstack I don't have a dns server inside each pod. The names don't get resolved and the result is a _bad gateway_ after a long time.

Removing [this line](https://github.com/kubernetes/contrib/blob/60008fc849d04582e35d12edcecf025fa6ac07e7/Ingress/controllers/nginx-alpha/controller.go#L45) would cause nginx to use the OS to resolve the names, hence would use _/etc/resolv.conf_ which is set by kubernetes.
",closed,False,2015-11-16 13:52:16,2015-11-17 08:07:10
contrib,maklemenz,https://github.com/kubernetes/contrib/pull/250,https://api.github.com/repos/kubernetes/contrib/issues/250,refs #249: Ingress: resolver option in nginx-alpha controller causes bad gateway,"#249

Resolver-option removed from Ingress nginx-alpha controller because it causes dns problems
",closed,True,2015-11-16 13:57:35,2015-11-16 23:09:59
contrib,cgwalters,https://github.com/kubernetes/contrib/pull/251,https://api.github.com/repos/kubernetes/contrib/issues/251,ansible: Consistently handle `http_proxy` not being defined,"The better fix for this might be putting the variables in
`$role/defaults`, but this works.
",closed,True,2015-11-16 14:06:19,2015-11-16 14:15:19
contrib,eparis,https://github.com/kubernetes/contrib/pull/252,https://api.github.com/repos/kubernetes/contrib/issues/252,SubmitQueue: Set as 'Status' in github for every PR,"As written this will change almost every PR on
https://github.com/kubernetes/kubernetes/pulls to show a yellow dot
instead of a green check (red X will stay red X). That may be a major
issue for some viewers of that page.

At the bottom of github we have statuses for CI, cla, etc. This adds a
status, to all PRs for the submit queue. It will add the same message as
seen below a PR at http://submit-queue.k8s.io in the github UI. These
messages are (hopefully) the correct next step for someone trying to get
their PR merged.

The 'details' link will link back to the submit-queue web page for the
given PR. The PR tab of the web page will not hae any additional useful
information, although the history tab might, if the PR every got greens
across the board.
",closed,True,2015-11-16 14:26:26,2015-11-25 18:50:47
contrib,eparis,https://github.com/kubernetes/contrib/pull/253,https://api.github.com/repos/kubernetes/contrib/issues/253,Make path labels regexp instead of prefix,,closed,True,2015-11-16 14:28:31,2015-11-18 22:10:38
contrib,emaildanwilson,https://github.com/kubernetes/contrib/issues/254,https://api.github.com/repos/kubernetes/contrib/issues/254,Scripts to deliver containers onto kubernetes clusters ,"If I'm a consumer of kubernetes clusters and I have my own docker service project(s) in github\github enterprise it would be nice to have a consistent and automated way to build containers, push them to a container registry, update the kube spec yaml stored in my github project with appropriate values and call commands against one or more kubernetes clusters to deploy the updates. Additionally I could then call more scripts to perform integration tests with other services and\or deploy to more kubernetes environment and so on...

The pull request I submitted gives the community the scripts they need to do this using both CircleCI and Jenkins, using the same set of deployment scripts for both CI systems. This provides a common starting point which can then be extended to add features and support for other CI systems.

This is certainly not the ONLY way this can be done but I thought it would make the most sense to provide it to the community here in the kubernetes project since it's specific to deploying onto kubernetes clusters. This also shows how to use kubectl to control multiple clusters which is extremely important to anyone that wants to provide HA and network isolation.

https://github.com/kubernetes/contrib/pull/215
",closed,False,2015-11-16 19:46:32,2015-11-18 20:54:48
contrib,pedro-r-marques,https://github.com/kubernetes/contrib/pull/255,https://api.github.com/repos/kubernetes/contrib/issues/255,Allow the user to specify the full path for local build install.,"Local build path will be different when using docker build; or when
downloading pre-built binaries from github. Let the user specify the
full path.
",closed,True,2015-11-16 23:16:39,2015-11-24 18:38:11
contrib,pedro-r-marques,https://github.com/kubernetes/contrib/pull/256,https://api.github.com/repos/kubernetes/contrib/issues/256,Make http_proxy configuration work across both RedHat and Debian,"The format for proxy variables is different across RedHat (existing) and Debian based systems (as tested with the apt repository provided by docker).
Make the docker install a separate play, rather than being a dependency so that it is easier to verify.
",closed,True,2015-11-16 23:30:25,2015-11-20 01:43:01
contrib,wstrange,https://github.com/kubernetes/contrib/issues/257,https://api.github.com/repos/kubernetes/contrib/issues/257,Ingress glbc pod: NodePort should have a firewall rule auto-created for health check,"It looks like the glbc pod gets created with a dynamic NodePort.

Under the GCE HTTP console, the health check for this service shows up as ""unhealthy"" - because no firewall rule has been created to allow the HTTP load balancer to probe the service. 

Users should not be required to create the firewall rule for this to work (very confusing). Ideally, any firewall rules should be auto-created. 
",closed,False,2015-11-17 01:37:06,2018-02-12 12:25:21
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/258,https://api.github.com/repos/kubernetes/contrib/issues/258,Default path to catch-all if unspecified.,"https://github.com/kubernetes/kubernetes/issues/17348

Fairly simple code reivew, @ArtfulCoder feel free to re-assign or ask for clarification
See https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/extensions/types.go#L571 for path spec.
",closed,True,2015-11-17 02:53:16,2015-11-18 21:51:30
contrib,maklemenz,https://github.com/kubernetes/contrib/issues/259,https://api.github.com/repos/kubernetes/contrib/issues/259,Ingress Proposal: nginx-alpha controller should be configurable through environment variables,"I currently run into some trouble: nginx refuses to start with the following message:

```
2015/11/17 11:01:49 Failed to execute nginx -s reload:
nginx: [emerg] could not build the server_names_hash,
you should increase server_names_hash_bucket_size: 32
```

I don't think it makes much sense to just set this option to a higher value, since nginx has a lot of options to tune.

Maybe its better if the controller.go would catch up environment variables or parameters to configure nginx.
So on could add the configuration in the environment variables part of the controller definition:

```
NGINX_HTTP_SERVER_NAMES_HASH_BUCKET_SIZE=128
NGINX_HTTP_GZIP=on
```

The controller would catch up all the environment variables starting with NGINX_HTTP_, lowercase the keys and use them as config option in the http-config block of the generated nginx.conf.

Sadly I've to admit that I'm still not familiar with golang.
",closed,False,2015-11-17 11:23:33,2015-11-19 08:38:19
contrib,aledbf,https://github.com/kubernetes/contrib/pull/260,https://api.github.com/repos/kubernetes/contrib/issues/260,Improve Ingress nginx defaults,,closed,True,2015-11-17 11:51:26,2015-11-20 15:22:05
contrib,pedro-r-marques,https://github.com/kubernetes/contrib/pull/261,https://api.github.com/repos/kubernetes/contrib/issues/261,Common opencontrail playbook between k8s and OpenShift.,"Update the opencontrail playbook with a version that supports:
- single or multiple interfaces;
- k8s and openshift;
- direct internet access or http proxy;

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/261)

<!-- Reviewable:end -->
",closed,True,2015-11-17 19:57:27,2016-09-21 23:02:18
contrib,krancour,https://github.com/kubernetes/contrib/pull/262,https://api.github.com/repos/kubernetes/contrib/issues/262,Add default.conf for nginx-alpha ingress controller,"`default.conf` was previously missing and build (e.g. `make container`) fails without it.
",closed,True,2015-11-18 20:32:12,2015-11-18 21:50:51
contrib,krancour,https://github.com/kubernetes/contrib/pull/263,https://api.github.com/repos/kubernetes/contrib/issues/263,Log errors getting ingresses in nginx-alpha ingress controller,"It's better not to swallow the errors that can occur here.  I had trouble getting this controller going and the swallowing of errors right here made my diagnosis harder than it should have been.  This pr fixes that.
",closed,True,2015-11-18 21:00:06,2015-11-18 21:47:28
contrib,eparis,https://github.com/kubernetes/contrib/pull/264,https://api.github.com/repos/kubernetes/contrib/issues/264,Ansible: etcd: listen on 0.0.0.0,"The etcd code listens on ansible_fqdn and then hard codes in 127.0.0.1.
We've had trouble where ansible_fqdn maps to localhost.localdomain and
now we are trying to bind on the same port twice.

Stop the madness, just listen on 0.0.0.0
",closed,True,2015-11-18 22:01:30,2015-11-20 17:53:12
contrib,eparis,https://github.com/kubernetes/contrib/pull/265,https://api.github.com/repos/kubernetes/contrib/issues/265,Fix travis flakes/failures,"We do a number of things here:
- do not re-use issue numbers
- update golang versions
- break the sleep loop when the test is over
- look for the 'reason' in the history as well as the end result
- add names to the test cases to make figuring out which case failed easier
",closed,True,2015-11-18 22:43:35,2015-11-20 17:53:13
contrib,krancour,https://github.com/kubernetes/contrib/pull/266,https://api.github.com/repos/kubernetes/contrib/issues/266,Enable cross-namespace ingress in nginx-alpha ingress controller.,"This allows you to run one ingress controller rc and fan out to services in multiple namespaces.
",closed,True,2015-11-19 03:44:18,2015-11-20 02:48:31
contrib,aledbf,https://github.com/kubernetes/contrib/pull/267,https://api.github.com/repos/kubernetes/contrib/issues/267,Reduce echoserver image size,"```
gcr.io/google_containers/echoserver  0.0  99b930b22297        2 minutes ago     215.1 MB
gcr.io/google_containers/echoserver  1.0  504e7d5be32b        2 weeks ago       688.8 MB
```
",closed,True,2015-11-19 04:16:22,2015-11-20 15:22:19
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/268,https://api.github.com/repos/kubernetes/contrib/issues/268,Loadbalancer/webserver examples,"Add some basic examples involving loadbalancers/webservers on Kubernetes. Mostly meant for what it says in the README. @aledbf since you're more familiar with loadbalancer/webserver config, review?
",closed,True,2015-11-19 20:57:14,2016-05-11 05:20:42
contrib,eparis,https://github.com/kubernetes/contrib/pull/269,https://api.github.com/repos/kubernetes/contrib/issues/269,Correctly state IsPR in the debug messages,"It was inverted. No functional change.
",closed,True,2015-11-19 21:59:18,2015-11-20 17:53:13
contrib,sebgoa,https://github.com/kubernetes/contrib/issues/270,https://api.github.com/repos/kubernetes/contrib/issues/270,nginx Ingress controller does not work without authentication,"Hi,

I have a kubernetes setup on my own hardware (not GKE), it seems that the nginx Ingress controller is looking for a token file for a service account ala GKE.

```
# docker logs 6653c3332d9b
2015/11/20 13:45:03 Failed to create client: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory.
```

https://github.com/kubernetes/contrib/tree/master/Ingress/controllers/nginx-alpha

Any ideas what's happening

thanks
",closed,False,2015-11-20 13:47:59,2016-01-31 10:30:51
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/271,https://api.github.com/repos/kubernetes/contrib/issues/271,Add some perfdash bits that I forgot to add.,"@gmarek 
",closed,True,2015-11-20 17:24:24,2016-01-07 22:27:37
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/272,https://api.github.com/repos/kubernetes/contrib/issues/272,Cluster tagging,"Tag all resources with user specified cluster name. Useful to match resources with the controller that created them in e2e tests.
",closed,True,2015-11-20 17:50:23,2015-11-30 19:18:42
contrib,krancour,https://github.com/kubernetes/contrib/pull/273,https://api.github.com/repos/kubernetes/contrib/issues/273,Add ingress to different backend ports in nginx-alpha ingress controller,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/273)

<!-- Reviewable:end -->
",closed,True,2015-11-20 18:20:08,2016-12-29 13:50:41
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/274,https://api.github.com/repos/kubernetes/contrib/issues/274,First draft of 'shame' report.,"This report is intended to be sent as an email. The purpose is to shame people into fixing the flaky tests they own.

Still a bit of a WIP.

Run with the command `$ ./mungegithub --token-file=$HOME/lavalamp-github-token --issue-reports=shame`

It outputs:

```
To: dawnchen@google.com,karl@mesosphere.com,madhusudancs@gmail.com,pmorie@redhat.com,quinton@google.com,vishnuk@google.com
Missing addresses for: ArtfulCoder, bprashanth, brendandburns, davidopp, fgrzadkowski, gmarek, ixdy, jdef, mikedanese, piosz, wojtek-t
Subject: Kubernetes flaky Test Report: 38 flaky tests

If you are in the To: line of this email, you have flaky tests to fix! Flaky
tests, even if they flake only a small percentage of the time, cause the merge
queue to become very long, which causes everyone on the team pain and
suffering.  Please either fix the tests assigned to you or find them an owner
who will fix them.

There were 11 P2/P3 issues which are not reported here.

Full report:
ArtfulCoder:
  [P0] 13818: ""flaky/broken e2e: Services should be able to change the type and nodeport settings of a service"" (71 days)

UNASSIGNED:
  [P1] 10300: ""Lack of visibility/attention to shippable/travis flakes may be hiding real bugs"" (1.5e+02 days)
  [P1] 12765: ""Flakey Test: Monitoring - should verify monitoring pods and all cluster nodes are available on influxdb using heapster. "" (98 days)
  [P1] 13515: ""GKE upgrade e2e tests leaking target pools, forwarding rules, and firewall rules"" (79 days)
  [P1] 14370: ""Document how to write good tests"" (59 days)
  [P1] 14691: ""oidc.TestOIDCDiscoverySecureConnection Flaky"" (53 days)
  [P1] 15381: ""Services should work after restarting apiserver flake"" (42 days)
  [P1] 16308: ""PD E2E Tests could leak PDs in some failure scenarios"" (25 days)
  [P1] 17018: ""Flaky: k8s.io/kubernetes/pkg/controller/framework_test.TestUpdate"" (11 days)
  [P1] 9121: ""Flaky test: TestUpgradeResponse"" (1.7e+02 days)

UNPRIORITIZED:
  [??] 14931: ""cni.TestCNIPlugin flake"" (50 days)
  [??] 15412: ""[mesos/docker] Flakey Smoke Test - TLS handshake timeout"" (42 days)
  [??] 15413: ""[mesos/docker] Flakey Smoke Test - provided IP is already allocated"" (42 days)
  [??] 15872: ""Flaky: Kubectl client Guestbook application should create and stop a working application"" (32 days)
  [??] 17181: ""exec.TestSelectPlugin is flaky"" (8 days)
  [??] 17256: ""Flaky Service test in per-PR jenkins"" (7 days)
  [??] 17332: ""unit test TestGenericScheduler is flaky"" (4.1 days)

bprashanth:
  [P0] 17518: ""Flaky test: Kubernetes e2e suite.GCE L7 LoadBalancer Controller should create GCE L7 loadbalancers and verify Ingress"" (1.1 days)

davidopp:
  [P1] 14421: ""runSchedulerNoPhantomPodsTest integration flake"" (58 days)

dchen1107:
  [P0] 15498: ""e2e: Reboot each node by triggering kernel panic and ensure they function upon restart flaky"" (39 days)

fgrzadkowski:
  [P1] 14695: ""flaky e2e: Network when a minion node becomes unreachable [replication controller] recreates pods scheduled on the unreachable minion node AND allows scheduling of pods on a minion after it rejoins the cluster"" (53 days)
  [P1] 14836: ""Visibility into gce-reboot failures is poor"" (51 days)

gmarek:
  [P0] 15139: ""\""SchedulerPredicates validates MaxPods\"" e2e test is flaky"" (46 days)

ixdy:
  [P0] 16828: ""kubernetes-soak-continuous-e2e-gce-1.1 broken on SchedulerPredicates tests"" (16 days)

jdef:
  [P1] 11821: ""Flaky: TestDQ_always_pop_earliest_deadline_multi"" (1.2e+02 days)
  [P1] 11857: ""Flaky: TestDQ_always_pop_earliest_deadline"" (1.2e+02 days)

madhusudancs:
  [P0] 17521: ""Flake: NodeOutOfDisk runs out of disk space"" (1.1 days)

mikedanese:
  [P0] 16385: ""e2e flake: failed to initialize within 300 seconds"" (24 days)
  [P0] 17583: ""Flake: Services should work after restarting kube-proxy"" (0.091 days)
  [P1] 13764: ""pkg/probe/http fails with Go 1.5 or 1.4"" (72 days)
  [P1] 14072: ""flaky e2e: Daemon set should launch a daemon pod on every node of the cluster"" (65 days)
  [P1] 16623: ""e2e flake: Daemon set should run and stop complex daemon"" (21 days)

piosz:
  [P0] 17584: ""Flake: Horizontal pod autoscaling [Autoscaling Suite] should scale from 1 pod to 3 pods and from 3 to 5 (via deployment, with scale resource: CPU)"" (0.09 days)
  [P1] 15496: ""Initial Resources should set initial resources based on historical data flaky"" (39 days)

pmorie:
  [P1] 13690: ""soak test failure: Downward API fails to provide pod IP as an env variable after a while"" (73 days)

quinton-hoole:
  [P1] 13062: ""Label and move slow e2e tests into a separate test job"" (91 days)
  [P1] 13828: ""Multiple e2e tests fail with \""Namespace x is active\"""" (71 days)

wojtek-t:
  [P1] 14899: ""TestMaxInFlight is flaky"" (50 days)
```
",closed,True,2015-11-20 22:30:17,2015-12-01 21:35:51
contrib,galthaus,https://github.com/kubernetes/contrib/pull/275,https://api.github.com/repos/kubernetes/contrib/issues/275,Add ansible network-service-install tag instead of flannel,"As additional network layers are added to the ansible playbook, it would be good to have a general tag to install and eventually configure the network layer so that a group variable can drive the networking choice and configuration, but the tag will ensure that one of the items is installed.
",closed,True,2015-11-21 00:21:22,2015-11-21 00:26:40
contrib,galthaus,https://github.com/kubernetes/contrib/issues/276,https://api.github.com/repos/kubernetes/contrib/issues/276,Ansible: feature of network-service tags instead of flannel/opencontrail,"It would be nice to have a couple tags for the ansible playbook that represented installing and configuring the network service layer.  Instead of having opencontrail and flannel, we should have network-service-install and network-service-config and let the group_var networking control which service is installed.

This allows for more generalized execution of the playbook from other yml files or orchestration engines.
",closed,False,2015-11-21 00:30:25,2015-11-24 14:43:47
contrib,galthaus,https://github.com/kubernetes/contrib/pull/277,https://api.github.com/repos/kubernetes/contrib/issues/277,Ansible: Add network service tags,"Addresses - #276
",closed,True,2015-11-21 00:34:01,2015-11-23 17:42:58
contrib,galthaus,https://github.com/kubernetes/contrib/issues/278,https://api.github.com/repos/kubernetes/contrib/issues/278,Ansible: master-only with opencontrail fails missing kubelet,"When running the play book with opencontrail as networking and setting up only a master node, the playbook will fail configuring the master because the opencontrail components need kubelet to manage local opencontrail containers.

This is not a problem is the master is a node.
",closed,False,2015-11-21 04:02:11,2015-11-24 14:43:27
contrib,galthaus,https://github.com/kubernetes/contrib/pull/279,https://api.github.com/repos/kubernetes/contrib/issues/279,Make sure to install kubelet on the master (if node not installed).,"This applies to systems that have opencontrail networking enabled.

This is a fix for #278
",closed,True,2015-11-21 04:03:03,2015-11-23 17:21:23
contrib,aledbf,https://github.com/kubernetes/contrib/pull/280,https://api.github.com/repos/kubernetes/contrib/issues/280,nginx Ingress controller using ConfigMap,"requires #333 

The use of third party resources is to enable custom nginx configuration without the use of custom images
## TODO:
- [X] configuration examples
- [x] tcp services
- [x] custom `dhparam.pem`
- [x] defaultBackend [404-server](https://github.com/kubernetes/contrib/tree/master/404-server) (is possible to provide a custom image)
- [x] provide similar image to 404-server to allow the customization of the default error pages and default index.html. Also return a response using accept header.
- [x] healthz check
- [ ] use [ConfigMap](https://github.com/kubernetes/kubernetes/pull/6245) instead third party resource
- [X] use record.NewBroadcaster()
- [x] avoid nginx reloads (using lua, thanks to @bakins)
",closed,True,2015-11-21 22:01:23,2016-02-16 15:29:06
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/281,https://api.github.com/repos/kubernetes/contrib/issues/281,Flannel helper that runs etcd and flannel in a pod.,,closed,True,2015-11-23 18:33:50,2015-11-25 06:39:06
contrib,eparis,https://github.com/kubernetes/contrib/pull/282,https://api.github.com/repos/kubernetes/contrib/issues/282,[WIP][mungegithub] OWNERS implementation,"Not enforced, just adds/removes a label. No way to know who needs to respond at any given moment. I'm sure 1,000 other problems....
",closed,True,2015-11-23 21:48:11,2016-02-09 02:08:04
contrib,eparis,https://github.com/kubernetes/contrib/pull/283,https://api.github.com/repos/kubernetes/contrib/issues/283,Submit Queue: Minor fixes,"- Make sure brand new PRs don't get to the front of the merge queue
- Minor cosmetic updates to queue page
- Meaningless angular version bumps
- Faster response to github e2e results
",closed,True,2015-11-24 17:21:15,2015-11-24 22:23:42
contrib,galthaus,https://github.com/kubernetes/contrib/pull/284,https://api.github.com/repos/kubernetes/contrib/issues/284,Update Readme to reflect new network-service tags,"Change references #276
",closed,True,2015-11-24 19:20:50,2016-01-07 22:34:09
contrib,eparis,https://github.com/kubernetes/contrib/pull/285,https://api.github.com/repos/kubernetes/contrib/issues/285,Submit Queue: Update static committers list,"Couple new people with pull access. Couple people moved from pull to
push. One person moved from push to pull. First time for that...
",closed,True,2015-11-24 20:43:34,2015-11-24 21:01:19
contrib,eparis,https://github.com/kubernetes/contrib/pull/286,https://api.github.com/repos/kubernetes/contrib/issues/286,Submit Queue: remove labels which are not correct,"If the bot added a label like 'kind/design' or 'kind/new-api' and the PR changes such that the label no longer is appropriate this will remove that label. If a kind/new-api label was added by something other than the bot, it will be left alone, even if the PR does not match the map.
",closed,True,2015-11-24 23:08:57,2015-11-25 18:50:45
contrib,eparis,https://github.com/kubernetes/contrib/pull/287,https://api.github.com/repos/kubernetes/contrib/issues/287,Submit Queue: Retest PRs every 48 hours,"We don't actually test every PR every 48 hours. Only those which seem close to merge. They have LGTM, their tests look good, etc.  If this were to run right this instant, it would hit against 4 PRs.
",closed,True,2015-11-25 00:08:46,2015-11-25 19:04:55
contrib,aledbf,https://github.com/kubernetes/contrib/pull/288,https://api.github.com/repos/kubernetes/contrib/issues/288,Add nginx 1.9.x image,"```
gcr.io/google_containers/nginx-slim 0.0          038a045ad0bc   15 seconds ago  22.66 MB
gcr.io/google_containers/nginx      latest       9f4cd86c1aec   8 months ago    425.7 MB
```
",closed,True,2015-11-25 02:28:54,2016-01-05 12:39:53
contrib,eparis,https://github.com/kubernetes/contrib/pull/289,https://api.github.com/repos/kubernetes/contrib/issues/289,SubmitQueue: Do not update status when E2E is re-running,"The normal loop could see E2E was not green and would update things. But
if this is being tested for merge, we shouldn't pay attention to it and
should leave the status as 'retesting'
",closed,True,2015-11-25 19:15:22,2015-11-25 19:19:56
contrib,aledbf,https://github.com/kubernetes/contrib/pull/290,https://api.github.com/repos/kubernetes/contrib/issues/290,Reduce echoheaders image size using nginx-slim image and lua,"requires #288
",closed,True,2015-11-26 01:16:30,2016-01-05 12:39:07
contrib,furikake,https://github.com/kubernetes/contrib/pull/291,https://api.github.com/repos/kubernetes/contrib/issues/291,Ansible tasks to install latest version of Docker,"I ran the Ansible playbook on CentOS 7 and it installed Docker 1.8 rather than 1.9.  I'm assuming that someone that wants a `localBuild` rather than `packageManager` for Kubernetes probably wants the latest for Docker too.  I certainly did.

I'm downloading the script locally first so that I don't have to use the `validate_certs=False` to work around SNI and Python.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/291)

<!-- Reviewable:end -->
",closed,True,2015-11-26 04:37:10,2016-09-09 14:00:05
contrib,jayunit100,https://github.com/kubernetes/contrib/pull/292,https://api.github.com/repos/kubernetes/contrib/issues/292,"Changes for Spinning up VMs on OS X, Updating Centos boxes, flannel","Here are some thanskgiving updates for 
- spinning up the VMs in vagrant on OSX. 
- IP address detect for vbox
- adding hints: docker example paths.
- adding hints: on how to get rid of the IP address option in easyrsa.
- flannel eth options in jinja w eth1 for vbox 
- Python selinux install directives
- Fixing the Centos base image

cc @eparis @justinsb 
",closed,True,2015-11-28 17:55:37,2015-12-08 15:13:07
contrib,sroze,https://github.com/kubernetes/contrib/issues/293,https://api.github.com/repos/kubernetes/contrib/issues/293,Add a new node with the ansible configuration,"I've a running cluster that I setup so easily with the Ansible playbook (thank you very much!), but now the cluster is growing and I need to add new nodes. 

If I just add it in the `[nodes]` list of the inventory file, then it'll reinstall the whole cluster (tried it, on a staging cluster thankfully). Is there a way to just add a new node?

Thanks you very much.
",closed,False,2015-11-29 14:30:00,2018-03-22 21:27:53
contrib,tmc,https://github.com/kubernetes/contrib/pull/294,https://api.github.com/repos/kubernetes/contrib/issues/294,[terraform-aws] add terraform aws provisioning scripts,"Mirrors upstream kube-up.sh behavior.
",closed,True,2015-11-29 17:23:28,2016-08-03 21:08:55
contrib,pesho,https://github.com/kubernetes/contrib/pull/295,https://api.github.com/repos/kubernetes/contrib/issues/295,Delay starting services until network is actually up,"Some services may attempt to start before a network interface is up, and then fail badly. A simple master or node server reboot is enough to reproduce - it will fail to bring up all required services more often than not (tested on Fedora 23). The services in question are:
- flanneld and kube-apiserver (on masters)
- flanneld and kube-proxy (on minion nodes)

The systemd unit files could have been updated directly to fix this, but as flanneld is installed from a distro package, a less invasive approach was taken instead: A dummy systemd target unit is introduced, which starts after network-online.target, but before the fragile services.
",closed,True,2015-11-29 23:35:25,2016-03-20 21:11:38
contrib,davidopp,https://github.com/kubernetes/contrib/pull/296,https://api.github.com/repos/kubernetes/contrib/issues/296,Add mikedanese to blunderbuss.,"@mikedanese 
",closed,True,2015-11-30 00:46:13,2015-11-30 02:15:14
contrib,paralin,https://github.com/kubernetes/contrib/issues/297,https://api.github.com/repos/kubernetes/contrib/issues/297,"Ingress glbc: can't get a token from the metadata service, not running on gce","I'm getting this error:

```
50s       49s       258       yasp      Ingress                       GCE       {loadbalancer-controller }   Post https://www.googleapis.com/compute/v1/projects/peaceful-parity-87002/zones/us-central1-f/instanceGroups?alt=json: oauth2/google: can't get a token from the metadata service; not running on GCE
```

I'm not running on the Container Engine, no. I am running on the cloud compute engine. Why does this happen? The logs get spammed with this error (event logs).
",closed,False,2015-11-30 03:28:36,2016-06-06 00:56:25
contrib,eparis,https://github.com/kubernetes/contrib/pull/298,https://api.github.com/repos/kubernetes/contrib/issues/298,Merge Bot: Fix infinite loop possible with missing CI tests,"Number of cleanups, but the main fix is in the last commit. After we stop paying attention to shippable altogether we can simplify the `ping_ci` and `ok-to-test` mungers. ping_ci was able to go into an infinite loop. Ping CI seemed to be able to trigger a bug in jenkins where only the e2e was getting run, instead of both e2e and unit. So make sure we get both of them.
",closed,True,2015-11-30 23:46:19,2015-12-01 21:42:09
contrib,eparis,https://github.com/kubernetes/contrib/pull/299,https://api.github.com/repos/kubernetes/contrib/issues/299,Rewrite relnotes tool using mungegithub/github,"Simplifies the code readability.
",closed,True,2015-12-01 00:07:00,2016-08-03 00:33:16
contrib,wojtek-t,https://github.com/kubernetes/contrib/issues/300,https://api.github.com/repos/kubernetes/contrib/issues/300,Merge-bot is not doing anything because of lack of API-tokens," Today I observed (for the second time) the following situation.
- mergebot initiates e2e/unit tests before merging the PR
- in the meantime it stucks because of lack of api-tokens:
  
  ```
  E1201 09:10:07.238529       1 github.go:1079] no mergeability information for ""docs(getting-started): add AZ_LOCATION information to README"" 17926, Skipping
  E1201 09:10:23.883793       1 github.go:63] Ran out of github API tokens. Sleeping for 38.70193685148333 minutes
  E1201 09:10:38.578980       1 github.go:63] Ran out of github API tokens. Sleeping for 38.45701710115 minutes
  ```
- once tests are finished, and PR is ready for merged (the internal e2e suites are also green) - it doesn't merge it
- once it is awake again, it starts again from the beginning
  
  Today - I manually merge https://github.com/kubernetes/kubernetes/pull/17836 exactly because of that (it was ready for merging and merge-bot was waiting for api-tokens).
  
  However, we can't afford for such situations with huge backlog of PRs.

@eparis @lavalamp 
",closed,False,2015-12-01 09:34:19,2016-01-26 17:00:36
contrib,eparis,https://github.com/kubernetes/contrib/pull/301,https://api.github.com/repos/kubernetes/contrib/issues/301,Submit Queue: re-check both e2e and unit test before merge,,closed,True,2015-12-01 15:31:01,2015-12-02 17:36:10
contrib,eparis,https://github.com/kubernetes/contrib/pull/302,https://api.github.com/repos/kubernetes/contrib/issues/302,SubmitQueue: Less log spam when we run out of tokens,"The spam was nice, since it was easy to find, but it was bad, because
all of the 'negative' sleeps weren't really sleeps. No one will see this
unless they look at the container logs.
",closed,True,2015-12-01 19:52:57,2015-12-02 17:36:10
contrib,eparis,https://github.com/kubernetes/contrib/pull/303,https://api.github.com/repos/kubernetes/contrib/issues/303,Submit Queue: better track github rate limit tokens,"The code would collect how many tokens were left at the end of the loop
but when the tokens would reset at the beginning of the next loop. So if
we slept just before a token reset we would wake up thinking we didn't
have many tokens and that the next reset was an hour away.

This parses the github headers on every API response and updates the
timeout and remaining count. Thus we should always be correct.
",closed,True,2015-12-01 21:17:50,2015-12-02 17:36:11
contrib,eparis,https://github.com/kubernetes/contrib/pull/304,https://api.github.com/repos/kubernetes/contrib/issues/304,Fix CI failures from shame report,,closed,True,2015-12-01 21:38:17,2015-12-01 21:42:10
contrib,aledbf,https://github.com/kubernetes/contrib/pull/305,https://api.github.com/repos/kubernetes/contrib/issues/305,Pass Host header in nginx-alpha Ingress controller,,closed,True,2015-12-02 17:37:22,2016-01-05 12:39:20
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/306,https://api.github.com/repos/kubernetes/contrib/issues/306,Add vacation checking to blunderbuss,"A bunch of us will be out of the office for a stretch soon. It'd be nice if PRs were routed to people who _aren't_ out. What's the best way to tell the blunderbuss munger that we're out of the office?

1) Have people make a gist named ""kubernetes-vacation"" with a human readable format indicating dates that they're unavailable.
2) Same as above, but people make their own ooo repo.
3) Have people add the string "" - OOO"" to their location in github while they're out. (This was easy, so it's in this PR right now)
4) Keep a file in the main repo that people just update.
5) Other ideas?

Some people don't want their vacation schedules to be public.
",closed,True,2015-12-03 01:20:51,2016-01-07 22:33:38
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/307,https://api.github.com/repos/kubernetes/contrib/issues/307,update blunderbuss assignee file,"There's a lot of obvious improvements to be made to this file. I made some of them.

FYI I'm adding to the reviewer list: @krousey @janetkuo @caesarxuchao @nikhiljindal @wojtek-t @bprashanth -- If the auto-assigner gives you something and you're not sure it's a good idea, please route it to someone etc.

@dchen1107 @davidopp @thockin -- you all might want to take a look and try to similarly spread the review load for your areas...
",closed,True,2015-12-03 19:16:39,2015-12-04 15:54:37
contrib,jayunit100,https://github.com/kubernetes/contrib/issues/308,https://api.github.com/repos/kubernetes/contrib/issues/308,"Update ansible default subnet ""stand out"" a little bit  (i.e. non 172)","Since docker conatainers are often 172.x.y.z even without flannel running, i wonder if we should consider a unique subnet for the default (200.x.y.z).  I realize this isn't a bug of any sort, so feel free to close if Im the only one that likes flannel to be on a totally different subnet :).  

For me, it just helps with debugging and sanity checking when running e2e tests and so on.
",closed,False,2015-12-04 15:04:13,2018-02-14 02:02:11
contrib,aledbf,https://github.com/kubernetes/contrib/pull/309,https://api.github.com/repos/kubernetes/contrib/issues/309,Update haproxy and add checks for services,,closed,True,2015-12-04 15:53:26,2016-01-05 12:40:24
contrib,eparis,https://github.com/kubernetes/contrib/pull/310,https://api.github.com/repos/kubernetes/contrib/issues/310,Alphabetize blunderbuss.yml for easy human reading,"simply `mungegithub blunderbuss-normalize`

also fix duplicate of `plugin` definition
",closed,True,2015-12-04 15:57:45,2016-01-06 18:23:29
contrib,davidopp,https://github.com/kubernetes/contrib/pull/311,https://api.github.com/repos/kubernetes/contrib/issues/311,Add derekwaynecarr to blunderbuss for Vagrant,"@derekwaynecarr @bgrant0607 @mikedanese 
",closed,True,2015-12-04 20:12:50,2015-12-04 20:43:02
contrib,eparis,https://github.com/kubernetes/contrib/pull/312,https://api.github.com/repos/kubernetes/contrib/issues/312,blunderbuss whitespace blunder,,closed,True,2015-12-04 21:17:35,2016-01-06 18:23:30
contrib,moserke,https://github.com/kubernetes/contrib/pull/313,https://api.github.com/repos/kubernetes/contrib/issues/313,Put client_max_body_size in server config if found in ingress annotation,,closed,True,2015-12-04 23:47:44,2015-12-06 01:12:46
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/314,https://api.github.com/repos/kubernetes/contrib/issues/314,GLBC should recover gracefully from default backend mangling,"If you go behind the controller's back and delete the default backend, it doesn't recreate (if fact it does weird things by trying to create a urlmap with a non-existent backend). It does for everything else. This should get fixed. 
",closed,False,2015-12-06 22:02:11,2015-12-11 01:07:00
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/315,https://api.github.com/repos/kubernetes/contrib/issues/315,Sync default backend if user deletes it.,"Simple change that does as the name suggests.
",closed,True,2015-12-06 22:58:15,2015-12-07 18:24:00
contrib,gayanlggd,https://github.com/kubernetes/contrib/issues/316,https://api.github.com/repos/kubernetes/contrib/issues/316,Nginx based ingress controller container not starting,"Hi all,

I am trying to deploy the nginx based ingress controller as of in [1]. But I have been unable to do so. Simply the container won't start. It gives the following error.

nginx-ingress-rm4p7       0/1       CrashLoopBackOff   9          18m

and 

18m     1s      106 {kubelet 10.245.1.3}    spec.containers{nginx}          Backoff     Back-off restarting failed docker container

Any help on this is highly appreciated.

[1] https://github.com/kubernetes/contrib/tree/master/Ingress/controllers/nginx-alpha
",closed,False,2015-12-07 09:45:25,2015-12-08 04:08:37
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/317,https://api.github.com/repos/kubernetes/contrib/issues/317,"Fixups for release-notes.go: end-of-branch issues, API rate limiting, and release-note filter flag","There were some issues with finding PRs and falling off the end of the branch, and the API rate limit, which I've resolved here.

This also adds a flag to allow a user to not filter on the `release-note` label, to list all PRs between two.
",closed,True,2015-12-07 21:07:57,2015-12-14 19:24:04
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/318,https://api.github.com/repos/kubernetes/contrib/issues/318,Make wording stronger,"Per https://github.com/kubernetes/kubernetes/issues/18303
",closed,True,2015-12-08 01:10:30,2015-12-08 02:12:40
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/319,https://api.github.com/repos/kubernetes/contrib/issues/319,add myself to blunderbuss for pkg/client/leaderelection,"I'd like to field these initially
",closed,True,2015-12-08 21:18:09,2015-12-08 22:00:24
contrib,jayunit100,https://github.com/kubernetes/contrib/pull/320,https://api.github.com/repos/kubernetes/contrib/issues/320,"Ansible configuration parameter for interface, deployments.","@eparis deployments and interfaces configuration.  
",closed,True,2015-12-08 22:40:31,2015-12-09 15:25:19
contrib,ZhuPeng,https://github.com/kubernetes/contrib/issues/321,https://api.github.com/repos/kubernetes/contrib/issues/321,Haproxy reload not correctly in service loadbalance,"we encounter the problem just like this [haproxy-issue-48](https://github.com/haproxy/haproxy/issues/48), and post a new [issue](https://github.com/haproxy/haproxy/issues/56). The old haproxy process will influence the new one that make the service visiting not stable. Until now I cannot find a solid solution and we make a little change when haproxy reload by `-sf`. if the old haproxy process still running, then kill it by `kill -9 <pid>`. It seems work fine but I don't know it would or not cause some bad affect when a lot of service create or modified. Did you have any suggestions or solutions that have no side effects?
",closed,False,2015-12-09 07:57:48,2016-01-13 22:02:10
contrib,jayunit100,https://github.com/kubernetes/contrib/issues/322,https://api.github.com/repos/kubernetes/contrib/issues/322,[ansible] Automate vbox  networking for mac os / vagrant,"Now that iface is parameterizable, im on the hook to make vbox ""just work"".
cc @eparis .  Will look at this sunday, 
- there is some generic /etc/hosts  functionality that needs to be used, rather than vagrant-hostmanager (which is what i currently use for vbox ips).
- I Will also try to see if there are any smarts that might be going into flannel to make it smarter before i automate it.
",closed,False,2015-12-09 15:27:28,2018-02-14 02:02:10
contrib,MrMitch17,https://github.com/kubernetes/contrib/issues/323,https://api.github.com/repos/kubernetes/contrib/issues/323,Deploying on Fedora 23 Fails with flannel opts,"Hello:

When deploying to Fedora 23 with one master and two nodes I get the following error.

TASK: [flannel | Install Flannel config file] ********************************\* 
{'msg': ""AnsibleUndefinedVariable: One or more undefined variables: 'flannel_opts' is undefined"", 'failed': True}

I tried commenting out line 12 in ansible/roles/flannel/templates/flanneld.j2 to see if that would do anything, but I still get the same error.

I also tried ./setup.sh --tags=flannel and get the following.

ERROR: tag(s) not found in playbook: flannel.  possible values: addons,binary-update,dns,docker,etcd,masters,network-service-config,network-service-install,nodes,pre-ansible,secrets
",closed,False,2015-12-09 23:08:25,2015-12-10 14:03:41
contrib,jayunit100,https://github.com/kubernetes/contrib/pull/324,https://api.github.com/repos/kubernetes/contrib/issues/324,"[ansible] Use | for nil check, fixes #323","this is the ""right"" way to do default filters. I don't think the ""if"" statement works they way i assumed it did.  My fault for not testing the negative case (i.e. where param was missing).  
- I got this from http://docs.ansible.com/ansible/playbooks_filters.html .
- It appears to work properly, tested both with and without flannel_opts defined...
",closed,True,2015-12-10 01:35:08,2015-12-10 14:03:37
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/325,https://api.github.com/repos/kubernetes/contrib/issues/325,use annotations to redefine health check endpoints,"Currently all backends need to serve a 200 on /
https://github.com/kubernetes/contrib/tree/master/Ingress/controllers/gce#health-checks

This makes it hard to run stock apps like word press that don't serve a 200 on /. We should have this configurable, it doesn't need to be blocked on the api exposing health checks.
",closed,False,2015-12-11 01:11:02,2018-11-13 13:57:09
contrib,zmerlynn,https://github.com/kubernetes/contrib/issues/326,https://api.github.com/repos/kubernetes/contrib/issues/326,[submit queue] Add a way to override the submit queue for a given Jenkins build,"This can be Google-only for now, but for long builds like `kubernetes-e2e-gce-autoscaling`, it would be really useful if the build cop could dump a file in GCS that just overrode the submit queue status and said ""look, I know what I'm doing, it's a known issue, keep going"".

(Also useful for, say, `gke-ci`, where it might actually be GKE oncall that needs to fix something, and it might be a while.)
",closed,False,2015-12-11 01:48:54,2016-06-10 02:04:56
contrib,pesho,https://github.com/kubernetes/contrib/pull/327,https://api.github.com/repos/kubernetes/contrib/issues/327,"service-loadbalancer: HAProxy: Disallow concurrent reload attempts, should fix #321","The script-file itself is used as a mutex, idea borrowed from https://github.com/mesosphere/marathon-lb/commit/598ff09b0c80349418c38c44808a616e77efec74.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/327)

<!-- Reviewable:end -->
",closed,True,2015-12-11 02:05:03,2016-12-20 22:43:36
contrib,aledbf,https://github.com/kubernetes/contrib/pull/328,https://api.github.com/repos/kubernetes/contrib/issues/328,Avoid simultaneous haproxy reloads,,closed,True,2015-12-11 04:47:59,2016-01-05 12:40:09
contrib,MrMitch17,https://github.com/kubernetes/contrib/issues/329,https://api.github.com/repos/kubernetes/contrib/issues/329,Fails on Fedora23 for apiserver_runtime_config undefined variable,"TASK: [master | write the config file for the api server] ********************\* 

fatal: [x.x.x.x] => {'msg': ""AnsibleUndefinedVariable: One or more undefined variables: 'apiserver_runtime_config' is undefined"", 'failed': True}
ansible/roles/master/templates/apiserver.j2:26

Appears it's possibly the same issue as #323 
",closed,False,2015-12-11 19:59:36,2018-02-14 02:02:11
contrib,Arano-kai,https://github.com/kubernetes/contrib/pull/330,https://api.github.com/repos/kubernetes/contrib/issues/330,Ansible: fix make-ca-cert,"Fixes #17911 (in kubernetes/kubernetes):
    Export empty proxy no more
    Cut CN's over 64 bytes
",closed,True,2015-12-14 11:45:59,2016-02-25 13:21:15
contrib,jayunit100,https://github.com/kubernetes/contrib/pull/331,https://api.github.com/repos/kubernetes/contrib/issues/331,API Server opts use | for nil check,"@eparis I think this is what you had in mind for this? It also fixes the other screw up i made with the pyhton style `if nil` check on apiserver
",closed,True,2015-12-14 18:58:00,2015-12-14 20:31:39
contrib,MDrollette,https://github.com/kubernetes/contrib/pull/332,https://api.github.com/repos/kubernetes/contrib/issues/332,use alpine baseimage for glbc,"33mb image versus 225mb
",closed,True,2015-12-15 00:54:07,2016-07-30 06:31:52
contrib,aledbf,https://github.com/kubernetes/contrib/pull/333,https://api.github.com/repos/kubernetes/contrib/issues/333,Update nginx and replace lua5.1 with luajit ,"The reason to replace lua with luajit is the support of [yield/resume in coroutines](https://github.com/openresty/lua-nginx-module#lua-coroutine-yieldingresuming)
",closed,True,2015-12-15 20:57:29,2016-02-16 15:30:05
contrib,ibotty,https://github.com/kubernetes/contrib/pull/334,https://api.github.com/repos/kubernetes/contrib/issues/334,fix typo,,closed,True,2015-12-17 13:20:07,2015-12-17 14:12:11
contrib,ZhuPeng,https://github.com/kubernetes/contrib/pull/335,https://api.github.com/repos/kubernetes/contrib/issues/335,"bugfix: set targetPort by string name, but didnot have a port name",,closed,True,2015-12-17 13:26:00,2015-12-17 13:28:25
contrib,ZhuPeng,https://github.com/kubernetes/contrib/pull/336,https://api.github.com/repos/kubernetes/contrib/issues/336,"service-loadbalancer: When set targetPort by string name and didnot have a port name, will found no endpoints in that service","The EndpointPorts's Name attr was corresponds to the ServicePort.Name. If we set EndpointPort Name, then we must set the same name to the corresponds ServiePort.Name on test file.
https://github.com/kubernetes/kubernetes/blob/master/pkg/api/types.go#L1420-L1431

We use a string name to represent TargetPort, which might set in Pod spec, and we do not set a ServicePort.Name or the TargetPort different with ServicePortName
https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/es-service.yaml#L12-L14
If I create the above service in kubernetes and it works well. But this service did not found any endpoints in service-loadbalancer.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/336)

<!-- Reviewable:end -->
",closed,True,2015-12-17 13:31:37,2016-12-19 17:50:06
contrib,rsmitty,https://github.com/kubernetes/contrib/issues/337,https://api.github.com/repos/kubernetes/contrib/issues/337,Heapster Deployment Broken,"The heapster deployment seems to be broken because in the Ansible playbooks because of a recent-ish change to the controller definition to allow for dynamic configurations. I suspect this is usually handled by Salt, but this does not occur during an Ansible based deployment. Here is the relevant commit that seems to introduce this issue:

https://github.com/kubernetes/kubernetes/commit/eee87558e06e07ee62362c61c3d0b49e29de4fdf

Removing the templating from the top of the service file seems to allow for creation as expected.
",closed,False,2015-12-18 05:43:40,2015-12-28 22:20:02
contrib,rsmitty,https://github.com/kubernetes/contrib/pull/338,https://api.github.com/repos/kubernetes/contrib/issues/338,Add ability to deploy kube-dash,"Closes #182. This will allow for kube-dash to be deployed by simply setting `kube-dash: true` in the all.yml variable file.
",closed,True,2015-12-18 05:59:28,2015-12-18 12:42:16
contrib,eparis,https://github.com/kubernetes/contrib/pull/339,https://api.github.com/repos/kubernetes/contrib/issues/339,Handle salt templated monitoring definitions,"Which were added https://github.com/kubernetes/kubernetes/commit/eee87558e06e07ee62362c61c3d0b49e29de4fdf

And fixes #337 
",closed,True,2015-12-18 13:12:33,2016-01-06 18:23:29
contrib,ZhuPeng,https://github.com/kubernetes/contrib/pull/340,https://api.github.com/repos/kubernetes/contrib/issues/340,Add service port,,closed,True,2015-12-21 09:52:10,2016-01-12 02:13:04
contrib,ZhuPeng,https://github.com/kubernetes/contrib/pull/341,https://api.github.com/repos/kubernetes/contrib/issues/341,Modify github to github.com,,closed,True,2015-12-21 10:46:11,2016-01-07 22:35:28
contrib,tommyknows,https://github.com/kubernetes/contrib/issues/342,https://api.github.com/repos/kubernetes/contrib/issues/342,"""Not Found"" when accessing service","I went through the Readme and tried out the proxy. 
I had to edit the haproxy.cfg file because I got the following error-messages when running /haproxy_reload:

``` shell
[ALERT] 357/114415 (26) : parsing [/etc/haproxy/haproxy.cfg:32] : 'listen' cannot handle unexpected argument ':1936'.
[ALERT] 357/114415 (26) : parsing [/etc/haproxy/haproxy.cfg:32] : please use the 'bind' keyword for listening addresses.
[ALERT] 357/114415 (26) : Error(s) found in configuration file : /etc/haproxy/haproxy.cfg
[WARNING] 357/114415 (26) : config : proxy 'stats' has no 'bind' directive. Please declare it as a backend if this was intended.
[WARNING] 357/114415 (26) : config : missing timeouts for proxy 'stats'.
   | While not properly invalid, you will certainly encounter various problems
   | with such a configuration. To fix this, please ensure that all following
   | timeouts are set to a non-zero value: 'client', 'connect', 'server'.
[WARNING] 357/114415 (26) : config : log format ignored for frontend 'httpfrontend' since it has no log address.
[ALERT] 357/114415 (26) : Fatal errors found in configuration.
```

As of these errors, I changed my /etc/haproxy/haproxy.cfg to:

``` shell
# Default haprxy config file. The service-loadbalancer uses
# go templates (http://golang.org/pkg/text/template/) to
# generate the config dynamically.
global
    daemon
    stats socket /tmp/haproxy
    server-state-file global
    server-state-base /var/state/haproxy/

defaults
        log     global
        mode    http
        option  httplog
        option  dontlognull
    timeout connect 5000
    timeout client 50000
    timeout server 50000
        errorfile 400 /etc/haproxy/errors/400.http
        errorfile 403 /etc/haproxy/errors/403.http
        errorfile 408 /etc/haproxy/errors/408.http
        errorfile 500 /etc/haproxy/errors/500.http
        errorfile 502 /etc/haproxy/errors/502.http
        errorfile 503 /etc/haproxy/errors/503.http
        errorfile 504 /etc/haproxy/errors/504.http
    # default default_backend. This allows custom default_backend in frontends
    default_backend default-backend

backend default-backend
  server localhost 127.0.0.1:8081

frontend httpfrontend
    # Frontend bound on all network interfaces on port 80
    bind *:80
    mode        http
```

When running the /haproxy_reload again, the haproxy started, but with the following errors:

``` shell
bash-4.3# /haproxy_reload
cat: can't open '/var/run/haproxy.pid': No such file or directory
[WARNING] 357/114616 (36) : config : log format ignored for frontend 'httpfrontend' since it has no log address.
```

Nonetheless, ps was showing the haproxy working:

``` shell
bash-4.3# ps -ef
PID   USER     TIME   COMMAND
    1 root       0:00 /service_loadbalancer --tcp-services=nginxsvc:80
   18 root       0:00 bash
   37 root       0:00 haproxy -f /etc/haproxy/haproxy.cfg -p /var/run/haproxy.pid -D -sf
   38 root       0:00 ps -ef
```

And I could reach the proxy in my browser, the message was ""not found"".
After starting the nginx-service (from the template), I could reach it via it's private Pod-IP.

``` shell
curl 10.1.101.3
<html>
<body style=""background:red"">
<h1>1.0.0</h1>
</body>
</html>
```

(Note here: I changed the default container to the coreos/example:1.0.0 because the other didn't start correctly)

When going back to my browser and trying to access it under /nginxsvc, I just get ""Not Found""...
It seems like the nginx-Pod does not get registered in the haproxy.

I used the following files:
**nginx-app.yml**

``` shell
apiVersion: v1
kind: Service
metadata:
  name: nginxsvc
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    name: http
  - port: 443
    protocol: TCP
    name: https
  selector:
    app: nginx

---
apiVersion: v1
kind: ReplicationController
metadata:
  name: my-nginx
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginxhttps
        image: coreos/example:1.0.0
        ports:
        - containerPort: 443
        - containerPort: 80
```

**rc.yaml**

``` shell
apiVersion: v1
kind: ReplicationController
metadata:
  name: service-loadbalancer
  labels:
    app: service-loadbalancer
    version: v1
spec:
  replicas: 1
  selector:
    app: service-loadbalancer
    version: v1
  template:
    metadata:
      labels:
        app: service-loadbalancer
        version: v1
    spec:
      nodeSelector:
        role: loadbalancer
      containers:
      - image: gcr.io/google_containers/servicelb:0.2
        imagePullPolicy: IfNotPresent
        name: haproxy
        ports:
        # All http services
        - containerPort: 80
          hostPort: 80
          protocol: TCP
        # haproxy stats
        - containerPort: 1936
          hostPort: 1936
          protocol: TCP
        resources: {}
        args:
        - --tcp-services=nginxsvc:80
```

Every help would be appreciated.
",closed,False,2015-12-24 11:53:08,2016-05-12 11:47:29
contrib,dalanlan,https://github.com/kubernetes/contrib/pull/343,https://api.github.com/repos/kubernetes/contrib/issues/343,validate sha1 sum for src file & dest file,"kill TODO

> // TODO: use the master election utility once it is merged in.

What does a master election utility exactly mean, if i may ask?
",closed,True,2015-12-30 08:44:48,2016-01-07 22:36:11
contrib,brendandburns,https://github.com/kubernetes/contrib/issues/344,https://api.github.com/repos/kubernetes/contrib/issues/344,GCE Ingress Controller leaks backend services,"We're seeing occasional leaks in e2e testing

@bprashanth 
",closed,False,2015-12-31 05:34:48,2016-08-19 14:39:17
contrib,dalanlan,https://github.com/kubernetes/contrib/pull/345,https://api.github.com/repos/kubernetes/contrib/issues/345,Use etcd client lib instead of go-etcd,,closed,True,2015-12-31 07:07:45,2016-01-07 22:36:36
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/346,https://api.github.com/repos/kubernetes/contrib/issues/346,AWS Ingress controller,"What follows are some very rough notes and a high level implementation sketch. Do not use it as an authoritative guide. When in doubt, please ask. 
# Motivation behind the Ingress 

These are what we expect the Ingress to support: https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/ingress.md

In short, the ingress is the resource someone would create to handle user traffic into the cluster. This involves name based virutal hosting, url routing, ssl termination etc (at least, in theory it can support ddos protection, firewalling, and a lot more -- think anything a CDN, nginx, haproxy does, but through a mixin of cloud and bare metal). This is convenient because clouds are great for HA and centralized certificate management, but bare metal counterparts have their strenghts too (or just cost cutting, or you already have hand tuned nginx configs, the list goes on...).

I'd suggest a rough sketch of how one might fulfill an Ingress on aws to begin with. 
# Rough implementation plan

The goal is to support an ingress like: https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/ingress.md#simple-fanout on aws. IIUC (I'm still learning about AWS) this will involve:
1. SSL termination through ELB
2. Leading to a bank of nodeport haproxies that implement the url/host routing

For 2 you can use the serviceloadbalancer (https://github.com/kubernetes/contrib/tree/master/service-loadbalancer), so lets focus on 1. Here's what you'll need to do:
- Implement support for a very basic Ingress: https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/ingress.md#single-service-ingress. When I create this it should trigger the aws controller (running in a pod in the cluster as a cluster addon) to facilitate HTTP routing to the specified backend service.
- Implement SSL support  
  - Create a secret with your certs
  - Add an annotation to the above ingress, this can be achieved via: kubectl annotate ing foo ssl='namespace/secretname' (http://kubernetes.io/v1.1/docs/user-guide/kubectl/kubectl_annotate.html)
    When I do this, the aws controller should create the appropriate cloud cert and open 443 for ssl traffic. 
- Replace your backend service with serviceloadbalancer, so you can use a single cert for all services in your cluster. You can ignore this for now.
# Overview of the gce controller

This is an overview of how the gce controller works, Don't let it limit your design capabilities :) https://github.com/kubernetes/contrib/tree/master/Ingress/controllers/gce#overview

In short: 
- There are a bunch of cloud resources we need to hook up to create a working l7 in gce. Each of thses is abstracted behind a pool interface. Each pool only knows how to handle one thing (eg: health checks, backends, instance groups, the l7 itself etc). 
- The bridge between the cloud provider and Kube are services of type node port. This is required because cloud providers don't allow us to shove random container ips into loadbalancers (yet). 
- The body of the gce controller only operates against the afore mentioned interfaces (https://github.com/kubernetes/contrib/blob/master/Ingress/controllers/gce/interfaces.go#L23), so you should be able to reuse a lot of that code. The job of the controller (https://github.com/kubernetes/contrib/blob/master/Ingress/controllers/gce/controller.go#L223) is to watch the kubeapi for changes to instances, services, ingresses and trigger the appropriate changes in the cloud objects, using one of the pools. I suggest not teaching the controller about cloud provider api directly. 
- If this controller feels too daunting, I suggest starting simple with a control loop like: https://github.com/kubernetes/contrib/blob/master/Ingress/controllers/nginx-alpha/controller.go. At the end of the day all you need is notifications when the Ingress in Kube changes. 
",closed,False,2016-01-03 06:14:10,2018-03-10 16:40:59
contrib,stephenwithav,https://github.com/kubernetes/contrib/issues/347,https://api.github.com/repos/kubernetes/contrib/issues/347,[ansible] how-to build certificates for a multi-master setup?,"A public/private key pair is only created for the first master.

How would I create and sign those pairs for the other masters?

As it stands, the Ansible playbook ends with a single failure on each of the masters except for the first.
",closed,False,2016-01-03 23:24:23,2018-03-09 07:07:35
contrib,ZhuPeng,https://github.com/kubernetes/contrib/pull/348,https://api.github.com/repos/kubernetes/contrib/issues/348,Add label selector,,closed,True,2016-01-04 03:52:39,2016-01-07 22:37:49
contrib,v1k0d3n,https://github.com/kubernetes/contrib/issues/349,https://api.github.com/repos/kubernetes/contrib/issues/349,"[ansible] vagrant openstack error: Multiple possible networks found, use a Network ID to be more specific.","No matter what I seem to do, this error doesn't go away. I have added the variable in the openstack_config.yml file via NetName and NetID. I have added it directly to the vagrant file, thinking that would resolve the issue...no luck. Are others having this issue as well?
",closed,False,2016-01-04 22:22:38,2018-02-14 02:02:10
contrib,linfan,https://github.com/kubernetes/contrib/pull/350,https://api.github.com/repos/kubernetes/contrib/issues/350,original 1.1 docs,"Contents all files in original v1.1 docs, except for the man folder and warning.png image
",closed,True,2016-01-05 05:06:18,2016-07-08 17:07:50
contrib,aledbf,https://github.com/kubernetes/contrib/pull/351,https://api.github.com/repos/kubernetes/contrib/issues/351,Update godep and go code to fix compilation errors,,closed,True,2016-01-05 14:37:04,2016-01-08 23:35:20
contrib,remoe,https://github.com/kubernetes/contrib/issues/352,https://api.github.com/repos/kubernetes/contrib/issues/352,[service-loadbalancer] can't build service with docker volume,"I try to build service-loadbalancer with google/golang container:

``` shell
 docker run --rm -i -t -e GOBIN=/go/bin -v ""$PWD""/go:/go -v ""$PWD"":/usr/src/myapp -w /usr/src/myapp google/golang /bin/bash -c ""go get -v -d . && GO15VENDOREXPERIMENT=1 GO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags '-w' -o service_loadbalancer ./service_loadbalancer.go ./loadbalancer_log.go""
```

It throws the following errors:

```
# k8s.io/kubernetes/pkg/util
/go/src/k8s.io/kubernetes/pkg/util/resource_container_linux.go:37: unknown configs.Cgroup field 'AllowAllDevices' in struct literal
# k8s.io/kubernetes/pkg/util/parsers
/go/src/k8s.io/kubernetes/pkg/util/parsers/parsers.go:30: undefined: parsers.ParseRepositoryTag
```
",closed,False,2016-01-06 14:05:15,2016-06-27 13:41:28
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/353,https://api.github.com/repos/kubernetes/contrib/issues/353,Add election code and examples,,closed,True,2016-01-06 20:45:22,2019-03-11 22:40:13
contrib,resouer,https://github.com/kubernetes/contrib/pull/354,https://api.github.com/repos/kubernetes/contrib/issues/354,Self added into whitelist,"To simplify the work process
",closed,True,2016-01-07 08:35:30,2016-01-07 16:19:13
contrib,remoe,https://github.com/kubernetes/contrib/pull/355,https://api.github.com/repos/kubernetes/contrib/issues/355,[service-loadbalancer] Readme url update,,closed,True,2016-01-08 08:54:12,2016-01-13 19:08:05
contrib,gmarek,https://github.com/kubernetes/contrib/pull/356,https://api.github.com/repos/kubernetes/contrib/issues/356,Update kubernetes godep for contrib,,closed,True,2016-01-08 15:17:25,2016-01-08 15:17:35
contrib,gtcno,https://github.com/kubernetes/contrib/issues/357,https://api.github.com/repos/kubernetes/contrib/issues/357,[Nginx Ingress Loadbalancer] Support daemonset ,"Should the loadbalancer pods be created by a daemonset instead of a rc thus we can be able to ensure that the pods will always be present on, say edge nodes?
",closed,False,2016-01-08 18:20:31,2016-02-16 21:08:22
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/358,https://api.github.com/repos/kubernetes/contrib/issues/358,Godep update,"@mikedanese @gmarek @bprashanth 

This is a monster.  Lets get it in ASAP so I don't have to rebase.

Please review the code changes part, the godep commit is pretty much pro-forma.
",closed,True,2016-01-08 19:09:55,2016-01-08 22:14:03
contrib,timstclair,https://github.com/kubernetes/contrib/issues/359,https://api.github.com/repos/kubernetes/contrib/issues/359,[submit queue] Add submit-blocking label,"**Proposal:**
Add an `unmerged-dependencies` label, which prevents LGTM'd and otherwise good-to-go PRs from being automatically merged. Alternatively, a more general `no-merge` label could accomplish the same thing.

**Motivation:**
There are sometimes PRs which have a dependency on another (unmerged) PR. If the dependency is soft (code runs & tests pass), then there is a risk that if the _depending_ PR is LGTM'd before the _dependent_ PR, it can be submitted first.

Here is an example of this happening: https://github.com/kubernetes/kubernetes/pull/17548

/cc @erictune 
",closed,False,2016-01-08 19:48:55,2016-03-07 18:27:18
contrib,timstclair,https://github.com/kubernetes/contrib/pull/360,https://api.github.com/repos/kubernetes/contrib/issues/360,[mungegithub] Add  label to block submit-queue,"Add a `unmerged-dependencies` label that blocks automatic merging. I'm open to changing this to a more generic label, such as `no-merge`. See associated issue https://github.com/kubernetes/contrib/issues/359 for more details.
",closed,True,2016-01-08 20:04:04,2016-03-03 18:09:39
contrib,aledbf,https://github.com/kubernetes/contrib/pull/361,https://api.github.com/repos/kubernetes/contrib/issues/361,Add keepalived controller to provide virtual IP addresses,,closed,True,2016-01-08 22:26:44,2016-01-11 20:12:48
contrib,spxtr,https://github.com/kubernetes/contrib/pull/362,https://api.github.com/repos/kubernetes/contrib/issues/362,"Don't comment ""Labelling this PR as size/X"", that's redundant.","I haven't tested this. The comment doesn't tell us anything new.  The only reason I can think of for it to exist is if you want an email notification or something, but that seems like a stretch.
",closed,True,2016-01-08 22:45:02,2016-01-11 07:01:40
contrib,raggi,https://github.com/kubernetes/contrib/pull/363,https://api.github.com/repos/kubernetes/contrib/issues/363,go2docker: remove path assumptions for portability,"The linux amd64 package does not conform to the layout previously expected by
the code. The new code relies on introspection of the installed go runtime
rather than assumptions about the package layout.

Put another way: this makes go2docker work with the standard go linux package installed using the standard instructions.
",closed,True,2016-01-08 22:45:15,2016-02-22 18:14:51
contrib,aledbf,https://github.com/kubernetes/contrib/pull/364,https://api.github.com/repos/kubernetes/contrib/issues/364,Use api.NamespaceDefault instead of string default,,closed,True,2016-01-08 23:44:41,2016-01-09 18:06:15
contrib,donbeave,https://github.com/kubernetes/contrib/issues/365,https://api.github.com/repos/kubernetes/contrib/issues/365,[service-loadbalancer] Can't get client IP address,"I'm running Kubernetes cluster 1.1.3 with 2 node (1 master and 1 minion).

I'm also runing Bare Metal Service Load Balancer, logs here:

```
I0110 16:05:09.552916       1 service_loadbalancer.go:550] Creating new loadbalancer: {Name:haproxy ReloadCmd:./haproxy_reload Config:/etc/haproxy/haproxy.cfg Template:template.cfg Algorithm: startSyslog:false lbDefAlgorithm:source}
E0110 16:05:09.553154       1 service_loadbalancer.go:241] Get : unsupported protocol scheme """"
I0110 16:05:09.677771       1 service_loadbalancer.go:468] Sync triggered by service default/kubernetes
I0110 16:05:09.778026       1 service_loadbalancer.go:470] Requeuing default/kubernetes because of error: deferring sync till endpoints controller has synced
I0110 16:05:09.778148       1 service_loadbalancer.go:468] Sync triggered by service default/mysql
I0110 16:05:09.778290       1 service_loadbalancer.go:430] Found service: {Name:nginx Ep:[10.2.52.12:80] FrontendPort:80 Host: Algorithm:source SessionAffinity:true CookieStickySession:false}
I0110 16:05:09.778444       1 service_loadbalancer.go:430] Found service: {Name:kubernetes:443 Ep:[188.166.XXX.XXX:443] FrontendPort:80 Host: Algorithm:source SessionAffinity:true CookieStickySession:false}
I0110 16:05:09.778524       1 service_loadbalancer.go:430] Found service: {Name:mysql:3306 Ep:[10.2.52.10:3306] FrontendPort:3306 Host: Algorithm:source SessionAffinity:true CookieStickySession:false}
I0110 16:05:09.790881       1 service_loadbalancer.go:294] haproxy -- cat: can't open '/var/run/haproxy.pid': No such file or directory
[WARNING] 009/160509 (29) : config : 'option forwardfor' ignored for frontend 'mysql:3306' as it requires HTTP mode.
[WARNING] 009/160509 (29) : config : 'option forwardfor' ignored for backend 'mysql:3306' as it requires HTTP mode.
[WARNING] 009/160509 (29) : config : 'option forwardfor' ignored for frontend 'nginx' as it requires HTTP mode.
[WARNING] 009/160509 (29) : config : 'option forwardfor' ignored for backend 'nginx' as it requires HTTP mode.
I0110 16:05:09.790988       1 service_loadbalancer.go:468] Sync triggered by service default/nginx
I0110 16:05:09.791137       1 service_loadbalancer.go:430] Found service: {Name:kubernetes:443 Ep:[188.166.XXX.XXX:443] FrontendPort:80 Host: Algorithm:source SessionAffinity:true CookieStickySession:false}
I0110 16:05:09.791241       1 service_loadbalancer.go:430] Found service: {Name:mysql:3306 Ep:[10.2.52.10:3306] FrontendPort:3306 Host: Algorithm:source SessionAffinity:true CookieStickySession:false}
I0110 16:05:09.791320       1 service_loadbalancer.go:430] Found service: {Name:nginx Ep:[10.2.52.12:80] FrontendPort:80 Host: Algorithm:source SessionAffinity:true CookieStickySession:false}
I0110 16:05:09.803515       1 service_loadbalancer.go:294] haproxy -- [WARNING] 009/160509 (35) : config : 'option forwardfor' ignored for frontend 'mysql:3306' as it requires HTTP mode.
[WARNING] 009/160509 (35) : config : 'option forwardfor' ignored for backend 'mysql:3306' as it requires HTTP mode.
[WARNING] 009/160509 (35) : config : 'option forwardfor' ignored for frontend 'nginx' as it requires HTTP mode.
[WARNING] 009/160509 (35) : config : 'option forwardfor' ignored for backend 'nginx' as it requires HTTP mode.
I0110 16:05:09.803605       1 service_loadbalancer.go:468] Sync triggered by service default/kubernetes
I0110 16:05:09.803779       1 service_loadbalancer.go:430] Found service: {Name:kubernetes:443 Ep:[188.166.XXX.XXX:443] FrontendPort:80 Host: Algorithm:source SessionAffinity:true CookieStickySession:false}
I0110 16:05:09.803932       1 service_loadbalancer.go:430] Found service: {Name:mysql:3306 Ep:[10.2.52.10:3306] FrontendPort:3306 Host: Algorithm:source SessionAffinity:true CookieStickySession:false}
I0110 16:05:09.804075       1 service_loadbalancer.go:430] Found service: {Name:nginx Ep:[10.2.52.12:80] FrontendPort:80 Host: Algorithm:source SessionAffinity:true CookieStickySession:false}
I0110 16:05:09.814751       1 service_loadbalancer.go:294] haproxy -- [WARNING] 009/160509 (41) : config : 'option forwardfor' ignored for frontend 'mysql:3306' as it requires HTTP mode.
[WARNING] 009/160509 (41) : config : 'option forwardfor' ignored for backend 'mysql:3306' as it requires HTTP mode.
[WARNING] 009/160509 (41) : config : 'option forwardfor' ignored for frontend 'nginx' as it requires HTTP mode.
[WARNING] 009/160509 (41) : config : 'option forwardfor' ignored for backend 'nginx' as it requires HTTP mode.
```

But in the end, when I access nginx by IP address, I still see Kubernetes internal IP in remote_addr instead of real client IP:

```
10.2.52.14 - - [10/Jan/2016:16:06:01 +0000] ""GET / HTTP/1.1"" 200 612 ""-"" ""curl/7.43.0"" ""-""
10.2.52.14 - - [10/Jan/2016:16:06:07 +0000] ""GET / HTTP/1.1"" 200 612 ""-"" ""curl/7.43.0"" ""-""
10.2.52.14 - - [10/Jan/2016:16:06:08 +0000] ""GET / HTTP/1.1"" 200 612 ""-"" ""curl/7.43.0"" ""-""
```

Maybe I was forget to add something?...

By the way, this is a list of my pods:

```
> kubectl get po -o wide --all-namespaces

NAMESPACE     NAME                                      READY     STATUS    RESTARTS   AGE       NODE
default       mysql                                     1/1       Running   0          37m       188.166.231.YYY
default       nginx                                     1/1       Running   0          30m       188.166.231.YYY
default       service-loadbalancer-d6zck                1/1       Running   0          9m        188.166.231.YYY
kube-system   kube-apiserver-188.166.182.XXX            1/1       Running   0          7h        188.166.182.XXX
kube-system   kube-controller-manager-188.166.182.XXX   1/1       Running   0          7h        188.166.182.XXX
kube-system   kube-dns-v9-4sp3y                         3/3       Running   0          1h        188.166.231.YYY
kube-system   kube-podmaster-188.166.182.XXX            2/2       Running   0          7h        188.166.182.XXX
kube-system   kube-proxy-188.166.182.XXX                1/1       Running   0          7h        188.166.182.XXX
kube-system   kube-proxy-188.166.231.YYY                1/1       Running   0          1h        188.166.231.YYY
kube-system   kube-scheduler-188.166.182.XXX            1/1       Running   0          7h        188.166.182.XXX
```
",closed,False,2016-01-10 16:16:43,2016-04-17 10:29:16
contrib,aledbf,https://github.com/kubernetes/contrib/issues/366,https://api.github.com/repos/kubernetes/contrib/issues/366,service-loadbalancer should enable the TPROXY flag,"This flag is required to get the real IP address of the connection in TCP mode.
see #365
",closed,False,2016-01-10 18:17:29,2016-01-13 22:02:10
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/367,https://api.github.com/repos/kubernetes/contrib/issues/367,GCE Loadbalancer controller doesn't report events anymore,"From logs:

```
E0111 00:56:27.615692       1 event.go:252] Unsupported event type: 'ADD'
...
E0111 00:57:39.362128       1 event.go:252] Unsupported event type: 'CREATE'
```

Maybe https://github.com/kubernetes/contrib/pull/358
",closed,False,2016-01-11 03:17:05,2016-01-26 05:51:48
contrib,aledbf,https://github.com/kubernetes/contrib/pull/368,https://api.github.com/repos/kubernetes/contrib/issues/368,Update go-github godep,"see https://github.com/google/go-github/issues/253
",closed,True,2016-01-11 19:19:12,2016-01-11 19:26:46
contrib,scheeles,https://github.com/kubernetes/contrib/pull/369,https://api.github.com/repos/kubernetes/contrib/issues/369,Update ansible etcd version to 2.2.3,,closed,True,2016-01-11 21:43:39,2016-03-20 15:29:13
contrib,wstrange,https://github.com/kubernetes/contrib/issues/370,https://api.github.com/repos/kubernetes/contrib/issues/370,git-sync should have an option to exit instead of looping forever,"Currently git-sync loops forever (with a 0 second delay) to sync changes from a git repo.

There are cases where you want  to sync once and then exit (Jobs, for example). 

The delay for syncing should probably be set to something more friendly (30 seconds?)
",closed,False,2016-01-13 01:24:43,2016-05-04 03:55:39
contrib,gmarek,https://github.com/kubernetes/contrib/pull/371,https://api.github.com/repos/kubernetes/contrib/issues/371,godep update,"Update K8s version. cc @mikedanese @smarterclayton 
",closed,True,2016-01-13 09:52:50,2016-01-13 11:16:29
contrib,gmarek,https://github.com/kubernetes/contrib/pull/372,https://api.github.com/repos/kubernetes/contrib/issues/372,Add a tool for comparing data embedded in test results,"Depends on the godep update.

Change of https://github.com/kubernetes/kubernetes/pull/19364 by creating it in contrib project. cc @mwielgus 
",closed,True,2016-01-13 10:13:29,2016-01-13 11:16:35
contrib,aledbf,https://github.com/kubernetes/contrib/pull/373,https://api.github.com/repos/kubernetes/contrib/issues/373,Remove old haproxy childs after reload,"- Update haproxy to 1.6.3
- Use dumb-init to remove old haproxy child processes
  https://github.com/Yelp/dumb-init
- Fix template identation
- Remove confusing logs messages

closes #321
closes #366
",closed,True,2016-01-13 17:37:05,2016-01-14 16:15:19
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/374,https://api.github.com/repos/kubernetes/contrib/issues/374,remove global Godeps and create one per subproject,,closed,True,2016-01-13 19:48:27,2016-01-19 18:02:39
contrib,remoe,https://github.com/kubernetes/contrib/pull/375,https://api.github.com/repos/kubernetes/contrib/issues/375, [service-loadbalancer] SSL termination support with HAProxy,"This pull request adds support for SSL termination.
## Installation instructions
- Add annotations to your service:

``` yaml
metadata:
  name: myservice
  annotations:
    serviceloadbalancer/lb.sslTerm: ""true""
    serviceloadbalancer/lb.aclMatch: ""-i /t1 /t2""
  labels:
```
- Create a secret with one of your favorite tool
- Update your loadbalancer POD to something like the following

``` yaml
        ports:
        # All http services
        - containerPort: 80
          hostPort: 80
          protocol: TCP
        # ssl term
        - containerPort: 443
          hostPort: 443
          protocol: TCP       
        # haproxy stats
        - containerPort: 1936
          hostPort: 1936
          protocol: TCP
        resources: {}     
        volumes:
        - name: secret-volume
          secret:
            secretName: my-secret
        volumeMounts:
        - mountPath: ""/ssl""
          name: secret-volume
```
- Add your SSL configuration to loadbalancer POD

``` yaml
      args:
      - --ssl-cert=/ssl/crt.pem
      - --ssl-ca-cert=/ssl/ca.crt
      - --namespace=default
```
- Define your https-frontend section in template.cfg

```
{{ if ne .sslCert """" }}
frontend httpsfrontend
    mode http
    bind :443 ssl {{ .sslCert }} ciphers ALL:!HIGH:!LOW:!MD5:!SEED:!EXP:MEDIUM:RC4-SHA verify required

{{range $i, $svc := .services.httpsTerm}}
    acl url_acl_{{$svc.Name}} path_beg {{$svc.AclMatch}}
    {{ if $svc.Host }}acl host_acl_{{$svc.Name}} hdr(host) {{$svc.Host}}
    use_backend {{$svc.Name}} if url_acl_{{$svc.Name}} or host_acl_{{$svc.Name}}
    {{ else }}use_backend {{$svc.Name}} if url_acl_{{$svc.Name}}
{{ end }}
{{end}}
{{end}}
```

Im not that experienced in english to be able to update the readme for instructions. I hope someone can help to finish this up.
",closed,True,2016-01-13 21:04:23,2016-01-19 16:16:24
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/376,https://api.github.com/repos/kubernetes/contrib/issues/376,Add a tool to automatically compare test results,"@gmarek feel free to close this or do whatever with it

depends on #374
",closed,True,2016-01-13 21:39:52,2016-01-15 07:22:36
contrib,aledbf,https://github.com/kubernetes/contrib/pull/377,https://api.github.com/repos/kubernetes/contrib/issues/377,Remove unused dependency,,closed,True,2016-01-13 22:24:17,2016-01-26 16:37:20
contrib,donbeave,https://github.com/kubernetes/contrib/issues/378,https://api.github.com/repos/kubernetes/contrib/issues/378,[service-loadbalancer] Ability to specify more then one host to service,"Any ideas? Is it good, or for every host need to create one service?
",closed,False,2016-01-14 06:01:14,2018-02-12 16:30:12
contrib,gmarek,https://github.com/kubernetes/contrib/pull/379,https://api.github.com/repos/kubernetes/contrib/issues/379,Test result comparator - main.,"cc @wojtek-t @mikedanese @davidopp @fgrzadkowski @mwielgus @piosz 

I put godeps in this directory, assuming that @mikedanese refactor will go through. 

Originally I had this in few commits, but after fun with godeps I have only one now. If requested I'll split it for easier review. (I'll certainly split off godeps as a first commit).

I'm sending it now to figure out who can is willing to review this tool. Normally it'd be @wojtek-t, but he's out until Feb and I'd rather have it merged sooner - it's not strictly necessary, as it's a standalone tool, so I won't need to do any rebases and it'll be available as soon as some reasonable PR is created.

I think that @mwielgus is the second best candidate, but I'm not sure he'll have time.
",closed,True,2016-01-14 15:36:55,2016-02-01 18:50:10
contrib,0rc,https://github.com/kubernetes/contrib/issues/380,https://api.github.com/repos/kubernetes/contrib/issues/380,change exec-healthz container so that it's based on alpine,"On my current project we are heavily using kubernetes and I’ve found healthz container very useful.
However, it can become even more useful if we can add command-line tools to perform more sophisticated health checks, like curl.
That is why I think changing base image to alpine may be helpful for other people as well. Container size will grow from about 7 to 11 Mb, but it will become extendable. People can just build their own containers based on original one and add any command-line tools they need for their health checks without changing original container.
",closed,False,2016-01-14 16:47:21,2016-01-27 07:37:34
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/381,https://api.github.com/repos/kubernetes/contrib/issues/381,Add a README,,closed,True,2016-01-15 06:34:26,2016-01-15 07:22:21
contrib,gmarek,https://github.com/kubernetes/contrib/pull/382,https://api.github.com/repos/kubernetes/contrib/issues/382,Test result comparator. First commit.,"cc @mikedanese @davidopp 
",closed,True,2016-01-15 09:14:42,2016-01-15 17:44:23
contrib,gmarek,https://github.com/kubernetes/contrib/pull/383,https://api.github.com/repos/kubernetes/contrib/issues/383,Test result comparator. Fix godeps,,closed,True,2016-01-15 10:58:08,2016-01-15 11:04:16
contrib,gmarek,https://github.com/kubernetes/contrib/pull/384,https://api.github.com/repos/kubernetes/contrib/issues/384,Add kubernetes-test-go to build blockers,"cc @ihmccreery @mikedanese @brendandburns
",closed,True,2016-01-15 11:34:16,2016-01-21 22:33:19
contrib,gmarek,https://github.com/kubernetes/contrib/pull/385,https://api.github.com/repos/kubernetes/contrib/issues/385,Utility for extrating test summaries from test results.,"cc @mikedanese @davidopp @wojtek-t @fgrzadkowski 

Assigning @mwielgus, as it'll be probably most convenient.
",closed,True,2016-01-15 17:54:29,2016-01-21 16:45:57
contrib,gmarek,https://github.com/kubernetes/contrib/pull/386,https://api.github.com/repos/kubernetes/contrib/issues/386,Comparator for log summaries.,"cc @mikedanese @davidopp @wojtek-t @fgrzadkowski

Assigning @mwielgus, as it'll be probably most convenient.
",closed,True,2016-01-15 18:01:22,2016-01-21 16:47:01
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/387,https://api.github.com/repos/kubernetes/contrib/issues/387,verify-flags-underscore generates wrong regex ,"Without https://github.com/kubernetes/contrib/pull/375, which added 2 flags, it raised no warnings for the nginx.tmpl file, but with it, it dumped out https://github.com/kubernetes/contrib/pull/375#issuecomment-172625212. I've asked that the author of the pr adds those lines to the exceptions file, we could also just ignore .tmpl files, but it still smells like a regex compiler bug.
",closed,False,2016-01-18 22:05:02,2018-02-12 16:30:12
contrib,rnagarajan12,https://github.com/kubernetes/contrib/issues/388,https://api.github.com/repos/kubernetes/contrib/issues/388,service loadbalancer - unsupported protocol scheme,"I am running kube cluster with three servers acting as master and node.
Overlay networking is flannel.

I0119 15:53:00.203897       1 service_loadbalancer.go:550] Creating new loadbalancer: {Name:haproxy ReloadCmd:./haproxy_reload Config:/etc/haproxy/haproxy.cfg Template:template.cfg Algorithm: startSyslog:false lbDefAlgorithm:roundrobin}
E0119 15:53:00.204072       1 service_loadbalancer.go:241] Get : unsupported protocol scheme """"
I0119 15:53:00.204514       1 loadbalancer_log.go:38] Starting syslog server for haproxy using /var/run/haproxy.log.socket as socket

This is the error, when I run rc.yaml,

cat rc.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: service-loadbalancer
  labels:
    app: service-loadbalancer
    version: v1
spec:
  replicas: 1
  selector:
    app: service-loadbalancer
    version: v1
  template:
    metadata:
      labels:
        app: service-loadbalancer
        version: v1
    spec:
      nodeSelector:
        role: loadbalancer
      containers:
      - image: gcr.io/google_containers/servicelb:0.2
        imagePullPolicy: Always
        name: haproxy
        ports:
        # All http services
        - containerPort: 8080
          hostPort: 80
          protocol: TCP
        # haproxy stats
        - containerPort: 1936
          hostPort: 1936
          protocol: TCP
        resources: {}
        args:
        - --tcp-services=hellotom-svc:8080
        - --syslog

$ kubectl get svc
NAME           CLUSTER_IP       EXTERNAL_IP   PORT(S)    SELECTOR        AGE
hellotom-svc   10.254.230.169   <none>        8080/TCP   app=hello-tom   2h
",closed,False,2016-01-19 16:03:12,2016-01-21 14:03:28
contrib,aledbf,https://github.com/kubernetes/contrib/pull/389,https://api.github.com/repos/kubernetes/contrib/issues/389,Show valid options,"Replace error

```
$ ./hack/for-go-proj.sh
./hack/for-go-proj.sh: line 25: 1: unbound variable
```

with

```
$ ./hack/for-go-proj.sh
missing subcommand: [build|install|test]
```
",closed,True,2016-01-19 18:24:36,2016-02-23 12:49:42
contrib,ixdy,https://github.com/kubernetes/contrib/pull/390,https://api.github.com/repos/kubernetes/contrib/issues/390,Add ihmccreery and spxtr to blunderbuss config,"@ihmccreery @spxtr 
",closed,True,2016-01-20 22:54:30,2016-01-20 23:42:07
contrib,ixdy,https://github.com/kubernetes/contrib/pull/391,https://api.github.com/repos/kubernetes/contrib/issues/391,Update Travis to Go 1.5.3,"@ihmccreery @mikedanese @eparis 
",closed,True,2016-01-20 23:10:26,2016-01-21 20:11:31
contrib,gmarek,https://github.com/kubernetes/contrib/pull/392,https://api.github.com/repos/kubernetes/contrib/issues/392,Comparator for resource usage.,"cc @fgrzadkowski @wojtek-t 
",closed,True,2016-01-21 16:24:31,2016-02-01 12:23:12
contrib,vishh,https://github.com/kubernetes/contrib/pull/393,https://api.github.com/repos/kubernetes/contrib/issues/393,Remove Daniel Marti `mvdan` from the munger whitelist.,"cc @mvdan 
",closed,True,2016-01-21 18:12:01,2016-01-21 18:43:01
contrib,gmarek,https://github.com/kubernetes/contrib/pull/394,https://api.github.com/repos/kubernetes/contrib/issues/394,Update perfdash with new performance tag,"cc @wojtek-t 
",closed,True,2016-01-21 19:00:29,2016-02-02 09:04:33
contrib,jojimt,https://github.com/kubernetes/contrib/issues/395,https://api.github.com/repos/kubernetes/contrib/issues/395,TASK: [kubernetes-addons] fails for heapster-controller.yaml.j2,"Seeing this error since the last hour or so:
TASK: [kubernetes-addons | MONITORING | Download monitoring files from Kubernetes repo] **\* 
ok: [k8master -> 127.0.0.1] => (item=grafana-service.yaml)
changed: [k8master -> 127.0.0.1] => (item=heapster-controller.yaml)
ok: [k8master -> 127.0.0.1] => (item=heapster-service.yaml)
ok: [k8master -> 127.0.0.1] => (item=influxdb-grafana-controller.yaml)
ok: [k8master -> 127.0.0.1] => (item=influxdb-service.yaml)

TASK: [kubernetes-addons | MONITORING | Convert pillar vars to ansible vars for monitoring files] **\* 
ok: [k8master -> 127.0.0.1] => (item=grafana-service.yaml)
changed: [k8master -> 127.0.0.1] => (item=heapster-controller.yaml)
ok: [k8master -> 127.0.0.1] => (item=heapster-service.yaml)
ok: [k8master -> 127.0.0.1] => (item=influxdb-grafana-controller.yaml)
ok: [k8master -> 127.0.0.1] => (item=influxdb-service.yaml)

TASK: [kubernetes-addons | MONITORING | Install template from converted saltfile] **\* 
ok: [k8master] => (item=grafana-service.yaml)
fatal: [k8master] => {'msg': ""AnsibleError: file: /tmp/kubernetes/addons/cluster-monitoring/heapster-controller.yaml.j2, line number: 2, error: no test named 'None'"", 'failed': True}
fatal: [k8master] => {'msg': 'One or more items failed.', 'failed': True, 'changed': False, 'results': [{'group': 'root', 'uid': 0, 'changed': False, 'owner': 'root', 'item': 'grafana-service.yaml', 'state': 'file', 'gid': 0, 'secontext': 'system_u:object_r:etc_t:s0', 'mode': '0755', 'invocation': {'module_name': 'template', 'module_args': ''}, 'path': '/etc/kubernetes/addons/cluster-monitoring/grafana-service.yaml', 'size': 431}, {'msg': ""AnsibleError: file: /tmp/kubernetes/addons/cluster-monitoring/heapster-controller.yaml.j2, line number: 2, error: no test named 'None'"", 'failed': True}]}

FATAL: all hosts have already failed -- aborting

Any ideas/suggestions?
",closed,False,2016-01-21 19:55:06,2018-02-14 01:01:13
contrib,ixdy,https://github.com/kubernetes/contrib/issues/396,https://api.github.com/repos/kubernetes/contrib/issues/396,[mungegithub] submit queue e2e status page should link to logs on GCS,"Recommendation from @justinsb: the http://submit-queue.k8s.io/ e2e status page should link to the logs on storage.cloud.google.com, ideally the most build # corresponding to the status shown.

For example, for `kubernetes-e2e-gce` build 10000, we'd link to https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gce/10000/.

Whenever we get around to a better external UI, we can link to that instead.

cc @kubernetes/goog-testing @eparis @lavalamp @brendandburns 
",closed,False,2016-01-22 03:56:58,2016-01-26 22:17:11
contrib,paulbakker,https://github.com/kubernetes/contrib/pull/397,https://api.github.com/repos/kubernetes/contrib/issues/397,Fixed incorrect environment variable,"Was using the GIT_SYNC_BRANCH twice.
",closed,True,2016-01-24 17:04:48,2016-02-13 05:22:50
contrib,paulbakker,https://github.com/kubernetes/contrib/pull/398,https://api.github.com/repos/kubernetes/contrib/issues/398,Set file permissions so another container can read them,"When used on Kubernetes as a side-car, the other container should have read permissions to the cloned files. This is not necessarily the case without explicitly setting it.

I ran into this problem in combination with an Nginx container and a shared volume.
",closed,True,2016-01-24 17:06:33,2016-02-13 05:22:02
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/399,https://api.github.com/repos/kubernetes/contrib/issues/399,Refactor L7 modules and use a single instance group,"First commit is part of an overdue refactor, second adds all ports to a single IG.
",closed,True,2016-01-25 17:21:48,2016-01-26 05:44:38
contrib,aledbf,https://github.com/kubernetes/contrib/pull/400,https://api.github.com/repos/kubernetes/contrib/issues/400,[WIP] Use unbound as caching DNS resolver,"This two gist contains the benchmark between current implementation (skydns) and unbound
- https://gist.github.com/aledbf/bc38682df4e0d0daf4c3
- https://gist.github.com/aledbf/c8212143b88b1aa45a35

From this benchmark I see some interesting differences:
- skydns latency is 5 times the latency of unbound
- skydns uses 100% of the CPU (unbound just 34%) during the test
- ~~skydns loses 7.69% of the requests~~  https://github.com/kubernetes/contrib/pull/400#issuecomment-175382313
- skydns QPS is 4 times lower than unbound
",closed,True,2016-01-25 19:01:26,2016-04-27 16:56:03
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/401,https://api.github.com/repos/kubernetes/contrib/issues/401,[mungegithub] Add a new munger that forces people to file or reference flaky tests,"@ixdy @spxtr @ihmccreery 

Ref: https://github.com/kubernetes/kubernetes/issues/20097
",closed,True,2016-01-26 00:50:47,2016-01-28 16:21:19
contrib,patrickeasters,https://github.com/kubernetes/contrib/pull/402,https://api.github.com/repos/kubernetes/contrib/issues/402,Resolved ansible errors in cluster-monitoring setup,"Due to changes made in commit [3da8d80](https://github.com/kubernetes/kubernetes/commit/3da8d80187b12db4e505bd2be5ef41dc819f0506]) in the kubernetes repo, this ansible playbook fails when trying to convert the pillar vars to ansible vars. This resolves that by changing the regex to match the new logic in the template.

I tested this in my lab and am able to setup cluster monitoring via the ansible role successfully now.

This also is reported in issue [395](https://github.com/kubernetes/contrib/issues/395)
",closed,True,2016-01-26 17:13:50,2016-01-26 20:33:50
contrib,eparis,https://github.com/kubernetes/contrib/pull/403,https://api.github.com/repos/kubernetes/contrib/issues/403,[submit queue] Update manual whitelists,,closed,True,2016-01-26 17:15:32,2016-01-28 16:22:09
contrib,eparis,https://github.com/kubernetes/contrib/pull/404,https://api.github.com/repos/kubernetes/contrib/issues/404,[submit queue] Link to logs for CI failures,"resolves #396
",closed,True,2016-01-26 17:29:23,2016-01-28 16:22:07
contrib,eparis,https://github.com/kubernetes/contrib/pull/405,https://api.github.com/repos/kubernetes/contrib/issues/405,[mungegithub] Helpers to list and delete comments,,closed,True,2016-01-26 20:25:15,2016-01-28 16:50:56
contrib,danehans,https://github.com/kubernetes/contrib/issues/406,https://api.github.com/repos/kubernetes/contrib/issues/406,Ansible: Add CoreOS Support,"I would like to add CoreOS support to the contrib/ansible project. I have a working implementation at [1]. Keep in mind [1] is very rough and still needs a lot of work before submitting a PR. It would be helpful to discuss design decisions with the project maintainer before submitting PRs. Your feedback would be greatly appreciated.

[1] https://github.com/danehans/contrib/tree/coreos
",closed,False,2016-01-26 23:13:46,2018-02-14 01:01:11
contrib,yujuhong,https://github.com/kubernetes/contrib/issues/407,https://api.github.com/repos/kubernetes/contrib/issues/407,[submit queue] should process older PRs sooner,"Currently, submit queue sorts the PRs based on PR number. This could be very misleading because many PRs stay open for a long time and become ready to merge only recently. We should have a better metric to gauge how long the PR has stayed in the queue and respect the order when processing. 

One idea is to sort the PRs by the lgtm label timestamps, so that the PR that has been ready the longest time gets to be picked up first.
",closed,False,2016-01-27 00:49:55,2018-02-13 16:53:10
contrib,eparis,https://github.com/kubernetes/contrib/pull/408,https://api.github.com/repos/kubernetes/contrib/issues/408,[submit queue] Order by priority label then by PR number,,closed,True,2016-01-27 16:32:08,2016-01-28 16:22:08
contrib,vishh,https://github.com/kubernetes/contrib/pull/409,https://api.github.com/repos/kubernetes/contrib/issues/409,Add Jimmi Dyson to the whitelist.,,closed,True,2016-01-27 19:23:48,2016-01-27 19:45:29
contrib,timothysc,https://github.com/kubernetes/contrib/issues/410,https://api.github.com/repos/kubernetes/contrib/issues/410,RFE: Add benchmark suite for stack performance analysis. (Stack Conformance),"As mentioned on @kubernetes/sig-node, ideally it would make a lot of sense to have a set of benchmarks amalgamated to generate reports.  In order to determine stack conformance, as well as performance or acceptance criteria.  

e.g. - docker 1.X = bad vintage and here's the data we all agree shows why. 

Which including rolling together several other bench-marking tools:
https://github.com/Random-Liu/docker_micro_benchmark

/cc @ncdc @Random-Liu @jeremyeder @kubernetes/sig-scalability 
",closed,False,2016-01-27 20:19:06,2018-02-12 19:32:09
contrib,sjpotter,https://github.com/kubernetes/contrib/issues/411,https://api.github.com/repos/kubernetes/contrib/issues/411,kube-up fails to upload release tars to gce if prefix changes,"Currently kube-up determines if it should stage the tars based on if the sha1 file exists.  However, as the staging is based on KUBE_GCE_INSTANCE_PREFIX, if one changes the prefix fetching the tars fails as they aren't where they are expected to be.
",closed,False,2016-01-27 20:52:55,2016-03-11 21:40:25
contrib,eparis,https://github.com/kubernetes/contrib/pull/412,https://api.github.com/repos/kubernetes/contrib/issues/412,[submit queue] fix flaky test,,closed,True,2016-01-28 03:47:51,2016-01-28 16:22:07
contrib,pmorie,https://github.com/kubernetes/contrib/pull/413,https://api.github.com/repos/kubernetes/contrib/issues/413,[submit queue] Add comment when PR requires rebase,"@eparis 
",closed,True,2016-01-28 04:26:27,2016-01-28 17:14:16
contrib,eparis,https://github.com/kubernetes/contrib/pull/414,https://api.github.com/repos/kubernetes/contrib/issues/414,Minor cleanups for retest munger,,closed,True,2016-01-28 16:30:29,2016-01-28 16:37:09
contrib,eparis,https://github.com/kubernetes/contrib/pull/415,https://api.github.com/repos/kubernetes/contrib/issues/415,Allow 2016 in boilerplate,,closed,True,2016-01-28 16:33:59,2016-01-28 16:37:08
contrib,eparis,https://github.com/kubernetes/contrib/pull/416,https://api.github.com/repos/kubernetes/contrib/issues/416,[submit queue] Leave queue intact when e2e tests are failing,"This means that you can always see the queue in the web interface, we'll
always merge in order, etc.

You'll also get a single message in the history when the queue
breaks/repairs, instead of a flood....
",closed,True,2016-01-28 16:44:22,2016-02-01 18:32:39
contrib,eparis,https://github.com/kubernetes/contrib/pull/417,https://api.github.com/repos/kubernetes/contrib/issues/417,[mungegithub] Moar tests!,,closed,True,2016-01-28 16:49:02,2016-01-28 16:50:56
contrib,eparis,https://github.com/kubernetes/contrib/pull/418,https://api.github.com/repos/kubernetes/contrib/issues/418,[submit queue] Update angular material to something actually released,,closed,True,2016-01-28 16:53:00,2016-01-28 16:53:25
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/419,https://api.github.com/repos/kubernetes/contrib/issues/419,[WIP] Block submit queue on serial jobs,"... in preparation for making others parallel

_Do not merge_ until these jobs are green (blocked on https://github.com/kubernetes/kubernetes/pull/19667).
",closed,True,2016-01-28 17:45:12,2016-02-08 16:48:24
contrib,dyep49,https://github.com/kubernetes/contrib/pull/420,https://api.github.com/repos/kubernetes/contrib/issues/420,Fix events for nginx-third-party ingress,"Fixes #367 for nginx-third-party ingress controller mirroring the changes in https://github.com/bprashanth/contrib/commit/3ef7578f680e54d62fdc1e34098ae19752d82e38
",closed,True,2016-01-28 18:20:50,2016-02-09 21:29:36
contrib,eparis,https://github.com/kubernetes/contrib/pull/421,https://api.github.com/repos/kubernetes/contrib/issues/421,[submit queue] Do not mis-order the submit queue in the web site,"We shouldn't sort it in javascript when the server gave it in order
",closed,True,2016-01-28 19:25:35,2016-01-28 20:23:07
contrib,pmorie,https://github.com/kubernetes/contrib/issues/422,https://api.github.com/repos/kubernetes/contrib/issues/422,Possible for submit queue to merge a sha other than what was tested,"There's a race condition in the submit queue where the queue may merge a sha other than what was tested:
For example: kubernetes/kubernetes#19716 : this PR was 5/5 checks green, then needed rebase after the merge bot had called for a final test.  I rebased, repushed, and re-added the LGTM label, and the PR merged even though 1/5 checks was green (presumably CLA check) in github for that sha.

@eparis 
",closed,False,2016-01-28 22:59:46,2016-11-08 23:49:29
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/423,https://api.github.com/repos/kubernetes/contrib/issues/423,Block submit-queue on new jobs,"- put `-test-go` at the top with `-build`
- rename `-gke-ci` to `-gke` (https://github.com/kubernetes/kubernetes/pull/20278)
- add `-gce-slow` and `-gke-slow` (https://github.com/kubernetes/kubernetes/pull/20278)
- remove `-gce-reboot` (will eventually be included in `-gce-serial` if we want to block submit queue on it)
- remove `-gce-parallel` (now covered by `-gce`; see https://github.com/kubernetes/kubernetes/pull/20278)
- remove `-autoscaling` (all GA tests now in `-slow`; see https://github.com/kubernetes/kubernetes/pull/20274)
",closed,True,2016-01-29 17:32:14,2016-01-29 17:47:20
contrib,eparis,https://github.com/kubernetes/contrib/pull/424,https://api.github.com/repos/kubernetes/contrib/issues/424,[mungegithub] Minor syntax fixes in the retest munger,,closed,True,2016-01-29 19:02:08,2016-01-29 21:40:56
contrib,eparis,https://github.com/kubernetes/contrib/pull/425,https://api.github.com/repos/kubernetes/contrib/issues/425,[mungegithub] Include the #IGNORE when asking merge bot to retest,"We know what we are doing and never do it because of a flake.
",closed,True,2016-01-29 22:20:16,2016-03-10 17:16:37
contrib,wienczny,https://github.com/kubernetes/contrib/issues/426,https://api.github.com/repos/kubernetes/contrib/issues/426,kube-keepalived-vip:0.1 not available at gcr.io,"The docker image referenced from project keepalived-vip is not available at gcr.io.

> # docker pull gcr.io/google_containers/kube-keepalived-vip:0.1
> Pulling repository gcr.io/google_containers/kube-keepalived-vip
> Tag 0.1 not found in repository gcr.io/google_containers/kube-keepalived-vip`
",closed,False,2016-01-30 22:14:15,2016-02-04 02:19:21
contrib,wienczny,https://github.com/kubernetes/contrib/issues/427,https://api.github.com/repos/kubernetes/contrib/issues/427,keepalived-vip using alpine package ,"The keepalived-vip is building keepalived from source. Since some days keepalived is available in apline testing (https://pkgs.alpinelinux.org/package/testing/x86_64/keepalived). I suppose it has not been availablel when keealived-vip was built. Are there any reasons not to use the repository package? If there are please document them ;)
",closed,False,2016-01-30 22:52:44,2016-02-01 17:39:36
contrib,rsmitty,https://github.com/kubernetes/contrib/issues/428,https://api.github.com/repos/kubernetes/contrib/issues/428,Ansible: Flannel Install Fails on Ubuntu,"While attempting to deploy a k8s cluster on Ubuntu hosts, I've encountered an issue where flannel does not install. This is because of the [client.yml](https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/tasks/client.yml) file and the way that the installation is handled. It wrongly assumes that flannel is in the package repository, regardless of OS (as long as it's not AtomicOS). This will need to be fixed to install from github releases for Ubuntu and I presume Debian. I'll take this on when I have a free moment, if someone will go ahead and assign it to me.
",closed,False,2016-02-01 02:44:33,2018-02-14 01:01:12
contrib,rsmitty,https://github.com/kubernetes/contrib/issues/429,https://api.github.com/repos/kubernetes/contrib/issues/429,Ansible: Etcd Gets Symlinked to Wrong Location,"On Ubuntu hosts, etcd gets installed via a github release; however, the [symlink portion](https://github.com/kubernetes/contrib/blob/master/ansible/roles/etcd/tasks/github-release.yml#L27) of the play links to /usr/local/bin. This seems to be incorrect later in the run where flannel tries to look for etcdctl to exist at /usr/bin, as seen [here](https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/tasks/config.yml#L16). I suspect that the github release play simply needs to be updated to link to /usr/bin instead (this seems to fix it in my environment). I'll take this on later this week if someone can assign to me.
",closed,False,2016-02-01 02:49:54,2016-03-20 15:44:15
contrib,ekozan,https://github.com/kubernetes/contrib/issues/430,https://api.github.com/repos/kubernetes/contrib/issues/430,Memory Leak on nginx-third-party,"Hello,

I think they has an memory leak or no GC on the lua part of nginx-third-party
In stand-by the memory constantly grow of 6 mb per hour

Config standard less than 100 access / hours 

https://snapshot.raintank.io/dashboard/snapshot/iuAQVc2zfj2ikLFeXZQA9j8vlobcQ8EJ
the memory drop is a restart of the po
",closed,False,2016-02-01 11:13:17,2016-02-09 20:21:14
contrib,gmarek,https://github.com/kubernetes/contrib/pull/431,https://api.github.com/repos/kubernetes/contrib/issues/431,Update test log scraper - allow multiple test results in a single file.,"cc @wojtek-t 
",closed,True,2016-02-01 14:25:57,2016-02-01 14:55:43
contrib,eparis,https://github.com/kubernetes/contrib/pull/432,https://api.github.com/repos/kubernetes/contrib/issues/432,[mungegithub] Only run ping-ci test on master,,closed,True,2016-02-01 15:00:58,2016-02-01 18:32:40
contrib,gmarek,https://github.com/kubernetes/contrib/pull/433,https://api.github.com/repos/kubernetes/contrib/issues/433,Comparator for prometheus metrics.,"cc @wojtek-t 
",closed,True,2016-02-01 15:04:23,2016-02-01 15:52:59
contrib,eparis,https://github.com/kubernetes/contrib/pull/434,https://api.github.com/repos/kubernetes/contrib/issues/434,[mungegithub] New munger to rerun CI if it is 'pending' for 24 hours,"At times the CI can mess up, say it is testing a PR, but not actually be
testings.  This munger will request a retest when it finds a mergeable
PR with LGTM which has been 'pending' for 24 hours.

We should really retest every PR which is mergeable and has been
'pending' for greater than some length of time, but at least LGTM will
keep the jenkins queue from exploding...
",closed,True,2016-02-01 16:14:08,2016-02-01 18:32:40
contrib,aledbf,https://github.com/kubernetes/contrib/pull/435,https://api.github.com/repos/kubernetes/contrib/issues/435,Update keepalived to use alpine package,"closes #427
",closed,True,2016-02-01 16:54:48,2016-02-16 15:30:19
contrib,eparis,https://github.com/kubernetes/contrib/issues/436,https://api.github.com/repos/kubernetes/contrib/issues/436,[compare] test flake in TestMetricCompareFailure,"https://travis-ci.org/kubernetes/contrib/jobs/106259932
",closed,False,2016-02-01 17:29:47,2016-02-01 19:57:20
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/437,https://api.github.com/repos/kubernetes/contrib/issues/437,Bump kube-keepalived-vip to 0.2,"Use gcr.io/google_containers/kube-keepalived-vip:0.2 in examples.
@aledbf @wienczny 
",closed,True,2016-02-01 17:50:37,2016-02-01 23:02:24
contrib,eparis,https://github.com/kubernetes/contrib/pull/438,https://api.github.com/repos/kubernetes/contrib/issues/438,[submit queue] Update whitelist based on repo access,,closed,True,2016-02-01 18:37:01,2016-02-01 18:38:05
contrib,claytantor,https://github.com/kubernetes/contrib/issues/439,https://api.github.com/repos/kubernetes/contrib/issues/439,"Ansible: Etcd not available to dict ""groups""","https://github.com/kubernetes/contrib/blob/4d3c7d4e9599812872abc1ddd5536e290792efe7/ansible/roles/master/templates/apiserver.j2

When attempting to stand up a master node using:

```
INVENTORY=hosts ./setup.sh --tags=masters
```

The ansible setup for a master node fails when trying to write the config file to the api server:

```
TASK: [master | write the config file for the api server] *********************
fatal: [10.81.77.104] => {'msg': ""AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'etcd'"", 'failed': True}
fatal: [10.81.77.104] => {'msg': ""AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'etcd'"", 'failed': True}

FATAL: all hosts have already failed -- aborting
```

From the task file:

```
- name: write the config file for the api server
  template: src=apiserver.j2 dest={{ kube_config_dir }}/apiserver
  notify:
    - restart apiserver
```

The call in question seems to be that the groups dict does not have an `etcd` entry:

```
# Location of the etcd cluster
KUBE_ETCD_SERVERS=""--etcd-servers={% for node in groups['etcd'] %}http://{{ node }}:2379{% if not loop.last %},{% endif %}{% endfor %}""
```

If this is an error on my part please excuse, and an explanation would be appreciated. 
",closed,False,2016-02-01 19:23:24,2016-02-01 20:40:32
contrib,gmarek,https://github.com/kubernetes/contrib/pull/440,https://api.github.com/repos/kubernetes/contrib/issues/440,Make compare-metrics test ignore order,"Fix #436

cc @mwielgus 
",closed,True,2016-02-01 19:34:47,2016-02-01 19:57:20
contrib,rsmitty,https://github.com/kubernetes/contrib/pull/441,https://api.github.com/repos/kubernetes/contrib/issues/441,updated symlink endpoint,"This fixes the bug #429 
",closed,True,2016-02-01 20:01:31,2016-02-03 21:41:15
contrib,eparis,https://github.com/kubernetes/contrib/pull/442,https://api.github.com/repos/kubernetes/contrib/issues/442,[mungegithub] Changes to make test deployment easier,"Include an example service.yaml and secret.yaml
Stop updating the rc.yaml checked into the repo (create a local.rc.yaml)
Make the Makefile a bit more flexible instead of hard coding gcr.io
Include a default dryrun on kubernetes option
Add a make help option to explain everything
",closed,True,2016-02-01 20:50:20,2016-02-01 21:43:11
contrib,eparis,https://github.com/kubernetes/contrib/pull/443,https://api.github.com/repos/kubernetes/contrib/issues/443,[mungegithub] Allow both ro and rw deployments,,closed,True,2016-02-01 22:55:17,2016-02-02 23:13:18
contrib,eparis,https://github.com/kubernetes/contrib/pull/444,https://api.github.com/repos/kubernetes/contrib/issues/444,[mungegithub] Minor comment fix in stale-pending-ci,,closed,True,2016-02-01 23:58:13,2016-02-02 23:13:16
contrib,eparis,https://github.com/kubernetes/contrib/pull/445,https://api.github.com/repos/kubernetes/contrib/issues/445,[mungegithub] Properly ignore retest requests from robots,"We had a loop to test if the commit was from a robot. But on finding a
message from a robot we would break that test loop instead of the main
loop. So we would still remove the comment.

Convert to kubernetes sets to make it easy to read/understand.
",closed,True,2016-02-02 01:40:11,2016-02-02 23:13:17
contrib,eparis,https://github.com/kubernetes/contrib/pull/446,https://api.github.com/repos/kubernetes/contrib/issues/446,[mungegithub] Enable the retest request without issue report complainer,,closed,True,2016-02-02 02:10:43,2016-02-02 23:13:17
contrib,eparis,https://github.com/kubernetes/contrib/pull/447,https://api.github.com/repos/kubernetes/contrib/issues/447,[mungegithub] retest munger complaint message update for clarity,,closed,True,2016-02-02 03:56:24,2016-02-02 23:13:19
contrib,codablock,https://github.com/kubernetes/contrib/pull/448,https://api.github.com/repos/kubernetes/contrib/issues/448,[Ingress nginx-third-party] Allow using Ingress resources without host specified,,closed,True,2016-02-02 08:55:58,2016-05-10 16:47:11
contrib,eparis,https://github.com/kubernetes/contrib/pull/449,https://api.github.com/repos/kubernetes/contrib/issues/449,[submit queue] Add jsafrane to submit queue whitelist,,closed,True,2016-02-02 14:48:31,2016-03-02 14:03:56
contrib,eparis,https://github.com/kubernetes/contrib/pull/450,https://api.github.com/repos/kubernetes/contrib/issues/450,[submit queue] Refresh issues in the submit queue,"This actually has 2 side effects.
- If LGTM was removed from an issue after it was queued it could still merge
- If labels which affected the queue position were changed after it was queued the position would not be updated

Historically #1 wasn't a huge problem since the queue was constantly cleared by e2e flakes/failures and so we got fresh data. Now we aren't clearing the queue constantly the issue data could get way out of date.
",closed,True,2016-02-02 22:14:09,2016-03-02 14:03:58
contrib,eparis,https://github.com/kubernetes/contrib/pull/451,https://api.github.com/repos/kubernetes/contrib/issues/451,[mungegithub] Another test flake munger message tweak,"Looks prettier and won't actually cause the k8s-bot to retest twice
",closed,True,2016-02-02 23:24:29,2016-03-02 14:03:57
contrib,eparis,https://github.com/kubernetes/contrib/issues/452,https://api.github.com/repos/kubernetes/contrib/issues/452,[mungegithub] rebuild-request munger misses some requests,"https://github.com/kubernetes/kubernetes/pull/20464#issuecomment-179030471
",closed,False,2016-02-03 11:56:04,2017-01-20 22:53:16
contrib,eparis,https://github.com/kubernetes/contrib/issues/453,https://api.github.com/repos/kubernetes/contrib/issues/453,[mungegithub] rebuild-request bot should accept a URL not just #number,"URLs are easier to copy/past anyway.
",closed,False,2016-02-03 11:56:49,2017-01-21 01:14:21
contrib,gmarek,https://github.com/kubernetes/contrib/pull/454,https://api.github.com/repos/kubernetes/contrib/issues/454,Fix test tag and output coloring,,closed,True,2016-02-03 16:35:57,2016-02-04 09:14:07
contrib,rsmitty,https://github.com/kubernetes/contrib/issues/455,https://api.github.com/repos/kubernetes/contrib/issues/455,Ansible: Allow for use of Docker yum repositories,"This will allow for newer versions of Docker engine to be installed (CentOS 7 only has v1.8 in the repos). I'll add a pull request for this functionality.
",closed,False,2016-02-03 19:39:40,2016-08-24 18:33:27
contrib,rsmitty,https://github.com/kubernetes/contrib/pull/456,https://api.github.com/repos/kubernetes/contrib/issues/456,Ansible: Allow ability to install from Docker yum repos,"Fixes #455. This will allow for installation of newer versions of docker engine by editing the group_var for docker provider.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/456)

<!-- Reviewable:end -->
",closed,True,2016-02-03 21:03:01,2016-09-09 13:59:36
contrib,rsmitty,https://github.com/kubernetes/contrib/pull/457,https://api.github.com/repos/kubernetes/contrib/issues/457,updated symlink endpoint,"Fixes #429. Will create symlink at /usr/bin instead of /usr/local/bin.
",closed,True,2016-02-03 21:49:15,2016-03-20 15:44:15
contrib,rsmitty,https://github.com/kubernetes/contrib/issues/458,https://api.github.com/repos/kubernetes/contrib/issues/458,Ansible: Support OverlayFS for Docker Storage,"Upon continuing to try deploying against CentOS 7 hosts, I've encountered another error which does not pop up when using Atomic. When the Docker service is launched for the first time the ""docker-storage-setup"" command is run. This command fails because it attempts to use the devicemapper driver and LVM to create a Docker volume group. The CentOS cloud image (in OpenStack and I presume other clouds) does not use LVM for the root volume and is just a single ext4 partition at /. 

However, this can be worked around by using the OverlayFS driver instead and seems to be what is actually the default on the Atomic OSes. An interesting writeup can be found [here](http://www.projectatomic.io/blog/2015/06/notes-on-fedora-centos-and-docker-storage-drivers/).

I'll submit a pull request with a change for this.
",closed,False,2016-02-03 23:14:16,2018-02-14 01:01:11
contrib,rsmitty,https://github.com/kubernetes/contrib/pull/459,https://api.github.com/repos/kubernetes/contrib/issues/459,support overlayfs for non-atomic hosts,"This will resolve issue #458. I'm also willing to add a group_var for ""docker_storage_driver"" or similar if that's something folks think would be useful as part of this. But it seems like this would be a required change anyways for any Fedora or CentOS non-atomic image in OpenStack, and I presume any other cloud using the provided ""Generic Cloud"" images from CentOS/Fedora.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/459)

<!-- Reviewable:end -->
",closed,True,2016-02-03 23:29:50,2016-12-19 18:07:04
contrib,wienczny,https://github.com/kubernetes/contrib/issues/460,https://api.github.com/repos/kubernetes/contrib/issues/460,keepalived password truncated,"The authentication password added to keepalived.conf is too long (strange thing to say).  But keepalived can't handle it:

```
kube-keepalived-vip: Opening file '/etc/keepalived/keepalived.conf'.
kube-keepalived-vip: Truncating auth_pass to 8 characters
kube-keepalived-vip: Configuration is using : 73471 Bytes
```

This is not fatal.
",closed,False,2016-02-04 02:36:32,2016-05-05 15:13:14
contrib,gmarek,https://github.com/kubernetes/contrib/issues/461,https://api.github.com/repos/kubernetes/contrib/issues/461,Write 'ColorWriter' for contrib/compare.,"Handling colors in a way that tabwriter behaves correctly is currently VERY ugly. This should be fixed by writing a decorator for io.Writer that handles that.
",closed,False,2016-02-04 12:39:12,2018-02-12 19:32:08
contrib,roberthbailey,https://github.com/kubernetes/contrib/issues/462,https://api.github.com/repos/kubernetes/contrib/issues/462,Submit Queue UI should have direct links to each tab,"Right now I can't share a link to the 'Queue' tab. It also means that when you reload the page it always sticks you back on the first tab, which is annoying. 

This should be trivial to do with html fragments. 

/cc @brendandburns 
",closed,False,2016-02-05 00:14:53,2016-02-05 07:52:39
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/463,https://api.github.com/repos/kubernetes/contrib/issues/463,Add fragment perma-links to the tabs in the merge-queue UI.,"Ref #462 

@roberthbailey @eparis 
",closed,True,2016-02-05 00:40:30,2016-02-05 07:08:53
contrib,eparis,https://github.com/kubernetes/contrib/pull/464,https://api.github.com/repos/kubernetes/contrib/issues/464,Disable ping-ci until travis comes back,,closed,True,2016-02-05 03:52:49,2016-03-02 14:03:58
contrib,wienczny,https://github.com/kubernetes/contrib/pull/465,https://api.github.com/repos/kubernetes/contrib/issues/465,Fix truncated password warning in keepalived,"Fix truncated password warning in keepalived by slicing the password in kube-keeplived-vip
",closed,True,2016-02-05 13:25:32,2016-02-17 21:54:43
contrib,pmorie,https://github.com/kubernetes/contrib/pull/466,https://api.github.com/repos/kubernetes/contrib/issues/466,[submit queue] Add author tag to rebase github munger comment,"@eparis
",closed,True,2016-02-06 20:48:10,2016-02-06 23:56:22
contrib,justinsb,https://github.com/kubernetes/contrib/pull/467,https://api.github.com/repos/kubernetes/contrib/issues/467,Initial support for federated builder status,"--jenkins-jobs can now include a `name=src` entry.

name is the name of the job.
src specifies the source for the output of builders

Limitations of initial implementation:
- only gs:// URLs are supported as src (Google Cloud Storage).
- all `name=src` entries are treated as non-gating, and
  will not contribute to the overall build status.

For example:
--jenkins-jobs=kubernetes-e2e-aws=gs://kubernetes-buildlogs-aws/logs/kubernetes-e2e-aws

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/467)

<!-- Reviewable:end -->
",closed,True,2016-02-07 01:10:29,2016-10-03 19:29:21
contrib,justinsb,https://github.com/kubernetes/contrib/pull/468,https://api.github.com/repos/kubernetes/contrib/issues/468,GZIP responses,"We might as well gzip the JSON responses; the history is 50KB+ and the
list of users is about 15KB.
",closed,True,2016-02-07 02:22:23,2016-03-22 15:32:50
contrib,justinsb,https://github.com/kubernetes/contrib/pull/469,https://api.github.com/repos/kubernetes/contrib/issues/469,Use a RWLock for the submit queue,"By doing this, we should be able to serve multiple readers concurrently.
",closed,True,2016-02-07 02:24:27,2016-02-07 03:30:20
contrib,justinsb,https://github.com/kubernetes/contrib/pull/470,https://api.github.com/repos/kubernetes/contrib/issues/470,[Depends on 467] UI support for federated build status,"Adds UI support for federated build status

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/470)

<!-- Reviewable:end -->
",closed,True,2016-02-07 03:00:13,2016-10-03 19:29:01
contrib,pmorie,https://github.com/kubernetes/contrib/pull/471,https://api.github.com/repos/kubernetes/contrib/issues/471,[submit queue] Add a couple victims to blunderbuss,"@eparis 
",closed,True,2016-02-08 15:58:56,2016-02-09 01:58:08
contrib,silasbw,https://github.com/kubernetes/contrib/pull/472,https://api.github.com/repos/kubernetes/contrib/issues/472,Remove the ciphers option from the httpsfrontend,"ciphers requires an argument and the following string (no-sslv3) isn't a
valid cipher, so haproxy was failing to start. The template config still
selects ciphers via ssl-default-bind-ciphers in global.
",closed,True,2016-02-08 23:33:12,2016-02-23 17:30:11
contrib,ixdy,https://github.com/kubernetes/contrib/issues/473,https://api.github.com/repos/kubernetes/contrib/issues/473,[mungegithub] ignore CI for gh-pages branch,"PRs to `gh-pages` won't be tested on Travis or Jenkins, so the munger shouldn't try to make tests run. (See e.g. kubernetes/kubernetes/pull/20344)
",closed,False,2016-02-09 01:06:15,2016-05-20 04:34:19
contrib,eparis,https://github.com/kubernetes/contrib/pull/474,https://api.github.com/repos/kubernetes/contrib/issues/474,WIP: [mungegithub] OWNERS implementation,"This PR is missing some key usability features.
- There is no way to know who needs to approve a given PR
- There is no way for me, as an approver, to know what PRs I need to approve.

I believe that before we turn this on, we REALLY need to solve those usability issues.
",closed,True,2016-02-09 02:07:28,2016-08-03 03:36:09
contrib,gmarek,https://github.com/kubernetes/contrib/pull/475,https://api.github.com/repos/kubernetes/contrib/issues/475,Fix perfdash,"This change should make perfdash usable as a place that gathers performance data from various sources. It extracts information from output of (mainly) density.go test, but it's able to use load.go results as well.

I'll try to get someone who knows JS to update the page, so there will be possible to have multiple tabs for different sources (e.g. Google, CoreOS, etc.). After it's done, if you want to publish your performance test data, you'll need to publish test outputs somewhere in the net, and add a scraper for it to this code.

Perfdash is running at http://perf-dash.k8s.io/

cc @hongchaodeng @timothysc @spiffxp 
",closed,True,2016-02-09 10:22:32,2016-02-09 20:57:05
contrib,gmarek,https://github.com/kubernetes/contrib/pull/476,https://api.github.com/repos/kubernetes/contrib/issues/476,Add a drop down in perfdash to select test kind,,closed,True,2016-02-09 14:12:31,2016-02-09 18:15:00
contrib,justinsb,https://github.com/kubernetes/contrib/pull/477,https://api.github.com/repos/kubernetes/contrib/issues/477,WIP: federated e2e runner,"WIP on the runner component of federated e2e testing.  Lots of problems still to solve, but a PR is a nice way to discuss & collaborate IMO.

Runs the e2e tests in a Docker container, and uploads results to a GCS bucket.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/477)

<!-- Reviewable:end -->
",closed,True,2016-02-09 16:49:19,2018-01-02 20:59:22
contrib,sandromello,https://github.com/kubernetes/contrib/issues/478,https://api.github.com/repos/kubernetes/contrib/issues/478,Improve clarity around auth errors in serviceloadbalancer,"I'am having some trouble starting haproxy.

Kubernetes Server Version: 1.1
Docker Version: 1.9.1 / Api: 1.21
Master/Minion OS: CoreOS - 899.6.0

This problem is related to:
https://github.com/kubernetes/kubernetes/issues/19441

When I started a load balancer replication container:

``` bash
kubectl create -f lb-rc.yaml
```

The haproxy process does not starts.

``` yaml
# lb-rc.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: service-loadbalancer
  labels:
    app: service-loadbalancer
    version: v1
spec:
  replicas: 1
  selector:
    app: service-loadbalancer
    version: v1
  template:
    metadata:
      labels:
        app: service-loadbalancer
        version: v1
    spec:
      nodeSelector:
        role: loadbalancer
      containers:
      - image: gcr.io/google_containers/servicelb:0.2
        imagePullPolicy: Always
        name: haproxy
        ports:
        # All http services
        - containerPort: 80
          hostPort: 80
          protocol: TCP
        resources: {}
```

The container has no ports associate with haproxy

``` bash
$ kubectl exec service-loadbalancer-7ke2x -it -- netstat -ltpn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 :::8081                 :::*                    LISTEN      1/service_loadbalan
```

Executing `./haproxy_reload` shows me the error

``` bash
$ kubectl exec service-loadbalancer-7ke2x -it -- ./haproxy_reload
cat: can't open '/var/run/haproxy.pid': No such file or directory
[ALERT] 039/183101 (39) : parsing [/etc/haproxy/haproxy.cfg:32] : 'listen' cannot handle unexpected argument ':1936'.
[ALERT] 039/183101 (39) : parsing [/etc/haproxy/haproxy.cfg:32] : please use the 'bind' keyword for listening addresses.
[ALERT] 039/183101 (39) : Error(s) found in configuration file : /etc/haproxy/haproxy.cfg
[WARNING] 039/183101 (39) : config : proxy 'stats' has no 'bind' directive. Please declare it as a backend if this was intended.
[WARNING] 039/183101 (39) : config : missing timeouts for proxy 'stats'.
   | While not properly invalid, you will certainly encounter various problems
   | with such a configuration. To fix this, please ensure that all following
   | timeouts are set to a non-zero value: 'client', 'connect', 'server'.
[WARNING] 039/183101 (39) : config : log format ignored for frontend 'httpfrontend' since it has no log address.
[ALERT] 039/183101 (39) : Fatal errors found in configuration.
error: error executing remote command: Error executing command in container: Error executing in Docker Container: 1
```

Checking the file /etc/haproxy/haproxy.cfg:

```
$ kubectl exec service-loadbalancer-7ke2x -it -- cat /etc/haproxy/haproxy.cfg |grep -C 1 listen
# haproxy stats, required hostport and firewall rules for :1936
listen stats :1936
    mode http
```

The /template.cfg is correctly configured when I start the service-loadbalancer container:

``` bash
$ kubectl exec service-loadbalancer-7ke2x -it -- cat /template.cfg |grep -C 1 listen
# haproxy stats, required hostport and firewall rules for :1936
listen stats
    bind *:1936
```

Does the image gcr.io/google_containers/servicelb:0.2 is updated correctly or I'am missing something?
",closed,False,2016-02-09 18:55:09,2018-02-12 22:35:08
contrib,aledbf,https://github.com/kubernetes/contrib/pull/479,https://api.github.com/repos/kubernetes/contrib/issues/479,Update nginx Ingress controller,"closes #430
closes #420
",closed,True,2016-02-09 19:58:33,2016-02-16 15:30:57
contrib,victorgp,https://github.com/kubernetes/contrib/pull/480,https://api.github.com/repos/kubernetes/contrib/issues/480,pod-master: Enable https connection to etcd servers,"I know the podmaster will be deprecated, but in the meantime, until that happens there are a lot of us that needs the podmaster to connect securely to etcd.
",closed,True,2016-02-10 17:43:32,2016-07-02 16:52:51
contrib,ixdy,https://github.com/kubernetes/contrib/issues/481,https://api.github.com/repos/kubernetes/contrib/issues/481,[mungegithub] auto-close PRs with no recent activity,"Per the [Pull Request process doc](https://github.com/kubernetes/kubernetes/blob/master/docs/devel/pull-requests.md#other-notes), we claim to close PRs that are older than two weeks.

Should we automate this process? Maybe if there have been no comments or pushes within a month, auto-close?

It'd probably make the list of PRs the munger has to look at smaller, at least...
",closed,False,2016-02-11 03:57:22,2016-07-15 18:44:35
contrib,gmarek,https://github.com/kubernetes/contrib/pull/482,https://api.github.com/repos/kubernetes/contrib/issues/482,Perfdash uses GCS as a source,"@spiffxp @jeremyeder @kubernetes/sig-scalability 
",closed,True,2016-02-11 14:43:27,2016-02-15 08:59:25
contrib,gmarek,https://github.com/kubernetes/contrib/issues/483,https://api.github.com/repos/kubernetes/contrib/issues/483,Cleanup perfdash frontend,"It's currently a messy JavaScript - we should make it cleaner. 

cc @kubernetes/sig-scalability @kubernetes/sig-testing 
",closed,False,2016-02-11 17:26:17,2018-06-17 09:54:58
contrib,atosatto,https://github.com/kubernetes/contrib/pull/484,https://api.github.com/repos/kubernetes/contrib/issues/484,Fix gen certs,"Implementing the changes to PR #330 suggested by @eparis to let these changes land to master. 
",closed,True,2016-02-15 17:09:41,2016-02-15 17:18:49
contrib,atosatto,https://github.com/kubernetes/contrib/pull/485,https://api.github.com/repos/kubernetes/contrib/issues/485,Replacing the virtualbox vm.box with the official centos box,"The `chef/centos-7.0` box is no more available on Atlas.
This PR replace it with the official centos 7 box.
",closed,True,2016-02-15 17:21:43,2016-02-25 13:21:50
contrib,justinsb,https://github.com/kubernetes/contrib/pull/486,https://api.github.com/repos/kubernetes/contrib/issues/486,Image builder that builds AWS AMIs,"Edit: no longer WIP, works!

This is a go script that we can use to build AWS AMIs, as in https://github.com/kubernetes/kubernetes/pull/21226

Functionality TODOs:
- [x] Make AMI public
- [x] Copy AMI to multiple regions
- [ ] Support other clouds (GCE?)

Other TODOs: comments & general code cleanup.
",closed,True,2016-02-15 18:27:25,2016-08-03 01:07:08
contrib,danehans,https://github.com/kubernetes/contrib/issues/487,https://api.github.com/repos/kubernetes/contrib/issues/487,Grafana with NodePort Fails with InfluxDB Error,"Grafana UI shows the following error:

Dashboard init failed
Template variables could not be initialized: InfluxDB Error: <br/>404 page not found

I'm using a NodePort to externally expose grafana/influxdb. I am able to hit these endpoints using the node ip/port. However, grafana is displaying: Dashboard init failed
Template variables could not be initialized: InfluxDB Error: <br/>404 page not found. Additionally, no data or graphs are present.

Per documentation, I commented out the grafana env's used for proxy api. Here is a gist that provides more details of my environment: https://gist.github.com/danehans/ffbf7642863ab9845b39

Any insight you can provide would be appreciated.
",closed,False,2016-02-15 19:54:10,2016-03-02 15:42:24
contrib,danehans,https://github.com/kubernetes/contrib/issues/488,https://api.github.com/repos/kubernetes/contrib/issues/488,service-loadbalancer does not work with kube dashboard,"I'm trying to use the service-loadbalancer to expose the kube dashboard [1]. I can curl the dashboard pod/svc ip from within the cluster. The haproxy.cfg file appears to be properly updated and I don't see any errors in the pod logs. I can hit the kube-dashboard service externally when using a NodePort. [2] provides more details of the issue.

[1] https://github.com/kubernetes/dashboard
[2] https://gist.github.com/danehans/fd1170fb6df98b47adb8
",closed,False,2016-02-16 00:23:17,2017-12-15 16:08:58
contrib,gmarek,https://github.com/kubernetes/contrib/pull/489,https://api.github.com/repos/kubernetes/contrib/issues/489,Small perfdash cleanup - defining an interface,,closed,True,2016-02-16 10:55:27,2016-02-23 19:09:41
contrib,aledbf,https://github.com/kubernetes/contrib/pull/490,https://api.github.com/repos/kubernetes/contrib/issues/490,Allow nginx Ingress controller run as DaemonSet,"fixes #357

requires #491
",closed,True,2016-02-17 12:19:14,2016-02-27 15:13:37
contrib,aledbf,https://github.com/kubernetes/contrib/pull/491,https://api.github.com/repos/kubernetes/contrib/issues/491,Update nginx slim,"CVE-2016-0747
CVE-2016-0742
CVE-2016-0746

http://nginx.org/en/CHANGES
",closed,True,2016-02-17 12:59:25,2016-02-23 12:49:18
contrib,aledbf,https://github.com/kubernetes/contrib/pull/492,https://api.github.com/repos/kubernetes/contrib/issues/492,Update haproxy image and service-loadbalancer,"- removes the custom build of haproxy
- service-loadbalancer uses haproxy image (gcr.io/google_containers/haproxy)
- closes #472
",closed,True,2016-02-17 13:21:28,2016-02-23 01:08:07
contrib,aledbf,https://github.com/kubernetes/contrib/pull/493,https://api.github.com/repos/kubernetes/contrib/issues/493,Add support for private repositories in git-sync,"This adds support for http authentication, shallow clone using depth flag and one time execution (exit 0 after initial clone)

ref kubernetes/kubernetes/issues/16247
",closed,True,2016-02-17 18:11:10,2016-03-26 00:00:21
contrib,timstclair,https://github.com/kubernetes/contrib/pull/494,https://api.github.com/repos/kubernetes/contrib/issues/494,Add a utility for inspecting go import trees,"I wrote this small utility for tracking down go dependency issues, and it seems like it could be useful to other people. There are alternatives (such as [godepgraph](https://github.com/kisielk/godepgraph) and [goviz](https://github.com/hirokidaichi/goviz)) but I couldn't find any that computed paths between packages.

/cc @lavalamp 
",closed,True,2016-02-17 21:14:25,2016-07-01 16:25:59
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/495,https://api.github.com/repos/kubernetes/contrib/issues/495,Handle glbc restarts gracefully,"If glbc restarts there's a chance it syncs services before the Ingress store has synced. We should wait till both have done their first list from the apiserver, via something like: https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/replication/replication_controller.go#L424 (which was recently fixed in kubernetes HEAD to do the right thing). 
",closed,False,2016-02-17 21:30:53,2018-02-12 21:34:11
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/496,https://api.github.com/repos/kubernetes/contrib/issues/496,Add code slush mode to merge bot,"We want to lock down what gets merged while we prepare for release. Last time we required two TLs to sign off on something, adding both LGTM and ok-to-merge labels.

Maybe we could do something simpler and make the bot look for the 1.2 milestone?

We'd like a small set of people to be aware of everything that is going in before we cut the release branch.

I can throw something together, but I thought I'd ask if anyone has opinions on how it should work first.
",closed,False,2016-02-18 00:23:06,2016-02-22 22:42:49
contrib,itxx00,https://github.com/kubernetes/contrib/issues/497,https://api.github.com/repos/kubernetes/contrib/issues/497,Ansible:  input file not found when copy master binaries,"```
TASK: [master | Copy master binaries] *****************************************
fatal: [192.168.199.5] => input file not found at /root/contrib/ansible/roles/_output/local/go/bin/kube-apiserver or /root/_output/local/go/bin/kube-apiserver

FATAL: all hosts have already failed -- aborting
```
",closed,False,2016-02-18 16:25:44,2016-02-20 14:03:40
contrib,therc,https://github.com/kubernetes/contrib/pull/498,https://api.github.com/repos/kubernetes/contrib/issues/498,Add -quiet option reduce log spam from exechealthz,"See kubernetes/kubernetes [#21285](https://github.com/kubernetes/kubernetes/issues/21285)

cc @bprashanth 
",closed,True,2016-02-18 17:40:45,2016-02-23 02:09:19
contrib,stevesloka,https://github.com/kubernetes/contrib/issues/499,https://api.github.com/repos/kubernetes/contrib/issues/499,Dependency Issue,"I have a random question about an error I'm getting. I'm doing some work in the service-loadbalancer project and have a build error. When I use the makefile build it's ok, just my IDE on OSX throws this error. 

Is there an update somewhere or flag I can set?

```
./service_loadbalancer.go:818: undefined: ""k8s.io/kubernetes/pkg/util"".NeverStop
./service_loadbalancer.go:819: undefined: ""k8s.io/kubernetes/pkg/util"".NeverStop
./service_loadbalancer.go:824: undefined: ""k8s.io/kubernetes/pkg/util"".Until
./service_loadbalancer.go:824: undefined: ""k8s.io/kubernetes/pkg/util"".NeverStop
```
",closed,False,2016-02-18 20:49:30,2016-02-19 02:25:44
contrib,emmanuel,https://github.com/kubernetes/contrib/pull/500,https://api.github.com/repos/kubernetes/contrib/issues/500,Add health check path,,closed,True,2016-02-19 01:27:13,2016-02-19 03:41:29
contrib,aledbf,https://github.com/kubernetes/contrib/pull/501,https://api.github.com/repos/kubernetes/contrib/issues/501,Update keepalived,"Required by the fix to the namespace watching issue
",closed,True,2016-02-19 21:03:33,2016-02-23 12:49:01
contrib,deftdawg,https://github.com/kubernetes/contrib/pull/502,https://api.github.com/repos/kubernetes/contrib/issues/502,Add DNS to requirements and troubleshooting tips for 1.2.0a7,"Update nginx-third-party docs to include requirement for working DNS and troubleshooting tips for 1.2.0-alpha7

FYI @aledbf 
",closed,True,2016-02-20 02:10:34,2016-02-23 05:01:16
contrib,victorgp,https://github.com/kubernetes/contrib/pull/503,https://api.github.com/repos/kubernetes/contrib/issues/503,Test,,closed,True,2016-02-20 13:18:00,2016-02-20 13:21:07
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/504,https://api.github.com/repos/kubernetes/contrib/issues/504,Add HTTPS to the GCE ingress controller,"Adds support for HTTPS, and 3 modes:
1. len(ing.Spec.TLS) == 0: pure http
2. len(ing.Spec.TLS) > 0: http on :80 + https on :443 with ing.Spec.TLS[0]
3. ing.Annotation[allowHTTP] == false: don't setup anything on :80

Second and third commits have real code. 

@kubernetes/goog-cluster feel free to jump in and take over as reviewers, assigning to Tim as defacto reviewer.
",closed,True,2016-02-22 02:34:21,2016-02-27 04:07:54
contrib,sgoodliff,https://github.com/kubernetes/contrib/issues/505,https://api.github.com/repos/kubernetes/contrib/issues/505,"ansible set_fact missing from ""roles/node/tasks/packageManagerInstall.yml""","Hi,

I had to add this block to
- name: Init the did_install fact
  set_fact:
    did_install: false

to ""roles/node/tasks/packageManagerInstall.yml""

to mimic 

roles/master/tasks/packageManagerInstall.yml

otherwise the ""generic nodes"" got didnt install kubernetes-node and the playbook stopped
",closed,False,2016-02-22 16:53:04,2018-02-14 01:01:10
contrib,emaildanwilson,https://github.com/kubernetes/contrib/pull/506,https://api.github.com/repos/kubernetes/contrib/issues/506,"continuous-delivery: Added tls support, improved health checking and support for solanoci","@brendandburns you mentioned I should ping you on updates I make to this.
",closed,True,2016-02-22 19:36:42,2016-04-15 21:46:20
contrib,emaildanwilson,https://github.com/kubernetes/contrib/issues/507,https://api.github.com/repos/kubernetes/contrib/issues/507,Improvements to scripts in contrib/continuousdelivery,"I added support for tls authentication to the kubernetes proxy when performing curl based health checks, improved the health check method, added support for solanoci and made a few other minor improvements\fixes.

https://github.com/kubernetes/contrib/pull/506
",closed,False,2016-02-22 20:34:57,2018-02-12 22:35:09
contrib,danehans,https://github.com/kubernetes/contrib/issues/508,https://api.github.com/repos/kubernetes/contrib/issues/508,ansible: ansible_fqdn fact causes failed deployments,"Per @eparis in [1] ansible_fqdn does not work in all situations. It is preferred to use a system fact IP address instead of ansible_fqdn.

[1] https://github.com/kubernetes/contrib/pull/244
",closed,False,2016-02-22 21:49:36,2016-06-08 20:27:14
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/509,https://api.github.com/repos/kubernetes/contrib/issues/509,Update image tags to reflect pushed images.,"@aledbf fyi. 
",closed,True,2016-02-23 02:24:29,2016-02-23 02:33:51
contrib,sttts,https://github.com/kubernetes/contrib/pull/510,https://api.github.com/repos/kubernetes/contrib/issues/510,Add default go FlagSet to re-enable glog flags,"Without this flags like `--v` from glog are not accessible.
",closed,True,2016-02-23 07:28:50,2016-02-23 14:28:24
contrib,oyvindio,https://github.com/kubernetes/contrib/issues/511,https://api.github.com/repos/kubernetes/contrib/issues/511,[Ingress nginx-third-party] nginx does not get initial configuration if nginx-third-party rc is annotated with a TCP Service which does not exist,"If the nginx-third-party Ingress controller is configured to expose a TCP Service and that Service does not exist, the initial configuration for nginx is not created in time, and the Pod ends up in a restart loop. 

To reproduce:
1. Deploy http-default-backend `kubectl create -f $contrib/Ingress/controllers/nginx-third-party/examples/default-backend.yaml && kubectl expose --port 8080 rc default-http-backend`
2. Deploy nginx-third-party: `kubectl create -f $contrib/Ingress/controllers/nginx-third-party/examples/rc-tcp.yaml`
3. Annotate with tcpservices annotation: `kubectl annotate rc nginx-ingress-3rdpartycfg ""nginx-ingress.kubernetes.io/tcpservices=default/bogus:1337""`
4. Kill the nginx-third-party Pods to force the new configuration: `k delete po -l name=nginx-ingress-lb`

Now you'll see something like this in the logs:

```
I0225 13:14:32.982404       1 utils.go:177] Waiting for default/bogus
...
E0225 13:14:37.910683       1 utils.go:65] Requeuing default/some-service, err Post http://127.0.0.1:8080/update-ingress: dial tcp 127.0.0.1:8080: getsockopt: connection refused
E0225 13:14:37.911002       1 utils.go:65] Requeuing default/some-other-service, err Post http://127.0.0.1:8080/update-ingress: dial tcp 127.0.0.1:8080: getsockopt: connection refused
```

From what I can understand, this is because [the code path that should write the initial config](https://github.com/kubernetes/contrib/blob/ae06390f72b0d5893a1fe318af51e6ee867da39b/Ingress/controllers/nginx-third-party/controller.go#L220) ends up spinning in [this wait.Poll loop](https://github.com/kubernetes/contrib/blob/ae06390f72b0d5893a1fe318af51e6ee867da39b/Ingress/controllers/nginx-third-party/utils.go#L178). This means that nginx doesn't start, so nothing is listening on localhost:8080, hence the connection refused errors. This continues untill the Pod is killed by Kubernetes due to failing health checks, as nginx is supposed to handle these as well, but it doesn't since it is not running.

I'm not really sure how to fix this, but I think it's a bad thing that one misconfigured TCP Service should end up breaking the Ingress controller for all Services.

Verified on Kubernetes 1.1.2/custom setup with `gcr.io/google_containers/nginx-third-party:0.3`.
",closed,False,2016-02-25 13:21:52,2016-03-07 20:37:55
contrib,aledbf,https://github.com/kubernetes/contrib/pull/512,https://api.github.com/repos/kubernetes/contrib/issues/512,Check if the dns is working properly. Skip invalid TCP services,"fixes #511
",closed,True,2016-02-25 17:55:22,2016-03-08 21:11:23
contrib,wienczny,https://github.com/kubernetes/contrib/pull/513,https://api.github.com/repos/kubernetes/contrib/issues/513,"keepalived-vip: Add support for custom templates, routes and routing rules","This PR adds support for custom templates by adding a flag to set a custom template file.
It also supports defining routes and routing rules using additional annotations.
The next release of keepalived is going to add support for routing rules.

New annotations are:
- k8s.io/public-vip-netmask 
- k8s.io/public-vip-route

This looks like this:

```
k8s.io/public-vip: ""11.22.33.44""
k8s.io/public-vip-netmask: ""24""
k8s.io/public-vip-route: ""11.22.33.254""
```

Add flags for:
- path to keepalived binary
- path to template filename
- path to keepalived.conf
- path to keepalived.pid
- template generation debug (does not start keepalived)
- to enable ipvs conntrack (implemented it before it was upstream and added to flag to keep upstream behavior by default)

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/513)

<!-- Reviewable:end -->
",closed,True,2016-02-26 00:22:23,2018-02-17 13:23:00
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/514,https://api.github.com/repos/kubernetes/contrib/issues/514,Add docker micro benchmark into kubernetes contrib.,"Add docker micro benchmark into kubernetes contrib.

The description and usage of the tool is in README.md.

Even though the tool is quite simple now, it could be a start. I'll add more benchmark when we need it in the future.

@kubernetes/sig-node 
",closed,True,2016-02-26 06:11:49,2016-03-16 01:04:51
contrib,timothysc,https://github.com/kubernetes/contrib/issues/515,https://api.github.com/repos/kubernetes/contrib/issues/515,Update prometheus to leverage our other work.,"@jimmidyson has some great examples https://vimeo.com/139706674 that would be useful to the broader community in going 0->introspective metrics.  IMHO we should modify the examples and possibly allow for customizing.  

/cc @jayunit100 
",closed,False,2016-02-26 17:26:59,2018-02-13 00:37:08
contrib,mikedanese,https://github.com/kubernetes/contrib/issues/516,https://api.github.com/repos/kubernetes/contrib/issues/516,Delete podmaster,"And update all documentation to --leader-elect flags
",closed,False,2016-02-26 19:31:14,2017-12-14 23:43:56
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/517,https://api.github.com/repos/kubernetes/contrib/issues/517,blunderbuss updates,,closed,True,2016-02-26 23:50:17,2016-02-27 00:47:25
contrib,roberthbailey,https://github.com/kubernetes/contrib/issues/518,https://api.github.com/repos/kubernetes/contrib/issues/518,Submit queue includes PRs that have been manually merged,"I was scanning through http://submit-queue.k8s.io/#/queue this evening and noticed that multiple PRs I've been following (https://github.com/kubernetes/kubernetes/pull/21004, https://github.com/kubernetes/kubernetes/pull/21441, https://github.com/kubernetes/kubernetes/pull/21507, https://github.com/kubernetes/kubernetes/pull/21721, and https://github.com/kubernetes/kubernetes/pull/21770) are listed in the queue (current length is 64) even though they have been manually merged. 

This means that the queue length is being reported incorrectly. 

/cc @goltermann 
",closed,False,2016-02-27 06:03:12,2016-03-14 18:22:09
contrib,aledbf,https://github.com/kubernetes/contrib/pull/519,https://api.github.com/repos/kubernetes/contrib/issues/519,Allow nginx Ingress controller run as DaemonSet,"replaces #490
",closed,True,2016-02-27 15:18:37,2016-03-07 02:01:53
contrib,justinsb,https://github.com/kubernetes/contrib/pull/520,https://api.github.com/repos/kubernetes/contrib/issues/520,[Depends on #467] Parse JUnit test results and expose failures via JSON,"Depends on #467 

This PR will parse the JUnit XML output results, and exposes the names of failing tests via JSON.

This is the first step towards being able to show the failures immediately in the test output, or to show graphs of test timings over build runs, or % reliablity of tests etc etc.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/520)

<!-- Reviewable:end -->
",closed,True,2016-02-27 23:03:49,2016-10-03 19:29:55
contrib,justinsb,https://github.com/kubernetes/contrib/pull/521,https://api.github.com/repos/kubernetes/contrib/issues/521,Munger: Don't lock while building users,"If we don't have a github token, we never build the users, and thus we
can't develop the UI etc.
",closed,True,2016-02-28 14:06:23,2016-06-01 21:07:35
contrib,aledbf,https://github.com/kubernetes/contrib/pull/522,https://api.github.com/repos/kubernetes/contrib/issues/522,Add reduced debian image,"```
gcr.io/google_containers/debian-slim    0.1 9b76e4e71135     27 seconds ago    40.05 MB
debian                                  jessie 256adf7015ca  12 days ago       125.1 MB
```
",closed,True,2016-02-29 13:41:31,2016-03-01 14:00:30
contrib,timstclair,https://github.com/kubernetes/contrib/pull/523,https://api.github.com/repos/kubernetes/contrib/issues/523,Tollerate issue URLs in rebuild requests,"The following are rendered equivalently, but currently treated differently by the rebuild munger. This PR fixes that.
- @k8s-bot test this issue: #522 
  
  `@k8s-bot test this issue: #522`
- @k8s-bot test this issue: https://github.com/kubernetes/contrib/issues/522
  
  `@k8s-bot test this issue: https://github.com/kubernetes/contrib/issues/522`
  
  (the kubernetes/kubernetes repo is hardcoded, but I changed it to contrib for illustration)
",closed,True,2016-03-01 00:08:43,2016-03-01 21:06:36
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/524,https://api.github.com/repos/kubernetes/contrib/issues/524,Remove ihmccreery from blunderbuss for test/,"Really, cc @kubernetes/goog-testing and @kubernetes/sig-testing should be thinking about better ways to target reviews to the teams who maintain the tests, rather than @ixdy.
",closed,True,2016-03-01 02:11:58,2016-03-01 16:33:16
contrib,aledbf,https://github.com/kubernetes/contrib/pull/525,https://api.github.com/repos/kubernetes/contrib/issues/525,Add reduced ubuntu docker image,"```
gcr.io/google_containers/ubuntu-slim 0.1    d49336ef257d        3 hours ago    55.09 MB
ubuntu                               15.10  8fba16db7a1f        13 days ago    135.5 MB
```
",closed,True,2016-03-01 17:23:41,2016-03-08 21:04:49
contrib,eparis,https://github.com/kubernetes/contrib/pull/526,https://api.github.com/repos/kubernetes/contrib/issues/526,Add a 'kind/old-docs' label to PRs which touch files moving out of tree,"... as we split the docs to their own repo
",closed,True,2016-03-01 21:53:07,2016-03-02 14:03:57
contrib,eparis,https://github.com/kubernetes/contrib/pull/527,https://api.github.com/repos/kubernetes/contrib/issues/527,Default to --dry-run,"Must use --dry-run=false if you want to make changes to github.

Helps make sure developers don't accidentally make changes to github.
",closed,True,2016-03-01 22:05:37,2016-03-02 14:03:53
contrib,eparis,https://github.com/kubernetes/contrib/pull/528,https://api.github.com/repos/kubernetes/contrib/issues/528,Label docs/design with kind/design,"@bgrant0607 
",closed,True,2016-03-01 22:16:40,2016-03-02 14:03:54
contrib,pwittrock,https://github.com/kubernetes/contrib/pull/529,https://api.github.com/repos/kubernetes/contrib/issues/529,serve_hostname debug logging after receiving SIGTERM,,closed,True,2016-03-02 18:28:02,2016-03-03 18:02:03
contrib,eparis,https://github.com/kubernetes/contrib/pull/530,https://api.github.com/repos/kubernetes/contrib/issues/530,Blunderbuss assignee from inside the kube repo,"Instead of having blunderbuss.yml here in contrib this actually looks at the real repo. So changes there are automatically picked up here. More info in one place! Should make maintenance of the assignees file easier for all.
",closed,True,2016-03-02 23:29:52,2016-03-09 18:23:19
contrib,ixdy,https://github.com/kubernetes/contrib/pull/531,https://api.github.com/repos/kubernetes/contrib/issues/531,Add @fejta and @spxtr to blunderbuss config for test/,"Largely `test/` PRs have mostly involve triaging to proper owners, but I'd like not to have to do that all myself.

cc @fejta @spxtr and also @kubernetes/sig-testing in case anyone else feels they should be on this
",closed,True,2016-03-03 01:37:29,2016-03-03 01:41:51
contrib,eparis,https://github.com/kubernetes/contrib/pull/532,https://api.github.com/repos/kubernetes/contrib/issues/532,blunderbuss: fix aws cloudprovider location,"@justinsb 
",closed,True,2016-03-03 01:45:36,2016-03-03 02:50:12
contrib,eparis,https://github.com/kubernetes/contrib/pull/533,https://api.github.com/repos/kubernetes/contrib/issues/533,Use current milestone after 'priorities' but before 'pr number',"This means a p0 with v1.3 will come before a p1 with v1.2. It's hard to decide how priority vs milestone relate. Is a p0 with no milestone before or after a p3 with v1.2? My algorithm says the p0 always comes first.

Priority first
Then due date of milestone
Then issue number
",closed,True,2016-03-03 03:22:59,2016-03-03 16:12:50
contrib,eparis,https://github.com/kubernetes/contrib/pull/534,https://api.github.com/repos/kubernetes/contrib/issues/534,Block auto merge based on PR touching illegal paths,"This should block any PR which touches certain docs directories from
merging by the submit queue.

This currently would hit 40 PRs. However this is not yet enabled. Only made available.

https://github.com/kubernetes/kubernetes/pull/7245
https://github.com/kubernetes/kubernetes/pull/12187
https://github.com/kubernetes/kubernetes/pull/13392
https://github.com/kubernetes/kubernetes/pull/14119
https://github.com/kubernetes/kubernetes/pull/14943
https://github.com/kubernetes/kubernetes/pull/15547
https://github.com/kubernetes/kubernetes/pull/16682
https://github.com/kubernetes/kubernetes/pull/17622
https://github.com/kubernetes/kubernetes/pull/17926
https://github.com/kubernetes/kubernetes/pull/18036
https://github.com/kubernetes/kubernetes/pull/18961
https://github.com/kubernetes/kubernetes/pull/19069
https://github.com/kubernetes/kubernetes/pull/19126
https://github.com/kubernetes/kubernetes/pull/19179
https://github.com/kubernetes/kubernetes/pull/19556
https://github.com/kubernetes/kubernetes/pull/20044
https://github.com/kubernetes/kubernetes/pull/20419
https://github.com/kubernetes/kubernetes/pull/20490
https://github.com/kubernetes/kubernetes/pull/20698
https://github.com/kubernetes/kubernetes/pull/21207
https://github.com/kubernetes/kubernetes/pull/21263
https://github.com/kubernetes/kubernetes/pull/21358
https://github.com/kubernetes/kubernetes/pull/21554
https://github.com/kubernetes/kubernetes/pull/21646
https://github.com/kubernetes/kubernetes/pull/21658
https://github.com/kubernetes/kubernetes/pull/21718
https://github.com/kubernetes/kubernetes/pull/21737
https://github.com/kubernetes/kubernetes/pull/21773
https://github.com/kubernetes/kubernetes/pull/21792
https://github.com/kubernetes/kubernetes/pull/21804
https://github.com/kubernetes/kubernetes/pull/21851
https://github.com/kubernetes/kubernetes/pull/21926
https://github.com/kubernetes/kubernetes/pull/21943
https://github.com/kubernetes/kubernetes/pull/21950
https://github.com/kubernetes/kubernetes/pull/22031
https://github.com/kubernetes/kubernetes/pull/22074
https://github.com/kubernetes/kubernetes/pull/22079
https://github.com/kubernetes/kubernetes/pull/22146
https://github.com/kubernetes/kubernetes/pull/22367
https://github.com/kubernetes/kubernetes/pull/22395
",closed,True,2016-03-03 04:13:24,2016-03-03 16:12:49
contrib,thockin,https://github.com/kubernetes/contrib/pull/535,https://api.github.com/repos/kubernetes/contrib/issues/535,Add my micro-demos to contrib,"These have been sitting in my own github for too long.
",closed,True,2016-03-03 06:50:09,2016-03-07 05:06:54
contrib,wojtek-t,https://github.com/kubernetes/contrib/issues/536,https://api.github.com/repos/kubernetes/contrib/issues/536,Merge bot is asking to run a test and those test doesn't start,"I've seen this a bunch of times today - see e.g.:
https://github.com/kubernetes/kubernetes/pull/22264
https://github.com/kubernetes/kubernetes/pull/21851
https://github.com/kubernetes/kubernetes/pull/21904

And a bunch of others

@eparis @lavalamp @mwielgus @gmarek @fgrzadkowski 
",closed,False,2016-03-03 13:02:44,2016-03-03 15:31:24
contrib,rutsky,https://github.com/kubernetes/contrib/pull/537,https://api.github.com/repos/kubernetes/contrib/issues/537,"fix typo: ""night"" -> ""might""",,closed,True,2016-03-03 14:54:35,2016-03-03 15:20:15
contrib,eparis,https://github.com/kubernetes/contrib/pull/538,https://api.github.com/repos/kubernetes/contrib/issues/538,Small example program using github helpers,"Which lets you write your own mechanism to look at github issues/PRs.
",closed,True,2016-03-03 15:55:50,2016-03-03 16:12:50
contrib,eparis,https://github.com/kubernetes/contrib/pull/539,https://api.github.com/repos/kubernetes/contrib/issues/539,submit queue: Enable the path block merge mungers,"This will block PRs which change docs from automatically merging.
",closed,True,2016-03-03 16:01:12,2016-03-09 18:23:19
contrib,eparis,https://github.com/kubernetes/contrib/pull/540,https://api.github.com/repos/kubernetes/contrib/issues/540,Give users more information about how the submit queue works,,closed,True,2016-03-03 22:50:46,2016-03-09 18:23:20
contrib,eparis,https://github.com/kubernetes/contrib/pull/541,https://api.github.com/repos/kubernetes/contrib/issues/541,submit-queue: rename do-not-auto-merge to do-not-merge,"Because we don't really wany people doing it by hand either...
",closed,True,2016-03-03 23:46:25,2016-03-09 18:23:22
contrib,eparis,https://github.com/kubernetes/contrib/pull/542,https://api.github.com/repos/kubernetes/contrib/issues/542,Fix Dockerfile to support latest changes,"Remove blunderbuss ADD since that comes from in tree
Add path-block ADD since we need that!
",closed,True,2016-03-04 01:41:18,2016-03-09 18:23:20
contrib,chaosaffe,https://github.com/kubernetes/contrib/issues/543,https://api.github.com/repos/kubernetes/contrib/issues/543,[Ingress nginx-third-party] Cannot utilize multiple destination TCP ports pointing to the same `<namespace>/<service>`,"Currently there is no way to select the destination port for a service. The annotation specifies the source port in `<namespace>/<service>:<source-port>`, but the destination port is [defaulted to the first port listed](https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx-third-party/utils.go#L168) in the service spec.

Per [the existing documentation](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx-third-party#exposing-tcp-services) we expose a TCP service via adding the annotation `nginx-ingress.kubernetes.io/tcpservices=<namespace>/<service>:<source-port>` to the replication controller.

We have a few options on how to identify the destination port, but I think the most logical would be to change the annotation to: `nginx-ingress.kubernetes.io/tcpservices/<source-port>=<namespace>/<service>:<destination-port>` where `<destination-port>` can either be the port or name from the ServicePort

/cc @bprashanth @aledbf 
",closed,False,2016-03-04 13:02:47,2016-03-28 15:10:17
contrib,eparis,https://github.com/kubernetes/contrib/pull/544,https://api.github.com/repos/kubernetes/contrib/issues/544,submit-queue: Remove closed PRs from queue,"Little bit of code de-dup/refactor. Then every time we run all PRs, we check those on the queue a second time. Thus those on the queue that get merged/closed will get noticed before they get to the top.
",closed,True,2016-03-04 16:41:53,2016-03-09 18:23:22
contrib,Q-Lee,https://github.com/kubernetes/contrib/pull/545,https://api.github.com/repos/kubernetes/contrib/issues/545,Adding a container that watches and vertically scales a pod,"We're moving this into contrib, which is a different repo. I don't see a way to migrate the pull request, so the link has been provided for context: https://github.com/kubernetes/kubernetes/pull/22477
",closed,True,2016-03-05 00:33:59,2016-03-05 00:34:54
contrib,Q-Lee,https://github.com/kubernetes/contrib/pull/546,https://api.github.com/repos/kubernetes/contrib/issues/546,Adding a container that watches and vertically scales a pod.,"@krousey @roberthbailey 

I'm moving this into contrib. The previous PR context has been linked.

Continuation of https://github.com/kubernetes/kubernetes/pull/22477
That PR lost context, so here it is: https://github.com/Q-Lee/kubernetes/commit/1c4b094db513e5b21f39cdfa04d138d85e5253d9#commitcomment-16513328
",closed,True,2016-03-05 00:40:54,2016-03-11 09:12:02
contrib,caesarxuchao,https://github.com/kubernetes/contrib/pull/547,https://api.github.com/repos/kubernetes/contrib/issues/547,Some improvements to micro-demo,,closed,True,2016-03-07 05:44:32,2016-03-25 18:22:41
contrib,danehans,https://github.com/kubernetes/contrib/pull/548,https://api.github.com/repos/kubernetes/contrib/issues/548,Adds CoreOS Support,"Adds initial support for running Kubernetes on CoreOS. This commit
adds :
1. Tasks and files to install pypy to CoreOS.
   ansible_python_interpreter was added to group_vars/all.yml to
   reference the Python installation path.
2. Roles were updated to reference systemd unit files or dropins
   that are specific to CoreOS.
3. Conditional logic added throughout based on the
   ansible_lsb.id == ""CoreOS"" system fact.

Closes Issue: https://github.com/kubernetes/contrib/issues/406
",closed,True,2016-03-07 18:02:59,2016-03-22 09:58:14
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/549,https://api.github.com/repos/kubernetes/contrib/issues/549,Service-loadbalancer allows custom return code on /,"@aledbf 
",closed,True,2016-03-08 03:05:10,2016-03-08 04:18:39
contrib,danehans,https://github.com/kubernetes/contrib/issues/550,https://api.github.com/repos/kubernetes/contrib/issues/550,Ansible Docker Insecure Registry,"Currently group_vars/all.yml does not state that an insecure_registrys value should include the registry port:

ex> 10.10.10.10:5000

Without including a port, pulling images from the insecure registry fails:

``
$ sudo docker pull 10.10.10.10:5000/my-image
Using default tag: latest
Error response from daemon: unable to ping registry endpoint https://10.10.10.10:5000/v0/
v2 ping attempt failed with error: Get https://10.10.10.10:5000/v2/: tls: oversized record received with length 20527
 v1 ping attempt failed with error: Get https://10.10.10.10:5000/v1/_ping: tls: oversized record received with length 20527

$ cat /etc/sysconfig/docker
INSECURE_REGISTRY='--insecure-registry=172.22.111.192 '
``

I will fix it, but would like to get feedback on what direction I should take:

A. Update group_vars/all.yml documentation stating that a :$REGISTRY_PORT should be included for each insecure_registrys value.
B. Add an insecure_registry_port var with a default of :5000 to ansible/roles/docker/tasks/main.yml.

Additionally, instead of using INSECURE_REGISTRY in /etc/sysconfig/docker I suggest aligning with the Docker documentation [1] and use DOCKER_OPTS in /etc/sysconfig/docker to specify the --insecure-registry and other daemon options.

[1] https://docs.docker.com/registry/insecure/
",closed,False,2016-03-08 06:55:48,2018-02-14 01:01:10
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/551,https://api.github.com/repos/kubernetes/contrib/issues/551,Redirect on x-forwarded-for=http in echoserver.,"Anyone sending x-forwarded-for probably cares about the protocol, @aledbf 
",closed,True,2016-03-08 19:17:06,2016-04-13 23:47:19
contrib,aledbf,https://github.com/kubernetes/contrib/pull/552,https://api.github.com/repos/kubernetes/contrib/issues/552,Use ubuntu-slim image,"This PR removes the use of `alpine` as base image in:
- keepalived
- haproxy image
- nginx-slim image
- nginx-third-party controller
- service-loadbalancer
",closed,True,2016-03-08 23:05:20,2016-03-25 22:57:01
contrib,devlinmr,https://github.com/kubernetes/contrib/pull/553,https://api.github.com/repos/kubernetes/contrib/issues/553,SSL-enabled Nginx Ingress Controller integrated Hashicorp Vault,"Based on the nginx-alpha controller, this adds functionality to configure SSL nginx servers for ingresses requiring it, pulling in certificates from Vault.

See README.md.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/553)

<!-- Reviewable:end -->
",closed,True,2016-03-09 16:28:34,2016-11-12 12:58:09
contrib,eparis,https://github.com/kubernetes/contrib/pull/554,https://api.github.com/repos/kubernetes/contrib/issues/554,Implement a new tool to watch for cherrypick-candidates,"This requires moving some things around as we now have one binary which
serves 2 very distinct web pages (submit-queue and cherrypick-queue)

Both queues need to look at very different sets of PRs. submit-queue
cares about open PRs. Cherrypick needs to look at open and closed PRs.
While at first glance it might seem easy to just make the submit-queue
ignore closed PRs (it is easy) it would mean that the submit queue would
have to walk all 22000+ issues on every loop, obviously ignoring most of
them.

Instead for the cherry-pick we are able to limit the issues we walk to a
given label (cherrypick-candidate), so even though we are walking all
issues (open or close) that number becomes quite managable. Obviously we
can't just walk the issues with that label for the submit queue...

Thus we end up with 1 binary which is becoming two distinct 'apps' with
distinct configuration. The makefile defaults to deploying and working
with the 'submit-queue' APP, but one can use APP=cherrypick
",closed,True,2016-03-09 18:14:22,2016-03-14 13:47:43
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/555,https://api.github.com/repos/kubernetes/contrib/issues/555,Auto create firewall rule and allow user specified static ips,"As the name indicates. Still need to add a few unittests.
",closed,True,2016-03-10 03:33:38,2016-03-12 03:09:09
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/556,https://api.github.com/repos/kubernetes/contrib/issues/556,Ingress controller gets confused at edge of quota limit,"If you have exactly Quota backends and edit an Ingress to remove one and add one, the Ingress controller is unable to perform the right actions because this step fails to add the new backend (because of quota limit): https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/controller/controller.go#L265, and this step fails to remove backend, because it's being used by the url map: https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/controller/controller.go#L259. The problem is that we never try to remove it from the url map: https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/controller/controller.go#L293
",closed,False,2016-03-10 07:20:23,2018-02-14 01:01:10
contrib,aknuds1,https://github.com/kubernetes/contrib/pull/557,https://api.github.com/repos/kubernetes/contrib/issues/557,Add Terraform/AWS,"Add support for deploying on AWS through [Terraform](http://terraform.io/). _Please note that this is completely unfinished, and written specifically with CoreOS in mind at this stage_.

This is intended for a simple staging setup, with two EC2 instances: One for the master and one for the worker.

The way you use it is like this:
1. Create a file 'aws-variables.sh', containing the following variables
    \* `AWS_ACCESS_KEY_ID`: Your AWS access key ID
    \* `AWS_SECRET_ACCESS_KEY`: Your secret AWS access key
    \* `AWS_REGION`: The AWS region you wish to deploy to
    \* `AWS_MASTER_IP`: The private IP you wish to assign to your master EC2 instance
    \* `AWS_WORKER_IP`: The private IP you wish to assign to your worker EC2 instance
    \* `AWS_KEY_NAME`: The name of the SSH key pair (registered in AWS) you wish to use
2. Execute `apply-terraform.sh`

`apply-terraform.sh` will invoke terraform on the 'staging.tf' file, which I suggest you read in order to figure out what is taking place while setting up the AWS infrastructure.

I am basically creating this PR in order to seek help from others in finishing this setup, as it's not yet working and I'm finding it difficult to figure out what to do.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/557)

<!-- Reviewable:end -->
",closed,True,2016-03-10 18:46:19,2018-01-03 12:06:00
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/558,https://api.github.com/repos/kubernetes/contrib/issues/558,Ingress controller should figure out a uid on its own,"Currently to tolerate multiple clusters within the same project the Ingress controller requires users to specify a --cluster-uid flag. It should just get some token from gce that guarantees uniqueness.
",closed,False,2016-03-10 22:33:12,2016-06-03 18:29:50
contrib,roberthbailey,https://github.com/kubernetes/contrib/pull/559,https://api.github.com/repos/kubernetes/contrib/issues/559,Replace the (unused) url-list flag with a paths flag ,"To allow the loader to hit multiple url paths and/or override the default url path on the host under test.

Fix the Makefile to work properly with godeps.
",closed,True,2016-03-10 23:44:57,2016-03-11 00:03:11
contrib,roberthbailey,https://github.com/kubernetes/contrib/pull/560,https://api.github.com/repos/kubernetes/contrib/issues/560,Bump the image to the newer version.,,closed,True,2016-03-11 00:17:14,2016-03-11 00:49:09
contrib,remoe,https://github.com/kubernetes/contrib/issues/561,https://api.github.com/repos/kubernetes/contrib/issues/561,[service-loadbalancer] never in sync?,"Hi
I try to test lb:0.4 on kube 1.1.8 (coreos 899.9.0/vmware). It doesn't run on this environment because it is never in sync here:

https://github.com/kubernetes/contrib/blob/f08086d4876281f7118fdcb16518eb069c556fa7/service-loadbalancer/service_loadbalancer.go#L511

Does anyone know, what this could be?

Thanks
",closed,False,2016-03-11 22:09:05,2016-03-11 23:18:15
contrib,roberthbailey,https://github.com/kubernetes/contrib/pull/562,https://api.github.com/repos/kubernetes/contrib/issues/562,Add a flag to customize the port of the http server being,"load tested.
",closed,True,2016-03-11 23:58:15,2016-05-26 17:19:25
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/563,https://api.github.com/repos/kubernetes/contrib/issues/563,Allow a user specified global static ip via annotation.,"This pr allows a user to specify an existing global static ip through the Ingress annotation: `kubernetes.io/ingress.global-static-ip-name` and have that assigned to both the HTTP and HTTPS forwarding rules created by the Ingress controller. 

It also refactors the TLS loading logic into an interface to facilitate loading from multiple sources (on file, apiserver, fake). 
",closed,True,2016-03-12 03:08:09,2016-03-28 15:37:08
contrib,Random-Liu,https://github.com/kubernetes/contrib/issues/564,https://api.github.com/repos/kubernetes/contrib/issues/564,Perfdash should support more e2e performance test.,"Now, the implementation of _Perfdash_ is specific to Density test and Load test. It is almost impossible to add support for other e2e performance tests without a refactoring.

We should refactor and generalize the code to make it support more e2e performance test.

@gmarek @yujuhong 
/cc @kubernetes/sig-testing 
",closed,False,2016-03-12 23:58:01,2016-04-18 22:40:59
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/565,https://api.github.com/repos/kubernetes/contrib/issues/565,Refactor Perfdash to let it support more e2e performance test,"For #564 and kubernetes/kubernetes#15554.
Based on kubernetes/kubernetes#22912.

This PR refactored Perfdash to use the new [performance data type](https://github.com/Random-Liu/kubernetes/blob/kubelet-perf/test/e2e/perftype/perftype.go#L45) added in kubernetes/kubernetes#22912.

With this PR, it is easy to add more e2e performance test support. You just need to:
1) Transform e2e performance test result into [`PerfData`](https://github.com/Random-Liu/kubernetes/blob/kubelet-perf/test/e2e/perftype/perftype.go#L45) in _k8s/kubernetes/test/e2e/perftype_, and print the `PerfData` in e2e test log.
2) Add corresponding jenkins project into `JenkinsProjects` in `config.go`.
3) Add test description and test name into `DescriptionName` in `config.go`.
Then, Perfdash will automatically and periodically fetch data from specified jenkins projects, parse the test log and generate graph.

I've tested the PR locally with a fake `downloader`. However it would be better to merge kubernetes/kubernetes#22912 first, and test this PR with real jenkins project.

@gmarek @yujuhong 
/cc @kubernetes/sig-testing 
",closed,True,2016-03-13 01:13:11,2016-04-18 22:08:31
contrib,eparis,https://github.com/kubernetes/contrib/pull/566,https://api.github.com/repos/kubernetes/contrib/issues/566,Clear 'cherrypick-candidate' if a PR was already merged,,closed,True,2016-03-15 02:03:42,2016-03-15 17:07:56
contrib,rosc,https://github.com/kubernetes/contrib/issues/567,https://api.github.com/repos/kubernetes/contrib/issues/567,Keepalived FAULT state,,closed,False,2016-03-15 14:08:44,2016-03-15 14:13:57
contrib,eparis,https://github.com/kubernetes/contrib/pull/568,https://api.github.com/repos/kubernetes/contrib/issues/568,Add do-not-merge to all cherry-picks which have not been approved,"We do not remove the label when the approval is given, since its hard to
know if this is the only reason not to merge the PR....
",closed,True,2016-03-15 23:36:02,2016-03-16 15:12:31
contrib,talset,https://github.com/kubernetes/contrib/pull/569,https://api.github.com/repos/kubernetes/contrib/issues/569,[service-loadbalancer] Update README,"In the service-loadbalancer Examples we use all-namespaces and the description say 'namespace=kube-system'

Just a little update to fix the description

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/569)

<!-- Reviewable:end -->
",closed,True,2016-03-16 10:18:42,2016-10-21 19:34:27
contrib,aledbf,https://github.com/kubernetes/contrib/pull/570,https://api.github.com/repos/kubernetes/contrib/issues/570,Update nginx controller to use endpoints. Remove lua to configure upstreams,"This PR:
- removes lua code to configure upstreams
- adds SSL support for ingress rules
- expose TCP services using a ConfigMap
- nginx.conf customization using a ConfigMap
- improves the verbose logs showing the diff in the configuration file
- dump configmap example

Using the flag `--v=2` it is possible to see in the logs the changes in the configuration when an endpoint is added/removed

```
I0316 12:24:37.581267       1 utils.go:148] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I0316 12:24:37.581356       1 utils.go:149] --- /tmp/922554809  2016-03-16 12:24:37.000000000 +0000
+++ /tmp/079811012  2016-03-16 12:24:37.000000000 +0000
@@ -235,7 +235,6 @@

     upstream default-echoheadersx {
         least_conn;
-        server 10.2.112.124:5000;
         server 10.2.208.50:5000;

     }
I0316 12:24:37.610073       1 command.go:69] change in configuration detected. Reloading...
```

Using the same flag but in level 5 `--v=5` it will configure nginx to run in debug mode

```
$ ./nginx-third-party-lb --dump-nginx—configuration
Example of ConfigMap to customize NGINX configuration:
data:
  body-size: 1m
  error-log-level: info
  gzip-types: application/atom+xml application/javascript application/json application/rss+xml
    application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json
    application/xhtml+xml application/xml font/opentype image/svg+xml image/x-icon
    text/css text/plain text/x-component
  hts-include-subdomains: ""true""
  hts-max-age: ""15724800""
  keep-alive: ""75""
  max-worker-connections: ""16384""
  proxy-connect-timeout: ""30""
  proxy-read-timeout: ""30""
  proxy-real-ip-cidr: 0.0.0.0/0
  proxy-send-timeout: ""30""
  server-name-hash-bucket-size: ""64""
  server-name-hash-max-size: ""512""
  ssl-buffer-size: 4k
  ssl-ciphers: ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA
  ssl-protocols: TLSv1 TLSv1.1 TLSv1.2
  ssl-session-cache: ""true""
  ssl-session-cache-size: 10m
  ssl-session-tickets: ""true""
  ssl-session-timeout: 10m
  use-gzip: ""true""
  use-hts: ""true""
  worker-processes: ""8""
metadata:
  name: custom-name
  namespace: a-valid-namespace
```

This enables customizations like just updating the timeouts:

```
$ kubectl get configmap nginx-load-balancer-conf -o yaml
apiVersion: v1
data:
  proxy-connect-timeout: ""10""
  proxy-read-timeout: ""120""
  proxy-send-imeout: ""120""
kind: ConfigMap
metadata:
  name: nginx-load-balancer-conf
```
",closed,True,2016-03-16 11:42:53,2016-03-25 19:06:01
contrib,gmarek,https://github.com/kubernetes/contrib/pull/571,https://api.github.com/repos/kubernetes/contrib/issues/571,Remove kubemark-100 from merge blockers.,"cc @eparis 
",closed,True,2016-03-16 12:30:33,2016-03-18 14:29:09
contrib,rimusz,https://github.com/kubernetes/contrib/pull/572,https://api.github.com/repos/kubernetes/contrib/issues/572,add internal load balancer for GKE,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/572)

<!-- Reviewable:end -->
",closed,True,2016-03-16 16:14:36,2017-12-19 12:37:03
contrib,talset,https://github.com/kubernetes/contrib/issues/573,https://api.github.com/repos/kubernetes/contrib/issues/573,[service-loadbalancer] Name based virtual doc,"In the document it is written 

```
Name based virtual hosting: Currently undocumented but possible via annotations.
```

How can we define the Host field ?
",closed,False,2016-03-16 16:52:52,2017-12-15 08:09:50
contrib,talset,https://github.com/kubernetes/contrib/pull/574,https://api.github.com/repos/kubernetes/contrib/issues/574,[ansible] Add uninstall playbook,"- create a playbooks and adhoc directory to put additional playbook
- Add uninstall.yml in order to uninstall or re-install the cluster with ansible
",closed,True,2016-03-16 20:34:03,2016-03-21 13:27:02
contrib,gmarek,https://github.com/kubernetes/contrib/pull/575,https://api.github.com/repos/kubernetes/contrib/issues/575,Create a test-util directory to gather test-handling related functions,"cc @spxtr @kubernetes/sig-scalability @kubernetes/sig-testing 
",closed,True,2016-03-17 15:04:06,2016-03-18 14:15:42
contrib,MikeSpreitzer,https://github.com/kubernetes/contrib/issues/576,https://api.github.com/repos/kubernetes/contrib/issues/576,Need a simple example CNI network plugin,"There should be a simple example CNI plugin.
",closed,False,2016-03-17 19:33:15,2016-04-19 02:58:18
contrib,jojimt,https://github.com/kubernetes/contrib/issues/577,https://api.github.com/repos/kubernetes/contrib/issues/577,Support Contiv Networking with Kubernetes,"Contiv (https://github.com/contiv) provides policy based container networking with kubernetes integration via CNI plugin as documented in https://github.com/contiv/netplugin/tree/master/mgmtfn/k8splugin. I would like to upstream the corresponding ansible changes to the contrib/ansible repo.
",closed,False,2016-03-17 19:44:39,2016-04-20 21:14:51
contrib,gmarek,https://github.com/kubernetes/contrib/pull/578,https://api.github.com/repos/kubernetes/contrib/issues/578,munger: Add a on option for e2e status checker to read data from GCS instead of Jenkins,"It's first of the hacks I need to get it to add more complex logic for handling scalability test failures. It's no-op as it currently does a separate check and only prints errors if it finds differences between Jenkins and GCS. We should keep it running for few days before switching to GCS only.

It's a small hack that I need, as @justinsb PRs are stalled. It's small though, so it should be pretty straightforward to review - @eparis can you review the changes from the second commit only? It'd be great if you could do this sooner rather than later.

cc @kubernetes/sig-scalability @kubernetes/sig-testing @spxtr @wojtek-t 
",closed,True,2016-03-17 21:08:30,2016-03-22 14:55:45
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/579,https://api.github.com/repos/kubernetes/contrib/issues/579,"Add TLS section to GLBC docs, and BETA_LIMITATIONS doc.","Documenting beta limitations is a pain for a controller with so many twists, because you start to feel bad for not fixing them. Also adds TLS docs. 
@kubernetes/goog-cluster 
",closed,True,2016-03-18 00:04:20,2016-03-19 21:38:32
contrib,MikeSpreitzer,https://github.com/kubernetes/contrib/pull/580,https://api.github.com/repos/kubernetes/contrib/issues/580,Introduced a simple example of a CNI plugin,"Resolves #576
",closed,True,2016-03-18 02:52:07,2016-04-19 02:58:18
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/581,https://api.github.com/repos/kubernetes/contrib/issues/581,Try official docker client in docker micro benchmark,"I changed the docker micro benchmark to using new docker client https://github.com/docker/engine-api, so as to:
1) Test whether there is performance regression (There shouldn't be, just by the way)
2) Have a try of new docker client.
It turns out that there is no performance difference.

@yujuhong @dchen1107 
",closed,True,2016-03-18 07:02:40,2016-06-03 18:09:52
contrib,eparis,https://github.com/kubernetes/contrib/pull/582,https://api.github.com/repos/kubernetes/contrib/issues/582,Skip golint on golang 1.4,"apparently lint dropped 1.4 support.
",closed,True,2016-03-18 15:00:34,2016-03-18 18:25:06
contrib,eparis,https://github.com/kubernetes/contrib/pull/583,https://api.github.com/repos/kubernetes/contrib/issues/583,Display icons in history when e2e starts failing and passing,,closed,True,2016-03-18 16:53:39,2016-03-18 17:19:27
contrib,eparis,https://github.com/kubernetes/contrib/pull/584,https://api.github.com/repos/kubernetes/contrib/issues/584,[mungegithub] Update godeps so we are at the latest and greatest,,closed,True,2016-03-18 17:20:19,2016-03-18 17:42:47
contrib,unicell,https://github.com/kubernetes/contrib/pull/585,https://api.github.com/repos/kubernetes/contrib/issues/585,Ansible: Add flannel support for Ubuntu with upstart,,closed,True,2016-03-18 17:57:44,2016-03-18 18:20:04
contrib,MansM,https://github.com/kubernetes/contrib/issues/586,https://api.github.com/repos/kubernetes/contrib/issues/586,ansible Flanneld on centos doesnt start,"TASK [flannel : Launch Flannel] ************************************************
changed: [kube-master]
fatal: [kube-node-1]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Job for flanneld.service failed because a timeout was exceeded. See \""systemctl status flanneld.service\"" and \""journalctl -xe\"" for details.\n""}
fatal: [kube-node-2]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Job for flanneld.service failed because a timeout was exceeded. See \""systemctl status flanneld.service\"" and \""journalctl -xe\"" for details.\n""}

journalctl -xe on node-1:
Mar 18 15:44:52 kube-node-1 flanneld[13777]: E0318 15:44:52.157605 13777 network.go:53] Failed to retrieve network config: client: etcd cluster is unavailable or misconfigured
Mar 18 15:44:53 kube-node-1 flanneld[13777]: E0318 15:44:53.158587 13777 network.go:53] Failed to retrieve network config: client: etcd cluster is unavailable or misconfigured
Mar 18 15:44:54 kube-node-1 flanneld[13777]: E0318 15:44:54.160032 13777 network.go:53] Failed to retrieve network config: client: etcd cluster is unavailable or misconfigured

[root@kube-master ~]# flanneld --version
0.5.3

[root@kube-master ~]# etcd --version
etcd Version: 2.2.2
Git SHA: 6335fdc
Go Version: go1.4.2
Go OS/Arch: linux/amd64

didnt make any changes to any vars
vagrant 1.8.1 (virtualbox)
ansible 2.0.1.0
OSX
",closed,False,2016-03-18 19:46:27,2016-03-27 13:08:06
contrib,jojimt,https://github.com/kubernetes/contrib/pull/587,https://api.github.com/repos/kubernetes/contrib/issues/587,Support Contiv Networking with Kubernetes,"Please refer https://github.com/kubernetes/contrib/issues/577.
Summary of changes:
1. Added a new role named contiv. This contains all contiv specific configuration
2. Added an option to start etcdin proxy mode on the nodes
3. Made some parameters configurable via variable settings, leaving the default unchanged with respect to original code.
",closed,True,2016-03-18 22:17:14,2016-03-24 18:30:58
contrib,unicell,https://github.com/kubernetes/contrib/pull/588,https://api.github.com/repos/kubernetes/contrib/issues/588,Ansible: Add flannel support for Ubuntu 14.04,,closed,True,2016-03-18 22:26:10,2016-03-29 14:54:18
contrib,spxtr,https://github.com/kubernetes/contrib/issues/589,https://api.github.com/repos/kubernetes/contrib/issues/589,Submit queue history should include manual merges,"Right now the history is incomplete, because we just stop getting updates about PRs that were once in the queue but got manually merged. There should be an event for that.
",closed,False,2016-03-19 00:07:16,2016-05-16 18:51:29
contrib,pwittrock,https://github.com/kubernetes/contrib/pull/590,https://api.github.com/repos/kubernetes/contrib/issues/590,Copy mungedocs,,closed,True,2016-03-19 00:49:28,2016-05-04 23:20:35
contrib,thockin,https://github.com/kubernetes/contrib/pull/591,https://api.github.com/repos/kubernetes/contrib/issues/591,Add a pv-provisioning demo,,closed,True,2016-03-19 04:21:44,2016-03-22 18:35:36
contrib,eparis,https://github.com/kubernetes/contrib/pull/592,https://api.github.com/repos/kubernetes/contrib/issues/592,submit-queue: Move the icons to the correct place.,"The place changed between the PR getting created and getting merged.
",closed,True,2016-03-19 15:37:33,2016-03-19 15:37:39
contrib,eparis,https://github.com/kubernetes/contrib/pull/593,https://api.github.com/repos/kubernetes/contrib/issues/593,submit-queue: gzip all responses,"PR list goes from 89.93k to 16.21k
",closed,True,2016-03-19 16:13:38,2016-03-22 15:32:24
contrib,gmarek,https://github.com/kubernetes/contrib/pull/594,https://api.github.com/repos/kubernetes/contrib/issues/594,WIP: First commit of a tool that reconfigures tests in response to test results,,closed,True,2016-03-21 13:31:15,2016-03-21 13:35:05
contrib,gmarek,https://github.com/kubernetes/contrib/pull/595,https://api.github.com/repos/kubernetes/contrib/issues/595,WIP: First commit of a tool that reconfigures tests in response to test re…,"…sults.

It's not done and extremely hacky, but if someone want to take a look I'm happy to hear any feedback.

cc @wojtek-t @eparis @kubernetes/sig-testing 
",closed,True,2016-03-21 16:41:52,2016-08-03 07:29:11
contrib,danehans,https://github.com/kubernetes/contrib/pull/596,https://api.github.com/repos/kubernetes/contrib/issues/596,Changes ansible_fqdn to ansible_default_ipv4.address in Ansible,"Previously ansible_fqdn was being used to set certain config
parameters. This resolves to 'localhost.localdomain' in too many
situations. This commit replaces ansible_fqdn with
ansible_default_ipv4.address to remove the need for name
resolution.

Fixes Issue: https://github.com/kubernetes/contrib/issues/508
",closed,True,2016-03-21 16:57:42,2016-03-22 00:20:15
contrib,ixdy,https://github.com/kubernetes/contrib/pull/597,https://api.github.com/repos/kubernetes/contrib/issues/597,Fix editdocs link,"Ref https://github.com/kubernetes/kubernetes/pull/23230#issuecomment-199412043

cc @janetkuo 
",closed,True,2016-03-21 18:43:32,2016-03-21 19:15:31
contrib,eparis,https://github.com/kubernetes/contrib/pull/598,https://api.github.com/repos/kubernetes/contrib/issues/598,submit-queue: do not label/comment if issue already has label,"bot got a 'little' spammy https://github.com/kubernetes/kubernetes/pull/22126
",closed,True,2016-03-21 20:30:19,2016-03-21 20:30:34
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/599,https://api.github.com/repos/kubernetes/contrib/issues/599,Ansible: CoreOS Support breaks RHEL Support,"By introducing support for CoreOS, I can no longer run playbook on RHEL. 

``` vim
05:37:33 TASK: [common | Set the bin directory path for CoreOS] ************************ 
05:37:33 fatal: [10.8.55.250] => error while evaluating conditional: ansible_lsb.id == ""CoreOS""
```

When running

```
ansible --inventory-file=INVENTORY_FILE masters -m setup
```

ansible_lsb is not defined.

What about to use `ansible_distribution` instead of `ansible_lsb`?

[1] https://github.com/kubernetes/contrib/pull/548
",closed,False,2016-03-22 10:03:18,2016-03-22 13:15:02
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/600,https://api.github.com/repos/kubernetes/contrib/issues/600,CoreOS Support: fix detection of CoreOS related tasks,"Currently, the playbook is broken on RHEL due to undefined facts:
- change ansible_lsb.id to ansible_distribution
- install flannel from github only on CoreOS

Fixes #599 
",closed,True,2016-03-22 11:32:02,2016-03-22 18:14:18
contrib,eparis,https://github.com/kubernetes/contrib/pull/601,https://api.github.com/repos/kubernetes/contrib/issues/601,mungegithub: Add a `make test` option,"We shouldn't build a container if the tests don't run.
",closed,True,2016-03-22 15:35:26,2016-03-22 15:35:39
contrib,eparis,https://github.com/kubernetes/contrib/issues/602,https://api.github.com/repos/kubernetes/contrib/issues/602,Switch munger to use GCS as a source of truth instead of Jenkins.,"```
I0322 17:13:04.205714       1 e2e.go:119] Got a non-success response 404 while reading data for kubernetes-e2e-gke/3126/finished.json
E0322 17:13:04.206332       1 submit-queue.go:242] GCS stable check returned different value than Jenkins.
```
",closed,False,2016-03-22 17:19:40,2016-04-21 20:31:36
contrib,danehans,https://github.com/kubernetes/contrib/issues/603,https://api.github.com/repos/kubernetes/contrib/issues/603,PR #600 Broke CoreOS Support,"https://github.com/kubernetes/contrib/pull/600 broke coreos support. As described in https://github.com/kubernetes/contrib/pull/600 the ansible_distribution fact does not support CoreOS. https://github.com/ansible/ansible/issues/15096 was created in the Ansible community to address this limitation/bug. In the meantime, ansible_lsb.id must be used as the fact to determine CoreOS.
",closed,False,2016-03-22 19:22:02,2016-03-24 21:49:07
contrib,danehans,https://github.com/kubernetes/contrib/pull/604,https://api.github.com/repos/kubernetes/contrib/issues/604,Fixes Ansible fact for determining CoreOS,"https://github.com/kubernetes/contrib/pull/600 broke CoreOS
support. This is because the ansible_distribution does not
properly support CoreOS. The following issue has been filed
in the Ansible community to address this problem:

https://github.com/ansible/ansible/issues/15096

Until the above issue is resolved, the ansible_lsb.id fact
must be used to support CoreOS.

Fixes Issue: https://github.com/kubernetes/contrib/issues/603
",closed,True,2016-03-22 19:27:40,2016-03-25 19:02:28
contrib,gmarek,https://github.com/kubernetes/contrib/pull/605,https://api.github.com/repos/kubernetes/contrib/issues/605,Move CheckFinishedStatus to test-utils,"@eparis @justinsb 
",closed,True,2016-03-23 12:29:34,2016-03-23 13:09:31
contrib,flashvoid,https://github.com/kubernetes/contrib/pull/606,https://api.github.com/repos/kubernetes/contrib/issues/606,Dev/rsearch,"# Reverse search

This is a utility for reverse search in kubernetes (find all objects who select given label).
# Usage

Provided binary can act as either client or server depending on command line options.

Server instance connects to Kubernetes API and monitors resource changes

```
./rsearch -c go/src/github.com/romana/contrib/rsearch/config.ini -s
```

Client then can query the server

```
./rsearch -c go/src/github.com/romana/contrib/rsearch/config.ini -h http://localhost -r 'tier/backend#'
```
- `-r` label to query, `#` sign is not a part of a label but terminator that need to be added to request (TODO at this point i'm not really sure it's needed)
- `-h` url address where server instance is running 
",closed,True,2016-03-23 12:41:44,2016-03-23 12:46:40
contrib,eparis,https://github.com/kubernetes/contrib/pull/607,https://api.github.com/repos/kubernetes/contrib/issues/607,mungegithub: do not set empty assignees is none set,"Some OWNERS files might have no assignees, and that's fine.
",closed,True,2016-03-23 13:16:18,2016-03-24 02:58:38
contrib,rutsky,https://github.com/kubernetes/contrib/pull/608,https://api.github.com/repos/kubernetes/contrib/issues/608,add missing list separator,"/cc @ingvagabund
",closed,True,2016-03-23 14:10:04,2016-03-24 02:03:36
contrib,roberthbailey,https://github.com/kubernetes/contrib/pull/609,https://api.github.com/repos/kubernetes/contrib/issues/609,Add andyzheng0831 to the whitelist,"So I don't have to keep flipping the labels around. 

/cc @andyzheng0831 
",closed,True,2016-03-23 20:25:26,2016-03-24 02:10:34
contrib,rutsky,https://github.com/kubernetes/contrib/pull/610,https://api.github.com/repos/kubernetes/contrib/issues/610,add Vagrant working directory to gitignore,"Vagrant writes in `.vagrant` directory cache information.
",closed,True,2016-03-23 21:11:05,2016-03-24 02:12:29
contrib,rutsky,https://github.com/kubernetes/contrib/pull/611,https://api.github.com/repos/kubernetes/contrib/issues/611,add ansible configuration for running it from vagrant directory,"This allows to run ansible directly on provisioned with Vagrant cluster
(from the `vagrant` directory).
",closed,True,2016-03-23 21:24:10,2016-03-24 02:14:01
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/612,https://api.github.com/repos/kubernetes/contrib/issues/612,"munger/mergebot: get config from file, so we can configure with a ConfigMap","Probably low priority but would be nifty to use our features like they're supposed to be used.

@eparis 
",closed,False,2016-03-23 21:32:49,2018-02-13 08:45:08
contrib,pwittrock,https://github.com/kubernetes/contrib/pull/613,https://api.github.com/repos/kubernetes/contrib/issues/613,Add kubelet-gce-e2e-ci jenkins job as merge queue blocker,"History:
http://kubekins.dls.corp.google.com/view/Node/job/kubelet-gce-e2e-ci/buildTimeTrend
",closed,True,2016-03-23 21:42:11,2016-03-24 17:13:45
contrib,eparis,https://github.com/kubernetes/contrib/pull/614,https://api.github.com/repos/kubernetes/contrib/issues/614,mungegithub: do not set empty assignees is none set,"Some OWNERS files might have no assignees, and that's fine.
",closed,True,2016-03-24 02:58:57,2016-03-24 02:59:42
contrib,eparis,https://github.com/kubernetes/contrib/pull/615,https://api.github.com/repos/kubernetes/contrib/issues/615,New munger to manage 'release-note' labels,"Rules:
All PRs must have one of
- `release-note`
- `release-note-action-required`
- `release-note-none`
- `release-note-label-needed`

Add `release-note-label-needed` to any PR without one of the above.

If a PR gets lgtm and still has `release-note-label-needed` remove LGTM and
message where to get more info.
",closed,True,2016-03-24 04:04:14,2016-03-29 21:37:37
contrib,justinsb,https://github.com/kubernetes/contrib/pull/616,https://api.github.com/repos/kubernetes/contrib/issues/616,munger: update location of WordSepNormalizeFunc,"It moved from k8s.io/kubernetes/pkg/util to
k8s.io/kubernetes/pkg/util/flag
",closed,True,2016-03-24 05:18:32,2016-03-24 15:57:44
contrib,adamschaub,https://github.com/kubernetes/contrib/pull/617,https://api.github.com/repos/kubernetes/contrib/issues/617,Ansibilizes CoreOS bootstrapping process.,"Ansibilizes python bootstrapping process for CoreOS. 

Unpacks bootstrap.sh into a series of raw Ansible tasks, giving a more transparent view into the python bootstrapping process.
",closed,True,2016-03-24 06:01:59,2016-03-24 20:29:23
contrib,deromka,https://github.com/kubernetes/contrib/pull/618,https://api.github.com/repos/kubernetes/contrib/issues/618,Fixed wrong KUBECTL_BIN,"Reverted the KUBECTL_BIN to the correct value: /usr/bin/kubectl, since the new one {{ bin_dir }} was referencing the common.yaml default which is /usr/local/bin
And there is not kubectl overthere, thus the addon services were not running

Here is the kube-addons service logs with the {{ bin_dir }} value:

`[root@kube-master system]# systemctl status -l kube-addons
● kube-addons.service - Kubernetes Addon Object Manager
   Loaded: loaded (/etc/systemd/system/kube-addons.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2016-03-23 21:01:15 GMT; 12h ago
     Docs: https://github.com/GoogleCloudPlatform/kubernetes
 Main PID: 27455 (kube-addons.sh)
   CGroup: /system.slice/kube-addons.service
           ├─27455 /bin/bash /usr/libexec/kubernetes/kube-addons.sh
           └─93200 sleep .5

Mar 24 09:17:43 kube-master kube-addons.sh[27455]: /usr/libexec/kubernetes/kube-addons.sh: line 155: /usr/local/bin/kubectl: No such file or directory
Mar 24 09:17:43 kube-master kube-addons.sh[27455]: /usr/libexec/kubernetes/kube-addons.sh: line 155: /usr/local/bin/kubectl: No such file or directory
Mar 24 09:17:44 kube-master kube-addons.sh[27455]: /usr/libexec/kubernetes/kube-addons.sh: line 155: /usr/local/bin/kubectl: No such file or directory
Mar 24 09:17:44 kube-master kube-addons.sh[27455]: /usr/libexec/kubernetes/kube-addons.sh: line 155: /usr/local/bin/kubectl: No such file or directory
Mar 24 09:17:45 kube-master kube-addons.sh[27455]: /usr/libexec/kubernetes/kube-addons.sh: line 155: /usr/local/bin/kubectl: No such file or directory
Mar 24 09:17:45 kube-master kube-addons.sh[27455]: /usr/libexec/kubernetes/kube-addons.sh: line 155: /usr/local/bin/kubectl: No such file or directory
Mar 24 09:17:46 kube-master kube-addons.sh[27455]: /usr/libexec/kubernetes/kube-addons.sh: line 155: /usr/local/bin/kubectl: No such file or directory
Mar 24 09:17:46 kube-master kube-addons.sh[27455]: /usr/libexec/kubernetes/kube-addons.sh: line 155: /usr/local/bin/kubectl: No such file or directory
Mar 24 09:17:47 kube-master kube-addons.sh[27455]: /usr/libexec/kubernetes/kube-addons.sh: line 155: /usr/local/bin/kubectl: No such file or directory
Mar 24 09:17:47 kube-master kube-addons.sh[27455]: /usr/libexec/kubernetes/kube-addons.sh: line 155: /usr/local/bin/kubectl: No such file or directory
[root@kube-master system]# which kubectl
/bin/kubectl
[root@kube-master system]# ps -`
",closed,True,2016-03-24 09:52:20,2016-03-28 14:39:08
contrib,rutsky,https://github.com/kubernetes/contrib/pull/619,https://api.github.com/repos/kubernetes/contrib/issues/619,fix ansible+vagrant+virtualbox+centos7 configuration,"Changes in Ansible + Vagrant configuration:
- By default on VirtualBox public interface is not `eth0`, but `eth1`: I automatically use proper interface for flannel and when generating `/etc/hosts`.
  
  This should fix #586 .
- Fix `bin_dir` on CentOS with K8S installed from packages.

These changes are _almost_ enough to start K8S cluster in Vagrant on VirtualBox + CentOS 7.
Unfortunately CentOS 7 yet have only [1.2.0alpha1](https://git.centos.org/summary/rpms!kubernetes.git) which doesn't include [commit with `successThreshold` and `failureThreshold` liveness probe parameter](https://github.com/kubernetes/kubernetes/commit/1e88a682da871a4a7f811dc5efcfe3890be8bbf8). This leads to errors in `kube-addons` like following one:

```
мар 24 09:43:38 kube-master kube-addons.sh[30042]: error validating ""/etc/kubernetes/addons/dns/skydns-rc.yaml"": error validating data: [found invalid field successThreshold for v1.Probe, found invalid field failureThreshold for v1.Probe]; if you choose to ignore these errors, turn validation off with --validate=false
```

To bypass this issue I locally patched kube-addons scripts to include `--validate=false` in `kubectl` invocations:

```
diff --git a/ansible/roles/kubernetes-addons/files/kube-addon-update.sh b/ansible/roles/kubernetes-addons/files/kube-addon-update.sh
index ede2832..ab0e023 100755
--- a/ansible/roles/kubernetes-addons/files/kube-addon-update.sh
+++ b/ansible/roles/kubernetes-addons/files/kube-addon-update.sh
@@ -259,7 +259,7 @@ function create-object() {
     log INFO ""Creating new ${obj_type} from file ${file_path} in namespace ${namespace}, name: ${obj_name}""
     # this will keep on failing if the ${file_path} disappeared in the meantime.
     # Do not use too many retries.
-    run-until-success ""${KUBECTL} create --namespace=${namespace} -f ${file_path}"" ${NUM_TRIES} ${DELAY_AFTER_ERROR_SEC}
+    run-until-success ""${KUBECTL} create --validate=false --namespace=${namespace} -f ${file_path}"" ${NUM_TRIES} ${DELAY_AFTER_ERROR_SEC}
 }

 function update-object() {
diff --git a/ansible/roles/kubernetes-addons/files/kube-addons.sh b/ansible/roles/kubernetes-addons/files/kube-addons.sh
index db555fe..916f4e4 100644
--- a/ansible/roles/kubernetes-addons/files/kube-addons.sh
+++ b/ansible/roles/kubernetes-addons/files/kube-addons.sh
@@ -118,7 +118,7 @@ function create-resource-from-string() {
   local -r config_name=$4;
   local -r namespace=$5;
   while [ ${tries} -gt 0 ]; do
-    echo ""${config_string}"" | ${KUBECTL} --namespace=""${namespace}"" create -f - && \
+    echo ""${config_string}"" | ${KUBECTL} --validate=false --namespace=""${namespace}"" create -f - && \
         echo ""== Successfully started ${config_name} in namespace ${namespace} at $(date -Is)"" && \
         return 0;
     let tries=tries-1;
```

Note that I tested with `source_type: packageManager` in `group_vars/all.yml`.

Cluster starts with:

```
$ vagrant up --provider virtualbox
[...]
PLAY RECAP *********************************************************************
kube-master                : ok=260  changed=78   unreachable=0    failed=0   
kube-node-1                : ok=125  changed=39   unreachable=0    failed=0   
kube-node-2                : ok=124  changed=39   unreachable=0    failed=0 
```

On kube-master:

```
[root@kube-master ~]# kubectl get nodes
NAME          LABELS                               STATUS    AGE
kube-node-1   kubernetes.io/hostname=kube-node-1   Ready     8m
kube-node-2   kubernetes.io/hostname=kube-node-2   Ready     8m
[root@kube-master ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                   READY     STATUS             RESTARTS   AGE
kube-system   elasticsearch-logging-v1-dq4kh         1/1       Running            0          9m
kube-system   elasticsearch-logging-v1-hkymz         1/1       Running            0          9m
kube-system   fluentd-elasticsearch-kube-node-1      1/1       Running            0          7m
kube-system   fluentd-elasticsearch-kube-node-2      1/1       Running            0          6m
kube-system   kibana-logging-v1-irn4b                0/1       CrashLoopBackOff   5          9m
kube-system   kube-dns-v11-0u775                     3/4       Running            3          9m
kube-system   monitoring-influxdb-grafana-v3-clyo8   2/2       Running            0          9m
```

I haven't tested that configured cluster is actually working (see failing kibana-logging above), but at least it looks like properly configured.

My goal is to setup cluster on CoreOS in Vagrant and these changes are required for this anyway (e.g. proper network interface configuration).
",closed,True,2016-03-24 14:31:33,2016-03-28 12:55:30
contrib,feliksik,https://github.com/kubernetes/contrib/pull/620,https://api.github.com/repos/kubernetes/contrib/issues/620,deploy to ubuntu 15.10 [WIP,"I have recently been working to deploy to Ubuntu 15.10 (preparing for systemd in the 16.04 LTS release). I would like to contribute, but am new to the k8s community. I like to have some alignment that assures me we will push the same direction. This PR is intended to gather some insight on how to make the ansible module more portable, as I think that is nontrivial. 

I see multiple people are using the contrib/ansible repo ( #428 #619 #586 #508 #406, a simple search: https://github.com/kubernetes/contrib/search?utf8=%E2%9C%93&q=ansible&type=Issues ). Some nice people like @eparis @rsmitty @danehans @rutsky have been working on improving it. 

I see that we attempt to make a number of things flexible: 
- k8s version, and binary/source distribution (i see the source_type variable is used). 
- OS distribution (the `when: ansible_distribution != ""CoreOS""` is used in roles/tasks/flannel/client). I am afraid this when: thingy is tricky to use for all kinds of different OS's. 
- the ip addresses of the peers, and the way in which to define this (`ansible_hostname` or `ansible_fqdn`, but what about if we want to use an internal network?)
- the users that are assumed to be present on the system (e.g. 'etcd' was a user not present on ubuntu; maybe no assumptions should be made at all)
- the location where `etcdctl` turns out to be installed (`usr/bin/etcdctl` didn't hold after installing on ubuntu)

Some choice to make things more portable: 
- standardize on certain choices (e.g. no matter the OS, the implementation of the etcdctl installer _must_ create a binary in /usr/bin ; but I think a better standard would be: no matter the OS, the binaries must end up in $PATH and we don't use a full path in calling the binary);
- be configurable (e.g. the IP address of the peers). You can configure in group_vars/all, but you can also split this into separate groups. Currently i see ansible_fqdn and ansible_hostnames used, but this is pretty big assumption (imagine eth1 is your internal net you which to use -- not external eth0)
- be modular (e.g. the tool that configures the network for ubuntu). 

Tools that create flexibility: 
- use of `group_vars`
- use of `host_vars`
- indirect variables (you can set `i_am_kubelet_with_name: {{ansible_hostname}}` in group_vars/nodes and it will be defined for all hosts in `nodes` group, but have a different value)
- modularity 1: dynamic filename of  `include: install_flannel_on_{{ansible_distribution}}.yml`
- modularity 2: running things conditionally with `when: ansible_distribution == ""CoreOS""`

I happen to have quite some experience with Ansible and have made significant effort with group_vars, host_vars, and levels of indirection to make things highly configurable. Ansible is super-liberal in how to approach such issues, making things very much a matter of taste.
It is certainly possible do do it more robust/portable, but I also figured it is not trivial (especially since you don't easily test on all environments). I think it requires a well chosen set of best practices. Is there a philosophy on how to these portability issues? How feels/is in the lead?

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/620)

<!-- Reviewable:end -->
",closed,True,2016-03-24 17:23:29,2016-09-23 07:54:10
contrib,adamschaub,https://github.com/kubernetes/contrib/pull/621,https://api.github.com/repos/kubernetes/contrib/issues/621,Ansibilizes CoreOS bootstrapping process.,"Ansibilizes python bootstrapping process for CoreOS.

Unpacks bootstrap.sh into a series of raw Ansible tasks, giving a more transparent view into the python bootstrapping process.
",closed,True,2016-03-24 20:32:43,2016-03-29 20:41:06
contrib,danehans,https://github.com/kubernetes/contrib/pull/622,https://api.github.com/repos/kubernetes/contrib/issues/622,Updates the Default bin_dir for Ansible,"Previously, the default bin_dir did not work for some platforms.
The new default follows the contrib/init project settings.

Fixes Issue: https://github.com/kubernetes/contrib/pull/618
",closed,True,2016-03-24 21:10:01,2016-03-24 21:38:54
contrib,danehans,https://github.com/kubernetes/contrib/issues/623,https://api.github.com/repos/kubernetes/contrib/issues/623,Kube Addons python_bin fact is not being properly updated for CoreOS,"```
$ systemctl status kube-addons
● kube-addons.service - Kubernetes Addon Object Manager
   Loaded: loaded (/etc/systemd/system/kube-addons.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2016-03-24 16:45:20 UTC; 1h 23min ago
     Docs: https://github.com/GoogleCloudPlatform/kubernetes
 Main PID: 5160 (kube-addons.sh)
   Memory: 980.0K
      CPU: 1min 839ms
   CGroup: /system.slice/kube-addons.service
           ├─ 5160 /bin/bash /opt/bin/kubernetes/kube-addons.sh
           └─26184 sleep 600

Mar 24 18:05:20 core-master kube-addons.sh[5160]: cat: write error: Broken pipe
Mar 24 18:05:20 core-master kube-addons.sh[5160]: /opt/bin/kubernetes/kube-addon-update.sh: line 102: python: command not found
Mar 24 18:05:20 core-master kube-addons.sh[5160]: /opt/bin/kubernetes/kube-addon-update.sh: line 102: python: command not found
Mar 24 18:05:20 core-master kube-addons.sh[5160]: cat: write error: Broken pipe
```

PYTHON_BIN env var from kube-addon-update.sh:

```
PYTHON_BIN=${PYTHON_BIN:-python}
```

^ should show PYTHON_BIN=${PYTHON_BIN:-/opt/bin/python} for CoreOS deployments.
",closed,False,2016-03-24 21:20:22,2016-03-24 21:48:26
contrib,danehans,https://github.com/kubernetes/contrib/pull/624,https://api.github.com/repos/kubernetes/contrib/issues/624,Fixes python_bin path for Kube Addons in Ansible,"Previously, the python_path was not being properly updated for
CoreOS systems. This patch moves python_bin fact management
from the kubernetes-addons role to the kubernetes role.

Fixes Issue: https://github.com/kubernetes/contrib/issues/623
",closed,True,2016-03-24 21:24:43,2016-03-24 21:42:37
contrib,jojimt,https://github.com/kubernetes/contrib/pull/625,https://api.github.com/repos/kubernetes/contrib/issues/625,Add contiv networking support (https://github.com/kubernetes/contrib/issues/577),"This PR brings in ansible changes needed to setup kubernetes with Contiv networking as outlined in https://github.com/kubernetes/contrib/issues/577.

Summary of changes:
1) Added a new role (contiv) that holds all contiv specific config code
2) Minor tweaks to etcd setup so contiv can access etcd from any node in proxy-mode
3) Exposed a few options via variables, with default values set so the default behavior will be unchanged.
4) Added missing ""validate_certs: False"" for a couple of addon file fetches.
",closed,True,2016-03-24 22:14:50,2016-04-21 13:32:32
contrib,rutsky,https://github.com/kubernetes/contrib/issues/626,https://api.github.com/repos/kubernetes/contrib/issues/626,update kube-addons.sh with upstream version,"I see that `kube-addons.sh` in this repo was patched to support custom Python (e.g. for Core OS), latest related fix is #624.

I propose to update `kube-addons.sh` with upstream version that already includes workaround for missing python - it uses Python from docker image when local Python is not available:
https://github.com/kubernetes/kubernetes/blob/b103f0f/cluster/saltbase/salt/kube-addons/kube-addons.sh#L28

This will reduce differences between contrib version and upstream version of `kube-addons.sh` and will simplify contrib Ansible scripts.

/cc @danehans, @eparis, @atosatto
",closed,False,2016-03-24 22:37:27,2016-04-01 14:59:02
contrib,rutsky,https://github.com/kubernetes/contrib/issues/627,https://api.github.com/repos/kubernetes/contrib/issues/627,Strange regexp in vagrant-ansible.yml,"Can someone comment on this part in https://github.com/kubernetes/contrib/blob/79b0123/ansible/vagrant/vagrant-ansible.yml#L15:

```
    - name: ""Remove hostname from localhost line""
      replace:
        dest=/etc/hosts
        regexp=""{{ item }} localhost?""
        replace=""localhost""
```

As far as I understand this regexp will match for example `kube-master localhost` and `kube-master localhos` (note missing last letter due to `?` in regexp) — is this expected behavior?
I think `{{ item }}\s+localhost$` regexp should be used if we want to just match string like `kube-master     localhost`.

This was introduced by @jasonbrooks in 79b0123a. Jason, can you comment on this?
",closed,False,2016-03-24 23:03:39,2018-02-14 02:02:09
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/628,https://api.github.com/repos/kubernetes/contrib/issues/628,mungegithub: `make container` doesn't,"```
docker build -t gcr.io/google_containers/submit-queue:2016-03-24-a5608e6 -f Dockerfile-submit-queue .
Sending build context to Docker daemon 11.07 MB
Step 1 : FROM google/debian:wheezy
 ---> 11971b6377ef
Step 2 : MAINTAINER Brendan Burns <bburns@google.com>
 ---> Using cache
 ---> d568fac21ec6
Step 3 : RUN apt-get update
 ---> Using cache
 ---> 2c3754bee849
Step 4 : RUN apt-get install -y -qq ca-certificates git
 ---> Running in b498d8ae75a9
Failed to fetch http://security.debian.org/pool/updates/main/libg/libgcrypt11/libgcrypt11_1.5.0-5+deb7u3_amd64.deb  404  Not Found [IP: 128.61.240.73 80]
Failed to fetch http://security.debian.org/pool/updates/main/g/gnutls26/libgnutls26_2.12.20-8+deb7u3_amd64.deb  404  Not Found [IP: 128.61.240.73 80]
Failed to fetch http://security.debian.org/pool/updates/main/k/krb5/libkrb5support0_1.10.1+dfsg-5+deb7u3_amd64.deb  404  Not Found [IP: 128.61.240.73 80]
Failed to fetch http://security.debian.org/pool/updates/main/k/krb5/libk5crypto3_1.10.1+dfsg-5+deb7u3_amd64.deb  404  Not Found [IP: 128.61.240.73 80]
Failed to fetch http://security.debian.org/pool/updates/main/k/krb5/libkrb5-3_1.10.1+dfsg-5+deb7u3_amd64.deb  404  Not Found [IP: 128.61.240.73 80]
Failed to fetch http://security.debian.org/pool/updates/main/k/krb5/libgssapi-krb5-2_1.10.1+dfsg-5+deb7u3_amd64.deb  404  Not Found [IP: 128.61.240.73 80]
Failed to fetch http://security.debian.org/pool/updates/main/o/openldap/libldap-2.4-2_2.4.31-2_amd64.deb  404  Not Found [IP: 128.61.240.73 80]
Failed to fetch http://security.debian.org/pool/updates/main/libs/libssh2/libssh2-1_1.4.2-1.1+deb7u1_amd64.deb  404  Not Found [IP: 128.61.240.73 80]
Failed to fetch http://security.debian.org/pool/updates/main/k/krb5/krb5-locales_1.10.1+dfsg-5+deb7u3_all.deb  404  Not Found [IP: 128.61.240.73 80]
Failed to fetch http://security.debian.org/pool/updates/main/o/openssl/openssl_1.0.1e-2+deb7u17_amd64.deb  404  Not Found [IP: 128.61.240.73 80]
E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
The command '/bin/sh -c apt-get install -y -qq ca-certificates git' returned a non-zero code: 100
make: *** [container] Error 1
```

Adding --fix-missing as per the error message doesn't help, either. Ideas? @eparis
",closed,False,2016-03-24 23:25:45,2016-03-25 00:33:41
contrib,danehans,https://github.com/kubernetes/contrib/pull/629,https://api.github.com/repos/kubernetes/contrib/issues/629,Adds Support for Kubernetes Internal Network Management,"Previously, the Ansible project only supported managing a
single interface. It's common for production deployments to
require separating network traffic. This patch adds support
to manage an internal interface (no default gw) and associated
static routes.
1. Supports CoreOS deployments (systemd-networkd).
2. Adds static route support (including multiple routes).

The code can be used as a reference for supporting other
distros.

Note: This PR does not configure any service to specifically
bind to a particular interface.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/629)

<!-- Reviewable:end -->
",closed,True,2016-03-24 23:44:31,2018-02-17 13:23:01
contrib,danehans,https://github.com/kubernetes/contrib/pull/630,https://api.github.com/repos/kubernetes/contrib/issues/630,Adds support for no_proxy,"Previously, the Ansible project supported setting http/https proxy
settings, but not no_proxy. Bypassing the http/https proxy is
common in production deployments. For example, a deployment may
need to bypass the http/https proxy to pull images from a local
Docker registry.
",closed,True,2016-03-25 00:09:12,2016-03-27 18:49:41
contrib,gmarek,https://github.com/kubernetes/contrib/pull/631,https://api.github.com/repos/kubernetes/contrib/issues/631,Add retries to reading data from GCS as it's not reliable enough.,"@spiffxp @wojtek-t 
",closed,True,2016-03-25 13:36:30,2016-03-25 16:29:13
contrib,eparis,https://github.com/kubernetes/contrib/pull/632,https://api.github.com/repos/kubernetes/contrib/issues/632,Include the title and first comment in submit queue merges,"This allows us to find that information in the git logs for later
processing.
",closed,True,2016-03-25 17:12:28,2016-03-29 17:34:15
contrib,eparis,https://github.com/kubernetes/contrib/pull/633,https://api.github.com/repos/kubernetes/contrib/issues/633,Clear cp candidate when picked via hack script,"mungegithub: Remove cp-candidate label if the words ""Automated cherry pick of #NNNNN"" are in the branch

Previously we looked for messages about the MERGE commit that went to master (aka using git cherry-pick). But the scripts in hack use `git am`. Though thankfully, at least for single picks, they have an easy to figure out distinctive title.
",closed,True,2016-03-25 17:22:58,2016-03-31 00:12:09
contrib,krousey,https://github.com/kubernetes/contrib/pull/634,https://api.github.com/repos/kubernetes/contrib/issues/634,Create container for mailing via remote smtp,"Note there is already a Makefile in this directory. This means that both

``` console
> APP=shame-mailer make
> APP=shame-mailer make push
```

work. I have not run `APP=shame-mailer make push` yet.

cc @lavalamp @ixdy @fejta 
",closed,True,2016-03-25 17:47:08,2016-03-25 22:17:42
contrib,mikedanese,https://github.com/kubernetes/contrib/issues/635,https://api.github.com/repos/kubernetes/contrib/issues/635,move ansible to github.com/kubernetes/turn-up,"I'm planing on creating a new repo to hold cluster deployment related automation. The goal is to eventually move `cluster/`into it. ansible/ would be a good seed since it's self contained. Yay or nay?

cc @eparis 
",closed,False,2016-03-25 19:06:38,2018-02-14 02:02:09
contrib,danehans,https://github.com/kubernetes/contrib/issues/636,https://api.github.com/repos/kubernetes/contrib/issues/636,source_type: localBuild fails with Ansible Vagrant Deployer,"```
TASK [master : Copy master binaries] *******************************************
failed: [kube-master] => (item=kube-apiserver) => {""failed"": true, ""item"": ""kube-apiserver"", ""msg"": ""could not find src=/Users/daneyonhansen/Desktop/_output/local/go/bin/kube-apiserver""}
failed: [kube-master] => (item=kube-scheduler) => {""failed"": true, ""item"": ""kube-scheduler"", ""msg"": ""could not find src=/Users/daneyonhansen/Desktop/_output/local/go/bin/kube-scheduler""}
failed: [kube-master] => (item=kube-controller-manager) => {""failed"": true, ""item"": ""kube-controller-manager"", ""msg"": ""could not find src=/Users/daneyonhansen/Desktop/_output/local/go/bin/kube-controller-manager""}
failed: [kube-master] => (item=kubectl) => {""failed"": true, ""item"": ""kubectl"", ""msg"": ""could not find src=/Users/daneyonhansen/Desktop/_output/local/go/bin/kubectl""}
failed: [kube-master] => (item=kubelet) => {""failed"": true, ""item"": ""kubelet"", ""msg"": ""could not find src=/Users/daneyonhansen/Desktop/_output/local/go/bin/kubelet""}

RUNNING HANDLER [master : restart daemons] *************************************
    to retry, use: --limit @../cluster.retry

PLAY RECAP *********************************************************************
kube-master                : ok=142  changed=35   unreachable=0    failed=1   
kube-node-1                : ok=71   changed=16   unreachable=0    failed=0   
kube-node-2                : ok=71   changed=16   unreachable=0    failed=0   

Ansible failed to complete successfully. Any error output should be
visible above. Please fix these errors and try again.
```

change source_type: packageManager in group_vars/all.yml and the Vagrant deployer works
",closed,False,2016-03-25 22:44:13,2018-02-14 02:02:10
contrib,danehans,https://github.com/kubernetes/contrib/pull/637,https://api.github.com/repos/kubernetes/contrib/issues/637,Updates Vagrant Documentation,"Previously, the Vagrant documentation was missing a few minor
steps. This patch adds verbosity to the Vagrant documentation that
should improve developer onboarding.
",closed,True,2016-03-25 23:24:21,2016-03-26 04:30:29
contrib,danehans,https://github.com/kubernetes/contrib/issues/638,https://api.github.com/repos/kubernetes/contrib/issues/638,openstack plugin required when when using Virtualbox Plugin for Ansible Vagrant,"Vagrant spits out an error requiring the openstack plugin even if you don't use the openstack provider.

```
$ vagrant --version
Vagrant 1.7.4
```
",closed,False,2016-03-25 23:27:52,2018-02-14 02:02:08
contrib,gosharplite,https://github.com/kubernetes/contrib/pull/639,https://api.github.com/repos/kubernetes/contrib/issues/639,HTTP -> HTTPS,,closed,True,2016-03-25 23:49:44,2016-03-26 00:08:05
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/640,https://api.github.com/repos/kubernetes/contrib/issues/640,Fix some typos in nginx README.md,"@aledbf 
",closed,True,2016-03-26 01:45:56,2016-03-26 05:07:56
contrib,danehans,https://github.com/kubernetes/contrib/pull/641,https://api.github.com/repos/kubernetes/contrib/issues/641,Fixes Insecure Docker Registry for CoreOS,"Previously, CoreOS would not read the INSECURE_REGISTRY key
in the docker config file. This commit adds --insecure-registry
entries to DOCKER_OPT in the docker config file for CoreOS.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/641)

<!-- Reviewable:end -->
",closed,True,2016-03-26 05:15:55,2018-02-17 23:33:00
contrib,aledbf,https://github.com/kubernetes/contrib/pull/642,https://api.github.com/repos/kubernetes/contrib/issues/642,Improve nginx-ingress-controller documentation.," Add flag to enable vts status module
",closed,True,2016-03-26 21:37:02,2016-03-29 22:17:35
contrib,rutsky,https://github.com/kubernetes/contrib/issues/643,https://api.github.com/repos/kubernetes/contrib/issues/643,Use `source_type: packageManager` by default in Ansible,"I think this is more reasonable default for users that want to use these scripts to deploy Kubernetes.
This will make deployment process one step easier and is good workaround for bugs like #636 for users that just want to get working installation of Kubernetes. 

`localBuild` is a good default for developers of Kubernetes, so if they are target audience of these script, then `localBuild` default should be kept.

/cc @eparis, @danehans, @stephenrlouie
",closed,False,2016-03-27 12:15:38,2016-03-29 13:20:41
contrib,rutsky,https://github.com/kubernetes/contrib/pull/644,https://api.github.com/repos/kubernetes/contrib/issues/644,add CoreOS support to Ansible+Vagrant+VirtualBox + fix initial vagrant-ansible.yml playbook,"This PR
- adds support for selecting OS images to use in Vagrant,
- adds Core OS image support,
- fixes issues with initial run of `vagrant-ansible.yml` (missing `/etc/hosts` on Core OS and configuring ansible_python_interpreter),
- update documentation.

Now you can setup K8S cluster in Vagrant+Ansible+VirtualBox using:

```
$ OS_IMAGE=coreos vagrant up --provider virtualbox
[...]
PLAY RECAP *********************************************************************
kube-master                : ok=257  changed=92   unreachable=0    failed=0   
kube-node-1                : ok=129  changed=54   unreachable=0    failed=0   
kube-node-2                : ok=128  changed=54   unreachable=0    failed=0
$ OS_IMAGE=coreos vagrant ssh kube-master                     
CoreOS stable (899.13.0)
core@kube-master ~ $ kubectl cluster-info                                                                                                         
Kubernetes master is running at http://localhost:8080
core@kube-master ~ $ kubectl get nodes
NAME          LABELS                               STATUS    AGE
kube-node-1   kubernetes.io/hostname=kube-node-1   Ready     1m
kube-node-2   kubernetes.io/hostname=kube-node-2   Ready     1m
core@kube-master ~ $ kubectl get pods --all-namespaces
NAMESPACE     NAME                                READY     STATUS    RESTARTS   AGE
kube-system   fluentd-elasticsearch-kube-node-1   1/1       Running   0          1m
kube-system   fluentd-elasticsearch-kube-node-2   1/1       Running   0          1m
core@kube-master ~ $ sudo systemctl status flanneld
● flanneld.service - flannel is an etcd backed overlay network for containers
   Loaded: loaded (/etc/systemd/system/flanneld.service; enabled; vendor preset: disabled)
   Active: active (running) since Sun 2016-03-27 12:27:43 UTC; 23min ago
 Main PID: 3126 (flanneld)
   Memory: 2.9M
      CPU: 75ms
   CGroup: /system.slice/flanneld.service
           └─3126 /opt/bin/flanneld

Mar 27 12:27:43 kube-master flanneld[3126]: I0327 12:27:43.158745 03126 main.go:130] Determining IP address of default interface
Mar 27 12:27:43 kube-master flanneld[3126]: I0327 12:27:43.158996 03126 main.go:188] Using 10.0.2.15 as external interface
Mar 27 12:27:43 kube-master flanneld[3126]: I0327 12:27:43.159057 03126 main.go:189] Using 10.0.2.15 as external endpoint
Mar 27 12:27:43 kube-master flanneld[3126]: I0327 12:27:43.162512 03126 etcd.go:129] Found lease (172.16.7.0/24) for current IP (10.0.2... reusing
Mar 27 12:27:43 kube-master flanneld[3126]: I0327 12:27:43.164223 03126 etcd.go:84] Subnet lease acquired: 172.16.7.0/24
Mar 27 12:27:43 kube-master flanneld[3126]: I0327 12:27:43.167300 03126 vxlan.go:153] Watching for L3 misses
Mar 27 12:27:43 kube-master flanneld[3126]: I0327 12:27:43.167329 03126 vxlan.go:159] Watching for new subnet leases
Mar 27 12:27:43 kube-master flanneld[3126]: I0327 12:27:43.168354 03126 vxlan.go:273] Handling initial subnet events
Mar 27 12:27:43 kube-master flanneld[3126]: I0327 12:27:43.168409 03126 device.go:159] calling GetL2List() dev.link.Index: 5
Mar 27 12:27:43 kube-master systemd[1]: Started flannel is an etcd backed overlay network for containers.
Hint: Some lines were ellipsized, use -l to show in full.
```

Core OS on other providers (OpenStask and libvirt) are not supported. It's easy to add them --- just specify proper Vagrant image --- but I don't have proper environment to test.

As you can see in the output above cluster is not properly setup --- there is at least one error with Flannel configuration in VirtualBox: wrong interface selected because Flannel settings are not properly passed to flannel.
@danehans in 7604d83e337e85af5c2ff32681067d5c7e75f127 you introduced `FLANNELD_OPTIONS` for Core OS that should mimic `FLANNEL_OPTIONS` for other configurations. However `FLANNEL_OPTIONS` value are passed to `flanneld` process as command line argument, and `FLANNELD_OPTIONS` is exposed to the Flannel as the environment variable which doesn't work for me and shouldn't according to Flannel documentation (each option from `FLANNEL_OPTIONS` like `--iface=eth1` should be converted to something like `FLANNEL_IFACE=eth1`).

I hope to fix this (and maybe other issues) next week in separate pull request.

/cc @eparis, @danehans,  @stephenrlouie 
",closed,True,2016-03-27 12:58:40,2016-03-27 14:12:10
contrib,danehans,https://github.com/kubernetes/contrib/pull/645,https://api.github.com/repos/kubernetes/contrib/issues/645,Changes source_type default to packageManager,"Fixes Issue: https://github.com/kubernetes/contrib/issues/643
",closed,True,2016-03-27 18:00:10,2016-03-27 18:41:44
contrib,luxas,https://github.com/kubernetes/contrib/pull/646,https://api.github.com/repos/kubernetes/contrib/issues/646,Add cross-compiling support for exechealthz,"Compile `exechealthz` for different architectures
Please push the generated images
Tracking issue: kubernetes/kubernetes#17981

@bprashanth @thockin @brendandburns @aledbf @eparis 
",closed,True,2016-03-27 20:32:28,2016-03-31 04:55:21
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/647,https://api.github.com/repos/kubernetes/contrib/issues/647,Clarify firewall rule doc.,,closed,True,2016-03-28 01:26:17,2016-03-28 01:39:22
contrib,rutsky,https://github.com/kubernetes/contrib/pull/648,https://api.github.com/repos/kubernetes/contrib/issues/648,use same /etc/sysconfig/flanneld env file as on other distributions,"In CentOS 7 flanneld.service contains:

```
ExecStart=/usr/bin/flanneld -etcd-endpoints=${FLANNEL_ETCD} -etcd-prefix=${FLANNEL_ETCD_KEY} $FLANNEL_OPTIONS
```

Which allows to use variables like `FLANNEL_ETCD` in
`/etc/sysconfig/flanneld`.

Currently used approach on CoreOS uses flannel-specific environment
variables `FLANNELD_*`. However there is no way to pass arbitrary options
to flanneld daemon using `FLANNELD_*` environment variables (and currently
used `FLANNELD_OPTIONS` doesn't work actually).

Lets start flanneld in the same way on CoreOS to unify using of
environment files.

This fixes settings public ethernet interface in Ansible+Vagrant+VirtualBox+CoreOS configuration, noted in #644.

/cc @eparis @danehans 
",closed,True,2016-03-28 14:02:39,2016-03-28 18:37:41
contrib,danehans,https://github.com/kubernetes/contrib/pull/649,https://api.github.com/repos/kubernetes/contrib/issues/649,Updates Ansible Vagrant Documentation,"93aae647510d29426c0de8d1c6bec875941 updated the default
package_manager. This commit updates the Vagrant doc to support
this new default setting.

8417f092090067dbbf8096815d50edaf77bfea2f introduced a Vagrant doc
bug. This commit updates references to Core OS 7 and coreos7 being
the default Vagrant image to fix this bug.
",closed,True,2016-03-28 18:17:33,2016-03-28 18:37:28
contrib,deromka,https://github.com/kubernetes/contrib/issues/650,https://api.github.com/repos/kubernetes/contrib/issues/650,Broken link in Ansible scripts for getting the kube-ui-rc.yaml file when kube-ui: true in all.yaml,"@danehans 
Broken link: https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/addons/kube-ui/kube-ui-rc.yaml

When kube-ui: true in all.yaml

TASK [kubernetes-addons : KUBE-UI | Assures /etc/kubernetes/addons/kube-ui dir exists] ***
changed: [10.57.50.161] => {""changed"": true, ""gid"": 0, ""group"": ""root"", ""mode"": ""0755"", ""owner"": ""root"", ""path"": ""/etc/kubernetes/addons/kube-ui"", ""secontext"": ""unconfined_u:object_r:etc_t:s0"", ""size"": 4096, ""state"": ""directory"", ""uid"": 0}

TASK [kubernetes-addons : KUBE-UI | Download kube-ui files from Kubernetes repo] ***
failed: [10.57.50.161] => (item=kube-ui-rc.yaml) => {""dest"": ""/etc/kubernetes/addons/kube-ui/"", ""failed"": true, ""gid"": 0, ""group"": ""root"", ""item"": ""kube-ui-rc.yaml"", ""mode"": ""0755"", ""msg"": ""Request failed"", ""owner"": ""root"", ""response"": ""HTTP Error 404: Not Found"", ""secontext"": ""unconfined_u:object_r:etc_t:s0"", ""size"": 4096, ""state"": ""directory"", ""status_code"": 404, ""uid"": 0, ""url"": ""https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/addons/kube-ui/kube-ui-rc.yaml""}
failed: [10.57.50.161] => (item=kube-ui-svc.yaml) => {""dest"": ""/etc/kubernetes/addons/kube-ui/"", ""failed"": true, ""gid"": 0, ""group"": ""root"", ""item"": ""kube-ui-svc.yaml"", ""mode"": ""0755"", ""msg"": ""Request failed"", ""owner"": ""root"", ""response"": ""HTTP Error 404: Not Found"", ""secontext"": ""unconfined_u:object_r:etc_t:s0"", ""size"": 4096, ""state"": ""directory"", ""status_code"": 404, ""uid"": 0, ""url"": ""https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/addons/kube-ui/kube-ui-svc.yaml""}
    to retry, use: --limit @cluster.retry

PLAY RECAP *********************************************************************
",closed,False,2016-03-28 18:30:15,2018-02-14 01:01:12
contrib,deromka,https://github.com/kubernetes/contrib/issues/651,https://api.github.com/repos/kubernetes/contrib/issues/651,"Accessing Kubernetes UI via browser, Getting Unauthorized","[Stackoverflow Issue](http://stackoverflow.com/questions/36270602/how-to-access-kubernetes-ui-via-browser)
",closed,False,2016-03-28 20:31:58,2017-03-14 11:36:24
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/652,https://api.github.com/repos/kubernetes/contrib/issues/652,Resilience fixes for the Ingress controller,"Reasoning for each commit:
1. UrlMap updating logic was too complicated, because it accounted for ""joining"" which we don't care about right now.
2. Errors at the beginning of the sync() function would prevent cloud urlmaps from getting cleaned up
3. The controller would silently forget about resources if it was restarted, and while down someone modified an Ingress
4. Retry creating GCE client since it can fail if we don't resolve metadata server
",closed,True,2016-03-28 21:46:31,2016-04-04 22:17:17
contrib,adamschaub,https://github.com/kubernetes/contrib/pull/653,https://api.github.com/repos/kubernetes/contrib/issues/653,Updated adhoc/uninstall.yml to support CoreOS,"Adds support for CoreOS distributions to the uninstall playbook.
",closed,True,2016-03-28 22:08:22,2016-03-28 23:22:26
contrib,adamschaub,https://github.com/kubernetes/contrib/pull/654,https://api.github.com/repos/kubernetes/contrib/issues/654,uninstall CoreOS support: Touch up uninstall CoreOS wording. Added a missing check for is_coreos,"Follow up to [1]

I didn't have any issues with restarting etcd2 [2]. Tested on CentOS 7. Left it as is.

[1] https://github.com/kubernetes/contrib/pull/653
[2] https://github.com/adamschaub/contrib/blob/929f316e7698cc74c57e04402ef24d4ae80b9c41/ansible/playbooks/adhoc/uninstall.yml#L61
",closed,True,2016-03-29 03:07:24,2016-04-06 00:18:03
contrib,danehans,https://github.com/kubernetes/contrib/pull/655,https://api.github.com/repos/kubernetes/contrib/issues/655,Updates Vagrant README,"1. Adds Ansible 1.9.4 for pip install
2. Updated NUM_MINIONS to NUM_NODES

The default version of Ansible for Mac OSX is 2.x. This version
does not work with CoreOS. Vagrantfile was updated in a
previous commit to go from NUM_MINIONS > NUM_NODES but the README
was not updated.
",closed,True,2016-03-29 03:38:24,2016-04-06 15:46:49
contrib,danehans,https://github.com/kubernetes/contrib/pull/656,https://api.github.com/repos/kubernetes/contrib/issues/656,Fixes Ansible Vagrant for CoreOS,"Previously, Vagrant would not properly set the box download
url:

kube-node-1: Downloading: https://storage.googleapis.com/\
.release.core-os.net/amd64-usr//coreos_production_vagrant.json

An error occurred while downloading the remote file. The error
message, if any, is reproduced below. Please fix this error and
try again.

The requested URL returned error: 400 Bad Request

This commit properly sets the coreos update_channel
and image_version variables.
",closed,True,2016-03-29 03:40:41,2016-03-29 13:50:32
contrib,xidui,https://github.com/kubernetes/contrib/issues/657,https://api.github.com/repos/kubernetes/contrib/issues/657,`No such host` error when running ansible scripts,"This is the error code.

```
fatal: [node-1-2 -> None]: FAILED! => {""changed"": true, ""cmd"": ""/usr/bin/etcdctl --no-sync --peers=http://node-1-2:2379 set /cluster.local/network/config < /tmp/flannel-conf.json"", ""delta"": ""0:00:00.014388"", ""end"": ""2016-03-29 16:04:55.553188"", ""failed"": true, ""rc"": 4, ""start"": ""2016-03-29 16:04:55.538800"", ""stderr"": ""Error:  client: etcd cluster is unavailable or misconfigured\nerror #0: dial tcp: lookup node-1-2: no such host"", ""stdout"": """", ""stdout_lines"": [], ""warnings"": []}
```

`node-1-2` is a node name which I created in the inventory.
But in the cmd, the OS can not detect what it is.

Ansible won't add it to the hosts automatically?
",closed,False,2016-03-29 08:19:10,2018-02-14 18:18:10
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/658,https://api.github.com/repos/kubernetes/contrib/issues/658,Set implicit value for did install fact for node role,"Init the did_install fact in packageManagerInstall.yml for node role.
Otherwise it is undefined for other distros than CoreOS or Fedora
",closed,True,2016-03-29 13:19:24,2016-03-29 18:03:35
contrib,rutsky,https://github.com/kubernetes/contrib/issues/659,https://api.github.com/repos/kubernetes/contrib/issues/659,"[Ansible] Core OS' Python generates warning ""pypy: /lib64/libssl.so.1.0.0: no version information available (required by /opt/bin/pypy/bin/pypy)""","Running Python installed on Core OS using [`pre-ansible` play](https://github.com/kubernetes/contrib/blob/master/ansible/roles/pre-ansible/tasks/coreos.yml) generates warnings:

```
# python 
/opt/bin/pypy/bin/pypy: /lib64/libssl.so.1.0.0: no version information available (required by /opt/bin/pypy/bin/pypy)
/opt/bin/pypy/bin/pypy: /lib64/libcrypto.so.1.0.0: no version information available (required by /opt/bin/pypy/bin/pypy)
Python 2.7.8 (f5dcc2477b97, Sep 18 2014, 11:33:30)
[PyPy 2.4.0 with GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>>> 
```

I see same warnings in kube-addons service log.

@adamschaub do you see same warnings with bootstrapped by your PR #621 Core OS?

/cc: @danehans 
",closed,False,2016-03-29 14:05:34,2018-02-23 20:51:54
contrib,danehans,https://github.com/kubernetes/contrib/issues/660,https://api.github.com/repos/kubernetes/contrib/issues/660,Update CICD to Include Ansible Deployment Tests,"Recently, several commits have been merged that cause breakage. This is understandable because the Ansible project supports different distros and developers typically test a patch against one distro. The existing contrib CICD system should be updated to run ansible deployment tests.
",closed,False,2016-03-29 15:23:13,2018-02-14 18:18:09
contrib,rutsky,https://github.com/kubernetes/contrib/pull/661,https://api.github.com/repos/kubernetes/contrib/issues/661,download Flannel releases to /opt/,"/usr/ is readonly on Core OS, and /opt/ should work on Ubuntu too.

Intallation failure on Core OS was introduces in #588 

/cc @unicell @danehans @stephenrlouie
",closed,True,2016-03-29 15:35:11,2016-03-29 16:32:15
contrib,rutsky,https://github.com/kubernetes/contrib/pull/662,https://api.github.com/repos/kubernetes/contrib/issues/662,"fix ""--v"" flag description","It's ""verbosity"" flag, the higher it is, the more verbose messages will
be printed.

It should mimic glog's ""--v"":
https://google-glog.googlecode.com/svn/trunk/doc/glog.html#verbose
",closed,True,2016-03-29 15:35:45,2016-03-29 16:32:38
contrib,rutsky,https://github.com/kubernetes/contrib/pull/663,https://api.github.com/repos/kubernetes/contrib/issues/663,Bump K8s version to 1.2.0 on Core OS + download k8s binaries one by one,"Kubernetes GitHub release tarball has size of about 400+ MB.
Previously it's being downloaded, extracted, and inner tarball extracted
one more time (on each host).
This took roughly 400+ MB \* NUM_NODES traffic and about 1 GB of space in
/tmp/ (on each host).

Starting from 1.2.0 it takes more than 1 GB of space in /tmp by Ansible,
which fails for example on Core OS, where /tmp is 1 GB ramdisk.

With these changes only required binaries are downloaded (not
compressed, but it's still significantly less than whole release tarball size).

/cc: @danehans @stephenrlouie @eparis 
",closed,True,2016-03-29 15:37:06,2016-03-29 20:20:28
contrib,rutsky,https://github.com/kubernetes/contrib/issues/664,https://api.github.com/repos/kubernetes/contrib/issues/664,"ansible tag ""binary-update"" doesn't apply to Kubernetes on Core OS","@eparis @danehans any idea why binaries are downloaded on CoreOS in `kubernetes` role, not in `masters`/`nodes` roles as done in other distributions?

I asked this question in #663, but I think this should be discussed as separate issue.
",closed,False,2016-03-29 18:30:03,2016-08-23 10:57:05
contrib,roberthbailey,https://github.com/kubernetes/contrib/pull/665,https://api.github.com/repos/kubernetes/contrib/issues/665,Add Amey-D and wonderfly to the whitelist.,"/cc @Amey-D @wonderfly
",closed,True,2016-03-29 19:12:55,2016-03-29 20:51:52
contrib,rutsky,https://github.com/kubernetes/contrib/pull/666,https://api.github.com/repos/kubernetes/contrib/issues/666,add requirement for minimal Vagrant version to use for Core OS images,"Vagrant prior version 1.8.0 doesn't write group variables into inventory file:
https://github.com/mitchellh/vagrant/commit/dd4ae1a51cfb246d561bced89d3b34ee90a0a38f

@stephenrlouie this is a check for issue that you observed with wrong `ansible_python_path`.
",closed,True,2016-03-29 19:54:53,2016-04-02 22:43:18
contrib,rutsky,https://github.com/kubernetes/contrib/issues/667,https://api.github.com/repos/kubernetes/contrib/issues/667,[ansible] addons fails to install on CoreOS due to missing python dependencies,"Currently bootstrapped in `pre-ansible` python doesn't include some modules which are required by [kube-addon-update.sh script](https://github.com/kubernetes/kubernetes/blob/master/cluster/saltbase/salt/kube-addons/kube-addon-update.sh#L103):

```
try:
        import pipes,sys,yaml
        y = yaml.load(sys.stdin)
        labels = y[""metadata""][""labels""]
        if (""kubernetes.io/cluster-service"", ""true"") not in labels.iteritems():
            # all add-ons must have the label ""kubernetes.io/cluster-service"".
            # Otherwise we are ignoring them (the update will not work anyway)
            print ""ERROR""
        else:
            print y[""kind""]
except Exception, ex:
        print ""ERROR""
```

/cc: @danehans 
",closed,False,2016-03-29 20:50:31,2016-04-01 15:00:35
contrib,eparis,https://github.com/kubernetes/contrib/pull/668,https://api.github.com/repos/kubernetes/contrib/issues/668,Use the issue Body not the first commit in merges,,closed,True,2016-03-29 20:52:08,2016-03-29 20:52:12
contrib,adamschaub,https://github.com/kubernetes/contrib/pull/669,https://api.github.com/repos/kubernetes/contrib/issues/669,CoreOS pre-ansible: un-hardcode bin_dir,"Updates CoreOS pre-ansible tasks so that bin_dir isn't hardcoded in coreos.yml. 

@eparis @danehans 
",closed,True,2016-03-29 21:18:11,2016-03-30 04:02:09
contrib,eparis,https://github.com/kubernetes/contrib/pull/670,https://api.github.com/repos/kubernetes/contrib/issues/670,Enable the release note munger,,closed,True,2016-03-29 21:48:03,2016-03-31 23:34:26
contrib,rutsky,https://github.com/kubernetes/contrib/issues/671,https://api.github.com/repos/kubernetes/contrib/issues/671,"add `pre-ansibe` role to `common` role deps, add `common` role to other roles deps","Initial discussion: https://github.com/kubernetes/contrib/pull/669#issuecomment-203120047

Currently `bin_dir` for Core OS is defined [`common` role](https://github.com/kubernetes/contrib/blob/6aaa2f148a5b1348422e9c6653b7c984273e93f1/ansible/roles/common/tasks/main.yml#L45), [`kubernetes` role](https://github.com/kubernetes/contrib/blob/0ba7c0a9076fd571f02214b4cde7ae92b65459d2/ansible/roles/kubernetes/tasks/main.yml#L16) and when #669 will be merged it will be defined in [`pre-ansible`](https://github.com/adamschaub/contrib/blob/coreos_python_bootstrap/ansible/roles/pre-ansible/tasks/main.yml#L23) too.
This multiple definition can be eliminated if `common` will depend on `pre-ansible`, and other roles will depend on `common` role (which seems logical to me).

Significant constraint: all roles must be run only once.

/cc: @eparis, @adamreese 
",closed,False,2016-03-29 22:06:29,2018-02-14 18:18:08
contrib,rutsky,https://github.com/kubernetes/contrib/pull/672,https://api.github.com/repos/kubernetes/contrib/issues/672,update kube-addons scripts with upstream version,"This fixes issues #626 and #667.

Now `kube-addon-update.sh` and `kube-addons.sh` downloaded from master
branch of Kubernetes project.

Also adds restart notification for kube-addons when scripts are updated.

Upstream version of these scripts uses custom Docker image with Python
when system Python is not available.
This image is built if Ansible runs on Core OS.

Tested on Vagrant+VirtualBox+Ansible+Core OS.

/cc: @danehans, @eparis
",closed,True,2016-03-29 22:40:43,2016-04-01 16:40:48
contrib,rutsky,https://github.com/kubernetes/contrib/issues/673,https://api.github.com/repos/kubernetes/contrib/issues/673,[ansible] add `source_type: docker` (run Kubernetes parts in containers),"Most of Kubernetes parts that are being deployed can be run in Docker containers.
In Core OS [installation guide](https://coreos.com/kubernetes/docs/latest/deploy-master.html) _all_ parts of Kubernetes cluster is deployed in Docker containers (with `kubelet` in `rkt` container due to issues with mount namespace propagation AFAIK).

Running Kubernetes in Docker should be almost identical on all platforms (since all platforms has Docker), so this may be good default way of running Kubernetes.

Also this corresponds to [cluster deployment proposal](https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/cluster-deployment.md): as I see all components will be run either as Docker container, systemd-nspawn container, or as DaemonSet.

I believe work on running Kubernetes components in containers is already started by core Kubernetes developers (for example https://github.com/kubernetes/kubernetes/issues/23233) and it would be nice to coordinate our efforts so that we wouldn't do same job several times.

/cc: @eparis, @danehans, @mikedanese 
",closed,False,2016-03-30 00:15:09,2018-02-14 01:01:11
contrib,aledbf,https://github.com/kubernetes/contrib/pull/674,https://api.github.com/repos/kubernetes/contrib/issues/674,Update nginx to 1.9.13,,closed,True,2016-03-30 02:23:48,2016-03-31 19:05:49
contrib,aledbf,https://github.com/kubernetes/contrib/pull/675,https://api.github.com/repos/kubernetes/contrib/issues/675,Add UDP load balancing to nginx ingress controller,,closed,True,2016-03-30 02:25:48,2016-03-30 20:34:44
contrib,deromka,https://github.com/kubernetes/contrib/pull/676,https://api.github.com/repos/kubernetes/contrib/issues/676,Updated kube-ui to use new kubernetes dashboard ui,"Updated kube-ui to use new kubernetes dashboard ui from [here](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboard)
",closed,True,2016-03-30 12:53:04,2016-03-30 19:21:36
contrib,rutsky,https://github.com/kubernetes/contrib/issues/677,https://api.github.com/repos/kubernetes/contrib/issues/677,[ansible] deploy etcd with authentication by default,"Currently deployed etcd available to all machines that have network access to the master node, which is undesirable for example when deploying Kubernetes in local network, when cluster is isolated from the Internet, but not isolated from machines in corporate network.

etcd role has configuration options to use certificate authentication (e.g. `etcd_url_scheme: https`), but doesn't include tasks to automatically generate certificates and distribute them along nodes.
",closed,False,2016-03-30 13:56:16,2018-02-14 00:00:13
contrib,edevil,https://github.com/kubernetes/contrib/issues/678,https://api.github.com/repos/kubernetes/contrib/issues/678,service loadbalancer - document needed service annotations,"I have created the loadbalancer replication controller, added the label to the nodes, and the balancer pods have started:

```
$ kubectl get pods
NAME                         READY     STATUS    RESTARTS   AGE
guestbook-8jycj              1/1       Running   0          19h
guestbook-o05ii              1/1       Running   0          19h
guestbook-xocz7              1/1       Running   1          19h
redis-master-s2y7n           1/1       Running   1          21h
redis-slave-a9f38            1/1       Running   1          20h
redis-slave-c81b0            1/1       Running   1          20h
service-loadbalancer-x1pkg   1/1       Running   0          53m
simpleweb-5vxiq              1/1       Running   0          19h
simpleweb-cpx32              1/1       Running   1          19h
```

However now I don't know to expose my existing services on the load-balancer. It's possible that I need to add some annotations to the services but this is not clear from the documentation. Output from balancer:

```
$ curl -v http://guestbook-brpx.westeurope.cloudapp.azure.com/
*   Trying 13.94.195.197...
* Connected to guestbook-brpx.westeurope.cloudapp.azure.com (13.94.195.197) port 80 (#0)
> GET / HTTP/1.1
> Host: guestbook-brpx.westeurope.cloudapp.azure.com
> User-Agent: curl/7.43.0
> Accept: */*
>
< HTTP/1.1 404 Not Found
< Date: Wed, 30 Mar 2016 11:21:54 GMT
< Content-Length: 9
< Content-Type: text/plain; charset=utf-8
<
* Connection #0 to host guestbook-brpx.westeurope.cloudapp.azure.com left intact
```
",closed,False,2016-03-30 14:39:38,2018-02-13 08:45:08
contrib,danehans,https://github.com/kubernetes/contrib/issues/679,https://api.github.com/repos/kubernetes/contrib/issues/679,[ansible] Provide External Access to Cluster Addon Services,"Currently, the ansible project supports cluster addons [1] by default. Many of the addon services should be exposed external to the cluster. The addon manifests do not provide a common way for externally exposing services [2]. The ansible project should provide this mechanism. Here are a few options to achieve this goal:
1. Use the contrib/service-loadbalancer project [3] in the kube-system namespace to manage each addon service that should be externally exposed.
2. Use the nginx ingress controller project [4] in the kube-system namespace to manage each addon service that should be externally exposed.

In either case, a new ansible role should be created to manage this functionality.

[1] https://github.com/kubernetes/kubernetes/tree/master/cluster/addons
[2] https://github.com/kubernetes/kubernetes/issues/23620
[3] https://github.com/kubernetes/contrib/tree/master/service-loadbalancer
[4] https://github.com/kubernetes/contrib/tree/master/ingress/controllers

/cc: @rutsky @adamschaub @eparis 
",closed,False,2016-03-30 16:38:23,2018-02-14 00:00:14
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/680,https://api.github.com/repos/kubernetes/contrib/issues/680,Allocate and remember cluster uid ,"Remembers existing cluster uids in a config map so even if it changes we don't delete/re-create working loadbalancers. This means once a cluster is up and running with a uid, it doesn't change no matter how many time the controller is restarted with a different uid, unless someone deletes the uid config map in kube-system. 

This is needed so we can rollout a change that auto-assigns uids to controllers, which in turn is required before we start running the controller on the master.
",closed,True,2016-03-31 01:22:25,2016-05-27 23:43:54
contrib,aledbf,https://github.com/kubernetes/contrib/pull/681,https://api.github.com/repos/kubernetes/contrib/issues/681,Update Ingress status information in nginx controller,"```
old-mbp:~ aledbf$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
echomap   -                       172.17.4.99
          foo.bar.com
          /foo          echoheaders-x:80
          bar.baz.com
          /bar          echoheaders-y:80
          /foo          echoheaders-x:80
old-mbp:~ aledbf$ kubectl describe  ing
Name:     echomap
Namespace:    default
Address:    172.17.4.99
Default backend:  default-http-backend:80 (<none>)
Rules:
  Host    Path  Backends
  ----    ----  --------
  foo.bar.com
        /foo  echoheaders-x:80 (10.2.68.4:8080)
  bar.baz.com
        /bar  echoheaders-y:80 (10.2.68.4:8080)
        /foo  echoheaders-x:80 (10.2.68.4:8080)
Annotations:
Events:
  FirstSeen LastSeen  Count From        SubobjectPath Type    Reason  Message
  --------- --------  ----- ----        ------------- --------  ------  -------
  38s   38s   1 {loadbalancer-controller }      Normal    CREATE  default/echomap
  38s   38s   1 {loadbalancer-controller }      Normal    CREATE  ip: 172.17.4.99
  38s   38s   1 {loadbalancer-controller }      Normal    UPDATE  default/echomap

```

```
old-mbp:~ aledbf$ kubectl get events -w
FIRSTSEEN                       LASTSEEN                        COUNT     NAME                             KIND                    SUBOBJECT                           TYPE      REASON             SOURCE                      MESSAGE
2016-03-31 00:03:55 -0300 CLT   2016-03-31 00:03:55 -0300 CLT   1         nginx-ingress-controller   ReplicationController             Normal    SuccessfulCreate   {replication-controller }   Created pod: nginx-ingress-controller-wdrix
2016-03-31 00:03:55 -0300 CLT   2016-03-31 00:03:55 -0300 CLT   1         nginx-ingress-controller-wdrix   Pod                 Normal    Scheduled   {default-scheduler }   Successfully assigned nginx-ingress-controller-wdrix to 172.17.4.99
2016-03-31 00:03:55 -0300 CLT   2016-03-31 00:03:55 -0300 CLT   1         nginx-ingress-controller-wdrix   Pod       spec.containers{nginx-ingress-lb}   Normal    Pulling   {kubelet 172.17.4.99}   pulling image ""aledbf/nginx-third-party:0.9""
2016-03-31 00:04:08 -0300 CLT   2016-03-31 00:04:08 -0300 CLT   1         nginx-ingress-controller-wdrix   Pod       spec.containers{nginx-ingress-lb}   Normal    Pulled    {kubelet 172.17.4.99}   Successfully pulled image ""aledbf/nginx-third-party:0.9""
2016-03-31 00:04:08 -0300 CLT   2016-03-31 00:04:08 -0300 CLT   1         nginx-ingress-controller-wdrix   Pod       spec.containers{nginx-ingress-lb}   Normal    Created   {kubelet 172.17.4.99}   Created container with docker id 68c6a6f8ec6d
2016-03-31 00:04:08 -0300 CLT   2016-03-31 00:04:08 -0300 CLT   1         nginx-ingress-controller-wdrix   Pod       spec.containers{nginx-ingress-lb}   Normal    Started   {kubelet 172.17.4.99}   Started container with docker id 68c6a6f8ec6d
2016-03-31 00:04:08 -0300 CLT   2016-03-31 00:04:08 -0300 CLT   1         echomap   Ingress             Normal    CREATE    {loadbalancer-controller }   default/echomap

2016-03-31 00:04:27 -0300 CLT   2016-03-31 00:04:27 -0300 CLT   1         nginx-ingress-controller   ReplicationController             Normal    SuccessfulDelete   {replication-controller }   Deleted pod: nginx-ingress-controller-wdrix
2016-03-31 00:04:27 -0300 CLT   2016-03-31 00:04:27 -0300 CLT   1         echomap   Ingress             Normal    DELETE    {loadbalancer-controller }   ip: 172.17.4.99
2016-03-31 00:04:57 -0300 CLT   2016-03-31 00:04:57 -0300 CLT   1         nginx-ingress-controller-wdrix   Pod       spec.containers{nginx-ingress-lb}   Normal    Killing   {kubelet 172.17.4.99}   Killing container with docker id 68c6a6f8ec6d: Need to kill pod.
```
",closed,True,2016-03-31 03:08:29,2016-04-01 19:00:43
contrib,Tedezed,https://github.com/kubernetes/contrib/issues/682,https://api.github.com/repos/kubernetes/contrib/issues/682,Error in [etcd : Write etcd config file] to 'ansible_default_ipv4.address' (HA),"**Error:**

```
TASK [etcd : Write etcd config file] *******************************************
fatal: [192.168.30.102]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""AnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_default_ipv4.address'""}
fatal: [192.168.30.101]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""AnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_default_ipv4.address'""}
fatal: [192.168.30.103]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""AnsibleUndefinedVariable: 'dict object' has no attribute 'ansible_default_ipv4.address'""}
```

**My solution:**
**1**`contrib/ansible/roles/etcd/templates/etcd.conf.j2`

```
{% macro initial_cluster() -%}
{% for host in groups[etcd_peers_group] -%}
{% if loop.last -%}
{{ hostvars[host]['ansible_hostname'] }}={{ etcd_peer_url_scheme }}://{{ hostvars[host]['ansible_default_ipv4.address'] }}:{{ etcd_peer_port }}
{%- else -%}
{{ hostvars[host]['ansible_hostname'] }}={{ etcd_peer_url_scheme }}://{{ hostvars[host]['ansible_default_ipv4.address'] }}:{{ etcd_peer_port }},
{%- endif -%}
{% endfor -%}
{% endmacro -%}
```

To

```
{% macro initial_cluster() -%}
{% for host in groups[etcd_peers_group] -%}
{% if loop.last -%}
{{ hostvars[host]['ansible_hostname'] }}={{ etcd_peer_url_scheme }}://{{ hostvars[host]['ansible_default_ipv4']['address'] }}:{{ etcd_peer_port }}
{%- else -%}
{{ hostvars[host]['ansible_hostname'] }}={{ etcd_peer_url_scheme }}://{{ hostvars[host]['ansible_default_ipv4']['address'] }}:{{ etcd_peer_port }},
{%- endif -%}
{% endfor -%}
{% endmacro -%}
```

**2**`contrib/ansible/roles/etcd/defaults/main.yaml`

```
etcd_initial_advertise_peer_urls: ""{{ etcd_peer_url_scheme }}://{{ ansible_default_ipv4.address }}:{{ etcd_peer_port }}""
etcd_listen_peer_urls: ""{{ etcd_peer_url_scheme }}://0.0.0.0:{{ etcd_peer_port }}""
etcd_advertise_client_urls: ""{{ etcd_url_scheme }}://{{ ansible_default_ipv4.address }}:{{ etcd_client_port }}""
etcd_listen_client_urls: ""{{ etcd_url_scheme }}://0.0.0.0:{{ etcd_client_port }}""
```

To

```
etcd_initial_advertise_peer_urls: ""{{ etcd_peer_url_scheme }}://{{ ansible_default_ipv4['address'] }}:{{ etcd_peer_port }}""
etcd_listen_peer_urls: ""{{ etcd_peer_url_scheme }}://0.0.0.0:{{ etcd_peer_port }}""
etcd_advertise_client_urls: ""{{ etcd_url_scheme }}://{{ ansible_default_ipv4['address'] }}:{{ etcd_client_port }}""
etcd_listen_client_urls: ""{{ etcd_url_scheme }}://0.0.0.0:{{ etcd_client_port }}""
```
",closed,False,2016-03-31 08:23:54,2016-04-07 08:12:41
contrib,eparis,https://github.com/kubernetes/contrib/pull/683,https://api.github.com/repos/kubernetes/contrib/issues/683,cherrypick: Do not log every time a PR doesn't mater,"This was a debug line which was left in.
",closed,True,2016-03-31 14:41:16,2016-03-31 14:41:24
contrib,eparis,https://github.com/kubernetes/contrib/pull/684,https://api.github.com/repos/kubernetes/contrib/issues/684,cherrypick: Do not use --fork-base in PR matching,"wheezy doesn't even have the option to `git merge-base`. And it doesn't work in jessie.
",closed,True,2016-03-31 15:30:03,2016-03-31 15:30:17
contrib,junelee05,https://github.com/kubernetes/contrib/issues/685,https://api.github.com/repos/kubernetes/contrib/issues/685,Kubernetes 1.1.8 cluster setup with Flannel 0.5.5: cannot ping pod-ips from different nodes ,"We currently have setup Kubernetes 1.1.8 cluster with two VMs, using Flannel 0.5.5 with vxlan backend:
    vm-1 (public-ip: 10.85.81.165):  master + worker-node
    vm-1 (public-ip: 10.85.81.165): worker-node

The VMs have ubuntu trusty installed.
# uname -a

Linux ngsn02 3.13.0-24-generic #46-Ubuntu SMP Thu Apr 10 19:11:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux

We followed this Kubernetes ubuntu setup guide:
http://kubernetes.io/docs/getting-started-guides/ubuntu/

The Flannel config at the etcd is:
# etcdctl get /coreos.com/network/config

{""Network"":""20.1.0.0/16"", ""SubnetLen"": 24, ""Backend"": {""Type"": ""vxlan"", ""VNI"": 1}}
# etcdctl ls /coreos.com/network/subnets

/coreos.com/network/subnets/20.1.36.0-24
/coreos.com/network/subnets/20.1.1.0-24
# etcdctl get /coreos.com/network/subnets/20.1.36.0-24

{""PublicIP"":""10.85.81.165"",""BackendType"":""vxlan"",""BackendData"":{""VtepMAC"":""de:71:6e:55:36:75""}}
# etcdctl get /coreos.com/network/subnets/20.1.1.0-24

{""PublicIP"":""10.85.81.166"",""BackendType"":""vxlan"",""BackendData"":{""VtepMAC"":""06:c8:be:55:f2:e6""}}

From master node vm-1:
# ip route

default via 10.85.81.129 dev eth0
10.85.81.128/25 dev eth0  proto kernel  scope link  src 10.85.81.165
20.1.0.0/16 dev flannel.1  proto kernel  scope link  src 20.1.36.0
20.1.36.0/24 dev docker0  proto kernel  scope link  src 20.1.36.1

From node vm-2:
~# ip route
default via 10.85.81.129 dev eth0
10.85.81.128/25 dev eth0  proto kernel  scope link  src 10.85.81.166
20.1.0.0/16 dev flannel.1  proto kernel  scope link  src 20.1.1.0
20.1.1.0/24 dev docker0  proto kernel  scope link  src 20.1.1.1

After the setup, we deployed two pods:
The pod-ip at node vm-1:  20.1.36.2:80
The pod-ip at node vm-2:  20.1.1.2:80

The problem is that we cannot ping the pod-id from the different node, while can ping the pod-id from the same node.

vm-1~# ping 20.1.1.2
PING 20.1.1.2 (20.1.1.2) 56(84) bytes of data.
^C
--- 20.1.1.2 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 1999ms

By reviewing the flannel.log, we found the ""L3 miss"" and ""AddL3 succeeded"" logs, but not any ""L2 miss"" and ""AddL2"" logs. 
## flannel.log from vm-1:

I0331 09:56:02.102326 01852 main.go:275] Installing signal handlers
I0331 09:56:02.106208 01852 main.go:188] Using 10.85.81.165 as external interface
I0331 09:56:02.106289 01852 main.go:189] Using 10.85.81.165 as external endpoint
E0331 09:56:02.278504 01852 network.go:53] Failed to retrieve network config: 100: Key not found (/coreos.com) [1]
E0331 09:56:03.279482 01852 network.go:53] Failed to retrieve network config: 100: Key not found (/coreos.com) [10]
E0331 09:56:04.280478 01852 network.go:53] Failed to retrieve network config: 100: Key not found (/coreos.com) [13]
E0331 09:56:05.281463 01852 network.go:53] Failed to retrieve network config: 100: Key not found (/coreos.com) [13]
I0331 09:56:06.633421 01852 etcd.go:204] Picking subnet in range 20.1.1.0 ... 20.1.255.0
I0331 09:56:06.634529 01852 etcd.go:84] Subnet lease acquired: 20.1.36.0/24
I0331 09:56:06.637852 01852 ipmasq.go:50] Adding iptables rule: FLANNEL -d 20.1.0.0/16 -j ACCEPT
I0331 09:56:06.644230 01852 ipmasq.go:50] Adding iptables rule: FLANNEL ! -d 224.0.0.0/4 -j MASQUERADE
I0331 09:56:06.650452 01852 ipmasq.go:50] Adding iptables rule: POSTROUTING -s 20.1.0.0/16 -j FLANNEL
I0331 09:56:06.658806 01852 ipmasq.go:50] Adding iptables rule: POSTROUTING ! -s 20.1.0.0/16 -d 20.1.0.0/16 -j MASQUERADE
I0331 09:56:06.665567 01852 vxlan.go:153] Watching for L3 misses
I0331 09:56:06.665658 01852 vxlan.go:159] Watching for new subnet leases
I0331 09:56:06.666996 01852 vxlan.go:273] Handling initial subnet events
I0331 09:56:06.667053 01852 device.go:159] calling GetL2List() dev.link.Index: 4
I0331 09:56:28.184209 01852 vxlan.go:232] Subnet added: 20.1.1.0/24
I0331 09:56:28.184401 01852 device.go:164] calling NeighAdd: 10.85.81.166, 06:c8:be:55:f2:e6
I0331 10:04:40.677618 01852 vxlan.go:345] L3 miss: 20.1.1.2
I0331 10:04:40.677708 01852 device.go:187] calling NeighSet: 20.1.1.2, 06:c8:be:55:f2:e6
I0331 10:04:40.677976 01852 vxlan.go:356] AddL3 succeeded
I0331 10:05:00.388439 01852 vxlan.go:340] Ignoring not a miss: 06:c8:be:55:f2:e6, 20.1.1.2
I0331 10:05:08.902727 01852 vxlan.go:345] L3 miss: 20.1.1.2
I0331 10:05:08.902816 01852 device.go:187] calling NeighSet: 20.1.1.2, 06:c8:be:55:f2:e6
I0331 10:05:08.903024 01852 vxlan.go:356] AddL3 succeeded
I0331 10:05:28.612475 01852 vxlan.go:340] Ignoring not a miss: 06:c8:be:55:f2:e6, 20.1.1.2
I0331 10:08:07.298189 01852 vxlan.go:345] L3 miss: 20.1.1.2
I0331 10:08:07.298292 01852 device.go:187] calling NeighSet: 20.1.1.2, 06:c8:be:55:f2:e6
I0331 10:08:07.298666 01852 vxlan.go:356] AddL3 succeeded
I0331 10:08:29.924520 01852 vxlan.go:340] Ignoring not a miss: 06:c8:be:55:f2:e6, 20.1.1.2
## .............

Also some more info from node vm-1 (node vm-2 info is similar):
# ip -d link show flannel.1

4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether de:71:6e:55:36:75 brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 1 local 10.85.81.165 dev eth0 port 32768 61000 nolearning ageing 300
# arp -n

Address                  HWtype  HWaddress           Flags Mask            Iface
10.85.81.129             ether   3c:8a:b0:8a:a3:31   C                     eth0
20.1.36.2                ether   02:42:14:01:24:02   C                     docker0
20.1.1.2                         (incomplete)                              flannel.1
10.85.81.166             ether   00:50:56:a9:58:7a   C                     eth0
# bridge fdb show dev flannel.1

06:c8:be:55:f2:e6 dst 10.85.81.166 self permanent
# bridge fdb show dev docker0

02:42:cf:32:13:6f vlan 0 permanent
# bridge fdb show dev eth0

01:00:5e:00:00:01 self permanent
33:33:00:00:00:01 self permanent
33:33:ff:a9:11:21 self permanent
# ifconfig

docker0   Link encap:Ethernet  HWaddr 02:42:cf:32:13:6f
          inet addr:20.1.36.1  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::42:cfff:fe32:136f/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1
          RX packets:80 errors:0 dropped:0 overruns:0 frame:0
          TX packets:25 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:20968 (20.9 KB)  TX bytes:3043 (3.0 KB)

eth0      Link encap:Ethernet  HWaddr 00:50:56:a9:11:21
          inet addr:10.85.81.165  Bcast:10.85.81.255  Mask:255.255.255.128
          inet6 addr: fe80::250:56ff:fea9:1121/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:14131 errors:0 dropped:13 overruns:0 frame:0
          TX packets:21040 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:1793686 (1.7 MB)  TX bytes:80814089 (80.8 MB)

flannel.1 Link encap:Ethernet  HWaddr de:71:6e:55:36:75
          inet addr:20.1.36.0  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::dc71:6eff:fe55:3675/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1
          RX packets:26 errors:0 dropped:0 overruns:0 frame:0
          TX packets:85 errors:0 dropped:8 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:2016 (2.0 KB)  TX bytes:9186 (9.1 KB)

Also we tried to use Flannel backend ""udp"", and it did not have this above-described issue when using  backend ""vxlan"". When using backend ""udp"", the pods from different nodes can ping/communicate with each other. And we noticed in the flannel.log file, both ""L3 miss""/""Added L3 succeeded"" and ""L2 miss""/""Added L2 succeeded"" logs. 

It is strange that when use Flannel backend ""vxlan"",  the pods from different nodes cannot ping/communicate with each other. And in the flannel.log file, only ""L3 miss""/""Added L3 succeeded"" logs are there, without any ""L2 miss""/""Added L2 succeeded"" logs. 

Any insight and help with this issue will be appreciated very much.
",closed,False,2016-03-31 18:12:02,2018-02-13 07:44:09
contrib,eparis,https://github.com/kubernetes/contrib/pull/686,https://api.github.com/repos/kubernetes/contrib/issues/686,cherrypick: auto-approve cherrypicks if the pick to master was approved,,closed,True,2016-03-31 18:56:32,2016-03-31 23:05:49
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/687,https://api.github.com/repos/kubernetes/contrib/issues/687,Clarify default backend creation.,"Clarify how users create a default backend so they don't see a crashlooping nginx controller.
@aledbf 
",closed,True,2016-03-31 19:32:29,2016-03-31 19:52:59
contrib,gmarek,https://github.com/kubernetes/contrib/pull/688,https://api.github.com/repos/kubernetes/contrib/issues/688,Add more logging to GCS stable check,"Additional log in e2e.go will probably be quite verbose, so we need to fix this problem after changing to GCS as source of truth. #602

cc @wojtek-t 
",closed,True,2016-03-31 19:48:15,2016-03-31 23:05:05
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/689,https://api.github.com/repos/kubernetes/contrib/issues/689,Nginx ingress controller sometimes cannot parse certs,"With the default rc and gcr.io/google_containers/nginx-ingress-controller:0.4, try: 

```
$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls.key -out /tmp/tls.crt -subj ""/CN=echoheaders/O=echoheaders""
$ echo ""
apiVersion: v1
kind: Secret
metadata:
  name: tls
data:
  tls.crt: `base64 -w 0 /tmp/tls.crt`
  tls.key: `base64 -w 0 /tmp/tls.key`
"" | kubectl create -f -

$ echo ""
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  tls:
  - secretName: tls
  backend:
    serviceName: echoheaders
    servicePort: 8080
"" | kubectl create -f -
```

Logs show:

```
W0331 19:58:05.403185       1 controller.go:511] err asn1: structure error: tags don't match (16 vs {class:0 tag:2 length:1 isCompound:false}) {optional:false explicit:false application:false defaultValue:<nil> tag:<nil> stringType:0 timeType:0 s
```

We should also print out the error btw.
@aledbf 
",closed,False,2016-03-31 21:29:16,2016-05-04 04:37:02
contrib,cevian,https://github.com/kubernetes/contrib/issues/690,https://api.github.com/repos/kubernetes/contrib/issues/690,[election] split-brain scenario ,"I have this in the log of my election container (named postgres-system-1) :

> postgres-system-1 is the leader
> E0331 22:29:34.261269       7 leaderelection.go:281] err: Endpoints ""postgres-system"" is forbidden: unable to UPDATE endpoints at this time because there are too many concurrent requests to increment quota
> I0331 22:29:38.577846       7 leaderelection.go:259] lock is held by postgres-system-0 and has not yet expired
> I0331 22:29:43.902087       7 leaderelection.go:259] lock is held by postgres-system-0 and has not yet expired
> I0331 22:29:48.398030       7 leaderelection.go:259] lock is held by postgres-system-0 and has not yet expired
> E0331 22:29:57.136505       7 leaderelection.go:281] err: endpoints ""postgres-system"" cannot be updated: the object has been modified; please apply your changes to the latest version and try again
> I0331 22:30:00.912678       7 leaderelection.go:259] lock is held by postgres-system-0 and has not yet expired
> I0331 22:30:05.480872       7 leaderelection.go:259] lock is held by postgres-system-0 and has not yet expired
> I0331 22:30:08.180503       7 leaderelection.go:259] lock is held by postgres-system-0 and has not yet expired

And yet a curl to 127.0.0.1:4040 gives:
`{""name"":""postgres-system-1""}`

I believe this is a split-brain bug.
",closed,False,2016-03-31 22:48:52,2018-06-29 22:48:43
contrib,eparis,https://github.com/kubernetes/contrib/pull/691,https://api.github.com/repos/kubernetes/contrib/issues/691,Enable the cp auto approve munger,,closed,True,2016-03-31 23:08:28,2016-03-31 23:08:35
contrib,eparis,https://github.com/kubernetes/contrib/pull/692,https://api.github.com/repos/kubernetes/contrib/issues/692,mungegithub: Fix logo since it moved in the kube repo,,closed,True,2016-03-31 23:12:33,2016-03-31 23:12:42
contrib,eparis,https://github.com/kubernetes/contrib/pull/693,https://api.github.com/repos/kubernetes/contrib/issues/693,mungegithub: and fix the other bad logo link,,closed,True,2016-03-31 23:46:05,2016-03-31 23:46:26
contrib,shilpapadgaonkar,https://github.com/kubernetes/contrib/issues/694,https://api.github.com/repos/kubernetes/contrib/issues/694,HA setup using ansible,"Hello,
There is an inventory file inventory.example.ha provided in this repo. Does this ha setup work as in the readme it mentions that the setup is only valid for single master?

Thanks.
",closed,False,2016-04-01 13:35:53,2016-04-06 11:36:58
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/695,https://api.github.com/repos/kubernetes/contrib/issues/695,Implicit SecurityContext setting for admission control,"What is the expected goal of the ansible playbook? To deploy/install kubernetes on cluster without further modification of configuration files and be able to run e2e tests successfully? Or the highest goal is to deploy the cluster and keep the default configuration for running e2e tests on user?

My situation: When I run the e2e tests after deploying my cluster with ansible, all EmptyDir e2e tests fail because SecurityContext for admission control is set to Deny all pods that set `pod.Spec.SecurityContext` in pod's specification (e.g. RunAsUser, SELinuxOptions).

Is it expected that a user must set admission control the way it conforms with his use cases? So actually it is expected for all EmptyDir tests to fail? I am assuming it is always expected to set/update  some minimal configuration (e.g. set secure port, change location of binaries, etc). However, I would expect that all e2e tests pass with the current default configuration settings.
",closed,False,2016-04-01 13:38:43,2016-04-01 15:49:36
contrib,rutsky,https://github.com/kubernetes/contrib/issues/696,https://api.github.com/repos/kubernetes/contrib/issues/696,[Ansible+Vagrant+VirtualBox] use static IP instead of DHCP,"Currently on Vagrant we rewrite `/etc/hosts` with public IPs of nodes that were received at the time of provision in [vagrant-ansible.yml](https://github.com/kubernetes/contrib/blob/master/ansible/vagrant/vagrant-ansible.yml#L17), e.g.:

```
$ cat /etc/hosts 
172.28.128.5 kube-master
172.28.128.3 kube-node-1
172.28.128.4 kube-node-2
```

this is OK for quick testing, but after some time IP addresses of nodes may change (for example if you shutdown host for weekend), which will lead to communication failures between different Kubernetes parts.

I suggest to use fixed IP addresses at least on VirtualBox, where VirtualBox software DHCP server is used.

The question is how to properly select IP addresses range to use?

/cc: @eparis @danehans
",closed,False,2016-04-01 13:49:05,2016-04-08 18:57:14
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/697,https://api.github.com/repos/kubernetes/contrib/issues/697,Enable security context in clusters by default," Enable security context in clusters by default to have testing environment match the production one

Fixes: #695
",closed,True,2016-04-01 15:04:24,2016-04-01 15:49:03
contrib,gmarek,https://github.com/kubernetes/contrib/pull/698,https://api.github.com/repos/kubernetes/contrib/issues/698,Add even more logging to munger,"Ref. #602 cc @wojtek-t 
",closed,True,2016-04-01 16:03:01,2016-04-01 16:19:38
contrib,eparis,https://github.com/kubernetes/contrib/pull/699,https://api.github.com/repos/kubernetes/contrib/issues/699,submit queue: Update chip links,"They used to point to
storage.cloud.google.com/
But that stopped working so point to
console.cloud.google.com/storage/
",closed,True,2016-04-01 17:16:48,2016-04-07 18:33:25
contrib,rutsky,https://github.com/kubernetes/contrib/issues/700,https://api.github.com/repos/kubernetes/contrib/issues/700,[Ansible+Vagrant+VirtualBox] K8s doesn't work due to incorrect endpoint IP of `kubernetes` service,"On Vagrant+VirtualBox public IP is on `eth1`, not on `eth0` (which is used for NAT-ed Internet access).
K8s apiserver when started by default uses default ethernet device to get public IP address, which leads to publishing unroutable IP:

```
kube-master ~ # kubectl get endpoints --namespace=default
NAME         ENDPOINTS       AGE
kubernetes   10.0.2.15:443   2d
kube-master ~ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:96:9e:8a brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 80637sec preferred_lft 80637sec
    inet6 fe80::a00:27ff:fe96:9e8a/64 scope link 
       valid_lft forever preferred_lft forever
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:48:ea:40 brd ff:ff:ff:ff:ff:ff
    inet 172.28.128.5/24 brd 172.28.128.255 scope global dynamic eth1
       valid_lft 679sec preferred_lft 679sec
    inet6 fe80::a00:27ff:fe48:ea40/64 scope link 
       valid_lft forever preferred_lft forever
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default 
    link/ether be:98:8d:1e:3e:6e brd ff:ff:ff:ff:ff:ff
    inet 172.16.11.0/12 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::bc98:8dff:fe1e:3e6e/64 scope link 
       valid_lft forever preferred_lft forever
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:48:9f:49:57 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 scope global docker0
       valid_lft forever preferred_lft forever
```

To workaround this it's enough to start add to `/etc/kubernetes/apiserver` in `KUBE_API_ARGS` variable `--bind-address=172.28.128.5` with proper public IP address.

After workaround:

```
kube-master ~ # kubectl get endpoints --all-namespaces                                                                                                                                                         
NAMESPACE     NAME                    ENDPOINTS                         AGE
default       kubernetes              172.28.128.5:443                  2d
kube-system   elasticsearch-logging   172.17.0.3:9200,172.17.0.5:9200   2d
kube-system   heapster                172.17.0.5:8082                   2d
kube-system   kibana-logging          172.17.0.4:5601                   2d
kube-system   kube-dns                172.17.0.3:53,172.17.0.3:53       2d
kube-system   monitoring-grafana      172.17.0.4:3000                   2d
kube-system   monitoring-influxdb     172.17.0.4:8083,172.17.0.4:8086   2d
```

This is not the first issue with default ethernet device on Vagrant+Virtualbox configuration, perhaps someone knows how to tune VirtualBox in Vagrantfile to use single interface for Internet and inter-VM communication?

/cc: @danehans 
",closed,False,2016-04-01 19:32:54,2016-04-08 18:57:18
contrib,unicell,https://github.com/kubernetes/contrib/pull/701,https://api.github.com/repos/kubernetes/contrib/issues/701,Avoid unnecessary 'changed' for kubernetes-addons playbooks,"Currently several addon spec file are always downloaded from Github,
each time Ansible run. Then installed to target configuration file after
inline changes.

This creates too many 'changed' noise and breaks the expectation that
Ansible run should be idempotent and only report 'changed' when really
does.

This change mute 'changed' message for downloading and inline replacing
part, and let the final install to /etc part report 'changed' or not.
",closed,True,2016-04-01 19:40:08,2016-04-02 02:15:53
contrib,stephenrlouie,https://github.com/kubernetes/contrib/issues/702,https://api.github.com/repos/kubernetes/contrib/issues/702,Conditional Downloads of tars / packages,"- Locally host large files such as K8s tar and flannel tar to speed up deployment
- Provide the option to host these files local to the deployment, if not we can still find them on the internet
- @danehans has demonstrated a significant speed up for deployment by adding download_variables in [group_vars/all/yml](https://github.com/danehans/contrib/blob/gem_master/ansible/group_vars/all.yml#L99)

Not sure what would be optimal for upstream, but having this feature would be helpful and I wanted to document it.

cc @rutsky @eparis @adamschaub @danehans 
",closed,False,2016-04-01 19:57:31,2018-02-14 00:00:13
contrib,gmarek,https://github.com/kubernetes/contrib/pull/703,https://api.github.com/repos/kubernetes/contrib/issues/703,GCSStable returns false in case of error,,closed,True,2016-04-02 15:06:58,2016-04-02 15:16:53
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/704,https://api.github.com/repos/kubernetes/contrib/issues/704,Readme for tls example.,,closed,True,2016-04-04 05:26:05,2016-04-04 06:21:33
contrib,luxas,https://github.com/kubernetes/contrib/pull/705,https://api.github.com/repos/kubernetes/contrib/issues/705,Fixup the exechealthz code to minimize mistakes,"@bprashanth 
",closed,True,2016-04-04 13:50:08,2016-04-04 16:47:04
contrib,joonathan,https://github.com/kubernetes/contrib/pull/706,https://api.github.com/repos/kubernetes/contrib/issues/706,Updated with correction for Ingress TLS spec,"Documentation fix for `error validating data: expected type array, for field spec.tls, got map; if you choose to ignore these errors, turn validation off with --validate=false`
",closed,True,2016-04-04 16:05:07,2016-04-06 17:09:26
contrib,aledbf,https://github.com/kubernetes/contrib/pull/707,https://api.github.com/repos/kubernetes/contrib/issues/707,Allow traffic to default server _ in nginx Ingress controller,,closed,True,2016-04-04 16:55:35,2016-04-10 22:16:06
contrib,gmarek,https://github.com/kubernetes/contrib/pull/708,https://api.github.com/repos/kubernetes/contrib/issues/708,Munger uses GCS as a source of truth,"Let's do this.

cc @kubernetes/sig-testing @spiffxp @spxtr @ixdy @justinsb 
",closed,True,2016-04-05 00:54:03,2016-04-11 15:39:37
contrib,thockin,https://github.com/kubernetes/contrib/pull/709,https://api.github.com/repos/kubernetes/contrib/issues/709,Use -o name on kubectl run,"Also arrange tmux panes for easier keybaord input.

Also rename scripts to have meaning on splits.
",closed,True,2016-04-05 04:15:03,2016-04-08 07:53:54
contrib,rootfs,https://github.com/kubernetes/contrib/pull/710,https://api.github.com/repos/kubernetes/contrib/issues/710,set KillMode=process for kubelet,"When a pod requests a e.g. glusterfs, kubelet mounter will in the end invoke the glusterfs mount daemon. The daemon stays till the volume is unmounted. If systemd stops kubelet with killMode=control-group (the default), both kubelet and glusterfs daemon is killed, while the container stays alive with a broken bind mount. When systemd starts kubelet again, even though kubelet is able to re-mount the volume, the broken bind mount in the container cannot be repaired. This is what happened in Kubernetes #13511

The proposed fix is to tell systemd to just kill kubelet and leave other processes alive by setting killMode=process, the glusterfs daemon stays with the container when kubelet is stopped.
",closed,True,2016-04-05 14:12:15,2016-04-12 17:42:38
contrib,adamschaub,https://github.com/kubernetes/contrib/pull/711,https://api.github.com/repos/kubernetes/contrib/issues/711,Fixes ipv4 address resolution of etcd peers in etcd config,"Simple fix to get static etcd cluster configuration working correctly.
",closed,True,2016-04-05 15:35:09,2016-04-06 15:47:15
contrib,rutsky,https://github.com/kubernetes/contrib/pull/712,https://api.github.com/repos/kubernetes/contrib/issues/712,[Vagrant+VBox+Ansible] use static ip + explicitly set API Server bind address,"Fixes #696 and #700 .

/cc @danehans @eparis @adamschaub 
",closed,True,2016-04-05 16:01:39,2016-04-08 18:57:14
contrib,spxtr,https://github.com/kubernetes/contrib/issues/713,https://api.github.com/repos/kubernetes/contrib/issues/713,It should be possible to run a local submit queue with fake GitHub/Jenkins.,"Right now we have the ability to do a `--dry-run`, which doesn't mutate GitHub but still tries to read from it and Jenkins. It would be useful to have `--fake`, which just generates fake PRs and the like for local testing.

I haven't quite looked through everything so this might be possible already, but if not I'd be happy to implement it.

@kubernetes/sig-testing what do you think?
",closed,False,2016-04-05 20:45:44,2016-04-11 16:35:06
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/714,https://api.github.com/repos/kubernetes/contrib/issues/714,Created variable for flannel download url,,closed,True,2016-04-05 21:16:12,2016-04-06 18:10:06
contrib,spxtr,https://github.com/kubernetes/contrib/pull/715,https://api.github.com/repos/kubernetes/contrib/issues/715,Add a fake e2e tester.,"Pass in `--fake-e2e` and all jobs will be stable. I'm really not sure if this is the idiomatic way of doing this in go, but it seems reasonable.

It's probably a good idea to switch the submit-queue_test to use this, but that can wait.

ref #713
",closed,True,2016-04-06 00:00:30,2016-04-07 14:45:28
contrib,rutsky,https://github.com/kubernetes/contrib/issues/716,https://api.github.com/repos/kubernetes/contrib/issues/716,[Vagrant+VBox+Ansible+CoreOS] DNS is not working in containers,"With #712 I managed to get almost working Kubernetes cluster (yes, again ""almost"" :) ).

```
kube-master ~ # kubectl cluster-info
Kubernetes master is running at http://localhost:8080
Elasticsearch is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
Heapster is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/heapster
Kibana is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kibana-logging
KubeDNS is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kube-dns
Grafana is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
InfluxDB is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb
kube-master ~ # kubectl get pods --all-namespaces
NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE
default       wip-2165332859-lc4zn                    1/1       Running   0          4h
kube-system   elasticsearch-logging-v1-790pr          1/1       Running   0          8h
kube-system   elasticsearch-logging-v1-sqtin          1/1       Running   1          8h
kube-system   fluentd-elasticsearch-kube-node-1       1/1       Running   0          5h
kube-system   fluentd-elasticsearch-kube-node-2       1/1       Running   0          5h
kube-system   heapster-v1.1.0.beta1-345332335-vhp32   4/4       Running   0          8h
kube-system   kibana-logging-v1-x0w4y                 1/1       Running   0          8h
kube-system   kube-dns-v11-wcwna                      4/4       Running   0          3h
kube-system   monitoring-influxdb-grafana-v3-h9bxv    2/2       Running   0          8h
```

DNS addon is started and passes it's own health checks, but DNS in other containers doesn't work:

```
kube-master ~ # kubectl run wip --rm --tty -i --image=busybox -- /bin/sh
Waiting for pod default/wip-2165332859-lc4zn to be running, status is Pending, pod ready: false

Hit enter for command prompt

/ # nslookup kubernetes.default.svc.cluster.local
Server:    10.254.0.10
Address 1: 10.254.0.10

nslookup: can't resolve 'kubernetes.default.svc.cluster.local'
/ # 
```

Also I see strange long hangs with some operations, e.g.

```
kube-master ~ # time kubectl exec --namespace=kube-system kube-dns-v11-wcwna -c healthz -- time nslookup kubernetes.default.svc.cluster.local 127.
0.0.1                                                                                                                                             
Server:    127.0.0.1
Address 1: 127.0.0.1 localhost

Name:      kubernetes.default.svc.cluster.local
Address 1: 10.254.0.1
real    0m 0.50s
user    0m 0.00s
sys     0m 0.00s

real    0m12.893s
user    0m0.026s
sys     0m0.016s
```

command execution took half a second, but whole `kubectl exec` took about 13 seconds (which is unacceptably long).
Perhaps there are some Flannel issues.

/cc @danehans @adamschaub
",closed,False,2016-04-06 00:30:52,2016-04-07 11:28:18
contrib,microadam,https://github.com/kubernetes/contrib/issues/717,https://api.github.com/repos/kubernetes/contrib/issues/717,Ingress - Unable to connect to LB on port 80,"Following the examples mentioned here:

https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx

I get everything created and running, however when I try and connect to my host on its external IP address and port 80, I just get connection refused.

Even trying to `curl http://127.0.0.1` on the machine itself returns connection refused, so its like the load balancing Pod is not actually listening on the hosts port 80.

Anyone have any suggestions as to how to go about debugging?

Thanks a lot
",closed,False,2016-04-06 13:09:21,2017-04-04 15:14:55
contrib,deromka,https://github.com/kubernetes/contrib/pull/718,https://api.github.com/repos/kubernetes/contrib/issues/718,"fixed error when no 'ansible_default_ipv4.address' defined on IPv4, u…","fixed error when no 'ansible_default_ipv4.address' defined on IPv4, updated to 'ansible_fqdn'
",closed,True,2016-04-06 13:36:40,2016-04-06 14:20:03
contrib,danehans,https://github.com/kubernetes/contrib/issues/719,https://api.github.com/repos/kubernetes/contrib/issues/719,[Ansible] v2.0.1.0 Does Not Work For CoreOS Deployments,"Ansible 2.0.1.0 (default for Mac OSX 10.10.5) does not work for CoreOS deployments.

```
# ansible --version
ansible 2.0.1.0
  config file = 
  configured module search path = Default w/o overrides
```

Run setup

```
# ./setup.sh 
[DEPRECATION WARNING]: Instead of sudo/sudo_user, use become/become_user and make sure become_method is 'sudo' (default). This feature will be removed 
in a future release. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.

# ./setup.sh 
[DEPRECATION WARNING]: Instead of sudo/sudo_user, use become/become_user and make sure become_method is 'sudo' (default). This feature will be removed 
in a future release. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.

PLAY ***************************************************************************

TASK [pre-ansible : Get os_version from /etc/os-release] ***********************
ok: [core-master]

TASK [pre-ansible : Get distro name from /etc/os-release] **********************
ok: [core-master]

TASK [pre-ansible : Init the is_coreos fact] ***********************************
ok: [core-master]

TASK [pre-ansible : Set the is_coreos fact] ************************************
ok: [core-master]

TASK [pre-ansible : include] ***************************************************
included: /root/contrib/ansible/roles/pre-ansible/tasks/coreos.yml for core-master

TASK [pre-ansible : CoreOS | Check if bootstrap is needed] *********************
fatal: [core-master]: FAILED! => {""changed"": false, ""failed"": true, ""rc"": 1, ""stderr"": """", ""stdout"": ""stat: cannot stat '/opt/bin/.bootstrapped': No such file or directory\r\n"", ""stdout_lines"": [""stat: cannot stat '/opt/bin/.bootstrapped': No such file or directory""]}
...ignoring

TASK [pre-ansible : CoreOS | Run bootstrap.sh] *********************************
changed: [core-master]

TASK [pre-ansible : Determine if Atomic] ***************************************
ok: [core-master]

TASK [pre-ansible : Set the is_atomic fact] ************************************
skipping: [core-master]

TASK [pre-ansible : include] ***************************************************
skipping: [core-master]

PLAY ***************************************************************************

TASK [setup] *******************************************************************
fatal: [core-master]: FAILED! => {""changed"": false, ""failed"": true, ""module_stderr"": """", ""module_stdout"": ""/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n/opt/bin/pypy/bin/pypy: /lib64/libssl.so.1.0.0: no version information available (required by /opt/bin/pypy/bin/pypy)\r\n/opt/bin/pypy/bin/pypy: /lib64/libcrypto.so.1.0.0: no version information available (required by /opt/bin/pypy/bin/pypy)\r\n/bin/sh: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n/bin/sh: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\nTraceback (most recent call last):\r\n  File \""app_main.py\"", line 75, in run_toplevel\r\n  File \""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup\"", line 5183, in <module>\r\n    main()\r\n  File \""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup\"", line 139, in main\r\n    data = run_setup(module)\r\n  File \""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup\"", line 83, in run_setup\r\n    facts = ansible_facts(module)\r\n  File \""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup\"", line 5118, in ansible_facts\r\n    facts.update(Facts().populate())\r\n  File \""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup\"", line 2258, in __init__\r\n    self.get_platform_facts()\r\n  File \""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup\"", line 2304, in get_platform_facts\r\n    self.get_distribution_facts()\r\n  File \""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup\"", line 2607, in get_distribution_facts\r\n    release = re.search(\""^GROUP=(.*)\"", data)\r\n  File \""/opt/bin/pypy/lib-python/2.7/re.py\"", line 142, in search\r\n    return _compile(pattern, flags).search(string)\r\nTypeError: expected a readable buffer object\r\n"", ""msg"": ""MODULE FAILURE"", ""parsed"": false}

NO MORE HOSTS LEFT *************************************************************
    to retry, use: --limit @cluster.retry

PLAY RECAP *********************************************************************
core-master                : ok=8    changed=1    unreachable=0    failed=1  
```

Traceback

```
Traceback (most recent call last):
  File ""app_main.py"", line 75, in run_toplevel
  File ""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup"", line 5183, in <module>
    main()
  File ""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup"", line 139, in main
    data = run_setup(module)
  File ""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup"", line 83, in run_setup
    facts = ansible_facts(module)
  File ""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup"", line 5118, in ansible_facts
    facts.update(Facts().populate())
  File ""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup"", line 2258, in __init__
    self.get_platform_facts()
  File ""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup"", line 2304, in get_platform_facts
    self.get_distribution_facts()
  File ""/home/core/.ansible/tmp/ansible-tmp-1459264218.19-221406660048084/setup"", line 2607, in get_distribution_facts
    release = re.search(""^GROUP=(.*)"", data)
  File ""/opt/bin/pypy/lib-python/2.7/re.py"", line 142, in search
    return _compile(pattern, flags).search(string)
TypeError: expected a readable buffer object
```

More information can be found here https://github.com/kubernetes/contrib/pull/655
",closed,False,2016-04-06 15:56:03,2018-02-14 00:00:12
contrib,wfhartford,https://github.com/kubernetes/contrib/pull/720,https://api.github.com/repos/kubernetes/contrib/issues/720,Fix daemonset arguments to match replication controller,"The example DaemonSet as it exists causes the pod to crash on startup with the following:

Error syncing pod, skipping: failed to ""StartContainer"" for ""nginx-ingress-lb"" with RunContainerError: ""runContainer: API error (500): Cannot start container 7eab41694a7d817665337724b5710bd17d7846f0212408aa9d6056e3211447a4: [8] System error: exec: \""/nginx-ingress-controller-lb\"": stat /nginx-ingress-controller-lb: no such file or directory\n""

Modifying the arguments to match the replication controller works.
",closed,True,2016-04-06 16:17:38,2016-04-19 22:54:52
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/721,https://api.github.com/repos/kubernetes/contrib/issues/721,submit queue dashboard should display time since last merge,"To make it easier to detect when it's been blocked.
",closed,False,2016-04-06 16:49:07,2016-04-12 17:41:14
contrib,adamschaub,https://github.com/kubernetes/contrib/issues/722,https://api.github.com/repos/kubernetes/contrib/issues/722,[ansible] systemd service configuration reload,"When performing a reinstall/reconfiguration, systemd management is reloaded through a notify action, usually after a service has been started. Some roles restart the service after this step, but it is not uniform between roles. This can lead to stale/default configurations being used during a reconfiguration.

I'd propose we move all service start/restart tasks to notify actions so that a daemon reload (if required) precedes starting/restarting the service.
",closed,False,2016-04-06 17:17:49,2018-02-14 00:00:12
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/723,https://api.github.com/repos/kubernetes/contrib/issues/723,Matching the url naming convention to the kube and flannel names,"- Changed naming convention to match kube and flannel
- Added the slash in the path, given @rutsky's comment: https://github.com/kubernetes/contrib/pull/714
",closed,True,2016-04-06 17:47:27,2016-04-08 15:01:43
contrib,rutsky,https://github.com/kubernetes/contrib/pull/724,https://api.github.com/repos/kubernetes/contrib/issues/724,"add ""etcd_interface"" variable ","Allows to specify host public interface on which etcd will listen.
Used to construct default advertisement addresses.

Useful on configuration when default interface is not connected to other
machines, such as in Vagrant+VirtualBox configuration.

Commit with setting interface in Vagrant+VirtualBox configuration will conflict with #712 - lets merge any of these PRs and I'll other one.

@adamschaub I included suggested change with looping in template: https://github.com/rutsky/contrib/blob/etcd-interface/ansible/roles/etcd/templates/etcd.conf.j2#L4

/cc: @danehans @eparis 
",closed,True,2016-04-06 18:49:09,2016-04-07 14:58:39
contrib,adamschaub,https://github.com/kubernetes/contrib/issues/725,https://api.github.com/repos/kubernetes/contrib/issues/725,[ansible] Add support for kube-master HA,"Production environments will require kube-master components to be run in a highly-available manner. See [1] and [2] for official guidelines.
#673 is a dependency for running these components in containers.

[1] http://kubernetes.io/docs/admin/high-availability/
[2] https://coreos.com/kubernetes/docs/latest/deploy-master.html

/cc @danehans @rutsky @stephenrlouie 
",closed,False,2016-04-06 19:44:55,2018-02-14 00:00:12
contrib,roberthbailey,https://github.com/kubernetes/contrib/issues/726,https://api.github.com/repos/kubernetes/contrib/issues/726,e2e links on submit queue dashboard are broken,"Links on http://submit-queue.k8s.io/#/e2e are currently broken. 

/cc @spxtr 
",closed,False,2016-04-06 19:50:57,2016-04-21 13:54:13
contrib,goltermann,https://github.com/kubernetes/contrib/issues/727,https://api.github.com/repos/kubernetes/contrib/issues/727,Submit Queue should show size of PRs,"Both http://submit-queue.k8s.io/#/queue and http://submit-queue.k8s.io/#/prs should probably include size info (lines added/deleted).

This can be useful for build cop to get an idea of which ones might be good candidates for manual merges when the queue is struggling.
",closed,False,2016-04-06 19:56:55,2016-04-11 16:35:20
contrib,ixdy,https://github.com/kubernetes/contrib/pull/728,https://api.github.com/repos/kubernetes/contrib/issues/728,Use http://ci-test.k8s.io/ for links on SQ dashboard,"Fixes #726 once kubernetes/kubernetes#20161 is fixed.

@spxtr @roberthbailey 
",closed,True,2016-04-06 20:44:33,2016-04-21 13:54:13
contrib,spxtr,https://github.com/kubernetes/contrib/pull/729,https://api.github.com/repos/kubernetes/contrib/issues/729,Add job health to the internal e2e page.,"Every munge loop, the submit queue now tracks which jobs are stable and which aren't, and keeps a running total. The internal e2e page now displays these percents next to the job like so:

![ss](https://cloud.githubusercontent.com/assets/7368979/14333710/efa28a64-fc04-11e5-886e-ec35120b75f8.png)

I'd like to have a chart showing health as a function of time, and I would like to have it save the health data across restarts. It also won't be tough to make the color scale from green to red as the job fails more and more. Those can happen in future PRs.

cc @kubernetes/sig-testing 
",closed,True,2016-04-06 21:45:18,2016-04-08 21:25:16
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/730,https://api.github.com/repos/kubernetes/contrib/issues/730,Ingress controller should verify contents of Kube secret match certs used for termination,"All controllers should do this so a user can simply update the Kubernetes Secret to update certs.
The fix in GCE would go here: https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/loadbalancers/loadbalancers.go#L332, while we're fixing this, we should probably also fix this todo: https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/controller/tls.go#L59, so updates to the secret are noticed immediately.
",closed,False,2016-04-06 21:59:34,2016-07-02 03:09:51
contrib,rutsky,https://github.com/kubernetes/contrib/pull/731,https://api.github.com/repos/kubernetes/contrib/issues/731,add Docker dropin to start it after Flannel on CoreOS,"Without this Docker can start before Flannel which will lead to
incorrect configuration of docker0 device (since configuration for it
prepares Flannel).

This should fix issue #716.

As this issue was not reported on other configuration, I believe on
other OSes Docker already depends on Flannel.

/cc: @danehans @eparis 
",closed,True,2016-04-07 12:02:20,2016-04-13 14:46:05
contrib,danehans,https://github.com/kubernetes/contrib/issues/732,https://api.github.com/repos/kubernetes/contrib/issues/732,[ansible] Add Cloud Provider Support,"Currently, the contrib/ansible project supports bare-metal and vagrant deployments. Deploying Kubernetes to a cloud such as AWS, GCE or OpenStack is a common request. Adding cloud provider support, i.e. 'cloud_provider == 'gce' will increase the adoption of contrib/ansible and provide a single interface to meet a wide range of deployment types.

/cc @eparis @rutsky @adamschaub @stephenrlouie 
",closed,False,2016-04-07 16:44:10,2018-02-14 00:00:09
contrib,donbeave,https://github.com/kubernetes/contrib/issues/733,https://api.github.com/repos/kubernetes/contrib/issues/733,[service-loadbalancer] Wrong client IP in X-Forwarded-For,"I have installed latest Kubernetes (1.2.1) and latest service-loadbalancer (gcr.io/google_containers/servicelb:0.4). And configured my service to use ""source"" algorithm and hostname, in result in my nginx logs I still can see ""X-Forwarded-For"" in same range as POD_NETWORK ip:

```
10.2.74.4 - - [07/Apr/2016:19:43:51 +0000] ""GET / HTTP/1.1"" 200 612 ""-"" ""curl/7.43.0"" ""10.2.74.1""
```

My nginx log format is:

```
    log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '
                      '$status $body_bytes_sent ""$http_referer"" '
                      '""$http_user_agent"" ""$http_x_forwarded_for""';
```
",closed,False,2016-04-07 19:49:28,2016-04-17 09:05:22
contrib,ixdy,https://github.com/kubernetes/contrib/issues/734,https://api.github.com/repos/kubernetes/contrib/issues/734,CI and PR test result viewer,"Pony project: rather than having ci-test.k8s.io and pr-test.k8s.io redirect to the GCS storage browser, we should have a website that's smarter about showing test results/logs/etc.

At a minimum, giving a Jenkins project name and build, we should show
- start time, duration, Kubernetes version, overall result (trivial from the `started.json` and `finished.json` files)
- list of all tests that ran/passed/failed and their duration (pretty simple from parsing the junit XML files) - and maybe expandos to show failing messages etc
- the build log
- download links for all artifacts

Getting more advanced, we could have an interface to show all build results for a given PR (already pretty trivial from the directory structure we have).

Getting much more advanced, it'd be great to have an interface to provide a git sha1 and see everywhere that has been tested - PR, CI, etc.

We can start by just looking at the Google-run kubernetes-jenkins GCS bucket, but we should plan to support federated results here too.

@kubernetes/sig-testing 
",closed,False,2016-04-07 20:42:16,2016-08-19 18:02:14
contrib,spxtr,https://github.com/kubernetes/contrib/pull/735,https://api.github.com/repos/kubernetes/contrib/issues/735,Submit queue: show the additions/deletions of PRs.,"![ss2](https://cloud.githubusercontent.com/assets/7368979/14366433/9464678a-fcc7-11e5-8562-089e10e6a9f1.png)

![ss3](https://cloud.githubusercontent.com/assets/7368979/14366436/9738f96c-fcc7-11e5-9899-d9b7eefd1d1d.png)

ref #727
cc @goltermann 
",closed,True,2016-04-07 20:52:11,2016-04-07 21:13:46
contrib,eparis,https://github.com/kubernetes/contrib/pull/736,https://api.github.com/repos/kubernetes/contrib/issues/736,Do not require (or set) release-note-* labels on non-master,,closed,True,2016-04-07 21:11:01,2016-04-07 22:12:26
contrib,gmarek,https://github.com/kubernetes/contrib/pull/737,https://api.github.com/repos/kubernetes/contrib/issues/737,Make utils URL configurable,"cc @wojtek-t 
",closed,True,2016-04-07 21:52:41,2016-04-08 14:58:45
contrib,eparis,https://github.com/kubernetes/contrib/pull/738,https://api.github.com/repos/kubernetes/contrib/issues/738,Require labels on non-master PRs whose 'parents' do not have release-note-*,,closed,True,2016-04-07 23:59:09,2016-04-08 02:24:08
contrib,paralin,https://github.com/kubernetes/contrib/issues/739,https://api.github.com/repos/kubernetes/contrib/issues/739,NGINX Default backend seems to be broken,"The NGINX config makes two servers, one on 80 and one on 8080. 

The one on `80` does not have the default backend configured, it just returns 200.

The one on `8080` has the default backend configured, but despite the backend returning `Content-Type: text/html`, the nginx server changes it to `Content-Type: application/octet-stream` which results in an Invalid in chrome.
",closed,False,2016-04-08 01:00:01,2016-06-29 17:16:33
contrib,eparis,https://github.com/kubernetes/contrib/pull/740,https://api.github.com/repos/kubernetes/contrib/issues/740,submit-queue: Display info to help understand the active queue order,,closed,True,2016-04-08 02:18:29,2016-04-08 02:35:43
contrib,gmarek,https://github.com/kubernetes/contrib/pull/741,https://api.github.com/repos/kubernetes/contrib/issues/741,Add go1.6 to hack/verify-gofmt.sh and fail verify check if go version…,"… is unknown

cc @eparis @ixdy 
",closed,True,2016-04-08 02:46:04,2016-04-08 03:00:28
contrib,eparis,https://github.com/kubernetes/contrib/pull/742,https://api.github.com/repos/kubernetes/contrib/issues/742,submit-queue: Minor display changes to waste less space,,closed,True,2016-04-08 03:01:05,2016-04-08 03:01:38
contrib,paralin,https://github.com/kubernetes/contrib/issues/743,https://api.github.com/repos/kubernetes/contrib/issues/743,NGINX ingress controller does not work properly with a named targetPort,"When using a named targetPort on the service it does not resolve which endpoint port to use, as far as I can tell.

This is a bit of a hard problem to solve because it's not immediately apparent from the endpoints list what the port name is.
",closed,False,2016-04-08 03:16:35,2016-05-02 16:01:56
contrib,danehans,https://github.com/kubernetes/contrib/pull/744,https://api.github.com/repos/kubernetes/contrib/issues/744,Fixes pypy url by removing trailing slash,,closed,True,2016-04-08 08:35:41,2016-04-08 13:56:39
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/745,https://api.github.com/repos/kubernetes/contrib/issues/745,Flannel URL base to contain everything except tar,"- To match how kube_download_url_base is handled.
- Similar to #723
- Base URL = path up to the final tar.
",closed,True,2016-04-08 13:29:48,2016-04-12 17:42:26
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/746,https://api.github.com/repos/kubernetes/contrib/issues/746,Fixed pypy tar to use the same naming convention as flannel and kube,,closed,True,2016-04-08 15:02:28,2016-04-08 15:02:57
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/747,https://api.github.com/repos/kubernetes/contrib/issues/747,pypy url_base naming convention to match flannel and kube,"- Finally clean PR

@rutsky @danehans @eparis. Could you all check this out? 

Rebase on #723 
",closed,True,2016-04-08 15:08:20,2016-04-08 22:23:15
contrib,rutsky,https://github.com/kubernetes/contrib/pull/748,https://api.github.com/repos/kubernetes/contrib/issues/748,conditionally create /etc/hosts file,"Now /etc/hosts file is not being touched (i.e. modification time is not
changed) if it already exists.

Removed unnecessary ""changed"" state in Ansible run summary.

/cc @danehans 
",closed,True,2016-04-08 17:23:36,2016-04-08 17:54:11
contrib,rutsky,https://github.com/kubernetes/contrib/pull/749,https://api.github.com/repos/kubernetes/contrib/issues/749,update K8s version used in github-release to 1.2.2,"/cc @danehans @eparis 
",closed,True,2016-04-08 18:39:16,2016-04-08 18:54:21
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/750,https://api.github.com/repos/kubernetes/contrib/issues/750,Added slash to kube url,"- For consistency

@rutsky @danehans @eparis 
",closed,True,2016-04-08 19:10:49,2016-04-08 19:13:45
contrib,gmarek,https://github.com/kubernetes/contrib/pull/751,https://api.github.com/repos/kubernetes/contrib/issues/751,Update munger tests so they will work for GCS based checker,"Needed for moving to GCS. @kubernetes/sig-testing 
",closed,True,2016-04-10 04:56:59,2016-04-11 01:53:24
contrib,abudargo,https://github.com/kubernetes/contrib/issues/752,https://api.github.com/repos/kubernetes/contrib/issues/752,"NGINX ingress controller pods couldn't run, shows CrashLoopBackOff","I'm trying using Ingress using this example:
[https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/daemonset]

but when i run the daemonset, the pods couldn't run
it show error like this:

> FailedSync    Error syncing pod, skipping: failed to ""StartContainer"" for ""nginx"" with CrashLoopBackOff: ""Back-off 20s restarting failed container=nginx pod=nginx-6btee_default(eef288eb-ff0c-11e5-8b33-0401bf497f01)""

and from the pods log, it show like this:

> main.go:101 unexpected error getting runtime information: resource name may not be empty

I wonder what resource name that was empty?
am i doing this right?
",closed,False,2016-04-10 11:30:51,2017-05-06 23:55:20
contrib,aledbf,https://github.com/kubernetes/contrib/pull/753,https://api.github.com/repos/kubernetes/contrib/issues/753,Update keepalived,"## 0.4
- Update keepalived to [1.2.20](https://github.com/acassen/keepalived/blob/master/ChangeLog)
- Use iptables parameter to not respond on addresses that it does not own
- Replace annotations with ConfigMap to specify services and IP addresses
- Avoid unnecessary reloads if the configuration did not change
- The parameter `--password` was removed because is not supported in vrrp v3
",closed,True,2016-04-10 14:38:27,2016-06-21 19:54:10
contrib,gmarek,https://github.com/kubernetes/contrib/pull/754,https://api.github.com/repos/kubernetes/contrib/issues/754,Fix WeakStable in munger and add tests,"Depends on #751

cc @wojtek-t 
",closed,True,2016-04-11 00:55:19,2016-04-11 03:24:42
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/755,https://api.github.com/repos/kubernetes/contrib/issues/755,Run nginx controller on a non-minion node,"Add a `--running-in-cluster=false` mode. This is useful for running the controller on any non-minion node (eg: edge router, kubernetes master, local desktop for development etc). 

Also adds an all-in-one rc example.
@aledbf 
",closed,True,2016-04-11 01:34:08,2016-04-11 16:58:42
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/756,https://api.github.com/repos/kubernetes/contrib/issues/756,Nginx ingress controller should continuously start/restart nginx if not running,"If nginx isn't able to run because of eg: port conflict, the controller should keep trying to restar it. 
I ran the controller with --net=host, and it failed initially because of a port conflict, and never really came up thereafter till I exec'd into the pod and ./nginx
",closed,False,2016-04-11 01:41:12,2016-04-11 01:42:49
contrib,BraunreutherA,https://github.com/kubernetes/contrib/issues/757,https://api.github.com/repos/kubernetes/contrib/issues/757,[Ansible] Starting of the docker service is not working,"Hi, when I execute the ansible playbooks on Fedora as described in http://kubernetes.io/docs/getting-started-guides/fedora/fedora_ansible_config/ the follwing error occurs:

```
RUNNING HANDLER [flannel : docker start service] *******************************
fatal: [kube-node-1.xxx.com]: FAILED! => {""changed"": false, ""failed"": true, ""invocation"": {""module_args"": {""arguments"": """", ""enabled"": null, ""name"": ""docker"", ""pattern"": null, ""runlevel"": ""default"", ""sleep"": null, ""state"": ""started""}, ""module_name"": ""service""}, ""msg"": ""Job for docker.service failed. See \""systemctl status docker.service\"" and \""journalctl -xe\"" for details.\n""}
```

This error occurs for all nodes. 

When I ssh into the server and execute I get the following error: 

```
Apr 11 21:18:41 xxx docker[6942]: time=""2016-04-11T21:18:41.866226699+02:00"" level=info msg=""Listening for HTTP on unix (/var/run/docker.sock)""
Apr 11 21:18:41 xxx docker[6942]: time=""2016-04-11T21:18:41.872257876+02:00"" level=error msg=""WARNING: No --storage-opt dm.thinpooldev specified, using loopback; this configuration is strongly discouraged for production use""
Apr 11 21:18:41 xxx docker[6942]: time=""2016-04-11T21:18:41.953991417+02:00"" level=info msg=""[graphdriver] using prior storage driver \""devicemapper\""""
Apr 11 21:18:41 xxx docker[6942]: time=""2016-04-11T21:18:41.956876689+02:00"" level=info msg=""Option DefaultDriver: bridge""
Apr 11 21:18:41 xxx docker[6942]: time=""2016-04-11T21:18:41.957503648+02:00"" level=info msg=""Option DefaultNetwork: bridge""
Apr 11 21:20:11 xxx systemd[1]: docker.service start operation timed out. Terminating.
Apr 11 21:20:11 xxx systemd[1]: docker.service: main process exited, code=exited, status=2/INVALIDARGUMENT
Apr 11 21:20:11 xxx systemd[1]: Failed to start Docker Application Container Engine.
Apr 11 21:20:11 xxx systemd[1]: Unit docker.service entered failed state.
Apr 11 21:20:11 xxx systemd[1]: docker.service failed.
```
",closed,False,2016-04-11 19:33:39,2018-02-14 18:18:09
contrib,eparis,https://github.com/kubernetes/contrib/pull/758,https://api.github.com/repos/kubernetes/contrib/issues/758,submit-queue: Give information about how long the queue has been blocked,"![sq](https://cloud.githubusercontent.com/assets/8093535/14441739/d462eec4-fffb-11e5-87cc-ba399124e7a6.png)

And how fast it is running...

These number ignore PRs with e2e-not-required and those merged by hand
(since both require almost nothing from the submit queue)

resolves #721 
",closed,True,2016-04-11 20:41:28,2016-04-12 17:41:14
contrib,gmarek,https://github.com/kubernetes/contrib/pull/759,https://api.github.com/repos/kubernetes/contrib/issues/759,Add tests for WeakStable and enable WeakStable logic,"@kubernetes/sig-testing @spxtr 
",closed,True,2016-04-11 22:00:42,2016-04-12 17:40:47
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/760,https://api.github.com/repos/kubernetes/contrib/issues/760,Parse cmdline flags so we get --v,"@aledbf 
",closed,True,2016-04-12 01:29:33,2016-04-12 01:49:41
contrib,adamschaub,https://github.com/kubernetes/contrib/pull/761,https://api.github.com/repos/kubernetes/contrib/issues/761,Implement kube-master HA for multiple masters,"We need to cover HA for kube-master components. Kubernetes HA cluster guide [1]. #725 

User specifies multiple nodes under the master role [2]. Ansible should seamlessly create a cluster with a collection of masters, with api services appropriately load balanced, and other kube-master components leader-elected.
- [x] Enable static etcd clustering (working with #711)
- [x] Deploy kube-apiserver on multiple nodes
- [x] Load balance kube-apiserver instances (e.g. add haproxy/keepalived roles)
- [x] Create controller-manager & scheduler services for each apiserver (leader-elected)

(optional, good to have)
- [x] add in support to run master components in containers (related, #673)

[1] http://kubernetes.io/docs/admin/high-availability/
[2] https://github.com/kubernetes/contrib/blob/master/ansible/inventory.example.ha

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/761)

<!-- Reviewable:end -->
",closed,True,2016-04-12 06:59:02,2018-02-17 12:22:03
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/762,https://api.github.com/repos/kubernetes/contrib/issues/762,Move code that should be maintained to dedicated repos,"contrib isn't monitored adequately for maintained code. Issues aren't triaged. PRs aren't reviewed. We need to fix those problems, but it's not a good idea to mix maintained and unmaintained code. 

Examples:
- addon-resizer
- ingress and service-loadbalancer
- test infra/utils and mungeithub
- do we still need release-notes?
- ansible?

cc @bprashanth @eparis @david-mcmahon @mikedanese @ixdy @Q-Lee 
",open,False,2016-04-12 16:59:44,2018-01-02 20:55:17
contrib,stephenrlouie,https://github.com/kubernetes/contrib/issues/763,https://api.github.com/repos/kubernetes/contrib/issues/763,Machine marked only as an ETCD. Node fails to start flanneld.service ,"- Found via /contrib/ansible/vagrant
- Listing a node in the inventory file just as an etcd node causes a flanneld.service start error
- In the example below node B will fail.

Example Inventory file:

```
[master]
A
[etcd]
B
[nodes]
C
```

-Going to the node it does not have its [Environment File](https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/templates/flanneld.service#L8) : `/etc/sysconfig/flanneld`
- **I believe the ansible role for etcd does not encompass all requirements to properly set up a stand alone etcd node that does not have any other role in the inventory file**

---

Initial Error and debugging output

```
TASK [flannel : Start flannel] *************************************************
fatal: [kube-node-1]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Job for flanneld.service failed because a configured resource limit was exceeded. See \""systemctl status flanneld.service\"" and \""journalctl -xe\"" for details.\n""}
```

_core@kube-node-1 ~ $ systemctl status flanneld.service_

```
core@kube-node-1 ~ $ systemctl status flanneld.service
● flanneld.service - flannel is an etcd backed overlay network for containers
   Loaded: loaded (/etc/systemd/system/flanneld.service; enabled; vendor preset: disabled)
   Active: failed (Result: resources)

Apr 12 16:51:36 kube-node-1 systemd[1]: flanneld.service: Failed to load environment files: No such file or directory
Apr 12 16:51:36 kube-node-1 systemd[1]: flanneld.service: Failed to run 'start' task: No such file or directory
Apr 12 16:51:36 kube-node-1 systemd[1]: Failed to start flannel is an etcd backed overlay network for containers.
Apr 12 16:51:36 kube-node-1 systemd[1]: flanneld.service: Unit entered failed state.
Apr 12 16:51:36 kube-node-1 systemd[1]: flanneld.service: Failed with result 'resources'.
```

_core@kube-node-1 ~ $ journalctl -xe_

```
-- The start-up result is done.
Apr 12 16:51:36 kube-node-1 systemd[1]: Reached target Network is Online.
-- Subject: Unit network-online.target has finished start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit network-online.target has finished starting up.
-- 
-- The start-up result is done.
Apr 12 16:51:36 kube-node-1 systemd[1]: flanneld.service: Failed to load environment files: No such file or directory
Apr 12 16:51:36 kube-node-1 systemd[1]: flanneld.service: Failed to run 'start' task: No such file or directory
Apr 12 16:51:36 kube-node-1 systemd[1]: Failed to start flannel is an etcd backed overlay network for containers.
-- Subject: Unit flanneld.service has failed
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit flanneld.service has failed.
-- 
-- The result is failed.
Apr 12 16:51:36 kube-node-1 systemd[1]: flanneld.service: Unit entered failed state.
Apr 12 16:51:36 kube-node-1 systemd[1]: flanneld.service: Failed with result 'resources'.
Apr 12 16:51:36 kube-node-1 sshd[2969]: Received disconnect from 10.0.2.2: 11: disconnected by user
Apr 12 16:51:36 kube-node-1 sshd[2969]: Disconnected from 10.0.2.2
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:prefs.cc(51)] certificate-report-to-send-update not present in /var/lib/update_engine/prefs
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:prefs.cc(51)] certificate-report-to-send-download not present in /var/lib/update_engine/prefs
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:prefs.cc(51)] aleph-version not present in /var/lib/update_engine/prefs
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:omaha_request_params.cc(60)] Current group set to stable
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:update_attempter.cc(475)] Already updated boot flags. Skipping.
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:update_attempter.cc(658)] Scheduling an action processor start.
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:action_processor.cc(36)] ActionProcessor::StartProcessing: OmahaRequestAction
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:prefs.cc(51)] previous-version not present in /var/lib/update_engine/prefs
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:omaha_request_action.cc(257)] Posting an Omaha request to https://public.update.core-os.net/v1/update/
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:omaha_request_action.cc(258)] Request: <?xml version=""1.0"" encoding=""UTF-8""?>
Apr 12 16:53:05 kube-node-1 update_engine[624]: <request protocol=""3.0"" version=""CoreOSUpdateEngine-0.1.0.0"" updaterversion=""CoreOSUpdateEngine-0.1.0.0"" installsource=""scheduler"" ismachine=""1"">
Apr 12 16:53:05 kube-node-1 update_engine[624]: <os version=""Chateau"" platform=""CoreOS"" sp=""899.15.0_x86_64""></os>
Apr 12 16:53:05 kube-node-1 update_engine[624]: <app appid=""{e96281a6-d1af-4bde-9a0a-97b76e56dc57}"" version=""899.15.0"" track=""stable"" bootid=""{0b51789f-0f65-484b-ba79-dee4f0699586}"" oem=""vagrant"" oemversion=""0.0.3"" alephversion=""8
Apr 12 16:53:05 kube-node-1 update_engine[624]: <ping active=""1""></ping>
Apr 12 16:53:05 kube-node-1 update_engine[624]: <updatecheck></updatecheck>
Apr 12 16:53:05 kube-node-1 update_engine[624]: <event eventtype=""3"" eventresult=""2"" previousversion=""0.0.0.0""></event>
Apr 12 16:53:05 kube-node-1 update_engine[624]: </app>
Apr 12 16:53:05 kube-node-1 update_engine[624]: </request>
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:libcurl_http_fetcher.cc(48)] Starting/Resuming transfer
Apr 12 16:53:05 kube-node-1 locksmithd[999]: LastCheckedTime=0 Progress=0 CurrentOperation=""UPDATE_STATUS_CHECKING_FOR_UPDATE"" NewVersion=0.0.0.0 NewSize=0
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:libcurl_http_fetcher.cc(164)] Setting up curl options for HTTPS
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:libcurl_http_fetcher.cc(427)] Setting up timeout source: 1 seconds.
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:prefs.cc(51)] update-server-cert-0-3 not present in /var/lib/update_engine/prefs
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:prefs.cc(51)] update-server-cert-0-2 not present in /var/lib/update_engine/prefs
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:prefs.cc(51)] update-server-cert-0-1 not present in /var/lib/update_engine/prefs
Apr 12 16:53:05 kube-node-1 update_engine[624]: [0412/165305:INFO:prefs.cc(51)] update-server-cert-0-0 not present in /var/lib/update_engine/prefs
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:libcurl_http_fetcher.cc(240)] HTTP response code: 200
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:libcurl_http_fetcher.cc(297)] Transfer completed (200), 267 bytes downloaded
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:omaha_request_action.cc(574)] Omaha request response: <?xml version=""1.0"" encoding=""UTF-8""?>
Apr 12 16:53:06 kube-node-1 update_engine[624]: <response protocol=""3.0"" server=""update.core-os.net"">
Apr 12 16:53:06 kube-node-1 update_engine[624]: <daystart elapsed_seconds=""0""></daystart>
Apr 12 16:53:06 kube-node-1 update_engine[624]: <app appid=""e96281a6-d1af-4bde-9a0a-97b76e56dc57"" status=""ok"">
Apr 12 16:53:06 kube-node-1 update_engine[624]: <updatecheck status=""noupdate""></updatecheck>
Apr 12 16:53:06 kube-node-1 update_engine[624]: </app>
Apr 12 16:53:06 kube-node-1 update_engine[624]: </response>
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:omaha_request_action.cc(394)] No update.
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:action_processor.cc(82)] ActionProcessor::ActionComplete: finished OmahaRequestAction, starting OmahaResponseHandlerAction
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:omaha_response_handler_action.cc(36)] There are no updates. Aborting.
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:action_processor.cc(68)] ActionProcessor::ActionComplete: OmahaResponseHandlerAction action failed. Aborting processing.
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:action_processor.cc(73)] ActionProcessor::ActionComplete: finished last action of type OmahaResponseHandlerAction
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:update_attempter.cc(283)] Processing Done.
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:update_attempter.cc(319)] No update.
Apr 12 16:53:06 kube-node-1 update_engine[624]: [0412/165306:INFO:update_check_scheduler.cc(82)] Next update check in 45m29s
Apr 12 16:53:06 kube-node-1 locksmithd[999]: LastCheckedTime=0 Progress=0 CurrentOperation=""UPDATE_STATUS_IDLE"" NewVersion=0.0.0.0 NewSize=0
Apr 12 17:03:41 kube-node-1 systemd[1]: Starting Cleanup of Temporary Directories...
-- Subject: Unit systemd-tmpfiles-clean.service has begun start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-tmpfiles-clean.service has begun starting up.
Apr 12 17:03:41 kube-node-1 systemd-tmpfiles[2993]: [/usr/lib64/tmpfiles.d/home.conf:10] Duplicate line for path ""/home"", ignoring.
Apr 12 17:03:41 kube-node-1 systemd-tmpfiles[2993]: [/usr/lib64/tmpfiles.d/home.conf:11] Duplicate line for path ""/srv"", ignoring.
Apr 12 17:03:41 kube-node-1 systemd[1]: Started Cleanup of Temporary Directories.
-- Subject: Unit systemd-tmpfiles-clean.service has finished start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-tmpfiles-clean.service has finished starting up.
-- 
-- The start-up result is done.
Apr 12 17:05:36 kube-node-1 systemd[1]: Started OpenSSH per-connection server daemon (10.0.2.2:56853).
-- Subject: Unit sshd@190-10.0.2.15:22-10.0.2.2:56853.service has finished start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sshd@190-10.0.2.15:22-10.0.2.2:56853.service has finished starting up.
-- 
-- The start-up result is done.
Apr 12 17:05:36 kube-node-1 sshd[2996]: Accepted publickey for core from 10.0.2.2 port 56853 ssh2: RSA SHA256:1M4RzhMyWuFS/86uPY/ce2prh/dVTHW7iD2RhpquOZA
Apr 12 17:06:18 kube-node-1 polkitd[1892]: Registered Authentication Agent for unix-process:3009:106541 (system bus name :1.39 [/usr/bin/pkttyagent --notify-fd 5 --fallback], object path /org/freedesktop/PolicyKit1/AuthenticationA
Apr 12 17:06:18 kube-node-1 polkitd[1892]: Unregistered Authentication Agent for unix-process:3009:106541 (system bus name :1.39, object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale en_US.UTF-8) (disconnected from 
Apr 12 17:06:21 kube-node-1 sudo[3014]:     core : TTY=pts/0 ; PWD=/home/core ; USER=root ; COMMAND=/bin/systemctl start flanneld.service
Apr 12 17:06:21 kube-node-1 polkitd[1892]: Registered Authentication Agent for unix-process:3014:106831 (system bus name :1.41 [/usr/bin/pkttyagent --notify-fd 5 --fallback], object path /org/freedesktop/PolicyKit1/AuthenticationA
Apr 12 17:06:21 kube-node-1 systemd[1]: flanneld.service: Failed to load environment files: No such file or directory
Apr 12 17:06:21 kube-node-1 systemd[1]: flanneld.service: Failed to run 'start' task: No such file or directory
Apr 12 17:06:21 kube-node-1 systemd[1]: Failed to start flannel is an etcd backed overlay network for containers.
-- Subject: Unit flanneld.service has failed
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit flanneld.service has failed.
-- 
-- The result is failed.
Apr 12 17:06:21 kube-node-1 systemd[1]: flanneld.service: Failed with result 'resources'.
Apr 12 17:06:21 kube-node-1 polkitd[1892]: Unregistered Authentication Agent for unix-process:3014:106831 (system bus name :1.41, object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale en_US.UTF-8) (disconnected from
```
",closed,False,2016-04-12 17:15:04,2016-04-14 16:21:42
contrib,lindison,https://github.com/kubernetes/contrib/issues/764,https://api.github.com/repos/kubernetes/contrib/issues/764,update: python-netaddr to netaddr (in README.md),"`sudo pip install python-netaddr`

should be

`sudo pip install netaddr`

source: https://pythonhosted.org/netaddr/installation.html

python-netaddr

`14:57 $ sudo pip install python-netaddr
Password:
The directory '/Users/Lindis/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/Users/Lindis/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Collecting python-netaddr
  Could not find a version that satisfies the requirement python-netaddr (from versions: )
No matching distribution found for python-netaddr
✘-1 ~ [develop L|●577✚ 1]`

netaddr

`14:57 $ sudo pip install netaddr
The directory '/Users/Lindis/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/Users/Lindis/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Collecting netaddr
  Downloading netaddr-0.7.18-py2.py3-none-any.whl (1.5MB)
    100% |████████████████████████████████| 1.5MB 921kB/s
Installing collected packages: netaddr
Successfully installed netaddr-0.7.18
✔ ~ [develop L|●577✚ 1]
14:57 $ uname -a
Darwin longfellowdeeds.local 15.3.0 Darwin Kernel Version 15.3.0: Thu Dec 10 18:40:58 PST 2015; root:xnu-3248.30.4~1/RELEASE_X86_64 x86_64
✔ ~ [develop L|●577✚ 1]`
",closed,False,2016-04-12 22:00:59,2016-07-29 13:33:01
contrib,eparis,https://github.com/kubernetes/contrib/pull/765,https://api.github.com/repos/kubernetes/contrib/issues/765,mungegithub: delete bot comments when they are no longer useful,"Almost all of the bot's comments serve a purpose that can be resolved.
Once they are resolved the comments no longer need to be present and
just make review more difficult.

This PR removes the comments created by this bot. It does NOT remove
comments created by the cherrypick or the k8s-bot.

Resolves: https://github.com/kubernetes/kubernetes/issues/24007
",closed,True,2016-04-13 00:02:44,2016-04-14 16:38:50
contrib,aledbf,https://github.com/kubernetes/contrib/pull/766,https://api.github.com/repos/kubernetes/contrib/issues/766,"Add support for named port, better docs for TLS nginx Ingress","fixes #743
fixes #781
fixes #858
fixes #871
closes #872
fixes #876
",closed,True,2016-04-13 02:20:39,2016-05-03 23:29:42
contrib,jlebon,https://github.com/kubernetes/contrib/pull/767,https://api.github.com/repos/kubernetes/contrib/issues/767,flannel/tasks/upstart-service.yml: fix indentation,"Otherwise ansible crashes with the singular cryptic message:

> ERROR: that is not a legal parameter in an Ansible task or handler
",closed,True,2016-04-13 02:48:14,2016-04-13 14:45:39
contrib,rutsky,https://github.com/kubernetes/contrib/pull/768,https://api.github.com/repos/kubernetes/contrib/issues/768,fix indentation,"Previously on some versions of Ansible this was reported as following error:

```
ERROR: that is not a legal parameter in an Ansible task or handler
```

(however my Ansible 2.0.0.2 was silent).

/cc @stephenrlouie 
",closed,True,2016-04-13 04:47:40,2016-04-13 13:51:26
contrib,mattthias,https://github.com/kubernetes/contrib/pull/769,https://api.github.com/repos/kubernetes/contrib/issues/769,Fix broken links in fluentd-sidecar-* docs,"That pr fixes two broken links in the logging/fluentd-sidecar-[gcp | es]/README.md by removing a superflous ""/contrib/""  in the path.
",closed,True,2016-04-13 08:39:53,2016-07-08 16:24:49
contrib,aknuds1,https://github.com/kubernetes/contrib/pull/770,https://api.github.com/repos/kubernetes/contrib/issues/770,Add Terraform on GCE implementation,"**Early attempt** to implement Kubernetes deployment on GCE via Terraform.

Creating PR to solicit feedback and hopefully contributions!

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/770)

<!-- Reviewable:end -->
",closed,True,2016-04-13 14:18:39,2016-09-08 03:03:36
contrib,gmarek,https://github.com/kubernetes/contrib/pull/771,https://api.github.com/repos/kubernetes/contrib/issues/771,Test-enable weak stable check for kubemark-500,"cc @wojtek-t @kubernetes/sig-testing 
",closed,True,2016-04-13 14:22:34,2016-04-14 16:22:01
contrib,jlebon,https://github.com/kubernetes/contrib/pull/772,https://api.github.com/repos/kubernetes/contrib/issues/772,pre-ansible tasks: avoid python to probe for Atomic,"It might not be installed yet. The following task will install it.
",closed,True,2016-04-13 14:29:26,2016-04-13 14:47:30
contrib,stephenrlouie,https://github.com/kubernetes/contrib/issues/773,https://api.github.com/repos/kubernetes/contrib/issues/773,"flannel_use_upstart = ""False"" instead of false on ansible version 1.9.4","In Ansible 1.9.4, [flannel_use_upstart](https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/tasks/client.yml#L17) is set to ""False""

```
TASK: [flannel | set_fact ] *************************************************** 
<192.168.2.3> ESTABLISH CONNECTION FOR USER: core
ok: [192.168.2.3] => {""ansible_facts"": {""flannel_use_upstart"": ""False""}}
<192.168.2.4> ESTABLISH CONNECTION FOR USER: core
ok: [192.168.2.4] => {""ansible_facts"": {""flannel_use_upstart"": ""False""}}
<192.168.2.5> ESTABLISH CONNECTION FOR USER: core
ok: [192.168.2.5] => {""ansible_facts"": {""flannel_use_upstart"": ""False""}}
```

In Ansible 2.0.1 [flannel_use_upstart](https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/tasks/client.yml#L17) is set to false

```
TASK [flannel : set_fact] ******************************************************
task path: /Users/stelouie/Documents/RIO/contrib/ansible/roles/flannel/tasks/client.yml:16
ok: [kube-master] => {""ansible_facts"": {""flannel_use_upstart"": false}, ""changed"": false, ""invocation"": {""module_args"": {""flannel_use_upstart"": false}, ""module_name"": ""set_fact""}}
ok: [kube-node-1] => {""ansible_facts"": {""flannel_use_upstart"": false}, ""changed"": false, ""invocation"": {""module_args"": {""flannel_use_upstart"": false}, ""module_name"": ""set_fact""}}
ok: [kube-node-2] => {""ansible_facts"": {""flannel_use_upstart"": false}, ""changed"": false, ""invocation"": {""module_args"": {""flannel_use_upstart"": false}, ""module_name"": ""set_fact""}}
```
- This causes a CoreOS install using ansible 1.9.4 to skip [systemd-service.yml](https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/tasks/systemd-service.yml) instead of running it like it should.

_Possible Fix_
- Require ansible 2.0.1
- Sloppily check for the string values [here](https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/tasks/github-release.yml#L39)

cc @rutsky @danehans 
## Edit

It attempts both _flannel_use_upstart_ and _not flannel_use_upstart_. I don't imagine that this is desired. Maybe connected to this https://github.com/ansible/ansible/issues/8629

[systemd-service.yml](https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/tasks/systemd-service.yml)
[upstart-service.yml](https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/tasks/upstart-service.yml)

```
TASK: [flannel | assert ] ***************************************************** 
skipping: [192.168.2.3]
skipping: [192.168.2.4]
skipping: [192.168.2.5]

TASK: [flannel | Add Flanneld Systemd Unit File] ****************************** 
skipping: [192.168.2.4]
skipping: [192.168.2.5]
skipping: [192.168.2.3]

TASK: [flannel | Create Docker systemd dropin directory] ********************** 
skipping: [192.168.2.3]
skipping: [192.168.2.5]
skipping: [192.168.2.4]

TASK: [flannel | Add Docker drop-in with dependency on Flannel] *************** 
skipping: [192.168.2.3]
skipping: [192.168.2.5]
skipping: [192.168.2.4]

TASK: [flannel | assert ] ***************************************************** 
skipping: [192.168.2.3]
skipping: [192.168.2.5]
skipping: [192.168.2.4]

TASK: [flannel | Create Upstart Script] *************************************** 
skipping: [192.168.2.4]
skipping: [192.168.2.3]
skipping: [192.168.2.5]
```
",closed,False,2016-04-13 16:11:58,2018-02-14 00:00:10
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/774,https://api.github.com/repos/kubernetes/contrib/issues/774,Add a multi-tls example,"@aledbf 
",closed,True,2016-04-13 17:10:23,2016-04-13 17:35:36
contrib,aledbf,https://github.com/kubernetes/contrib/issues/775,https://api.github.com/repos/kubernetes/contrib/issues/775,Ingress controllers should handle edge cases uniformly x-platform,"#### Ideas:
- [ ] root URL (mapping to a different url in the upstream)
  - [ ] global per controlller `—root-url`
  - [ ] per Ingress rule using an annotation `root-url`. This means the ingress only contains one rule
- [ ] upstreams to external resources (like /google proxy to http://www.google.cl)

```
kind: Ingress
metadata:
  name: external-ingress
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: external:www.google.cl
          servicePort: 80
        path: /google
```
- [X] rules that overlap (firt rule wins, event with warning) [766](https://github.com/kubernetes/contrib/pull/766)

```
$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
demo      -                       172.17.4.99
          foo.bar.com
          /             echoheaders-x:80
foo-tls   -                       172.17.4.99
          foo.bar.com
          /             echoheaders-x:80
          bar.baz.com
          /             echoheaders-y:80
```
#### NGINX controller e2e tests:
- [ ] no default backend
- [ ] invalid default backend
- [ ] no ingress
- [ ] single ingress:
  - rule
    - [ ] invalid service
    - [ ] invalid port
    - [ ] no endpoints
    - [ ] no host (it should use the _ nginx server)
    - [ ] without path
- [ ] multiple ingress:
  - rule
    - [ ] invalid service
    - [ ] invalid port
    - [ ] no endpoints
    - [ ] with same path (error or just a warning?)
    - [ ] without path
- [ ] TLS rules:
  - [ ] invalid certs
  - [ ] invalid hosts (fails verification)
  - [ ] invalid service
  - [ ] invalid port
  - [ ] no endpoints
  - [ ] no host (it should use the _ nginx server)
- [ ] tcp configmap
  - [ ] invalid service
  - [ ] invalid port
  - [ ] no endpoints
- [ ] custom nginx configmap
  - [ ] change timeouts
  - [ ] invalid fields
",closed,False,2016-04-13 18:26:29,2016-11-10 19:39:49
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/776,https://api.github.com/repos/kubernetes/contrib/issues/776,Fix for setting the upstart fact for ansible prior to 2.0.2,"Addresses: #773
- Caused by some upstream ansible bugs around converting strings to booleans. Listed in the issue.
- Tested on CoreOS + VirtualBox + Vagrant and Centos + Virtualbox +Vagrant 
",closed,True,2016-04-13 18:57:17,2016-04-14 11:51:38
contrib,jasonbrooks,https://github.com/kubernetes/contrib/issues/777,https://api.github.com/repos/kubernetes/contrib/issues/777,[ansible] kube-addons service not creating kube-system namespace,"Something's amiss w/ the [kube-addons script](https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/saltbase/salt/kube-addons/kube-addons.sh) that the `kube-addons.service` calls, in the part that creates the kube-system namespace. It seems to boil down to a problem with `kubectl apply -f`. `kubectl create -f` works, but `apply` doesn't:

```
[root@centos-1 ~]# cat /etc/kubernetes/addons/namespace.yaml | /usr/bin/kubectl create -f -
namespace ""kube-system"" created

[root@centos-1 ~]# kubectl delete namespace kube-system
namespace ""kube-system"" deleted

[root@centos-1 ~]# cat /etc/kubernetes/addons/namespace.yaml | /usr/bin/kubectl apply -f -
Error from server: error when retrieving current configuration of:
&{0xc2082dcbe0 0xc208101b20  kube-system STDIN 0xc208640a50 0xc208640780 }
from server for: ""STDIN"": namespaces ""kube-system"" not found
```

Simply changing `apply` to `create` in the script works, but the change from `create` to `apply` was explicitly made upstream: https://github.com/kubernetes/kubernetes/pull/23612.
",closed,False,2016-04-13 19:08:24,2016-04-14 18:49:38
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/778,https://api.github.com/repos/kubernetes/contrib/issues/778,removed etcd from flannel requirements,"- Fixes: #763 
- Single etcd machines can now be deployed, tested and confirmed single etcd, HA etcd on CoreOS and CentOS
",closed,True,2016-04-13 20:43:20,2016-04-14 16:21:42
contrib,adamschaub,https://github.com/kubernetes/contrib/pull/779,https://api.github.com/repos/kubernetes/contrib/issues/779,"Uninstall: Removed service configuration files, cleaned up flannel files","Includes miscellaneous fixes for coreos uninstall.

/cc @danehans @eparis 
",closed,True,2016-04-13 20:45:26,2016-04-14 16:23:15
contrib,jdavis7257,https://github.com/kubernetes/contrib/issues/780,https://api.github.com/repos/kubernetes/contrib/issues/780,[service-loadbalancer] Service load balancer SSL termination should rewrite URLs,"The service load balancer should rewrite URLs to root based on the AclMatch annotation that is used to setup SSL termination. The template for the httpsTerm backend template doesn't contain the reqrep that the http backend does. This causes requests to be sent to the backend server on whatever path is in the acl instead of root. 

Adding `reqrep ^([^\ :]*)\ /{{$svc.AclMatch}}[/]?(.*) \1\ /\2`
to the backend for httpsTerm within the template should resolve this issue and make https termination behave the same as http loadbalancing.
",closed,False,2016-04-14 01:35:07,2018-01-14 19:16:58
contrib,iSOcH,https://github.com/kubernetes/contrib/issues/781,https://api.github.com/repos/kubernetes/contrib/issues/781,Nginx Ingress Controller problem with named targetPort in Service,"service.yaml:

```
apiVersion: v1
kind: Service
metadata:
  name: kibana-logging
  namespace: kube-system
  labels:
    k8s-app: kibana-logging
    kubernetes.io/cluster-service: ""true""
    kubernetes.io/name: ""Kibana""
spec:
  ports:
  - port: 5601
    protocol: TCP
    targetPort: 5601
  selector:
    k8s-app: kibana-logging
```

with ingress.yaml:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kibana-logging
  namespace: kube-system
spec:
  rules:
  - host: phive-logs.pmi.ag
    http:
      paths:
      - backend:
          serviceName: kibana-logging
          servicePort: 5601
        path: /
```

works flawlessly :)

but if i set `targetPort: ui` (as it is named in the pod definition) nginx ingress controller logs: `service kube-system/kibana-logging does no have any active endpoints` ... but:

```
$ kubectl get ep --namespace=kube-system kibana-logging
NAME             ENDPOINTS          AGE
kibana-logging   10.103.11.3:5601   4h
```
",closed,False,2016-04-14 12:34:02,2016-05-02 16:01:56
contrib,sebgoa,https://github.com/kubernetes/contrib/issues/782,https://api.github.com/repos/kubernetes/contrib/issues/782,Getting 301 error during tests,"Hi, I am doing a fresh test of this.

I deployed all the components with:

```
KUBE_ROOT=""https://raw.githubusercontent.com/kubernetes/contrib/master/ingress/controllers/nginx""

kubectl create -f ""${KUBE_ROOT}/examples/default-backend.yaml""
kubectl expose rc default-http-backend --port=80 --target-port=8080 --name=default-http-backend
kubectl create -f ""${KUBE_ROOT}/examples/default/rc-default.yaml""
kubectl run echoheaders --image=gcr.io/google_containers/echoserver:1.3 --replicas=1 --port=8080
kubectl expose deployment echoheaders --port=80 --target-port=8080 --name=echoheaders-x
kubectl expose deployment echoheaders --port=80 --target-port=8080 --name=echoheaders-y
kubectl create -f ""${KUBE_ROOT}/examples/ingress.yaml""
```

Everything seems to run, the ingress rule is there

```
$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS        AGE
echomap   -                       45.55.233.82   48m
          foo.bar.com   
          /foo          echoheaders-x:80
          bar.baz.com   
          /bar          echoheaders-y:80
          /foo          echoheaders-x:80
```

But I am getting 301 errors:

```
root@foobar:~# curl -v http://45.55.233.82:80/foo -H 'Host: foo.bar.com'
* Hostname was NOT found in DNS cache
*   Trying 45.55.233.82...
* Connected to 45.55.233.82 (45.55.233.82) port 80 (#0)
> GET /foo HTTP/1.1
> User-Agent: curl/7.35.0
> Accept: */*
> Host: foo.bar.com
> 
< HTTP/1.1 301 Moved Permanently
* Server nginx/1.9.13 is not blacklisted
< Server: nginx/1.9.13
< Date: Thu, 14 Apr 2016 21:43:04 GMT
< Content-Type: text/html
< Content-Length: 185
< Connection: keep-alive
< Location: https://foo.bar.com/foo
< 
<html>
<head><title>301 Moved Permanently</title></head>
<body bgcolor=""white"">
<center><h1>301 Moved Permanently</h1></center>
<hr><center>nginx/1.9.11</center>
</body>
</html>
* Connection #0 to host 45.55.233.82 left intact
```

The echo headers app seem to be working fine.

```
root@foobar:~# curl 10.0.0.87:80
CLIENT VALUES:
client_address=45.55.233.82
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://10.0.0.87:8080/

SERVER VALUES:
server_version=nginx: 1.9.11 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=10.0.0.87
user-agent=curl/7.35.0
BODY:
-no body in request-
```

Not sure where to look, the nginx conf file on the ingress controller seems to be fine.
",closed,False,2016-04-14 21:49:05,2016-04-18 17:11:21
contrib,danehans,https://github.com/kubernetes/contrib/pull/783,https://api.github.com/repos/kubernetes/contrib/issues/783,[ansible] Adds Support for Contiv,"Supercedes https://github.com/kubernetes/contrib/pull/625

This PR brings in ansible changes needed to setup Kubernetes
with Contiv networking as outlined in:

https://github.com/kubernetes/contrib/issues/577

Change Summary:

1) Added the contiv role that holds all contiv specific config code.
2) Expose minimal contiv configuration options in group_vars/all
3) Updates ansible/cluster.yml to expose contiv role.
4) Updates etcd to support legacy (:4001/:7001) ports and adds
   proxy support.
5) Added missing ""validate_certs: False"" for a couple of addon
   file fetches.
",closed,True,2016-04-15 06:54:24,2016-04-20 18:22:16
contrib,nottix,https://github.com/kubernetes/contrib/issues/784,https://api.github.com/repos/kubernetes/contrib/issues/784,HTTPS automatic redirect,"Hi,

i'm using the new TLS feature on 2 of our domains, is it possibile to disable automatic redirect from HTTP to HTTPS? 

Thank you.
",closed,False,2016-04-15 12:12:26,2016-06-29 17:18:14
contrib,ApsOps,https://github.com/kubernetes/contrib/issues/785,https://api.github.com/repos/kubernetes/contrib/issues/785,addon-resizer pod_nanny doesn't work in kubernetes cluster with disabled auth,"I run a kubernetes cluster without `ServiceAccount` in `admission-control` flag so it doesn't have any secrets available. Hence, `pod_nanny` fails to start with the following error:

```
F0415 12:05:06.053612 1 pod_nanny.go:73] open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
```

I was facing same issue with heapster, which I could resolve by using `?inClusterConfig=false` in source url as per [these instructions](https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md#kubernetes).

I believe the issue is at [this line](https://github.com/kubernetes/contrib/blob/master/addon-resizer/nanny/main/pod_nanny.go#L71) since instantiating the client with `InClusterConfig`.

There should be an option to run `pod_nanny` without auth.

cc: @Q-Lee 
",closed,False,2016-04-15 12:30:14,2018-02-20 12:33:01
contrib,jdavis7257,https://github.com/kubernetes/contrib/pull/786,https://api.github.com/repos/kubernetes/contrib/issues/786,Proposed change to https load balancing template,"This change should make https load balancing act just like http services with only the sslTerm annotation set. It still retains the option to specify an AclMatch which will not rewrite the url as we can't guarantee that the acl will be a valid path.
",closed,True,2016-04-17 02:26:45,2016-04-17 02:29:51
contrib,jdavis7257,https://github.com/kubernetes/contrib/pull/787,https://api.github.com/repos/kubernetes/contrib/issues/787,SSL termination URL rewritting changes,"These changes will allow ssl termination to work without an AclMatch and it will act just like the http services. If the user wants they can specify an AclMatch which will make the load balancer act the same as it does currently.
",closed,True,2016-04-17 02:56:49,2016-04-22 05:02:01
contrib,donbeave,https://github.com/kubernetes/contrib/issues/788,https://api.github.com/repos/kubernetes/contrib/issues/788,[service-loadbalancer] Error Get http://localhost:1936: dial tcp: lookup localhost on 8.8.8.8:53: no such host,"Kubernetes: 1.2.1
service-loadbalancer: gcr.io/google_containers/servicelb:0.4

I am getting this error every time when trying to add or delete new service:

```
I0417 09:25:33.638085       6 service_loadbalancer.go:615] Error Get http://localhost:1936: dial tcp: lookup localhost on 8.8.8.8:53: no such host
```

rc.yaml:

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: service-loadbalancer
  labels:
    app: service-loadbalancer
    version: v1
spec:
  replicas: 1
  selector:
    app: service-loadbalancer
    version: v1
  template:
    metadata:
      labels:
        app: service-loadbalancer
        version: v1
    spec:
      nodeSelector:
        role: loadbalancer
      containers:
      - image: gcr.io/google_containers/servicelb:0.4
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            host: 127.0.0.1
            path: /healthz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        name: haproxy
        ports:
        # All http services
        - containerPort: 80
          hostPort: 80
          protocol: TCP
        # haproxy stats
        - containerPort: 1936
          hostPort: 1936
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
      dnsPolicy: ClusterFirst
      hostNetwork: true
      restartPolicy: Always
      securityContext: {}
      terminationGracePeriodSeconds: 60
```
",closed,False,2016-04-17 09:59:44,2018-02-13 08:45:09
contrib,andrewmichaelsmith,https://github.com/kubernetes/contrib/issues/789,https://api.github.com/repos/kubernetes/contrib/issues/789,nginx-ingress-controller:0.5 build from forked repo,"When I run nginx-ingress-controller:0.5 the log line says it comes from anothr repo (bprashanth/contrib) which hasn't been updated in 8 months. This seems like it might be wrong? But I'm not sure how I can check what images are supposed to be built where.

```
$ docker run -it gcr.io/google_containers/nginx-ingress-controller:0.5
I0417 15:40:14.852814       1 main.go:94] Using build: https://github.com/bprashanth/contrib.git - git-7fbd252
```
",closed,False,2016-04-17 15:41:38,2016-07-02 03:08:09
contrib,alanhartless,https://github.com/kubernetes/contrib/issues/790,https://api.github.com/repos/kubernetes/contrib/issues/790,Ingress nginx controller Readme references a --custom-error-service flag but doesn't exist,"See https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx#custom-errors. When I use the suggested `--custom-error-service` flag, the pod fails to deploy with `unknown flag: --custom-error-service`.
",closed,False,2016-04-17 20:06:10,2016-07-02 03:08:34
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/791,https://api.github.com/repos/kubernetes/contrib/issues/791,"Create firewall rule, update certs when secrets are updated","Fixes #1 on https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md.

Also allows certificate update when the Kubernetes secret changes, and contains changes in preperation for running the controller on the master.

Assigning to Minhan, @kubernetes/goog-cluster fyi
",closed,True,2016-04-18 01:22:31,2017-06-20 19:54:51
contrib,danehans,https://github.com/kubernetes/contrib/issues/792,https://api.github.com/repos/kubernetes/contrib/issues/792,[ansible] Add Ansible Galaxy Support,"Adding support for Ansible Galaxy [1] will make it easy to find, reuse and share contrib/ansible roles. The contrib/ansible roles can be bundled using librarian-ansible [2].

[1] https://galaxy.ansible.com/
[2] https://github.com/bcoe/librarian-ansible
",closed,False,2016-04-18 03:21:17,2018-02-14 00:00:11
contrib,danehans,https://github.com/kubernetes/contrib/issues/793,https://api.github.com/repos/kubernetes/contrib/issues/793,[ansible] Allow Roles to be loosely coupled,"For the most part, roles within contrib/ansible are tightly coupled. For example, I can not easily swap out the contrib/ansible etcd role with a different ectd role from a upstream such as Ansible Galaxy [1].

[1] https://galaxy.ansible.com/
",closed,False,2016-04-18 03:27:14,2018-02-14 00:00:11
contrib,thockin,https://github.com/kubernetes/contrib/pull/794,https://api.github.com/repos/kubernetes/contrib/issues/794,Demo UX cleanups,,closed,True,2016-04-18 04:55:51,2016-04-21 04:06:42
contrib,piosz,https://github.com/kubernetes/contrib/pull/795,https://api.github.com/repos/kubernetes/contrib/issues/795,Added placeholder for Cluster Autoscaler,,closed,True,2016-04-18 10:12:06,2016-04-18 10:15:49
contrib,piosz,https://github.com/kubernetes/contrib/pull/796,https://api.github.com/repos/kubernetes/contrib/issues/796,Library to operate on MIGs in GCE for Cluster Autoscaler,"ref https://github.com/kubernetes/kubernetes/issues/24404
",closed,True,2016-04-18 15:35:36,2016-04-22 08:17:12
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/797,https://api.github.com/repos/kubernetes/contrib/issues/797,add HOME env variable for kube-addons service,"Implements upstream change https://github.com/kubernetes/kubernetes/pull/23975, which fixes https://github.com/kubernetes/kubernetes/issues/23973.
",closed,True,2016-04-18 20:09:25,2016-05-24 13:18:36
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/798,https://api.github.com/repos/kubernetes/contrib/issues/798,Fix the perfdash,"@gmarek I'm sorry.
It turns out that I forgot to update the health check endpoint in the pod.yaml.
",closed,True,2016-04-18 22:21:03,2016-04-18 22:28:35
contrib,alanhartless,https://github.com/kubernetes/contrib/issues/799,https://api.github.com/repos/kubernetes/contrib/issues/799,Nginx Ingress Controller - Secure default backend and/or support for wildcards?,"Currently it's not possible to support wildcard subdomains for the Nginx ingress controller due to Kubernetes Ingress resources only allowing a legitimate domain for host. In the scenario that a wildcard domain SSL cert needs to be used for a lot of subdomains (think SaaS with custom subdomain names), it's a bit overkill to list every subdomain. 

I thought of two solutions. 

One - support via a flag to secure the default backend server with the default SSL certificate where the backend server can be something more than just a simple 404. This of course limits to a single cert and wouldn't be able to support multiple wildcard domains. 

Two - use a flag with a ""wildcard"" placeholder that's replaced with `*` when generating the hosts. The placeholder can be a little funny looking since the host is limited to alphanumeric keys (like `0wildcard0`, `wildcard-` or just `wildcard` if for sure not being used. 

I've implemented both for my uses but wanted to see if either (or both) would be something interested in  before I submit a PR (they feel a little hackish :-) ). Thoughts?

It would be GREAT if paths would support Nginx location regex but I know that's an Ingress limitation. 
",closed,False,2016-04-18 23:56:56,2017-04-04 15:15:57
contrib,alanhartless,https://github.com/kubernetes/contrib/issues/800,https://api.github.com/repos/kubernetes/contrib/issues/800,Nginx Ingress Controller - Config file not updated when a secret is updated,"It seems that updating a secret does not trigger the config file to be updated. I have a script that is automating let's encrypt. But after the certificate is fetched and the Secret resource updated, the nginx configuration is not rebuilt (so hosts waiting on the certificate are updated). Is this intentional?
",closed,False,2016-04-19 02:42:45,2016-05-26 03:46:03
contrib,gitpveck,https://github.com/kubernetes/contrib/issues/801,https://api.github.com/repos/kubernetes/contrib/issues/801,[ansible] remove enablerepo pkgMgrInstallers/fedora-install.yml,"enablerepo is not applicable in the Fedora | Install kubernetes master and Fedora | Install kubernetes node  tasks. It causes the play on fedora hosts to fail.
",closed,False,2016-04-19 09:15:23,2016-09-07 21:29:44
contrib,Ladicle,https://github.com/kubernetes/contrib/pull/802,https://api.github.com/repos/kubernetes/contrib/issues/802,Support the github-release type of ansible for Ubuntu 14.04.,"Already, this playbook supported localBuildinstall for Ubuntu 14.04. 
But it isn't available the github-rerease of source type.

In addition, Fixed minor bugs of configuration file for upstart that were found during the deployment.
",closed,True,2016-04-19 10:11:03,2016-05-26 16:56:22
contrib,Ladicle,https://github.com/kubernetes/contrib/pull/803,https://api.github.com/repos/kubernetes/contrib/issues/803,Move task to copy kube-gen-token script,"Task to copy kube-gen-token script is in secrets.yml.
However, secrets.yml is called only the first node of master.

The `kube-gen-token.sh` is also used in main.yml of kubernetes-addons.
Fail to deploy HA Cluster, in the second or later master node that does not have a script.
",closed,True,2016-04-19 10:36:59,2016-05-27 00:18:57
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/804,https://api.github.com/repos/kubernetes/contrib/issues/804,Cluster autoscaler make and main,,closed,True,2016-04-19 11:56:32,2016-04-19 13:06:07
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/805,https://api.github.com/repos/kubernetes/contrib/issues/805,Configuration flags for cluster autoscaler,"https://github.com/kubernetes/kubernetes/issues/24404
",closed,True,2016-04-19 13:45:42,2016-04-19 15:32:56
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/806,https://api.github.com/repos/kubernetes/contrib/issues/806,Add support for centos & fedora atomic and regular fedora to Vagrantfile,,closed,True,2016-04-19 16:14:18,2016-05-09 15:04:37
contrib,arni-inaba,https://github.com/kubernetes/contrib/pull/807,https://api.github.com/repos/kubernetes/contrib/issues/807,fix readme link to fluentd Elasticsearch sidecar,,closed,True,2016-04-19 16:16:16,2016-05-26 16:56:07
contrib,ryanschwartz,https://github.com/kubernetes/contrib/issues/808,https://api.github.com/repos/kubernetes/contrib/issues/808,[keepalived-vip] unicast_peer should only include VRRP peers,"With multi-node clusters, setting a label for restriction by nodeSelector in the daemonset spec and specifying `--use-unicast=true`, all service nodes are listed as unicast peers, rather than the subset of nodes that are specified by the nodeSelector.

```
 ✓ ( 11:27:05 ) ⦿ ryan@juniper
 ▶︎ ~/repos/vst/contrib/keepalived-vip ▶︎  master ● ▶︎ kubectl get no --show-labels
NAME            STATUS                     AGE       LABELS
10.10.34.71     Ready                      3d        kubernetes.io/hostname=10.10.34.71,type=vip
10.10.34.72     Ready                      3d        kubernetes.io/hostname=10.10.34.72,type=vip
10.10.34.73     Ready                      3d        kubernetes.io/hostname=10.10.34.73,type=vip
10.10.34.74     Ready                      3d        kubernetes.io/hostname=10.10.34.74
10.10.34.75     Ready                      3d        kubernetes.io/hostname=10.10.34.75
10.10.34.76     Ready                      3d        kubernetes.io/hostname=10.10.34.76
172.21.10.162   Ready,SchedulingDisabled   3d        kubernetes.io/hostname=172.21.10.162
 ✓ ( 11:27:12 ) ⦿ ryan@juniper
 ▶︎ ~/repos/vst/contrib/keepalived-vip ▶︎  master ● ▶︎ grep -A2 nodeSelector vip-daemonset.yaml
      nodeSelector:
        type: vip
 ✓ ( 11:27:29 ) ⦿ ryan@juniper
 ▶︎ ~/repos/vst/contrib/keepalived-vip ▶︎  master ● ▶︎ kubectl get pods
NAME                        READY     STATUS    RESTARTS   AGE       NODE
hostnames-1ni2l             1/1       Running   0          1h        10.10.34.74
hostnames-6rrss             1/1       Running   0          1h        10.10.34.73
hostnames-fmt0n             1/1       Running   0          1h        10.10.34.76
hostnames-lyb6w             1/1       Running   0          1h        10.10.34.71
hostnames-qo1xu             1/1       Running   0          1h        10.10.34.72
hostnames-zn7cn             1/1       Running   0          1h        10.10.34.75
kube-keepalived-vip-1hpkc   1/1       Running   0          53m       10.10.34.72
kube-keepalived-vip-75q8d   1/1       Running   0          53m       10.10.34.71
kube-keepalived-vip-t1egp   1/1       Running   0          53m       10.10.34.73
 ✓ ( 11:27:48 ) ⦿ ryan@juniper
 ▶︎ ~/repos/vst/contrib/keepalived-vip ▶︎  master ● ▶︎ kubectl exec kube-keepalived-vip-1hpkc cat /etc/keepalived/keepalived.conf


global_defs {
  vrrp_version 3
  vrrp_iptables KUBE-KEEPALIVED-VIP
}

vrrp_instance vips {
  state BACKUP
  interface bond0
  virtual_router_id 50
  priority 101
  nopreempt
  advert_int 1

  track_interface {
    bond0
  }


  unicast_src_ip 10.10.34.72
  unicast_peer {
    10.10.34.71
    10.10.34.73
    10.10.34.74
    10.10.34.75
    10.10.34.76
    172.21.10.162
  }
<SNIP>
}
```
",closed,False,2016-04-19 16:45:36,2016-05-08 21:36:23
contrib,aledbf,https://github.com/kubernetes/contrib/pull/809,https://api.github.com/repos/kubernetes/contrib/issues/809,WIP: [keepalived-vip] Add support to filter nodes,"closes #808
",closed,True,2016-04-19 17:43:44,2016-04-25 21:38:52
contrib,rmmh,https://github.com/kubernetes/contrib/pull/810,https://api.github.com/repos/kubernetes/contrib/issues/810,"Add Gubernator, a GAE proxy for displaying test results stored on GCS","Currently running at http://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/kubernetes-soak-continuous-e2e-gce/5043

@spxtr @fejta 
",closed,True,2016-04-19 23:04:21,2016-04-29 18:54:28
contrib,aledbf,https://github.com/kubernetes/contrib/pull/811,https://api.github.com/repos/kubernetes/contrib/issues/811,[git-sync] Allow random failures,"This PR introduces a flag to allow failures in the sync operation

```
$ docker run -it \
> -e GIT_SYNC_REPO=""https://********** repo url **********.git"" \
> -e GIT_SYNC_DEST=""/git"" \
> -e GIT_SYNC_MAX_SYNC_FAILURES=""10"" \
> gcr.io/google_containers/git-sync
2016/04/21 12:52:17 setting up the git credential cache
2016/04/21 12:52:18 clone ""https://********** repo url **********.git"": Cloning into '/git'...
2016/04/21 12:52:19 fetch ""master"": From https://********** repo url **********
 * branch            master     -> FETCH_HEAD
Already up-to-date.
2016/04/21 12:52:19 reset ""HEAD"": HEAD is now at befa882 feat(*): incorporación de versión 06958961
2016/04/21 12:52:19 wait 10 seconds
2016/04/21 12:52:29 done
2016/04/21 12:52:30 fetch ""master"": From https://********** repo url **********
 * branch            master     -> FETCH_HEAD
Already up-to-date.
2016/04/21 12:52:30 reset ""HEAD"": HEAD is now at befa882 feat(*): incorporación de versión 06958961
2016/04/21 12:52:30 wait 10 seconds
2016/04/21 12:52:40 done
```

after disconnecting internet access

```
2016/04/21 12:53:20 unexpected error syncing repo: error running command ""git pull origin master"" : exit status 1: fatal: unable to access 'https://********** repo url **********.git/': Could not resolve host: ********** host **********
2016/04/21 12:53:20 wait 10 seconds before retryng
2016/04/21 12:54:10 unexpected error syncing repo: error running command ""git pull origin master"" : exit status 1: fatal: unable to access 'https://********** repo url **********.git/': Could not resolve host: ********** host **********
2016/04/21 12:54:10 wait 10 seconds before retryng
2016/04/21 12:55:00 unexpected error syncing repo: error running command ""git pull origin master"" : exit status 1: fatal: unable to access 'https://********** repo url **********.git/': Could not resolve host: ********** host **********
2016/04/21 12:55:00 wait 10 seconds before retryng
2016/04/21 12:55:50 unexpected error syncing repo: error running command ""git pull origin master"" : exit status 1: fatal: unable to access 'https://********** repo url **********.git/': Could not resolve host: ********** host **********
2016/04/21 12:55:50 wait 10 seconds before retryng
2016/04/21 12:56:40 unexpected error syncing repo: error running command ""git pull origin master"" : exit status 1: fatal: unable to access 'https://********** repo url **********.git/': Could not resolve host: ********** host **********
2016/04/21 12:56:40 wait 10 seconds before retryng
2016/04/21 12:57:30 error syncing repo: error running command ""git pull origin master"" : exit status 1: fatal: unable to access 'https://********** repo url **********.git/': Could not resolve host: ********** host **********
```
",closed,True,2016-04-19 23:55:58,2016-05-26 17:59:51
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/812,https://api.github.com/repos/kubernetes/contrib/issues/812,Cluster-Autoscaler - Kubernetes client config builder,,closed,True,2016-04-20 09:36:27,2016-04-20 10:16:29
contrib,piosz,https://github.com/kubernetes/contrib/pull/813,https://api.github.com/repos/kubernetes/contrib/issues/813,Removed unused deps,"cc @fgrzadkowski 
",closed,True,2016-04-20 10:45:51,2016-04-20 11:32:36
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/814,https://api.github.com/repos/kubernetes/contrib/issues/814,Cluster-autoscaler: Unscheduled pod watch,"kubernetes/kubernetes#24404

cc: @fgrzadkowski @piosz 
",closed,True,2016-04-20 12:11:06,2016-04-21 08:24:04
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/815,https://api.github.com/repos/kubernetes/contrib/issues/815,Perfdash: Add version support in perfdash,"For https://github.com/kubernetes/kubernetes/pull/24542

This PR makes the perfdash aware of the metrics version.
Now, perfdash will only show the result with the newest version.

@gmarek Could you help me take a look? The change is minor. :)

/cc @yujuhong @vishh 
",closed,True,2016-04-20 18:30:19,2016-04-23 08:48:10
contrib,williamcaban,https://github.com/kubernetes/contrib/pull/816,https://api.github.com/repos/kubernetes/contrib/issues/816,Remove single inner quotes for insecure registry option,"Remove single inner quotes which prevented docker from starting in CentOS when using _insecure registry_.

When the following entry is generated in CentOS (/etc/sysconfig/docker), the Docker service won't start and complain about misconfiguration. When the single inner quotes are removed everything works fine.

Docker do not start --> INSECURE_REGISTRY=""'--insecure-registry=gcr.io '""

Docker start Okay --> INSECURE_REGISTRY=""--insecure-registry=gcr.io ""
",closed,True,2016-04-20 20:01:35,2016-04-21 12:57:21
contrib,danehans,https://github.com/kubernetes/contrib/issues/817,https://api.github.com/repos/kubernetes/contrib/issues/817,[ansible] Simplify Networking Options,"kubernetes/contrib/ansible now supports 3 different container networking implementations. How these config options are presented should be simplified. Duplication/overlap should be removed.

/cc: @eparis 
",closed,False,2016-04-20 20:34:42,2018-02-13 22:59:13
contrib,nottix,https://github.com/kubernetes/contrib/issues/818,https://api.github.com/repos/kubernetes/contrib/issues/818,Nginx Ingress Controller - Missing healthcheck params in upstream ,"Hi,

With current version (0.5) nginx marks a proxied server with fail status when a single request fails.

I think that the ningx template needs the healthcheck parameters for upstream section, like this:

upstream backend {
   server 10.1.0.102 max_fails=3 fail_timeout=30s;
}
",closed,False,2016-04-20 20:35:58,2016-05-30 20:03:23
contrib,spxtr,https://github.com/kubernetes/contrib/issues/819,https://api.github.com/repos/kubernetes/contrib/issues/819,The submit queue page takes forever to load,"Looking at the timeline, we've loaded all of the JSON within about a second, but we spend the next 5-7 seconds laying out the tabs/lists and then loading a billion github avatars.

http://submit-queue.k8s.io/
",closed,False,2016-04-20 22:37:15,2016-05-05 16:45:16
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/820,https://api.github.com/repos/kubernetes/contrib/issues/820,Cluster-autoscaler: Utils + more scale up flow,"kubernetes/kubernetes#24404

cc: @piosz @fgrzadkowski 
",closed,True,2016-04-21 10:00:51,2016-04-22 06:02:03
contrib,aledbf,https://github.com/kubernetes/contrib/pull/821,https://api.github.com/repos/kubernetes/contrib/issues/821,Update ubuntu-slim to next Ubuntu LTS version,,closed,True,2016-04-21 13:15:25,2016-04-21 18:14:39
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/822,https://api.github.com/repos/kubernetes/contrib/issues/822,Cluster-autoscaler: New node count estimator,"kubernetes/kubernetes#24404

cc: @piosz @fgrzadkowski 
",closed,True,2016-04-21 14:56:57,2016-04-21 17:49:09
contrib,eparis,https://github.com/kubernetes/contrib/pull/823,https://api.github.com/repos/kubernetes/contrib/issues/823,cherrypick: give a link to other potentially interesting PRs,"Adds a link to:
https://github.com/kubernetes/kubernetes/pulls?&q=is:pr+is:open+-base:master+-label:cherrypick-candidate+-label:cherrypick-approved
![cp](https://cloud.githubusercontent.com/assets/8093535/14720515/dbc652d8-07cf-11e6-9c0e-e8be05b0dfdf.png)
",closed,True,2016-04-21 18:43:57,2016-04-21 21:13:39
contrib,rmmh,https://github.com/kubernetes/contrib/pull/824,https://api.github.com/repos/kubernetes/contrib/issues/824,[mungegithub] Only request 40px avatars.,,closed,True,2016-04-21 20:42:16,2016-04-21 21:12:48
contrib,aledbf,https://github.com/kubernetes/contrib/pull/825,https://api.github.com/repos/kubernetes/contrib/issues/825,[nginx] Update nginx to 1.10.0 and add module for sticky sessions,,closed,True,2016-04-21 21:18:24,2016-05-02 00:12:30
contrib,jojimt,https://github.com/kubernetes/contrib/issues/826,https://api.github.com/repos/kubernetes/contrib/issues/826,Missing proxy environment settings on some url tasks,"Some of the url tasks in ansible/roles/kubernetes-addons/tasks/main.yml are missing the env setting for proxy. As a result, these tasks fail when executed behind a proxy.
",closed,False,2016-04-21 23:38:31,2016-04-22 14:52:27
contrib,jojimt,https://github.com/kubernetes/contrib/pull/827,https://api.github.com/repos/kubernetes/contrib/issues/827,Fix https://github.com/kubernetes/contrib/issues/826,"Added the missing environment setting and validate_certs, consistent with other addon tasks.
",closed,True,2016-04-21 23:41:23,2016-04-22 14:52:23
contrib,piosz,https://github.com/kubernetes/contrib/pull/828,https://api.github.com/repos/kubernetes/contrib/issues/828,Cluster-autoscaler: Implemented waiting for operation in GCE Manager,"ref https://github.com/kubernetes/kubernetes/issues/24404
",closed,True,2016-04-22 12:12:51,2016-04-22 12:36:09
contrib,nottix,https://github.com/kubernetes/contrib/issues/829,https://api.github.com/repos/kubernetes/contrib/issues/829,Nginx Ingress Controller - Routing to service,"Hi,

i found that ""Nginx Ingress Controller"" routes all traffic directly to pod bypassing services, why?
@aledbf 

Thank you.
",closed,False,2016-04-22 12:53:18,2017-05-08 04:18:13
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/830,https://api.github.com/repos/kubernetes/contrib/issues/830,Cluster-autoscaler: check node/mig state,"cc: @piosz @fgrzadkowski 
",closed,True,2016-04-22 14:19:07,2016-04-22 18:21:57
contrib,rmmh,https://github.com/kubernetes/contrib/pull/831,https://api.github.com/repos/kubernetes/contrib/issues/831,Optimize submit-queue frontend,"The Queue tab is fully painted in .7s vs 5s before.
- Default to showing the Queue tab, since that's the most relevant one.
- Load tab data lazily, to avoid the delay of rendering everything first.
- Update AngularJS dependency from 1.5.3 to 1.5.5 (seems harmless).
- Vendor a patched version of angular-material.js with faster rendering.
  angular/material#5681
",closed,True,2016-04-22 21:10:25,2016-04-23 18:01:08
contrib,danehans,https://github.com/kubernetes/contrib/issues/832,https://api.github.com/repos/kubernetes/contrib/issues/832,[ansible] run tests to verify operational status of cluster,"Currently the contrib/ansible project does not perform tests to validate the operational status of the k8s cluster deployed by ansible. I suggest creating a test role that performs the following:

check cluster health
`$ kubectl cluster-info`
The exit code should be 0

Check that nodes have registered. Each node should show STATUS Ready. Use grep/awk/print to verify for each node in list STATUS Ready
`$ kubectl get nodes`

Create web svc manifest for testing:
$ cat > web-svc.yml << EOF
https://gist.githubusercontent.com/danehans/b797bd6f8fb1d9d3aa7a/raw/72e14298e7b21666956bd31987059910acaef4e6/web-app.yml
EOF

Deploy web.yml to the k8s cluster
`$ kubectl create -f web.yml`

Verify the svc nodePort works using curl:
`$ curl kube-node-1:30302 # should return a 0 exit code
$ curl kube-node-2:30302 # should return a 0 exit code`

List the pods to get their names
**Note**: Use grep/awk/print to get the name of the first pod in the rc.
`$ kubectl get pods
NAME        READY     STATUS    RESTARTS   AGE
web-6rydi   1/1       Running   0          20m
web-zibtt   1/1       Running   0          20m`

Get the IP of the first pod in the rc:
`$ kubectl describe pod web-6rydi | grep IP`

Use kubectl exec to curl from pod 1 to pod 2’s IP:
`$ kubectl exec web-6rydi -i -t -- ping 172.16.47.6`
",closed,False,2016-04-23 05:26:48,2018-02-13 22:59:14
contrib,danehans,https://github.com/kubernetes/contrib/issues/833,https://api.github.com/repos/kubernetes/contrib/issues/833,[ansible] support multiple clusters,"Currently, contrib/ansible is designed around a single cluster deployment. The project should support the following multiple cluster scenarios:
1. Single location, separate clusters based on deployment need, ie. Staging/Test/Prod
2. Multiple geographic locations. Multiple clusters, spread across multiple locations.

**Note**: kubernetes/kubernetes is adding support for multiple clusters using federation (aka Ubernetes):
https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/federation.md
",closed,False,2016-04-23 05:44:30,2018-02-13 22:59:12
contrib,danehans,https://github.com/kubernetes/contrib/issues/834,https://api.github.com/repos/kubernetes/contrib/issues/834,[ansible] Fix Cluster Addons for CentOS,"Cluster addons were fixed for coreos deployments with https://github.com/kubernetes/contrib/pull/672/files

However, the addons are not functioning for centos. Review https://github.com/kubernetes/contrib/pull/672/files and investigate why it only works on coreos, port the req'd functionality to centos
",closed,False,2016-04-23 05:49:23,2016-08-24 15:35:02
contrib,eparis,https://github.com/kubernetes/contrib/pull/835,https://api.github.com/repos/kubernetes/contrib/issues/835,State why we are enforcing the release note process on a cherrypick,"Some 'cherrypicks' don't have parents. Some have parents that didn't set
a release note. This PR will add a comment why we are enforcing the
release note process.
",closed,True,2016-04-23 17:59:48,2016-04-23 18:01:41
contrib,eparis,https://github.com/kubernetes/contrib/pull/836,https://api.github.com/repos/kubernetes/contrib/issues/836,Convert mungegithub to use deployments instead of RCs,"Simplifies makefile mess
",closed,True,2016-04-23 18:00:13,2016-04-23 18:00:56
contrib,alekssaul,https://github.com/kubernetes/contrib/pull/837,https://api.github.com/repos/kubernetes/contrib/issues/837,Scale-demo html files pointing to incorrect Kubernetes logo.png location,"Kubernetes logos are located under logo folder.
",closed,True,2016-04-23 20:35:27,2016-04-23 21:28:55
contrib,danehans,https://github.com/kubernetes/contrib/issues/838,https://api.github.com/repos/kubernetes/contrib/issues/838,[ansible] Refactor Kubernetes Roles Based on Services,"The current kubernetes ansible roles are organized into masters and nodes. This makes breaking the master/nodes services into individual units of management, deployment, etc.. In the past, I have seen issues with other projects that do not manage services individually. I suggest changing master into individual roles based on master service, such as:
1. kube-apiserver
2. kube-controller-mansger
3. kube-scheduler

The same for the node role:
1. kubelet
2. kube-proxy

and the client role:
1. kubectl

A single master inventory would stay the same.

site manifest (i.e. cluster.yml) would look like:

```
- hosts: all
  gather_facts: false
  sudo: yes
  roles:
    - pre-ansible
  tags:
    - pre-ansible

# Install etcd
- hosts: etcd
  sudo: yes
  roles:
    - etcd
  tags:
    - etcd

- hosts: all
  sudo: yes
  roles:
    - common
    - docker
  tags:
    - docker

# install flannel
- hosts:
    - masters
    - nodes
  sudo: yes
  roles:
    - { role: flannel, when: networking == 'flannel' }
  tags:
    - network-service-install

# install kube master services
- hosts: masters
  sudo: yes
  roles:
    - kube-apiserver
   - kube-controller-manager
   - kube-scheduler
   - kubectl
   - kubernetes-addons
  tags:
    - masters
    - dns

# install kubernetes on the nodes
- hosts: nodes
  sudo: yes
  roles:
    - kubelet
    - kube-proxy
  tags:
    - nodes
```
",closed,False,2016-04-24 20:41:14,2018-02-15 06:29:03
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/839,https://api.github.com/repos/kubernetes/contrib/issues/839,Cluster-autoscaler: scale up cluster if needed.,"kubernetes/kubernetes#24404

cc: @piosz @fgrzadkowski 
",closed,True,2016-04-25 13:31:20,2016-04-25 14:31:07
contrib,eparis,https://github.com/kubernetes/contrib/pull/840,https://api.github.com/repos/kubernetes/contrib/issues/840,Remove old jenkins comments,"This PR will remove 'old' jenkins pass/fail comments. Only the last comment will remain in the PR. For example in https://github.com/kubernetes/kubernetes/pull/24514 this will remove:
- https://github.com/kubernetes/kubernetes/pull/24514#issuecomment-212243186
- https://github.com/kubernetes/kubernetes/pull/24514#issuecomment-213247524
- https://github.com/kubernetes/kubernetes/pull/24514#issuecomment-213556240
- https://github.com/kubernetes/kubernetes/pull/24514#issuecomment-213916894

But will leave:
- https://github.com/kubernetes/kubernetes/pull/24514#issuecomment-213924286
",closed,True,2016-04-25 13:38:39,2016-04-26 15:40:47
contrib,justinsb,https://github.com/kubernetes/contrib/pull/841,https://api.github.com/repos/kubernetes/contrib/issues/841,"[WIP/RFC] Simple ingress -> DNS controller, using AWS Route53","This is a simple controller, that observes the hostnames configured on ingress resources, and configures them in AWS Route53 (where the HostedZones are already configured).  This controller would be launched as a pod (single-instance), and is configured with a service to expose (likely the nginx service).  It watches for that service, observes the ingress name/IP.  It also watches all ingress objects.  Whenever an ingress object is created with a hostname rule, we try to configure that in Route53, pointing the name to the configured service.

It (mostly?) works, but I have done no optimization yet (e.g. to reduce the number of API calls).  Also I would naturally add a backend for GCE DNS (and I'm sure there will be a desire for other services also, hence it is pluggable)

Comments appreciated (though I think mostly on the concept rather than the details of the implementation at this stage)

@iterion - similar to work you've done for services

cc @bprashanth because you seem to be the ingress guru - can you give me any pointers about whether this is useful, who I should talk to etc.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/841)

<!-- Reviewable:end -->
",closed,True,2016-04-25 16:27:02,2016-12-18 19:19:27
contrib,aledbf,https://github.com/kubernetes/contrib/pull/842,https://api.github.com/repos/kubernetes/contrib/issues/842,[keepalived-vip] Add support to filter nodes in unicast and UDP services,"closes #808
",closed,True,2016-04-25 21:39:20,2016-05-08 21:36:23
contrib,adamschaub,https://github.com/kubernetes/contrib/pull/843,https://api.github.com/repos/kubernetes/contrib/issues/843,Loadbalancer role (keepalived/haproxy),"Added load-balancer role for HA master configuration of cluster. 

When there is more than one master node, an haproxy and leader-elected keepalived pod (via unicast) is run on each master node. A VIP is specified in group_vars for the apiserver. Haproxy uses round-robin load balancing with SSL pass-through.

This PR depends on #761. I'll make changes here as that evolves.

/cc: @danehans @rutsky @eparis

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/843)

<!-- Reviewable:end -->
",closed,True,2016-04-25 22:08:24,2018-02-17 12:22:02
contrib,thockin,https://github.com/kubernetes/contrib/pull/844,https://api.github.com/repos/kubernetes/contrib/issues/844,Remove stale things from micro-demos README,,closed,True,2016-04-25 23:05:16,2016-04-26 06:41:20
contrib,MikeSpreitzer,https://github.com/kubernetes/contrib/pull/845,https://api.github.com/repos/kubernetes/contrib/issues/845,Bug fix to definition of thenet,"When the CNI_COMMAND is DEL, the environment variable `thenet` was
being referenced but not defined.
",closed,True,2016-04-26 02:24:54,2016-05-10 16:24:06
contrib,simonswine,https://github.com/kubernetes/contrib/pull/846,https://api.github.com/repos/kubernetes/contrib/issues/846,Adapt nginx config for supporting proxy_protocol on HTTP/HTTPS customer facing,"Hi everyone,

I tried to use this controller on my baremetal cluster. (The real frontend is haproxy based and doesn't terminate HTTP/HTTPS. It just forwards the client IP information with proxy_protocol.

The current master nginx ingress controller doesn't not enable proxy protocol for these customer facing sockets, when use-proxy-protocol is true. 

I changed the template accordingly

Cheers,

Christian
",closed,True,2016-04-26 08:25:29,2016-05-06 15:59:09
contrib,bretthoerner,https://github.com/kubernetes/contrib/issues/847,https://api.github.com/repos/kubernetes/contrib/issues/847,Nginx Ingress Controller - Allow scoping by selectors,"By my reading of the code (which could be wrong) the nginx-ingress-controller only allows you to ""scope"" your ingress objects by namespace.

I have a case where I'd like to run 2 nginx-ingress-controllers in the same namespace, each with different configuration. But within that single namespace each nginx-ingress-controller will ""see"" and use all ingress objects.

Does it make sense to allow for the label/selector pattern on ingress objects and nginx-ingress-controller deployments? This is something I'm willing to work on, but it's probably best someone verify my assumptions are correct here.
",closed,False,2016-04-26 15:31:11,2017-04-04 15:16:26
contrib,eparis,https://github.com/kubernetes/contrib/pull/848,https://api.github.com/repos/kubernetes/contrib/issues/848,Relax the matching regexp to ignore links,"The link forms have changed quite a bit over time.

This results in over 1400 removed comments from open PRs
",closed,True,2016-04-26 16:07:08,2016-04-26 16:23:51
contrib,simonswine,https://github.com/kubernetes/contrib/pull/849,https://api.github.com/repos/kubernetes/contrib/issues/849,Fix nginx ingress controller bug around config map merging,"Hi I just discovered a bug around the nginx config merge (defaults are merged with configmap)
- a config map bool value of false cannot overwritte a true value from
  defaults
- implement merging in ReadConfig
- remove helper function merge
- adds tests to ensure config is read properly
",closed,True,2016-04-26 16:23:13,2016-05-02 22:10:17
contrib,simonswine,https://github.com/kubernetes/contrib/pull/850,https://api.github.com/repos/kubernetes/contrib/issues/850,ingress: adds configurable SSL redirect nginx controller,"Hi everyone,

I want my services behind the ingress to decide if HTTPS is strictly required or not. So I added the feature to disable the 301 redirect to the https:/ URL.

To make this actually work #849 needs to be merged before. (Otherwise you cannot override and default value true with a false)

Cheers,

Christian
",closed,True,2016-04-26 16:36:41,2016-06-08 17:25:22
contrib,danehans,https://github.com/kubernetes/contrib/issues/851,https://api.github.com/repos/kubernetes/contrib/issues/851,[ansible] Support Security Best Practices,"For a Kubernetes deployment to support production workloads, the cluster must be secure. The contrib/ansible project should support the ability to deploy and routinely verify the security measures applied to the cluster. Additionally, the security measures available and applied should be well documented for auditing purposes. It is recommended to use [DISA STIGs](http://iase.disa.mil/stigs/Pages/index.aspx) as the reference model for security measures.
",closed,False,2016-04-26 16:59:14,2018-02-13 22:59:12
contrib,danehans,https://github.com/kubernetes/contrib/issues/852,https://api.github.com/repos/kubernetes/contrib/issues/852,[ansible] Support Lifecycle Management,"Currently, the contrib/ansible project does a good job of performing an initial deployment of a Kubernetes cluster. After the cluster is initially deployed, operators have a multitude of tasks that must be performed to manage the lifecycle of the cluster. For example:
- Add a master or node to the cluster
- Upgrade the version of Kubernetes
- etc..

Ansible playbooks should be created, tested, documented, supported, etc. to support common lifecycle management tasks. Additionally, the overall design of the project should be addressed to support multiple playbooks.
",closed,False,2016-04-26 17:14:54,2018-02-22 15:23:08
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/853,https://api.github.com/repos/kubernetes/contrib/issues/853,Perfdash: Fetch build logs from latest to oldest,"Fetch build logs from latest to oldest so as to make sure metrics with newer version are parsed first.

/cc @gmarek 
",closed,True,2016-04-26 22:03:55,2016-04-26 22:07:17
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/854,https://api.github.com/repos/kubernetes/contrib/issues/854,Bump up the perfdash version to 0.6,"Minor change.

As @vishh suggested, we should have a better version control for perfdash image. :)

@gmarek BTW, I updated the running perfdash first. It would run happily now~
",closed,True,2016-04-27 01:57:40,2016-04-27 05:52:55
contrib,roberthbailey,https://github.com/kubernetes/contrib/issues/855,https://api.github.com/repos/kubernetes/contrib/issues/855,Auto-assigner should be smarter about cherry picks,"We should codify the release branch owner(s) and auto-assign all cherry picks to a release branch to the onwer(s). 

/cc @bgrant0607 @zmerlynn @eparis 
",closed,False,2016-04-27 17:52:39,2018-02-13 16:53:08
contrib,telepenin,https://github.com/kubernetes/contrib/pull/856,https://api.github.com/repos/kubernetes/contrib/issues/856,Refactoring duplicate sections,"Added with_items in the sections instead duplicate sections.
",closed,True,2016-04-27 18:20:30,2016-04-28 06:33:30
contrib,telepenin,https://github.com/kubernetes/contrib/pull/857,https://api.github.com/repos/kubernetes/contrib/issues/857,Dummy grains variable,"Currently task ""DNS | Install Template from converted saltfiles"" broken, because skydns-rc.yaml.in contains ""grains"" variable https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/addons/dns/skydns-rc.yaml.in
",closed,True,2016-04-27 18:30:45,2016-04-27 18:34:37
contrib,mikesplain,https://github.com/kubernetes/contrib/issues/858,https://api.github.com/repos/kubernetes/contrib/issues/858,Ingress servers duplicated when multiple ingresses,"When I test against the ingress controller in https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx, using the ingresses in https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/examples/ingress.yaml, multiple of each IP are listed the server config of nginx:

```
    upstream default-echoheaders-x-80 {
        least_conn;
        server 10.138.27.3:8080;
        server 10.138.27.3:8080;

    }

    upstream default-echoheaders-y-80 {
        least_conn;
        server 10.138.27.3:8080;

    }
```

From what I can tell, this causes issues when combined with the `proxy_next_upstream` flag in nginx for bigger server configs like:
![nginx vhost traffic status monitor 2016-04-27 16-21-41](https://cloud.githubusercontent.com/assets/881965/14866961/3333a9b2-0c94-11e6-80a9-9470df96f55f.png)

Is it possible to de-dup this server list or is this for some reason intentional?
",closed,False,2016-04-27 20:22:31,2016-05-02 16:01:56
contrib,andrew-j-price,https://github.com/kubernetes/contrib/pull/859,https://api.github.com/repos/kubernetes/contrib/issues/859,On Atomic hosts address the conditional check ''etcd-legacy' not in..,"The following error occurs when running against Atomic hosts.  

TASK [etcd : Open etcd client legacy port with iptables] ***********************
fatal: [<host>]: FAILED! => {""failed"": true, ""msg"": ""The conditional check ''etcd-legacy' not in iptablesrules.stdout and networking == contiv' failed. The error was: error while evaluating conditional ('etcd-legacy' not in iptablesrules.stdout and networking == contiv): 'contiv' is undefined\n\nThe error appears to have been in... 
",closed,True,2016-04-28 03:10:52,2016-04-28 17:12:54
contrib,theobolo,https://github.com/kubernetes/contrib/issues/860,https://api.github.com/repos/kubernetes/contrib/issues/860,Nginx Ingress Controller - the /foo redirection is not working ?,"Hi guys,

I work with the Ingress / Nginx Ingress Controller, but i can't get the /foo redirection working.

I deployed on a Kubernetes cluster a daemonset of Nginx ingress controller, default backend is the http-default-backend example.

This is my ingress object : 

```
# apimobile Ingress
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: apimobile-map
spec:
  rules:
  - host: azure.coursierprive.com
    http:
      paths:
      - path: /apimobile
        backend:
          serviceName: application-apimobile
          servicePort: 80
  - host: kube-20160428-120550-loadbalancer.northeurope.cloudapp.azure.com
    http:
      paths:
      - path: /
        backend:
          serviceName: application-apimobile
          servicePort: 80

```

My second rule is working but not the first one, if i try to reach my api on : azure.coursierprive.com/apimobile 

I face the : backend - 404 

I tried with Jenkins portal instead of my API but i face the same problem.
",closed,False,2016-04-28 15:35:04,2018-09-19 07:12:13
contrib,danehans,https://github.com/kubernetes/contrib/issues/861,https://api.github.com/repos/kubernetes/contrib/issues/861,[ansible] Add Vagrant Support for Atomic,"contrib/ansible code supports multiple OS's, including atomic. However, atomic is not supported in contrib/ansible/vagrant. This makes it difficult for dev's to test patches against this OS, causing bugs to be introduced at a higher rate than other OS's. Since atomic is supported in the ansible code, vagrant support should be added as well.
",closed,False,2016-04-28 16:04:50,2018-02-13 22:59:10
contrib,ixdy,https://github.com/kubernetes/contrib/issues/862,https://api.github.com/repos/kubernetes/contrib/issues/862,shame mailer mysteriously stopped working,"The daily shame mailer mysteriously stopped working a few days ago.

From the log on Jenkins:

```
Running gcr.io/google_containers/shame-mailer:2016-03-25-d0e88f1
I0426 15:40:53.052672       6 github.go:215] Made 0 API calls since the last Reset 0.000000 calls/sec
Error: Error running shame: exit status 1
Usage:
  mungegithub [flags]
  mungegithub [command]

Available Commands:
  gencommiters Generate the list of people with commit access

Flags:
      --address="":8080"": The address to listen on for HTTP Status
      --alsologtostderr[=false]: log to standard error as well as files
      --block-path-config=""block-path.yaml"": file containing the pathnames to block or not block
      --blunderbuss-reassign[=false]: Assign PRs even if they're already assigned; use with -dry-run to judge changes to the assignment algorithm
      --committers=""./committers.txt"": File in which the list of authorized committers is stored; only used if this list cannot be gotten at run time.  (Merged with whitelist; separate so that it can be auto-generated)
      --dry-run[=true]: If true, don't actually merge anything
      --e2e-status-context=""Jenkins GCE e2e"": The name of the github status context for the e2e PR Builder
      --generated-files-config=""generated-files.txt"": file containing the pathname to label mappings
      --issue-reports=[]: A list of issue reports to run. If set, will run the reports and exit.
      --jenkins-host=""http://jenkins-master:8080"": The URL for the jenkins job to watch
      --jenkins-jobs=[kubelet-gce-e2e-ci,kubernetes-build,kubernetes-test-go,kubernetes-e2e-gce,kubernetes-e2e-gce-slow,kubernetes-e2e-gke,kubernetes-e2e-gke-slow,kubernetes-e2e-gce-scalability,kubernetes-kubemark-5-gce]: Comma separated list of jobs in Jenkins to use for stability testing
      --kubernetes-dir=""./gitrepos/kubernetes"": Path to git checkout of kubernetes tree
      --labels=[]: CSV list of label which should be set on processed PRs. Unset is all labels.
      --log-backtrace-at=:0: when logging hits line file:N, emit a stack trace
      --log-dir="""": If non-empty, write log files in this directory
      --log-flush-frequency=5s: Maximum number of seconds between log flushes
      --logtostderr[=true]: log to standard error instead of files
      --max-pr-number=9223372036854775807: The maximum PR to start with
      --min-pr-number=0: The minimum PR to start with
      --once[=false]: If true, run one loop and exit
      --organization=""kubernetes"": The github organization to scan
      --path-label-config=""path-label.txt"": file containing the pathname to label mappings
      --period=10m0s: The period for running mungers
      --pr-mungers=[]: A list of pull request mungers to run
      --project=""kubernetes"": The github project to scan
      --required-contexts=[]: Comma separate list of status contexts required for a PR to be considered ok to merge
      --shame-cc="""": Cc: header for shame report
      --shame-from="""": From: header for shame report
      --shame-reply-to="""": Reply-To: header for shame report
      --shame-report-cmd=""tee -a shame.txt"": command to execute, passing the report as stdin
      --state=""open"": State of PRs to process: 'open', 'all', etc
      --stderrthreshold=2: logs at or above this threshold go to stderr
      --token="""": The OAuth Token to use for requests.
      --token-file="""": The file containing the OAuth Token to use for requests.
      --unit-status-context=""Jenkins unit/integration"": The name of the github status context for the unit PR Builder
      --use-http-cache[=true]: If true, use a client side HTTP cache for API requests.
      --user-whitelist=""./whitelist.txt"": Path to a whitelist file that contains users to auto-merge.  Required.
      --v=0: log level for V logs
      --vmodule=: comma-separated list of pattern=N settings for file-filtered logging
      --weak-stable-jobs=[kubernetes-kubemark-500-gce]: Comma separated list of jobs in Jenkins to use for stability testing that needs only weak success
      --whitelist-override-label=""ok-to-merge"": Github label, if present on a PR it will be merged even if the author isn't in the whitelist
      --www=""www"": Path to static web files to serve from the webserver

Use ""mungegithub [command] --help"" for more information about a command.

F0426 15:40:55.803371       6 mungegithub.go:125] Error running shame: exit status 1
```

Unfortunately, there's nothing in the error message to indicate why this is failing.

Also, we're still using the same image, which had been working previously. Maybe the GitHub query or the SMTP call are failing? It'd be helpful if there were any sort of error besides ""exit status 1"".

cc @eparis @krousey 
",closed,False,2016-04-28 20:27:16,2016-04-28 21:42:59
contrib,ixdy,https://github.com/kubernetes/contrib/pull/863,https://api.github.com/repos/kubernetes/contrib/issues/863,Make shame command print out to stdout/stderr,"We're running `mailx` in verbose mode, but the stdout/stderr was going nowhere, which was of no use.

This change helped me root-cause #862.

@eparis @krousey @lavalamp 
",closed,True,2016-04-28 21:31:18,2016-04-28 21:36:22
contrib,telepenin,https://github.com/kubernetes/contrib/issues/864,https://api.github.com/repos/kubernetes/contrib/issues/864,[Ingress] Error with execute examples/default/rc-default.yaml,"When I try to execute `kubectl create -f examples/default/rc-default.yaml` I get error in docker:

```
[root@node1 ~]# docker logs 3d61f7980354
I0429 10:43:06.322202       1 main.go:94] Using build: https://github.com/bprashanth/contrib.git - git-7fbd252
E0429 10:43:06.322292       1 config.go:239] Expected to load root CA config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory
F0429 10:43:36.350474       1 main.go:123] unexpected error getting runtime information: timed out waiting for the condition
```

What can decide this problem?
k8s v1.2.3
",closed,False,2016-04-29 11:03:47,2016-07-08 17:12:54
contrib,telepenin,https://github.com/kubernetes/contrib/issues/865,https://api.github.com/repos/kubernetes/contrib/issues/865,[Ansible] k8s without ingress,"When I create new cluster with master and one node. I get k8s v1.2.0 without [Ingress](http://kubernetes.io/docs/user-guide/ingress/).
How I can add ingress support to ready cluster?
Or how I can add something in provision to add Ingress functionality?
",closed,False,2016-04-29 11:09:30,2016-05-06 13:52:57
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/866,https://api.github.com/repos/kubernetes/contrib/issues/866,Cluster-autoscaler: build k8s scheduler node info structure based on kubectl drain,"kubernetes/kubernetes#24404

It would be nice if :
https://github.com/kubernetes/kubernetes/pull/24929
https://github.com/kubernetes/kubernetes/pull/24923
were merged before this PR (although it is not a super hard requirement, we can update godeps again once they are merged).

cc: @piosz @fgrzadkowski
",closed,True,2016-04-29 12:12:16,2016-05-04 12:28:18
contrib,eparis,https://github.com/kubernetes/contrib/issues/867,https://api.github.com/repos/kubernetes/contrib/issues/867,mungegithub misbehaves when github returns 502s,"Likely a problem because the 'branch name' helper does not return an error, so the munger worked with """" instead of realizing there was an error. The helper was originally intended to check the positive, but was used to check the negative, in which case the distinction of 'error' is important. There are probably other helpers that really should return an error code  :-(

```
github.go:1074] Error commits for PR 24164: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24164/commits?per_page=100: 502 Server Error []
lgtm_after_commit.go:73] PR 24164 unable to determine lastModified or lgtmTime
github.go:428] Error getting PR# 24164: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24164: 502 Server Error []
github.go:428] Error getting PR# 24164: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24164: 502 Server Error []
github.go:627] Adding labels [do-not-merge] to PR 24164
github.go:1327] Commenting ""This PR is not for the master branch but does not have the `cherrypick-approved` label. Adding the `do-not-merge` label."" in 24164
github.go:428] Error getting PR# 24164: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24164: 502 Server Error []
github.go:428] Error getting PR# 24164: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24164: 502 Server Error []
github.go:1074] Error commits for PR 24164: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24164/commits?per_page=100: 502 Server Error []
github.go:428] Error getting PR# 24164: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24164: 502 Server Error []
github.go:428] Error getting PR# 24164: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24164: 502 Server Error []
github.go:428] Error getting PR# 24164: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24164: 502 Server Error []
github.go:428] Error getting PR# 24164: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24164: 502 Server Error []
github.go:1074] Error commits for PR 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549/commits?per_page=100: 502 Server Error []
lgtm_after_commit.go:73] PR 24549 unable to determine lastModified or lgtmTime
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:627] Adding labels [do-not-merge] to PR 24549
github.go:1327] Commenting ""This PR is not for the master branch but does not have the `cherrypick-approved` label. Adding the `do-not-merge` label."" in 24549
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:1074] Error commits for PR 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549/commits?per_page=100: 502 Server Error []
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
submit-queue.go:678] 24549: unknown err: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:428] Error getting PR# 24549: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24549: 502 Server Error []
github.go:428] Error getting PR# 24554: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554: 502 Server Error []
github.go:428] Error getting PR# 24554: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554: 502 Server Error []
github.go:627] Adding labels [do-not-merge] to PR 24554
github.go:1327] Commenting ""This PR is not for the master branch but does not have the `cherrypick-approved` label. Adding the `do-not-merge` label."" in 24554
github.go:428] Error getting PR# 24554: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554: 502 Server Error []
github.go:1074] Error commits for PR 24554: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554/commits?per_page=100: 502 Server Error []
github.go:428] Error getting PR# 24554: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554: 502 Server Error []
github.go:428] Error getting PR# 24554: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554: 502 Server Error []
github.go:428] Error getting PR# 24554: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554: 502 Server Error []
submit-queue.go:678] 24554: unknown err: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554: 502 Server Error []
github.go:428] Error getting PR# 24554: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554: 502 Server Error []
github.go:428] Error getting PR# 24554: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554: 502 Server Error []
github.go:428] Error getting PR# 24554: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24554: 502 Server Error []
github.go:428] Error getting PR# 24557: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24557: 502 Server Error []
github.go:428] Error getting PR# 24710: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24710: 502 Server Error []
github.go:428] Error getting PR# 24710: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24710: 502 Server Error []
github.go:627] Adding labels [do-not-merge] to PR 24710
github.go:1327] Commenting ""This PR is not for the master branch but does not have the `cherrypick-approved` label. Adding the `do-not-merge` label."" in 24710
github.go:428] Error getting PR# 24710: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24710: 502 Server Error []
github.go:1074] Error commits for PR 24710: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24710/commits?per_page=100: 502 Server Error []
github.go:428] Error getting PR# 24710: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24710: 502 Server Error []
github.go:428] Error getting PR# 24710: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24710: 502 Server Error []
submit-queue.go:678] 24710: unknown err: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24710: 502 Server Error []
github.go:428] Error getting PR# 24710: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24710: 502 Server Error []
github.go:428] Error getting PR# 24710: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24710: 502 Server Error []
github.go:428] Error getting PR# 24710: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24710: 502 Server Error []
github.go:428] Error getting PR# 24718: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24718: 502 Server Error []
github.go:428] Error getting PR# 24718: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24718: 502 Server Error []
github.go:627] Adding labels [do-not-merge] to PR 24718
github.go:1327] Commenting ""This PR is not for the master branch but does not have the `cherrypick-approved` label. Adding the `do-not-merge` label."" in 24718
github.go:428] Error getting PR# 24718: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24718: 502 Server Error []
github.go:1074] Error commits for PR 24718: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24718/commits?per_page=100: 502 Server Error []
github.go:428] Error getting PR# 24718: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24718: 502 Server Error []
github.go:428] Error getting PR# 24718: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24718: 502 Server Error []
github.go:428] Error getting PR# 24718: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24718: 502 Server Error []
submit-queue.go:678] 24718: unknown err: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24718: 502 Server Error []
github.go:428] Error getting PR# 24718: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24718: 502 Server Error []
github.go:428] Error getting PR# 24718: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24718: 502 Server Error []
github.go:929] PR 24718 setting ""Submit Queue"" Github status to ""unknown failure""
github.go:428] Error getting PR# 24835: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835: 502 Server Error []
github.go:428] Error getting PR# 24835: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835: 502 Server Error []
github.go:627] Adding labels [do-not-merge] to PR 24835
github.go:1327] Commenting ""This PR is not for the master branch but does not have the `cherrypick-approved` label. Adding the `do-not-merge` label."" in 24835
github.go:428] Error getting PR# 24835: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835: 502 Server Error []
github.go:1074] Error commits for PR 24835: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835/commits?per_page=100: 502 Server Error []
github.go:428] Error getting PR# 24835: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835: 502 Server Error []
github.go:428] Error getting PR# 24835: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835: 502 Server Error []
github.go:428] Error getting PR# 24835: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835: 502 Server Error []
submit-queue.go:678] 24835: unknown err: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835: 502 Server Error []
github.go:428] Error getting PR# 24835: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835: 502 Server Error []
github.go:428] Error getting PR# 24835: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835: 502 Server Error []
github.go:428] Error getting PR# 24835: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24835: 502 Server Error []
github.go:428] Error getting PR# 24836: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836: 502 Server Error []
github.go:428] Error getting PR# 24836: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836: 502 Server Error []
github.go:627] Adding labels [do-not-merge] to PR 24836
github.go:1327] Commenting ""This PR is not for the master branch but does not have the `cherrypick-approved` label. Adding the `do-not-merge` label."" in 24836
github.go:428] Error getting PR# 24836: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836: 502 Server Error []
github.go:1074] Error commits for PR 24836: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836/commits?per_page=100: 502 Server Error []
github.go:428] Error getting PR# 24836: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836: 502 Server Error []
github.go:428] Error getting PR# 24836: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836: 502 Server Error []
github.go:428] Error getting PR# 24836: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836: 502 Server Error []
submit-queue.go:678] 24836: unknown err: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836: 502 Server Error []
github.go:428] Error getting PR# 24836: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836: 502 Server Error []
github.go:428] Error getting PR# 24836: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836: 502 Server Error []
github.go:428] Error getting PR# 24836: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24836: 502 Server Error []
github.go:428] Error getting PR# 24838: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24838: 502 Server Error []
github.go:428] Error getting PR# 24838: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24838: 502 Server Error []
github.go:627] Adding labels [do-not-merge] to PR 24838
github.go:1327] Commenting ""This PR is not for the master branch but does not have the `cherrypick-approved` label. Adding the `do-not-merge` label."" in 24838
github.go:428] Error getting PR# 24838: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24838: 502 Server Error []
github.go:1074] Error commits for PR 24838: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24838/commits?per_page=100: 502 Server Error []
github.go:428] Error getting PR# 24838: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24838: 502 Server Error []
github.go:428] Error getting PR# 24838: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24838: 502 Server Error []
submit-queue.go:678] 24838: unknown err: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24838: 502 Server Error []
github.go:428] Error getting PR# 24838: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24838: 502 Server Error []
github.go:428] Error getting PR# 24838: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24838: 502 Server Error []
github.go:428] Error getting PR# 24838: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24838: 502 Server Error []
github.go:428] Error getting PR# 24841: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24841: 502 Server Error []
github.go:428] Error getting PR# 24841: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24841: 502 Server Error []
github.go:627] Adding labels [do-not-merge] to PR 24841
github.go:1327] Commenting ""This PR is not for the master branch but does not have the `cherrypick-approved` label. Adding the `do-not-merge` label."" in 24841
github.go:428] Error getting PR# 24841: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24841: 502 Server Error []
github.go:1074] Error commits for PR 24841: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24841/commits?per_page=100: 502 Server Error []
blunderbuss.go:125] Couldn't find an owner for: .gitignore
blunderbuss.go:137] No owners found for PR 24897
github.go:1074] Error commits for PR 24921: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24921/commits?per_page=100: 502 Server Error []
github.go:1074] Error commits for PR 24921: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24921/commits?per_page=100: 502 Server Error []
github.go:1074] Error commits for PR 24921: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24921/commits?per_page=100: 502 Server Error []
github.go:428] Error getting PR# 24923: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24923: 502 Server Error []
github.go:428] Error getting PR# 24923: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24923: 502 Server Error []
github.go:627] Adding labels [do-not-merge] to PR 24923
github.go:1327] Commenting ""This PR is not for the master branch but does not have the `cherrypick-approved` label. Adding the `do-not-merge` label."" in 24923
github.go:428] Error getting PR# 24923: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24923: 502 Server Error []
github.go:1074] Error commits for PR 24923: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24923/commits?per_page=100: 502 Server Error []
github.go:428] Error getting PR# 24923: GET https://api.github.com/repos/kubernetes/kubernetes/pulls/24923: 502 Server Error []
blunderbuss.go:156] Assigning 24961 to mikedanese who had a 24.14% chance to be assigned (previously assigned to <nil>)
github.go:1124] Assigning PR# 24961  to mikedanese
github.go:627] Adding labels [size/L] to PR 24961
github.go:1327] Commenting ""Labelling this PR as size/L"" in 24961
github.go:627] Adding labels [release-note-label-needed] to PR 24961
github.go:929] PR 24961 setting ""Submit Queue"" Github status to ""Github CI tests are not green.""
github.go:217] Made 4123 API calls since the last Reset 2.663085 calls/sec
mungegithub.go:77] Not sleeping as we took more than 10m0s to complete one loop
```
",closed,False,2016-04-29 12:31:42,2017-01-21 01:20:45
contrib,rmmh,https://github.com/kubernetes/contrib/pull/868,https://api.github.com/repos/kubernetes/contrib/issues/868,Fix submit-queue build health loading.,"self.builds depends on self.health, but was not being updated when health changed.
",closed,True,2016-04-29 18:59:44,2016-04-29 20:41:18
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/869,https://api.github.com/repos/kubernetes/contrib/issues/869,More PR automation,"- Explicitly notify the PR author if the PR is stuck: tests failed, needs rebase, needs release note, etc. The notifications that don't mention anyone won't rise above github notification noise.
- Explicitly notify the PR assignee if the PR needs them to do something: ok to test, ok-to-merge, e2e--not-required, reapply lgtm label after rebase, release note label, etc.
- Ping the PR due to lack of inactivity, notifying either the PR author or the assignee, depending on who last touched the PR (keep track of whose court it's in)
- Create a way to postpone the next ping for a fixed amount of time (day, week, month, next release cycle)
- Reassign PRs whose assignees never responded
- Reassign PRs that need approval from someone other than the current assignee (part of OWNERS proposal)

@fejta @philips @eparis 
",closed,False,2016-04-29 20:14:49,2018-01-22 19:03:22
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/870,https://api.github.com/repos/kubernetes/contrib/issues/870,Flannel version number in defaults,"- we should be handling the flannel version numbers in defaults, not setting the fact in the task
- validated on Vagrant + CoreOS
",closed,True,2016-04-29 21:15:50,2016-04-29 22:21:12
contrib,ajohnstone,https://github.com/kubernetes/contrib/issues/871,https://api.github.com/repos/kubernetes/contrib/issues/871,Ingress - SSL should not redirect by default ,"The nginx.tmpl redirects to https by default, I'm not sure this should be the standard behaviour.

```
        {{ if $server.SSL }}
        if ($scheme = http) {
            return 301 https://$host$request_uri;
        }
        {{ end }}
```

All the examples will break for example...

```
$ curl -v --resolve foo.bar.com:80:54.171.67.92 foo.bar.com/foo
* Added foo.bar.com:80:54.171.67.92 to DNS cache
* Hostname foo.bar.com was found in DNS cache
*   Trying 54.171.67.92...
* Connected to foo.bar.com (54.171.67.92) port 80 (#0)
> GET /foo HTTP/1.1
> Host: foo.bar.com
> User-Agent: curl/7.47.0
> Accept: */*
> 
< HTTP/1.1 301 Moved Permanently
< Server: nginx/1.9.13
< Date: Sat, 30 Apr 2016 06:24:53 GMT
< Content-Type: text/html
< Content-Length: 185
< Connection: keep-alive
< Location: https://foo.bar.com/foo
< 
<html>
<head><title>301 Moved Permanently</title></head>
<body bgcolor=""white"">
<center><h1>301 Moved Permanently</h1></center>
<hr><center>nginx/1.9.11</center>
</body>
</html>
* Connection #0 to host foo.bar.com left intact
```
",closed,False,2016-04-30 06:36:32,2016-05-02 16:01:56
contrib,ajohnstone,https://github.com/kubernetes/contrib/issues/872,https://api.github.com/repos/kubernetes/contrib/issues/872,Ingress - nginx - option to specify path to nginx.tmpl ,"The nginx.tmpl is stored in the root of the docker image. It would be ideal to place within a directory and/or specify the path to the nginx.tmpl

An example use case is to override the nginx.tmpl using a configmap.

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-ingress-controller
  labels:
    k8s-app: nginx-ingress-lb
spec:
  replicas: 1
  selector:
    k8s-app: nginx-ingress-lb
  template:
    metadata:
      labels:
        k8s-app: nginx-ingress-lb
        name: nginx-ingress-lb
    spec:
      terminationGracePeriodSeconds: 60      
      containers:
      - image: gcr.io/google_containers/nginx-ingress-controller:0.5
        name: nginx-ingress-lb
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10249
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        # use downward API
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        ports:
        - containerPort: 80
          hostPort: 80
        - containerPort: 443
          hostPort: 4444
        args:
        - /nginx-ingress-controller
        - --default-backend-service=default/default-http-backend
        volumeMounts:
          - mountPath: /etc/nginx-template-tmpl
            name: nginx-template-volume
            readOnly: true
      volumes:
        - name: nginx-template-volume
          configMap:
            name: nginx
            items:
            - key: nginx.tmpl
              path: nginx.tmpl

```

Hard coded here.
https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/nginx/template.go#L46
",closed,False,2016-04-30 12:46:52,2016-05-02 16:01:56
contrib,rakesh-k,https://github.com/kubernetes/contrib/issues/873,https://api.github.com/repos/kubernetes/contrib/issues/873,ansible build for master fails,"Trying to configure the master through ansible but running into failure at the end. Any help is highly appreciated:

```
TASK [master : Enable apiserver] ***********************************************
fatal: [cd3labvdapp002]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Job for kube-apiserver.service failed because the control process exited with error code. See \""**systemctl status kube-apiserver.service**\"" and \""journalctl -xe\"" for details.\n""}

NO MORE HOSTS LEFT *************************************************************
        to retry, use: --limit @cluster.retry

PLAY RECAP *********************************************************************
cd3labvdapp002             : ok=70   changed=1    unreachable=0    failed=1

[rakeshk@cd3labvdapp002 ansible]$ systemctl status kube-apiserver.service
● kube-apiserver.service - Kubernetes API Server
   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
   Active: activating (start) since Sat 2016-04-30 09:31:52 EDT; 18s ago
     Docs: https://github.com/GoogleCloudPlatform/kubernetes
 Main PID: 12035 (kube-apiserver)
   Memory: 7.6M
   CGroup: /system.slice/kube-apiserver.service
           └─12035 /usr/bin/kube-apiserver --logtostderr=true --v=0 --etcd-servers=http://cd3labvdapp003:2379 --insecure-bind-address=127.0.0.1 --secure-port=443 --allow...
[rakeshk@cd3labvdapp002 ansible]$

```
",closed,False,2016-04-30 13:37:12,2016-04-30 21:40:52
contrib,ajohnstone,https://github.com/kubernetes/contrib/issues/874,https://api.github.com/repos/kubernetes/contrib/issues/874,"ingress - nginx param - ""dump-nginx—configuration""","Change option to use standard ascii characters
",closed,False,2016-05-01 17:17:56,2016-05-02 00:01:59
contrib,ajohnstone,https://github.com/kubernetes/contrib/pull/875,https://api.github.com/repos/kubernetes/contrib/issues/875,"ingress - nginx - non-ascii character used for option - ""dump-nginx—configuration""","fixes #874
non-ascii character used.
",closed,True,2016-05-01 17:21:19,2016-05-02 00:01:59
contrib,ajohnstone,https://github.com/kubernetes/contrib/issues/876,https://api.github.com/repos/kubernetes/contrib/issues/876,ingress - nginx - default http path rule creates an additional server.,"The default http path rule creates an additional server, with an empty server_name and does not map to ""_"". An Example config.

```
# An Ingress with 2 hosts and 3 endpoints
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: echomap
spec:
  rules:
  - http:
      paths:
      - path: /testpath
        backend:
          serviceName: echoheaders-x
          servicePort: 80
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        backend:
          serviceName: echoheaders-x
          servicePort: 80
  - host: bar.baz.com
    http:
      paths:
      - path: /bar
        backend:
          serviceName: echoheaders-y
          servicePort: 80
      - path: /foo
        backend:
          serviceName: echoheaders-x
          servicePort: 80
```

nginx config

```
    server {
        listen 80;
        ...
        server_name ;
    }
    server {
        listen 80;
        ...
        server_name _;
        ...
    }
    server {
        listen 80;
        ...
        server_name bar.baz.com;
        ...
    }
```

full example.

https://gist.github.com/ajohnstone/4b75473fb2e8937d5fc7144d46aad4c4#file-02-nginx-conf-L2-L296
",closed,False,2016-05-01 22:00:33,2016-05-02 16:01:56
contrib,cescoferraro,https://github.com/kubernetes/contrib/issues/877,https://api.github.com/repos/kubernetes/contrib/issues/877,Basic Authentication,"I want to be able to protect some of my kube-system service (dashboard, kubedash, ...) yet still exposing them with the nginx-controller without having to change thos service's code base.
But cannot find a way to implement basic authentication as described in [the NGING docs](http://nginx.org/en/docs/http/ngx_http_auth_basic_module.html)

Am I missing something?
",closed,False,2016-05-01 23:30:42,2016-05-31 20:43:52
contrib,ajohnstone,https://github.com/kubernetes/contrib/issues/878,https://api.github.com/repos/kubernetes/contrib/issues/878,ingress - nginx - specify namespace of service within rules,"It would be ideal to specify another namespace of a service within the ingress rules.

my use case is to wrap authentication over the logging without providing kubernetes accounts to developers/qas etc.

""ELB -> bitly/oauth2_proxy -> ingress routing -> logging"".

```
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    namespace: default
  spec:
    rules:
    - http:
        paths:
        - backend:
            nameSpace: kube-system
            serviceName: kibana-logging
            servicePort: 5601
          path: /logs
```
",closed,False,2016-05-02 10:30:15,2019-02-12 18:10:58
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/879,https://api.github.com/repos/kubernetes/contrib/issues/879,Cluster-autoscaler - check generic predicates,"kubernetes/kubernetes#24404

Requires that https://github.com/kubernetes/contrib/pull/866 is merged before.

cc: @piosz @fgrzadkowski
",closed,True,2016-05-02 11:17:38,2016-05-04 14:22:53
contrib,simonswine,https://github.com/kubernetes/contrib/pull/880,https://api.github.com/repos/kubernetes/contrib/issues/880,ansible: make the flannel backend configurable,"Needed to  support other backends like aws-vpc, host-gw

@eparis 
",closed,True,2016-05-02 13:14:25,2016-05-02 13:22:41
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/881,https://api.github.com/repos/kubernetes/contrib/issues/881,Moving etcd version from tasks to defaults,"- moved version from `set_fact` in the main task to `defaults/main.yml`
",closed,True,2016-05-02 16:38:05,2016-05-02 17:14:01
contrib,david-mcmahon,https://github.com/kubernetes/contrib/issues/882,https://api.github.com/repos/kubernetes/contrib/issues/882,Add release-note-experimental to release-note label checking,"Can we add this one to the mix too?  More probably coming. 
",closed,False,2016-05-02 18:02:59,2016-06-01 23:15:08
contrib,adamschaub,https://github.com/kubernetes/contrib/issues/883,https://api.github.com/repos/kubernetes/contrib/issues/883,Recent Sky DNS pod breaks vagrant deployment,"[1] added some templating to skydns-rc.yml.in (lines [2]) which breaks the vagrant deployment. It depends on facts collected by Salt.

`TASK [kubernetes-addons : DNS | Install Template from converted saltfile] ******
fatal: [kube-node-1]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""AnsibleUndefinedVariable: ERROR! 'grains' is undefined""}
fatal: [kube-master]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""AnsibleUndefinedVariable: ERROR! 'grains' is undefined""}
`
[3] removes the offending configuration and should be merged shortly.

cc/ @danehans @eparis 

[1] https://github.com/kubernetes/kubernetes/pull/24124
[2] https://github.com/mikedanese/kubernetes/blob/fa9d79df75d0a5326fb6ca72308ebbaf5bde603e/cluster/addons/dns/skydns-rc.yaml.in#L22-L24
[3] https://github.com/kubernetes/kubernetes/pull/25029
",closed,False,2016-05-02 18:31:32,2016-05-02 19:03:25
contrib,joncak,https://github.com/kubernetes/contrib/issues/884,https://api.github.com/repos/kubernetes/contrib/issues/884,SSL as a Reverse Proxy on Nginx Controller ? ,"Hi, i'm using the nginx and GCE official ingress controllers, but the reverse fot the backend service  is http only, it is posible a proxy pass with https support or maybe using the redirect.

`proxy_redirect      http://localhost:8080 https://secure.domain.com;`
",closed,False,2016-05-02 19:08:08,2016-06-02 21:53:57
contrib,wstrange,https://github.com/kubernetes/contrib/issues/885,https://api.github.com/repos/kubernetes/contrib/issues/885,Standardise or document Ingress path expressions across implementations,"Path expressions in the Ingress are provider specific - so a path expression (say /foo/) that works in one provider does not work the same in another.  

For example:
path: /foo/

Seems to result in different behaviour on GKE vs.  the nginx controller. 

If it is not realistic to standardise paths, the providers should document the interpretation of the path expression ( example: does /foo/ match  /foo/bar?)
",closed,False,2016-05-02 21:12:22,2017-04-04 15:16:45
contrib,rakesh-k,https://github.com/kubernetes/contrib/issues/886,https://api.github.com/repos/kubernetes/contrib/issues/886,add-ons don't get installed properly,"Must say up front - I am trying the install on redhat. Having said that after correcting the paths little bit I am able to get the cluster up and running. But for some reason the add-ons don't get installed even though I see the activities  related to grafana, heapster, and influxdb in the log file. 

This is what I get after the install:

```
[rakeshk@cd3labvdapp002 ~]$ kubectl cluster-info
Kubernetes master is running at http://localhost:8080
```

From the log:

```

2016-05-02 16:22:51,989 p=18914 u=rakeshk |  task path: /home/rakeshk/contrib/ansible/roles/kubernetes-addons/tasks/cluster-monitoring.yml:11
2016-05-02 16:22:53,512 p=18914 u=rakeshk |  ok: [cd3labvdapp002 -> localhost] => (item=grafana-service.yaml) => {""changed"": false, ""checksum_dest"": ""487a140bd7f826b16eec34c6cd293c5e9a04f774"", ""checksum_src"": ""487a140bd7f826b16eec34c6cd293c5e9a04f774"", ""dest"": ""/tmp/kubernetes/addons/cluster-monitoring/grafana-service.yaml.j2"", ""gid"": 1012, ""group"": ""rakeshk"", ""item"": ""grafana-service.yaml"", ""md5sum"": ""8b492d5ee6bf197684d9c3cedd86201d"", ""mode"": ""0664"", ""msg"": ""OK (431 bytes)"", ""owner"": ""rakeshk"", ""size"": 431, ""src"": ""/tmp/tmpsHRowM"", ""state"": ""file"", ""uid"": 1012, ""url"": ""https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/addons/cluster-monitoring/influxdb/grafana-service.yaml""}
2016-05-02 16:22:53,548 p=18914 u=rakeshk |  ok: [cd3labvdapp002 -> localhost] => (item=heapster-controller.yaml) => {""changed"": false, ""checksum_dest"": ""d65ae026a787914518a8b9227b5e16c024d11fb8"", ""checksum_src"": ""680ed4f1282025987cad1ff8bebef9bf1c62ae8e"", ""dest"": ""/tmp/kubernetes/addons/cluster-monitoring/heapster-controller.yaml.j2"", ""gid"": 1012, ""group"": ""rakeshk"", ""item"": ""heapster-controller.yaml"", ""md5sum"": ""e0b52e80a2bb0e8ec74b925412305629"", ""mode"": ""0664"", ""msg"": ""OK (3688 bytes)"", ""owner"": ""rakeshk"", ""size"": 3688, ""src"": ""/tmp/tmpQpRU3R"", ""state"": ""file"", ""uid"": 1012, ""url"": ""https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/addons/cluster-monitoring/influxdb/heapster-controller.yaml""}
2016-05-02 16:22:53,583 p=18914 u=rakeshk |  ok: [cd3labvdapp002 -> localhost] => (item=heapster-service.yaml) => {""changed"": false, ""checksum_dest"": ""fa6a6611afc134933df625660e5044f5a485d077"", ""checksum_src"": ""fa6a6611afc134933df625660e5044f5a485d077"", ""dest"": ""/tmp/kubernetes/addons/cluster-monitoring/heapster-service.yaml.j2"", ""gid"": 1012, ""group"": ""rakeshk"", ""item"": ""heapster-service.yaml"", ""md5sum"": ""f455a4c445fb61865170494cbc0d872e"", ""mode"": ""0664"", ""msg"": ""OK (258 bytes)"", ""owner"": ""rakeshk"", ""size"": 258, ""src"": ""/tmp/tmpjYya6a"", ""state"": ""file"", ""uid"": 1012, ""url"": ""https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/addons/cluster-monitoring/influxdb/heapster-service.yaml""}
2016-05-02 16:22:53,616 p=18914 u=rakeshk |  ok: [cd3labvdapp002 -> localhost] => (item=influxdb-grafana-controller.yaml) => {""changed"": false, ""checksum_dest"": ""2c8d5bbd5117cbaab7926d9bb04976c2c8c781b1"", ""checksum_src"": ""2c8d5bbd5117cbaab7926d9bb04976c2c8c781b1"", ""dest"": ""/tmp/kubernetes/addons/cluster-monitoring/influxdb-grafana-controller.yaml.j2"", ""gid"": 1012, ""group"": ""rakeshk"", ""item"": ""influxdb-grafana-controller.yaml"", ""md5sum"": ""24e5ba5b0d0bc75ec64728f8aed3e7d7"", ""mode"": ""0664"", ""msg"": ""OK (2423 bytes)"", ""owner"": ""rakeshk"", ""size"": 2423, ""src"": ""/tmp/tmp7_aGKc"", ""state"": ""file"", ""uid"": 1012, ""url"": ""https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/addons/cluster-monitoring/influxdb/influxdb-grafana-controller.yaml""}
2016-05-02 16:22:53,650 p=18914 u=rakeshk |  ok: [cd3labvdapp002 -> localhost] => (item=influxdb-service.yaml) => {""changed"": false, ""checksum_dest"": ""bc359e21f8779fe54a15c4ca4616f4d90f2d6bca"", ""checksum_src"": ""bc359e21f8779fe54a15c4ca4616f4d90f2d6bca"", ""dest"": ""/tmp/kubernetes/addons/cluster-monitoring/influxdb-service.yaml.j2"", ""gid"": 1012, ""group"": ""rakeshk"", ""item"": ""influxdb-service.yaml"", ""md5sum"": ""1d195f93019f20ffece7a0d5c177f06a"", ""mode"": ""0664"", ""msg"": ""OK (351 bytes)"", ""owner"": ""rakeshk"", ""size"": 351, ""src"": ""/tmp/tmpFcdZ2E"", ""state"": ""file"", ""uid"": 1012, ""url"": ""https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/addons/cluster-monitoring/influxdb/influxdb-service.yaml""}

```

...

```

2016-05-02 16:22:56,194 p=18914 u=rakeshk |  TASK [kubernetes-addons : MONITORING | Install template from converted saltfile] ***
2016-05-02 16:22:56,195 p=18914 u=rakeshk |  task path: /home/rakeshk/contrib/ansible/roles/kubernetes-addons/tasks/cluster-monitoring.yml:55
2016-05-02 16:22:59,649 p=18914 u=rakeshk |  ok: [cd3labvdapp002] => (item=grafana-service.yaml) => {""changed"": false, ""gid"": 0, ""group"": ""root"", ""item"": ""grafana-service.yaml"", ""mode"": ""0755"", ""owner"": ""root"", ""path"": ""/etc/kubernetes/addons/cluster-monitoring/grafana-service.yaml"", ""size"": 431, ""state"": ""file"", ""uid"": 0}
2016-05-02 16:22:59,685 p=18914 u=rakeshk |  ok: [cd3labvdapp002] => (item=heapster-controller.yaml) => {""changed"": false, ""gid"": 0, ""group"": ""root"", ""item"": ""heapster-controller.yaml"", ""mode"": ""0755"", ""owner"": ""root"", ""path"": ""/etc/kubernetes/addons/cluster-monitoring/heapster-controller.yaml"", ""size"": 3119, ""state"": ""file"", ""uid"": 0}
2016-05-02 16:22:59,723 p=18914 u=rakeshk |  ok: [cd3labvdapp002] => (item=heapster-service.yaml) => {""changed"": false, ""gid"": 0, ""group"": ""root"", ""item"": ""heapster-service.yaml"", ""mode"": ""0755"", ""owner"": ""root"", ""path"": ""/etc/kubernetes/addons/cluster-monitoring/heapster-service.yaml"", ""size"": 258, ""state"": ""file"", ""uid"": 0}
2016-05-02 16:22:59,764 p=18914 u=rakeshk |  ok: [cd3labvdapp002] => (item=influxdb-grafana-controller.yaml) => {""changed"": false, ""gid"": 0, ""group"": ""root"", ""item"": ""influxdb-grafana-controller.yaml"", ""mode"": ""0755"", ""owner"": ""root"", ""path"": ""/etc/kubernetes/addons/cluster-monitoring/influxdb-grafana-controller.yaml"", ""size"": 2423, ""state"": ""file"", ""uid"": 0}
2016-05-02 16:22:59,799 p=18914 u=rakeshk |  ok: [cd3labvdapp002] => (item=influxdb-service.yaml) => {""changed"": false, ""gid"": 0, ""group"": ""root"", ""item"": ""influxdb-service.yaml"", ""mode"": ""0755"", ""owner"": ""root"", ""path"": ""/etc/kubernetes/addons/cluster-monitoring/influxdb-service.yaml"", ""size"": 351, ""state"": ""file"", ""uid"": 0}
```
",closed,False,2016-05-02 21:55:33,2016-08-23 17:38:34
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/887,https://api.github.com/repos/kubernetes/contrib/issues/887,Fix nginx ingress unittests.,"Bad merge between https://github.com/kubernetes/contrib/pull/849 and #766 
@aledbf 
",closed,True,2016-05-02 22:59:10,2016-05-02 23:15:17
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/888,https://api.github.com/repos/kubernetes/contrib/issues/888,[wip] Pets,"Start growing some pets. Needs the petset controller from https://github.com/kubernetes/kubernetes/pull/24912.
@smarterclayton 
",closed,True,2016-05-03 02:25:19,2016-05-18 21:02:48
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/889,https://api.github.com/repos/kubernetes/contrib/issues/889,Bump up nginx controller to 0.61,"@aledbf 
0.61 contains https://github.com/kubernetes/contrib/pull/849
",closed,True,2016-05-03 17:28:48,2016-05-03 18:47:37
contrib,ixdy,https://github.com/kubernetes/contrib/pull/890,https://api.github.com/repos/kubernetes/contrib/issues/890,Add a flag to control which domains we send shame emails to,"Intent is to make it easy to add permitted domains like redhat.com, as discussed in https://github.com/kubernetes/test-infra/pull/11#discussion_r61953400, all without having to rebuild everything.

cc @kubernetes/sig-testing 
",closed,True,2016-05-03 21:27:10,2016-05-03 22:20:25
contrib,apelisse,https://github.com/kubernetes/contrib/pull/891,https://api.github.com/repos/kubernetes/contrib/issues/891,Close stale pr,"Close pull-request that have not been modified in a while

Fixes #481
",closed,True,2016-05-03 21:47:44,2016-07-15 18:44:35
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/892,https://api.github.com/repos/kubernetes/contrib/issues/892,create an example on how to run startup-scripts with a daemonset,"cc @aronchick @fabioy @roberthbailey

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/892)

<!-- Reviewable:end -->
",closed,True,2016-05-03 23:29:30,2017-04-07 22:08:22
contrib,aledbf,https://github.com/kubernetes/contrib/pull/893,https://api.github.com/repos/kubernetes/contrib/issues/893,NGINX Ingress controller - Add example of custom error pages in nginx ingress controller,,closed,True,2016-05-04 03:01:13,2016-05-08 21:35:34
contrib,gouyang,https://github.com/kubernetes/contrib/pull/894,https://api.github.com/repos/kubernetes/contrib/issues/894,[ansible] Make sure kubelet is restarted,,closed,True,2016-05-04 03:21:38,2016-05-04 03:23:14
contrib,gouyang,https://github.com/kubernetes/contrib/pull/895,https://api.github.com/repos/kubernetes/contrib/issues/895,[ansible] Make sure kubelet service is up.,"When kubelet service is stopped, run the playbook does not get the
service up.
",closed,True,2016-05-04 03:32:02,2016-05-05 01:13:32
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/896,https://api.github.com/repos/kubernetes/contrib/issues/896,Allow to open cadvisor 4194/tcp port when deploying nodes.,"If the 4194/tcp port is not opened when deploying nodes, some e2e tests proxing to cadvisor fail with:

```
Error: 'dial tcp 10.8.52.190:4194: getsockopt: no route to host'
Trying to reach: 'http://10.8.52.190:4194/containers/'
```

This patch does not change the current list of ports opened publicly. 
It just provides to CI operator a choice to decide to open the port when running e2e tests for:

*\* `[k8s.io] Proxy version v1 [It] should proxy to cadvisor`
*\* `[k8s.io] Proxy version v1 [It] should proxy to cadvisor using proxy subresource`

Not sure if keeping the port closed to public by default is intended. Maybe it should be opened by default and the `open_cadvisor_port` variable removed.
",closed,True,2016-05-04 11:11:16,2016-05-04 14:30:54
contrib,databus23,https://github.com/kubernetes/contrib/issues/897,https://api.github.com/repos/kubernetes/contrib/issues/897,Nginx Ingress Controller - Option to disable interception of errors,"It seems to me right now there is no way to configure the nginx ingress controller to **not** intercept errors and serve them from the default/error backend.

In our case this is problematic. Our backends serve meaningful error pages for 500 errors which contain exception ids which help us debugging problems when customers create support requests.
We also have a JSON API backend where 404 error contain additional information that help api clients to fail more gracefully.

How about adding an option to (potentially partially) disable the _error interception_ ?
I can submit an initial PR of this would be considered useful.
",closed,False,2016-05-04 11:34:06,2016-05-28 01:10:16
contrib,aledbf,https://github.com/kubernetes/contrib/pull/898,https://api.github.com/repos/kubernetes/contrib/issues/898,NGINX Ingress controller Sort locations,"Location / must be the last one
",closed,True,2016-05-04 11:56:14,2016-05-04 15:38:01
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/899,https://api.github.com/repos/kubernetes/contrib/issues/899,Etcd refactor,"The refactor breaks down the etcd role into smaller component plays that could be reused in larger playbooks, such as cluster wide, deployments, upgrades, etc. If all the roles are more modular it would make additional playbooks much easier to create and this gives us a consistent pattern to follow with the other roles. 

Hit an ansible bug: https://github.com/ansible/ansible/issues/13818 that effects ansible > 2.0, so I rolled back to 1.9.4, and this works. The bug makes ansible unable to find the templates/files in the role when run from Vagrant.
- Related to #793
- Should make #852 easier
- An example of what #838 should be

---

Would appreciate feedback @danehans @rutsky @adamschaub 

Refactor of etcd role to be modeled after [openstack-ansible](https://github.com/openstack/openstack-ansible/tree/kilo) and [hpe-helion](https://github.com/hpe-helion-os/helion-ansible). Confirmed on Virtualbox + CoreOS and Virtualbox + Centos
- I think we should always use the binaries from github, regardless of OS, so we can always control the version of etcd present. CoreOS comes with etcd, we would have to uninstall and install the specified version for CoreOS. Where Centos uses the [generic package installer](http://docs.ansible.com/ansible/package_module.html) as seen [here](https://github.com/kubernetes/contrib/blob/master/ansible/roles/etcd/tasks/main.yml#L2). It does not support versions, we currently just specify `latest` and end up with the latest copy of etcd that is in yum/apt-get. (A very unlucky person could be running the install script and the version served by yum/apt-get might get changed during their deployment, leaving them with different versions of etcd) Ubuntu is the only one using the binaries right now, seen [here](https://github.com/kubernetes/contrib/blob/master/ansible/roles/etcd/tasks/main.yml#L11). Using binaries will let anyone regardless of OS to use a specific version of etcd.

Design Question:
- Not sure where `deploy.yml` should live. I put it one level up from `tasks` right now since its wraps a bunch of `etcd/tasks` playbooks together
- How granular do we want bottom level playbooks that live in the task file.
  - `install.yml` has all install instructions regardless of OS.
  - `install.yml` just cycles through each OS supported, and calls plays like `install-coreos.yml` `install-ubuntu.yml`
    - Right now I'm breaking it out if there is more than 2 statements that apply to the same `when`, see `coreos-start.yml` and `coreos-stop.yml`. I see this is being a bit more manageable down the road at the cost of the directory getting a little crowded.
",closed,True,2016-05-04 15:52:06,2016-05-13 15:25:05
contrib,apelisse,https://github.com/kubernetes/contrib/pull/900,https://api.github.com/repos/kubernetes/contrib/issues/900,Make verify-all.sh verbose for travis,"Currently, you can't see why the script fails from the output on travis,
so you have to re-run the command locally. Make the script verbose.
",closed,True,2016-05-04 23:58:23,2016-05-09 15:04:14
contrib,piosz,https://github.com/kubernetes/contrib/issues/901,https://api.github.com/repos/kubernetes/contrib/issues/901,[submit-queue] Wrong lgtm label handling,"Submit Queue is removing lgtm label in https://github.com/kubernetes/kubernetes/pull/24253 despite that the PR didn't change. 

From its logs it seems something is broken:

```
I0505 07:05:31.070833       1 lgtm_after_commit.go:78] PR: 24253 lgtm:2016-04-29 07:48:10 +0000 UTC  lastModified:2016-05-05 06:57:29 +0000 UTC
I0505 07:05:31.070935       1 github.go:1327] Commenting ""PR changed after LGTM, removing LGTM."" in 24253
I0505 07:05:31.300853       1 github.go:661] Removing label ""lgtm"" to PR 24253
I0505 07:05:31.754954       1 github.go:929] PR 24253 setting ""Submit Queue"" Github status to ""Github CI tests are not green.""
I0505 07:35:29.196452       1 lgtm_after_commit.go:78] PR: 24253 lgtm:2016-04-29 07:48:10 +0000 UTC  lastModified:2016-05-05 06:57:29 +0000 UTC
I0505 07:35:29.196529       1 github.go:1327] Commenting ""PR changed after LGTM, removing LGTM."" in 24253
I0505 07:35:29.402405       1 github.go:661] Removing label ""lgtm"" to PR 24253
I0505 07:35:29.800642       1 github.go:929] PR 24253 setting ""Submit Queue"" Github status to ""PR does not have LGTM.""
```
",closed,False,2016-05-05 07:55:31,2017-04-27 14:04:21
contrib,zoidbergwill,https://github.com/kubernetes/contrib/pull/902,https://api.github.com/repos/kubernetes/contrib/issues/902,Update Markdown for code segments,,closed,True,2016-05-05 09:11:42,2016-05-09 19:17:27
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/903,https://api.github.com/repos/kubernetes/contrib/issues/903,Cluster-autoscaler: Dockerfile,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-05 09:38:52,2016-05-05 09:59:06
contrib,simonswine,https://github.com/kubernetes/contrib/issues/904,https://api.github.com/repos/kubernetes/contrib/issues/904,[ingress/controllers/nginx] per service configuration proposal,"I started using nginx-ingress for my private cluster a couple of days ago. I can already see the strong need to have per service configuration options. They would allow me to run only one set of nginx-ingress controllers in your cluster, but different configurations. I took a short look at the issues and I can see the need for something like this is definitely there.

I am thinking of uses cases like these:
- authentication with http auth (#877)
- custom-error pages enable/disable, range of error codes to intercept, link custom error-backend (#897)
- HSTS enable/disable 
- HTTPS enforcement (301 redirect) (#784) (#877)
- there are probably a lot more

All of these could be configured on nginx `location` - level (makes no sense for HSTS)

My plan for implementing that would be:
- create an annotation for the service object which links a configmap
- read and watch the linked config object and respect them when building the nginx.conf

How do you think about that?

I would be willing to do the implementation, but probably need some help around watching config objects.
",closed,False,2016-05-05 10:18:14,2016-06-29 14:59:26
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/905,https://api.github.com/repos/kubernetes/contrib/issues/905,Cluster-autoscaler: different base image + yaml,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-05 13:22:40,2016-05-05 13:35:53
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/906,https://api.github.com/repos/kubernetes/contrib/issues/906,Cluster-autoscaler: relax url format,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-05 13:45:12,2016-05-05 14:30:17
contrib,rakesh-k,https://github.com/kubernetes/contrib/issues/907,https://api.github.com/repos/kubernetes/contrib/issues/907,how to use uninstall playbook,"There is an uninstall playbook under playbook/adhoc - probably it is meant to uninstall packages installed by ansible. Question is how do I use it? (of course, I am an ansible newbie)
",closed,False,2016-05-05 18:54:37,2016-05-05 19:22:44
contrib,aledbf,https://github.com/kubernetes/contrib/pull/908,https://api.github.com/repos/kubernetes/contrib/issues/908,Update haproxy image,,closed,True,2016-05-05 21:32:02,2016-05-08 21:32:58
contrib,spxtr,https://github.com/kubernetes/contrib/pull/909,https://api.github.com/repos/kubernetes/contrib/issues/909,"Retest stale tests after 96 hours, not 48.","@eparis assuming this works I think it should go in ASAP. We currently can't merge because Jenkins is overloaded and it takes too long for a test to get through the Jenkins queue.
",closed,True,2016-05-05 22:53:16,2016-05-09 15:03:55
contrib,jgmize,https://github.com/kubernetes/contrib/pull/910,https://api.github.com/repos/kubernetes/contrib/issues/910,Allow environment variable usage in Makefile,,closed,True,2016-05-06 02:25:39,2016-05-06 13:24:14
contrib,gmarek,https://github.com/kubernetes/contrib/pull/911,https://api.github.com/repos/kubernetes/contrib/issues/911,Make WeakStable blocking - it was running OK for a couple weeks,"cc @kubernetes/sig-testing 
",closed,True,2016-05-06 09:16:37,2016-05-06 13:36:59
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/912,https://api.github.com/repos/kubernetes/contrib/issues/912,Cluster-autoscaler: mig node fix,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-06 10:15:52,2016-05-06 10:50:09
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/913,https://api.github.com/repos/kubernetes/contrib/issues/913,Cluster autoscaler: Fix zone for operation,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-06 11:35:06,2016-05-06 11:46:45
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/914,https://api.github.com/repos/kubernetes/contrib/issues/914,Cluster-autoscaler: don't grow MIG beyond max size,"cc: @fgrzadkowski @piosz 
",closed,True,2016-05-06 12:56:44,2016-05-06 13:15:22
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/915,https://api.github.com/repos/kubernetes/contrib/issues/915,Automatically add retest-not-required to PRs that only touch docs,"cc @fejta @eparis 
",closed,False,2016-05-06 17:47:27,2016-07-19 22:09:35
contrib,rakesh-k,https://github.com/kubernetes/contrib/issues/916,https://api.github.com/repos/kubernetes/contrib/issues/916,duplicate iptables entries with subsequent runs of ansible,"I was hoping that ansible runs would be idempotent but I am noticing that every run keeps inserting rules in iptables without checking for its existence. This is a sample output of `iptables -L` :

```

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
ACCEPT     all  --  anywhere             anywhere             /* flannel subnet */
```
",closed,False,2016-05-06 19:35:26,2018-02-14 01:01:12
contrib,eparis,https://github.com/kubernetes/contrib/pull/917,https://api.github.com/repos/kubernetes/contrib/issues/917,Turn off blunderbuss,"Currently the github api is lying to us!  some time around 3:05 eastern
github started claiming that all issues were unassigned. See for example
the output about PR 16000 from

https://api.github.com/repos/kubernetes/kubernetes/issues?direction=asc&sort=created&state=open&per_page=100&page=13

```
  {
    ""url"": ""https://api.github.com/repos/kubernetes/kubernetes/issues/16000"",
    ""repository_url"": ""https://api.github.com/repos/kubernetes/kubernetes"",
    ""labels_url"": ""https://api.github.com/repos/kubernetes/kubernetes/issues/16000/labels{/name}"",
    ""comments_url"": ""https://api.github.com/repos/kubernetes/kubernetes/issues/16000/comments"",
    ""events_url"": ""https://api.github.com/repos/kubernetes/kubernetes/issues/16000/events"",
    ""html_url"": ""https://github.com/kubernetes/kubernetes/pull/16000"",
    ""id"": 112490923,
    ""number"": 16000,
    ""title"": ""Changed docker storage driver to overlayfs"",
    ""user"": {
      ""login"": ""aanm"",
      ""id"": 5714066,
      ""avatar_url"": ""https://avatars.githubusercontent.com/u/5714066?v=3"",
      ""gravatar_id"": """",
      ""url"": ""https://api.github.com/users/aanm"",
      ""html_url"": ""https://github.com/aanm"",
      ""followers_url"": ""https://api.github.com/users/aanm/followers"",
      ""following_url"": ""https://api.github.com/users/aanm/following{/other_user}"",
      ""gists_url"": ""https://api.github.com/users/aanm/gists{/gist_id}"",
      ""starred_url"": ""https://api.github.com/users/aanm/starred{/owner}{/repo}"",
      ""subscriptions_url"": ""https://api.github.com/users/aanm/subscriptions"",
      ""organizations_url"": ""https://api.github.com/users/aanm/orgs"",
      ""repos_url"": ""https://api.github.com/users/aanm/repos"",
      ""events_url"": ""https://api.github.com/users/aanm/events{/privacy}"",
      ""received_events_url"": ""https://api.github.com/users/aanm/received_events"",
      ""type"": ""User"",
      ""site_admin"": false
    },
    ""labels"": [
      {
        ""url"": ""https://api.github.com/repos/kubernetes/kubernetes/labels/cla:%20yes"",
        ""name"": ""cla: yes"",
        ""color"": ""bfe5bf""
      },
      {
        ""url"": ""https://api.github.com/repos/kubernetes/kubernetes/labels/needs-ok-to-merge"",
        ""name"": ""needs-ok-to-merge"",
        ""color"": ""ededed""
      },
      {
        ""url"": ""https://api.github.com/repos/kubernetes/kubernetes/labels/release-note-label-needed"",
        ""name"": ""release-note-label-needed"",
        ""color"": ""db5a64""
      },
      {
        ""url"": ""https://api.github.com/repos/kubernetes/kubernetes/labels/size/XS"",
        ""name"": ""size/XS"",
        ""color"": ""009900""
      }
    ],
    ""state"": ""open"",
    ""locked"": false,
    ""assignee"": null,
    ""milestone"": null,
    ""comments"": 17,
    ""created_at"": ""2015-10-21T01:01:16Z"",
    ""updated_at"": ""2016-05-03T12:58:31Z"",
    ""closed_at"": null,
    ""pull_request"": {
      ""url"": ""https://api.github.com/repos/kubernetes/kubernetes/pulls/16000"",
      ""html_url"": ""https://github.com/kubernetes/kubernetes/pull/16000"",
      ""diff_url"": ""https://github.com/kubernetes/kubernetes/pull/16000.diff"",
      ""patch_url"": ""https://github.com/kubernetes/kubernetes/pull/16000.patch""
    },
    ""body"": ""Changed docker storage driver to `overlayfs` so the creation and removal of docker containers could be faster.\r\n\r\nDepends on https://github.com/kubernetes/kubernetes/pull/15998\r\n\r\nSigned-off-by: André Martins <aanm90@gmail.com>""
  },
```

This turns off the auto-assigner. It should be reverted when github stops lyin' bro!
",closed,True,2016-05-06 20:19:38,2016-05-07 13:55:30
contrib,rakesh-k,https://github.com/kubernetes/contrib/issues/918,https://api.github.com/repos/kubernetes/contrib/issues/918,no endpoint for kube-dns,"Managed to get the install work without error but the kube-dns seem to be configured wrong. This is the error that I get in the j`ournalctl -xe`

`Couldn't find an endpoint for kube-system/kube-dns:dns: missing service`

Kube-dns pod seems to be there but not in running state:

```
[rakeshk@cd3labvdapp002 ansible]$ kubectl get pod --all-namespaces
NAMESPACE     NAME                                   READY     STATUS             RESTARTS   AGE
kube-system   elasticsearch-logging-v1-gmbpy         1/1       Running            0          50m
kube-system   elasticsearch-logging-v1-uqm8g         1/1       Running            0          50m
kube-system   fluentd-elasticsearch-cd3labvdapp002   1/1       Running            1          50m
kube-system   fluentd-elasticsearch-cd3labvdapp003   1/1       Running            0          50m
kube-system   fluentd-elasticsearch-cd3labvdapp004   1/1       Running            0          50m
kube-system   fluentd-elasticsearch-cd3labvdapp005   1/1       Running            0          50m
kube-system   kibana-logging-v1-g44aj                0/1       CrashLoopBackOff   14         50m
kube-system   kube-dns-v11-26f48                     2/4       CrashLoopBackOff   17         18m
kube-system   kubedash-xhoa3                         1/1       Running            0          50m
kube-system   kubernetes-dashboard-v1.0.1-tqoi7      1/1       Running            0          50m
kube-system   monitoring-influxdb-grafana-v3-1dcfe   2/2       Running            0          50m
```

```
[rakeshk@cd3labvdapp002 ansible]$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""2"", GitVersion:""v1.2.0"", GitCommit:""b57e8bdc7c3871e3f6077b13c42d205ae1813fbd"", GitTreeState:""clean""}
Server Version: version.Info{Major:""1"", Minor:""2"", GitVersion:""v1.2.0"", GitCommit:""b57e8bdc7c3871e3f6077b13c42d205ae1813fbd"", GitTreeState:""clean""}
[rakeshk@cd3labvdapp002 ansible]$ kubectl get e
endpoints  event
[rakeshk@cd3labvdapp002 ansible]$ kubectl get endpoints
NAME         ENDPOINTS           AGE
kubernetes   172.27.36.101:443   1h
[rakeshk@cd3labvdapp002 ansible]$ kubectl get endpoints --namespace=""kube-system""
NAME                    ENDPOINTS                             AGE
elasticsearch-logging   192.160.29.4:9200,192.160.9.3:9200    53m
heapster                <none>                                53m
kibana-logging                                                53m
kube-dns                                                      53m
kubedash                192.160.61.3:8289                     53m
kubernetes-dashboard    192.160.85.3:9090                     53m
monitoring-grafana      192.160.29.3:3000                     53m
monitoring-influxdb     192.160.29.3:8083,192.160.29.3:8086   53m
[rakeshk@cd3labvdapp002 ansible]$
```
",closed,False,2016-05-06 20:48:51,2017-10-24 13:13:51
contrib,rmmh,https://github.com/kubernetes/contrib/pull/919,https://api.github.com/repos/kubernetes/contrib/issues/919,Disable Angular debug information for a ~20% speed boost.,"https://docs.angularjs.org/guide/production
",closed,True,2016-05-06 21:17:05,2016-05-06 21:23:50
contrib,cjcullen,https://github.com/kubernetes/contrib/issues/920,https://api.github.com/repos/kubernetes/contrib/issues/920,Merge bot could be more helpful when flakes require a retest,"See https://github.com/kubernetes/kubernetes/issues/25212

I hardly ever get the syntax right for the ""@k8s-bot please retest this"" command. It would be nice if a failed test gave me the exact command (without the actual flake issue) that I needed to get a retest.

I was also confused by the behavior from the issue linked above. My command did trigger a retest, but @k8s-merge-robot didn't agree. To further confuse me, my command to @k8s-bot was deleted.
",closed,False,2016-05-06 22:11:28,2018-02-13 16:53:09
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/921,https://api.github.com/repos/kubernetes/contrib/issues/921,Petset examples,"Broken out from https://github.com/kubernetes/contrib/pull/888
Requires https://github.com/kubernetes/kubernetes/pull/23567, https://github.com/kubernetes/kubernetes/pull/25284

Main motivation:
1. Don't run a custom pid1
2. Split up install and bootstrap into init containers
3. Only use custom images when we absolutely need to

I don't think it matters where we put this, but I'm open to moving it into examples
",closed,True,2016-05-07 01:02:28,2016-05-24 18:19:52
contrib,andreamelloncelli,https://github.com/kubernetes/contrib/pull/922,https://api.github.com/repos/kubernetes/contrib/issues/922,Multimaster vagrant,"Added an environment variable to choose the number of masters to Vagrantfile.
",closed,True,2016-05-07 14:52:14,2016-05-07 15:05:07
contrib,andreamelloncelli,https://github.com/kubernetes/contrib/pull/923,https://api.github.com/repos/kubernetes/contrib/issues/923,Added multimaster environment variable,,closed,True,2016-05-07 15:09:00,2016-06-07 19:28:33
contrib,ajcrowe,https://github.com/kubernetes/contrib/pull/924,https://api.github.com/repos/kubernetes/contrib/issues/924,GCE Ingress Controller - Added flag for healthcheck interval,"When using GCE ingress and the default 1 second interval you can end up generating a large number of requests to your services. This is made worse if you have a large cluster as it will be receiving the health check to each node for each service every second, it wouldn't be unrealistic for you to end up with 100rps from just health checks in a large cluster.

I believe there are discussions to have more granular ways to configure healthcheck and other params, but for now this added a flag to allow the interval to be increased.

Much like the `--health-check-path` this is simply `--health-check-internval`

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/924)

<!-- Reviewable:end -->
",closed,True,2016-05-07 16:02:22,2016-11-15 19:19:53
contrib,eparis,https://github.com/kubernetes/contrib/pull/925,https://api.github.com/repos/kubernetes/contrib/issues/925,Re-enable blunderbuss,"github fixed their API
",closed,True,2016-05-07 23:11:54,2016-05-09 15:02:33
contrib,aledbf,https://github.com/kubernetes/contrib/pull/926,https://api.github.com/repos/kubernetes/contrib/issues/926,[nginx-ingress-controller] Custom errors should be optional,"Without this change the errors in the upstream server are redirected to the default backend.

fixes #897
",closed,True,2016-05-08 23:05:13,2016-05-28 01:10:16
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/927,https://api.github.com/repos/kubernetes/contrib/issues/927,kubernetes-addons: cluster/saltbase/salt/kube-addons changed to cluster/addons/addon-manager,"cluster/saltbase/salt/kube-addons/namespace.yaml -> cluster/addons/addon-manager/namespace.yaml
",closed,True,2016-05-09 05:52:04,2016-05-09 16:55:18
contrib,gmarek,https://github.com/kubernetes/contrib/pull/928,https://api.github.com/repos/kubernetes/contrib/issues/928,Make WeakStable even weaker,"For some reason failures tend to go in pairs - we should ignore one additional failure.
",closed,True,2016-05-09 07:21:15,2016-05-10 12:46:22
contrib,stephenrlouie,https://github.com/kubernetes/contrib/issues/929,https://api.github.com/repos/kubernetes/contrib/issues/929,Vagrant environment disregard for contrib/ansible/group_vars/all.yml,"Vagrant: 1.8.1
Ansible: 2.0.1.0

Virtual Environment is not using `contrib/ansible/group_vars/all.yml` in its deployment. The [ansible](http://docs.ansible.com/ansible/intro_inventory.html) docs say it is supported:
- Tip: In Ansible 1.2 or later the group_vars/ and host_vars/ directories can exist in the playbook directory OR the inventory directory. If both paths exist, variables in the playbook directory will override variables set in the inventory directory.

Steps to prove it:
1. Add `pypy_download_url_base: ""http://www.google.com""` to `contrib/ansible/group_vars/all.yml` (This should cause the build to fail)
2. Start the vagrant environment and it will work, even though we just gave a bad download path for pypy

Current State:
We work around it by adding the variables into the inventory file, see the python interpreter for coreos in the [Vagrantfile](https://github.com/kubernetes/contrib/blob/master/ansible/vagrant/Vagrantfile#L240) which goes against an [ansible design principal](http://docs.ansible.com/ansible/intro_inventory.html#splitting-out-host-and-group-specific-data) of not putting variables in the inventory files. 

We should be uncommenting [this line](https://github.com/kubernetes/contrib/blob/master/ansible/group_vars/all.yml#L33) in `group_vars/all.yml`

I'll have a follow up PR soon
",closed,False,2016-05-09 15:57:25,2018-02-13 22:59:11
contrib,alanhartless,https://github.com/kubernetes/contrib/issues/930,https://api.github.com/repos/kubernetes/contrib/issues/930,Nginx Ingress Controller - Updating Nginx ConfigMap doesn't trigger nginx.conf update,"If I push an update to nginx's ConfigMap resource, it doesn't seem the nginx.conf is updated anytime ""soon."" I have to push a bogus ingress change to force it to happen. Is there a better way to get it to recognize ConfigMap changes?
",closed,False,2016-05-09 16:18:38,2016-06-02 21:53:57
contrib,simonswine,https://github.com/kubernetes/contrib/issues/931,https://api.github.com/repos/kubernetes/contrib/issues/931,[ansible] problems with external artifacts (e.g. from github),"Currently the latest ansible code fails to deploy a master on CoreOS, because there are artifacts missing, due to ongoing development on the mainline kubernetes github repo:

```
TASK [kubernetes-addons : Make sure the system services namespace exists] ******
fatal: [10.251.0.10]: FAILED! => {""changed"": false, ""dest"": ""/etc/kubernetes/addons/"", ""failed"": true, ""gid"": 0, ""group"": ""root"", ""mode"": ""0755"", ""msg"": ""Request failed"", ""owner"": ""root"", ""response"": ""HTTP Error 404: Not Found"", ""size"": 4096, ""state"": ""directory"", ""status_code"": 404, ""uid"": 0, ""url"": ""https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/saltbase/salt/kube-addons/namespace.yaml""}
```

While this bug is rather easy to ""fix"" it shows a more general problem: We are relying on a lot of external files/resources to spin up a kubernetes with ansible.

As an intermediate fix I am gonna do the following in a follow up pull request:

Instead of using the `master` I am gonna use `v{{ kube_version }}` to get the resources for the related tag. This should solve issues like that for now and would give use more reproducible ansible results.

I think ultimately we should review our heavy reliance on external resources to configure kubernetes with ansible. I think this could create plenty of issues like:
- compatibility only with latest/master k8s release
- really hard to run k8s in private network envs (no general internet access)

I would strongly suggest we should aim to reduce the external resource (e.g. things that do not come with the ansible code) to the following:
- k8s build artifacts from `{{ kube_download_url_base }}`
- docker images of all sorts

Let me know how you think about this!
",closed,False,2016-05-09 16:21:58,2018-02-15 02:25:04
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/932,https://api.github.com/repos/kubernetes/contrib/issues/932,Cluster-autoscaler scale down impl,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-09 16:22:48,2016-05-10 13:40:05
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/933,https://api.github.com/repos/kubernetes/contrib/issues/933,WIP: group_vars symlink for vagrant inventory file,"- Addresses #929 
- Confirmed on Virtualbox + CoreOS & VirtualBox + CentOS
- Removes the Virtualbox 1.8 dependency 
- Removes variables in inventory file

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/933)

<!-- Reviewable:end -->
",closed,True,2016-05-09 17:39:57,2016-08-24 20:21:24
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/934,https://api.github.com/repos/kubernetes/contrib/issues/934,More configurable health checks for GCLB,"To do this we'd pipe a richer datastructure through the `Add` method: https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/backends/backends.go#L140, something like:

``` go

type healthCheck {
    path string
    interval time.Duration
}

type backendInfo struct {
  nodePort int
  hc healthCheck
}
```

And then read those values from annotations here: https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/controller/controller.go#L238 (similar method called ListBackendInfo) and pipe it through to https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/controller/controller.go#L271.

Essentially if any Ingress specifice `ingress.beta.kubernetes.io/health-check-path: ""/foo""` and/or `ingress.beta.kubernetes.io/health-check-interval: ""duraiton""`, we should program the health check appropriately. 
",closed,False,2016-05-09 18:13:01,2018-02-13 14:51:08
contrib,sekka1,https://github.com/kubernetes/contrib/issues/935,https://api.github.com/repos/kubernetes/contrib/issues/935,Ingress Not Forwarding to sub routes,"I am using this Ingress `gcr.io/google_containers/nginx-ingress-controller:0.61` and it is not forwarding to sub routes such as `/eventData` or `/api/v1/sdkErrors`.  Those routes are returning the default 404 backend.  However the root / route works.

Ingress yaml

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  creationTimestamp: 2016-03-28T22:20:47Z
  generation: 1
  name: ticket-test
spec:
  rules:
  - host: ticket.test.com
    http:
      paths:
      - backend:
          serviceName: aa-test-http-server-svc
          servicePort: 80
        path: /
      - backend:
          serviceName: aa-test-http-server-svc
          servicePort: 80
        path: /eventData
      - backend:
          serviceName: aa-test-http-server-svc
          servicePort: 80
        path: /api/v1/sdkErrors
      - backend:
          serviceName: aa-test-http-server-svc
          servicePort: 80
        path: /tpat
status:
  loadBalancer: {}
```

nginx.conf in the ingress pod:

```

# configuration file /etc/nginx/nginx.conf:

daemon off;

worker_processes 16;

pid /run/nginx.pid;

worker_rlimit_nofile 131072;

pcre_jit on;

events {
    multi_accept        on;
    worker_connections  16384;
    use                 epoll; 
}

http {
    vhost_traffic_status_zone shared:vhost_traffic_status:10m;

    # lus sectrion to return proper error codes when custom pages are used
    lua_package_path '.?.lua;./etc/nginx/lua/?.lua;/etc/nginx/lua/vendor/lua-resty-http/lib/?.lua;';
    init_by_lua_block {        
        require(""error_page"")
    }

    sendfile            on;
    aio                 threads;
    tcp_nopush          on;
    tcp_nodelay         on;

    log_subrequest      on;

    reset_timedout_connection on;

    keepalive_timeout 75s;

    types_hash_max_size 2048;
    server_names_hash_max_size 512;
    server_names_hash_bucket_size 100;

    include /etc/nginx/mime.types;
    default_type text/html;

    gzip on;
    gzip_comp_level 5;
    gzip_http_version 1.1;
    gzip_min_length 256;
    gzip_types application/atom+xml application/javascript application/json application/rss+xml application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/svg+xml image/x-icon text/css text/plain text/x-component;    
    gzip_proxied any;
    gzip_vary on;


    client_max_body_size ""2000m"";



    log_format upstreaminfo '$remote_addr - '
        '[$proxy_add_x_forwarded_for] - $remote_user [$time_local] ""$request"" $status $body_bytes_sent ""$http_referer"" ""$http_user_agent"" '
        '$request_length $request_time $upstream_addr $upstream_response_length $upstream_response_time $upstream_status';

    access_log /var/log/nginx/access.log upstreaminfo;
    error_log  /var/log/nginx/error.log notice;

    # Custom dns resolver.
    resolver 172.16.0.2 valid=30s;


    map $http_upgrade $connection_upgrade {
        default upgrade;
        ''      close;
    }

    # trust http_x_forwarded_proto headers correctly indicate ssl offloading
    map $http_x_forwarded_proto $pass_access_scheme {
      default $http_x_forwarded_proto;
      ''      $scheme;
    }

    # Map a response error watching the header Content-Type
    map $http_accept $httpAccept {
        default          html;
        application/json json;
        application/xml  xml;
        text/plain       text;
    }

    map $httpAccept $httpReturnType {
        default          text/html;
        json             application/json;
        xml              application/xml;
        text             text/plain;
    }

    server_name_in_redirect off;
    port_in_redirect off;

    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

    # turn on session caching to drastically improve performance

    ssl_session_cache builtin:1000 shared:SSL:10m;
    ssl_session_timeout 10m;


    # allow configuring ssl session tickets
    ssl_session_tickets on;

    # slightly reduce the time-to-first-byte
    ssl_buffer_size 4k;


    # allow configuring custom ssl ciphers
    ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA';
    ssl_prefer_server_ciphers on;




    # Custom error pages
    proxy_intercept_errors on;

    error_page 403 = @custom_403;
    error_page 404 = @custom_404;
    error_page 405 = @custom_405;
    error_page 408 = @custom_408;
    error_page 413 = @custom_413;
    error_page 501 = @custom_501;
    error_page 502 = @custom_502;
    error_page 503 = @custom_503;
    error_page 504 = @custom_504;

    # In case of errors try the next upstream server before returning an error
    proxy_next_upstream                     error timeout invalid_header http_502 http_503 http_504 ;


    upstream prod-aa-test-http-server-svc-80 {

        least_conn;

        server 10.20.162.16:3000;
        server 10.20.238.9:3000;
        server 10.20.51.17:3000;

    }

    upstream upstream-default-backend {

        least_conn;

        server 10.20.145.5:8080;
        server 10.20.166.12:8080;
        server 10.20.183.5:8080;

    }



    server {
        listen 80;


        server_name _;




        location / {
            proxy_set_header Host                   $host;

            # Pass Real IP
            proxy_set_header X-Real-IP              $remote_addr;

            # Allow websocket connections
            proxy_set_header                        Upgrade           $http_upgrade;
            proxy_set_header                        Connection        $connection_upgrade;

            proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Host       $host;
            proxy_set_header X-Forwarded-Port       $server_port;
            proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;

            proxy_redirect                          off;
            proxy_buffering                         off;

            proxy_http_version                      1.1;

            proxy_pass http://upstream-default-backend;
        }



        # this is required to avoid error if nginx is being monitored
        # with an external software (like sysdig)
        location /nginx_status {
            allow 127.0.0.1;
            deny all;

            access_log off;
            stub_status on;
        }


        location @custom_403 {
            internal;
            content_by_lua_block {
                openURL(403)
            }
        }

        location @custom_404 {
            internal;
            content_by_lua_block {
                openURL(404)
            }
        }

        location @custom_405 {
            internal;
            content_by_lua_block {
                openURL(405)
            }
        }

        location @custom_408 {
            internal;
            content_by_lua_block {
                openURL(408)
            }
        }

        location @custom_413 {
            internal;
            content_by_lua_block {
                openURL(413)
            }
        }

        location @custom_502 {
            internal;
            content_by_lua_block {
                openURL(502)
            }
        }

        location @custom_503 {
            internal;
            content_by_lua_block {
                openURL(503)
            }
        }

        location @custom_504 {
            internal;
            content_by_lua_block {
                openURL(504)
            }
        }

    }

    server {
        listen 80;


        server_name test-http-server-prod1-prod.kube-prod1.vungle.io;




        location / {
            proxy_set_header Host                   $host;

            # Pass Real IP
            proxy_set_header X-Real-IP              $remote_addr;

            # Allow websocket connections
            proxy_set_header                        Upgrade           $http_upgrade;
            proxy_set_header                        Connection        $connection_upgrade;

            proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Host       $host;
            proxy_set_header X-Forwarded-Port       $server_port;
            proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;

            proxy_redirect                          off;
            proxy_buffering                         off;

            proxy_http_version                      1.1;

            proxy_pass http://prod-aa-test-http-server-svc-80;
        }




        location @custom_403 {
            internal;
            content_by_lua_block {
                openURL(403)
            }
        }

        location @custom_404 {
            internal;
            content_by_lua_block {
                openURL(404)
            }
        }

        location @custom_405 {
            internal;
            content_by_lua_block {
                openURL(405)
            }
        }

        location @custom_408 {
            internal;
            content_by_lua_block {
                openURL(408)
            }
        }

        location @custom_413 {
            internal;
            content_by_lua_block {
                openURL(413)
            }
        }

        location @custom_502 {
            internal;
            content_by_lua_block {
                openURL(502)
            }
        }

        location @custom_503 {
            internal;
            content_by_lua_block {
                openURL(503)
            }
        }

        location @custom_504 {
            internal;
            content_by_lua_block {
                openURL(504)
            }
        }

    }

    server {
        listen 80;


        server_name ticket.test.com;


        location / {
            proxy_set_header Host                   $host;

            # Pass Real IP
            proxy_set_header X-Real-IP              $remote_addr;

            # Allow websocket connections
            proxy_set_header                        Upgrade           $http_upgrade;
            proxy_set_header                        Connection        $connection_upgrade;

            proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Host       $host;
            proxy_set_header X-Forwarded-Port       $server_port;
            proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;

            proxy_redirect                          off;
            proxy_buffering                         off;

            proxy_http_version                      1.1;

            proxy_pass http://prod-aa-test-http-server-svc-80;
        }



        location /api/v1/sdkErrors {
            proxy_set_header Host                   $host;

            # Pass Real IP
            proxy_set_header X-Real-IP              $remote_addr;

            # Allow websocket connections
            proxy_set_header                        Upgrade           $http_upgrade;
            proxy_set_header                        Connection        $connection_upgrade;

            proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Host       $host;
            proxy_set_header X-Forwarded-Port       $server_port;
            proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;

            proxy_redirect                          off;
            proxy_buffering                         off;

            proxy_http_version                      1.1;

            proxy_pass http://prod-aa-test-http-server-svc-80;
        }

        location /eventData {
            proxy_set_header Host                   $host;

            # Pass Real IP
            proxy_set_header X-Real-IP              $remote_addr;

            # Allow websocket connections
            proxy_set_header                        Upgrade           $http_upgrade;
            proxy_set_header                        Connection        $connection_upgrade;

            proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Host       $host;
            proxy_set_header X-Forwarded-Port       $server_port;
            proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;

            proxy_redirect                          off;
            proxy_buffering                         off;

            proxy_http_version                      1.1;

            proxy_pass http://prod-aa-test-http-server-svc-80;
        }

        location /tpat {
            proxy_set_header Host                   $host;

            # Pass Real IP
            proxy_set_header X-Real-IP              $remote_addr;

            # Allow websocket connections
            proxy_set_header                        Upgrade           $http_upgrade;
            proxy_set_header                        Connection        $connection_upgrade;

            proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Host       $host;
            proxy_set_header X-Forwarded-Port       $server_port;
            proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;

            proxy_redirect                          off;
            proxy_buffering                         off;

            proxy_http_version                      1.1;

            proxy_pass http://prod-aa-test-http-server-svc-80;
        }




        location @custom_403 {
            internal;
            content_by_lua_block {
                openURL(403)
            }
        }

        location @custom_404 {
            internal;
            content_by_lua_block {
                openURL(404)
            }
        }

        location @custom_405 {
            internal;
            content_by_lua_block {
                openURL(405)
            }
        }

        location @custom_408 {
            internal;
            content_by_lua_block {
                openURL(408)
            }
        }

        location @custom_413 {
            internal;
            content_by_lua_block {
                openURL(413)
            }
        }

        location @custom_502 {
            internal;
            content_by_lua_block {
                openURL(502)
            }
        }

        location @custom_503 {
            internal;
            content_by_lua_block {
                openURL(503)
            }
        }

        location @custom_504 {
            internal;
            content_by_lua_block {
                openURL(504)
            }
        }

    }


    # default server, including healthcheck
    server {
        listen 8080 default_server reuseport;

        location /healthz {
            access_log off;
            return 200;
        }

        location /nginx_status {

            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;

        }

        location / {
            proxy_pass             http://upstream-default-backend;
        }

        location @custom_403 {
            internal;
            content_by_lua_block {
                openURL(403)
            }
        }

        location @custom_404 {
            internal;
            content_by_lua_block {
                openURL(404)
            }
        }

        location @custom_405 {
            internal;
            content_by_lua_block {
                openURL(405)
            }
        }

        location @custom_408 {
            internal;
            content_by_lua_block {
                openURL(408)
            }
        }

        location @custom_413 {
            internal;
            content_by_lua_block {
                openURL(413)
            }
        }

        location @custom_502 {
            internal;
            content_by_lua_block {
                openURL(502)
            }
        }

        location @custom_503 {
            internal;
            content_by_lua_block {
                openURL(503)
            }
        }

        location @custom_504 {
            internal;
            content_by_lua_block {
                openURL(504)
            }
        }

    }

    # default server for services without endpoints
    server {
        listen 8181;

        location / {
            content_by_lua_block {
                openURL(503)
            }
        }        
    }    
}

stream {
# TCP services


# UDP services

}




# configuration file /etc/nginx/mime.types:

types {
    text/html                             html htm shtml;
    text/css                              css;
    text/xml                              xml;
    image/gif                             gif;
    image/jpeg                            jpeg jpg;
    application/javascript                js;
    application/atom+xml                  atom;
    application/rss+xml                   rss;

    text/mathml                           mml;
    text/plain                            txt;
    text/vnd.sun.j2me.app-descriptor      jad;
    text/vnd.wap.wml                      wml;
    text/x-component                      htc;

    image/png                             png;
    image/tiff                            tif tiff;
    image/vnd.wap.wbmp                    wbmp;
    image/x-icon                          ico;
    image/x-jng                           jng;
    image/x-ms-bmp                        bmp;
    image/svg+xml                         svg svgz;
    image/webp                            webp;

    application/font-woff                 woff;
    application/java-archive              jar war ear;
    application/json                      json;
    application/mac-binhex40              hqx;
    application/msword                    doc;
    application/pdf                       pdf;
    application/postscript                ps eps ai;
    application/rtf                       rtf;
    application/vnd.apple.mpegurl         m3u8;
    application/vnd.ms-excel              xls;
    application/vnd.ms-fontobject         eot;
    application/vnd.ms-powerpoint         ppt;
    application/vnd.wap.wmlc              wmlc;
    application/vnd.google-earth.kml+xml  kml;
    application/vnd.google-earth.kmz      kmz;
    application/x-7z-compressed           7z;
    application/x-cocoa                   cco;
    application/x-java-archive-diff       jardiff;
    application/x-java-jnlp-file          jnlp;
    application/x-makeself                run;
    application/x-perl                    pl pm;
    application/x-pilot                   prc pdb;
    application/x-rar-compressed          rar;
    application/x-redhat-package-manager  rpm;
    application/x-sea                     sea;
    application/x-shockwave-flash         swf;
    application/x-stuffit                 sit;
    application/x-tcl                     tcl tk;
    application/x-x509-ca-cert            der pem crt;
    application/x-xpinstall               xpi;
    application/xhtml+xml                 xhtml;
    application/xspf+xml                  xspf;
    application/zip                       zip;

    application/octet-stream              bin exe dll;
    application/octet-stream              deb;
    application/octet-stream              dmg;
    application/octet-stream              iso img;
    application/octet-stream              msi msp msm;

    application/vnd.openxmlformats-officedocument.wordprocessingml.document    docx;
    application/vnd.openxmlformats-officedocument.spreadsheetml.sheet          xlsx;
    application/vnd.openxmlformats-officedocument.presentationml.presentation  pptx;

    audio/midi                            mid midi kar;
    audio/mpeg                            mp3;
    audio/ogg                             ogg;
    audio/x-m4a                           m4a;
    audio/x-realaudio                     ra;

    video/3gpp                            3gpp 3gp;
    video/mp2t                            ts;
    video/mp4                             mp4;
    video/mpeg                            mpeg mpg;
    video/quicktime                       mov;
    video/webm                            webm;
    video/x-flv                           flv;
    video/x-m4v                           m4v;
    video/x-mng                           mng;
    video/x-ms-asf                        asx asf;
    video/x-ms-wmv                        wmv;
    video/x-msvideo                       avi;
}

```
",closed,False,2016-05-09 18:19:15,2016-05-20 18:45:07
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/936,https://api.github.com/repos/kubernetes/contrib/issues/936,Ingress godeps uprev,,closed,True,2016-05-09 19:26:22,2016-05-09 19:41:35
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/937,https://api.github.com/repos/kubernetes/contrib/issues/937,Add node problem detector,"Here is a simple implementation of NodeProblemDetector for https://github.com/kubernetes/kubernetes/issues/23028.

The node problem detector is mainly composed of two parts:
- **ProblemDetector**. It is responsible for:
  - Collecting and consolidating events and conditions from different problem daemons;
  - Translating and aggregating them into `Event`s and `NodeCondition`s;
  - Syncing with apiserver.
- **KernelMonitor** is a simple kernel log monitor for demonstrating the Node Problem API. It is one (the only one for now) of the problem daemons. It monitors the kernel log and reports problems to ProblemDetector.
  - KernelMonitor is in charge of the ""kernel condition"" node condition.
  - If there is a permanent kernel error detected, it will report ""kernel condition"" (only `KernelDeadlock` for now) with corresponding reason and message to the problem detector.
  - If there is a temporary error detected, it will report an event to the problem detector.
  - KernelMonitor uses https://github.com/hpcloud/tail to monitor the kernel log, and uses regular expression to match problems in the kernel log. The patterns of problems are configured in the config file `config/kernel-monitor.json`. Now some known kernel bugs (https://github.com/kubernetes/kubernetes/issues/20096, https://github.com/kubernetes/kubernetes/issues/19986) have been added, and it would be easy to support other issue only if the pattern is stable and can be expressed in regular expression.
  - A lot of TODOs:
    - Support more flexible pattern match such as only matching start&end line etc.
    - Support journald.
    - ...

The node problem detector is a DaemonSet. It needs a ConfigMap `node-problem-detector-config` generated from `config/` directory. The bootstrapping is dumb now. To start the node problem detector, you need to:
1. Make sure the config mount, kernel log mount and apiserver endpoint in node-problem-detector.yaml are properly configured.
2. Make sure the log path and issue patterns `config/kernel-monitor.json` are properly configured.
3. Create ConfigMap `node-problem-detector-config` from directory `config/`.
4. Create DaemonSet `node-problem-detector.yaml`.

Now ProblemDetector and KernelMonitor are running in the same container and of course the same DaemonSet. In the future, we may want to:
- Option 1: Move the logic of ProblemDetector into kubelet, and leave problem daemons (such as KernelMonitor) as separate DaemonSets reporting directly to kubelet.
- Option 2: Move ProblemDetector into a DaemonSet, all problem daemons run as separate DaemonSets reporting to ProblemDetector.
- Option 3: Run ProblemDetector and problem daemons in separate containers, but wrap them in one DaemonSet.

This has not been decided yet, and may need more discussion. :)

@dchen1107 
/cc @kubernetes/sig-node 
",closed,True,2016-05-09 23:36:58,2016-05-17 23:14:43
contrib,rakesh-k,https://github.com/kubernetes/contrib/issues/938,https://api.github.com/repos/kubernetes/contrib/issues/938,Heapster controller creation fails with validation error,"Trying to manually run the `heapster-controller.yaml` through kubectl fails with following error (kubernetes version 1.2). I just don't know if running with `validation=false` is a good idea or not.

`error validating ""heapster-controller.yaml"": error validating data: the server could not find the requested resource (get .extensions); if you choose to ignore these errors, turn validation off with --validate=false`

Here is the content of the file:

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: heapster-v1.1.0.beta1
  namespace: kube-system
  labels:
    k8s-app: heapster
    kubernetes.io/cluster-service: ""true""
    version: v1.1.0.beta1
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: heapster
      version: v1.1.0.beta1
  template:
    metadata:
      labels:
        k8s-app: heapster
        version: v1.1.0.beta1
    spec:
      containers:
        - image: gcr.io/google_containers/heapster:v1.1.0-beta1
          name: heapster
          resources:
            # keep request = limit to keep this container in guaranteed class
            limits:
              cpu: 100m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 200Mi
          command:
            - /heapster
            - --source=kubernetes.summary_api:''
            - --sink=influxdb:http://monitoring-influxdb:8086
            - --metric_resolution=60s
        - image: gcr.io/google_containers/heapster:v1.1.0-beta1
          name: eventer
          resources:
            # keep request = limit to keep this container in guaranteed class
            limits:
              cpu: 100m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 200Mi
          command:
            - /eventer
            - --source=kubernetes:''
            - --sink=influxdb:http://monitoring-influxdb:8086
        - image: gcr.io/google_containers/addon-resizer:1.0
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 100Mi
            requests:
              cpu: 50m
              memory: 100Mi
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          command:
            - /pod_nanny
            - --cpu=100m
            - --extra-cpu=0m
            - --memory=200Mi
            - --extra-memory=4Mi
            - --threshold=5
            - --deployment=heapster-v1.1.0.beta1
            - --container=heapster
            - --poll-period=300000
        - image: gcr.io/google_containers/addon-resizer:1.0
          name: eventer-nanny
          resources:
            limits:
              cpu: 50m
              memory: 100Mi
            requests:
              cpu: 50m
              memory: 100Mi
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          command:
            - /pod_nanny
            - --cpu=100m
            - --extra-cpu=0m
            - --memory=200Mi
            - --extra-memory=500Ki
            - --threshold=5
            - --deployment=heapster-v1.1.0.beta1
            - --container=eventer
            - --poll-period=300000
```
",closed,False,2016-05-10 00:36:36,2018-02-13 12:49:10
contrib,AntAreS24,https://github.com/kubernetes/contrib/issues/939,https://api.github.com/repos/kubernetes/contrib/issues/939,Cannot Curl the service within the cluster,"I'm new to K8S, so happy to be pointed to the right direction.

I've been struggling to understand why my setup doesn't work. I have a Vagrant file that creates 3 VMs. 1 K8S master and 2 Nodes. https://github.com/AntAreS24/containerisation

I can see the service and rc of my cluster up and running:

``` console
core@master ~ $ kubectl get rc
NAME          DESIRED   CURRENT   AGE
test-be-rc   2         2         1d
```

``` console
core@master ~ $ kubectl get services
NAME               CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes         10.100.0.1     <none>        443/TCP    1d
test-be-service   10.100.94.82                 7000/TCP   10m
```

and the pods are running:

``` console
core@master ~ $ kubectl get pods
NAME                READY     STATUS             RESTARTS   AGE
test-be-rc-8me95   1/1       Running   0          33m
test-be-rc-86ij0   1/1       Running   0          33m
```

If I check the logs for the pods, I get what I expect and if I curl the pod on the specific node, I get a response:

``` console
core@node02 ~ $ docker ps
CONTAINER ID        IMAGE                                COMMAND                  CREATED              STATUS              PORTS               NAMES
329f0dd70f41        drr.test.com:5000/businessevents    ""/bin/sh -c /start.sh""   About a minute ago   Up About a minute                       k8s_k8be.deeefc6_test-be-rc-8me95_default
11e6-a673-080027a75e9e_6db5d57b
d05393f2b4d7        gcr.io/google_containers/pause:2.0   ""/pause""                 About a minute ago   Up About a minute                       k8s_POD.253600cc_test-be-rc-8me95_default
11e6-a673-080027a75e9e_faca0cf8
```

``` console
core@node02 ~ $ curl http://172.18.0.3:7000/Channels/HTTPChannel/trigger_processing?name=normal&number=1
[1] 2578
{test: 'body'}
```

I tried setting the service type as `nothing`, `NodePorts` and `LoadBalancer`, but I'm getting the same result.

``` console
core@master ~ $ kubectl describe service test-be-service
Name:                   test-be-service
Namespace:              default
Labels:                 name=test-be-pod,tier=backend
Selector:               name=test-be-pod,tier=backend
Type:                   LoadBalancer
IP:                     10.100.94.82
Port:                   http    7000/TCP
NodePort:               http    32149/TCP
Endpoints:              <none>
Session Affinity:       None
No events.
```

``` console
core@master ~ $ kubectl describe rc
Name:           test-be-rc
Namespace:      default
Image(s):       ddr.test.com:5000/businessevents
Selector:       app=test-be-pod,tier=backend
Labels:         name=test-be,tier=backend
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Volumes:
  beconfig:
    Type:       HostPath (bare host directory volume)
    Path:       /home/core/be
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath   Type            Reason                  Message
  ---------     --------        -----   ----                            -------------   --------        ------                  -------
  13m           13m             1       {replication-controller }                       Normal          SuccessfulCreate        Created pod: test-be-rc-8me95
  13m           13m             1       {replication-controller }                       Normal          SuccessfulCreate        Created pod: test-be-rc-oxag4
  10m           10m             1       {replication-controller }                       Normal          SuccessfulCreate        Created pod: test-be-rc-86ij0
```

``` console
core@master ~ $ kubectl describe pods
Name:           test-be-rc-86ij0
Namespace:      default
Node:           172.17.8.102/172.17.8.102
Start Time:     Tue, 10 May 2016 05:27:34 +0000
Labels:         app=test-be-pod,tier=backend
Status:         Running
IP:             172.18.0.3
Controllers:    ReplicationController/test-be-rc
Containers:
  k8be:
    Container ID:       docker://87b3e1bc1a57ca0383556a889a688347c914b69ea083ba1c2dc3e01dec2f7845
    Image:              ddr.test.com:5000/businessevents
    Image ID:           docker://sha256:48e6bd0642a752cc871eb7f2151f23da719c30654c53dbc8801d0c43b5fae9a2
    Port:               7000/TCP
    QoS Tier:
      cpu:              BestEffort
      memory:           BestEffort
    State:              Running
      Started:          Tue, 10 May 2016 05:27:38 +0000
    Ready:              True
    Restart Count:      0
    Environment Variables:
      BE_ENGINE_EAR_1:  /data/be/sample.ear
      BE_ENGINE_CDD_1:  /data/be/sample.cdd
      BE_ENGINE_UNIT_1: sample-inf
      BE_ENGINE_TRA_1:  /data/be/be-engine.tra
      BE_ENGINE_JMX_1:  9990
Conditions:
  Type          Status
  Ready         True
Volumes:
  beconfig:
    Type:       HostPath (bare host directory volume)
    Path:       /home/core/be
  default-token-v4sdg:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-v4sdg
Events:
  FirstSeen     LastSeen        Count   From                    SubobjectPath           Type            Reason          Message
  ---------     --------        -----   ----                    -------------           --------        ------          -------
  11m           11m             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned test-be-rc-86ij0 to 172.17.8.102
  11m           11m             1       {kubelet 172.17.8.102}  spec.containers{k8be}   Normal          Pulling         pulling image ""ddr.test.com:5000/businessevents""
  11m           11m             1       {kubelet 172.17.8.102}  spec.containers{k8be}   Normal          Pulled          Successfully pulled image ""ddr.test.com:5000/businessevents""
  11m           11m             1       {kubelet 172.17.8.102}  spec.containers{k8be}   Normal          Created         Created container with docker id 87b3e1bc1a57
  11m           11m             1       {kubelet 172.17.8.102}  spec.containers{k8be}   Normal          Started         Started container with docker id 87b3e1bc1a57


Name:           test-be-rc-8me95
Namespace:      default
Node:           172.17.8.102/172.17.8.102
Start Time:     Tue, 10 May 2016 05:23:59 +0000
Labels:         app=test-be-pod,tier=backend
Status:         Running
IP:             172.18.0.2
Controllers:    ReplicationController/test-be-rc
Containers:
  k8be:
    Container ID:       docker://329f0dd70f414008b860aea8eeb5112b5b4d72a3d41bf6058ec2ab4a1cb0b41c
    Image:              ddr.test.com:5000/businessevents
    Image ID:           docker://sha256:48e6bd0642a752cc871eb7f2151f23da719c30654c53dbc8801d0c43b5fae9a2
    Port:               7000/TCP
    QoS Tier:
      cpu:              BestEffort
      memory:           BestEffort
    State:              Running
      Started:          Tue, 10 May 2016 05:24:03 +0000
    Ready:              True
    Restart Count:      0
    Environment Variables:
      BE_ENGINE_EAR_1:  /data/be/sample.ear
      BE_ENGINE_CDD_1:  /data/be/sample.cdd
      BE_ENGINE_UNIT_1: sample-inf
      BE_ENGINE_TRA_1:  /data/be/be-engine.tra
      BE_ENGINE_JMX_1:  9990
Conditions:
  Type          Status
  Ready         True
Volumes:
  beconfig:
    Type:       HostPath (bare host directory volume)
    Path:       /home/core/be
  default-token-v4sdg:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-v4sdg
Events:
  FirstSeen     LastSeen        Count   From                    SubobjectPath           Type            Reason          Message
  ---------     --------        -----   ----                    -------------           --------        ------          -------
  14m           14m             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned test-be-rc-8me95 to 172.17.8.102
  14m           14m             1       {kubelet 172.17.8.102}  spec.containers{k8be}   Normal          Pulling         pulling image ""ddr.test.com:5000/businessevents""
  14m           14m             1       {kubelet 172.17.8.102}  spec.containers{k8be}   Normal          Pulled          Successfully pulled image ""ddr.test.com:5000/businessevents""
  14m           14m             1       {kubelet 172.17.8.102}  spec.containers{k8be}   Normal          Created         Created container with docker id 329f0dd70f41
  14m           14m             1       {kubelet 172.17.8.102}  spec.containers{k8be}   Normal          Started         Started container with docker id 329f0dd70f41
```

``` console
core@master ~ $ curl http://127.0.0.1:7000
curl: (7) Failed to connect to 127.0.0.1 port 7000: Connection refused
core@master ~ $ curl http://172.17.8.100:7000
curl: (7) Failed to connect to 172.17.8.100 port 7000: Connection refused
core@master ~ $ curl http://10.100.94.82:7000

[ctrl+c]
core@master ~ $ curl http://10.100.94.82:32149

[ctrl+c]
```

This last 2 commands actually never complete.

What am I missing? Do I need an Ingress even if it's running locally? I don't really want the full load balancing setup, I just want to be able to call the service on a specific port and get it to route to any pod available. If I kill a pod, it shouldn't impact the call to the service.

Any help really appreciated.

Ben
",closed,False,2016-05-10 05:42:55,2018-02-13 22:59:10
contrib,eparis,https://github.com/kubernetes/contrib/pull/940,https://api.github.com/repos/kubernetes/contrib/issues/940,Make copy when deleting instead of messing up range operations,"We would walk the labels and comments lists and could then delete from
them. Since we used the form `a = append(a[:i], a[i+1:...])` we were
actually changing the list as it was being used in a `range a`
statement. This could cause the comment deleter and the LGTM label
checker to go nuts.
",closed,True,2016-05-10 12:41:49,2016-05-10 13:06:14
contrib,elemoine,https://github.com/kubernetes/contrib/issues/941,https://api.github.com/repos/kubernetes/contrib/issues/941,[ansible] Monitoring & logging services not started,"I followed the [ansible/vagrant README](https://github.com/kubernetes/contrib/blob/master/ansible/vagrant/README.md). Ansible successfully finished but the monitoring and logging services are not started:

``` bash
[static-67 vagrant (master u=)]$ vagrant ssh kube-master
[vagrant@kube-master ~]$ kubectl cluster-info
Kubernetes master is running at http://localhost:8080
```

I ran `vagrant up --provider libvirt` with the default options. So CentOS7 is used in my case. I also tried to run `vagrant provision` again as indicated in the README, but that did not help.
",closed,False,2016-05-10 13:03:29,2016-05-24 13:36:12
contrib,simonswine,https://github.com/kubernetes/contrib/pull/942,https://api.github.com/repos/kubernetes/contrib/issues/942,[ansible] includes kubernetes addons from release tag 'v1.2.4',"This should include all external resources/artifacts needed for kube-addons. I decided to get them from the stable release tag 'v1.2.4'. (as proposed in #931)
- includes scripts `get_files.sh` / `get_templates.sh` to get updated versions from k8s project easily
  cf. #931 
- compatibility with `eventer` and `metrics` memory limits from newer heapster replication controllers
- build new `python-image` only if Dockerfile actually changed

This should fix #941, #938 as well
",closed,True,2016-05-10 13:26:53,2016-05-24 13:36:12
contrib,aledbf,https://github.com/kubernetes/contrib/pull/943,https://api.github.com/repos/kubernetes/contrib/issues/943,Move Ingress godeps to vendor/,,closed,True,2016-05-10 13:31:21,2016-05-10 18:12:27
contrib,gouyang,https://github.com/kubernetes/contrib/issues/944,https://api.github.com/repos/kubernetes/contrib/issues/944,[ansible] namespace kube-system is not created,"namespace `kube-system` is not created because kube-addons.sh is expecting to find it at /opt/namespace.yaml.

https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/addon-manager/kube-addons.sh#L68
",closed,False,2016-05-10 14:03:25,2016-06-01 03:15:38
contrib,gouyang,https://github.com/kubernetes/contrib/pull/945,https://api.github.com/repos/kubernetes/contrib/issues/945,Change namespace.yaml path to /opt,"Fixes #944
",closed,True,2016-05-10 14:11:50,2016-05-26 16:47:02
contrib,aledbf,https://github.com/kubernetes/contrib/pull/946,https://api.github.com/repos/kubernetes/contrib/issues/946,[nginx-ingress-controller] Allow authentication in Ingress rules,"fixes #877
",closed,True,2016-05-10 14:55:14,2016-05-31 20:43:52
contrib,eparis,https://github.com/kubernetes/contrib/pull/947,https://api.github.com/repos/kubernetes/contrib/issues/947,Update mungegithub to support vendor/,,closed,True,2016-05-10 16:05:32,2016-05-13 05:02:15
contrib,rakesh-k,https://github.com/kubernetes/contrib/issues/948,https://api.github.com/repos/kubernetes/contrib/issues/948,Change to expose hostname instead of localhost for the apiserver etc.,"The k8s cluster installation on my VMs leads to this:

```
[rakeshk@cd3labvdapp002 system]$ kubectl cluster-info
Kubernetes master is running at http://localhost:8080
Elasticsearch is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
Heapster is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/heapster
Kibana is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kibana-logging
KubeDNS is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubedash is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kubedash
kubernetes-dashboard is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
Grafana is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
InfluxDB is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb
[rakeshk@cd3labvdapp002 system]$
```

 I would rather expose the hostname for these URLs so that it can be accessed by other hosts. What config change is needed?
",closed,False,2016-05-10 16:52:03,2017-04-25 19:35:32
contrib,eparis,https://github.com/kubernetes/contrib/issues/949,https://api.github.com/repos/kubernetes/contrib/issues/949,Perfdash Godeps are invalid,"2cd65f7 is not a commit in kubernetes. And yet it is claimed in https://github.com/kubernetes/contrib/blob/master/perfdash/Godeps/Godeps.json#L20

This was done in https://github.com/kubernetes/contrib/pull/815 by @Random-Liu 

This is an example of why verify-godeps.sh is run in the main tree. So we don't get random non-godep-restorable cruft.
",closed,False,2016-05-10 19:56:54,2016-05-11 16:23:38
contrib,rakesh-k,https://github.com/kubernetes/contrib/issues/950,https://api.github.com/repos/kubernetes/contrib/issues/950,sample admission-controls files,"The kube-addons.sh file looks for admission-controls objects as .yaml or .json files but the upstream pull does not get it. I searched for some samples in order to hand-create those but I am out of luck. Can it be made available so that it can be pulled during the installation?
",closed,False,2016-05-10 21:05:30,2018-02-13 12:49:11
contrib,rakesh-k,https://github.com/kubernetes/contrib/issues/951,https://api.github.com/repos/kubernetes/contrib/issues/951,script to shutdown and start the k8s cluster,"I have seen references to kubeup and kubedown scripts (not sure if it applied to entire cluster or just a node) but have not found those within the installed base. It will be really nice to have that feature available so that the dev setups can share the environment when not in use. Point me in right direction if it already exists.
",closed,False,2016-05-10 22:09:38,2018-02-13 13:50:08
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/952,https://api.github.com/repos/kubernetes/contrib/issues/952,ingress controllers should watch secrets,"Currently there is no code that translates secrets <-> ingress. we wait for the periodic relist, then lookup secrets and create certs. We should notice when secrets change and invoke a sync on the associated ingress. 
",closed,False,2016-05-11 00:12:03,2018-02-16 07:54:00
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/953,https://api.github.com/repos/kubernetes/contrib/issues/953,ingress controller should health check straight to endpoints,"Scrape endpoints, look for a http readiness probe, make that the health check url. https://github.com/kubernetes/contrib/tree/master/ingress/controllers/gce#health-checks
",closed,False,2016-05-11 00:12:58,2018-02-13 22:59:11
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/954,https://api.github.com/repos/kubernetes/contrib/issues/954,GCE ingress controller should clarify behavior when service port doesn't exist,"Essentially:
1. Send an event
2. What should happen: don't update the url map till the Service has a nodeport, but don't freak out either. Just wait for the node port. 

1 surely doesn't happen today, we just log, 2 _should_ happen but there have been reports that it doesn't. 
",closed,False,2016-05-11 00:14:57,2018-02-13 22:59:11
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/955,https://api.github.com/repos/kubernetes/contrib/issues/955,Fix perfdash godeps,"@eparis Fixed the Godeps. Sorry for making the trouble.
",closed,True,2016-05-11 07:34:04,2016-05-11 12:57:01
contrib,eparis,https://github.com/kubernetes/contrib/pull/956,https://api.github.com/repos/kubernetes/contrib/issues/956,[mungegithub] Log the body of removed comments,,closed,True,2016-05-11 17:00:37,2016-05-11 17:28:42
contrib,dmitrig01,https://github.com/kubernetes/contrib/issues/957,https://api.github.com/repos/kubernetes/contrib/issues/957,Cannot build ingress,"Hi, I am trying to make some changes to the nginx ingress, but I cannot get it to build. The steps I took are:

First, `go get github.com/kubernetes/contrib`

Then, `cd $GOPATH/src/github.com/kubernetes/contrib/ingress` (NOTE: it breaks if I do `cd $GOPATH/src/github.com/kubernetes/contrib/Ingress`)

Then, `godep restore`

It complains about `speter.net/go/exp/math/dec/inf` missing. So I did a find-and-replace replacing that with `gopkg.in/inf.v0`. Suboptimal, but it worked.

Then I tried to run `make controller` in `nginx`.

I got `controller.go:43:2: cannot find package ""k8s.io/contrib/ingress/controllers/nginx/nginx"" in any of: .....`

So I replaced `""k8s.io/contrib/ingress/controllers/nginx/nginx""` with `""github.com/kubernetes/contrib/ingress/controllers/nginx/nginx""`. Which seemed to fix that. Also suboptimal.

Now it finally builds.

For reference,

```
$ godep version
godep v62 (darwin/amd64/go1.6)
$ go version
go version go1.6 darwin/amd64
```
",closed,False,2016-05-11 20:19:44,2016-05-11 23:41:17
contrib,aledbf,https://github.com/kubernetes/contrib/pull/958,https://api.github.com/repos/kubernetes/contrib/issues/958,[nginx-slim] update nginx and add module for digest authentication,"fixes #960
",closed,True,2016-05-11 23:35:59,2016-05-28 00:57:04
contrib,pbitty,https://github.com/kubernetes/contrib/pull/959,https://api.github.com/repos/kubernetes/contrib/issues/959,These are some temporary tweaks to allow us to prototype our Kubernetes setup.,"**NOTE:**  _This is not meant to be merged!_
",closed,True,2016-05-12 03:04:23,2016-05-12 03:04:42
contrib,alanhartless,https://github.com/kubernetes/contrib/issues/960,https://api.github.com/repos/kubernetes/contrib/issues/960,Nginx Ingress Controller - Safari 9 craps out with http2,"I've literally wasted hours upon hours tracking this down. :-)

Using the nginx controller with SSL, Safari 9 will refuse to load any site. I'm sure this is a bug on Safari's part but I finally tracked it down to http2 in the listen 443 ssl line. I had to create a custom template to remove the http2 lines in order to get Safari (mobile or mac) to load our sites. None of the other browsers including older Safari browsers had an issue. I think http2 support was added in recent Safari version on the latest OS but obviously something is not right. Unfortunately, it wasn't just my local mac safari either as we had customers all over the world complain they couldn't access the sites with safari! 

Maybe this should be a setting? Or if there's a known workaround I couldn't figure out? 

https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/nginx.tmpl#L157
",closed,False,2016-05-12 05:00:30,2016-05-26 04:02:32
contrib,AblionGE,https://github.com/kubernetes/contrib/issues/961,https://api.github.com/repos/kubernetes/contrib/issues/961,Cannot launch keepalived (timed out),"Hello,
When I try to install keepalived, I encounter a problem which is described in the logs from Kubernetes : 

```
I0512 08:35:42.346381       1 keepalived.go:161] cleaning ipvs configuration
I0512 08:35:42.349105       1 main.go:109] starting LVS configuration
F0512 08:36:12.398938       1 controller.go:261] Error getting POD information: timed out waiting to observe own status as Running
```

I tried with the original code from the repo. Any idea on what could go wrong ? Thanks!
",closed,False,2016-05-12 08:49:38,2018-02-14 05:05:10
contrib,aledbf,https://github.com/kubernetes/contrib/pull/962,https://api.github.com/repos/kubernetes/contrib/issues/962,[nginx-ingress-controller] Enable configuration to disable http2,"fixes #960

Latest safari have issues with http2 https://trac.nginx.org/nginx/ticket/979
",closed,True,2016-05-12 10:49:14,2016-05-26 04:02:32
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/963,https://api.github.com/repos/kubernetes/contrib/issues/963,Replace GoogleCloudPlatform with kubernetes,"Replace `https://raw.githubusercontent.com/GoogleCloudPlatform` with `https://raw.githubusercontent.com/kubernetes` prefix

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/963)

<!-- Reviewable:end -->
",closed,True,2016-05-12 13:37:41,2016-08-23 12:52:38
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/964,https://api.github.com/repos/kubernetes/contrib/issues/964,[cluster-autoscaler] Update kubernetes dependencies to HEAD,"Update kubernetes dependencies to HEAD & fix code where necessary.
",closed,True,2016-05-12 17:37:41,2016-05-12 19:29:41
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/965,https://api.github.com/repos/kubernetes/contrib/issues/965,etcd ansible_temp_dir use instead of hard coded,"- should use the variable defined in [common/defaults](https://github.com/kubernetes/contrib/blob/master/ansible/roles/common/defaults/main.yaml#L8)
- removed the directory creation since its in [common/tasks](https://github.com/kubernetes/contrib/blob/master/ansible/roles/common/tasks/main.yml#L51-L52)
",closed,True,2016-05-12 20:12:26,2016-05-12 20:14:11
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/966,https://api.github.com/repos/kubernetes/contrib/issues/966,Updated the etcd_download_url,"- Variablized the download url, only applicable for Ubuntu. Removed the v from the version number.
- Follows the same pattern we use in [flannel](https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/defaults/main.yaml#L18-L19) , [kube](https://github.com/kubernetes/contrib/blob/master/ansible/roles/kubernetes/defaults/main.yml#L21), and [pypy](https://github.com/kubernetes/contrib/blob/master/ansible/roles/pre-ansible/defaults/main.yml#L10-L11)
",closed,True,2016-05-12 20:19:21,2016-05-12 21:33:28
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/967,https://api.github.com/repos/kubernetes/contrib/issues/967,Created a top level playbooks dir and scripts dir,"**Dependency** - #933 because the `group_vars` directory needs to be in the _same directory_ as the `inventory` file or `deploy.yml`
- rename of the top level script from cluster to deploy. 
- Modeled after [openstack-ansible](https://github.com/openstack/openstack-ansible)
- top level play books will all live in the playbooks dir, called by scripts in the scripts dir. This organizes us well for new playbooks, such as uninstall, upgrade, etc.

Confirmed on Virtualbox & CoreOS & CentOS and BM
",closed,True,2016-05-12 20:56:12,2016-08-12 13:19:18
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/968,https://api.github.com/repos/kubernetes/contrib/issues/968,Cluster-autoscaler for K8s v1.3,"This is an umbrella bug for coding efforts required to finish cluster autoscaler for kubernetes 1.3.
",closed,False,2016-05-12 21:53:29,2016-06-24 20:53:22
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/969,https://api.github.com/repos/kubernetes/contrib/issues/969,Cluster-autoscaler: integrate scale-up and scale-down,"Ref: #968
",closed,False,2016-05-12 21:54:07,2016-06-13 21:13:42
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/970,https://api.github.com/repos/kubernetes/contrib/issues/970,Cluster-autoscaler: notice that cluster enlargement didn't help for a particular pod,"Ref: #968
",closed,False,2016-05-12 21:54:55,2016-06-09 18:17:58
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/971,https://api.github.com/repos/kubernetes/contrib/issues/971,Cluster-autoscaler: clean up cluster_autoscaler.go,"Ref: #968
",closed,False,2016-05-12 21:59:22,2017-04-19 10:38:13
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/972,https://api.github.com/repos/kubernetes/contrib/issues/972,Cluster-autoscaler: only scale down when the node was unused for longer time,"Ref: #968
",closed,False,2016-05-12 22:00:14,2016-06-13 21:13:17
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/973,https://api.github.com/repos/kubernetes/contrib/issues/973,Cluster-autoscaler: use a non-certificate based way to connect to api-server when running on master. ,"Ref: #968 

Just like scheduler.
",closed,False,2016-05-12 22:03:51,2016-05-16 07:23:14
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/974,https://api.github.com/repos/kubernetes/contrib/issues/974,Cluster-autoscaler: fix problems with GCP access from GCE master.,"Ref: #968 
",closed,False,2016-05-12 22:06:35,2016-06-24 20:50:20
contrib,atosatto,https://github.com/kubernetes/contrib/issues/975,https://api.github.com/repos/kubernetes/contrib/issues/975,RFC: Ansible Vagrant Providers,"Given the work done by @adamschaub in #761 and all the other people involved in the review, I'm wondering which Vagrant providers do we actually need to support to allow an easy and agile development and testing of the Ansible codebase.

Right now, the Vagrantfile is really complex and difficult to maintain.
To simplify its structure, I believe that we could drop the support to the OpenStack and/or to the Libvirt providers focusing on providing an easy way to test the different operating systems supported by the codebase in Virtualbox (for local development and testing) and a popular public cloud provider like DigitalOcean, AWS or GCloud (for multiple masters and minions provisioning).

Personally, this change would perfectly fit my workflow. 
Right now I'm using Virtualbox for local development and DigitalOcean to test PR #761.

@eparis @ingvagabund @danehans @rutsky @adamschaub do you believe that this changes could fit your workflow? Are you using different providers for your daily development of the project?

Thank you for you comments. Actually, I'm also looking for suggestions and insides on the tools and workflows you adopt to work at the project. 😉 
",closed,False,2016-05-12 22:28:59,2018-02-17 08:18:02
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/976,https://api.github.com/repos/kubernetes/contrib/issues/976,[ansible] add support for vagrant-aws provider to Vagrantfile,,closed,True,2016-05-13 00:06:46,2016-05-27 19:17:45
contrib,piosz,https://github.com/kubernetes/contrib/pull/977,https://api.github.com/repos/kubernetes/contrib/issues/977,[Cluster autoscaler] Moved cluster autoscaler godeps to vendor,,closed,True,2016-05-13 08:07:23,2016-05-13 09:05:37
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/978,https://api.github.com/repos/kubernetes/contrib/issues/978,[cluster-autoscaler] Add support for PodScheduled condition.,"The new logic is the following:
1. Take time of the newest node becoming available
2. Reset status for `PodScheduled` condition to `Unknown` for all the pods with status `True` and reason `Unschedulable` with `LastTransitionTime` before timestamp from 1)
3. Proceed only for newer unschedulable pods.

Ref: #25553
",closed,True,2016-05-13 10:07:51,2016-05-13 13:37:39
contrib,piosz,https://github.com/kubernetes/contrib/pull/979,https://api.github.com/repos/kubernetes/contrib/issues/979,Use all resources to compute node reservation,,closed,True,2016-05-13 12:33:36,2016-05-13 14:31:50
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/980,https://api.github.com/repos/kubernetes/contrib/issues/980,Cluster-autoscaler: Don't include unchedulable nodes in node list,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-13 13:31:39,2016-05-13 13:45:53
contrib,eparis,https://github.com/kubernetes/contrib/pull/981,https://api.github.com/repos/kubernetes/contrib/issues/981,"Get rid of godep at build, we have vendor/",,closed,True,2016-05-13 14:12:12,2016-05-13 14:12:18
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/982,https://api.github.com/repos/kubernetes/contrib/issues/982,"etcd refactor, more granular tasks for etcd. renamed top level playbooks","The first of the roles to undergo a refactor to be more like [openstack-ansible](https://github.com/openstack/openstack-ansible). Granular roles to enable more top level playbooks to be made such as `upgrade.yml` `uninstall.yml`, etc. This breaks down the few task files into more granular files. Renamed the top level playbooks to be more descriptive, once we have more playbooks at the top level we should use something like #967 to organize the top level directory. The current issue is roles are made for a first time deployment and that is it, the role structure did not leave much room for modification / reuse in other scenarios.

The end state is to deploy each role like a 'micro-service' that has granular control so one can spin up, spin down, individual roles without having damaging effects across the cluster. All the roles should be broken down in a similar way, and broken out into their own repos. The ansible folder could have a repo called kube-ansible, or [kube-deploy](https://github.com/kubernetes/kube-deploy), and each role will have its own repo that will get pulled in by kube-deploy as specified in a [requirements file](https://github.com/openstack/openstack-ansible/blob/master/ansible-role-requirements.yml).
- Validated on Virtualbox: CoreOS & CentOS

Could you all take a glance and provide feedback? I think contrib/ansible needs to start taking steps in this direction if we want it to be usable for production deployments and be plug-able for future deployment.

CC @danehans @eparis @adamschaub @ingvagabund 
",closed,True,2016-05-13 15:36:17,2016-06-30 16:34:14
contrib,petercm,https://github.com/kubernetes/contrib/issues/983,https://api.github.com/repos/kubernetes/contrib/issues/983,GCE Ingress fails to sync instance group in multi-az cluster,"When an ingress is spun up in a multi-zone cluster, no nodes end up in the k8s-ig instance group. My logs show

`I0513 12:21:19.826319       1 instances.go:130] Node pool encountered a 404, ignoring: googleapi: Error 404: The resource 'projects/myproject/zones/us-east1-c/instances/gke-mazcluster-default-pool-67470956-1n15' was not found, notFound`

(gke-mazcluster-default-pool-67470956-1n15 is not found in us-east1-c because it is in us-east1-d)

I believe the underlying issue here relates to getting the gce cloudprovider without config:

In [ClusterManager.getGCEClient()](https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/controller/cluster_manager.go#L193)

The GCE provider is retrieved with `nil` as the config param.
 `cloudInterface, err := cloudprovider.GetCloudProvider(""gce"", nil)`

Which means `cfg.Global.Multizone` would never get set, managedZones will not get set to nil and instead we will always be looking in the primary zone.

I believe the fix should be similar to kubernetes/kubernetes#23769 but I don't know how you might/if it's possible to get a reference to the kubelet and configured CloudProvider here vs. how it can be done in a plugin.

@bprashanth
",closed,False,2016-05-13 15:37:32,2016-06-09 20:17:38
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/984,https://api.github.com/repos/kubernetes/contrib/issues/984,Cluster-autoscaler: Add scripts for starting/stopping cluster autoscaler.,"Add scripts for starting/stopping cluster autoscaler.

Ref: https://github.com/kubernetes/contrib/issues/968
",closed,True,2016-05-13 15:45:29,2016-05-13 16:25:54
contrib,rakesh-k,https://github.com/kubernetes/contrib/pull/985,https://api.github.com/repos/kubernetes/contrib/issues/985,Update README.md,"corrected the ingress name - from uppercase I to lowercase I to match the directory name.
",closed,True,2016-05-13 15:51:34,2016-05-26 16:41:54
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/986,https://api.github.com/repos/kubernetes/contrib/issues/986,Cluster autoscaler: more debug in estimates and pod conditions,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-13 16:22:49,2016-05-13 16:42:31
contrib,ArtfulCoder,https://github.com/kubernetes/contrib/pull/987,https://api.github.com/repos/kubernetes/contrib/issues/987,dnsmasq docker image,,closed,True,2016-05-13 21:37:59,2016-05-13 23:10:58
contrib,mikedanese,https://github.com/kubernetes/contrib/issues/988,https://api.github.com/repos/kubernetes/contrib/issues/988,mungebot doesn't know about release-note-experimental,"cc @lavalamp @david-mcmahon @eparis 
",closed,False,2016-05-13 23:40:39,2016-06-08 20:12:49
contrib,apelisse,https://github.com/kubernetes/contrib/pull/989,https://api.github.com/repos/kubernetes/contrib/issues/989,Create new application to pull and push metrics,"The goal of this application is to pull information from different
sources and push them to a centralized TSDB.

It is based on mungegithub code a lot.

This is obviously a very early alpha version that misses:
- tests ..
- interesting command-line options
- containerization
- a thousand different things
",closed,True,2016-05-13 23:59:29,2016-06-09 16:29:02
contrib,zzy302,https://github.com/kubernetes/contrib/issues/990,https://api.github.com/repos/kubernetes/contrib/issues/990,"Issue about scale-demo, container aggregator logs""Error getting pods: empty input1.000167427s""","Hello, 
I'am running the test scale-demo refer to http://blog.kubernetes.io/2016/03/1000-nodes-and-beyond-updates-to-Kubernetes-performance-and-scalability-in-12.html . But from the web page, I can only see the numbers of NGINX and VEGETA, not QPS or RESPONSE TIME, but CPU load on node is high, then I checked the logs and found the aggregator logs of error like ""Error getting pods: empty input1.000167427s"". 

Some screen shots are attached, 

thanks for the help.

![](http://i2.buimg.com/33e58121775b0067.png)
![](http://i2.buimg.com/94d04c892325caf7.png)
![](http://i2.buimg.com/88325bf93cb349d1.png)
",closed,False,2016-05-16 02:48:34,2018-02-13 21:58:09
contrib,chbatey,https://github.com/kubernetes/contrib/pull/991,https://api.github.com/repos/kubernetes/contrib/issues/991,Fix typo in example godep restore,,closed,True,2016-05-16 07:14:55,2016-05-24 13:56:36
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/992,https://api.github.com/repos/kubernetes/contrib/issues/992,cluster-autoscaler: Fix slicing pods into old pods and new pods,"Previously after updating pod status all pods had `LastTransitionTime` after the threshold (time of newest node becoming available).

Additionally fix our makefile so that it's not required to set `REGISTRY` for every rule.

https://github.com/kubernetes/kubernetes/issues/24404

@mwielgus @jszczepkowski @piosz 
",closed,True,2016-05-16 10:16:30,2016-05-16 12:17:30
contrib,tmrts,https://github.com/kubernetes/contrib/pull/993,https://api.github.com/repos/kubernetes/contrib/issues/993,Add tmrts to the whitelist,"/cc @eparis 
",closed,True,2016-05-16 14:55:42,2016-05-23 20:12:17
contrib,johnnygofed,https://github.com/kubernetes/contrib/pull/994,https://api.github.com/repos/kubernetes/contrib/issues/994,adsd,"ss
",closed,True,2016-05-16 19:18:55,2016-05-16 19:19:03
contrib,mrahbar,https://github.com/kubernetes/contrib/pull/995,https://api.github.com/repos/kubernetes/contrib/issues/995,extracted etcd_download_file_name and added usage in github-release.yml,"I ran into this when trying to spin up the cluster on a ubuntu cluster.
Mainly the v of version was missing but I extracted it as a constant so it is consistent 
with the overall style.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/995)

<!-- Reviewable:end -->
",closed,True,2016-05-16 20:18:18,2016-09-06 07:22:34
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/996,https://api.github.com/repos/kubernetes/contrib/issues/996,Wild guess at inability to find LGTM,,closed,True,2016-05-16 20:39:19,2016-05-17 03:49:20
contrib,ArtfulCoder,https://github.com/kubernetes/contrib/pull/997,https://api.github.com/repos/kubernetes/contrib/issues/997,Disable kubelet-gce-e2e-ci till issues with node e2e in PR Builder are resolved.,"@ixdy @pwittrock 
",closed,True,2016-05-16 22:04:30,2016-05-16 22:46:11
contrib,spxtr,https://github.com/kubernetes/contrib/pull/998,https://api.github.com/repos/kubernetes/contrib/issues/998,"SQ Health for the last day, not since restart.","I also renamed the E2E tab to not be ""Google Internal""

@eparis I pointed this at you because I don't know who else to ask to review mungegithub stuff. Feel free to reassign.
",closed,True,2016-05-16 22:47:59,2016-05-17 12:17:17
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/999,https://api.github.com/repos/kubernetes/contrib/issues/999,Cluster-autoscaler Move scale up code to separate file,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-17 10:59:48,2016-05-17 11:50:15
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1000,https://api.github.com/repos/kubernetes/contrib/issues/1000,Cluster-autoscaler: ScaleUp and ScaleDown integration,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-17 13:42:13,2016-05-18 10:53:11
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/1001,https://api.github.com/repos/kubernetes/contrib/issues/1001,refactor common,"- Better naming convention on registered variables
- Removed init facts, put in `defaults/main.yml`

Validated via Virtualbox on CoreOS and CentOS
- plan on a follow up PR to move the `docker_config_dir` to the docker role where it should belong.
",closed,True,2016-05-17 19:13:25,2016-06-29 12:25:57
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1002,https://api.github.com/repos/kubernetes/contrib/issues/1002,[nginx-ingress-controller] Allow custom health checks in upstreams,"fixes #818
",closed,True,2016-05-17 20:08:10,2016-05-30 20:03:23
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1003,https://api.github.com/repos/kubernetes/contrib/issues/1003,DO NOT MERGE: cache hit debugging code,,closed,True,2016-05-17 20:57:11,2016-05-27 04:49:21
contrib,adamschaub,https://github.com/kubernetes/contrib/issues/1004,https://api.github.com/repos/kubernetes/contrib/issues/1004,Change to saltbase has broken kube-addons tasks,"Refactor of saltbase in kubernetes/kubernetes#23600 has removed files needed for kubernetes-addons. Need some tweaks to reflect the change.

```
TASK [kubernetes-addons : Make sure the system services namespace exists] ******
fatal: [kube-master]: FAILED! => {""changed"": false, ""dest"": ""/etc/kubernetes/addons/"", ""failed"": true, ""gid"": 0, ""group"": ""root"", ""mode"": ""0755"", ""msg"": ""Request failed"", ""owner"": ""root"", ""response"": ""HTTP Error 404: Not Found"", ""size"": 4096, ""state"": ""directory"", ""status_code"": 404, ""uid"": 0, ""url"": ""https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/saltbase/salt/kube-addons/namespace.yaml""}
```
",closed,False,2016-05-17 23:08:42,2016-05-31 20:59:26
contrib,adamschaub,https://github.com/kubernetes/contrib/issues/1005,https://api.github.com/repos/kubernetes/contrib/issues/1005,Change to saltbase has broken kube-addons tasks,"Refactor of saltbase in kubernetes/kubernetes#23600 has removed files needed for kubernetes-addons. Need some tweaks to reflect the change.

```
TASK [kubernetes-addons : Make sure the system services namespace exists] ******
fatal: [kube-master]: FAILED! => {""changed"": false, ""dest"": ""/etc/kubernetes/addons/"", ""failed"": true, ""gid"": 0, ""group"": ""root"", ""mode"": ""0755"", ""msg"": ""Request failed"", ""owner"": ""root"", ""response"": ""HTTP Error 404: Not Found"", ""size"": 4096, ""state"": ""directory"", ""status_code"": 404, ""uid"": 0, ""url"": ""https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/saltbase/salt/kube-addons/namespace.yaml""}
```

Ref: kubernetes/kubernetes#23233
",closed,False,2016-05-17 23:10:35,2016-05-17 23:20:12
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1006,https://api.github.com/repos/kubernetes/contrib/issues/1006,detect unneeded retests,"tested! works!
",closed,True,2016-05-17 23:12:55,2016-05-18 05:33:54
contrib,MHBauer,https://github.com/kubernetes/contrib/pull/1007,https://api.github.com/repos/kubernetes/contrib/issues/1007,checkout into k8s.io,"- using github.com will cause the build to blow up

mostly copied from the kubernetes devel docs.
",closed,True,2016-05-17 23:37:10,2016-05-26 16:39:23
contrib,itayariel,https://github.com/kubernetes/contrib/pull/1008,https://api.github.com/repos/kubernetes/contrib/issues/1008,Update link in documentaion,,closed,True,2016-05-18 09:01:15,2016-05-26 16:39:01
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1009,https://api.github.com/repos/kubernetes/contrib/issues/1009,Cluster-autoscaler: PredicateChecker and better nodeInfo handling for ScaleDown,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-18 11:42:50,2016-05-20 15:44:33
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1010,https://api.github.com/repos/kubernetes/contrib/issues/1010,Cluster-autoscaler: Fast drain impl that doesn't talk to api server,"cc: @piosz @fgrzadkowski 

Depends on #1009 
",closed,True,2016-05-18 14:01:34,2016-05-23 08:05:47
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1011,https://api.github.com/repos/kubernetes/contrib/issues/1011,ignore failures that don't happen twice in a row,"Actually parse the junit result, allow merging to continue as long as we were able to get the list of failed tests and it doesn't intersect with the same list for the previous run.
",closed,True,2016-05-18 21:25:50,2016-05-19 12:21:12
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1012,https://api.github.com/repos/kubernetes/contrib/issues/1012,Fix bugs,"Found a bug that will consume tokens until it triggers an ""abuse prevention mechanism"".
",closed,True,2016-05-18 22:38:27,2016-05-18 22:57:31
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1013,https://api.github.com/repos/kubernetes/contrib/issues/1013,Add a /metrics endpoint to submit-queue for external monitoring,"/cc @eparis

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1013)

<!-- Reviewable:end -->
",closed,True,2016-05-18 23:57:10,2016-10-29 00:02:44
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/1014,https://api.github.com/repos/kubernetes/contrib/issues/1014,readme to spec new netaddr name,"- `python-netaddr` -> `netaddr`
- added `--provider virtualbox` under the virtualbox section

Fixes: #764
",closed,True,2016-05-19 13:32:14,2016-07-29 13:32:50
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/1015,https://api.github.com/repos/kubernetes/contrib/issues/1015,refactor for docker_config_dir to be in docker role,"Validated on Virtualbox: CoreOS and CentOS
",closed,True,2016-05-19 15:42:25,2016-05-26 16:38:43
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1016,https://api.github.com/repos/kubernetes/contrib/issues/1016,"Merge queue: after running tests, keep a PR at the top of the queue.","It often happens in the merge queue:

PR A is being tested.
PR B is added into the queue.
PR B is higher priority than PR A.
Head flakes.
PR A tests finish.
Head goes green.
PR B starts getting tested.

Bug report: at that last step, due to my optimization in #1006, if PR A were checked again, it'd get merged immediately. Instead, PR B is checked, then PR A tests are run AGAIN.

Desired behavior is to leave a PR that's just been tested at the top of the queue until head goes green. Then: if it's mergable, merge it. Otherwise, sort the queue again.
",closed,False,2016-05-19 18:08:45,2016-05-26 17:43:35
contrib,eparis,https://github.com/kubernetes/contrib/pull/1017,https://api.github.com/repos/kubernetes/contrib/issues/1017,[mungegithub] Little angular and whitespace cleanup,,closed,True,2016-05-19 18:49:05,2016-05-24 15:05:22
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/1018,https://api.github.com/repos/kubernetes/contrib/issues/1018,virtualbox as the default provider,"- My assumption is most people don't have an openstack cluster at their disposal to work with, so why is it the default, we should order these by which is the most likely vagrant deployment platform first and so on... If it's openstack -> libvirt -> virtualbox then we can close here, just wanted to bring it into question. 
",closed,True,2016-05-19 19:15:00,2016-05-26 16:38:18
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1019,https://api.github.com/repos/kubernetes/contrib/issues/1019,Update echoheaders,"This reverts the redirect to port 443
",closed,True,2016-05-19 20:13:39,2016-05-26 04:03:34
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1020,https://api.github.com/repos/kubernetes/contrib/issues/1020,Merge queue: add a way for the build cop to mark a failure as a flake,"... or as due to a reason that shouldn't stop the queue.
",closed,False,2016-05-19 20:58:24,2016-06-10 00:07:07
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1021,https://api.github.com/repos/kubernetes/contrib/issues/1021,File flake issues part 1,"Next step is to manage github issues. Right now this produces a page with the following output:

```
[
  {
    ""Job"": ""kubernetes-e2e-gce"",
    ""Number"": 17234,
    ""Test"": ""[k8s.io] Kubectl client [k8s.io] Kubectl logs should be able to retrieve and filter logs [Conformance] {Kubernetes e2e suite}"",
    ""Reason"": ""/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:940\nMay 19 16:16:10.333: Verified 0 of 1 pods , error : timed out waiting for the condition""
  },
  {
    ""Job"": ""kubernetes-e2e-gce-scalability"",
    ""Number"": 7615,
    ""Test"": ""[k8s.io] Load capacity [Feature:Performance] should be able to handle 30 pods per node {Kubernetes e2e suite}"",
    ""Reason"": ""/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/load.go:71\nExpected\n    \u003cint\u003e: 1\nnot to be \u003e\n    \u003cint\u003e: 0""
  }
]
```

I think the tests caught all my bugs.
",closed,True,2016-05-19 23:35:47,2016-05-23 22:46:58
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1022,https://api.github.com/repos/kubernetes/contrib/issues/1022,unit tests take over an hour,"Submit queue just gave up on a good PR because of this.
",closed,True,2016-05-20 00:01:26,2016-05-20 12:50:29
contrib,asalkeld,https://github.com/kubernetes/contrib/pull/1023,https://api.github.com/repos/kubernetes/contrib/issues/1023,"Assign issues based on the ""fixes #<n>"" in the PR body","This is initially from a discussion on kubernetes-dev about the struggle to set the assignee
field in issues. It does not solve the problem of setting this field as the developer intends to
work on an issue, but at least fills it in once the PR is created. Ideas welcome...
",closed,True,2016-05-20 06:45:15,2016-05-27 03:15:48
contrib,wojtek-t,https://github.com/kubernetes/contrib/issues/1024,https://api.github.com/repos/kubernetes/contrib/issues/1024,Merge queue crashed,"Merge queue just crashed with the following error:

E0520 07:57:15.093393       1 github.go:1155] Can't load commit kubernetes kubernetes bf1f3d37206b0a69c42b8df64c84942c07d5fc3e: GET https://api.github.com/repos/kubernetes/kubernetes/commits/bf1f3d37206b0a69c42b8df64c84942c07d5fc3e: 500  []
E0520 07:57:15.133638       1 github.go:899] Error getting events for issue: GET https://api.github.com/repos/kubernetes/kubernetes/issues/25921/events?page=1&per_page=100: 500  []
panic: runtime error: invalid memory address or nil pointer dereference
[signal 0xb code=0x1 addr=0x0 pc=0x4a4572]

goroutine 1 [running]:
panic(0x9910e0, 0xc820014060)
    /usr/local/google/home/dbsmith/.gvm/gos/go1.6/src/runtime/panic.go:464 +0x3e6
k8s.io/contrib/mungegithub/github.(_MungeObject).LabelCreator(0xc82cacea10, 0xc82000be26, 0xc, 0x0, 0x0)
    /usr/local/google/home/dbsmith/code/k3/src/k8s.io/contrib/mungegithub/github/github.go:577 +0x62
k8s.io/contrib/mungegithub/mungers.(_PathLabelMunger).Munge(0xc820112570, 0xc82cacea10)
    /usr/local/google/home/dbsmith/code/k3/src/k8s.io/contrib/mungegithub/mungers/path_label.go:153 +0x5d6
k8s.io/contrib/mungegithub/mungers.MungeIssue(0xc82cacea10, 0x0, 0x0)
    /usr/local/google/home/dbsmith/code/k3/src/k8s.io/contrib/mungegithub/mungers/mungers.go:121 +0x9a
k8s.io/contrib/mungegithub/github.(_Config).ForEachIssueDo(0xc82006db00, 0xb597a0, 0x0, 0x0)
    /usr/local/google/home/dbsmith/code/k3/src/k8s.io/contrib/mungegithub/github/github.go:1551 +0xc56
main.doMungers(0xc82006db00, 0x0, 0x0)
    /usr/local/google/home/dbsmith/code/k3/src/k8s.io/contrib/mungegithub/mungegithub.go:65 +0x20a
main.main.func1(0xc820082e00, 0xc820112750, 0x0, 0x3, 0x0, 0x0)
    /usr/local/google/home/dbsmith/code/k3/src/k8s.io/contrib/mungegithub/mungegithub.go:106 +0x30f
k8s.io/contrib/mungegithub/vendor/github.com/spf13/cobra.(_Command).execute(0xc820082e00, 0xc82000a290, 0x3, 0x3, 0x0, 0x0)
    /usr/local/google/home/dbsmith/code/k3/src/k8s.io/contrib/mungegithub/vendor/github.com/spf13/cobra/command.go:561 +0x62c
k8s.io/contrib/mungegithub/vendor/github.com/spf13/cobra.(_Command).ExecuteC(0xc820082e00, 0xc820082e00, 0x0, 0x0)
    /usr/local/google/home/dbsmith/code/k3/src/k8s.io/contrib/mungegithub/vendor/github.com/spf13/cobra/command.go:651 +0x55c
k8s.io/contrib/mungegithub/vendor/github.com/spf13/cobra.(_Command).Execute(0xc820082e00, 0x0, 0x0)
    /usr/local/google/home/dbsmith/code/k3/src/k8s.io/contrib/mungegithub/vendor/github.com/spf13/cobra/command.go:610 +0x2d
main.main()
    /usr/local/google/home/dbsmith/code/k3/src/k8s.io/contrib/mungegithub/mungegithub.go:124 +0x395

@lavalamp @eparis @gmarek 
",closed,False,2016-05-20 08:14:07,2016-06-01 21:05:57
contrib,jszczepkowski,https://github.com/kubernetes/contrib/pull/1025,https://api.github.com/repos/kubernetes/contrib/issues/1025,Added events for cluster autoscaler.,"Added events for cluster autoscaler.
",closed,True,2016-05-20 12:42:27,2016-05-20 15:17:41
contrib,piosz,https://github.com/kubernetes/contrib/pull/1026,https://api.github.com/repos/kubernetes/contrib/issues/1026,[Cluster autoscaler] Added protection againts adding unnecessary nodes,"cc @fgrzadkowski
",closed,True,2016-05-20 16:34:50,2016-05-23 11:47:44
contrib,rsmitty,https://github.com/kubernetes/contrib/issues/1027,https://api.github.com/repos/kubernetes/contrib/issues/1027,Bump nginx-ingress-controller versions for new image,"After the merge of #898 into master, the nginx controller image needs to be rebuilt with a new version tag. I plan to resolve this one myself by bumping the versions in the Makefile and various templates under the nginx folder.
",closed,False,2016-05-20 19:11:42,2016-05-20 20:46:01
contrib,rsmitty,https://github.com/kubernetes/contrib/pull/1028,https://api.github.com/repos/kubernetes/contrib/issues/1028,[ingress] bump makefile and rc specifications to v0.62 for a new build,"Closes #1027. The following files are version bumped, with my understanding that a new build of the nginx-ingress-controller image will be auto-built from this merge. Merging this will allow for #898 to make it's way into the official image.
",closed,True,2016-05-20 19:24:35,2016-05-20 20:46:01
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1029,https://api.github.com/repos/kubernetes/contrib/issues/1029,Contributor leaderboard,"Dashboard with contributor stats:
- PRs merged (last week, quarter, all time)
- PRs reviewed (most comments/PR and/or applied LGTM?)
- flaky test issues fixed
- bugs fixed
- help-wanted issues fixed

By individual and by company.
",closed,False,2016-05-20 19:42:02,2018-01-23 02:59:13
contrib,stevesloka,https://github.com/kubernetes/contrib/pull/1030,https://api.github.com/repos/kubernetes/contrib/issues/1030,Add support for f5 BIG-IP,"This is still a little bit wip against #12. I wrote the infoblox piece a while back and just realized that I need to implement the response bodies to get the IP's to pass to f5. So I'll look at that and get it cleaned up soon. 

My question for everyone else is how are you managing IP's? At my company we're using Infoblox to implement DNS & IP addresses, but want to make sure it works for everyone. 

//cc @bprashanth @chrissnell @vipulsabhaya

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1030)

<!-- Reviewable:end -->
",closed,True,2016-05-21 02:39:31,2017-10-02 18:34:22
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1031,https://api.github.com/repos/kubernetes/contrib/issues/1031,Auto-file issues part 2,"An example issue - I just made a repo for testing: https://github.com/lavalamp/issue-testing/issues/5

first 4 commits are in #1021
",closed,True,2016-05-22 05:01:11,2016-05-24 01:03:27
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1032,https://api.github.com/repos/kubernetes/contrib/issues/1032,[wip] Teach l7 controller about zones,"Still needs unittests
",closed,True,2016-05-23 02:48:29,2016-06-13 17:43:16
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1033,https://api.github.com/repos/kubernetes/contrib/issues/1033,Cluster-autoscaler: do full scheduling check in calculating node utilization in scale-down,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-23 10:52:36,2016-05-23 12:41:02
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1034,https://api.github.com/repos/kubernetes/contrib/issues/1034,Cluster-autoscaler: clear hostname when looking for a new node for pod,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-23 14:20:01,2016-05-23 14:43:57
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1035,https://api.github.com/repos/kubernetes/contrib/issues/1035,Cluster-autoscaler: scaledown logging fix,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-23 15:43:34,2016-05-24 05:08:23
contrib,piosz,https://github.com/kubernetes/contrib/pull/1036,https://api.github.com/repos/kubernetes/contrib/issues/1036,Cluster-autoscaler: Implemented unit test for FilterOutSchedulable function,"as requested in https://github.com/kubernetes/contrib/pull/1026#discussion_r64196555
",closed,True,2016-05-23 15:58:15,2016-05-24 20:28:02
contrib,roberthbailey,https://github.com/kubernetes/contrib/issues/1037,https://api.github.com/repos/kubernetes/contrib/issues/1037,Submit Queue's Bot Stats tab should make it clear when it was last restarted,"@goltermann pointed out to me that the E2E Tests tab shows when the queue was last restarted, but I always forget to look there when I'm trying to figure out the state. The Bot Stats tab under INFO shows a bunch of useful times, and it would be nice if right above ""Next Run Loop"" it showed ""Queue Start Time"" or some such heading with the date and time at which the queue was last restarted. 
",closed,False,2016-05-23 17:08:21,2016-05-31 20:53:28
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1038,https://api.github.com/repos/kubernetes/contrib/issues/1038,cluster-autoscaler: Events reports wrong number of mig instances,"Event reports:

```
sizes (current/new): 3/4
```

wheres it should be:

```
sizes (current/new): 3/7
```
",closed,False,2016-05-23 21:29:31,2016-05-23 21:32:01
contrib,ArtfulCoder,https://github.com/kubernetes/contrib/pull/1039,https://api.github.com/repos/kubernetes/contrib/issues/1039,removed unused file,,closed,True,2016-05-23 21:37:49,2016-05-23 21:38:33
contrib,ArtfulCoder,https://github.com/kubernetes/contrib/pull/1040,https://api.github.com/repos/kubernetes/contrib/issues/1040,removed tabs in Makefile before conditionals,,closed,True,2016-05-23 21:48:38,2016-05-23 21:54:18
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1041,https://api.github.com/repos/kubernetes/contrib/issues/1041,Cluster-autoscaler: enforce min mig size in scale-down,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-23 21:52:10,2016-05-24 05:07:00
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1042,https://api.github.com/repos/kubernetes/contrib/issues/1042,Submit Queue / munger: keep state over restarts,"Could be as simple as storing a static file in GCS or as complex as adding a database to the things we run in the utility cluster.

P0 Requirements:
- remember merge stats across restarts.
",closed,False,2016-05-23 22:39:51,2016-08-26 00:26:12
contrib,brendandburns,https://github.com/kubernetes/contrib/issues/1043,https://api.github.com/repos/kubernetes/contrib/issues/1043,"Merge bot re-runs all tests, doesn't detect fresh ones.","On the following PR:

https://github.com/kubernetes/kubernetes/pull/25374

MergeBot said that tests were too old and re-ran.  While it looks like unit tests were too old, there were fresh e2e test results that had only been run 2 hours previously.

MergeBot should only re-run tests that are actually stale using specific commands (e.g. `unit test this`) instead of re-running all tests.
",closed,False,2016-05-23 23:41:47,2018-02-13 17:54:10
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1044,https://api.github.com/repos/kubernetes/contrib/issues/1044,Keep test pr on top redo,"Try to fix #1016 
",closed,True,2016-05-23 23:47:48,2016-05-26 17:43:35
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1045,https://api.github.com/repos/kubernetes/contrib/issues/1045,merge bot doesn't read the junit XML files that the unit tests make,"...because they are named differently from the e2e tests.

I don't care if we change the unit test output or change the merge bot. @ixdy opinions?
",closed,False,2016-05-24 06:18:29,2016-06-14 17:44:52
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1046,https://api.github.com/repos/kubernetes/contrib/issues/1046,BeforeSuite failure should count as entire run failing,"It's not a normal test, it means the cluster startup failed.
",closed,False,2016-05-24 06:21:48,2018-02-13 22:59:12
contrib,nottix,https://github.com/kubernetes/contrib/issues/1047,https://api.github.com/repos/kubernetes/contrib/issues/1047,Nginx Ingress Controller - Infinite loop requests,"Hi @aledbf ,

i'm having this errors on log (also in the latest version):

`127.0.0.1 - [127.0.0.1] - - [24/May/2016:07:44:19 +0000] ""GET / HTTP/1.1"" 503 23342 ""-"" ""lua-resty-http/0.07 (Lua) ngx_lua/10001"" 117 14.774 - - - -
2016/05/24 07:44:19 [info] 23#23: *45353 client 127.0.0.1 closed keepalive connection
127.0.0.1 - [127.0.0.1] - - [24/May/2016:07:44:19 +0000] ""GET / HTTP/1.1"" 503 23343 ""-"" ""lua-resty-http/0.07 (Lua) ngx_lua/10001"" 117 14.774 - - - -
2016/05/24 07:44:19 [info] 23#23: *45351 client 127.0.0.1 closed keepalive connection
127.0.0.1 - [127.0.0.1] - - [24/May/2016:07:44:19 +0000] ""GET / HTTP/1.1"" 503 23344 ""-"" ""lua-resty-http/0.07 (Lua) ngx_lua/10001"" 117 14.775 - - - -
2016/05/24 07:44:19 [info] 23#23: *45349 client 127.0.0.1 closed keepalive connection
127.0.0.1 - [127.0.0.1] - - [24/May/2016:07:44:19 +0000] ""GET / HTTP/1.1"" 503 23345 ""-"" ""lua-resty-http/0.07 (Lua) ngx_lua/10001"" 117 14.775 - - - -`

it is repeating infinitely on our 3 instances.

Thank you.
",closed,False,2016-05-24 08:03:28,2016-05-24 16:56:19
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/1048,https://api.github.com/repos/kubernetes/contrib/issues/1048,cluster-autoscaler: Use all predicates in simulator,"Ref https://github.com/kubernetes/kubernetes/issues/24404

@davidopp @piosz @mwielgus 
",closed,True,2016-05-24 10:17:13,2016-05-24 13:05:00
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1049,https://api.github.com/repos/kubernetes/contrib/issues/1049,"Cluster-autoscaler: do not delete nodes with kube-system, not-mirrored, not-daemon set pods","cc: @piosz @fgrzadkowski 
",closed,True,2016-05-24 10:29:00,2016-05-24 15:30:14
contrib,thesuperzapper,https://github.com/kubernetes/contrib/issues/1050,https://api.github.com/repos/kubernetes/contrib/issues/1050,[ansible] Addons/kube-system fail to create on Fedora 23,"After pulling the /kubernetes/contrib/ master, and deploying onto my Fedora 23 cluster, I found that all `kube-addons` and the `kube-system` namespace, failed to deploy, effectively leaving the cluster in a useless state. 

After some troubleshooting, I found PR #942 and #797, both of which I applied locally, and then deployed, my cluster now scaffolds without a problem.

Can we get #942 / #797 moving? 
Or at least, can someone verify that the master repo is broken in its current state for new deploys?
",closed,False,2016-05-24 11:52:03,2016-05-25 11:03:52
contrib,clanstyles,https://github.com/kubernetes/contrib/issues/1051,https://api.github.com/repos/kubernetes/contrib/issues/1051,nginx ingress controller wont compile,"```
make controller
rm -f nginx-ingress-controller
CGO_ENABLED=0 GOOS=linux godep go build -a -installsuffix cgo -ldflags \
    ""-w -X main.version=git-69ba059 -X main.gitRepo=git@github.com:kubernetes/contrib.git"" \
    -o nginx-ingress-controller
# github.com/kubernetes/contrib/ingress/controllers/nginx
./controller.go:126: cannot use kubeClient (type *""github.com/kubernetes/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/unversioned"".Client) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/unversioned"".Client in argument to nginx.NewManager
./controller.go:341: cannot use cfg (type *""github.com/kubernetes/contrib/ingress/vendor/k8s.io/kubernetes/pkg/api"".ConfigMap) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/api"".ConfigMap in argument to lbc.nginx.ReadConfig
godep: go exit status 2
Makefile:14: recipe for target 'controller' failed
make: *** [controller] Error 1
```
",closed,False,2016-05-24 13:30:29,2016-06-29 17:16:03
contrib,simonswine,https://github.com/kubernetes/contrib/pull/1052,https://api.github.com/repos/kubernetes/contrib/issues/1052,ansible: upgrade default kube version to 1.2.4,"I followed @elemoine 's suggestion and raised the default kube version to the latest release

CoreOS upgrade from 1.2.2 and from scratch works fine locally
",closed,True,2016-05-24 14:10:11,2016-05-24 15:01:13
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1053,https://api.github.com/repos/kubernetes/contrib/issues/1053,Cluster-autoscaler: fast drain tests,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-24 14:38:52,2016-05-24 20:41:40
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1054,https://api.github.com/repos/kubernetes/contrib/issues/1054,[nginx-ingress-controller] Add ssl certificate checksum to template,"fixes #800
",closed,True,2016-05-24 16:09:54,2016-05-26 03:46:03
contrib,ArtfulCoder,https://github.com/kubernetes/contrib/pull/1055,https://api.github.com/repos/kubernetes/contrib/issues/1055,Removed dnsmasq.conf,,closed,True,2016-05-24 17:53:46,2016-05-24 18:29:04
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/1056,https://api.github.com/repos/kubernetes/contrib/issues/1056,make weakly stable builds disableable,"cc @eparis @lavalamp
",closed,True,2016-05-24 18:40:44,2016-05-24 19:23:41
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1057,https://api.github.com/repos/kubernetes/contrib/issues/1057,[nginx-ingress-controller]: Remove loadBalancer ip on shutdown,,closed,True,2016-05-24 18:51:05,2016-05-26 02:57:07
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1058,https://api.github.com/repos/kubernetes/contrib/issues/1058,Submit queue is wildly optimistic,"Right now it claims `Estimated Merging 38238.829 PRs per day.` If only!
",closed,False,2016-05-24 19:11:14,2016-05-24 19:22:54
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1059,https://api.github.com/repos/kubernetes/contrib/issues/1059,committing current correct options,,closed,True,2016-05-24 19:28:45,2016-05-24 19:41:56
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1060,https://api.github.com/repos/kubernetes/contrib/issues/1060,Submit queue: behavior when a closed issue exists for a flake,"When a new flake comes in for a test with an issue that's been closed, the desired behavior is:
1. Open a new issue
2. Link to the previous issue (if relatively easy).
",closed,False,2016-05-24 23:01:37,2016-05-27 03:12:51
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1061,https://api.github.com/repos/kubernetes/contrib/issues/1061,handle closed & duplicate issues,"Fixes #1060. Running this against my test repo now.
",closed,True,2016-05-25 01:00:43,2016-05-27 03:12:51
contrib,lth2015,https://github.com/kubernetes/contrib/pull/1062,https://api.github.com/repos/kubernetes/contrib/issues/1062,add mongodb replicaset with ceph,"add a sample which mongodb replicaset using kubernetes and ceph rbd.
",closed,True,2016-05-25 02:44:37,2016-05-28 03:47:16
contrib,simonswine,https://github.com/kubernetes/contrib/pull/1063,https://api.github.com/repos/kubernetes/contrib/issues/1063,ingress: nginx controller watches referenced tls secrets,"This enables the controller to reload certificates on changes in the secrets.

We are maintaining a list of referenced certificates in secrMetadata to only reload/sync the controller when something in referenced certificates changes.
",closed,True,2016-05-25 10:40:51,2016-06-03 17:34:52
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1064,https://api.github.com/repos/kubernetes/contrib/issues/1064,Enable CPU and Memory accounting for systemd services,"Enable CPU and Memory accounting for systemd services by default when building from local sources.
For distribution packages (rpms, deb, etc.) let the installation mechanism to take care of accounting.

Quoting:
Fix system container detection in kubelet on systemd.

This fixed environments where CPU and Memory Accounting were not enabled on the unit
that launched the kubelet or docker from reporting the root cgroup when
monitoring usage stats for those components.

From: https://github.com/kubernetes/kubernetes/pull/25982
",closed,True,2016-05-25 15:00:26,2016-05-26 16:35:46
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1065,https://api.github.com/repos/kubernetes/contrib/issues/1065,Cluster-autoscaler: simulator node related code tests,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-25 15:25:48,2016-05-25 22:54:30
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1066,https://api.github.com/repos/kubernetes/contrib/issues/1066,Cluster-autoscaler: clear utilization map on node deletion,"cc: @piosz @fgrzadkowski
",closed,True,2016-05-25 15:39:35,2016-05-26 05:36:59
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1067,https://api.github.com/repos/kubernetes/contrib/issues/1067,Cluster-autoscaler: skip scale down if schedulable but not scheduled pods are present,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-25 15:50:19,2016-05-26 19:29:54
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1068,https://api.github.com/repos/kubernetes/contrib/issues/1068,"Submit queue: PRs with ""risky-change"" label should be merged during business hours.","Take a flag with a range of times, e.g. --risky-changes-allowed=8am-1pm,11pm-4am (these are California times, but the flag should take UTC times). Then sort PRs with this label into the queue such that they'll get to the top around when the window opens. We can actively manage their position in the queue if the queue is blocked etc.

This is to try and get these things merged when humans are watching the jenkins jobs. We shouldn't merge risky changes at 5pm and go home. The times I put above are roughly mornings for MTV and WAW.
",closed,False,2016-05-25 17:27:58,2016-08-26 00:24:56
contrib,freehan,https://github.com/kubernetes/contrib/pull/1069,https://api.github.com/repos/kubernetes/contrib/issues/1069,add util to list files in bucket with matching prefix,"First part of #1045
",closed,True,2016-05-25 20:20:56,2016-05-25 21:52:21
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1070,https://api.github.com/repos/kubernetes/contrib/issues/1070,Merge queue: look up old test results in case of downtime,"First commit is in #1061
",closed,True,2016-05-25 20:38:59,2016-05-27 20:44:00
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1071,https://api.github.com/repos/kubernetes/contrib/issues/1071,Submit queue: don't file individual flake issues when groups of tests fail in a suite,"It's too noisy.

Alternatively, file each issue but also file an issue for the group and cross-link.
",closed,False,2016-05-25 20:40:56,2016-05-27 21:31:39
contrib,eparis,https://github.com/kubernetes/contrib/pull/1072,https://api.github.com/repos/kubernetes/contrib/issues/1072,Path blocking config ignore 'federation' docs,"https://github.com/kubernetes/kubernetes/pull/26313
",closed,True,2016-05-26 04:02:09,2016-05-26 16:32:37
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1073,https://api.github.com/repos/kubernetes/contrib/issues/1073,cluster-autoscaler: we should write logs to a file and save it in jenkins,"Basically have access similarly to other components like scheduler or apiserver. Now it's hard to debug e2e failures.
",closed,False,2016-05-26 06:43:51,2016-06-13 21:15:22
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1074,https://api.github.com/repos/kubernetes/contrib/issues/1074,cluster-autoscaler: Log that we are capping by MAX in scale-up,"I think that currently we don't do this. If we want to add 4 machines, but due MAX_NODES we can only add 1 node we are not logging anything (or I don't see it.)
",closed,False,2016-05-26 06:50:53,2016-06-09 23:04:17
contrib,teijinyer,https://github.com/kubernetes/contrib/issues/1075,https://api.github.com/repos/kubernetes/contrib/issues/1075,"service-loadbalancer: It does't have a filter,like ingress?",,closed,False,2016-05-26 07:42:18,2018-02-13 20:57:09
contrib,pieterlange,https://github.com/kubernetes/contrib/pull/1076,https://api.github.com/repos/kubernetes/contrib/issues/1076,Update nginx to version 1.11.0,"Small but very useful ($request_id) upgrade from nginx 1.10.0 to 1.11.0.

http://nginx.org/en/CHANGES
",closed,True,2016-05-26 11:30:40,2016-05-26 12:59:44
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/1077,https://api.github.com/repos/kubernetes/contrib/issues/1077,Moved openstack to the last vagrant option,"@eparis following you comment in #1018
",closed,True,2016-05-26 17:33:15,2016-05-26 17:39:00
contrib,freehan,https://github.com/kubernetes/contrib/pull/1078,https://api.github.com/repos/kubernetes/contrib/issues/1078,add support for test util,,closed,True,2016-05-26 18:53:40,2016-05-26 23:56:23
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1079,https://api.github.com/repos/kubernetes/contrib/issues/1079,WIP: [nginx-ingress-controller] Add support for rewrite,"fixes #860
",closed,True,2016-05-26 21:38:19,2016-05-31 16:56:37
contrib,freehan,https://github.com/kubernetes/contrib/pull/1080,https://api.github.com/repos/kubernetes/contrib/issues/1080,fix junit output file scanning logic,"closes: #1045
",closed,True,2016-05-26 23:49:58,2016-05-27 20:44:33
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1081,https://api.github.com/repos/kubernetes/contrib/issues/1081,Don't file individual issues when big groups of tests fail.,"Fixes #1071

Builds on #1070. Only the last commit is new.
",closed,True,2016-05-27 00:36:05,2016-05-27 21:31:39
contrib,asalkeld,https://github.com/kubernetes/contrib/pull/1082,https://api.github.com/repos/kubernetes/contrib/issues/1082,Enable the assign-fixes githubmunger,"The code for this munger has just been merged, this just enables it.

https://github.com/kubernetes/contrib/blob/master/mungegithub/mungers/assign-fixes.go

This munger will search for ""fixes #<num>"" in a PR message text and assign the referenced issues to the owner of the PR.
",closed,True,2016-05-27 03:54:13,2016-06-07 18:14:26
contrib,eparis,https://github.com/kubernetes/contrib/pull/1083,https://api.github.com/repos/kubernetes/contrib/issues/1083,Introduce a flag to block merges for certain milestones,"This blocks merge for v1.4 and next-candidate. When we open for v1.4 we
should remove `v1.4` from the flags and remove `next-candidate` from all
PRs.
",closed,True,2016-05-27 04:23:11,2016-05-27 21:22:51
contrib,eparis,https://github.com/kubernetes/contrib/pull/1084,https://api.github.com/repos/kubernetes/contrib/issues/1084,Show retests avoided,"And some other minor display updates
",closed,True,2016-05-27 04:23:40,2016-05-27 13:32:36
contrib,tongda,https://github.com/kubernetes/contrib/pull/1085,https://api.github.com/repos/kubernetes/contrib/issues/1085,fix the etcd file name mismatch,,closed,True,2016-05-27 05:43:17,2016-05-27 15:52:16
contrib,micheleorsi,https://github.com/kubernetes/contrib/issues/1086,https://api.github.com/repos/kubernetes/contrib/issues/1086,[nginx ingress controller] Requests lost while re-deploying,"We are trying to define a configuration where no requests are lost while re-deploying of nginx-ingress-controller. 

So we have this configuration 
- F5 in front of our bare-metal nodes
- Deployment resource that we apply with this configuration

```
  replicas: 1
  minReadySeconds: 15
  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 100
...

      containers:
      - image: <our-image-forked-from-the-gcr.io/google_containers/nginx-ingress-controller:0.62-with-some-logs>
        name: nginx-ingress-ctrl
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10249
            scheme: HTTP
          initialDelaySeconds: 20
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command:
              - cat
              - /tmp/ready
          initialDelaySeconds: 15
          timeoutSeconds: 5
          periodSeconds: 5
          successThreshold: 2
          failureThreshold: 1
        lifecycle:
          preStop:
            exec:
              # SIGTERM triggers a quick exit; gracefully terminate instead
              command: [""rm"",""/tmp/ready""]
              command: sleep 10
              command: nginx - q quit
          postStart:
            exec:
              command:
                - ""touch""
                - ""/tmp/ready""
```

So the idea is to be really strict on the readiness probe in order that as soon as the preStop is called, we take out that machine from ""balancing"".

The readinessProbe check works: once the pod receives the preStop hook I can see that it goes to ""Ready: false"".
But the problem is that the nginx ingress controller pod continues to handle requests. So that when kubernetes shut down it, a number of requests return errors. (to be precise here is the error: Request 'standard' failed: java.io.IOException: Remotely closed)

This is some logging:

```
I0527 11:39:38.648945       1 main.go:186] Received SIGTERM, shutting down
I0527 11:39:38.648990       1 controller.go:905] updating 2 Ingress rule/s
I0527 11:39:38.651683       1 controller.go:918] Updating loadbalancer service2-production/service2-ingress. Removing IP 10.100.0.104
I0527 11:39:38.716403       1 event.go:216] Event(api.ObjectReference{Kind:""Ingress"", Namespace:""service2-production"", Name:""service2-ingress"", UID:""c93eeab0-21cf-11e6-b056-8a5128ea7197"", APIVersion:""extensions"", ResourceVersion:""9347650"", FieldPath:""""}): type: 'Normal' reason: 'UPDATE' service2-production/service2-ingress
I0527 11:39:38.716485       1 event.go:216] Event(api.ObjectReference{Kind:""Ingress"", Namespace:""service2-production"", Name:""service2-ingress"", UID:""c93eeab0-21cf-11e6-b056-8a5128ea7197"", APIVersion:""extensions"", ResourceVersion:""9347419"", FieldPath:""""}): type: 'Normal' reason: 'DELETE' ip: 10.100.0.104
I0527 11:39:38.718317       1 controller.go:918] Updating loadbalancer service1-production/service1-ingress. Removing IP 10.100.0.104
I0527 11:39:38.719995       1 controller.go:379] not contained: [{10.100.0.105 } {10.100.0.103 } {10.100.0.102 } {10.100.0.1 } {10.100.0.107 }]
I0527 11:39:38.720038       1 controller.go:380] Updating loadbalancer service2-production/service2-ingress with IP 10.100.0.104
I0527 11:39:38.804214       1 event.go:216] Event(api.ObjectReference{Kind:""Ingress"", Namespace:""service1-production"", Name:""service1-ingress"", UID:""d09f3143-20d4-11e6-b056-8a5128ea7197"", APIVersion:""extensions"", ResourceVersion:""9347653"", FieldPath:""""}): type: 'Normal' reason: 'UPDATE' service1-production/service1-ingress
I0527 11:39:38.804687       1 controller.go:890] shutting down controller queues
I0527 11:39:38.804722       1 controller.go:951] shutting down NGINX loadbalancer controller
I0527 11:39:38.804738       1 main.go:144] Handled quit, awaiting pod deletion
I0527 11:39:38.804756       1 event.go:216] Event(api.ObjectReference{Kind:""Ingress"", Namespace:""service1-production"", Name:""service1-ingress"", UID:""d09f3143-20d4-11e6-b056-8a5128ea7197"", APIVersion:""extensions"", ResourceVersion:""9347422"", FieldPath:""""}): type: 'Normal' reason: 'DELETE' ip: 10.100.0.104
I0527 11:39:38.820874       1 event.go:216] Event(api.ObjectReference{Kind:""Ingress"", Namespace:""service2-production"", Name:""service2-ingress"", UID:""c93eeab0-21cf-11e6-b056-8a5128ea7197"", APIVersion:""extensions"", ResourceVersion:""9347650"", FieldPath:""""}): type: 'Normal' reason: 'CREATE' ip: 10.100.0.104
I0527 11:39:38.822353       1 controller.go:379] not contained: [{10.100.0.105 } {10.100.0.103 } {10.100.0.102 } {10.100.0.1 } {10.100.0.107 }]
I0527 11:39:38.822374       1 controller.go:380] Updating loadbalancer service1-production/service1-ingress with IP 10.100.0.104
I0527 11:39:38.916628       1 event.go:216] Event(api.ObjectReference{Kind:""Ingress"", Namespace:""service1-production"", Name:""service1-ingress"", UID:""d09f3143-20d4-11e6-b056-8a5128ea7197"", APIVersion:""extensions"", ResourceVersion:""9347653"", FieldPath:""""}): type: 'Normal' reason: 'CREATE' ip: 10.100.0.104
```

The strange thing is (in my opinion) that it recreates IP after it deletes it.

The ingress doesn't reflect the expected situation but it still has a lot of IP assigned for that specific rule.

I tried also to put the nginx -s quit in the preStop, but it is even worse because it continues to handle requests and then the nginx disappears.
",closed,False,2016-05-27 12:44:24,2016-05-27 16:03:06
contrib,timothysc,https://github.com/kubernetes/contrib/issues/1087,https://api.github.com/repos/kubernetes/contrib/issues/1087,"ansible: proxy tests fail on source deploys, missing package deps.","When we install via packages our .specs pull in all the appropriate dependencies that are needed, via the package manager.

When we are doing source deploys this is not the case, and we could potentially be missing a number of dependencies.  (socat) was the one that caused test failures on Conformance. 

/cc @ingvagabund 
",closed,False,2016-05-27 14:59:38,2016-05-31 15:33:31
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1088,https://api.github.com/repos/kubernetes/contrib/issues/1088,Cluster-autoscaler: Tests for simulator predicates,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-27 15:18:40,2016-05-31 08:20:35
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1089,https://api.github.com/repos/kubernetes/contrib/issues/1089,Cluster-autoscaler: Tests for utilization calculator in scale down,"cc: @piosz @fgrzadkowski 
",closed,True,2016-05-27 17:07:33,2016-05-31 14:04:07
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1090,https://api.github.com/repos/kubernetes/contrib/issues/1090,Detect PRs from new contributors,"And post useful onboarding information, such as pointers to contributing guidelines, advice about how to get started, how to find similar issues, help-wanted, tec.
",closed,False,2016-05-27 18:55:10,2018-02-15 01:24:06
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1091,https://api.github.com/repos/kubernetes/contrib/issues/1091,Introduce a flag to block merges for certain milestones,"This blocks merge for v1.4 and next-candidate. When we open for v1.4 we
should remove `v1.4` from the flags and remove `next-candidate` from all
PRs.
",closed,True,2016-05-27 21:21:05,2016-05-27 21:22:30
contrib,agustincastanio,https://github.com/kubernetes/contrib/issues/1092,https://api.github.com/repos/kubernetes/contrib/issues/1092,Nginx ingress controller: Failed to list *extensions.Ingress,"Hi, I am trying to get the [nginx controller example](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx) working but it keeps restarting because health checks fail.

**Environment details:**

```
[root@master nginx]# cat /etc/redhat-release
CentOS Linux release 7.2.1511 (Core)
```

```
[root@master nginx]# kubectl version
Client Version: version.Info{Major:""1"", Minor:""2"", GitVersion:""v1.2.0"", GitCommit:""b57e8bdc7c3871e3f6077b13c42d205ae1813fbd"", GitTreeState:""clean""}
Server Version: version.Info{Major:""1"", Minor:""2"", GitVersion:""v1.2.0"", GitCommit:""b57e8bdc7c3871e3f6077b13c42d205ae1813fbd"", GitTreeState:""clean""}
```

```
[root@master nginx]# kubectl get nodes
NAME      LABELS                         STATUS    AGE
node1     kubernetes.io/hostname=node1   Ready     7d
node2     kubernetes.io/hostname=node2   Ready     7d
```

Skydns is in place and working:

```
[root@master nginx]# kubectl exec busybox -- nslookup kubernetes.default
Server:    10.254.0.10
Address 1: 10.254.0.10

Name:      kubernetes.default
Address 1: 10.254.0.1
```

I can query ingress:

```
[root@master nginx]# curl http://localhost:8080/apis/extensions/v1beta1/ingress
```

Results: [ingress.txt](https://github.com/kubernetes/contrib/files/287346/ingress.txt)

**Steps to reproduce the problem:**

Create svc, rc and ing:

```
[root@master nginx]# kubectl create -f examples/default/rc-default.yaml
kubectl run echoheaders --image=gcr.io/google_containers/echoserver:1.4 --replicas=1 --port=8080
kubectl expose rc echoheaders --port=80 --target-port=8080 --name=echoheaders-x
kubectl expose rc echoheaders --port=80 --target-port=8080 --name=echoheaders-y
kubectl create -f examples/ingress.yaml
kubectl create -f examples/default-backend.yaml
kubectl expose rc default-http-backend --port=80 --target-port=8080 --name=default-http-backend
```

Verify creation:

```
 [root@master nginx]# kubectl get pods -o wide --all-namespaces
NAMESPACE     NAME                             READY     STATUS    RESTARTS   AGE       NODE
default       busybox                          1/1       Running   32         1d        node1
default       default-http-backend-pmzzt       1/1       Running   0          49m       node1
default       echoheaders-a2108                1/1       Running   0          55m       node2
default       nginx-ingress-controller-3nu5q   1/1       Running   22         15m       node1
kube-system   kube-dns-v11-uglga               4/4       Running   21         2h        node2
```

Query controller logs:

```
[root@master nginx]# kubectl logs nginx-ingress-controller-3nu5q
I0527 21:10:06.640238       1 main.go:96] Using build: https://github.com/bprashanth/contrib.git - git-9204d9a
I0527 21:10:06.728087       1 main.go:131] Validated default/default-http-backend as the default backend
W0527 21:10:06.729233       1 ssl.go:92] no file dhparam.pem found in secrets
I0527 21:10:06.733158       1 controller.go:938] starting NGINX loadbalancer controller
I0527 21:10:06.733537       1 command.go:33] Starting NGINX process...
E0527 21:10:06.743962       1 reflector.go:216] k8s.io/contrib/ingress/controllers/nginx/controller.go:941: Failed to list *extensions.Ingress: the server could not find the requested resource
E0527 21:10:07.746560       1 reflector.go:216] k8s.io/contrib/ingress/controllers/nginx/controller.go:941: Failed to list *extensions.Ingress: the server could not find the requested resource
E0527 21:10:08.748845       1 reflector.go:216] k8s.io/contrib/ingress/controllers/nginx/controller.go:941: Failed to list *extensions.Ingress: the server could not find the requested resource
E0527 21:10:09.751552       1 reflector.go:216] k8s.io/contrib/ingress/controllers/nginx/controller.go:941: Failed to list *extensions.Ingress: the server could not find the requested resource
```

Could you please help me with this issue?
",closed,False,2016-05-27 21:29:00,2016-05-30 12:12:16
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1093,https://api.github.com/repos/kubernetes/contrib/issues/1093,[nginx-ingress-controller] Add support for rate limiting in ingress rule locations,,closed,True,2016-05-27 22:03:55,2016-05-31 18:19:58
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1094,https://api.github.com/repos/kubernetes/contrib/issues/1094,"When building kubernetes from sources, some runtime deps are missing","When running e2e tests on kubernetes built from sources, some runtime dependencies are missing.

IMHO, this is not actually needed to deploy the cluster. Though, the runtime dependencies are needed to run kubernetes properly. This is relevant only for deployments when k8s is built from sources. If k8s is installed from a distribution packages, missing dependencies are already there.

So far the deps are installed only for Fedora, RHEL and CentOS. Not sure how this is handled for CoreOS or other OSes. 

Fixes: #1087 
",closed,True,2016-05-27 23:18:47,2016-05-31 15:33:31
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1095,https://api.github.com/repos/kubernetes/contrib/issues/1095,string casting doesn't work that way,"@freehan 

...going to push this shortly
",closed,True,2016-05-28 04:06:17,2016-05-31 20:10:15
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1096,https://api.github.com/repos/kubernetes/contrib/issues/1096,PR metrics dashboard,"PR latency histograms by size 
",closed,False,2016-05-28 05:13:45,2018-01-17 07:48:50
contrib,dylangrafmyre,https://github.com/kubernetes/contrib/issues/1097,https://api.github.com/repos/kubernetes/contrib/issues/1097,Echoheaders image pull fails on tag not found,"When running `kubectl run echoheaders --image=gcr.io/google_containers/echoserver:1.4 --replicas=1 --port=8080 from
https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/README.md

The pod fails to pull the image with  `Tag 1.4 not found` error. 

However, I am able to successfully pull `gcr.io/google_containers/defaultbackend:1.0` for the default-backend deployment.
",closed,False,2016-05-29 04:27:09,2016-05-29 05:04:55
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1098,https://api.github.com/repos/kubernetes/contrib/issues/1098,Fix dry run bug & add context,"@eparis @wojtek-t 

Fix problems the two of you found. Will push live shortly.
",closed,True,2016-05-29 17:39:44,2016-05-29 17:54:04
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1099,https://api.github.com/repos/kubernetes/contrib/issues/1099,no milestone means the empty milestone,,closed,True,2016-05-29 17:54:28,2016-05-29 17:55:54
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1100,https://api.github.com/repos/kubernetes/contrib/issues/1100,ansible: collect various logs from cluster,"Playbook I run after each e2e test suite to collect logs of k8s components (`kubelet`, `kube-proxy`, `kube-apiserver`, `kube-controller-manager`, `kube-scheduler`) and `kernel.log`.

So far tested on `RHEL`, `Fedora` and `CentOS`.
",closed,True,2016-05-30 11:34:15,2016-06-15 10:42:07
contrib,nottix,https://github.com/kubernetes/contrib/issues/1101,https://api.github.com/repos/kubernetes/contrib/issues/1101,Nginx Ingress Controller - Support for geoip,"Hi @aledbf ,

is it possibile to use directives

`geoip_country /usr/share/GeoIP/GeoIP.dat;
vhost_traffic_status_filter_by_set_key $geoip_country_code country::*;`

in the latest version of Nginx ingress controller?

I need it to to calculate traffic for individual country.

Thank you.
",closed,False,2016-05-30 17:32:14,2018-12-16 15:28:22
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1102,https://api.github.com/repos/kubernetes/contrib/issues/1102,[nginx-ingress-controller]: Add support for geoip in stats,"fixes #1101
",closed,True,2016-05-30 18:44:34,2018-12-15 16:42:57
contrib,piosz,https://github.com/kubernetes/contrib/pull/1103,https://api.github.com/repos/kubernetes/contrib/issues/1103,Gce token,,closed,True,2016-05-30 20:37:40,2016-05-31 12:51:51
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1104,https://api.github.com/repos/kubernetes/contrib/issues/1104,[nginx-ingress-controller] cleanup before update updating nginx controller to 0.7,"fixes #930
fixes #884
",closed,True,2016-05-31 15:45:32,2016-06-02 21:53:57
contrib,eparis,https://github.com/kubernetes/contrib/pull/1105,https://api.github.com/repos/kubernetes/contrib/issues/1105,Slight re-write to NewIssue dry-run handling,"print a message so you know if it triggered with --dry-run and count the
API tokens.
",closed,True,2016-05-31 20:46:23,2016-06-01 22:06:45
contrib,eparis,https://github.com/kubernetes/contrib/pull/1106,https://api.github.com/repos/kubernetes/contrib/issues/1106,Do not count vendor/ when calculating size,,closed,True,2016-05-31 20:51:37,2016-06-01 21:05:42
contrib,eparis,https://github.com/kubernetes/contrib/pull/1107,https://api.github.com/repos/kubernetes/contrib/issues/1107,Fix crash,"fixes #1024
",closed,True,2016-05-31 20:56:27,2016-06-01 21:05:57
contrib,eparis,https://github.com/kubernetes/contrib/pull/1108,https://api.github.com/repos/kubernetes/contrib/issues/1108,Get rid of 'ok-to-merge',"https://github.com/kubernetes/kubernetes/issues/24228
",closed,True,2016-05-31 21:03:34,2016-06-01 22:21:56
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1109,https://api.github.com/repos/kubernetes/contrib/issues/1109,Do not update merge rate if we don't run e2e,"If we don't run e2e tests, it makes it very easy to merge quickly, and
that makes the merge rate very high. Just do not include non-e2e merges
into the rate so that we keep it relevant.
",closed,True,2016-05-31 21:37:17,2016-05-31 22:33:14
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1110,https://api.github.com/repos/kubernetes/contrib/issues/1110,[nginx-slim] Update nginx to 1.11.1 - CVE-2016-4450,"Security: a segmentation fault might occur in a worker process while writing a specially crafted request body to a temporary file (CVE-2016-4450)
",closed,True,2016-05-31 21:47:01,2016-06-02 21:14:40
contrib,david-mcmahon,https://github.com/kubernetes/contrib/pull/1111,https://api.github.com/repos/kubernetes/contrib/issues/1111,Add release-note-experimental as a valid label.,"ref #882

@eparis is there any doc I can follow on how to test and rollout changes in contrib?
",closed,True,2016-05-31 22:31:56,2016-06-01 21:35:51
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1112,https://api.github.com/repos/kubernetes/contrib/issues/1112,Make flake URLs point to Gubernator instead of storage.googleapis.com.,"The previous URL was not useful (see kubernetes/kubernetes#26509).

individualFlakeSource/brokenJobSource.ID() now returns things like:
""/kubernetes-jenkins/logs/e2e-gce/123/\n"", which allows some flexibility
to make the Body() return different URLs.

There are tests for compatibility with old ID() results.
",closed,True,2016-06-01 00:08:13,2016-06-02 21:25:49
contrib,galan,https://github.com/kubernetes/contrib/issues/1113,https://api.github.com/repos/kubernetes/contrib/issues/1113,Ingress broken - creates new empty Instance-Group,"When I started implementing the Ingress resource in our infrastructure provisioning (2016-05-09) the ingress worked like a charm. Rolling this out to our staging cluster failed yesterday. The ingress successfully creates the L7 HTTP LoadBalancer, creates all entities, but the backend-service did not have any instances configured. Turns out that the Ingress creates a **new Instance Group** called `k8s-ig`, which has no instances associated with it (0) where there should be 4 in our case.

Btw.: The health-check path has changed since then from `/` to `/healthz`, why is that, and how is it possible to reconfigure it?

Tested on gke against cluster version 1.2.1 and 1.2.4.

The Ingress resource:

```
{
    ""apiVersion"": ""extensions/v1beta1"",
    ""kind"": ""Ingress"",
    ""metadata"": {
        ""name"": ""de-01"",
        ""labels"": {
            ""profile"": ""staging""
        }
    },
    ""spec"": {
        ""tls"": [
            {
                ""secretName"": ""staging-de-01""
            }
        ],
        ""backend"": {
            ""serviceName"": ""nginx"",
            ""servicePort"": 80
        }
    }
}
```

Image of the empty instance-group, should have 4 nodes (gke-staging-default-506a85c4-group):
![screenshot_2016-06-01_12-44-31](https://cloud.githubusercontent.com/assets/2453957/15708046/95aacfc0-27fc-11e6-8edd-686cf6a99f61.png)

Background: We are using K8s since a year, created the L7 LB entities all by ourself previously. We switched to the Network-LB (which works great), but the Network-LB is unable to provide the client-ip, so we have to switch back to the L7 and the Ingress seems like a good opportunity.
",closed,False,2016-06-01 11:34:21,2017-04-20 12:13:33
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1114,https://api.github.com/repos/kubernetes/contrib/issues/1114,Cluster-autoscaler: remove node infos without node,"Fixes e2e - scale up/scale down tests.

cc: @piosz @fgrzadkowski @jsz
",closed,True,2016-06-01 13:12:10,2016-06-01 13:25:08
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1115,https://api.github.com/repos/kubernetes/contrib/issues/1115,[ubuntu-slim]: Remove locales,"This update reduces the size of the image in 12MB

```
gcr.io/google_containers/ubuntu-slim  0.3   6e333e89d38c   9 hours ago 47.33 MB
gcr.io/google_containers/ubuntu-slim  0.2   010be51edff2   5 weeks ago 59.37 MB
```
",closed,True,2016-06-01 14:05:51,2016-06-02 17:46:43
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/1116,https://api.github.com/repos/kubernetes/contrib/issues/1116,remove podmaster which is deprecated for --leader-elect,,closed,True,2016-06-01 16:55:39,2016-06-30 22:20:54
contrib,piosz,https://github.com/kubernetes/contrib/pull/1117,https://api.github.com/repos/kubernetes/contrib/issues/1117,Fixed issue with not provided cloud config,"cc @fgrzadkowski @jszczepkowski 
",closed,True,2016-06-01 17:32:16,2016-06-01 18:13:22
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1118,https://api.github.com/repos/kubernetes/contrib/issues/1118,mungers/flakesync tests take too long,"```
SIGQUIT: quit
PC=0x45aea1 m=0
goroutine 0 [idle]:
runtime.futex(0x66d0a8, 0x0, 0x0, 0x0, 0x0, 0x66c9f0, 0x0, 0x0, 0x40da24, 0x66d0a8, ...)
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/runtime/sys_linux_amd64.s:302 +0x21
runtime.futexsleep(0x66d0a8, 0x0, 0xffffffffffffffff)
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/runtime/os1_linux.go:40 +0x53
runtime.notesleep(0x66d0a8)
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/runtime/lock_futex.go:145 +0xa4
runtime.stopm()
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/runtime/proc.go:1535 +0x10b
runtime.findrunnable(0xc820022000, 0x0)
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/runtime/proc.go:1973 +0x739
runtime.schedule()
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/runtime/proc.go:2072 +0x24f
runtime.park_m(0xc820000a80)
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/runtime/proc.go:2137 +0x18b
runtime.mcall(0x7fff1e4f67f0)
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/runtime/asm_amd64.s:233 +0x5b
goroutine 1 [chan receive, 9 minutes]:
testing.RunTests(0x5d8160, 0x666a20, 0x2, 0x2, 0xc820076001)
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/testing/testing.go:583 +0x8d2
testing.(*M).Run(0xc820049f08, 0xc8200760e8)
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/testing/testing.go:515 +0x81
main.main()
    k8s.io/contrib/mungegithub/mungers/flakesync/_test/_testmain.go:56 +0x117
goroutine 5 [runnable]:
type..eq.k8s.io/contrib/mungegithub/mungers/flakesync.key(0xc820046e28, 0xc820074260, 0x4244399d13c7d175)
    /home/travis/gopath/src/k8s.io/contrib/mungegithub/mungers/flakesync/cache.go:1
k8s.io/contrib/mungegithub/mungers/flakesync.(*Cache).lookup(0xc8200145a0, 0x592898, 0x3, 0x6, 0x0, 0x0)
    /home/travis/gopath/src/k8s.io/contrib/mungegithub/mungers/flakesync/cache.go:132 +0xce
k8s.io/contrib/mungegithub/mungers/flakesync.(*Cache).Get(0xc8200145a0, 0x592898, 0x3, 0x6, 0xc82001c3c0, 0x0, 0x0)
    /home/travis/gopath/src/k8s.io/contrib/mungegithub/mungers/flakesync/cache.go:122 +0x4f
k8s.io/contrib/mungegithub/mungers/flakesync.TestBasic(0xc82001e120)
    /home/travis/gopath/src/k8s.io/contrib/mungegithub/mungers/flakesync/cache_test.go:47 +0x467
testing.tRunner(0xc82001e120, 0x666a20)
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/testing/testing.go:473 +0x98
created by testing.RunTests
    /home/travis/.gimme/versions/go1.6.linux.amd64/src/testing/testing.go:582 +0x892
rax    0xca
rbx    0x0
rcx    0xffffffffffffffff
rdx    0x0
rdi    0x66d0a8
rsi    0x0
rbp    0x1
rsp    0x7fff1e4f6650
r8     0x0
r9     0x0
r10    0x0
r11    0x286
r12    0x1740
r13    0x20
r14    0x1
r15    0x8
rip    0x45aea1
rflags 0x286
cs     0x33
fs     0x0
gs     0x0
*** Test killed with quit: ran too long (10m0s).
FAIL    k8s.io/contrib/mungegithub/mungers/flakesync    600.065s
```
",closed,False,2016-06-01 18:11:42,2016-06-23 19:43:16
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1119,https://api.github.com/repos/kubernetes/contrib/issues/1119,Create github-fetcher to download and save github,"This new program downloads and saves github database for a specific
project into a local database (MySql). The goal is to be able to query
this database without worrying about rate-limiting, and using a SQL
query language.

It currently downloads 2 types of resources:
- Issues, including pull-requests and including labels
- IssueEvents, that is all events associated to Issues

It is intended to run in a container and fetch/sleep/loop.

It is missing tests right now but it's big enough already.
",closed,True,2016-06-01 19:23:59,2016-07-06 20:56:30
contrib,david-mcmahon,https://github.com/kubernetes/contrib/issues/1120,https://api.github.com/repos/kubernetes/contrib/issues/1120,Add dev/release/deployment tooling and doc for mungers,"This could possibly extend to the rest of this repo as well.  I don't have a 10K foot view yet of exactly what's in here.

A recent PR (#1111) was in process and the author (me) fat fingered a deployment before the merge was done.   This is less than ideal and as this repo gets more contributors it's best to have the end-to-end dev->deployment cycle guided by tooling as much as possible.  Documentation should also be included but focus on being minimalist (because doc gets out of date) and let the tooling itself be the documented guide.

There are other sub-repos thinking along the same lines such as https://github.com/kubernetes/minikube/issues/130 and https://github.com/kubernetes/release has already started work to provide a fully guided release process for kubernetes/kubernetes.  We can probably leverage this existing tooling to help with the sub-repos in this area as well.

cc @eparis @lavalamp @mikedanese 
",closed,False,2016-06-01 21:39:29,2018-02-13 20:57:09
contrib,teijinyer,https://github.com/kubernetes/contrib/issues/1121,https://api.github.com/repos/kubernetes/contrib/issues/1121,service-loadbalancer/service_loadbalancer.go ，I can‘t go build it,"error：./service_loadbalancer.go:352: undefined: ""k8s.io/kubernetes/pkg/util"".RateLimiter
I don’t find this interface。
@aledbf 
",closed,False,2016-06-02 03:20:14,2016-06-02 09:47:57
contrib,Random-Liu,https://github.com/kubernetes/contrib/issues/1122,https://api.github.com/repos/kubernetes/contrib/issues/1122,hack/for-go-proj.sh doesn't work if there is windows package in vendor directory,"`hack/for-go-proj.sh` tries to build and install golang packages recursively with `./...` https://github.com/kubernetes/contrib/blob/master/hack/for-go-proj.sh#L50

For old Godeps directory ""_workspace"", it works well. But after switching to golang 1.6, all the dependencies in ""vendor"" directory will be built and installed.

If there are windows packages included in ""vendor"", for example `engine-api` relies on ""github.com/Microsoft/go-winio"", `hack/for-go-proj.sh` will fail.
",closed,False,2016-06-02 06:16:04,2018-02-13 20:57:09
contrib,micheleorsi,https://github.com/kubernetes/contrib/issues/1123,https://api.github.com/repos/kubernetes/contrib/issues/1123,"[ingress/controllers/nginx] Nginx shutdown doesn't gracefully close ""keep-alive"" connections","We are analyzing the behaviour of re-deploying nginx ingress controller with a lot of requests flooding. Basically we have gatling or ab (command line tool) that performs a lot of parallel requests to our kubernetes cluster for a while.

With default nginx configuration we discovered that:
- if clients don't request keep-alive connection the process is really smooth (0 errors)
- if clients request keep-alive connection we have a lot fails (java.io.IOException: Remotely closed)

We tried several things and the latest one was to gracefully shutdown nginx in the preStop hook with this command:
`/usr/sbin/nginx -s quit`

The expected behaviour would be that nginx maintains keep-alive connections before receiving the SIGTERM. Then, once it receives the -s quit, it starts to block new keep-alive connections (with ""Connection: keep-alive, close"" header) and notifies the client that it should close the kept alive connections.
On the other hand the observed behaviour is that nginx continues to use alive connection until it dies and client receives ""java.io.IOException: Remotely closed"".

Finally we also tried to modify the parameter ""keepalive_timeout"" in nginx configration to ""0"". In this way nginx never accepts keep-alive connections (with ""Connection: keep-alive, close"" header) and we have a smooth results of 0 errors.

Obviously it is not the best configuration because we don't optimise number of connections used and we have a strong feeling that we are missing something ..
",closed,False,2016-06-02 14:55:20,2016-07-12 15:04:24
contrib,piosz,https://github.com/kubernetes/contrib/pull/1124,https://api.github.com/repos/kubernetes/contrib/issues/1124,[Addon resizer] Added ExponentialEstimator to pod nanny,"Once CA is enabled the cluster size is being changed pretty frequently which cases Heapster downtime. To prevent from this we want to adjust resources less.

cc @a-robinson @fgrzadkowski @jszczepkowski @mwielgus @roberthbailey
",closed,True,2016-06-02 15:32:50,2016-06-09 18:58:17
contrib,girishkalele,https://github.com/kubernetes/contrib/pull/1125,https://api.github.com/repos/kubernetes/contrib/issues/1125,Multi-architecture dnsmasq container builds + rename container to kub…,"Fixed builds for architecture amd64 - don't have a solution for arm/arm64/ppc yet.
",closed,True,2016-06-02 18:36:59,2016-06-08 20:20:25
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1126,https://api.github.com/repos/kubernetes/contrib/issues/1126,make broken test runs accumulate in a single issue,"Requested by @fejta 

Also fix #1118
",closed,True,2016-06-02 20:25:15,2016-06-02 21:16:27
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1127,https://api.github.com/repos/kubernetes/contrib/issues/1127,Pushed to prod,"Minor fixes & tweaks-- pushed just now.
",closed,True,2016-06-02 21:39:15,2016-06-02 21:39:25
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1128,https://api.github.com/repos/kubernetes/contrib/issues/1128,add quotes for clarity,,closed,True,2016-06-02 22:03:20,2016-06-03 12:31:04
contrib,dcwangmit01,https://github.com/kubernetes/contrib/issues/1129,https://api.github.com/repos/kubernetes/contrib/issues/1129,[contrib/ansible] Failure to provision a working kube cluster due to unresolveable hostnames,"I'm having trouble getting the most recent contrib/ansible scripts to successfully provision a Kubernetes cluster.  It used to work many days ago around kubernetes/contrib master branch commit 3634e300, but now fails at commit f48ce77f.  I'm hoping someone can provide advice that may allow me to use the unmodified stock code.  Otherwise I'm looking for approval of a proposed change which I can turn into a PR.

At some point recently, the contrib/ansible scripts seemed to have changed so that host arguments to processes are now hostnames instead of IPs.  At kubernetes/contrib commit 3634e300 the arguments to flanneld, kube-proxy, and kubelet were IP-based, and now they are hostname based.  This is also true of the contents of the kube config file found on the master at: `/etc/kubernetes/kubectl.kubeconfig`

For example:

```
# before around master branch commit 3634e300
/usr/bin/flanneld -etcd-endpoints=http://{ip-address-of-the-master}:2379 ...
# now around master branch commit f48ce77f 
/usr/bin/flanneld -etcd-endpoints=http://{hostname-of-the-master}:2379 ...
```

Because the hostnames of cluster peers are unresolveable, the Kubernetes cluster fails to function.

In order to work around the problem, I created additional ansible tasks to update the /etc/hosts file of every node so that hostnames may resolve into IP addresses.  This solves the problem, but with the drawback of slowing down the ansible deploy due to the gathering of facts.

```
diff --git a/ansible/roles/pre-ansible/tasks/main.yml b/ansible/roles/pre-ansible/tasks/main.yml
index 9b21de4..21138b2 100644
--- a/ansible/roles/pre-ansible/tasks/main.yml
+++ b/ansible/roles/pre-ansible/tasks/main.yml
@@ -39,3 +39,22 @@

 - include: fedora-dnf.yml
   when: os_version.stdout|int >= 22 and 'Fedora' in distro.stdout and is_atomic is not defined
+
+- name: Gather cluster networking facts for /etc/hosts
+  setup:
+  register: networking_facts
+  tags:
+    - ""/etc/hosts""
+
+- name: Insert/update /etc/hosts using ansible cluster facts
+  blockinfile:
+    dest: /etc/hosts
+    block: |
+      {% for host in groups['all'] %}
+        {{- hostvars[host].ansible_default_ipv4.address }} {{ host }}
+      {% endfor %}
+    marker: ""# {mark} ANSIBLE MANAGED BLOCK""
+    create: no
+  when: networking_facts
+  tags:
+    - ""/etc/hosts""
```

Tested on:

```
Kube Masters and Nodes: CentOS Linux release 7.2.1511 (Core)
Dev Machine: ansible 2.1.0.0
```

This is my first time touching Ansible.  Feedback is appreciated.  Thanks!
",closed,False,2016-06-02 22:11:26,2016-07-29 17:45:42
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1130,https://api.github.com/repos/kubernetes/contrib/issues/1130,[nginx-ingress-controller] Improve docs and examples,,closed,True,2016-06-02 22:56:37,2016-06-23 05:37:43
contrib,MaxDaten,https://github.com/kubernetes/contrib/issues/1131,https://api.github.com/repos/kubernetes/contrib/issues/1131,ingress controllers (full)-chain support?,"Hi,

we are currently using a Lets Encrypt service to create and renew certificates. Besides #952 makes it currently impossible (?) to update the cert for the ingress controller automatically (maybe an kubectl apply/replace on the ingress is enough to force a reload?), we need to install the full-chain, otherwise android would reject the cert (https://community.letsencrypt.org/t/on-android-6-0-1-the-certificate-is-untrusted/7815). Is there a support for the full-chain? The certificates for the GCE Load-Balancer offers a field to enter the (full)chain.pem.
",closed,False,2016-06-03 10:44:29,2018-02-13 21:58:10
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1132,https://api.github.com/repos/kubernetes/contrib/issues/1132,Cluster autoscaler: More debugs around scale down,"cc: @piosz @jszczepkowski @fgrzadkowski
",closed,True,2016-06-03 15:20:39,2016-06-03 19:16:51
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1133,https://api.github.com/repos/kubernetes/contrib/issues/1133,Ubernetes multizone and custom healthchecks,"1. Adds multi zone support to glbc
2. Pipes health checks up from the readiness probe of a pod to GCLB

@kubernetes/goog-cluster fyi
Fixes https://github.com/kubernetes/contrib/issues/983 and https://github.com/kubernetes/contrib/issues/953
",closed,True,2016-06-03 18:31:06,2016-06-09 20:17:38
contrib,eparis,https://github.com/kubernetes/contrib/pull/1134,https://api.github.com/repos/kubernetes/contrib/issues/1134,mungegithub: Delete all Jenkins code - we use GCS,,closed,True,2016-06-03 20:21:24,2016-06-07 16:03:08
contrib,eparis,https://github.com/kubernetes/contrib/pull/1135,https://api.github.com/repos/kubernetes/contrib/issues/1135,submit-queue: redo retest logic,"This changes the required status logic a fair bit.
1. All PRs must pass all 'required' contexts at least once
2. Change `e2e-not-required` label name to `retest-not-required`
3. Create 2 types of 'required' contexts:
   
    `RequiredStatusContexts` and `RequiredRetestContexts`
   
   All must be success to merge a PR but only RequiredRetestContexts
   must be true twice.
4. Get rid of the custom flags for our unit and e2e tests
5. New flag to tell the bot how to request the requisite retests
",closed,True,2016-06-03 20:21:39,2016-06-08 04:35:56
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1136,https://api.github.com/repos/kubernetes/contrib/issues/1136,[nginx-ingress-controller]: Fix nginx rewrite rule order,,closed,True,2016-06-04 18:06:43,2016-06-10 21:46:13
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1137,https://api.github.com/repos/kubernetes/contrib/issues/1137,Update haproxy image,,closed,True,2016-06-04 19:03:33,2016-06-08 16:42:15
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1138,https://api.github.com/repos/kubernetes/contrib/issues/1138,[keepalived-vip]: Update keepalived and godeps,,closed,True,2016-06-04 19:30:41,2016-06-08 17:01:12
contrib,louis-paul,https://github.com/kubernetes/contrib/pull/1139,https://api.github.com/repos/kubernetes/contrib/issues/1139,[404-server] Parse command line flags,"The command line flags were ignored, even though they are defined in the code. This made impossible changing the port of the server.
",closed,True,2016-06-05 13:28:48,2016-08-03 16:17:01
contrib,edouardKaiser,https://github.com/kubernetes/contrib/issues/1140,https://api.github.com/repos/kubernetes/contrib/issues/1140,[ingress/controllers/nginx] Use Service Virtual IP instead of maintaining Pod list,"Is there a way to tell the NGINX Ingress controller to use the Service Virtual IP Address instead of maintaining the Pods IP addresses in the upstream configuration ?

I couldn't find it. If not, I think it would be good. Because with the current situation, when we scale down a service. the Ingress controller does not work in harmony with the Replication Controller of the service.

That means, some requests to the Ingress Controller will fail while waiting for the Ingress Controller to be updated. 

If we use the Service Virtual IP address, we can let kube-proxy do its job in harmony with the replication controller and we have a seamless down scaling.
",closed,False,2016-06-06 06:04:27,2019-04-04 15:54:27
contrib,sander-su,https://github.com/kubernetes/contrib/issues/1141,https://api.github.com/repos/kubernetes/contrib/issues/1141,[ingress/controllers/nginx] ip whitelisting,"We run our kubernetes cluster in aws but would like to restrict access to some services based on ip. This could be users coming from our corporate network our devs who use a vpn to get into the aws vpc but do not need a fully kubernetes account. Basic auth is a very nice step forward but not restrictive enough yet.

I would be great if we define comma seperated cidr blocks also via a configmap.
e.g. internal would be 10.0.0.0/16 for aws vpc
corporate would be x.x.x.x/x, y.y.y.y/y
Using a configmaps gives the option of changing (corporate) ip's at one place instead of every ingress.
",closed,False,2016-06-06 06:22:16,2016-06-14 01:34:20
contrib,eparis,https://github.com/kubernetes/contrib/pull/1142,https://api.github.com/repos/kubernetes/contrib/issues/1142,Delete ping_ci - We don't use it any more,,closed,True,2016-06-06 14:22:30,2016-06-06 18:25:22
contrib,eparis,https://github.com/kubernetes/contrib/pull/1143,https://api.github.com/repos/kubernetes/contrib/issues/1143,Create a README for mungegithub,,closed,True,2016-06-06 14:22:46,2016-06-06 18:25:28
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1144,https://api.github.com/repos/kubernetes/contrib/issues/1144,[nginx-ingress-controller] Add cidr whitelist support,"fixes #1141
",closed,True,2016-06-06 18:32:14,2016-06-14 01:34:20
contrib,eparis,https://github.com/kubernetes/contrib/pull/1145,https://api.github.com/repos/kubernetes/contrib/issues/1145,stop looking in .git for .go files to gofmt,"This obviously goes nuts  :)
",closed,True,2016-06-06 18:56:01,2016-06-06 18:56:04
contrib,luxas,https://github.com/kubernetes/contrib/pull/1146,https://api.github.com/repos/kubernetes/contrib/issues/1146,Add a tiny-glibc image,"Please review and push: @girishkalele 

cc @bprashanth
",closed,True,2016-06-06 20:43:43,2016-06-10 21:23:53
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1147,https://api.github.com/repos/kubernetes/contrib/issues/1147,reference previous issues for suites,"If we file a new issue for a broken test suite, link to previous issues for that suite.
",closed,True,2016-06-06 21:23:55,2016-06-07 02:27:36
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1148,https://api.github.com/repos/kubernetes/contrib/issues/1148,Cluster-autoscaler: enable scale down by default,"cc: @piosz  @jszczepkowski  @fgrzadkowski 
",closed,True,2016-06-07 14:40:35,2016-06-07 15:05:02
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1149,https://api.github.com/repos/kubernetes/contrib/issues/1149,submit-queue: Merge bot is stuck with no good reason,"For some reason merge bot is not doing anything useful, even though the tests are green and there are some PRs which are green and lgtmed. Some logs:

```
I0607 14:49:56.870418       1 mungegithub.go:74] Sleeping for 1m40.341412323s
I0607 14:51:37.212433       1 mungegithub.go:59] Running mungers
I0607 14:51:39.579295       1 repo-updates.go:130] Loaded config from ./gitrepos/kubernetes:7476d97781563b70e8b89a8bd3f99ea75ae6c290
I0607 14:51:40.215262       1 github.go:586] 26792 labelEvent: searched 27 events for label lgtm, found at index 23
I0607 14:51:40.650369       1 github.go:586] 26354 labelEvent: searched 13 events for label lgtm, found at index 5
I0607 14:51:41.063271       1 github.go:586] 26188 labelEvent: searched 29 events for label lgtm, found at index 28
I0607 14:51:41.575873       1 github.go:586] 26179 labelEvent: searched 34 events for label lgtm, found at index 33
I0607 14:51:42.127765       1 github.go:586] 26806 labelEvent: searched 16 events for label lgtm, found at index 15
I0607 14:51:42.569750       1 github.go:586] 26904 labelEvent: searched 15 events for label lgtm, found at index 12
I0607 14:51:43.002407       1 github.go:586] 26915 labelEvent: searched 52 events for label lgtm, found at index 43
I0607 14:51:43.458169       1 github.go:586] 26735 labelEvent: searched 18 events for label lgtm, found at index 15
I0607 14:51:43.870098       1 github.go:586] 26907 labelEvent: searched 15 events for label lgtm, found at index 14
I0607 14:51:44.246974       1 github.go:586] 26803 labelEvent: searched 23 events for label lgtm, found at index 22
I0607 14:51:44.672907       1 github.go:586] 26831 labelEvent: searched 13 events for label lgtm, found at index 12
I0607 14:53:34.149222       1 github.go:586] 23733 labelEvent: searched 44 events for label lgtm, found at index 41
I0607 14:53:37.601802       1 github.go:586] 23818 labelEvent: searched 26 events for label lgtm, found at index 16
I0607 14:53:38.469527       1 github.go:586] 23830 labelEvent: searched 24 events for label lgtm, found at index 19
I0607 14:53:40.799625       1 github.go:586] 23858 labelEvent: searched 51 events for label lgtm, found at index 46
I0607 14:54:01.781374       1 github.go:586] 24308 labelEvent: searched 43 events for label lgtm, found at index 11
I0607 14:54:09.361781       1 github.go:586] 24438 labelEvent: searched 32 events for label lgtm, found at index 6
I0607 14:54:14.861835       1 github.go:586] 24705 labelEvent: searched 25 events for label lgtm, found at index 22
I0607 14:54:50.453228       1 github.go:586] 24802 labelEvent: searched 27 events for label lgtm, found at index 24
I0607 14:54:54.264717       1 github.go:586] 24910 labelEvent: searched 20 events for label lgtm, found at index 6
I0607 14:54:55.075935       1 github.go:586] 24916 labelEvent: searched 45 events for label lgtm, found at index 14
I0607 14:55:00.184521       1 github.go:586] 25024 labelEvent: searched 30 events for label lgtm, found at index 29
I0607 14:55:05.859780       1 github.go:586] 25036 labelEvent: searched 54 events for label lgtm, found at index 53
I0607 14:55:10.415162       1 github.go:586] 25085 labelEvent: searched 92 events for label lgtm, found at index 83
I0607 14:55:21.047350       1 github.go:586] 25307 labelEvent: searched 21 events for label lgtm, found at index 20
I0607 14:55:36.241760       1 github.go:586] 25526 labelEvent: searched 34 events for label lgtm, found at index 30
I0607 14:55:44.508549       1 github.go:586] 25612 labelEvent: searched 28 events for label lgtm, found at index 12
I0607 14:55:51.108975       1 github.go:586] 25710 labelEvent: searched 10 events for label lgtm, found at index 9
I0607 14:55:52.696310       1 github.go:586] 25737 labelEvent: searched 85 events for label lgtm, found at index 39
I0607 14:55:56.082201       1 github.go:586] 25769 labelEvent: searched 19 events for label lgtm, found at index 16
I0607 14:55:59.964197       1 github.go:586] 25816 labelEvent: searched 47 events for label lgtm, found at index 41
I0607 14:56:08.328754       1 github.go:586] 25866 labelEvent: searched 25 events for label lgtm, found at index 20
E0607 14:56:15.133256       1 github.go:1552] no mergeability information for ""implement EBS and Cinder attacher/detacher "" 25888, Skipping
E0607 14:56:17.690188       1 github.go:1552] no mergeability information for ""implement EBS and Cinder attacher/detacher "" 25888, Skipping
I0607 14:56:20.142196       1 github.go:586] 25922 labelEvent: searched 39 events for label lgtm, found at index 35
I0607 14:56:21.441060       1 github.go:586] 25934 labelEvent: searched 24 events for label lgtm, found at index 19
I0607 14:56:29.367371       1 github.go:586] 26012 labelEvent: searched 21 events for label lgtm, found at index 15
I0607 14:56:30.539641       1 github.go:586] 26016 labelEvent: searched 55 events for label lgtm, found at index 47
I0607 14:56:37.992147       1 github.go:586] 26057 labelEvent: searched 33 events for label kind/api-change, found at index 2
I0607 14:56:41.930783       1 github.go:586] 26143 labelEvent: searched 13 events for label lgtm, found at index 8
I0607 14:56:47.792343       1 github.go:586] 26179 labelEvent: searched 34 events for label lgtm, found at index 33
I0607 14:56:48.644186       1 github.go:586] 26179 labelEvent: searched 34 events for label lgtm, found at index 33
I0607 14:56:49.448967       1 github.go:586] 26188 labelEvent: searched 29 events for label lgtm, found at index 28
I0607 14:56:50.239027       1 github.go:586] 26188 labelEvent: searched 29 events for label lgtm, found at index 28
E0607 14:56:50.386559       1 blunderbuss.go:128] No owners found for PR 26189
I0607 14:56:52.521322       1 github.go:586] 26240 labelEvent: searched 19 events for label lgtm, found at index 12
I0607 14:56:54.446295       1 github.go:586] 26276 labelEvent: searched 11 events for label lgtm, found at index 8
I0607 14:56:57.250668       1 github.go:586] 26332 labelEvent: searched 23 events for label lgtm, found at index 22
I0607 14:56:59.105665       1 github.go:586] 26354 labelEvent: searched 13 events for label lgtm, found at index 5
I0607 14:56:59.949201       1 github.go:586] 26354 labelEvent: searched 13 events for label lgtm, found at index 5
I0607 14:57:00.209436       1 github.go:586] 26355 labelEvent: searched 11 events for label lgtm, found at index 10
I0607 14:57:05.161543       1 github.go:586] 26387 labelEvent: searched 18 events for label lgtm, found at index 11
I0607 14:57:12.124198       1 github.go:586] 26467 labelEvent: searched 17 events for label lgtm, found at index 9
I0607 14:57:13.875815       1 github.go:586] 26494 labelEvent: searched 30 events for label lgtm, found at index 17
I0607 14:57:16.019620       1 github.go:586] 26502 labelEvent: searched 11 events for label lgtm, found at index 4
I0607 14:57:16.887835       1 github.go:586] 26504 labelEvent: searched 9 events for label lgtm, found at index 5
I0607 14:57:27.832605       1 github.go:586] 26589 labelEvent: searched 20 events for label lgtm, found at index 17
I0607 14:57:29.050608       1 github.go:586] 26596 labelEvent: searched 12 events for label lgtm, found at index 8
I0607 14:57:30.072984       1 github.go:586] 26598 labelEvent: searched 25 events for label lgtm, found at index 22
I0607 14:57:31.512879       1 github.go:586] 26603 labelEvent: searched 13 events for label lgtm, found at index 10
I0607 14:57:32.386274       1 github.go:586] 26611 labelEvent: searched 11 events for label lgtm, found at index 8
I0607 14:57:42.776459       1 github.go:586] 26688 labelEvent: searched 11 events for label lgtm, found at index 4
I0607 14:57:44.236033       1 github.go:586] 26694 labelEvent: searched 53 events for label lgtm, found at index 48
I0607 14:57:46.516952       1 github.go:586] 26702 labelEvent: searched 18 events for label lgtm, found at index 13
I0607 14:57:48.977286       1 github.go:586] 26710 labelEvent: searched 50 events for label lgtm, found at index 43
I0607 14:57:49.982778       1 github.go:586] 26716 labelEvent: searched 22 events for label lgtm, found at index 19
I0607 14:57:52.308144       1 github.go:586] 26735 labelEvent: searched 18 events for label lgtm, found at index 15
I0607 14:57:53.018595       1 github.go:586] 26735 labelEvent: searched 18 events for label lgtm, found at index 15
I0607 14:57:53.729491       1 github.go:586] 26746 labelEvent: searched 17 events for label lgtm, found at index 10
I0607 14:57:56.705697       1 github.go:586] 26754 labelEvent: searched 19 events for label lgtm, found at index 14
I0607 14:57:57.781882       1 github.go:586] 26755 labelEvent: searched 22 events for label lgtm, found at index 13
I0607 14:58:00.328597       1 github.go:586] 26761 labelEvent: searched 33 events for label lgtm, found at index 10
I0607 14:58:02.943898       1 github.go:586] 26767 labelEvent: searched 20 events for label lgtm, found at index 16
I0607 14:58:11.939769       1 github.go:586] 26790 labelEvent: searched 6 events for label lgtm, found at index 4
I0607 14:58:13.159343       1 github.go:586] 26792 labelEvent: searched 27 events for label lgtm, found at index 23
I0607 14:58:14.122732       1 github.go:586] 26792 labelEvent: searched 27 events for label lgtm, found at index 23
I0607 14:58:16.891282       1 github.go:586] 26803 labelEvent: searched 23 events for label lgtm, found at index 22
I0607 14:58:17.738482       1 github.go:586] 26803 labelEvent: searched 23 events for label lgtm, found at index 22
I0607 14:58:18.081319       1 github.go:586] 26806 labelEvent: searched 16 events for label lgtm, found at index 15
I0607 14:58:18.882843       1 github.go:586] 26806 labelEvent: searched 16 events for label lgtm, found at index 15
I0607 14:58:22.514516       1 github.go:586] 26823 labelEvent: searched 24 events for label lgtm, found at index 17
I0607 14:58:24.745097       1 github.go:1028] PR 26829 setting ""Submit Queue"" Github status to ""Github CI tests are not green.""
I0607 14:58:25.152658       1 github.go:586] 26830 labelEvent: searched 14 events for label lgtm, found at index 13
I0607 14:58:26.580683       1 github.go:586] 26831 labelEvent: searched 13 events for label lgtm, found at index 12
I0607 14:58:27.470625       1 github.go:586] 26831 labelEvent: searched 13 events for label lgtm, found at index 12
I0607 14:58:29.261016       1 github.go:586] 26854 labelEvent: searched 9 events for label lgtm, found at index 4
I0607 14:58:30.153182       1 github.go:586] 26855 labelEvent: searched 7 events for label lgtm, found at index 6
I0607 14:58:33.760539       1 github.go:586] 26862 labelEvent: searched 12 events for label lgtm, found at index 11
I0607 14:58:35.531398       1 github.go:586] 26867 labelEvent: searched 7 events for label lgtm, found at index 6
I0607 14:58:36.717987       1 github.go:586] 26868 labelEvent: searched 8 events for label lgtm, found at index 6
I0607 14:58:37.783922       1 github.go:586] 26872 labelEvent: searched 9 events for label lgtm, found at index 6
I0607 14:58:40.245451       1 github.go:586] 26880 labelEvent: searched 7 events for label lgtm, found at index 6
I0607 14:58:45.873137       1 github.go:586] 26904 labelEvent: searched 15 events for label lgtm, found at index 12
I0607 14:58:46.661072       1 github.go:586] 26904 labelEvent: searched 15 events for label lgtm, found at index 12
I0607 14:58:46.997009       1 github.go:586] 26907 labelEvent: searched 15 events for label lgtm, found at index 14
I0607 14:58:47.837145       1 github.go:586] 26907 labelEvent: searched 15 events for label lgtm, found at index 14
I0607 14:58:48.334524       1 github.go:586] 26908 labelEvent: searched 24 events for label kind/new-api, found at index 14
I0607 14:58:50.732353       1 github.go:586] 26915 labelEvent: searched 52 events for label lgtm, found at index 43
I0607 14:58:51.647298       1 github.go:586] 26915 labelEvent: searched 52 events for label lgtm, found at index 43
I0607 14:59:09.507288       1 github.go:586] 26959 labelEvent: searched 19 events for label lgtm, found at index 10
I0607 14:59:11.140827       1 github.go:586] 26962 labelEvent: searched 12 events for label lgtm, found at index 7
I0607 14:59:12.590530       1 github.go:223] Made 7097 API calls since the last Reset 12.770816 calls/sec
```

After this there's a next sleep & loop.

Currently there're 11 PRs in the queue.
",closed,False,2016-06-07 15:06:48,2016-06-07 17:55:40
contrib,brandong954,https://github.com/kubernetes/contrib/issues/1150,https://api.github.com/repos/kubernetes/contrib/issues/1150,"Can't POST Ingress to API using curl, but kubectl works fine.","Trying to execute: 

`curl -X POST -d @ingress.yaml  -H 'Content-type:application/yaml' http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/somenamespace/ingresses`

ingress.yaml:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: echomap
  namespace: somenamespace
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        backend:
          serviceName: echoheaders-x
          servicePort: 80
  - host: bar.baz.com
    http:
      paths:
      - path: /bar
        backend:
          serviceName: echoheaders-y
          servicePort: 80
      - path: /foo
        backend:
          serviceName: echoheaders-x
          servicePort: 80
```

But the call fails with:

```
{
  ""kind"": ""Status"",
  ""apiVersion"": ""v1"",
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""the object provided is unrecognized (must be of type Ingress): yaml: mapping values are not allowed in this context"",
  ""reason"": ""BadRequest"",
  ""code"": 400
}
```

Even though when executing `kubectl create -f ./ingress.yaml`, everything works just fine.
",closed,False,2016-06-07 17:06:08,2018-06-14 11:31:38
contrib,eparis,https://github.com/kubernetes/contrib/pull/1151,https://api.github.com/repos/kubernetes/contrib/issues/1151,Clear sq.interruptedObj no matter what,"If the following happens
- PR starts retest
- the whole queue blocks
- PR retests fail

The PR will end up on the 'sq.interruptedObj'. When the global e2e goes
back green
- the interrupted PR will get picked first
- enter doGithubE2EAndMerge()
- fail out of doGithubE2EAndMerge() because sq.validForMerge(obj) fails
- get picked again as the interrupted PR
- goto 2

resolves #1149 
",closed,True,2016-06-07 17:37:00,2016-06-07 17:55:41
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1152,https://api.github.com/repos/kubernetes/contrib/issues/1152,Automatically assign newly created flake issues to test owners.,,closed,True,2016-06-07 22:39:54,2016-06-14 18:30:32
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1153,https://api.github.com/repos/kubernetes/contrib/issues/1153,auto-file issues for test runs even if they don't block the merge queue,"Now that we're autofiling issues and people are getting out of the habit of looking at the jekins jobs, we need to start filing issues for test failures on jobs that don't block the submit queue. e.g., https://github.com/kubernetes/kubernetes/issues/27011 if auto filed would have dozens if not hundreds of comments.
",closed,False,2016-06-08 00:03:21,2016-06-13 16:39:34
contrib,wojtek-t,https://github.com/kubernetes/contrib/issues/1154,https://api.github.com/repos/kubernetes/contrib/issues/1154,Merge-bot is not picking my PR,"I'm seeing the following on my PR:

```
 Submit Queue — Unable to determine is PR is mergeable. Will try again later.
```

This is blocking merge of that PR.

@eparis @lavalamp 
",closed,False,2016-06-08 06:20:36,2016-06-08 17:03:58
contrib,whs,https://github.com/kubernetes/contrib/pull/1155,https://api.github.com/repos/kubernetes/contrib/issues/1155,nginx-ingress-controller: Use DefaultClientConfig to create clientConfig,"This PR use the configuration generated from DefaultClientConfig, instead of trying to generate one so that the server URL and credentials can be specified from the command line.

The runtime pod detection haven't been improved, and probably will fail if you use remote server. To workaround this, use `--running-in-cluster=false` and manually load a load balancer service configuration.

An image with this PR is available at [willwill/nginx-ingress-controller](https://hub.docker.com/r/willwill/nginx-ingress-controller/) on Docker Hub.
",closed,True,2016-06-08 08:47:56,2016-06-14 04:43:33
contrib,luxas,https://github.com/kubernetes/contrib/pull/1156,https://api.github.com/repos/kubernetes/contrib/issues/1156,Cross-compile 404-server,"Please review and push.
This is based on `exechealthz`, which @bprashanth reviewed earlier

@bprashanth @girishkalele 
",closed,True,2016-06-08 09:57:38,2016-06-11 07:29:24
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1157,https://api.github.com/repos/kubernetes/contrib/issues/1157,Cluster-autoscaler: basic metrics for ca,"cc: @piosz @jszczepkowski
",closed,True,2016-06-08 11:30:38,2016-06-08 21:35:54
contrib,eparis,https://github.com/kubernetes/contrib/pull/1158,https://api.github.com/repos/kubernetes/contrib/issues/1158,Do not check status if no statuses required,,closed,True,2016-06-08 12:18:57,2016-06-08 12:19:02
contrib,eparis,https://github.com/kubernetes/contrib/pull/1159,https://api.github.com/repos/kubernetes/contrib/issues/1159,There is no space in the status name!,,closed,True,2016-06-08 12:35:28,2016-06-08 12:36:22
contrib,eparis,https://github.com/kubernetes/contrib/pull/1160,https://api.github.com/repos/kubernetes/contrib/issues/1160,Fix display of queue,,closed,True,2016-06-08 13:39:20,2016-06-08 13:40:41
contrib,eparis,https://github.com/kubernetes/contrib/issues/1161,https://api.github.com/repos/kubernetes/contrib/issues/1161,Assign fixes unable to assign to non-collaborators,"If a person is not a kubernetes collaborator they are not able to be assigned. So the submit-queue prints lots of errors every 10 minutes like:

```
assign-fixes.go:90] Assigning 24568 to screeley44 (previously assigned to <nil>)
github.go:1217] Assigning PR# 24568  to screeley44
github.go:1222] Error assigning issue# 24568 to screeley44: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/24568: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]

assign-fixes.go:90] Assigning 24877 to paralin (previously assigned to <nil>)
github.go:1217] Assigning PR# 24877  to paralin
github.go:1222] Error assigning issue# 24877 to paralin: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/24877: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
```
",closed,False,2016-06-08 17:31:46,2017-08-20 09:41:23
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1162,https://api.github.com/repos/kubernetes/contrib/issues/1162,cluster-autoscaler: We shouldn't delete nodes with pods with local storage,"We shouldn't delete nodes which have pods with local storage like `emptyDir` or `hostPath`.

@piosz @jszczepkowski 
",closed,False,2016-06-08 20:41:14,2018-07-03 09:39:27
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1163,https://api.github.com/repos/kubernetes/contrib/issues/1163,cluster-autoscaler: Increase how often we try to scale up/down,"Currently it's once a minute. That's very rare. We should aim for something like `10s`.

@piosz @jszczepkowski 
",closed,False,2016-06-08 20:42:51,2016-06-10 12:35:38
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1164,https://api.github.com/repos/kubernetes/contrib/issues/1164,cluster-autoscaler: Run performance tests for large clusters,"We should understand what are our scalability limits and how fast can we react to pending pods or unused capacity in large clusters.

@piosz @jszczepkowski 
",closed,False,2016-06-08 20:44:05,2016-06-13 21:09:20
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1165,https://api.github.com/repos/kubernetes/contrib/issues/1165,Cluster-autoscaler: use port 8085 for ca prometheus metrics,"CA uses host network and 8080 is already used.

cc: @piosz @fgrzadkowski @jszczepkowski 
",closed,True,2016-06-09 12:22:21,2016-06-09 12:24:08
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1166,https://api.github.com/repos/kubernetes/contrib/issues/1166,Cluster-autoscaler: Fix logging in simulator/cluster.go,"cc: @piosz @fgrzadkowski @jszczepkowski 
",closed,True,2016-06-09 12:24:34,2016-06-09 12:31:39
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1167,https://api.github.com/repos/kubernetes/contrib/issues/1167,Cluster-autoscaler: start http server in a separate goroutine,,closed,True,2016-06-09 12:35:10,2016-06-09 15:21:16
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1168,https://api.github.com/repos/kubernetes/contrib/issues/1168,cluster-autoscaler: Rename reservation to utilization,"In many places we are using word `reservation` which has a special meaning. We should rename it all to utilization!

@mwielgus @piosz @jszczepkowski 
",closed,False,2016-06-09 14:25:26,2016-06-09 23:05:57
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/1169,https://api.github.com/repos/kubernetes/contrib/issues/1169,cluster-autoscaler: Don't delete nodes with pods with local storage,"Don't delete nodes with pods with local storage.

Add flags to control whether we fail on draining nodes with kube-system pods & pods with local storage.

Fixes https://github.com/kubernetes/contrib/issues/1162

@piosz @jszczepkowski 
",closed,True,2016-06-09 16:04:15,2016-06-10 11:54:33
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/1170,https://api.github.com/repos/kubernetes/contrib/issues/1170,cluster-autoscaler: Increase main loop frequency to 10s,"Increase main loop frequency to 10s

Fixes https://github.com/kubernetes/contrib/issues/1163

@mwielgus @piosz @jszczepkowski 
",closed,True,2016-06-09 16:37:36,2016-06-10 12:35:39
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/1171,https://api.github.com/repos/kubernetes/contrib/issues/1171,cluster-autoscaler: Rename reservation to utilization,"Rename reservation to utilization

Fixes https://github.com/kubernetes/contrib/issues/1168

@mwielgus @piosz @jszczepkowski 
",closed,True,2016-06-09 17:18:44,2016-06-09 23:05:57
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1172,https://api.github.com/repos/kubernetes/contrib/issues/1172,cluster-autoscaler: Rename scale-down function to better describe what they do,"Currently number of functions have `utilization` in their name, even though we actually checking if the node can be removed and utilization is only an optimization to find good candidates. We should rename those functions for better readability.

@mwielgus @piosz @jszczepkowski 
",closed,False,2016-06-09 17:48:05,2016-06-09 23:06:25
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/1173,https://api.github.com/repos/kubernetes/contrib/issues/1173,cluster-autoscaler: Rename scale down function to better inform what they do.,"Rename scale down function to better inform what they do.

Fixes https://github.com/kubernetes/contrib/issues/1172

@mwielgus @piosz @jszczepkowski 
",closed,True,2016-06-09 17:56:22,2016-06-09 23:06:25
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/1174,https://api.github.com/repos/kubernetes/contrib/issues/1174,cluster-autoscaler: Log that we are capping to MAX.,"Log that we are capping to MAX.

Fixes https://github.com/kubernetes/contrib/issues/1074

@mwielgus @piosz @jszczepkowski 
",closed,True,2016-06-09 17:57:23,2016-06-09 23:04:17
contrib,piosz,https://github.com/kubernetes/contrib/pull/1175,https://api.github.com/repos/kubernetes/contrib/issues/1175,addon-resizer: Bumped version to 1.1,,closed,True,2016-06-09 19:04:04,2016-06-09 19:27:32
contrib,Quentin-M,https://github.com/kubernetes/contrib/pull/1176,https://api.github.com/repos/kubernetes/contrib/issues/1176,Mungebot: Inactivity pinger,"Hi,

That is a first draft intended to improve the mungebot by implementing two specific points from #869:

> - Ping the PR due to lack of inactivity, notifying either the PR author or the assignee, depending on who last touched the PR (keep track of whose court it's in)
> - Create a way to postpone the next ping for a fixed amount of time (day, week, month, next release cycle)

This new munger comments PRs when:
- No answer has been given for 2 weeks on the PR
- No answer has been given for 2 weeks on any review (code) comment that aren't outdated.

Depending on who commented last, the added comment will either mention (@) the PR assignee or the PR owner.

The next ping can be postponed or cancelled by commenting back:
- `@k8s-merge-robot ignore inactivity`
- `@k8s-merge-robot postpone by X day(s)/week(s)/month(s)`
",closed,True,2016-06-09 21:51:51,2016-07-31 16:01:55
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1177,https://api.github.com/repos/kubernetes/contrib/issues/1177,Add admin page & way to mark failures as resolved,"Fix #1020
",closed,True,2016-06-09 22:26:11,2016-06-10 00:12:35
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1178,https://api.github.com/repos/kubernetes/contrib/issues/1178,submit queue: persist github cache to disk,"So that restarting or pushing a new queue doesn't have to wait for tokens.
",closed,False,2016-06-10 00:26:01,2016-08-04 16:33:19
contrib,asalkeld,https://github.com/kubernetes/contrib/pull/1179,https://api.github.com/repos/kubernetes/contrib/issues/1179,assign-fixes: label issues with has-pr-posted and add a comment,"1. only assign issues to collaborators
2. add a label has-pr-posted to make searching for issues easier
3. leave a comment (only once) with the author of the PR
4. remove UsersWithAccess as it is not used

fixes #1161

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1179)

<!-- Reviewable:end -->
",closed,True,2016-06-10 07:36:30,2016-10-23 23:45:31
contrib,florian-besser,https://github.com/kubernetes/contrib/issues/1180,https://api.github.com/repos/kubernetes/contrib/issues/1180,Service-Loadbalancer: Failed to create client due to missing token,"I'm using the following yaml file:

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: service-loadbalancer
  labels:
    app: service-loadbalancer
    version: v1
spec:
  replicas: 1
  selector:
    app: service-loadbalancer
    version: v1
  template:
    metadata:
      labels:
        app: service-loadbalancer
        version: v1
    spec:
      nodeSelector:
        role: loadbalancer
      containers:
      - image: gcr.io/google_containers/servicelb:0.4
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        name: haproxy
        ports:
        # All http services
        - containerPort: 80
          hostPort: 80
          protocol: TCP
        # haproxy stats
        - containerPort: 1936
          hostPort: 1936
          protocol: TCP
        resources: {}
```

And then run it using:
`kubectl create -f loabBalancer.yaml`

I get:

```
kubectl get pods -o wide
NAME                         READY     STATUS             RESTARTS   AGE       NODE
service-loadbalancer-v3414   0/1       CrashLoopBackOff   7          14m       minion1
```

Log shows:

```
kubectl logs service-loadbalancer-v3414
I0610 08:24:23.023231       6 service_loadbalancer.go:605] Creating new loadbalancer: {Name:haproxy ReloadCmd:./haproxy_reload Config:/etc/haproxy/haproxy.cfg Template:template.cfg Algorithm: startSyslog:false sslCert: sslCaCert: lbDefAlgorithm:roundrobin}
I0610 08:24:23.024285       6 service_loadbalancer.go:688] No tcp/https services specified
F0610 08:24:23.024383       6 service_loadbalancer.go:701] **Failed to create client: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory**
```

This token should either not be required or the README.md should instruct users how to generate/populate it to the servicelb container.
",closed,False,2016-06-10 08:32:09,2018-02-15 06:29:04
contrib,piosz,https://github.com/kubernetes/contrib/pull/1181,https://api.github.com/repos/kubernetes/contrib/issues/1181,addon-resizer: migrated to kubernetes client in version 1.3,,closed,True,2016-06-10 12:38:52,2016-06-10 18:13:50
contrib,piosz,https://github.com/kubernetes/contrib/pull/1182,https://api.github.com/repos/kubernetes/contrib/issues/1182,addon-resizer: Bumped version to 1.2,"cc @fgrzadkowski @Q-Lee 
",closed,True,2016-06-10 13:04:25,2016-06-10 14:17:03
contrib,sebgoa,https://github.com/kubernetes/contrib/issues/1183,https://api.github.com/repos/kubernetes/contrib/issues/1183,Documentation for PetSet zookeeper needed,"Hi @bprashanth, I am looking at your petset demos:
`https://github.com/kubernetes/contrib/tree/master/pets/zookeeper`

I am running 1.3.0-alpha.5 with dns add ons.

I am struggling to get this running. The init container don't seem to run and I get missing `zkServer.sh not found` error messages.

Any hints on how to test this ? what configs to 1.3.0-alpha.5 needs to be made if any ?

thanks

-seb
",closed,False,2016-06-10 13:36:46,2018-02-14 03:03:10
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1184,https://api.github.com/repos/kubernetes/contrib/issues/1184,Cluster-autoscaler: more debugs around scale down,"cc: @piosz @fgrzadkowski @jszczepkowski 
",closed,True,2016-06-10 13:53:21,2016-06-10 14:18:08
contrib,naveensrinivasan,https://github.com/kubernetes/contrib/pull/1185,https://api.github.com/repos/kubernetes/contrib/issues/1185,Updated README to fix the dead link to the gcp.,"Updated README to fix the dead link to the gcp.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1185)

<!-- Reviewable:end -->
",closed,True,2016-06-10 16:23:06,2016-10-19 17:51:41
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1186,https://api.github.com/repos/kubernetes/contrib/issues/1186,submit queue: don't re-check mergability on a PR if it hasn't changed,"As pointed out by github support.

I think this should save us 1 token per unchanged & unmergable PR per loop?
",closed,False,2016-06-10 16:45:34,2017-02-10 17:41:55
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1187,https://api.github.com/repos/kubernetes/contrib/issues/1187,add gke-serial into the list,"We just made it green.

I probably won't push this until something causes the merges to stop, since restarts are expensive.
",closed,True,2016-06-10 17:53:38,2016-06-10 18:42:09
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1188,https://api.github.com/repos/kubernetes/contrib/issues/1188,Cluster-autoscaler: timing in scale down fix,"cc: @fgrzadkowski @piosz @jszczepkowski 
",closed,True,2016-06-10 19:08:38,2016-06-11 00:44:45
contrib,girishkalele,https://github.com/kubernetes/contrib/pull/1189,https://api.github.com/repos/kubernetes/contrib/issues/1189,DNSPerf container build and RC yaml,"This is the DNSPerf container used to run benchmarks and reliability tests against the kubedns pod

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1189)

<!-- Reviewable:end -->
",closed,True,2016-06-10 20:18:51,2016-09-21 22:09:24
contrib,eparis,https://github.com/kubernetes/contrib/pull/1190,https://api.github.com/repos/kubernetes/contrib/issues/1190,Write github cache to disk,"This seems to work! It will grow unbound (we can look into that)

Just a clean submit queue run created 39M of data.

A single run of all mungers is still running and is around 100M. I'll
update when I know what it looks like overnight.
",closed,True,2016-06-10 20:24:57,2016-06-24 04:00:08
contrib,Q-Lee,https://github.com/kubernetes/contrib/pull/1191,https://api.github.com/repos/kubernetes/contrib/issues/1191,Kubelet monitoring,"Sidecar to monitor a single kubelet, and push results to the GCM v3 API.
",closed,True,2016-06-10 21:02:03,2016-07-20 22:24:28
contrib,luxas,https://github.com/kubernetes/contrib/pull/1192,https://api.github.com/repos/kubernetes/contrib/issues/1192,Update dnsmasq to 1.3; fix the multiarch build and remove tiny-glibc,"@thockin @girishkalele @bprashanth 
",closed,True,2016-06-10 21:24:49,2016-06-16 18:26:57
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1193,https://api.github.com/repos/kubernetes/contrib/issues/1193,File issues for failures even in non-blocking jobs,"Fixes #1153
",closed,True,2016-06-10 22:15:56,2016-06-13 16:39:34
contrib,nottix,https://github.com/kubernetes/contrib/issues/1194,https://api.github.com/repos/kubernetes/contrib/issues/1194,Nginx Ingress Controller - Stats by country does not work,"Hi,

the latest stats by country does not work. When i go to /nginx_status page there aren't any filters by country.

Thank you.
",closed,False,2016-06-10 22:17:06,2016-06-20 16:41:31
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/1195,https://api.github.com/repos/kubernetes/contrib/issues/1195,Enable operating on microvalues in addong resizer,"Since we want to enable passing values smaller than e.g. 1 millicore per node, we need a solution other  than operating on MilliValues.

@gmarek 
",closed,True,2016-06-13 08:43:11,2016-06-13 11:14:48
contrib,freehan,https://github.com/kubernetes/contrib/pull/1196,https://api.github.com/repos/kubernetes/contrib/issues/1196,fix junit xml with testsuites tag,"fix: #1045
",closed,True,2016-06-13 21:53:15,2016-06-13 23:08:15
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1197,https://api.github.com/repos/kubernetes/contrib/issues/1197,change list of test suites,"Preparing for a push...
",closed,True,2016-06-13 23:06:44,2016-06-14 00:50:43
contrib,nikhiljindal,https://github.com/kubernetes/contrib/pull/1198,https://api.github.com/repos/kubernetes/contrib/issues/1198,Adding kubernetes-e2e-gce-federation to list of merge blocking suites,"cc @kubernetes/sig-cluster-federation @lavalamp 
Will wait for the test to be green for a while before merging this.

Ref https://github.com/kubernetes/kubernetes/issues/26723

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1198)

<!-- Reviewable:end -->
",closed,True,2016-06-13 23:36:00,2016-10-04 20:32:47
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1199,https://api.github.com/repos/kubernetes/contrib/issues/1199,fix UI for flakes & nonblocking suites,"Tired of the UI being totally wrong and confusing. Turns out to be easy.

![image](https://cloud.githubusercontent.com/assets/647318/16028118/d1b9cff0-3190-11e6-91c9-7aca69450a2f.png)
",closed,True,2016-06-14 01:01:42,2016-06-14 03:25:46
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1200,https://api.github.com/repos/kubernetes/contrib/issues/1200,add overall mergability to health,"So we can make our chart accurate. @spxtr 
",closed,True,2016-06-14 01:13:38,2016-06-14 03:26:11
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1201,https://api.github.com/repos/kubernetes/contrib/issues/1201,make resolutions work in weak stable jobs,,closed,True,2016-06-14 03:22:24,2016-06-14 03:25:18
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1202,https://api.github.com/repos/kubernetes/contrib/issues/1202,Another UI tweak,,closed,True,2016-06-14 03:46:35,2016-06-14 13:04:45
contrib,eparis,https://github.com/kubernetes/contrib/pull/1203,https://api.github.com/repos/kubernetes/contrib/issues/1203,Display when the test are 'yellow',"![troubled](https://cloud.githubusercontent.com/assets/8093535/16125462/cc242c88-33c1-11e6-89bb-e7751d857d67.png)

They aren't quite red..  So give a slightly different header and message
for ignorable flakes and manual overrides
",closed,True,2016-06-14 12:46:41,2016-06-17 13:22:23
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1204,https://api.github.com/repos/kubernetes/contrib/issues/1204,Add emergency stop button,,closed,True,2016-06-14 16:55:07,2016-06-14 23:06:26
contrib,freehan,https://github.com/kubernetes/contrib/pull/1205,https://api.github.com/repos/kubernetes/contrib/issues/1205,add auto prioritization for flake issues,"fixes: #1206
",closed,True,2016-06-14 20:03:45,2016-06-15 22:39:05
contrib,freehan,https://github.com/kubernetes/contrib/issues/1206,https://api.github.com/repos/kubernetes/contrib/issues/1206,Auto Prioritize Flake Issues ,"Now we have flake issue auto filing. The natural next step is to auto prioritize this issues. 
",closed,False,2016-06-14 20:42:02,2016-06-15 22:39:05
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1207,https://api.github.com/repos/kubernetes/contrib/issues/1207,Cluster-autoscaler: review flag defaults,,closed,False,2016-06-14 20:50:03,2016-06-24 20:54:20
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1208,https://api.github.com/repos/kubernetes/contrib/issues/1208,add metrics for health poller,"@rmmh 
",closed,True,2016-06-14 22:25:11,2016-06-14 23:06:10
contrib,goltermann,https://github.com/kubernetes/contrib/pull/1209,https://api.github.com/repos/kubernetes/contrib/issues/1209,Separate non-blocking from blocking builds in E2E results page.,"Changing http://submit-queue.k8s.io/#/e2e into:

![nonblocking](https://cloud.githubusercontent.com/assets/9358478/16063595/38d646be-324f-11e6-9629-278d972bdaf5.jpg)

Also making it easy to just run a static server by pulling down current data, then running from submit-queue/www (good idea @rmmh)
",closed,True,2016-06-14 23:41:38,2016-06-15 22:39:13
contrib,eparis,https://github.com/kubernetes/contrib/pull/1210,https://api.github.com/repos/kubernetes/contrib/issues/1210,[wip] Break e2e tracking into its own service,"The e2e stuff has a life of its own and is too tightly coupled with the merge queue. This starts the coding to break all of the e2e stuff into its own service (which the merge queue can make use of)

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1210)

<!-- Reviewable:end -->
",closed,True,2016-06-14 23:49:37,2016-12-05 20:05:33
contrib,aabed,https://github.com/kubernetes/contrib/pull/1211,https://api.github.com/repos/kubernetes/contrib/issues/1211,"resolves the ""this module requires key=value”","replace = with : to resolve the error 
this module requires key=value arguments” when key=value args supplied

resolves #1221 
",closed,True,2016-06-14 23:56:43,2016-06-16 16:19:34
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1212,https://api.github.com/repos/kubernetes/contrib/issues/1212,submit queue: merge rate broken,"Shows as zero even though it's merging things. #1208 changes this but I don't see how it could break it.
",closed,False,2016-06-15 05:26:02,2016-06-15 21:51:58
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1213,https://api.github.com/repos/kubernetes/contrib/issues/1213,Fix stats on retest-not-required merges,"Fixes #1212
",closed,True,2016-06-15 05:54:44,2016-06-15 21:51:58
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1214,https://api.github.com/repos/kubernetes/contrib/issues/1214,Cluster-autoscaler: panic in GetMigForInstance in gce e2e tests,"cc: @piosz @jszczepkowski @fgrzadkowski 

```
panic: runtime error: invalid memory address or nil pointer dereference
[signal 0xb code=0x1 addr=0x0 pc=0x519518]

goroutine 1 [running]:
panic(0x1b54d00, 0xc820016070)
    /usr/local/google/home/mwielgus/.gvm/gos/go1.6.2/src/runtime/panic.go:481 +0x3e6
k8s.io/contrib/cluster-autoscaler/utils/gce.(*GceManager).GetMigForInstance(0x0, 0xc82053c900, 0x0, 0x0, 0x0)
    /usr/local/google/home/mwielgus/go/src/k8s.io/contrib/cluster-autoscaler/utils/gce/gce.go:162 +0x4c8
main.CheckMigsAndNodes(0xc8205658e0, 0x3, 0x4, 0x0, 0x0, 0x0)
    /usr/local/google/home/mwielgus/go/src/k8s.io/contrib/cluster-autoscaler/utils.go:240 +0x222
main.main()
    /usr/local/google/home/mwielgus/go/src/k8s.io/contrib/cluster-autoscaler/cluster_autoscaler.go:135 +0xdb2
```
",closed,False,2016-06-15 09:50:57,2016-06-15 11:16:57
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1215,https://api.github.com/repos/kubernetes/contrib/issues/1215,Cluster-autoscaler: cloud config error handling,"cc: @piosz @jszczepkowski @fgrzadkowski 
",closed,True,2016-06-15 10:19:25,2016-06-15 11:00:31
contrib,fuzzyhandle,https://github.com/kubernetes/contrib/issues/1216,https://api.github.com/repos/kubernetes/contrib/issues/1216,Change proxy config,"/etc/kubernetes/proxy needs to have an option to use a specifig proxy-mode

e.g.
KUBE_PROXY_ARGS=""--kubeconfig={{ kube_config_dir }}/proxy.kubeconfig --proxy-mode=userspace""

proxymode needs to be defined as userspace in default and can be overridden to iptables if the user needs it
",closed,False,2016-06-15 10:32:04,2018-02-14 05:05:10
contrib,piosz,https://github.com/kubernetes/contrib/pull/1217,https://api.github.com/repos/kubernetes/contrib/issues/1217,Use default token when tokenUrl not specified,,closed,True,2016-06-15 12:29:36,2016-06-15 12:51:17
contrib,coryfklein,https://github.com/kubernetes/contrib/pull/1218,https://api.github.com/repos/kubernetes/contrib/issues/1218,"Fix typo ""does no"" -> ""does not""","My first contribution. I've signed the CLA.
",closed,True,2016-06-15 17:58:39,2016-06-15 18:02:41
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1219,https://api.github.com/repos/kubernetes/contrib/issues/1219,Evaluate Docker's processes and tools,"https://blog.docker.com/2016/06/open-source-docker-part-2-processes/
- workflow stage labels
- status labels (e.g., failing-ci)
- how to become a maintainer

https://blog.docker.com/2016/06/open-source-docker-tooling-automation/
- chat-driven commands
- post instructions to PRs
- PR triage
- identify affected releases
- metrics
- reports

cc @fejta @pwittrock 
",closed,False,2016-06-15 21:01:13,2018-01-16 20:21:00
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1220,https://api.github.com/repos/kubernetes/contrib/issues/1220,Reduce size of redis image for pets using alpine:3.4,"```
gcr.io/google_containers/redis-install  0.2  50c8a6c1d914  About a minute ago   108.5 MB
gcr.io/google_containers/redis-install  0.1  e23c2ffde782  3 weeks ago          272.1 MB
```
",closed,True,2016-06-16 04:06:46,2016-06-20 16:41:57
contrib,aabed,https://github.com/kubernetes/contrib/issues/1221,https://api.github.com/repos/kubernetes/contrib/issues/1221,msg: this module requires key=value arguments ,"I get this message when I try to deploy on ubuntu

```
TASK: [flannel | Ubuntu | Configure Docker to use Flannel network] ************ 
changed: [kube-master-2] => (item={'regexp': '. /run/flannel/subnet.env', 'line': '. /run/flannel/subnet.env'})
changed: [kube-node-3] => (item={'regexp': '. /run/flannel/subnet.env', 'line': '. /run/flannel/subnet.env'})
changed: [kube-node-2] => (item={'regexp': '. /run/flannel/subnet.env', 'line': '. /run/flannel/subnet.env'})
changed: [kube-master-1] => (item={'regexp': '. /run/flannel/subnet.env', 'line': '. /run/flannel/subnet.env'})
changed: [kube-node-4] => (item={'regexp': '. /run/flannel/subnet.env', 'line': '. /run/flannel/subnet.env'})
changed: [kube-node-1] => (item={'regexp': '. /run/flannel/subnet.env', 'line': '. /run/flannel/subnet.env'})
failed: [kube-master-2] => (item={'regexp': '^DOCKER_OPTS=', 'line': 'DOCKER_OPTS=""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}""'}) => {""failed"": true, ""item"": {""line"": ""DOCKER_OPTS=\""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}\"""", ""regexp"": ""^DOCKER_OPTS=""}}
msg: this module requires key=value arguments (['dest=/etc/default/docker', 'state=present', 'line=DOCKER_OPTS=--bip', '${FLANNEL_SUBNET}', '--mtu', '${FLANNEL_MTU}', 'regexp=^DOCKER_OPTS='])
failed: [kube-node-2] => (item={'regexp': '^DOCKER_OPTS=', 'line': 'DOCKER_OPTS=""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}""'}) => {""failed"": true, ""item"": {""line"": ""DOCKER_OPTS=\""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}\"""", ""regexp"": ""^DOCKER_OPTS=""}}
msg: this module requires key=value arguments (['dest=/etc/default/docker', 'state=present', 'line=DOCKER_OPTS=--bip', '${FLANNEL_SUBNET}', '--mtu', '${FLANNEL_MTU}', 'regexp=^DOCKER_OPTS='])
failed: [kube-node-3] => (item={'regexp': '^DOCKER_OPTS=', 'line': 'DOCKER_OPTS=""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}""'}) => {""failed"": true, ""item"": {""line"": ""DOCKER_OPTS=\""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}\"""", ""regexp"": ""^DOCKER_OPTS=""}}
msg: this module requires key=value arguments (['dest=/etc/default/docker', 'state=present', 'line=DOCKER_OPTS=--bip', '${FLANNEL_SUBNET}', '--mtu', '${FLANNEL_MTU}', 'regexp=^DOCKER_OPTS='])
failed: [kube-master-1] => (item={'regexp': '^DOCKER_OPTS=', 'line': 'DOCKER_OPTS=""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}""'}) => {""failed"": true, ""item"": {""line"": ""DOCKER_OPTS=\""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}\"""", ""regexp"": ""^DOCKER_OPTS=""}}
msg: this module requires key=value arguments (['dest=/etc/default/docker', 'state=present', 'line=DOCKER_OPTS=--bip', '${FLANNEL_SUBNET}', '--mtu', '${FLANNEL_MTU}', 'regexp=^DOCKER_OPTS='])
failed: [kube-node-4] => (item={'regexp': '^DOCKER_OPTS=', 'line': 'DOCKER_OPTS=""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}""'}) => {""failed"": true, ""item"": {""line"": ""DOCKER_OPTS=\""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}\"""", ""regexp"": ""^DOCKER_OPTS=""}}
msg: this module requires key=value arguments (['dest=/etc/default/docker', 'state=present', 'line=DOCKER_OPTS=--bip', '${FLANNEL_SUBNET}', '--mtu', '${FLANNEL_MTU}', 'regexp=^DOCKER_OPTS='])
failed: [kube-node-1] => (item={'regexp': '^DOCKER_OPTS=', 'line': 'DOCKER_OPTS=""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}""'}) => {""failed"": true, ""item"": {""line"": ""DOCKER_OPTS=\""--bip ${FLANNEL_SUBNET} --mtu ${FLANNEL_MTU}\"""", ""regexp"": ""^DOCKER_OPTS=""}}
msg: this module requires key=value arguments (['dest=/etc/default/docker', 'state=present', 'line=DOCKER_OPTS=--bip', '${FLANNEL_SUBNET}', '--mtu', '${FLANNEL_MTU}', 'regexp=^DOCKER_OPTS='])

FATAL: all hosts have already failed -- aborting

```
## Suggested fix:

https://github.com/kubernetes/contrib/pull/1211
I used it and it worked 
",closed,False,2016-06-16 09:52:25,2016-06-16 16:19:34
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/1222,https://api.github.com/repos/kubernetes/contrib/issues/1222,make verify a blocking build,"cc @lavalamp 
",closed,True,2016-06-16 17:06:45,2016-06-17 01:09:22
contrib,from-nibly,https://github.com/kubernetes/contrib/pull/1223,https://api.github.com/repos/kubernetes/contrib/issues/1223,added domain name replacement for kleenestar. subdomain,"i set up a loop that looks for domains that start with kleenestar. Then replaces that prefix with *. on the nginx side. This is a intermediate solution while we try to fix the validation problem with ingress denying \* wild cards. 

issue link: https://github.com/kubernetes/contrib/issues/799

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1223)

<!-- Reviewable:end -->
",closed,True,2016-06-16 20:52:39,2016-08-17 23:28:56
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1224,https://api.github.com/repos/kubernetes/contrib/issues/1224,Add /health.svg endpoint to serve a small queue status badge.,"This is inspired by shields.io, but is served directly.

Future work: show overall health status at a glance (maybe using sparklines?)

![health-good](https://cloud.githubusercontent.com/assets/211868/16134800/32e5f1e2-33d4-11e6-9103-0a925131933b.png)
![health-bad](https://cloud.githubusercontent.com/assets/211868/16134720/a7e70608-33d3-11e6-82db-7456fae62805.png)
",closed,True,2016-06-16 22:09:10,2016-06-17 00:57:25
contrib,manjukt,https://github.com/kubernetes/contrib/pull/1225,https://api.github.com/repos/kubernetes/contrib/issues/1225,WSO2 service lb for APIM,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1225)

<!-- Reviewable:end -->
",closed,True,2016-06-17 05:23:42,2018-02-17 12:22:03
contrib,edouardKaiser,https://github.com/kubernetes/contrib/issues/1226,https://api.github.com/repos/kubernetes/contrib/issues/1226,Ingress Controller doesn't handle very well when master is down,"I just had to update the box where my Kubernetes master is running.
While the box was rebooting, the Ingress Controller lost all the secrets (TLS), and the HTTP basic auth configuration. In the end, all the Ingress Resources were just listening on 80. No more TLS, no more HTTP Basic Auth (and probably all the other configurations depending on annotations).

When master is not available, it shouldn't really update its nginx.conf. Thoughts ?
",closed,False,2016-06-17 05:32:09,2016-06-23 05:44:12
contrib,owensk,https://github.com/kubernetes/contrib/pull/1227,https://api.github.com/repos/kubernetes/contrib/issues/1227,HACK: Never forget k8s endpoints,,closed,True,2016-06-17 17:08:14,2016-06-17 17:08:26
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1228,https://api.github.com/repos/kubernetes/contrib/issues/1228,add build cop tools,"@eparis @mikedanese 
",closed,True,2016-06-17 17:40:37,2016-06-23 17:15:09
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1229,https://api.github.com/repos/kubernetes/contrib/issues/1229,Document how to do common tasks with a github CLI,"https://github.com/github/hub
https://github.com/stephencelis/ghi
http://nodegh.io/

For example:

```
ghi list -r kubernetes -g  -s open -L kind/flake,team/control-plane

-r <repo>
-g to not restrict output to your own assigned issues
-s <state>
-L labels <comma separated list>, there is a flag for labels to exclude also.
```

cc @mikedanese @lavalamp @david-mcmahon @matchstick 
",closed,False,2016-06-17 18:15:54,2018-01-22 16:46:02
contrib,mikedanese,https://github.com/kubernetes/contrib/issues/1230,https://api.github.com/repos/kubernetes/contrib/issues/1230,mungegithub: SQ is trying to assign issues to people not in k8s org,"Which is invalid

```
E0617 01:22:08.601896       1 github.go:1228] Error assigning issue# 22485 to pstauffer: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/22485: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
E0617 01:25:55.248509       1 github.go:1228] Error assigning issue# 24568 to screeley44: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/24568: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
E0617 01:26:36.620666       1 github.go:1228] Error assigning issue# 24877 to paralin: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/24877: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
E0617 01:28:24.151443       1 github.go:1228] Error assigning issue# 25383 to asalkeld: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/25383: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
E0617 01:29:33.465465       1 github.go:1228] Error assigning issue# 20627 to asalkeld: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/20627: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
E0617 01:29:33.591463       1 github.go:1228] Error assigning issue# 26927 to asalkeld: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/26927: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
E0617 01:29:59.404684       1 github.go:1228] Error assigning issue# 25807 to andreykurilin: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/25807: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
E0617 01:30:11.698570       1 github.go:1228] Error assigning issue# 19636 to tnachen: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/19636: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
E0617 01:30:19.824485       1 github.go:1228] Error assigning issue# 25858 to keontang: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/25858: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
E0617 01:30:21.011350       1 github.go:1228] Error assigning issue# 22436 to simonswine: PATCH https://api.github.com/repos/kubernetes/kubernetes/issues/22436: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid}]
```

@eparis @lavalamp 
",closed,False,2016-06-17 18:20:22,2016-06-17 22:23:22
contrib,nikhiljindal,https://github.com/kubernetes/contrib/issues/1231,https://api.github.com/repos/kubernetes/contrib/issues/1231,Can the bot find or file flake issue for me,"Today if a test flakes on tests run by PR builder, I need to manually find a bug corresponding to that flake (find an existing one or file a new one) and then point the k8s-bot to it and request it to retest.

Would be great if I can just say ""@k8s-bot Its a flake"" and the bot finds the appropriate issue and pokes it or files a new one for me and reruns the tests.

the bot does this already (filing and finding existing flakes) for Jenkins failures. 

cc @lavalamp 
",closed,False,2016-06-17 18:27:05,2018-02-14 06:06:09
contrib,freehan,https://github.com/kubernetes/contrib/pull/1232,https://api.github.com/repos/kubernetes/contrib/issues/1232,fix bug in auto prioritization,,closed,True,2016-06-17 18:35:12,2016-06-17 21:54:27
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1233,https://api.github.com/repos/kubernetes/contrib/issues/1233,[nginx-ingress-controller]: Avoid sync without a reachable master,"fixes #1226
",closed,True,2016-06-17 21:18:38,2016-06-23 05:56:44
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1234,https://api.github.com/repos/kubernetes/contrib/issues/1234,"Revert ""Enable the assign-fixes githubmunger""","Fix #1230 
Reverts kubernetes/contrib#1082
",closed,True,2016-06-17 21:59:36,2016-06-17 22:23:22
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1235,https://api.github.com/repos/kubernetes/contrib/issues/1235,Fix stats by country in nginx status page,"![captura de pantalla 2016-06-17 a las 6 27 05 p m](https://cloud.githubusercontent.com/assets/161571/16166422/7f0081a8-34c1-11e6-87c9-19609dfd58e1.png)

fixes #1194
",closed,True,2016-06-17 22:25:37,2016-06-20 16:41:31
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1236,https://api.github.com/repos/kubernetes/contrib/issues/1236,[nginx-slim]: Update nginx to add dynamic TLS records and spdy,"https://blog.cloudflare.com/optimizing-tls-over-tcp-to-reduce-latency/
https://blog.cloudflare.com/introducing-http2/
",closed,True,2016-06-18 19:36:17,2016-07-01 23:04:40
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1237,https://api.github.com/repos/kubernetes/contrib/issues/1237,Move *-serial to nonblocking.,"Also remove the -subnet test which is apparently not useful. (per @gmarek)
",closed,True,2016-06-18 20:15:56,2016-06-20 19:00:01
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1238,https://api.github.com/repos/kubernetes/contrib/issues/1238,[nginx-ingress-controller]: Add support for dynamic TLS records and spdy,"requires #1236 
",closed,True,2016-06-18 21:59:45,2016-07-01 23:30:35
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1239,https://api.github.com/repos/kubernetes/contrib/issues/1239,[nginx-ingress-controller]: Add support for conditional log of urls,,closed,True,2016-06-20 02:45:06,2016-06-22 17:47:42
contrib,sttts,https://github.com/kubernetes/contrib/pull/1240,https://api.github.com/repos/kubernetes/contrib/issues/1240,WIP: etcd petset example,,closed,True,2016-06-20 09:13:35,2016-06-30 08:24:23
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1241,https://api.github.com/repos/kubernetes/contrib/issues/1241,Add 1-minute expiration to badge (Github's proxy has 1 year expiry).,"If the cache-control header is missing, Github's HTTPS proxy defaults to 1 year expiration, which is NOT what we want: https://github.com/atmos/camo/blob/master/server.js#L129
",closed,True,2016-06-20 18:22:13,2016-06-20 22:12:48
contrib,nikhiljindal,https://github.com/kubernetes/contrib/pull/1242,https://api.github.com/repos/kubernetes/contrib/issues/1242,Adding federation e2e to non blocking test suite,"Adding federation e2e test suite to the list of tracked tests so that we at least have issues filed while we wait for https://github.com/kubernetes/contrib/pull/1198.

cc @lavalamp 
",closed,True,2016-06-20 20:27:14,2016-06-20 20:47:55
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1243,https://api.github.com/repos/kubernetes/contrib/issues/1243,add link to health chart,"We really need to parameterize this, but until then, put it by the other link.
",closed,True,2016-06-20 21:03:41,2016-06-21 00:16:23
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1244,https://api.github.com/repos/kubernetes/contrib/issues/1244,Update image on ingress example,"Just updating the image from something ancient to the one we have in gcr.io 
",closed,True,2016-06-20 22:14:12,2016-06-20 22:44:01
contrib,freehan,https://github.com/kubernetes/contrib/pull/1245,https://api.github.com/repos/kubernetes/contrib/issues/1245,bug fix in e2e.go unit test,"just realize I made a mistake in unit test. 
",closed,True,2016-06-21 00:11:29,2016-06-21 00:29:18
contrib,zmerlynn,https://github.com/kubernetes/contrib/issues/1246,https://api.github.com/repos/kubernetes/contrib/issues/1246,Rev GCE provider code in L7LB,"After https://github.com/kubernetes/kubernetes/pull/27741 merges, please:
- Rev the GCE provider code and push a new L7LB applicable 
- Push PRs in the main repo for both `release-1.2` and `release-1.3` once done

(I would handle it but there's another issue lurking here.)
",closed,False,2016-06-21 16:46:59,2018-02-14 06:06:09
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1247,https://api.github.com/repos/kubernetes/contrib/issues/1247,Update Ingress godep,"@aledbf fyi
",closed,True,2016-06-21 20:48:35,2016-06-21 22:30:25
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1248,https://api.github.com/repos/kubernetes/contrib/issues/1248,ingress controllers should use delayed queue,"So they don't hotloop: https://github.com/kubernetes/kubernetes/issues/27634#issuecomment-226842741

@aledbf 
",closed,False,2016-06-22 00:02:01,2016-07-02 00:35:12
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1249,https://api.github.com/repos/kubernetes/contrib/issues/1249,Reduce number of backends in basic ingress example,"Delete a backend service so we fall under default quota.
",closed,True,2016-06-22 16:56:43,2016-06-22 19:28:39
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1250,https://api.github.com/repos/kubernetes/contrib/issues/1250,Investigate mention-bot,"https://github.com/facebook/mention-bot

Thanks to caniszczyk for pointing this out.

cc @fejta 
",closed,False,2016-06-22 18:27:26,2016-07-15 03:20:28
contrib,zmerlynn,https://github.com/kubernetes/contrib/issues/1251,https://api.github.com/repos/kubernetes/contrib/issues/1251,Fix Godeps in cluster-autoscaler,"I'm about to put up an embarassing PR to fix https://github.com/kubernetes/kubernetes/issues/27821 that will manually alter the `vendor/` directory, because the `Godeps` are currently in conflict, but we need to get a release out:

```
$ godep go build ./...
# k8s.io/contrib/cluster-autoscaler/utils/gce
utils/gce/gce.go:65: cannot use gce.NewAltTokenSource(cfg.Global.TokenURL, cfg.Global.TokenBody) (type ""k8s.io/kubernetes/vendor/golang.org/x/oauth2"".TokenSource) as type ""k8s.io/contrib/cluster-autoscaler/vendor/golang.org/x/oauth2"".TokenSource in assignment:
        ""k8s.io/kubernetes/vendor/golang.org/x/oauth2"".TokenSource does not implement ""k8s.io/contrib/cluster-autoscaler/vendor/golang.org/x/oauth2"".TokenSource (wrong type for Token method)
                have Token() (*""k8s.io/kubernetes/vendor/golang.org/x/oauth2"".Token, error)
                want Token() (*""k8s.io/contrib/cluster-autoscaler/vendor/golang.org/x/oauth2"".Token, error)
# k8s.io/contrib/cluster-autoscaler/simulator
simulator/cluster.go:78: undefined: cmd.GetPodsForDeletionOnNodeDrain
simulator/nodes.go:31: undefined: cmd.GetPodsForDeletionOnNodeDrain
godep: go exit status 2
```
",closed,False,2016-06-22 20:05:39,2017-04-19 10:38:30
contrib,zmerlynn,https://github.com/kubernetes/contrib/pull/1252,https://api.github.com/repos/kubernetes/contrib/issues/1252,Autoscaler: Super hacky workaround for GCE provider crashloop,"Fix to https://github.com/kubernetes/kubernetes/issues/27821. Borrows the necessary piece of https://github.com/kubernetes/kubernetes/pull/27741 to parse the `gce.conf`.

Also opened https://github.com/kubernetes/contrib/issues/1251 to fix Godeps correctly.

Forgive me.
",closed,True,2016-06-22 20:14:27,2016-06-22 20:30:37
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1253,https://api.github.com/repos/kubernetes/contrib/issues/1253,[nginx-ingress-controller]: Use delayed queue,"#1248
",closed,True,2016-06-22 21:48:39,2016-06-23 05:40:16
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1254,https://api.github.com/repos/kubernetes/contrib/issues/1254,Use memcached to persist github cache out-of-process.,"Alternative implementation: #1190 (disk-backed)

Not sure which is better. This depends on another pod instead of an attached PD.
",closed,True,2016-06-22 22:10:27,2016-07-01 00:03:48
contrib,mnshaw,https://github.com/kubernetes/contrib/pull/1255,https://api.github.com/repos/kubernetes/contrib/issues/1255,get logs for failed pods in node e2e tests,"@timstclair 

Initial commit, still work in progress. 
",closed,True,2016-06-23 00:00:23,2016-07-08 16:59:38
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1256,https://api.github.com/repos/kubernetes/contrib/issues/1256,"Revert ""[nginx-ingress-controller]: Avoid sync without a reachable master""","Reverts kubernetes/contrib#1233
",closed,True,2016-06-23 05:56:50,2016-06-23 06:09:49
contrib,manjukt,https://github.com/kubernetes/contrib/issues/1257,https://api.github.com/repos/kubernetes/contrib/issues/1257,kube-apiserver reports TSL bad certificate issue,"Hi,
I'm new to Kubernetes and trying to setup cluster with 1 master and 4 nodes with Service load balancer. After the service load balancer pod creation, the kube-apiserver reports following error contineously...

$ systemctl status -l kube-apiserver
● kube-apiserver.service - Kubernetes API Server
Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
Active: active (running) since Wed 2016-06-22 22:26:03 EDT; 12min ago
Docs: https://github.com/GoogleCloudPlatform/kubernetes
Main PID: 5253 (kube-apiserver)
Memory: 61.4M
CGroup: /system.slice/kube-apiserver.service
└─5253 /usr/bin/kube-apiserver --logtostderr=true --v=0 --etcd_servers=http://127.0.0.1:2379 --address=0.0.0.0 --port=8080 --kubelet_port=10250 --allow_privileged=true --service-cluster-ip-range=172.17.0.0/16 --admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota --service_account_key_file=/etc/kubernetes/key/serviceaccount.key --client-ca-file=/srv/kubernetes/ca.crt --tls-cert-file=/srv/kubernetes/server.cert --tls-private-key-file=/srv/kubernetes/server.key

Jun 22 22:38:16 kubemaster-01 kube-apiserver[5253]: I0622 22:38:16.699924 5253 logs.go:41] http: TLS handshake error from xx.xx.xx.xx:49486: remote error: bad certificate
## Jun 22 22:38:16 kubemaster-01 kube-apiserver[5253]: I0622 22:38:16.699975 5253 logs.go:41] http: TLS handshake error from xx.xx.xx.xx:49487: remote error: bad certificate

xx.xx.xx.xx is the node where the pod exists.
Any help will be greatly appreciated.

-Thanks,
Manju
",closed,False,2016-06-23 10:29:53,2018-02-14 07:07:09
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1258,https://api.github.com/repos/kubernetes/contrib/issues/1258,[nginx-ingress-controller]: Avoid sync without a reachable master,"fixes #1233 
",closed,True,2016-06-23 14:38:41,2016-06-25 06:31:08
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1259,https://api.github.com/repos/kubernetes/contrib/issues/1259,[nginx-ingress-controller] Release 0.8,,closed,True,2016-06-23 15:06:21,2016-07-02 15:54:27
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1260,https://api.github.com/repos/kubernetes/contrib/issues/1260,fix silly test bug,"Fixes #1118 for real this time.

Turns out it's not a locking problem at all.
",closed,True,2016-06-23 17:50:26,2016-06-23 19:43:16
contrib,girishkalele,https://github.com/kubernetes/contrib/pull/1261,https://api.github.com/repos/kubernetes/contrib/issues/1261,Improve build-cop README.md,,closed,True,2016-06-23 18:49:06,2016-06-23 19:09:47
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1262,https://api.github.com/repos/kubernetes/contrib/issues/1262,limit number of flakes we keep in memory,"Need this because I'm going to start reading flakes from PR builder runs, too.
",closed,True,2016-06-23 19:46:59,2016-06-24 19:53:00
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1263,https://api.github.com/repos/kubernetes/contrib/issues/1263,[cluster-autoscaler]: Update godeps,,closed,True,2016-06-23 21:52:10,2016-07-01 15:26:48
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1264,https://api.github.com/repos/kubernetes/contrib/issues/1264,Add option to load test owners from a URL.,"For example, a CSV file stored in a GCS bucket, updated by some other
process.
",closed,True,2016-06-23 22:33:44,2016-07-21 22:08:00
contrib,aLostEngineer,https://github.com/kubernetes/contrib/issues/1266,https://api.github.com/repos/kubernetes/contrib/issues/1266,nginx-alpha Ingress Controller ProxyPass syntax issue,"The ProxyPass URL must have a trailing forward-slash to properly proxy to the root of another http service. Furthermore, the Ingress points must be declared with trailing slashes to match.

Example: with the current setup, if I have a ingress `/a` pointing to a backend nginx service, the Ingress service will keep the `/a` subdirectory appended, forcing the backend service to try to access `<service>/a`, instead of the webroot.

Adding a trailing forward slash to the Ingress point and nginx config will properly forward HTTP requests.
",closed,False,2016-06-24 15:45:10,2016-06-24 17:31:08
contrib,aLostEngineer,https://github.com/kubernetes/contrib/pull/1267,https://api.github.com/repos/kubernetes/contrib/issues/1267,Add trailing slashes to nginx-alpha ingress config,"This address most of #1266. Does not account for failure to add trailing slashes to ingress resource.
",closed,True,2016-06-24 15:47:35,2016-06-24 16:46:08
contrib,aLostEngineer,https://github.com/kubernetes/contrib/issues/1268,https://api.github.com/repos/kubernetes/contrib/issues/1268,nginx-alpha IC redirects for missing trailing slashes,"If #1267 is accepted, we should add template to create a 301 redirect to the proper path when the trailing slash is omitted in the Ingress config.

Example: if ingress point is `<server>/a` it should redirect to `<server>/a/` to properly handle ProxyPass
",closed,False,2016-06-24 15:53:32,2016-06-24 16:45:19
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1269,https://api.github.com/repos/kubernetes/contrib/issues/1269,open for every milestone,"Pushing live now.
",closed,True,2016-06-24 19:57:50,2016-06-24 20:06:33
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1270,https://api.github.com/repos/kubernetes/contrib/issues/1270,fix deployment,"Live now.
",closed,True,2016-06-24 20:06:11,2016-06-24 20:07:25
contrib,ixdy,https://github.com/kubernetes/contrib/issues/1271,https://api.github.com/repos/kubernetes/contrib/issues/1271,"submit queue ""details"" link on PRs no longer works","The submit queue adds a commit status with a details link to something like http://submit-queue.k8s.io/#/queue?prDisplay=27798&historyDisplay=27798.

This no longer seems to be a valid page.
",closed,False,2016-06-24 21:23:15,2016-06-24 21:56:54
contrib,eparis,https://github.com/kubernetes/contrib/pull/1272,https://api.github.com/repos/kubernetes/contrib/issues/1272,submit-queue: update the 'details' link in github,"We were sending people to the front page of the submit-queue. But then
we changed the front page to be the queue instead of the 'all PRs'.

While this link it's particularly useful, at least seeing your PR most
of the time is better than not...

fixes #1271 
",closed,True,2016-06-24 21:36:07,2016-06-24 21:56:54
contrib,foxish,https://github.com/kubernetes/contrib/pull/1273,https://api.github.com/repos/kubernetes/contrib/issues/1273,Fixed clone command to use the github username,,closed,True,2016-06-24 23:23:36,2016-07-08 14:11:58
contrib,fejta,https://github.com/kubernetes/contrib/pull/1274,https://api.github.com/repos/kubernetes/contrib/issues/1274,Switch graph to use svg instead of png,"http://storage.googleapis.com/kubernetes-test-history/k8s-queue-health.svg exists (and the .png's content type is actually svg)
",closed,True,2016-06-25 00:10:00,2016-06-25 03:22:50
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1275,https://api.github.com/repos/kubernetes/contrib/issues/1275,Submit queue crashed (probably),"Last thing in logs:

`
E0626 01:00:06.970839       1 github.go:1205] Can't load commit kubernetes kubernetes 61fb52780122c51f0c29f099c8ab6f1bfbd24ec4: Get https://api.github.com/repos/kubernetes/kubernetes/commits/61fb52780122c51f0c29f099c8ab6f1bfbd24ec4: dial tcp: i/o timeout
`
",closed,False,2016-06-26 05:07:53,2018-02-14 08:08:09
contrib,thockin,https://github.com/kubernetes/contrib/pull/1276,https://api.github.com/repos/kubernetes/contrib/issues/1276,Add gcsweb,"This is a VERY simple webserver that offers selected GCS buckets for browsing.
The HTML is pathetic, but it seems to work (it's as fast as the GCE-hosted
browser (which is not fast), but far less pretty).

This is just the main.  There's no test for it.  The HTML needs help.  There's no Dockerfile or Makefile.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1276)

<!-- Reviewable:end -->
",closed,True,2016-06-26 21:15:03,2016-10-01 04:00:37
contrib,krousey,https://github.com/kubernetes/contrib/pull/1277,https://api.github.com/repos/kubernetes/contrib/issues/1277,Add soak tests to the dashboard,,closed,True,2016-06-27 17:49:27,2016-06-27 18:04:24
contrib,a-robinson,https://github.com/kubernetes/contrib/pull/1278,https://api.github.com/repos/kubernetes/contrib/issues/1278,Update the sidecar logging example with a more recent fluentd build,,closed,True,2016-06-28 02:26:35,2016-07-14 18:25:50
contrib,iterion,https://github.com/kubernetes/contrib/pull/1279,https://api.github.com/repos/kubernetes/contrib/issues/1279,Add AWS,,closed,True,2016-06-28 04:09:30,2016-06-28 04:10:25
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/1280,https://api.github.com/repos/kubernetes/contrib/issues/1280,ansible: refactor roles so kubernetes-addons role runs with the minimal subset of tasks,"When I have a kubernetes already installed and setup on a cluster, I would like to setup the kube-addons on the same cluster without running through the `kubernetes` and `master` roles.

Currently, when running the `kubernetes-addons` the ansible tries to re-install the kubernetes and if there is a newer version of it available, it gets installed. Which is not demanded in some scenarios.
",closed,False,2016-06-28 12:29:48,2016-08-24 18:33:23
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1281,https://api.github.com/repos/kubernetes/contrib/issues/1281,File flakes for presubmit runs,"Still working on this-- mostly clean up at the moment.
",closed,True,2016-06-28 16:25:33,2016-07-07 21:23:36
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1282,https://api.github.com/repos/kubernetes/contrib/issues/1282,Cluster-autoscaler: fix for multi-mig autoscaling,"cc: @fgrzadkowski @piosz @jszczepkowski 

Fixes #1283
",closed,True,2016-06-28 18:37:05,2016-06-28 19:28:27
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1283,https://api.github.com/repos/kubernetes/contrib/issues/1283,Cluster autoscaler requires all migs to be autoscaled,"Otherwise it doesn't do any scale up or scale down.
",closed,False,2016-06-28 18:54:11,2016-06-29 07:20:52
contrib,ciwang,https://github.com/kubernetes/contrib/pull/1284,https://api.github.com/repos/kubernetes/contrib/issues/1284,Munger to add retest-not-required label,"Munger checks if all files are .md files. If so, adds
retest-not-required label to avoid running e2e tests when not necessary.
Added retest-not-required option to --pr-mungers.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1284)

<!-- Reviewable:end -->
",closed,True,2016-06-29 01:46:33,2016-09-08 00:04:08
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1285,https://api.github.com/repos/kubernetes/contrib/issues/1285,Ingress controller ux fixes,"I don't know if this will make 1.3, will try, if not I'll be pushing it out as a minor release cherrypick. 
These ux fixes actually make a difference:
- doesn't hot loop on failure
- doesn't spam activity log with unnecessary firewall updates
- allows non-gce ingress to coexist in the same cluster
- doesn't adopt complex http readiness probes as health checks (probes with headers/hosts/https)
- allows one to continue running the controller on older clusters as a pod (by fixing GCE client creation)
",closed,True,2016-06-29 02:04:38,2016-06-29 23:15:40
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1286,https://api.github.com/repos/kubernetes/contrib/issues/1286,PRs in the queue should sort by first LGTM date,"...instead of by PR number. I think this would be fairer to people.
",closed,False,2016-06-29 02:52:14,2016-08-19 03:09:13
contrib,gouyang,https://github.com/kubernetes/contrib/issues/1287,https://api.github.com/repos/kubernetes/contrib/issues/1287,[ansible] Put did_install varible to group_vars//all.yml,"I'm running testing on the same environment over and over, it takes lots of time to make sure the packages `kubernetes, docker, etcd, flannel` are latest each time. These packages are already latest so I want to skip checking latest package in the ansible script.

Run the default playbook will install openssl and PyYAML as well, it's not so obvious to user if they install package manually, so intend to not touch them.
",closed,False,2016-06-29 07:00:51,2016-07-18 06:07:26
contrib,gouyang,https://github.com/kubernetes/contrib/pull/1288,https://api.github.com/repos/kubernetes/contrib/issues/1288,Put did_install fact into group_vars,"Fixes #1287
",closed,True,2016-06-29 07:27:12,2016-07-18 06:08:17
contrib,simonswine,https://github.com/kubernetes/contrib/pull/1289,https://api.github.com/repos/kubernetes/contrib/issues/1289,ingress: nginx suggest kube-lego for automated cert management,"@aledbf @bprashanth 

Btw: after a release of version 0.8, I would be able to update the docs in the Kube-Lego project, to use the upstream nginx-ingress-controller image
",closed,True,2016-06-29 07:29:20,2016-07-16 00:51:03
contrib,siddharthist,https://github.com/kubernetes/contrib/issues/1290,https://api.github.com/repos/kubernetes/contrib/issues/1290,[ansible] Use already-present certificates,"In [Mantl](https://mantl.io), we already have TLS certificates that we'd like to use for as many purposes as possible (to reduce attack surface, ease frequent rotation, etc). Can we use these Ansible roles with already generated certificates, and skip the generation setup here?
",closed,False,2016-06-29 10:29:41,2018-02-14 18:18:11
contrib,siddharthist,https://github.com/kubernetes/contrib/issues/1291,https://api.github.com/repos/kubernetes/contrib/issues/1291,[ansible] Upload roles to Ansible galaxy,"To make these roles more reusable in other projects, they should be uploaded to Ansible Galaxy. 
",closed,False,2016-06-29 10:33:18,2018-02-14 09:09:11
contrib,iterion,https://github.com/kubernetes/contrib/pull/1292,https://api.github.com/repos/kubernetes/contrib/issues/1292,[cluster-autoscaler]: Add AWS Support,"Adding support for scaling AWS AutoScalingGroups. I've tested this in a test cluster on AWS (with a few more changes needed to allow this to work on Kubernetes v1.2) and everything seemed to function correctly.

It's currently a bit messy to use AWS. You must specify `--cloud-provider=aws` and provide the ASG name and zone in a GCE formatted url: `https://content.googleapis.com/compute/v1/projects/dummy/zones/us-east-1a/autoscalingGroupName`

PTAL and let me know how I can improve this to get it merged. I haven't yet tested this against a v1.3 cluster or against a GCE cluster.
",closed,True,2016-06-29 15:06:29,2016-07-11 16:58:42
contrib,girishkalele,https://github.com/kubernetes/contrib/issues/1293,https://api.github.com/repos/kubernetes/contrib/issues/1293,Need pre-commit hook to run tests on client side,"Saw a failure in the travis CI build log for my PR - #1189 

It would help if we mirrored the steps in .travis.yml in a pre-commit hook so that users can verify changes locally.
",closed,False,2016-06-29 18:00:36,2018-02-14 10:10:09
contrib,raggi,https://github.com/kubernetes/contrib/pull/1294,https://api.github.com/repos/kubernetes/contrib/issues/1294,go2docker: add -expose flag for exposing ports,"Complete the TODO for adding an -expose flag.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1294)

<!-- Reviewable:end -->
",closed,True,2016-06-30 01:18:30,2018-02-17 12:22:03
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1295,https://api.github.com/repos/kubernetes/contrib/issues/1295,Introduce etcd petset,"Example of etcd cluster via petsets

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1295)

<!-- Reviewable:end -->
",closed,True,2016-06-30 08:23:30,2017-08-09 20:22:39
contrib,Nalum,https://github.com/kubernetes/contrib/pull/1296,https://api.github.com/repos/kubernetes/contrib/issues/1296,Fix formatting,"Fix the formatting of this README
",closed,True,2016-06-30 08:53:57,2016-07-02 00:23:50
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1297,https://api.github.com/repos/kubernetes/contrib/issues/1297,Cluster-autoscaler: print version information,"cc: @piosz @jszczepkowski @fgrzadkowski 
",closed,True,2016-06-30 11:05:34,2016-06-30 14:04:33
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/1298,https://api.github.com/repos/kubernetes/contrib/issues/1298,"Ansible: generalization of roles, CI, joined effort","**Decomposition of roles to elevate modularity**

As pointed by @stephenrlouie, it makes sense to generalize the roles and make them independent of playbooks [2]. To deploy kubernetes on one node cluster, there is no need for flannel. Sometimes, only kubernetes addons are needed.

Related issues:
- share the roles in Ansible galaxy [1]: https://github.com/kubernetes/contrib/issues/1291
- deploy kubernetes with provided certificates (skip certificate generator): https://github.com/kubernetes/contrib/issues/1290
- create top level directories: https://github.com/kubernetes/contrib/pull/967

**Joined effort**

Some roles are already shared via Ansible galaxy:
- etcd role: https://github.com/retr0h/ansible-etcd
- docker role: https://github.com/angstwad/docker.ubuntu, https://github.com/jamesdbloom/ansible-install-docker, https://github.com/marklee77/ansible-role-docker
- flannel role: https://github.com/openshift/openshift-ansible/tree/master/roles/flannel, https://github.com/goblain/ansible-role-flannel, https://github.com/vmware/ansible-flannel
- kubernetes role: https://github.com/hidekuro/ansible-role-kubernetes

Others can be found for sure. It makes sense to collaborate with individual folks and elevate the effort.

**CI**

Run various tests on various parts of playbooks to extend the application to various linux distributions. Testing for regressions is important as well. Naming few:
- smoke tests (testing individual roles on various OSes)
- unit-tests (testing created facts on various versions of ansible on various OSes)
- some sort of dashboard with daily digests of run playbooks on various OSes

Both smoke tests and unit-tests can be run over each PR. Daily digest can be taken from periodic jobs run from master HEAD.

**Other effort**:

Now, sooner or later it is desired to move the entire ansible code under kube-deploy repository:
- https://github.com/kubernetes/contrib/issues/635
- https://github.com/kubernetes/contrib/issues/762

Make the deployment faster:
- https://github.com/kubernetes/contrib/issues/1599

[1] http://docs.ansible.com/ansible/galaxy.html
[2] https://github.com/openstack/openstack-ansible
",closed,False,2016-06-30 11:53:05,2018-03-11 21:08:35
contrib,Nalum,https://github.com/kubernetes/contrib/pull/1299,https://api.github.com/repos/kubernetes/contrib/issues/1299,Formatting fix,,closed,True,2016-06-30 14:12:52,2016-07-06 20:47:26
contrib,raggi,https://github.com/kubernetes/contrib/pull/1300,https://api.github.com/repos/kubernetes/contrib/issues/1300,go2docker: address issues with the bundled CA roots,"The bundled CA roots are out of date. One commit imports the up to date Debian ca-certificates.crt.
The second commit adds a flag to the binary to allow users to supply a root bundle.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1300)

<!-- Reviewable:end -->
",closed,True,2016-06-30 22:21:59,2018-07-18 19:12:41
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1301,https://api.github.com/repos/kubernetes/contrib/issues/1301,Cluster autoscaler: add handlign for nill mig configs (in case not all l node pools are autoscaled),"With the previous fix #1282 a possibility for null pointer exceptions/panics was opened as GceManager started to return nills in case of nodes not belonging to an autoscaled mig. This PR fixes the NPE possibilities there and should be included in the release version ASAP.

cc: @piosz @jszczepkowski @fgrzadkowski 
",closed,True,2016-06-30 23:40:56,2016-07-01 15:25:11
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1302,https://api.github.com/repos/kubernetes/contrib/issues/1302,Dashboard to show stats for all repos,"Now that we're sprouting lots of repos, it would be useful to have a single dashboard with PR / issue / submit queue stats for all the repos.

Maybe such a thing already exists.
",closed,False,2016-07-01 06:45:37,2018-01-17 07:45:13
contrib,wernight,https://github.com/kubernetes/contrib/issues/1303,https://api.github.com/repos/kubernetes/contrib/issues/1303,QUIC & HTTP/2 Ingress controller,"I see there are [various Ingress controller](https://github.com/kubernetes/contrib/tree/master/ingress/controllers), even though I don't know how to choose in my Kuberentes Yaml files.

I'd like to suggest having one that enabled QUIC and HTTP/2 like https://devsisters.github.io/goquic/ does. It should also support WebSockets. That would allow easy creation of a network Ingress frontend that would improve speed for many users transparently. The current GoQuic doesn't support virtualhost so that would have to be implemented somehow.
",closed,False,2016-07-01 09:49:36,2018-02-20 23:44:01
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1304,https://api.github.com/repos/kubernetes/contrib/issues/1304,Make mungegithub work on all our repos,"cc @kubernetes/contributor-experience 
",closed,False,2016-07-01 15:57:19,2016-08-15 09:36:37
contrib,ekozan,https://github.com/kubernetes/contrib/issues/1305,https://api.github.com/repos/kubernetes/contrib/issues/1305,Commit a85ed07 : Break Coreos install with ansible ,"```
TASK [etcd : Call restart etcd] ************************************************
skipping: [***]

TASK [etcd : Call restart etcd2] ***********************************************
included: /Development/Kube/contrib/ansible/roles/etcd/tasks/etcd2_restart.yml for ***

TASK [etcd : restart etcd2] ****************************************************
fatal: [***]: FAILED! => {""failed"": true, ""msg"": ""The conditional check 'etcd2_modified == true and etcd_started.changed == false' failed. The error was: error while evaluating conditional (etcd2_modified == true and etcd_started.changed == false): 'etcd_started' is undefined\n\nThe error appears to have been in '/Development/Kube/contrib/ansible/roles/etcd/tasks/main.yml': line 12, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n- name: Call restart etcd2\n  ^ here\n""}
```
",closed,False,2016-07-01 16:29:30,2016-07-05 19:41:33
contrib,foxish,https://github.com/kubernetes/contrib/pull/1306,https://api.github.com/repos/kubernetes/contrib/issues/1306,Added check-labels munger and corresponding tests.,"https://github.com/kubernetes/contrib/issues/1423
The munger checks for changes in label config yaml file and creates labels as necessary in the repository.
",closed,True,2016-07-01 19:39:35,2016-08-02 22:16:51
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1307,https://api.github.com/repos/kubernetes/contrib/issues/1307,"oops, move soak to nonblocking","Pushing live now.
",closed,True,2016-07-01 22:23:58,2016-07-06 15:46:02
contrib,thockin,https://github.com/kubernetes/contrib/pull/1308,https://api.github.com/repos/kubernetes/contrib/issues/1308,Apostrophes are wrong,,closed,True,2016-07-03 03:08:20,2016-07-03 03:31:08
contrib,merlin83,https://github.com/kubernetes/contrib/pull/1309,https://api.github.com/repos/kubernetes/contrib/issues/1309,Fix incorrect variable in Call restart etcd2's conditional,"Bugfix for incorrect variable name

Fixes: #1305
",closed,True,2016-07-03 17:27:42,2016-07-05 19:42:52
contrib,kayrus,https://github.com/kubernetes/contrib/pull/1310,https://api.github.com/repos/kubernetes/contrib/issues/1310,"Ingress: Updated ingress docs, avoid nginx-alpha use.","<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1310)

<!-- Reviewable:end -->
",closed,True,2016-07-04 12:21:13,2016-10-13 05:59:51
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1311,https://api.github.com/repos/kubernetes/contrib/issues/1311,Cluster-autoscaler: support other cloud providers,"This is an umbrella bug for all issues and PR for supporting other (than GCE/GKE) cloud providers in cluster autoscaler.
",closed,False,2016-07-04 14:12:42,2017-04-19 10:39:00
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1312,https://api.github.com/repos/kubernetes/contrib/issues/1312,Cluster-autoscaler: cloud provider interface,"https://github.com/kubernetes/kubernetes/tree/master/pkg/cloudprovider doesn't define a concept of NodePool/Group/Mig and doesn't have add/delete node in the main interface (although some implementation have it). However we may reuse some fragments of K8s implementation. We could also expand the kubernetes interface although we are not sure how/if the similar capabilities exist in other cloud providers and how to implement them. Once we have some implementations in contrib we may think about moving them to K8s.

Ref: #1311
cc: @piosz @jszczepkowski @fgrzadkowski 
",closed,True,2016-07-04 14:13:45,2016-07-06 07:51:19
contrib,ryane,https://github.com/kubernetes/contrib/pull/1313,https://api.github.com/repos/kubernetes/contrib/issues/1313,zookeeper pet: fix typo in readme for failover cmd,"$1 is not defined and the `nc zoo-$1.zk:2181` command fails with output like:

```
nc: bad address 'zoo-.zk:2181'
zoo-0
zoo-1
nc: bad address 'zoo-.zk:2181'
zoo-2
nc: bad address 'zoo-.zk:2181'
...
```

it should run `nc zoo-$i.zk:2181` instead
",closed,True,2016-07-05 02:30:36,2016-08-02 21:52:06
contrib,fcvarela,https://github.com/kubernetes/contrib/issues/1314,https://api.github.com/repos/kubernetes/contrib/issues/1314,[nginx-ingress-controller] Nginx aborts reloads w/ certificate errors,"The ingress controller writes certificates to /etc/nginx-ssl on a goroutine and reload nginx on another. On a busy nginx controller, I've observed that nginx throws errors like:

`2016/07/05 11:32:56 [emerg] 13#13: PEM_read_bio_X509(""/etc/nginx-ssl/xxx-yyy"") failed (SSL: error:0906D066:PEM routines:PEM_read_bio:bad end line)`

After this happens any rc/pod/deployment restart that results in pods getting new IPs will make the entire upstream unavailable (503 on nginx) as the aborted reload renders it unaware of those changes.

Manual inspection of nginx.conf shows the upstream does have the new ips, that the certificates and keys are correct.

A second manual reload solves the issue.

This is caused by writing certificates/keys using ioutil (async) while nginx is trying to read them. I have a working and verified patch that changes it to tempfiles and atomic renames.
",closed,False,2016-07-05 11:45:36,2016-07-06 16:11:01
contrib,fcvarela,https://github.com/kubernetes/contrib/pull/1315,https://api.github.com/repos/kubernetes/contrib/issues/1315,Addresses #1314 [nginx-ingress-controller ssl nginx reload abort],"Replaces async file writes with atomic renaming of temp files.
",closed,True,2016-07-05 11:54:19,2016-07-06 16:10:54
contrib,mikesplain,https://github.com/kubernetes/contrib/issues/1316,https://api.github.com/repos/kubernetes/contrib/issues/1316,[nginx-ingress-controller] real_ip_header is duplicate when useProxyProtocol is enabled ,"It seems as though since https://github.com/kubernetes/contrib/commit/a152cc44707b8f214666ba7468ceb6575478333b, if useProxyProtocol is enabled, real_ip_header is set twice causing an error when reloading nginx: 

```
nginx: [emerg] ""real_ip_header"" directive is duplicate in /etc/nginx/nginx.conf:65
```

This prevents the ingress controller from starting in 0.8 if useProxyProtocol is enabled.
",closed,False,2016-07-05 16:20:20,2016-07-06 17:41:38
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1317,https://api.github.com/repos/kubernetes/contrib/issues/1317,[nginx-ingress-controller] Fix duplicated real_ip_header,"fixes #1316
",closed,True,2016-07-05 16:38:35,2016-07-05 17:44:27
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1318,https://api.github.com/repos/kubernetes/contrib/issues/1318,[nginx-ingress-controller]: Release 0.8.1,,closed,True,2016-07-05 17:35:53,2016-07-06 18:45:39
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/1319,https://api.github.com/repos/kubernetes/contrib/issues/1319,etcd ansible role: file naming convention,"- most of the files had hyphen names, besides restart.
",closed,True,2016-07-05 19:50:13,2016-07-05 19:51:10
contrib,stephenrlouie,https://github.com/kubernetes/contrib/pull/1320,https://api.github.com/repos/kubernetes/contrib/issues/1320,etcd-role: hyphen naming convention added to restart files,"- to match the rest of the etcd files
",closed,True,2016-07-05 20:29:40,2016-07-07 15:49:57
contrib,wernight,https://github.com/kubernetes/contrib/pull/1321,https://api.github.com/repos/kubernetes/contrib/issues/1321,Update README.md,"Fixes markdown.
",closed,True,2016-07-06 08:19:08,2016-07-07 11:06:28
contrib,gmarek,https://github.com/kubernetes/contrib/issues/1322,https://api.github.com/repos/kubernetes/contrib/issues/1322,k8s-bot: there should be a command to rerun GKE smoke tests only,"I wasn't able to find one. cc @eparis @lavalamp 
",closed,False,2016-07-06 08:43:17,2016-07-08 08:31:52
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1323,https://api.github.com/repos/kubernetes/contrib/issues/1323,Cluster-autoscaler: cloud provider interface implementation for GCE,"cc: @piosz @fgrzadkowski @jszczepkowski

Part of #1311
",closed,True,2016-07-06 13:48:37,2016-07-07 12:19:51
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1324,https://api.github.com/repos/kubernetes/contrib/issues/1324,GCE Ingress docs update,"Update docs for 1.3
",closed,True,2016-07-06 20:39:55,2016-07-09 03:07:27
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/1325,https://api.github.com/repos/kubernetes/contrib/issues/1325,systemd defaults: remove ServiceAccount from admissions,"As a starting user of kubernetes I would like to use the default account as much as possible and discover concept of admissions later. Currently, when the kubernetes is installed and the `kube-apiserver` started, I can not start playing with k8s right away. I have to tune the configuration first.

Let's discuss what are the disadvantages of disabling the `ServiceAccount` admission control [1].

[1] https://github.com/kubernetes/contrib/blob/master/init/systemd/environ/apiserver#L23
",closed,False,2016-07-07 17:20:30,2016-09-07 14:22:57
contrib,shinyben,https://github.com/kubernetes/contrib/pull/1326,https://api.github.com/repos/kubernetes/contrib/issues/1326,Update the Makefile to allow local_dryrun to work,,closed,True,2016-07-07 20:23:33,2016-07-07 20:26:39
contrib,dnascimento,https://github.com/kubernetes/contrib/issues/1327,https://api.github.com/repos/kubernetes/contrib/issues/1327,[nginx-ingress-controller] JWT and Client SSL Verify,"NGINX can handle JWT token validation to provide authentication on APIs. 
Verify Client SSL Certificate should also be possible.

It requires to change the spec in kubernetes project to add:

```
 jwt:
    - hosts:
        - XXXXXX
 client-verify:
     - hosts:
         - XXXXX
```
",closed,False,2016-07-08 02:55:28,2018-02-14 11:11:11
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1328,https://api.github.com/repos/kubernetes/contrib/issues/1328,Cluster-autoscaler: use cloud provider interface in the code,"Plus remove SampleNode() function from CloudProvider interface.

Ref: #1311

cc: @iterion @piosz @fgrzadkowski @jszczepkowski 
",closed,True,2016-07-08 11:34:04,2016-07-18 07:23:52
contrib,cvle,https://github.com/kubernetes/contrib/issues/1329,https://api.github.com/repos/kubernetes/contrib/issues/1329,Nginx Ingress Controller - Default Backend when using HTTPS,"NGINX redirects all HTTP traffic for undefined hosts to the default backend. But when using HTTPS the traffic all goes to the first defined TLS host in the generated `nginx.conf`. 

By default NGINX should direct all traffic regardless of HTTP or HTTPS to the default backend. When using HTTPS NGINX should serve a default self-signed certificate.

What do you think?
",closed,False,2016-07-08 11:41:22,2016-07-20 17:35:16
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1330,https://api.github.com/repos/kubernetes/contrib/issues/1330,Add steps for Fedora,"I took me some reading and head shaking to make it run on Fedora 24.
",closed,True,2016-07-08 12:22:06,2016-07-08 14:00:38
contrib,yvespp,https://github.com/kubernetes/contrib/issues/1331,https://api.github.com/repos/kubernetes/contrib/issues/1331,Nginx Ingress Controller - global default certificate,"I want to be able to configure a certificate in the namespace of the Ingress Controller and it should be used for all Ingress rules, regardless of their namespace and their Ingress TLS configuration.
An Individual Ingress resource should still be able to overwrite it.

**Use-case:**
Wild card DNS and certificate are set up for *.mycorp.com. A user can now setup an ingress rule for myns.mycorp.com/app1 and it will be protected with TLS without having to configure certs or a TLS ingress rule.

**Implementation proposal:**
New optional command line parameter for the controller: a reference to the default secret.
If present and the cert matches spec.rules.host, TLS will be enabled.
If a rule specifies it's own cert, it will overwrite the default cert.
TCP and UDP services are not affected.

What do you think? Would it make sense to add this to the controller?

I tried to solve this via this Ingress resource that the doc mentions but I could not get it to work:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: no-rules-map
spec:
  tls:
    secretName: testsecret
  backend:
    serviceName: s1
    servicePort: 80
```

How is this supposed to work?
Also all 30 seconds or so the following log message appears in the Ingress controller log:

```
I0708 14:26:54.489232       1 controller.go:933] Using the secret ingresssecret as source for the default SSL certificate
```
",closed,False,2016-07-08 14:48:33,2016-07-20 18:34:08
contrib,dims,https://github.com/kubernetes/contrib/pull/1332,https://api.github.com/repos/kubernetes/contrib/issues/1332,Allow updates to kubectl-cheatsheet.md,,closed,True,2016-07-08 15:19:36,2016-07-13 13:23:16
contrib,shinyben,https://github.com/kubernetes/contrib/pull/1333,https://api.github.com/repos/kubernetes/contrib/issues/1333,Jenkins comment deleter updated to new format,,closed,True,2016-07-08 15:32:14,2016-07-08 16:02:19
contrib,simonswine,https://github.com/kubernetes/contrib/issues/1334,https://api.github.com/repos/kubernetes/contrib/issues/1334,pet/redis: redis-server won't run,"I am not too sure what changed, but I am not able to run the redis PetSet.

The `install` and `bootstrap` container succeeds, but somehow the binary links against musl c library which is not available in the latest debian:jessie:

```
root@9e6ec95b7fab:/# ldd /opt/redis/redis-server
        linux-vdso.so.1 (0x00007ffed4774000)
        libc.musl-x86_64.so.1 => not found
```

after install musl everything works fine

``````
root@9e6ec95b7fab:/# apt-get install --reinstall musl
[...]

root@9e6ec95b7fab:/# /opt/redis/redis-server
215:C 08 Jul 16:54:28.225 # Warning: no config file specified, using the default config. In order to specify a config file use /opt/redis/redis-server /path/to/redis.conf
                _._
           _.-``__ ''-._
      _.-``    `.  `_.  ''-._           Redis 3.2.0 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 215
  `-._    `-._  `-./  _.-'    _.-'
 |`-._`-._    `-.__.-'    _.-'_.-'|
 |    `-._`-._        _.-'_.-'    |           http://redis.io
  `-._    `-._`-.__.-'_.-'    _.-'
 |`-._`-._    `-.__.-'    _.-'_.-'|
 |    `-._`-._        _.-'_.-'    |
  `-._    `-._`-.__.-'_.-'    _.-'
      `-._    `-.__.-'    _.-'
          `-._        _.-'
              `-.__.-'

``````

@bprashanth 
",closed,False,2016-07-08 16:57:39,2018-02-14 11:11:11
contrib,shinyben,https://github.com/kubernetes/contrib/pull/1335,https://api.github.com/repos/kubernetes/contrib/issues/1335,Updated k8s bot to find old messages and delete them in favor of newe…,"…r messages
",closed,True,2016-07-08 17:21:11,2016-07-11 21:10:17
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1336,https://api.github.com/repos/kubernetes/contrib/issues/1336,[nginx-ingress-controller]: Add annotation to skip ingress rule,,closed,True,2016-07-08 21:01:58,2016-07-20 17:33:17
contrib,shinyben,https://github.com/kubernetes/contrib/pull/1337,https://api.github.com/repos/kubernetes/contrib/issues/1337,Spammy jenkins bot fix,,closed,True,2016-07-08 21:17:12,2016-07-12 15:52:40
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1338,https://api.github.com/repos/kubernetes/contrib/issues/1338,[nginx-ingress-controller]: Add HTTPS default backend,"fixes #1331 
fixes #1329 
",closed,True,2016-07-08 21:20:40,2016-07-25 21:26:44
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1339,https://api.github.com/repos/kubernetes/contrib/issues/1339,"Update election code, client and godeps",,closed,True,2016-07-08 23:14:28,2016-07-14 05:45:23
contrib,fejta,https://github.com/kubernetes/contrib/pull/1340,https://api.github.com/repos/kubernetes/contrib/issues/1340,Track issues added/removed/tested,"Added three integers, which increment in the following way:
- `issuesAdded` - whenever an item is added to the queue
- `issuesRemoved` - whenever an item is removed from the queue
- `issuesTested` -- whenever an item is tested

These values will allow us to track how rate we dequeue items. This rate usually differs from both the merge rate (we remove items without merging them) as well as the change in the size of the queue (we can add items as quickly as we remove them).

This also allows us to distinguish how quickly we tend to merge things from how quickly we test things (since we can test things but not merge them if they fail tests).

A couple additional small refactorings:
- Alphabetize `submitQueueStats`
- Encapsulate deleting issues from the queue into `deleteIssue()`

Test version visible at http://146.148.108.121/sq-stats
",closed,True,2016-07-11 06:53:33,2016-07-13 18:59:04
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1341,https://api.github.com/repos/kubernetes/contrib/issues/1341,Cluster-autoscaler: scaledown node usage trackers,"Right now on successful scale down we reset utilization information on all nodes, even if they had nothing in common during the simulation and utilization calculation. This PR brings in a data structure that remembers which nodes were found useful for which nodes during scale down simulations so that only nodes that used the same other nodes for simulated pod rescheduling are reset.

cc: @piosz @fgrzadkowski @jszczepkowski 
",closed,True,2016-07-11 13:57:49,2016-07-14 12:04:50
contrib,Trane9991,https://github.com/kubernetes/contrib/pull/1342,https://api.github.com/repos/kubernetes/contrib/issues/1342,updated kube-dashboard to the version 1.1.0,,closed,True,2016-07-11 13:58:42,2016-07-11 14:34:49
contrib,kokhang,https://github.com/kubernetes/contrib/pull/1343,https://api.github.com/repos/kubernetes/contrib/issues/1343,Kubernetes multi-backend service loadbalancer,"This addresses issue https://github.com/kubernetes/contrib/issues/1506
# High Available Multi-Backend Load balancer

The project implements a load balancer controller that will provide a high available and load balancing access to HTTP and TCP kubernetes applications. It will support different type of loadbalancer backends ranging from software LB such as nginx, to hardward LB such as F5 and cloud LB like Openstack LBaaS.

To create a loadbalancer, users will just have to create a configmap and provide two inputs:
- Service name
- Service namespace

A configmap targeted to the loadbalancer controller would look like:

```
apiVersion: v1
kind: ConfigMap
metadata:
    name: configmap-my-service
    labels:
        app: loadbalancer
data:
    namespace: ""default""
    target-service-name: ""my-service""
```

Once the configmap is created, a new backend is create and configured to loadbalance the provided service.

Our goal is to have this controller listen to ingress events, rather than config map for generating config rules. Currently, this controller watches for configmap resources to create and configure backend. Eventually, this will be changed to watch for ingress resource instead. This feature is still being planned in kubernetes since the current version of ingress does not support layer 4 routing.

This controller is designed to easily integrate and create different load balancing backends. From software, hardware to cloud loadbalancer. Our initial featured backends are software loadbalacing (with keepalived and nginx), hardware loadbalancing with F5 and cloud loadbalancing with Openstack LBaaS v2 (Octavia).
### Software Loadbalancer via Daemon and VIPs

In the case of software loadbalancer, this controller works with loadbalancer-controller daemons which are deployed across nodes and will server as high available loadbalancers.  The loadbalancer controller will communicate with the daemons via a configmap resource.
These daemon controllers use keepalived and nginx to provide the high availability loadbalancing via the use of VIPs. VIPS are allocated to every service that's being loadbalanced. This will allow multiple services that bind to the same ports to work.
For F5 and Openstack LBaaS, the loadbalancer controllers talk to the appropriate servers via their APIs. So loadbalancer-controller daemons are not needed.
### Difference between this and [service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer) or [nginx](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx).

Service-loadbalancer is a great option but it is only tailored for software loadbalancer using HAProxy and it is not designed in a way that it can be easily decoupled. 

The nginx ingress controller is only for nginx and only works for layer 7 applications. This project is intended to provide support for many different backends and work with all kubernetes applications (layer 7 and layer4).

Service-loadbalancer support for L4 is very limited. The binding-port needs to be open and specified as a hostPort during the controller creation. This forces the users to specify and open the ports at the beginning. This will also prevent two different services to loadbalance on the same port (ie running two mysql services). This projects uses VIPs to resolve this limitation.
## Examples:
### Software Loadbalancer using keepalived and nginx

1) First we need to create the loadbalancer controller.

```
$ kubectl create -f examples/kube-loadbalancer-rc.yaml
```

2) The loadbalancer daemon pod will only start in nodes that are labeled type: loadbalancer. Label the nodes you want the daemon to run on

```
$ kubectl label node my-node1 type=loadbalancer
```

3) Create our sample app, which consists of a service and replication controller resource: 

```
$ kubectl create -f examples/coffee-app.yaml
```

4) Create configmap for the sample app service. This will be used to configure the loadbalancer backend:

```
$ kubectl create -f coffee-configmap.yaml
```

5) Get the bind IP generated by the loadbalancer controller from the configmap.

```
$ kubectl get configmap configmap-coffee-svc -o yaml
apiVersion: v1
data:
  bind-ip: ""10.0.0.10""
  namespace: default
  target-service-name: coffee-svc
kind: ConfigMap
metadata:
  creationTimestamp: 2016-06-17T22:30:03Z
  labels:
    app: loadbalancer
  name: configmap-coffee-svc
  namespace: default
  resourceVersion: ""157728""
  selfLink: /api/v1/namespaces/default/configmaps/configmap-coffee-svc
  uid: 08e12303-34db-11e6-87da-fa163eefe713
```

6) To access your app:

```
 $ curl http://10.0.0.10
  <!DOCTYPE html>
  <html>
  <head>
  <title>Hello from NGINX!</title>
  <style>
      body {
          width: 35em;
          margin: 0 auto;
          font-family: Tahoma, Verdana, Arial, sans-serif;
      }
  </style>
  </head>
  <body>
  <h1>Hello!</h1>
  <h2>URI = /coffee</h2>
  <h2>My hostname is coffee-rc-mu9ns</h2>
  <h2>My address is 10.244.0.3:80</h2>
  </body>
  </html>
```

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1343)

<!-- Reviewable:end -->
",closed,True,2016-07-11 17:03:04,2017-11-20 03:13:17
contrib,micheleorsi,https://github.com/kubernetes/contrib/issues/1344,https://api.github.com/repos/kubernetes/contrib/issues/1344,[nginx ingress controller] Nginx statistics based on IP:PORT,"Once we started to collect statistics from the /nginx_status page we noticed that they are based on IP:PORT, instead of name-of-pod.

Which means that when pods come and go (with different pod name but eventually with the same IP:PORT) NGINX ""inherits"" the history of a previous pod that was using the same IP:PORT (but had different name).

Would it be possible to have nginx statistcs based on name-of-pod instead of IP:PORT?

Thanks
",closed,False,2016-07-11 17:04:39,2016-08-26 01:13:21
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1345,https://api.github.com/repos/kubernetes/contrib/issues/1345,Cluster-autoscaler: correctly support completely empty nodes,"Make sure that appropriate node info is created also for nodes that have absolutely no pods running on them.
",closed,False,2016-07-11 17:10:59,2017-04-19 15:03:25
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1346,https://api.github.com/repos/kubernetes/contrib/issues/1346,Load test owners from local kubernetes repo clone.,"I tested this locally with `./mungegithub --dry-run --pr-mungers=flake-manager --test-owners-csv /tmp/owners.csv`
",closed,True,2016-07-11 18:47:32,2016-07-13 23:28:22
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1347,https://api.github.com/repos/kubernetes/contrib/issues/1347,Cluster-autoscaler: pod relocation hints,"Average cluster, for most of the time, remains more or less stable. So pod rescheduling location calculated in scale down 10sec ago are most likely are still valid. This PR makes advantage of this. Benefits:
- Scale down will work MUCH faster
- Scale down will try to put pods on the same nodes which will be important once #1341 goes in. This PR, #1341 and following PRs will allow fine grain utilization/not-needed status resets.

cc: @piosz @jszczepkowski @fgrzadkowski 
",closed,True,2016-07-11 19:13:02,2016-07-14 10:07:07
contrib,shinyben,https://github.com/kubernetes/contrib/pull/1348,https://api.github.com/repos/kubernetes/contrib/issues/1348,Use kubectl to create secret instead of sed,"Ha! Rhymes

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1348)

<!-- Reviewable:end -->
",closed,True,2016-07-11 20:52:09,2017-09-29 15:49:34
contrib,foxish,https://github.com/kubernetes/contrib/pull/1349,https://api.github.com/repos/kubernetes/contrib/issues/1349,Make submit-queue run against other projects,"#1304

Added feature to access google-cloud-storage and specify bucket
and directory parameters from the command-line.
",closed,True,2016-07-12 02:57:46,2016-07-21 21:49:13
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1350,https://api.github.com/repos/kubernetes/contrib/issues/1350,[nginx-ingress-controller]: Improve performance (listen backlog=net.core.somaxconn),,closed,True,2016-07-12 03:05:44,2017-01-19 00:10:17
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1351,https://api.github.com/repos/kubernetes/contrib/issues/1351,[nginx-ingress-controller]: Avoid generation of invalid ssl certificates,"This change avoids the generation of the pem file used by nginx in case of errors in the certificate
",closed,True,2016-07-12 10:21:34,2016-07-20 20:36:23
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1352,https://api.github.com/repos/kubernetes/contrib/issues/1352,Cluster-autoscaler: use full drain logic in scale down.,"Grace period in particular.

cc: @piosz @jszczepkowski @fgrzadkowski 
",closed,False,2016-07-12 12:51:49,2017-04-19 10:40:11
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1353,https://api.github.com/repos/kubernetes/contrib/issues/1353,Cluster-autoscaler: use disruption budget,,closed,False,2016-07-12 12:54:44,2016-10-21 08:28:37
contrib,erictune,https://github.com/kubernetes/contrib/issues/1354,https://api.github.com/repos/kubernetes/contrib/issues/1354,nginx-slim: bake daemon off option into image,"To use ningx with docker, you need to set `daemon off`.
So, people who run this image end up adding these 5 lines to pod spec:

```
        command:
        - nginx
        args:
        - -g
        - ""daemon off;""
```

Would it make sense to add:
`RUN echo ""daemon off;"" >> /etc/nginx/nginx.conf`
to the Dockerfile so that people don't have to add those lines? 
",closed,False,2016-07-12 17:49:17,2016-07-13 20:39:37
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1355,https://api.github.com/repos/kubernetes/contrib/issues/1355,[nginx-slim]: Update nginx to 1.11.2,"fixes #1354
",closed,True,2016-07-12 18:18:41,2016-07-20 20:05:54
contrib,gregory-lyons,https://github.com/kubernetes/contrib/pull/1356,https://api.github.com/repos/kubernetes/contrib/issues/1356,Atomic clone and updates for git-sync,"The git-sync sidecar container pulls Git content into a shared volume, but another container may begin accessing the incomplete repository while the initial pull is occurring. There has been some discussion of the matter here: https://github.com/kubernetes/kubernetes/issues/17676.

This change provides the option to make the initial clone/pull atomic (i.e. the target directory will not appear in the filesystem until the pull is complete). I've included atomic.md as a readme with more information about usage and implementation, but can merge that into the existing README.md instead if that is preferred. 

@thockin @huggsboson
",closed,True,2016-07-12 18:55:16,2016-08-19 05:09:13
contrib,shinyben,https://github.com/kubernetes/contrib/pull/1357,https://api.github.com/repos/kubernetes/contrib/issues/1357,[mungegithub] better error checking and error returns,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1357)

<!-- Reviewable:end -->
",closed,True,2016-07-12 19:59:30,2018-02-17 12:22:02
contrib,eparis,https://github.com/kubernetes/contrib/pull/1358,https://api.github.com/repos/kubernetes/contrib/issues/1358,[WIP] remove multiple ok-to-test comments as well,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1358)

<!-- Reviewable:end -->
",closed,True,2016-07-12 21:18:22,2017-04-07 20:34:34
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1359,https://api.github.com/repos/kubernetes/contrib/issues/1359,Tool to close pull requests,"Try an alternative approach to ""closing staling PR"" aka #891 
",closed,True,2016-07-12 23:43:36,2016-07-15 04:50:07
contrib,gmarek,https://github.com/kubernetes/contrib/pull/1360,https://api.github.com/repos/kubernetes/contrib/issues/1360,Remove gce-scalability from blockers list,"cc @eparis @lavalamp 
",closed,True,2016-07-13 08:18:36,2016-07-13 20:43:57
contrib,talset,https://github.com/kubernetes/contrib/pull/1361,https://api.github.com/repos/kubernetes/contrib/issues/1361,[ansible] Add master_cluster_hostname param,"- In order to add access to the api with an internal and public fqdn
  We added 2 variables in order to update the server.crt
- Add master_cluster_hostname variable
- Add kube_master_cluster_public_hostname variable
  
  TODO : we should use the internal name to configure the nodes

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1361)

<!-- Reviewable:end -->
",closed,True,2016-07-13 13:58:13,2017-03-06 17:37:35
contrib,wittrock,https://github.com/kubernetes/contrib/issues/1362,https://api.github.com/repos/kubernetes/contrib/issues/1362,[GCE L7LB controller] watch cluster uid config map so updates propogate without restart,"After upgrading two of my clusters in the same project from versions 1.2.5 to version 1.3 of kubernetes, their GCE objects are clobbering each other, and it seems to be because my manually-set `cluster-uid` got deleted during the upgrade. 

```
➜  ~ kubectl describe configMap --namespace kube-system
Name:       ingress-uid
Namespace:  kube-system
Labels:     <none>
Annotations:    <none>

Data
===
uid:    0 bytes
```

Even if I edit this configMap and add a cluster-uid, my GCE resources still clobber each other. For instance, all load balancers created by creating ingresses point to the same GCE instance group, called `k8s-ig`. Previously, they pointed to `k8s-ig--my-manually-set-uid`. 

Based on #680 by @bprashanth, it looks like new objects should be created with whatever is in that configMap first, but that's not happening. 

This is fixed if I spin up a new cluster and create a test ingress--those objects are properly namespaced.

Given that I can't fix the issue by editing ingress-uid manually, is there another workaround? 
",closed,False,2016-07-13 21:08:20,2018-02-15 04:27:05
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1363,https://api.github.com/repos/kubernetes/contrib/issues/1363,Use existing uid if one is found,"Without this if we create some ingresses we will get eg: a forwarding rule like ""foo-uid"". Now if we restart 
the ingress controller, and while it's down delete the configmap where it stores its uid, it will come back, see an existing ingress, but wrongly record the uid as ""empty string"". This will cause the ingress to ignore the old forwarding rule, backends etc.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1363)

<!-- Reviewable:end -->
",closed,True,2016-07-13 22:56:54,2016-08-12 18:03:58
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1364,https://api.github.com/repos/kubernetes/contrib/issues/1364,Fix test owners csv path.,,closed,True,2016-07-13 23:13:44,2016-07-13 23:44:56
contrib,madhusudancs,https://github.com/kubernetes/contrib/pull/1365,https://api.github.com/repos/kubernetes/contrib/issues/1365,Import kubeform - a Kubernetes Terraform plugin.,"This should probably be in a separate repository, but upon @bgrant0607's suggestion I am putting it here until we find a better home.
",closed,True,2016-07-14 00:23:45,2016-07-14 01:42:55
contrib,madhusudancs,https://github.com/kubernetes/contrib/pull/1366,https://api.github.com/repos/kubernetes/contrib/issues/1366,Import kubeform - a Kubernetes Terraform plugin.,"This should probably be in a separate repository, but upon @bgrant0607's suggestion I am putting it here until we find a better home.

Supersedes #1365
",closed,True,2016-07-14 01:42:26,2016-07-21 17:28:26
contrib,tushar281188,https://github.com/kubernetes/contrib/issues/1367,https://api.github.com/repos/kubernetes/contrib/issues/1367,kubernetes/Ingress: Getting connection refused error on running nginx-ingress replication controller,"Hi,

I am trying to run ingress controller example but getting ""Get https://204.17.5.1:443/apis/extensions/v1beta1/ingresses: dial tcp 204.17.5.1:443: getsockopt: connection refused"" error, where ""204.17.5.1"" is kubernetes service cluster IP. 

I changed port in rc.yaml to 443 but still facing the same error. 

Found nearly similar conversation at ""http://stackoverflow.com/questions/34147547/can-not-access-kubernetes-master-apis-via-https-dial-tcp-10-200-0-1443-i-o-tim"", but no luck. 

Can someone guide me on this.

Example link that I followed ""https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx-alpha""

K8s version : 1.2.5
",closed,False,2016-07-14 11:17:31,2016-08-26 01:10:26
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/1368,https://api.github.com/repos/kubernetes/contrib/issues/1368,"ansible: when installing flanel from f25, it has different systemd env variables","I did that [1], changelog:

``` vim
* Wed Jun 29 2016 jchaloup <jchaloup@redhat.com> - 0.5.5-7
- Own /usr/libexec/flannel directory
- make envs in service and config file canonical
```

This needs to be reflected in the playbook.

[1] http://koji.fedoraproject.org/koji/buildinfo?buildID=776500
",closed,False,2016-07-14 14:24:10,2016-07-17 16:56:26
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1369,https://api.github.com/repos/kubernetes/contrib/issues/1369,Better events for the gce ingress controller,"At least: 
1. Service not found, because its common for people to try to ingress a service in a different namespace
2. Service doesn't have node port
3. What health check and port the url is using
4. tls cert issues (wrong keys in secret, bad certs)
",closed,False,2016-07-14 16:24:26,2018-02-15 23:46:01
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1370,https://api.github.com/repos/kubernetes/contrib/issues/1370,Cluster-autoscaler: use node usage tracker for better utilization resets,"Follow up of #1341.

There remaining work is:
- Better node selection in rescheduling checks so that the possible reschedulings are evenly spread across the nodes.
- Include number of nodes that would be cleared in node to remove selection.

cc: @piosz @fgrzadkowski @jszczepkowski 
",closed,True,2016-07-14 16:26:00,2016-07-15 14:59:07
contrib,aabed,https://github.com/kubernetes/contrib/issues/1371,https://api.github.com/repos/kubernetes/contrib/issues/1371,Creating tokens for new nodes,"Everything works fine when you create a full cluster, but when you want to add a new node to the cluster with _--limit_ option the task for generating tokens is never called because

```
- include: gen_tokens.yml
  when: inventory_hostname == groups['masters'][0] 
```

So how about adding a condition so that it would generate token for the new added node

```
- name: Check existence for token files
  stat: ""{{ kube_script_dir }}/kube-gen-token.sh {{ item[0] }}-{{ item[1] }}""
  with_nested:
    - [ ""system:controller_manager"", ""system:scheduler"", ""system:kubectl"" ]
    - ""{{ groups['masters'] }}""
  register: token_file 
```

```
- include: gen_tokens.yml
  when: (inventory_hostname == groups['masters'][0]) or (token_file.isreg == False ) 
```
",closed,False,2016-07-15 15:24:12,2018-03-09 16:16:39
contrib,david-mcmahon,https://github.com/kubernetes/contrib/issues/1372,https://api.github.com/repos/kubernetes/contrib/issues/1372,Show details per user on master and branch PR state via dashboard,"Open/closed in both locations and relevant labels cherrypick-*.
cc @matchstick @lavalamp @eparis @bgrant0607 

Moved from https://github.com/kubernetes/kubernetes/issues/27403
",closed,False,2016-07-15 16:45:31,2017-01-21 01:42:22
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1373,https://api.github.com/repos/kubernetes/contrib/issues/1373,Skip retest for docs,"Fixes #915.

Creates a new mungers that will check changed files, and label as ""retest-not-required"" if it contains only "".md"" files.
",closed,True,2016-07-15 20:50:48,2016-07-20 01:34:32
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1374,https://api.github.com/repos/kubernetes/contrib/issues/1374,Simplify size calculation,"I found an optimization in the way we compute the size of a pull-request.

Please let me know if this makes sense as it's pretty much untested (sorry, not sure how to test easily).
This depends on a commit common with #1373  
",closed,True,2016-07-15 20:59:08,2016-07-18 22:04:08
contrib,foxish,https://github.com/kubernetes/contrib/pull/1375,https://api.github.com/repos/kubernetes/contrib/issues/1375,Added new feature to handle test options.,"- The required-retest-contexts are used by several mungers. Previously, it was [a global variable](https://github.com/kubernetes/contrib/issues/1304#issuecomment-231817807) which would get referenced irrespective of the parameters supplied via the command-line.
- We abstract the retest-context flag out as a feature that the mungers that want to can access them.
- In future, if multiple mungers start to use any other test related parameters, they could be added into the test-options feature. 
",closed,True,2016-07-15 21:46:51,2016-07-18 22:07:38
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1376,https://api.github.com/repos/kubernetes/contrib/issues/1376,Add close-stale-pr to munger list,"Makefile has a list of mungers to run, and this one has been omitted.
",closed,True,2016-07-15 22:23:20,2016-07-18 21:57:17
contrib,osxi,https://github.com/kubernetes/contrib/pull/1377,https://api.github.com/repos/kubernetes/contrib/issues/1377,Cluster-autoscaler: cloud provider interface implementation for AWS,"Part of https://github.com/kubernetes/contrib/issues/1311

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1377)

<!-- Reviewable:end -->
",closed,True,2016-07-15 22:30:54,2016-08-18 08:46:35
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1378,https://api.github.com/repos/kubernetes/contrib/issues/1378,WIP: [nginx-ingress-controller] improve nginx performance,"Changing container /proc values without the need of `--privileged`
",closed,True,2016-07-15 23:09:38,2016-07-16 00:32:34
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1379,https://api.github.com/repos/kubernetes/contrib/issues/1379,[nginx-ingress-controller] improve nginx performance,"Changing container `/proc` values with a privileged sidecar.
NGINX reads `net.core.somaxconn` to increase the size of backlog queue of pending connections
",closed,True,2016-07-16 15:27:56,2016-07-20 20:00:36
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1380,https://api.github.com/repos/kubernetes/contrib/issues/1380,Introduce etcd playbook,"Following effort in https://github.com/kubernetes/contrib/pull/967, this PR introduces `etcd-setup` playbook. Changes:
- introduce localhost inventory
- merge `etcd_modified` and `etcd2_modified`, update `restart etcd` task so it can be run as self-standing task
",closed,True,2016-07-16 15:51:58,2016-08-12 16:15:46
contrib,jcmoraisjr,https://github.com/kubernetes/contrib/issues/1381,https://api.github.com/repos/kubernetes/contrib/issues/1381,[service-loadbalancer] fixes and improvements,"I've implemented a couple of fixes and improvements on service loadbalancer in order to use it on our company. Have a look at [this compare](https://github.com/kubernetes/contrib/compare/master...jcmoraisjr:features).

Let me know in what commits you are interested and I'll create the PRs.
",closed,False,2016-07-16 20:46:36,2017-12-17 14:21:37
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1382,https://api.github.com/repos/kubernetes/contrib/issues/1382,Extend README.md with troubleshooting,"Add inventory example, fix a type and introduce troubleshooting section for known issues.

Fixes: #1368
",closed,True,2016-07-17 10:43:49,2016-07-17 16:56:26
contrib,atombender,https://github.com/kubernetes/contrib/issues/1383,https://api.github.com/repos/kubernetes/contrib/issues/1383,[nginx-ingress-controller] Basic auth header is not stripped,"If you use basic auth in an ingress, then the `Authorization` header is passed along to the proxied upstream, which can result in incorrect behaviour if the upstream doesn't expect one.

For example, [Drone](https://github.com/drone/drone) will ignore its session cookie if the auth header is specified.

The correct behaviour is to strip the header and not pass it to the upstream.

Until this is fixed, I've been trying to find a workaround by injecting a custom Nginx directive, but I can't a way. Is there one?
",closed,False,2016-07-18 01:28:43,2018-12-11 16:28:13
contrib,atombender,https://github.com/kubernetes/contrib/pull/1384,https://api.github.com/repos/kubernetes/contrib/issues/1384,Unset Authorization header when proxying,"Fixes #1383.
",closed,True,2016-07-18 02:34:01,2016-07-20 17:54:58
contrib,foxish,https://github.com/kubernetes/contrib/pull/1385,https://api.github.com/repos/kubernetes/contrib/issues/1385,Removing defaults from code and making command-line options explicit in deployment.yaml.,"Continuing work on #1304. 

We have a large number of defaults specified for various flags according to the main kubernetes repository. 
The defaults don't make sense as we make mungegithub work with other repositories.
- The first commit adds logging to print the values of various command-line parameters.
- The second removes defaults and includes them in the deployment.yaml file. 
",closed,True,2016-07-18 07:25:32,2016-07-21 21:17:43
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1386,https://api.github.com/repos/kubernetes/contrib/issues/1386,Remove non-blocking builds stability % from SQ,"The stability of non-blocking builds is always 100% because they are
never blocking the submit-queue. As this is useless, let's just not
display it.
",closed,True,2016-07-18 18:52:31,2016-07-18 21:56:52
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1387,https://api.github.com/repos/kubernetes/contrib/issues/1387,Run close-stale-pr in mungegithub,"Fixed deployment.yaml (and removed it from Makefile where it is not used ...)
",closed,True,2016-07-18 22:35:53,2016-07-20 02:25:52
contrib,apelisse,https://github.com/kubernetes/contrib/issues/1388,https://api.github.com/repos/kubernetes/contrib/issues/1388,Makefile has MUNGERS and mungegithub/submit-queue/deployment.yaml duplicates the list,"In `mungegithub/Makefile` there is a list of all mungers in `MUNGERS`.
The same list is duplicated in `mungegithub/submique-queue/deployment.yaml` making things kind of confusing.

We should probably find a way to DRY.
",closed,False,2016-07-18 22:39:30,2017-01-21 01:41:17
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1389,https://api.github.com/repos/kubernetes/contrib/issues/1389,Finish OWNERS implementation,"Described here:
https://github.com/kubernetes/contrib/pull/474#issuecomment-226249631

cc @eparis @davidopp @philips @mike-saparov @hongchaodeng @kubernetes/contributor-experience 

@philips Please add your suggestions here.",closed,False,2016-07-19 01:16:49,2017-02-10 17:30:29
contrib,timstclair,https://github.com/kubernetes/contrib/issues/1390,https://api.github.com/repos/kubernetes/contrib/issues/1390,Munger to echo labels in comments,"Github doesn't include labels anywhere in email notifications, which can make it hard to prioritize Kubernetes issues by email.

Until github adds this feature, I propose we have the k8s-bot echo label additions or removals in a comment, in a format that's easily parsed by email filters.

Would this feature be welcome, or too spammy?

/cc @matchstick 
",closed,False,2016-07-19 18:20:13,2018-02-15 20:43:03
contrib,nikhiljindal,https://github.com/kubernetes/contrib/issues/1391,https://api.github.com/repos/kubernetes/contrib/issues/1391,Mergebot merged a PR that had needs-rebase label,"https://github.com/kubernetes/kubernetes/pull/26298 was merged by mergebot even though it had the needs-rebase label.

cc @k8s-oncall as FYI

cc @lavalamp @eparis 
",closed,False,2016-07-19 19:14:03,2016-08-12 00:27:41
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1392,https://api.github.com/repos/kubernetes/contrib/issues/1392,[keepalived-vip]: Update keepalived,,closed,True,2016-07-19 20:07:56,2016-07-25 20:20:25
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1393,https://api.github.com/repos/kubernetes/contrib/issues/1393,[nginx-ingress-controller] Release 0.8.2,,closed,True,2016-07-19 20:17:15,2016-07-25 20:27:59
contrib,foxish,https://github.com/kubernetes/contrib/pull/1394,https://api.github.com/repos/kubernetes/contrib/issues/1394,Removing the use of gitrepo volume.,"Issue #1304 

The gitrepo volume is difficult to parameterize as we move towards configmaps to deploy mungegithub to different repositories. This change uses an emptyDir instead for the /gitrepos mount, and moves the repository cloning logic to within mungegithub.
",closed,True,2016-07-19 21:24:31,2016-07-22 20:58:01
contrib,foxish,https://github.com/kubernetes/contrib/pull/1395,https://api.github.com/repos/kubernetes/contrib/issues/1395,Deploying with configmaps,"Issue #1304 

This is the proposed structure and flow of how we will deploy mungegithub to multiple repositories with configmaps. The directory structure of contrib/mungegithub/submit-queue/ will look like the following:

```
.
├── deployment
│   ├── k8s-contrib
│   │   ├── configmap.yaml
│   │   ├── pvc.yaml
│   │   ├── pv.yaml
│   │   ├── secret.yaml
│   │   └── service.yaml
│   ├── k8s-kubernetes
│   │   ├── configmap.yaml
│   │   ├── pvc.yaml
│   │   ├── pv.yaml
│   │   ├── secret.yaml
│   │   └── service.yaml
│   └── k8s-docs
│   │   ├── configmap.yaml
│   │   ├── pvc.yaml
│   │   ├── pv.yaml
│   │   ├── secret.yaml
│   │   └── service.yaml
├── deployment.yaml
└── www
```

The makefile's deploy target would take an additional argument, the `CONFIGMAP`.  The deployment command for example, for k8s-kubernetes, the main repository like:

```
REPO=<> APP=submit-queue CONFIGMAP=k8s-kubernetes make deploy 
```

make deploy will work after one:
- creates the corresponding secret.
- creates PVC & PV if necessary.
",closed,True,2016-07-19 22:48:38,2016-07-30 16:09:47
contrib,andreassolberg,https://github.com/kubernetes/contrib/issues/1396,https://api.github.com/repos/kubernetes/contrib/issues/1396,GLBC - Issues about effectively replace / updating a certificate,"I've successfully managed to setup an ingress with a certificate. However, I have not yet found a way to reliably replace the certificate on the ingress.

The documentation at:
- https://github.com/kubernetes/contrib/tree/master/ingress/controllers/gce

does not mention updating or replacing certificates. I would suggest that a section is added to cover this.

I expected that when I replace the cert secret, and applied an replacement to the ingress object with a minor change (in example a route change), it would fetch the updated certs. This did not seem to happen.

Deleting the ingress, and recreating it did partly work, but creates severe downtime. I think that when creating the ingress with the same name, the old cert is cached/user. 

Right now, I need to deploy a new ingress with a new ID in parallell, getting a new IP, and switch in DNS, before phasing out the old one. Very cumbersome.
",closed,False,2016-07-20 06:13:04,2017-04-04 17:20:52
contrib,UTSLibraryITS,https://github.com/kubernetes/contrib/issues/1397,https://api.github.com/repos/kubernetes/contrib/issues/1397,Exechealthz unable to resolve DNS when running as a Kubernetes Pod,"The **gcr.io/google_containers/exechealthz** container uses Busybox as it's base image. The Busybox image is unable to perform DNS resolution when running in a Kubernetes pod.

For example, the KubeDNS add on uses exechealthz `nslookup` command as a health check. This does not work.

You can recreate the issue by using the example pod: https://github.com/kubernetes/contrib/blob/master/exec-healthz/pod.json

The same issue occurs when using busybox directly (_pod definition below_):

`# kubectl create -f pod.json`

```
pod ""simple"" created
```

`# kubectl exec simple -c test-container nslookup github.com 8.8.8.8`

```
Server:    8.8.8.8
Address 1: 8.8.8.8

nslookup: can't resolve 'github.com'
error: error executing remote command: Error executing command in container: Error executing in Docker Container: 1
```

`# kubectl exec simple -c healthz nslookup github.com 8.8.8.8`

```
Server:    8.8.8.8
Address 1: 8.8.8.8

nslookup: can't resolve 'github.com'
error: error executing remote command: Error executing command in container: Error executing in Docker Container: 1
```

`# kubectl exec  simple -c healthz wget github.com`

```
wget: bad address 'github.com'
error: error executing remote command: Error executing command in container: Error executing in Docker Container: 1
```

---
### pod.json

```
{
  ""kind"": ""Pod"",
  ""apiVersion"": ""v1"",
  ""metadata"": {
    ""name"": ""simple""
  },
  ""spec"": {
    ""containers"": [
      {
        ""name"": ""healthz"",
        ""image"": ""gcr.io/google_containers/exechealthz:1.1"",
        ""args"": [
          ""-cmd=nslookup localhost""
        ],
        ""ports"": [
          {
            ""containerPort"": 8080,
            ""protocol"": ""TCP""
          }
        ]
      },
      {
        ""name"":""test-container"",
        ""image"":""busybox"",
        ""command"": [""sh"", ""-c"", ""while true; do sleep 100; done""],
        ""livenessProbe"": {
          ""httpGet"": {
            ""path"": ""/healthz"",
            ""port"":8080
          },
          ""initialDelaySeconds"": 10,
          ""timeoutSeconds"": 2
        }
      }
    ]
  }
}
```
",closed,False,2016-07-20 08:05:49,2018-02-14 15:15:11
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1398,https://api.github.com/repos/kubernetes/contrib/issues/1398,Mitigate HTTPoxy Vulnerability,"https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
",closed,True,2016-07-20 13:56:10,2016-07-20 17:56:17
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/pull/1399,https://api.github.com/repos/kubernetes/contrib/issues/1399,[cluster-autoscaler] Add leader election to cluster autoscaler,"Please note that this changes how we can pass flags to CA. In particular `-v` is incorrect and should be `--v`. Command lines in kubernetes scripts will have to be updated when releasing CA.

https://github.com/kubernetes/kubernetes/issues/21124

@jszczepkowski 
",closed,True,2016-07-20 14:29:11,2016-07-21 14:20:49
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1400,https://api.github.com/repos/kubernetes/contrib/issues/1400,Cluster-autoscaler: shuffle nodes in scale down,"So that candidate nodes use different nodes to move their pods. So that removal of one nodes doesn't reset utilization counters on all other pods. Please take a look at #1341 #1370 for more context. 

cc: @piosz @fgrzadkowski @jszczepkowski 
",closed,True,2016-07-20 15:04:35,2016-07-21 13:29:49
contrib,gambol99,https://github.com/kubernetes/contrib/pull/1401,https://api.github.com/repos/kubernetes/contrib/issues/1401,default tls option ,"- adding an extra command line option -default-tls which is a reference to a k8s secret which will be used if no secretName name is defined in the ingress resource. This permits us to reuse certificates from one namespace across others. A use case being keeping a wildcard certificate in one namespace, but allow others to use it.
- shifting the tag to version 0.8.2
- fixed the bug in the ssl.go, should not use temporaryPemFile.Name() on error, it will be nil
",closed,True,2016-07-20 15:36:04,2016-07-20 16:32:28
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1402,https://api.github.com/repos/kubernetes/contrib/issues/1402,Improve close pr,"- Improve text
- Add mentions
- Add expected close date
- Remove stale comments (after PR becomes active again) 
",closed,True,2016-07-20 16:51:36,2016-07-21 22:05:29
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1403,https://api.github.com/repos/kubernetes/contrib/issues/1403,Activate skip-retest-for-docs in mungegithub,"Really activate this time ...
",closed,True,2016-07-20 17:05:26,2016-07-22 16:54:57
contrib,gambol99,https://github.com/kubernetes/contrib/pull/1404,https://api.github.com/repos/kubernetes/contrib/issues/1404,cross namespace tls secrets,"- adding an extra commad line option --cross-permitted which is a comma separated list of cross namespace secrets a ingress resource can request. This permits us to reuse certificates from one namespace across others. A use case being keeping a wildcard certificate in one namespace, but allow others to use it.
- shifting the tag to version 0.8.2
- fixed the bug in the ssl.go, should not use temporaryPemFile.Name() on error, it will be nil

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1404)

<!-- Reviewable:end -->
",closed,True,2016-07-20 17:37:08,2016-08-23 08:22:45
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1405,https://api.github.com/repos/kubernetes/contrib/issues/1405,mungegithub should manage CLA labels,"Similar to the Google bot.

After we switch to the CNCF CLA.

Example using CNCF CLA (on a cncf repo):
https://github.com/cncf/demo/pull/7

It would probably be easier than running a separate bot.
",closed,False,2016-07-20 17:37:59,2016-11-29 04:53:33
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1406,https://api.github.com/repos/kubernetes/contrib/issues/1406,Optimize github rate-limit quota usage of bot,"Forked from #1390

Github says we should be using webhooks instead of polling.

cc @lavalamp @kubernetes/contributor-experience 
",closed,False,2016-07-20 17:54:52,2017-01-21 01:17:57
contrib,AdrianGPrado,https://github.com/kubernetes/contrib/issues/1407,https://api.github.com/repos/kubernetes/contrib/issues/1407,[Ansible] sudo depreciation warning,"The current playbook uses the ansible module `sudo` which is depreciated and will be removed in future releases. This causes a warning and it will be an error in the future.

```
[DEPRECATION WARNING]: Instead of sudo/sudo_user, use become/become_user and make sure become_method is 'sudo' (default). This feature will be removed in a future release. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.
```

Instead of sudo use the ansible module [become](http://docs.ansible.com/ansible/become.html#become).
",closed,False,2016-07-20 18:26:22,2016-07-31 16:29:15
contrib,AdrianGPrado,https://github.com/kubernetes/contrib/pull/1408,https://api.github.com/repos/kubernetes/contrib/issues/1408,[Ansible] #1407 sudo depreciation warning: sudo -> become,"```
[DEPRECATION WARNING]: Instead of sudo/sudo_user, use become/become_user and make sure become_method is 'sudo' (default). This feature will be removed in a future release. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.
```

Usage example from Ansible documentation:

```
- name: Ensure the httpd service is running
  service:
     name: httpd
    state: started
  become: true
```

Ansible [become module](http://docs.ansible.com/ansible/become.html#become) default user is `root`

Fixes: #1407 
",closed,True,2016-07-20 18:32:44,2016-08-01 18:33:34
contrib,imsky,https://github.com/kubernetes/contrib/issues/1409,https://api.github.com/repos/kubernetes/contrib/issues/1409,"[Ansible] option to keep easyrsa ""context directory""","it would be useful to keep the easyrsa directory (https://github.com/kubernetes/contrib/blob/master/ansible/roles/kubernetes/files/make-ca-cert.sh#L74) around on the client side in order to generate additional client certificates.
",closed,False,2016-07-20 18:33:43,2017-11-15 03:08:26
contrib,imsky,https://github.com/kubernetes/contrib/issues/1410,https://api.github.com/repos/kubernetes/contrib/issues/1410,[Ansible] FLANNEL_ETCD needs to reference internal master IP,"i periodically get timeouts when setting up Flannel on AWS since the etcd host is inaccessible.

i use a dev machine, so i reference the external IPs in inventory, however the etcd config advertises on the internal IPs, so Flannel is unable to reach the etcd cluster.

i'm not sure if it is simple to create an automated workaround, but this should be referenced in docs.
",closed,False,2016-07-20 18:59:46,2017-11-15 03:08:19
contrib,foxish,https://github.com/kubernetes/contrib/issues/1411,https://api.github.com/repos/kubernetes/contrib/issues/1411,[mungegithub] Deployment of cherrypick from the Makefile does not work. ,"When we try something along the lines of:
`REPO=docker.io/eparis APP=cherrypick KUBECONFIG=/path/to/kubeconfig make deploy`
The image name is not updated when creating cherrypick/local.deployment.yaml.

This may be because mungegithub/cherrypick/deployment.yaml points to `docker.io/eparis/cherrypick:2016-03-14-7fb1dae`, instead of a `gcr.io` image.
",closed,False,2016-07-20 19:09:52,2016-08-05 16:35:46
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1412,https://api.github.com/repos/kubernetes/contrib/issues/1412,Update go-github for mungegithub,"Update to a new version, that will allow us to use Assignee**s**.
Because go-github made breaking changes to their API, the diff is kind of ugly. I've tried to carefully replace everywhere (thanks to go compiler). `make test` is passing for me.
",closed,True,2016-07-20 19:28:53,2016-07-21 21:23:36
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1413,https://api.github.com/repos/kubernetes/contrib/issues/1413,Active PR status dashboard,"Script that generates a text version:

```
#!/bin/zsh -f

GITHUB_USER=you
# Generate a token here: https://github.com/settings/tokens
GITHUB_TOKEN=XXX

curl() {
  command curl -u ${GITHUB_USER}:${GITHUB_TOKEN} ""$@""
}

mkdir -p ~/.github-cache
cd ~/.github-cache

# -F means re-fetch everything
# -p means just re-fetch the per-PR files (not reliable)
while getopts Fp flag; do
  case $flag in
    F)
      refetch=y
      ;;
    p)
      fetchprs=y
      ;;
  esac
done

shift $(( OPTIND - 1 ))

if [[ ""$refetch"" == y ]] {
  rm pulls?.json
  url='https://api.github.com/repos/kubernetes/kubernetes/pulls?per_page=100'
  i=1

  echo -n ""Fetching $url\r""

  while curl -s -D headers ""$url"" > pulls${i}.json; do
    next=$(grep -Po 'Link: .*?<\Khttps:[^>]+(?=>;\s*rel=""next"")' headers)
    if [[ -z ""$next"" ]]; then
      break
    fi
    i=$(( i + 1 ))
    url=""${next}""
    echo -n ""Fetching $url\r""
  done
  echo
  echo Got the list
}


jq "".[] | select(.assignee.login==\""${GITHUB_USER}\"" or .user.login==\""${GITHUB_USER}\"")"" pulls*.json > ${GITHUB_USER}-pulls.json

n=$(jq -r .number ${GITHUB_USER}-pulls.json | wc -l)
m=1

if [[ ""$refetch"" = y || ""$fetchprs"" = y ]] {
  jq -r '""\(.number) \(.url)""' ${GITHUB_USER}-pulls.json | while read number url; do
    echo -n ""[$m/$n] Fetching $number ........ $url\r""
    curl -s ""$url"" > ""$number"".json
    echo -n ""[$m/$n] Fetching $number labels\r""
    curl -s ""https://api.github.com/repos/kubernetes/kubernetes/issues/$number/labels"" > ""${number}-labels.json""
    echo -n ""[$m/$n] Fetching $number comments\r""
    curl -s $(jq -r .comments_url ""$number.json"") > ""${number}-comments.json""
    echo -n ""[$((m++ ))/$n] Fetching $number statuses\r""
    curl -s $(jq -r .statuses_url ""$number.json"") > ""${number}-statuses.json""
  done
  echo 
  echo Got the PRs
}

(
  echo ""AUTHOR,UPDATED,BY,DESCRIPTION,LGTM?,STATE,MSG,NUMBER""
  for pr in $(jq -r -s 'sort_by(.updated_at) | reverse | .[] | .number' ${GITHUB_USER}-pulls.json); do
    echo -n ""$(jq -r '""\(.user.login)""' < ${pr}.json),""
    echo -n ""$(./foo.py $(jq -r .updated_at < ${pr}.json)),""
    echo -n ""$(jq -r 'sort_by(.updated_at) | .[] | .user.login' ${pr}-comments.json | tail -1),""
    echo -n ""$(jq -r .title < ${pr}.json),""
    echo -n ""$(jq -r 'map(select(.name == ""lgtm"")) | .[] | .name' < ${pr}-labels.json) ""
    echo -n ""$(jq -r 'if .mergeable == false then ""nope"" else empty end' < ${pr}.json),""
    echo -n ""$(jq -r 'map(select(.context==""Submit Queue"")) | sort_by(.updated_at) | reverse | .[] | (.state,.description)' ${pr}-statuses.json | head -2 | tr '\n,' ',;')""
    echo -n ""#${pr}""
    echo
  done
) | column -s, -t
```

It sounds like @rmmh may be working on something similar.
",closed,False,2016-07-20 20:12:24,2016-09-29 06:24:04
contrib,foxish,https://github.com/kubernetes/contrib/pull/1414,https://api.github.com/repos/kubernetes/contrib/issues/1414,Parameterized submit-queue health/status pages.,"Issue #1304 

Mungegithub generates several status pages showing queue health and PR merge queue status.
The URLs in these pages, as well as some contents need to be parameterizable as we run against other repositories.
",closed,True,2016-07-21 02:32:59,2016-07-22 20:54:08
contrib,foxish,https://github.com/kubernetes/contrib/issues/1415,https://api.github.com/repos/kubernetes/contrib/issues/1415,[mungegithub] Modify mungegithub to use config files instead of command-line arguments.,"The number of command-line arguments is getting large and unwieldy. We should switch to reading config files instead.
It would also let us group these command-line options into logical sub-units which is currently not possible.
",closed,False,2016-07-21 03:07:10,2017-01-21 01:40:59
contrib,madhusudancs,https://github.com/kubernetes/contrib/pull/1416,https://api.github.com/repos/kubernetes/contrib/issues/1416,Add a Makefile for building kubeform.,"Also, fix the import path.

Please review only the last commit here. This PR depends on #1366.
",closed,True,2016-07-21 05:09:08,2016-07-21 21:23:55
contrib,foxish,https://github.com/kubernetes/contrib/pull/1417,https://api.github.com/repos/kubernetes/contrib/issues/1417,Added more explicit instructions to locally deploy/test mungegithub,"- Added some instructions that I felt were missing based on my experience running it locally.
- Reworded some of the build instructions.
",closed,True,2016-07-21 06:22:39,2016-08-01 18:23:10
contrib,piosz,https://github.com/kubernetes/contrib/pull/1418,https://api.github.com/repos/kubernetes/contrib/issues/1418,[CA] Moved listers to util package,"This is required to make them usable from outside of the project.
",closed,True,2016-07-22 08:37:40,2016-07-22 09:22:50
contrib,piosz,https://github.com/kubernetes/contrib/pull/1419,https://api.github.com/repos/kubernetes/contrib/issues/1419,Initial implementation of rescheduler,"ref https://github.com/kubernetes/kubernetes/issues/29023
",closed,True,2016-07-22 10:17:58,2016-07-22 12:29:01
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1420,https://api.github.com/repos/kubernetes/contrib/issues/1420,mungegithub: Fix comment regex to NOT repost,"The regex to find that we have already added a warning is broken
(because of a change in punctuation). This is causing the mungebot to
repost the warning every time.

Fixing the regex will remove previous comments and prevent the bot from
posting too much.
",closed,True,2016-07-22 17:18:35,2016-07-22 17:20:22
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1421,https://api.github.com/repos/kubernetes/contrib/issues/1421,Remove RegisterStaleComments as it is not used,"Minor nit, removes useless code.
",closed,True,2016-07-22 20:24:55,2016-08-04 22:04:46
contrib,foxish,https://github.com/kubernetes/contrib/issues/1422,https://api.github.com/repos/kubernetes/contrib/issues/1422,[mungegithub] ping inactive reviewers and reassign,"Formerly part of issue #869 
- Ping the PR due to lack of inactivity, notifying either the PR author or the assignee, depending on who last touched the PR (keep track of whose court it's in).
- Create a way to postpone the next ping for a fixed amount of time (day, week, month, next release cycle).
- Reassign PRs whose assignees never responded.

Additionally:
- label stale PRs.
",closed,False,2016-07-22 21:00:13,2017-01-21 01:37:23
contrib,foxish,https://github.com/kubernetes/contrib/issues/1423,https://api.github.com/repos/kubernetes/contrib/issues/1423,[mungegithub] Add munger to create and maintain labels on a repository,"- Take a YAML file stored in the repository and create labels on the repository.
- Add labels to repository as the file is updated.
",closed,False,2016-07-22 21:55:40,2016-08-05 16:33:34
contrib,rutsky,https://github.com/kubernetes/contrib/pull/1424,https://api.github.com/repos/kubernetes/contrib/issues/1424,always run Atomic determination test in Ansible,"Similar to how `always_run` is specified for commands above in the file.

Without this `ansible-playbook` fails if being run with in
[check mode](https://docs.ansible.com/ansible/playbooks_checkmode.html):

```
$ ansible-playbook -i inventory deploy-cluster.yml -t pre-ansible -DC -v
Using /home/bob/stuff/kubernetes/contrib/ansible/ansible.cfg as config file
[DEPRECATION WARNING]: Instead of sudo/sudo_user, use become/become_user and make sure become_method is 'sudo' (default).
This feature will be
removed in a future release. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.

PLAY [all] *********************************************************************

TASK [pre-ansible : Get os_version from /etc/os-release] ***********************
ok: [bob] => {""changed"": false, ""rc"": 0, ""stderr"": """", ""stdout"": ""\""16.04\""\r\n"", ""stdout_lines"": [""\""16.04\""""]}

TASK [pre-ansible : Get distro name from /etc/os-release] **********************
ok: [bob] => {""changed"": false, ""rc"": 0, ""stderr"": """", ""stdout"": ""\""Ubuntu\""\r\n"", ""stdout_lines"": [""\""Ubuntu\""""]}

TASK [pre-ansible : Init the is_coreos fact] ***********************************
ok: [bob] => {""ansible_facts"": {""is_coreos"": false}, ""changed"": false}

TASK [pre-ansible : Set the is_coreos fact] ************************************
skipping: [bob] => {""changed"": false, ""skip_reason"": ""Conditional check failed"", ""skipped"": true}

TASK [pre-ansible : Set the bin directory path for CoreOS] *********************
skipping: [bob] => {""changed"": false, ""skip_reason"": ""Conditional check failed"", ""skipped"": true}

TASK [pre-ansible : include] ***************************************************
skipping: [bob] => {""changed"": false, ""skip_reason"": ""Conditional check failed"", ""skipped"": true}

TASK [pre-ansible : Determine if Atomic] ***************************************
skipping: [bob] => {""changed"": false, ""skipped"": true}

TASK [pre-ansible : Set the is_atomic fact] ************************************
fatal: [bob]: FAILED! => {""failed"": true, ""msg"": ""The conditional check 's.rc == 0' failed. The error was: error while evaluating conditional (s.r
c == 0): 'dict object' has no attribute 'rc'\n\nThe error appears to have been in '/home/bob/stuff/kubernetes/contrib/ansible/roles/pre-ansible/ta
sks/main.yml': line 36, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\
n\n- name: Set the is_atomic fact\n  ^ here\n""}

NO MORE HOSTS LEFT *************************************************************
        to retry, use: --limit @deploy-cluster.retry

PLAY RECAP *********************************************************************
bob                        : ok=4    changed=0    unreachable=0    failed=1
```

In check mode Ansible skips potentially mutating `raw` command from execution,
which leads to not properly initialized `s` variable with a following failure
of the next command that tries to check `s.rc` value.

Tested on Ubuntu 16.04 with Ansible 2.1.0.0.
",closed,True,2016-07-22 23:41:30,2016-07-25 15:58:44
contrib,madhusudancs,https://github.com/kubernetes/contrib/pull/1425,https://api.github.com/repos/kubernetes/contrib/issues/1425,Return the right error message when kubeconfig update fails.,,closed,True,2016-07-23 01:19:13,2016-07-25 23:00:39
contrib,madhusudancs,https://github.com/kubernetes/contrib/pull/1426,https://api.github.com/repos/kubernetes/contrib/issues/1426,Add kubeconfig delete support to kubeform.,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1426)

<!-- Reviewable:end -->
",closed,True,2016-07-23 01:27:48,2018-01-22 17:11:33
contrib,girishkalele,https://github.com/kubernetes/contrib/pull/1427,https://api.github.com/repos/kubernetes/contrib/issues/1427,[WIP] DNS nanny - Autoscale replication controller based on number of nodes,"Adaption of the addon-resizer nanny worked out pretty well.
",closed,True,2016-07-23 03:55:55,2016-08-09 21:55:08
contrib,hongchaodeng,https://github.com/kubernetes/contrib/pull/1428,https://api.github.com/repos/kubernetes/contrib/issues/1428,lgtm handler: find lgtm comment and add label,"ref: #1389

This PR replaces LGTMAfterCommit munger with LGTMHandler that

```
// LGTMHandler will
// - apply the LGTM label if reviewer has commented ""LGTM, or
// - notify reviewer if non-reviewer has commented ""LGTM"", or
// - remove the LGTM label from an PR which has been updated since the reviewer added LGTM
```
",closed,True,2016-07-23 18:08:22,2016-08-18 03:02:58
contrib,hongchaodeng,https://github.com/kubernetes/contrib/pull/1429,https://api.github.com/repos/kubernetes/contrib/issues/1429,[WIP] Approval munger:,"Depends on: #1428

This PR adds approval munger which checks if an PR has LGTM and approver said ""approved"". The munger would apply the approval label which should be used by submit queue bot.

TODO:
- Define approver and implement it (blocked on this)
- Notify an approver.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1429)

<!-- Reviewable:end -->
",closed,True,2016-07-23 21:37:13,2016-09-13 23:23:14
contrib,foxish,https://github.com/kubernetes/contrib/issues/1430,https://api.github.com/repos/kubernetes/contrib/issues/1430,[mungegithub] Cherrypick application logging several errors,"When inspecting stdout/err for cherrypick, one sees several messages of the kind:

```
E0724 03:49:47.562869       1 cherrypick-clear-after-merge.go:76] Unable to find the fork point for branch release-1.4. fatal: Not a valid object name origin/release-1.4
```
",closed,False,2016-07-24 03:51:38,2017-01-21 01:41:52
contrib,gregory-lyons,https://github.com/kubernetes/contrib/pull/1431,https://api.github.com/repos/kubernetes/contrib/issues/1431,Add support for SSH in git-sync,"This PR for git-sync adds the ability to use SSH for pulling git content. 

**Implementation Notes:**
- Running as a sidecar container, git-sync needs to avoid requiring the user to confirm use of the SSH key, so we override the /usr/bin/ssh binary with our own script (ssh-wrapper.sh) that has the necessary flags, and git then uses that script by default.
- Currently, secret files are always mounted with permission 0444. This is not restrictive enough to be used as an SSH key, so we chmod the mounted secret file to 0400. https://github.com/kubernetes/kubernetes/pull/28936 is in progress to allow for explicitly setting the permissions of a secret, and that can be incorporated into a future version of git-sync to avoid the chmod command.
- Setting the mounted secret volume with readOnly: true (as suggested in the example here: http://kubernetes.io/docs/user-guide/secrets/#use-case-pod-with-ssh-keys) actually makes it impossible to use the secret as an SSH key. As mentioned above, the secret will be mounted as 0444, but the read-only nature prevents running the necessary chmod command. I've included a warning about this in ssh.md and in the error message if the chmod fails. Again, the PR above should allow for avoiding this issue.

There are more detailed instructions for usage in the ssh.md readme, which I can move into the main README.md if that is preferred.
",closed,True,2016-07-25 09:09:10,2016-07-30 16:54:26
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1432,https://api.github.com/repos/kubernetes/contrib/issues/1432,[mungegithub]  Create Munger To Automatically Add Labels To Issues,"Issues currently require manual labeling for routing.   Let's automate the process with some simple machine learning.  
",closed,False,2016-07-25 15:36:28,2016-08-03 21:04:40
contrib,foxish,https://github.com/kubernetes/contrib/pull/1433,https://api.github.com/repos/kubernetes/contrib/issues/1433,Fixed crashing submit-queue when container restarts.,"While emptyDir presents an empty directory when a pod is killed and recreated by the deployment, it retains contents between container
restarts. This change guards against that scenario by not cloning the repository when a `PROJECT_NAME` directory already exists.
",closed,True,2016-07-25 16:30:09,2016-07-25 21:15:05
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1434,https://api.github.com/repos/kubernetes/contrib/issues/1434,Assign test owners to default as a fallback. Pick a random owner if there are multiple options.,"See also: kubernetes/kubernetes#29564
",closed,True,2016-07-25 20:38:27,2016-08-02 22:50:18
contrib,mml,https://github.com/kubernetes/contrib/pull/1435,https://api.github.com/repos/kubernetes/contrib/issues/1435,Delete dead code in cluster_manager.,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1435)

<!-- Reviewable:end -->
",closed,True,2016-07-25 22:53:16,2016-08-12 21:13:33
contrib,foxish,https://github.com/kubernetes/contrib/pull/1436,https://api.github.com/repos/kubernetes/contrib/issues/1436,Deployment of mungegithub using ConfigMaps on the kubernetes main repository,"https://github.com/kubernetes/contrib/pull/1395 is split in two for easier testing.
This is the first part, which updates the configmap, service, secret, pv, pvc and deployment for the main repo.

Before we use this to deploy a new submit-queue, we need to do the following on the utility cluster:
- Create a new secret named k8s-kubernetes-github-token
- Create a new PV & PVC named k8s-kubernetes-cache

cc @lavalamp @apelisse @pwittrock 
",closed,True,2016-07-26 00:10:02,2016-07-29 21:07:45
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1437,https://api.github.com/repos/kubernetes/contrib/issues/1437,Add automatic issue labeler,"Resolves #1432

The simple_app.py file loads 2 pretrained machine learning models that have learned patterns on existing issues.  For new issues, the munger file will send a request to the webapp with the new issue's title and body, and the webapp will respond with the team and component labels that it thinks are appropriate. 
",closed,True,2016-07-26 00:26:42,2016-08-03 21:04:40
contrib,foxish,https://github.com/kubernetes/contrib/pull/1438,https://api.github.com/repos/kubernetes/contrib/issues/1438,OWNERS files for each directory under contrib,"Used a modified version of https://github.com/coreos/findowner (at https://github.com/foxish/findowner/tree/foxish-edits) to generate the YAML files.
It populates the top 3 committers over the past 6 months as potential assignees. 

PTAL if the lists make sense and we can get blunderbuss running on contrib.
cc @lavalamp @bgrant0607 @apelisse @pwittrock 
",closed,True,2016-07-26 07:12:29,2016-08-01 21:16:18
contrib,goltermann,https://github.com/kubernetes/contrib/issues/1439,https://api.github.com/repos/kubernetes/contrib/issues/1439,Script or automation to ping list of GitHub issues,"Sometimes we want to manually ping a set of issues with the same text - something ""this is a v1.3 bug, only 1w left in that release.""  That's not something we would fully automate, since the list and reason will vary over time.

Having a script or even UI where I could put
1a - list of GitHub IDs (or)
1b - GitHub issue query
2 - text to be put in each issue
and have it auto-ping all those issues could be useful for burning down bug lists.
",closed,False,2016-07-27 14:27:06,2018-02-14 17:17:10
contrib,chiradeep,https://github.com/kubernetes/contrib/pull/1440,https://api.github.com/repos/kubernetes/contrib/issues/1440,Provide an Ingress Controller that use Citrix NetScaler,"This contribution provides an Ingress Controller that can configure a Citrix NetScaler.
",closed,True,2016-07-27 19:04:23,2016-07-30 00:08:27
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1441,https://api.github.com/repos/kubernetes/contrib/issues/1441,Move ingress to a new repo,"I've been meaning to split ingress off into its own repo but haven't had time. All ingress controllers are already modular enough to run in their own containers, and already have Godeps in the contrib/ingress/_vendor directory. 

Currently we get some tooling for free in contrib (persubmit verification, munging etc) that would be nice to continue to have. It would also be good to nail down a concrete release and test plan. The latter doesn't block going to a new repo, but if we don't do it now I fear we'll just never do it. 

The release plan right now is: whenever we feel like, directly in contrib/releases, and the test plan right now is: e2e in main kubernetes repo. Ideally we should at least sketch out a proposal to run e2es with ingress ToT in addition to the e2es in main repo, just like we do for cadvisor. 

https://github.com/kubernetes/contrib/issues/762
",closed,False,2016-07-27 22:56:22,2018-02-16 09:56:05
contrib,mdshuai,https://github.com/kubernetes/contrib/issues/1442,https://api.github.com/repos/kubernetes/contrib/issues/1442,[Ansible] How install different kubernetes version clusters on fedora/rhel,"Sometime I want to install a latest k8s version, like( v1.3.3 ,  v1.4.0-alpha.1), I notice there is a version parameter https://github.com/kubernetes/contrib/blob/master/ansible/roles/kubernetes/defaults/main.yml#L2
But if I change it to different version, it still install a k8s cluster with v1.2.
How to install different version k8s cluster with ansible on fedora/rhel? How to install k8s cluster from github release with ansible on fedora/rhel? thanks so much.
",closed,False,2016-07-28 09:29:01,2016-08-26 07:31:41
contrib,ide,https://github.com/kubernetes/contrib/pull/1443,https://api.github.com/repos/kubernetes/contrib/issues/1443,Update the README for the nginx-slim image,"The NGINX image actually adds back SPDY support, which was removed when NGINX fully launched HTTP/2. There is also a patch for dynamic sizing of the TLS records that look at the TCP segment sizes.
",closed,True,2016-07-28 10:21:52,2016-08-10 21:38:15
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1444,https://api.github.com/repos/kubernetes/contrib/issues/1444,Refactor kubernetes roles,"Continuing effort [1] of cleaning roles for kubernetes, master and node roles.

TODO:
- [x] clean `kubernetes` role
- [x] move `download_bins.yml` under `node` and `master` and enable installation of downloaded binaries for Fedora, CentOS and other OSes.
- [x] extend the installation options with installation of distribution rpm (not just the latest)
- [x] clean and refactor `node` role
- [x] clean and refactor `master` role
- [x] introduce `deploy-master.yml` and `deploy-node.yml` playbooks, include them in the top level playbook
- [x] tag relevant tasks and introduce new scripts
- [x] move CPU and Memory accounting under node role (inside `kubelet-configuration.yml`)

Some items of the TODO (new playbooks, new scripts) can be resolved only once the https://github.com/kubernetes/contrib/pull/967 is merged.

[1] https://github.com/kubernetes/contrib/issues/1298

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1444)

<!-- Reviewable:end -->
",closed,True,2016-07-28 12:49:45,2016-08-24 05:33:37
contrib,apelisse,https://github.com/kubernetes/contrib/issues/1445,https://api.github.com/repos/kubernetes/contrib/issues/1445,[mungegithub] Bot mis-labels PRs when Github fails,"Looks like Github had a hiccup yesterday. We should deal with that properly.

Trying to find the full list of where it happened:
https://github.com/kubernetes/kubernetes/pull/29618
https://github.com/kubernetes/kubernetes/pull/29619
https://github.com/kubernetes/kubernetes/pull/29622
https://github.com/kubernetes/kubernetes/pull/29623
https://github.com/kubernetes/kubernetes/pull/29631
https://github.com/kubernetes/kubernetes/pull/29632
https://github.com/kubernetes/kubernetes/pull/29634
https://github.com/kubernetes/kubernetes/pull/29639
https://github.com/kubernetes/kubernetes/pull/29648
https://github.com/kubernetes/kubernetes/pull/29649
https://github.com/kubernetes/kubernetes/pull/29650
https://github.com/kubernetes/kubernetes/pull/29652
https://github.com/kubernetes/kubernetes/pull/29653
https://github.com/kubernetes/kubernetes/pull/29655
https://github.com/kubernetes/kubernetes/pull/29658
https://github.com/kubernetes/kubernetes/pull/29663

I'll go through the code and see if we have proper error handling.

Impacted mungers:
mungers/cherrypick-label-unapproved.go
mungers/docs-no-retest.go
",closed,False,2016-07-28 16:09:00,2017-01-21 01:42:53
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1446,https://api.github.com/repos/kubernetes/contrib/issues/1446,mungegithub: Stop process if it fails,"Currently, the code keeps running if it fails to fetch a pull-request
(maybe because Github is down) doing things it shouldn't do (like adding
labels).
Just stop as soon as we can if we detect an error.

Fixes #1445

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1446)

<!-- Reviewable:end -->
",closed,True,2016-07-28 16:21:41,2017-04-19 07:39:16
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1447,https://api.github.com/repos/kubernetes/contrib/issues/1447,Expose pprof handlers from healthz,,closed,True,2016-07-28 16:53:20,2016-07-28 21:38:25
contrib,SleepyBrett,https://github.com/kubernetes/contrib/issues/1448,https://api.github.com/repos/kubernetes/contrib/issues/1448,NGINX ingress - Invalid memory or nil pointer dereference,"This issue started happening recently and I've been unable to tease out what the issue is thus far: Here are the error logs of my pod with --v=3

```
I0728 18:22:03.538191       1 main.go:99] Using build: https://github.com/bprashanth/contrib.git - git-b195d9b
I0728 18:22:03.850164       1 main.go:134] Validated kube-system/default-http-backend as the default backend
I0728 18:22:03.850325       1 utils.go:66] nameservers to use: [25.0.0.10]
W0728 18:22:03.850482       1 ssl.go:132] no file dhparam.pem found in secrets
I0728 18:22:03.903589       1 controller.go:1128] starting NGINX loadbalancer controller
I0728 18:22:03.903983       1 command.go:35] Starting NGINX process...
I0728 18:22:03.907639       1 reflector.go:213] Starting reflector *extensions.Ingress (30s) from k8s.io/contrib/ingress/controllers/nginx/controller.go:1131
I0728 18:22:03.907807       1 reflector.go:253] Listing and watching *extensions.Ingress from k8s.io/contrib/ingress/controllers/nginx/controller.go:1131
I0728 18:22:03.908725       1 reflector.go:213] Starting reflector *api.Endpoints (30s) from k8s.io/contrib/ingress/controllers/nginx/controller.go:1132
I0728 18:22:03.908783       1 reflector.go:253] Listing and watching *api.Endpoints from k8s.io/contrib/ingress/controllers/nginx/controller.go:1132
I0728 18:22:03.909050       1 reflector.go:213] Starting reflector *api.Service (30s) from k8s.io/contrib/ingress/controllers/nginx/controller.go:1133
I0728 18:22:03.909112       1 reflector.go:253] Listing and watching *api.Service from k8s.io/contrib/ingress/controllers/nginx/controller.go:1133
I0728 18:22:03.909336       1 reflector.go:213] Starting reflector *api.Secret (30s) from k8s.io/contrib/ingress/controllers/nginx/controller.go:1134
I0728 18:22:03.909367       1 reflector.go:253] Listing and watching *api.Secret from k8s.io/contrib/ingress/controllers/nginx/controller.go:1134
I0728 18:22:03.910913       1 reflector.go:213] Starting reflector *api.ConfigMap (30s) from k8s.io/contrib/ingress/controllers/nginx/controller.go:1135
I0728 18:22:03.910960       1 reflector.go:253] Listing and watching *api.ConfigMap from k8s.io/contrib/ingress/controllers/nginx/controller.go:1135
I0728 18:22:04.121433       1 utils.go:89] syncing kube-system/kube-controller-manager
I0728 18:22:05.122474       1 utils.go:91] requeuing kube-system/kube-controller-manager, err deferring sync till endpoints controller has synced
I0728 18:22:05.122565       1 utils.go:89] syncing default/kubernetes
I0728 18:22:05.125479       1 controller.go:992] getting endpoints for service kube-system/default-http-backend and port 8080
I0728 18:22:05.125534       1 controller.go:1060] endpoints found: [{25.1.36.2 8080 0 0} {25.1.39.2 8080 0 0} {25.1.66.2 8080 0 0}]
I0728 18:22:05.126085       1 ssl.go:111] found [b7067b2f53db] common names: 1
I0728 18:22:05.126229       1 controller.go:992] getting endpoints for service kube-system/default-http-backend and port 8080
I0728 18:22:05.126260       1 controller.go:1060] endpoints found: [{25.1.36.2 8080 0 0} {25.1.39.2 8080 0 0} {25.1.66.2 8080 0 0}]
W0728 18:22:05.126321       1 utils.go:231] system net.core.somaxconn=128. Using NGINX default (511)
I0728 18:22:05.126877       1 template.go:79] NGINX configuration: {""backlogSize"":511,""cfg"":{""bodySize"":""0m"",""customHttpErrors"":[],""enableDynamicTlsRecords"":true,""enableSpdy"":true,""errorLogLevel"":""notice"",""gzipTypes"":""application/atom+xml application/javascript application/json application/rss+xml application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/svg+xml image/x-icon text/css text/plain text/x-component"",""hsts"":true,""hstsIncludeSubdomains"":true,""hstsMaxAge"":""15724800"",""keepAlive"":75,""maxWorkerConnections"":16384,""proxyConnectTimeout"":5,""proxyReadTimeout"":60,""proxyRealIpCidr"":""0.0.0.0/0"",""proxySendTimeout"":60,""retryNonIdempotent"":false,""serverNameHashBucketSize"":256,""serverNameHashMaxSize"":512,""skipAccessLogUrls"":[],""sslBufferSize"":""4k"",""sslCiphers"":""ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA"",""sslProtocols"":""TLSv1 TLSv1.1 TLSv1.2"",""sslRedirect"":true,""sslSessionCache"":true,""sslSessionCacheSize"":""10m"",""sslSessionTickets"":true,""sslSessionTimeout"":""10m"",""useGzip"":true,""useHttp2"":true,""vtsStatusZoneSize"":""10m"",""whitelistSourceRange"":[],""workerProcesses"":""1""},""customErrors"":false,""defResolver"":""25.0.0.10"",""servers"":[{""Name"":""_"",""Locations"":[{""Path"":""/"",""IsDefBackend"":true,""Upstream"":{""Name"":""upstream-default-backend"",""Backends"":[{""Address"":""25.1.36.2"",""Port"":""8080"",""MaxFails"":0,""FailTimeout"":0},{""Address"":""25.1.39.2"",""Port"":""8080"",""MaxFails"":0,""FailTimeout"":0},{""Address"":""25.1.66.2"",""Port"":""8080"",""MaxFails"":0,""FailTimeout"":0}],""Secure"":false},""Auth"":{""Type"":"""",""Realm"":"""",""File"":"""",""Secured"":false},""RateLimit"":{""Connections"":{""Name"":"""",""Limit"":0,""Burst"":0,""SharedSize"":0},""RPS"":{""Name"":"""",""Limit"":0,""Burst"":0,""SharedSize"":0}},""Redirect"":{""Target"":"""",""AddBaseURL"":false,""SSLRedirect"":false},""SecureUpstream"":false,""Whitelist"":{""CIDR"":null}}],""SSL"":false,""SSLCertificate"":"""",""SSLCertificateKey"":"""",""SSLPemChecksum"":""""}],""sslDHParam"":"""",""tcpUpstreams"":[],""udpUpstreams"":[],""upstreams"":[{""Name"":""upstream-default-backend"",""Backends"":[{""Address"":""25.1.36.2"",""Port"":""8080"",""MaxFails"":0,""FailTimeout"":0},{""Address"":""25.1.39.2"",""Port"":""8080"",""MaxFails"":0,""FailTimeout"":0},{""Address"":""25.1.66.2"",""Port"":""8080"",""MaxFails"":0,""FailTimeout"":0}],""Secure"":false}]}
E0728 18:22:05.127121       1 runtime.go:58] Recovered from panic: ""invalid memory address or nil pointer dereference"" (runtime error: invalid memory address or nil pointer dereference)
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/runtime/runtime.go:52
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/runtime/runtime.go:40
/usr/lib/google-golang/src/runtime/asm_amd64.s:479
/usr/lib/google-golang/src/runtime/panic.go:458
/usr/lib/google-golang/src/text/template/exec.go:140
/usr/lib/google-golang/src/runtime/asm_amd64.s:479
/usr/lib/google-golang/src/runtime/panic.go:458
/usr/lib/google-golang/src/runtime/panic.go:62
/usr/lib/google-golang/src/runtime/sigpanic_unix.go:24
/usr/lib/google-golang/src/text/template/exec.go:186
/usr/lib/google-golang/src/text/template/exec.go:175
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/controllers/nginx/nginx/template.go:83
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/controllers/nginx/nginx/command.go:65
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/controllers/nginx/controller.go:461
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/controllers/nginx/controller.go:150
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/controllers/nginx/utils.go:90
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/controllers/nginx/utils.go:64
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/wait/wait.go:86
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/wait/wait.go:87
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/wait/wait.go:49
/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/controllers/nginx/utils.go:64
/usr/lib/google-golang/src/runtime/asm_amd64.s:2086
```

Thoughts?

UPDATE: this is clearly a problem with my template (which I'm injecting via volume mounted configmap) trying to figure out if it's the template itself (probable).

Confirmed this was a problem in my template,  a nicer error message might be helpful here.
",closed,False,2016-07-28 18:29:00,2016-08-05 20:57:45
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1449,https://api.github.com/repos/kubernetes/contrib/issues/1449,Remove get commits,"Remove as many occurrences of GetCommits as possible. 
There is one left in `cherrypick-clear-after-merge` but it only applies to cherrypick candidates, and there are not so many of these...
Hopefully it can have a good impact on token usage.
",closed,True,2016-07-28 20:18:35,2016-08-01 17:48:18
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1450,https://api.github.com/repos/kubernetes/contrib/issues/1450,[nginx-ingress-controller]: Check for errors in nginx template,"fixes #1448
",closed,True,2016-07-28 21:43:04,2016-08-05 20:57:45
contrib,amarruedo,https://github.com/kubernetes/contrib/issues/1451,https://api.github.com/repos/kubernetes/contrib/issues/1451,NGINX Ingress Controller - Error reading file /etc/nginx/GeoLiteCity.dat,"Hi!

I am experiencing the folowing issue with nginx ingress controller:

```
I0729 08:28:02.618643       1 event.go:216] Event(api.ObjectReference{Kind:""Ingress"", Namespace:""default"", Name:""xxxx-ingress"", UID:""33ca4b44-3df0-11e6-950a-42010af00123"", APIVersion:""extensions"", ResourceVersion:""400630"", FieldPath:""""}): type: 'Normal' reason: 'CREATE' ip: xxx.xxx.xxx.xxx
I0729 08:28:02.619053       1 event.go:216] Event(api.ObjectReference{Kind:""Ingress"", Namespace:""default"", Name:""xxxx-ingress"", UID:""33ca4b44-3df0-11e6-950a-42010af00123"", APIVersion:""extensions"", ResourceVersion:""400647"", FieldPath:""""}): type: 'Normal' reason: 'UPDATE' default/xxxx-ingress
E0729 08:28:02.988214       1 command.go:44] nginx error: signal: killed
E0729 08:28:02.990789       1 command.go:87] failed to execute nginx -s reload: Error reading file /etc/nginx/GeoLiteCity.dat
2016/07/29 08:28:02 [emerg] 19#19: GeoIP_open(""/etc/nginx/GeoLiteCity.dat"") failed in /etc/nginx/nginx.conf:29
nginx: [emerg] GeoIP_open(""/etc/nginx/GeoLiteCity.dat"") failed in /etc/nginx/nginx.conf:29
I0729 08:28:31.266472       1 main.go:193] Received SIGTERM, shutting down
```

Everything worked fine for me prior to version 0.7 of nginx controller (i.e. prior to introducing GeoIP). Now, it fails to reload nginx configuration and container crashes, which leads the nginx pod to a CrashLoopBackOff state.

I just changed the nginx image version in my deployment from 0.6 to 0.8.1 (I've tested with 0.7 as well, and I get the same error). 

Is there anything I'm missing?

Thanks in advance for your help.
",closed,False,2016-07-29 08:54:55,2016-12-26 16:37:24
contrib,pwittrock,https://github.com/kubernetes/contrib/pull/1452,https://api.github.com/repos/kubernetes/contrib/issues/1452,Support embedding assignmentConfigs in .md files.,,closed,True,2016-07-29 21:07:23,2016-08-02 17:51:24
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1453,https://api.github.com/repos/kubernetes/contrib/issues/1453,Fix health check link,"Just convering health_check to health_checks
",closed,True,2016-07-29 21:55:03,2016-07-30 23:49:04
contrib,foxish,https://github.com/kubernetes/contrib/pull/1454,https://api.github.com/repos/kubernetes/contrib/issues/1454,Fix pv definition to match running instance.,"The name of the PD mounted on the utility cluster is kubernetes-cache.

cc @lavalamp 
",closed,True,2016-07-29 22:12:16,2016-07-29 22:13:56
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1455,https://api.github.com/repos/kubernetes/contrib/issues/1455,Mention assignees and author when LGTM is removed,"After a push, the LGTM flag is removed by the bot. It can be hard to
notice because it doesn't mention anybody.

Make it easier for people to notice by mentioning reviewers and author,
and keep the pull-request moving.
",closed,True,2016-07-30 04:04:45,2016-07-30 17:50:32
contrib,foxish,https://github.com/kubernetes/contrib/pull/1456,https://api.github.com/repos/kubernetes/contrib/issues/1456,Adding deployment files for contrib and docs repos,"https://github.com/kubernetes/contrib/issues/1304

The PV, PVC, secret and configmap for each of those repos
Also modifications to the ingress to accommodate the new services.

cc @lavalamp @apelisse @pwittrock 
",closed,True,2016-07-30 17:57:12,2016-08-02 18:39:54
contrib,zllovesuki,https://github.com/kubernetes/contrib/issues/1457,https://api.github.com/repos/kubernetes/contrib/issues/1457,NGINX Ingress Controller - net.core.somaxconn=128,"The setup works fine (btw thank you community :) ), however the logs is flooded with:

```
W0801 05:21:58.225140       1 utils.go:231] system net.core.somaxconn=128. Using NGINX default (511)
```

Is there any way to tune `net.core.somaxconn` or ignore the log?
",closed,False,2016-08-01 05:25:05,2016-09-22 17:30:12
contrib,aman-tiwari,https://github.com/kubernetes/contrib/pull/1458,https://api.github.com/repos/kubernetes/contrib/issues/1458,Added URL param keyed sticky sessions,"This pull request enables using sticky sessions keyed by a URL param `session`, enabled through annotation `serviceloadbalancer/lb.url-param-sticky-session`. 
Additionally, the parameters for the stick table can now be configured by annotation `serviceloadbalancer/lb.stick-table-params`.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1458)

<!-- Reviewable:end -->
",closed,True,2016-08-01 21:46:32,2018-02-18 12:46:00
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1459,https://api.github.com/repos/kubernetes/contrib/issues/1459,Nginx controller should work with local-up-cluster,"It's the best way to get people familiar with ingress, but currently it fails with:

```
I0801 22:40:56.415722       1 main.go:99] Using build: https://github.com/bprashanth/contrib.git - git-b195d9b
F0801 22:41:26.419623       1 main.go:128] unexpected error getting runtime information: timed out waiting for the condition
```

I _think_ this is because there is no service account. It should just default to using localhost:8080, i thought the client library did this but i might be wrong. I thought `clientConfig` would default to this if nothing else worked, but obviously I'm wrong or missing an option. Can take a look when i have time to debug, but fyi @aledbf 
",closed,False,2016-08-01 22:48:21,2016-08-17 20:40:11
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1460,https://api.github.com/repos/kubernetes/contrib/issues/1460,Mention code,"Create generic code to mention people, so that we don't reimplement it in each munger. It comes with tests and interesting utility functions.
",closed,True,2016-08-01 23:46:20,2016-08-07 04:11:35
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1461,https://api.github.com/repos/kubernetes/contrib/issues/1461,move kubemark to nonblocking,"@gmarek & @wojtek-t... I'm really sorry but when I told @quinton-hoole I wasn't taking any more post-submit tests that didn't have corresponding presubmit tests (e.g., federation tests), he was like, ""Kubemark does that,"" and I was like, ""Well, I'll kick it out next time it causes a prolonged blockage."" Still trying to figure out what the current problem is, if it's not a particular PR then I'll reconsider.
",closed,True,2016-08-01 23:52:24,2016-08-02 06:14:45
contrib,FourSigma,https://github.com/kubernetes/contrib/pull/1462,https://api.github.com/repos/kubernetes/contrib/issues/1462,GCE ingress http-only annotation naming fix,"```
GCE ingress http-only annotation naming fix
```
",closed,True,2016-08-02 05:12:12,2016-08-02 21:00:23
contrib,pieterlange,https://github.com/kubernetes/contrib/issues/1463,https://api.github.com/repos/kubernetes/contrib/issues/1463,NGINX ingress controller - make ingress.class option configurable,"The newly added ""ingress class"" option allows cluster administrators to select which set of ingress controllers serves certain Ingress objects by setting an `kubernetes.io/ingress.class` annotation. Currently the NGINX controller is hardcoded to look for ingresses with an ""nginx"" value or nonexistent/empty annotation.

If we make this option configurable through the configmap settings, cluster admins would be able to run different sets of ingress controllers and have external network policing on the ingresses. EG one could run some Ingress controllers for ""consumer"" traffic and another set of ingress controllers that only serve cluster admin/monitoring services. One could also imagine this being used for slow/safe replacements of the ingress controller set itself.
",closed,False,2016-08-02 09:07:16,2017-11-28 10:22:22
contrib,rihabbanday,https://github.com/kubernetes/contrib/issues/1464,https://api.github.com/repos/kubernetes/contrib/issues/1464,Error while generating secret json config in /tmp/secret.json,"I am trying to create a service loadbalancer using ""rc.yaml"" file as mentioned in https://github.com/kubernetes/contrib/tree/master/service-loadbalancer whic gets successfully deployed. Further I am trying the nginx https service example mentioned in https://github.com/kubernetes/kubernetes/tree/master/examples/https-nginx. however when I do `make keys secret KEY=/tmp/nginx.key CERT=/tmp/nginx.crt SECRET=/tmp/secret.json` I get the errors:

`make_secret.go:29:2: cannot find package ""k8s.io/kubernetes/pkg/api"" in any of:
        /usr/lib/go/src/pkg/k8s.io/kubernetes/pkg/api (from $GOROOT)
        /root/go/src/k8s.io/kubernetes/pkg/api (from $GOPATH)
make_secret.go:34:2: cannot find package ""k8s.io/kubernetes/pkg/api/install"" in any of:
        /usr/lib/go/src/pkg/k8s.io/kubernetes/pkg/api/install (from $GOROOT)
        /root/go/src/k8s.io/kubernetes/pkg/api/install (from $GOPATH)
make_secret.go:30:2: cannot find package ""k8s.io/kubernetes/pkg/apimachinery/registered"" in any of:
        /usr/lib/go/src/pkg/k8s.io/kubernetes/pkg/apimachinery/registered (from $GOROOT)
        /root/go/src/k8s.io/kubernetes/pkg/apimachinery/registered (from $GOPATH)
make_secret.go:31:2: cannot find package ""k8s.io/kubernetes/pkg/runtime"" in any of:
        /usr/lib/go/src/pkg/k8s.io/kubernetes/pkg/runtime (from $GOROOT)
        /root/go/src/k8s.io/kubernetes/pkg/runtime (from $GOPATH)
make: *** [secret] Error 1`

I have installed go using binaries and my version is
`go version go1.2.1 linux/amd64`.

ANy suggestions to fix this error?
",closed,False,2016-08-02 09:17:37,2018-03-18 12:03:59
contrib,mstrzele,https://github.com/kubernetes/contrib/issues/1465,https://api.github.com/repos/kubernetes/contrib/issues/1465,Can't use this repo as a submodule,"Seems that someone added empty dirs as a submodules in `kubeform` directory. Unfortunately this causes error when `contrib` is being used as a submodule in other repository:

```
$ git submodule update --init --recursive
fatal: no submodule mapping found in .gitmodules for path 'kubeform/vendor/github.com/Sirupsen/logrus'
```
",closed,False,2016-08-02 13:20:22,2016-08-02 22:04:12
contrib,mstrzele,https://github.com/kubernetes/contrib/pull/1466,https://api.github.com/repos/kubernetes/contrib/issues/1466,Remove empty submodules from kubeform,"Dependencies for kubeform are managed by [glide](https://glide.sh/), so no need to store empty diretories. This change should allow to use this repository (`contrib`) as a submodule and should fix #1465.
",closed,True,2016-08-02 13:29:39,2016-08-02 22:04:12
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1467,https://api.github.com/repos/kubernetes/contrib/issues/1467,[nginx-ingress-controller]: Use ClientConfig to configure connection,"fixes #1459

Running with `docker run`:

```
core@localhost ~ $ docker run -it aledbf/nginx-third-party:0.31 bash
root@f6a96f46eab0:/# export KUBERNETES_MASTER=http://172.17.4.99:8080
root@f6a96f46eab0:/# /nginx-ingress-controller --default-backend-service=default/nginx-errors
I0802 14:44:58.604384       7 main.go:94] Using build: https://github.com/aledbf/contrib - git-5b9146a
W0802 14:44:58.605282       7 main.go:118] unexpected error getting runtime information: unable to get POD information (missing POD_NAME or POD_NAMESPACE environment variable)
I0802 14:44:58.607270       7 main.go:123] Validated default/nginx-errors as the default backend
W0802 14:44:58.611322       7 ssl.go:132] no file dhparam.pem found in secrets
I0802 14:44:58.615637       7 controller.go:1128] starting NGINX loadbalancer controller
I0802 14:44:58.615902       7 command.go:35] Starting NGINX process...
```

Running inside in a cluster:

```
I0802 14:47:50.254736       1 main.go:94] Using build: https://github.com/aledbf/contrib - git-5b9146a
I0802 14:47:50.254920       1 merged_client_builder.go:103] No kubeconfig could be created, falling back to service account.
I0802 14:47:50.343440       1 main.go:123] Validated default/nginx-errors as the default backend
W0802 14:47:50.343677       1 ssl.go:132] no file dhparam.pem found in secrets
I0802 14:47:50.347322       1 controller.go:1128] starting NGINX loadbalancer controller
I0802 14:47:50.347870       1 command.go:35] Starting NGINX process...
```

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1467)

<!-- Reviewable:end -->
",closed,True,2016-08-02 14:49:54,2016-08-17 20:40:11
contrib,foxish,https://github.com/kubernetes/contrib/pull/1468,https://api.github.com/repos/kubernetes/contrib/issues/1468,Adding submit-queue munger to the contrib deployment.,"The check-labels munger (https://github.com/kubernetes/contrib/pull/1306) needs to be merged before, so that it can create the necessary labels.

Also adding the submit-queue munger to the deployment to enable automatic merging of PRs with LGTM, after they pass CI tests.
",closed,True,2016-08-02 21:07:36,2016-08-02 22:45:18
contrib,foxish,https://github.com/kubernetes/contrib/pull/1469,https://api.github.com/repos/kubernetes/contrib/issues/1469,Removing non-org members,"@aledbf
@danehans
@simonswine
@rutsky
@itayariel
@stephenrlouie
@mattthias
@jdavis7257

Please add yourself to the kubernetes org, so that PR assignments can be made. 
",closed,True,2016-08-02 21:35:58,2016-08-02 22:07:41
contrib,foxish,https://github.com/kubernetes/contrib/issues/1470,https://api.github.com/repos/kubernetes/contrib/issues/1470,No assignees for some PRs,"We have [11 PRs](https://github.com/kubernetes/contrib/pulls?q=is%3Aopen+is%3Apr+no%3Aassignee) to which Blunderbuss is unable to make an assignment. We must investigate why it fails to make an assignment.

Logs from Blunderbuss:

```
W0802 23:14:15.888785       1 blunderbuss.go:117] Couldn't find an owner for: dnsperf/Dockerfile
W0802 23:14:15.888881       1 blunderbuss.go:117] Couldn't find an owner for: dnsperf/Makefile
W0802 23:14:15.889008       1 blunderbuss.go:117] Couldn't find an owner for: dnsperf/README.md
W0802 23:14:15.889626       1 blunderbuss.go:117] Couldn't find an owner for: dnsperf/dnsperf-job.yaml
W0802 23:14:15.889647       1 blunderbuss.go:117] Couldn't find an owner for: dnsperf/queryfile-local-services
W0802 23:14:15.889703       1 blunderbuss.go:117] Couldn't find an owner for: dnsperf/runjob.sh
E0802 23:14:15.890100       1 blunderbuss.go:128] No owners found for PR 1189
```

It appears that some of them are PRs which add code to directories which have since been deleted from the repo. We should possibly close those PRs.
",closed,False,2016-08-02 23:22:35,2016-08-05 20:15:27
contrib,foxish,https://github.com/kubernetes/contrib/pull/1471,https://api.github.com/repos/kubernetes/contrib/issues/1471,Turning on blunderbuss on the docs submit-queue.,,closed,True,2016-08-03 01:52:19,2016-08-03 05:04:36
contrib,foxish,https://github.com/kubernetes/contrib/issues/1472,https://api.github.com/repos/kubernetes/contrib/issues/1472,Details link from the Submit Queue should point at the right instance,"![screenshot from 2016-08-02 19 06 24](https://cloud.githubusercontent.com/assets/906471/17351540/6ec1bdd8-58e4-11e6-95f4-fa27c569dcbc.png)

It currently points at the main repository's submit-queue on contrib as well.
",closed,False,2016-08-03 02:09:30,2018-02-27 00:05:52
contrib,foxish,https://github.com/kubernetes/contrib/pull/1473,https://api.github.com/repos/kubernetes/contrib/issues/1473,Adding labels.yaml for check-labels munger.,"List of labels to be created in the contrib repository.
The list may need to be pruned, in case some labels do not make sense for contrib.
",closed,True,2016-08-03 08:40:58,2016-08-05 22:05:58
contrib,foxish,https://github.com/kubernetes/contrib/pull/1474,https://api.github.com/repos/kubernetes/contrib/issues/1474,Adding early exit condition for submit-queue merging,"When there are no e2e tests to be run, we can merge after LGTM. Pre-submit tests are already being performed as part of https://github.com/foxish/contrib/blob/4216d7608c9906edc5607c4092f4d8119f4511fc/mungegithub/mungers/submit-queue.go#L1133

PTAL @lavalamp @eparis 
",closed,True,2016-08-03 15:15:16,2016-08-05 18:04:51
contrib,foxish,https://github.com/kubernetes/contrib/pull/1475,https://api.github.com/repos/kubernetes/contrib/issues/1475,Deployment flag to control owners embedded in md.,"The docs repository reads owners from the front-matter in markdown
files as well as dedicated OWNERS files.

cc @pwittrock @lavalamp 
",closed,True,2016-08-03 17:50:48,2016-08-03 19:04:39
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1476,https://api.github.com/repos/kubernetes/contrib/issues/1476,Added New Label Munger to configmap.yaml,"Modified the name of the issue categorizer munger and
added the issue-triager to the configmap.yaml
",closed,True,2016-08-03 21:40:41,2016-08-03 22:34:42
contrib,foxish,https://github.com/kubernetes/contrib/pull/1477,https://api.github.com/repos/kubernetes/contrib/issues/1477,Adding lgtm-after-commit munger to contrib,"This munger removes LGTM if commits are pushed after LGTM label was applied.
",closed,True,2016-08-03 22:17:32,2016-08-03 22:45:13
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1478,https://api.github.com/repos/kubernetes/contrib/issues/1478,Changed the Label Munger Post From Localhost,"It now posts to issue-triager-service instead of
localhost.
",closed,True,2016-08-03 22:46:32,2016-08-03 22:47:23
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1479,https://api.github.com/repos/kubernetes/contrib/issues/1479,Split By Comma Instead of Semicolon,,closed,True,2016-08-03 22:55:17,2016-08-03 22:56:14
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1480,https://api.github.com/repos/kubernetes/contrib/issues/1480,"Revert ""lgtm handler: find lgtm comment and add label""","Reverts kubernetes/contrib#1428

This change causes the bot to fight with itself (e.g., when another munger enforces the release note process).
",closed,True,2016-08-03 23:35:28,2016-08-03 23:42:27
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1481,https://api.github.com/repos/kubernetes/contrib/issues/1481,Fixed the Typo in Component,"Changed From componentS to component
",closed,True,2016-08-03 23:47:12,2016-08-03 23:47:27
contrib,foolusion,https://github.com/kubernetes/contrib/issues/1482,https://api.github.com/repos/kubernetes/contrib/issues/1482,nginx ingress high latency / low rps vs service,"calling via internal aws elb

```
Running` 30s test @ http://elwin.k8s-a.ttoapps.aws.cloud.nordstrom.net/?userid=mm&teamid=test
  500 goroutine(s) running concurrently
50085 requests in 30.16710187s, 17.19MB read
Requests/sec:           1660.25
Transfer/sec:           583.49KB
Avg Req Time:           301.159048ms
Fastest Request:        87.959178ms
Slowest Request:        1.559768394s
Number of Errors:       0
```

vs

calling via service

```
Running 30s test @ http://elwin/?userid=mm&teamid=test
  500 goroutine(s) running concurrently
3742341 requests in 29.776728744s, 1.09GB read
Requests/sec:           125680.06
Transfer/sec:           37.64MB
Avg Req Time:           3.978355ms
Fastest Request:        76.667µs
Slowest Request:        410.97233ms
Number of Errors:       0
```

calling via http-router with header Host: elwin.k8s-a.ttoapps.aws.cloud.nordstrom.net

```
Running 30s test @ http://25.0.0.20/?teamid=test&userid=mm
  500 goroutine(s) running concurrently
47453 requests in 30.1639595s, 16.29MB read
Requests/sec:        1573.17
Transfer/sec:        552.87KB
Avg Req Time:        317.829847ms
Fastest Request:    16.719018ms
Slowest Request:    895.236499ms
Number of Errors:    0
```

I am adding a snapshot of my config files. To start the service from scratch i would run the following.

```
kubectl create -f elwin-mongo.yaml
kubectl create -f elwin-storage.yaml
kubectl create -f elwin.yaml
```

to run the benchmark i run 

```
kubectl create -f elwin-wrk.yaml
```

i think you would need to modify the ingress in `elwin.yaml` to whatever your host is.

The service code is available at https://foolusion/github.com/choices if you need it for any reason. Let me know if you would like me to run anything to check performance.

[config.zip](https://github.com/kubernetes/contrib/files/400601/config.zip)
",closed,False,2016-08-04 00:35:04,2016-08-09 20:11:10
contrib,gouyang,https://github.com/kubernetes/contrib/issues/1483,https://api.github.com/repos/kubernetes/contrib/issues/1483,[ansible] Move role flannel before role docker in deploy-cluster.yml,"This is a real problem I met during testing.
Steps to reproduce the problem:
- There are two host A and B, inventory file looks like on host A.
  
  ```
  
  [masters]
  host-A
  
  [etcd]
  host-A
  
  [nodes]
  host-A
  host-B
  ```
- Run `setup.sh` on host A, the cluster is up.
- After some time, Host A is down and I can use host B only.
- The inventory on host B looks like

```
[masters]
host-B

[etcd]
host-B

[nodes]
host-B
```
- Make some changes to /etc/sysconfig/docker or /etc/sysconfig/docker-network
- Run `setup.sh` on host B, It failed at 

```
RUNNING HANDLER [docker : docker fix start on btrfs] ***************************
skipping: [dell-per610-004.abc.com]

RUNNING HANDLER [docker : docker start service] ********************************
fatal: [dell-per610-004.abc.com]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""A dependency job for docker.service failed. See 'journalctl -xe' for details.\n""}
```

It failed because the docker dependency service flanneld could not be started as the flanneld cannot reach the old etcd server host-A.

If move role flannel before role docker, the configure will be overwritten to host-B and the playbook will not exit.
",closed,False,2016-08-04 03:22:45,2018-02-15 19:42:04
contrib,gouyang,https://github.com/kubernetes/contrib/issues/1484,https://api.github.com/repos/kubernetes/contrib/issues/1484,[ansible] Move role node before role kube-addons,"Make node ready before deploy addons because addons are running on node eventually. It reduce unnecessary logs at least.
",closed,False,2016-08-04 06:06:55,2018-02-14 20:20:10
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1485,https://api.github.com/repos/kubernetes/contrib/issues/1485,Cluster-autoscaler: max node total count flag for CIDR,"In some cases we cannot grow the cluster beyond a certain number of nodes. This PR adds a flag to pass this value + appropriate handling in scale up. The cluster won't be forcefully scaled down to meet the passed value.

cc: @piosz @fgrzadkowski 
",closed,True,2016-08-04 14:01:25,2016-08-04 15:29:37
contrib,foxish,https://github.com/kubernetes/contrib/pull/1486,https://api.github.com/repos/kubernetes/contrib/issues/1486,Adding the check-labels munger to docs and contrib,"Addressing issue https://github.com/kubernetes/kubernetes.github.io/issues/124
",closed,True,2016-08-05 16:36:26,2016-08-07 04:11:39
contrib,aledbf,https://github.com/kubernetes/contrib/issues/1487,https://api.github.com/repos/kubernetes/contrib/issues/1487,[nginx-ingress-controller] Improve nginx default,"Default values of `types_hash_max_size`, `server_names_hash_max_size` and `server_names_hash_bucket_size` should take in account to the number of Ingress rules and adjust the default value accordingly
",closed,False,2016-08-05 16:48:01,2016-09-01 21:11:47
contrib,timstclair,https://github.com/kubernetes/contrib/issues/1488,https://api.github.com/repos/kubernetes/contrib/issues/1488,Do not require release-note labels for test or tools PRs.,"Changes to certain files / directories almost never require release-notes. If a PR only touches those files, it should not require the release-note label. Candidate files include:
- `test/**`
- `**/*_test.go`
- `hack/**`
- `examples/**`
",closed,False,2016-08-05 18:03:17,2018-02-16 20:06:04
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1489,https://api.github.com/repos/kubernetes/contrib/issues/1489,submit-queue: Display uncached request count to github API,"We currently display number of total requests to github API, and number
of cached requests. Let's print number of uncached request as this is
the number that matters the most.
",closed,True,2016-08-05 18:27:13,2016-08-05 20:05:58
contrib,foxish,https://github.com/kubernetes/contrib/pull/1490,https://api.github.com/repos/kubernetes/contrib/issues/1490,[Experimental] Getting PR lists instead of one,"Breaks the bot right now due to the absence of information in the PR list about:
- mergeable (submit-queue)
- merged (submit-queue)
- additions/deletions (size)
",closed,True,2016-08-05 18:28:49,2016-08-05 18:28:53
contrib,foxish,https://github.com/kubernetes/contrib/issues/1491,https://api.github.com/repos/kubernetes/contrib/issues/1491,Blunderbuss should pick owners for top-level directory additions,"When directory additions are made to the top-level of the repository, blunderbuss does not make assignments, or find owners.
Example: no assignees for #1343, #1189, #892, #841
",closed,False,2016-08-05 18:55:16,2016-08-05 23:43:25
contrib,colemickens,https://github.com/kubernetes/contrib/issues/1492,https://api.github.com/repos/kubernetes/contrib/issues/1492,nginx-ingress-controller: support for oauth2_proxy,"I'd like to have an nginx-ingress-controller that that works with https://github.com/bitly/oauth2_proxy,

It looks like this will require more than surface level changes to nginx-ingress-controller. I'm curious if this is something that is worth accommodating in this ""upstream"" nginx-ingress-controller or if it would be invasive enough to not be desired?

I'm just starting to look into this but it seems like it would requiring:
- augmenting the nginx template to accept other chunks of config for configuring oauth2_proxy
- alternatively, modifying it to optionally accept another ConfigMap that contains the nginx template so users could do arbitrary other config changes while still benefiting from the rest of the nginx-ingress-controller machinery
- needs to consume a Secret to set relevant env vars or flags for oauth2_proxy (the oauth client_secret)

The idea here is to be able to deploy Ingress along with kube-lego (for SSL to protect auth info/cookies) and then have oauth2_proxy configured  to accept users for a certain Azure Active Directory tenant or to a GitHub organization so that I can automatically give my coworkers access to cluster service while keeping others out.

I suppose this could also be done as an extra hop, with another nginx with oauth2_proxy sitting infront of the ingress nginx Pods but that seems a bit less clean.

I'm curious if anyone else is interested in this, and if it makes sense to be part of the upstream nginx-ingress-controller or if its more appropriate as a separate project?
",closed,False,2016-08-05 18:55:19,2019-01-09 05:34:33
contrib,eparis,https://github.com/kubernetes/contrib/pull/1493,https://api.github.com/repos/kubernetes/contrib/issues/1493,Properly handle OWNERS files in the root directory,"They were being looked up in the map as """" but being stored in the map
as ""."". This makes both sides use ""/"" for the root.

fixes #1491
",closed,True,2016-08-05 20:45:52,2016-08-05 23:43:25
contrib,foxish,https://github.com/kubernetes/contrib/pull/1494,https://api.github.com/repos/kubernetes/contrib/issues/1494,Reverting commit to README.md.,"Edited using the web UI :(
",closed,True,2016-08-05 21:52:34,2016-08-05 22:54:08
contrib,foxish,https://github.com/kubernetes/contrib/pull/1495,https://api.github.com/repos/kubernetes/contrib/issues/1495,More central and readable logging of mungegithub command-line options,"People often forget to log new command-line options as they are added because the two were decoupled. This change would let us log entire
structs **as long as the required fields are exported** (which is a bit contentious because we are breaking encapsulation).

With this change, we'd get logging of options that looks a lot nicer as well:

```
I0806 22:28:27.669612   90248 mungegithub.go:90] {
  ""Org"": ""foxish"",
  ""Project"": ""test-repo"",
  ""Token"": """",
  ""TokenFile"": ""./.token"",
  ""Address"": "":8080"",
  ""WWWRoot"": ""submit-queue/www/"",
  ""HTTPCacheDir"": """",
  ""HTTPCacheSize"": 1000,
  ""MinPRNumber"": 0,
  ""MaxPRNumber"": 9223372036854775807,
  ""DryRun"": true,
  ""PendingWaitTime"": null,
  ""MinIssueNumber"": 0,
  ""PRMungersList"": [
    ""submit-queue""
  ],
  ""IssueReportsList"": [],
  ""Once"": false,
  ""Period"": 10000000000,
  ""Repos"": null,
  ""GCSInfo"": null,
  ""TestOptions"": null
}
I0806 22:28:27.669698   90248 github.go:224] Made 0 API calls since the last Reset 0.000000 calls/sec
I0806 22:28:27.669769   90248 features.go:54] Initializing feature: google-cloud-storage
I0806 22:28:27.669777   90248 features.go:54] Initializing feature: test-options
I0806 22:28:27.669862   90248 mungers.go:92] {
  ""BlockingJobNames"": [],
  ""NonBlockingJobNames"": [],
  ""PresubmitJobNames"": [],
  ""WeakStableJobNames"": [],
  ""FakeE2E"": false,
  ""Committers"": """",
  ""RequiredStatusContexts"": [
    ""continuous-integration/travis-ci/pr""
  ],
  ""DoNotMergeMilestones"": [],
  ""RequiredRetestContexts"": [],
  ""RetestBody"": ""@k8s-bot test this [submit-queue is verifying that this PR is safe to merge]"",
  ""Metadata"": {
    ""ProjectName"": ""Test-Repo"",
    ""ChartUrl"": """",
    ""HistoryUrl"": """",
    ""RepoPullUrl"": ""https://github.com/foxish/test-repo/pulls/""
  }
}
```

as opposed to what we currently have:

```
I0806 22:26:57.506125   88136 google-cloud-storage.go:56] gcs-bucket: """"
I0806 22:26:57.506140   88136 google-cloud-storage.go:57] gcs-logs-dir: """"
I0806 22:26:57.506155   88136 google-cloud-storage.go:58] pull-logs-dir: """"
I0806 22:26:57.506169   88136 google-cloud-storage.go:59] pull-key: """"
I0806 22:26:57.506183   88136 features.go:54] Initializing feature: test-options
I0806 22:26:57.506198   88136 test-options.go:48] required-retest-contexts: []string{""""}
I0806 22:26:57.506224   88136 submit-queue.go:381] jenkins-jobs: []string{}
I0806 22:26:57.506239   88136 submit-queue.go:382] nonblocking-jenkins-jobs: []string{}
I0806 22:26:57.506254   88136 submit-queue.go:383] presubmit-jobs: []string{}
I0806 22:26:57.506269   88136 submit-queue.go:384] weak-stable-jobs: []string{}
I0806 22:26:57.506284   88136 submit-queue.go:385] required-contexts: []string{""continuous-integration/travis-ci/pr""}
I0806 22:26:57.506302   88136 submit-queue.go:386] required-retest-contexts: []string{}
I0806 22:26:57.506317   88136 submit-queue.go:387] do-not-merge-milestones: []string{}
I0806 22:26:57.506332   88136 submit-queue.go:388] admin-port: 9999
I0806 22:26:57.506350   88136 submit-queue.go:389] retest-body: ""@k8s-bot test this [submit-queue is verifying that this PR is safe to merge]""
I0806 22:26:57.506368   88136 submit-queue.go:390] fake-e2e: false
I0806 22:26:57.506383   88136 submit-queue.go:391] chart-url: """"
I0806 22:26:57.506399   88136 submit-queue.go:392] history-url: """"
```
",closed,True,2016-08-07 05:32:55,2016-08-11 23:33:49
contrib,foxish,https://github.com/kubernetes/contrib/issues/1496,https://api.github.com/repos/kubernetes/contrib/issues/1496,Some imports being vendored and present in Godeps.,"I ran into this issue today when trying to get https://github.com/spf13/viper running with mungegithub. 

```
~/g/s/k/c/mungegithub (no-more-cmdline ⚡↩☡) set -x GO15VENDOREXPERIMENT 1; go build
# k8s.io/contrib/mungegithub
./mungegithub.go:52: cannot use cmd.Flags().Lookup(""once"") (type *""k8s.io/contrib/mungegithub/vendor/github.com/spf13/pflag"".Flag) as type *""github.com/spf13/pflag"".Flag in argument to viper.BindPFlag
```

The reason I saw now was that we [vendor the spf13/*](https://github.com/kubernetes/contrib/tree/master/mungegithub/vendor/github.com/spf13) imports and also have it in [godeps](https://github.com/kubernetes/contrib/blob/master/mungegithub/Godeps/Godeps.json#L89). Isn't this an anti-pattern, because it leads to situations like these when we mix vendored and non-vendored imports?
",closed,False,2016-08-07 06:25:21,2016-08-08 12:53:16
contrib,foxish,https://github.com/kubernetes/contrib/pull/1497,https://api.github.com/repos/kubernetes/contrib/issues/1497,Makes use of a config-file possible along with command-line options,"Fixes https://github.com/kubernetes/contrib/issues/1415.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1497)

<!-- Reviewable:end -->
",closed,True,2016-08-07 10:25:45,2016-10-18 17:54:40
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1498,https://api.github.com/repos/kubernetes/contrib/issues/1498,[nginx-ingress-controller] Refactoring of template handling,"This also watch for changes in the template

fixes #1457

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1498)

<!-- Reviewable:end -->
",closed,True,2016-08-07 23:54:09,2016-11-18 07:03:15
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1499,https://api.github.com/repos/kubernetes/contrib/issues/1499,Change the status from 200 to 500 Internal Error,,closed,True,2016-08-08 23:40:55,2016-08-09 00:00:58
contrib,yujuhong,https://github.com/kubernetes/contrib/issues/1500,https://api.github.com/repos/kubernetes/contrib/issues/1500,labels: [Errno 2] No such file or directory: './models/trained_teams_model.pkl',"There are [many issues](https://github.com/kubernetes/kubernetes/issues?utf8=%E2%9C%93&q=%20label%3A%22%5BErrno%202%5D%20No%20such%20file%20or%20directory%3A%20%27.%2Fmodels%2Ftrained_teams_model.pkl%27%22%20) being labeled `[Errno 2] No such file or directory: './models/trained_teams_model.pkl'` by the k8s-merge-bot in the past few days. 
",closed,False,2016-08-08 23:51:21,2016-08-09 01:52:19
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1501,https://api.github.com/repos/kubernetes/contrib/issues/1501,Correctly handle ingress.class in GCE controller ,"This bug would only get activated when a user has both `ingess.class=gce` and `ingress.class=nginx` ingresses active in the same GCE/GKE cluster, and would manifest as a set of cloud resources created wastefully for the `ingress.class=nginx` ingress as well.

We were previously only ignoring ingress.class (documented here: https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md#disabling-glbc) when the ingress was created/deleted/modified. There's a chance another ingress with the correct class results in us entering the `sync` routine and listing all ingresses. The listing routine was not smart enough to ignore `ingress.class=nginx`, so we ended up creating resources for the nginx ingress anyway. 

The second commit fixes some of the nginx examples to include a `readiness` probe that is == liveness probe. 

Minhan or Girish, whichever one has spare cycles first. 
",closed,True,2016-08-09 02:29:28,2016-08-11 20:43:48
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1502,https://api.github.com/repos/kubernetes/contrib/issues/1502,Revert redis image,"Temporarily revert the image till we can figure out https://github.com/kubernetes/contrib/issues/1334
",closed,True,2016-08-09 16:32:11,2016-08-09 20:39:22
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1503,https://api.github.com/repos/kubernetes/contrib/issues/1503,Don't clobber backends.,"If a parallel ingress controller updates the backend service with another instance group, don't clobber it, just make sure the instance group you put in is there. 
",closed,True,2016-08-10 01:32:13,2016-08-12 00:13:51
contrib,ghost,https://github.com/kubernetes/contrib/issues/1504,https://api.github.com/repos/kubernetes/contrib/issues/1504,nginx-ingress-controller: the server does not allow access to the requested resource,"I'm trying to run nginx-ingress-controller from rc-default.yaml. Doing it asis causes the following error:
`I0810 15:56:28.267295       1 main.go:99] Using build: https://github.com/bprashanth/contrib.git - git-b195d9b F0810 15:56:28.353709       1 main.go:128] unexpected error getting runtime information: the server does not allow access to the requested resource (get nodes xxx)`

Environment is kubernetes cluster. Default service account has necessary permissions, so API should be accessible.

 Any ideas why this happens?
",closed,False,2016-08-10 16:19:00,2016-08-26 01:07:00
contrib,foxish,https://github.com/kubernetes/contrib/pull/1505,https://api.github.com/repos/kubernetes/contrib/issues/1505,Added labels yaml file to deployment.,"Looks like https://github.com/kubernetes/contrib/pull/1497 could take a little longer, so, adding a new commandline option to sync labels for now.

cc @lavalamp @bgrant0607 
",closed,True,2016-08-10 18:50:54,2016-08-10 19:41:48
contrib,kokhang,https://github.com/kubernetes/contrib/issues/1506,https://api.github.com/repos/kubernetes/contrib/issues/1506,contrib/service-loadbalancer does not support other type of loadbalancers and has limited L4 support,"Service-loadbalancer is very integrated with HAProxy and it is not designed in a way that it can be easily decoupled.

Service-loadbalancer support for L4 is also very limited. The binding-port needs to be open and specified as a hostPort during the controller creation. This forces users to specify and open the ports at the beginning. 

Because it is using the node port, this will prevent two different services to loadbalance on the same port (ie running two mysql services). 
",closed,False,2016-08-10 20:58:10,2018-02-14 21:20:59
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1507,https://api.github.com/repos/kubernetes/contrib/issues/1507,Figure out readiness probe for nginx ingress that works behind a CP lb,"Flesh out the details discussed in: https://github.com/kubernetes/contrib/pull/1501#discussion-diff-74324489 and update the probes accordingly. 
",closed,False,2016-08-10 21:12:18,2016-09-26 22:44:12
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/1508,https://api.github.com/repos/kubernetes/contrib/issues/1508,Add kubelet-serial-gce-e2e-ci to non-blocking.,"Add kubelet-serial-gce-e2e-ci to non-blocking.

/cc @kubernetes/sig-node 
",closed,True,2016-08-10 22:30:26,2016-08-10 22:58:03
contrib,mml,https://github.com/kubernetes/contrib/issues/1509,https://api.github.com/repos/kubernetes/contrib/issues/1509,Connect the repo to Reviewable,"Please connect this repo at https://reviewable.kubernetes.io/repositories so that all PRs get linked to reviews. Thanks!
",closed,False,2016-08-10 22:46:05,2018-02-14 22:21:54
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1510,https://api.github.com/repos/kubernetes/contrib/issues/1510,Do not retest if retest-not-required is applied,"When a PR hasn't been tested in a while, it is retested by the bot.
This behavior also happens when the PR has the label retest-not-required
or retest-not-required-docs-only, because the label is initially meant
for the submit-queue. Let's not retest if it's not required.
",closed,True,2016-08-11 03:29:53,2016-08-11 07:38:05
contrib,danushkaf,https://github.com/kubernetes/contrib/issues/1511,https://api.github.com/repos/kubernetes/contrib/issues/1511,How to enable access logs in haproxy.,"We tried several things but couldn't get it working. We are using haproxy as a pod.
",closed,False,2016-08-11 05:10:54,2018-02-15 01:24:04
contrib,foxish,https://github.com/kubernetes/contrib/pull/1512,https://api.github.com/repos/kubernetes/contrib/issues/1512,Deploy mongodb replicaset using petsets.,"https://github.com/kubernetes/kubernetes/issues/30289

Initial support for mongodb replicasets.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1512)

<!-- Reviewable:end -->
",closed,True,2016-08-11 06:29:35,2016-09-13 03:29:43
contrib,piosz,https://github.com/kubernetes/contrib/pull/1513,https://api.github.com/repos/kubernetes/contrib/issues/1513,[ca] Factored out common functions in drain logic,,closed,True,2016-08-11 07:39:06,2016-08-11 08:48:43
contrib,gregory-lyons,https://github.com/kubernetes/contrib/pull/1514,https://api.github.com/repos/kubernetes/contrib/issues/1514,git-sync: Fix .git path for worktrees,"@mikedanese @huggsboson

Found a bug in my implementation of atomic pulling for git-sync in https://github.com/kubernetes/contrib/pull/1356.

When using ""git worktree add"" to create a new worktree directory, that worktree directory contains a .git file instead of a .git directory. The .git file is simply a reference to a path within the root .git directory:

```
gitdir: /git/.git/worktrees/rev-...
```

Because that reference path is absolute, any other container that attempts to run a git command inside the worktree will fail if the volume was mounted under a different name. For example, if another container mounts the volume at /repo and then tries to run a git command in one of the worktree directories within /repo, it will fail because the .git file in that worktree will point to a /git path that doesn't exist.

To avoid the bug, we overwrite the .git file in the worktree to use a relative path instead of an absolute path.

```
gitdir: ../.git/worktrees/rev-...
```
",closed,True,2016-08-11 09:04:22,2016-08-11 15:53:46
contrib,piosz,https://github.com/kubernetes/contrib/pull/1515,https://api.github.com/repos/kubernetes/contrib/issues/1515,[ca] Create kube client function,,closed,True,2016-08-11 14:45:57,2016-08-11 15:07:03
contrib,foxish,https://github.com/kubernetes/contrib/issues/1516,https://api.github.com/repos/kubernetes/contrib/issues/1516,[mungegithub] Set up an E2E test for the bot,"With the right use of fakes and mocks, we should be able to setup a comprehensive e2e test for the bot, so that we can make changes more confidently. This should go hand in hand with the canary style of deployment, and a standard deployment plan for the submit-queue.
",closed,False,2016-08-11 17:28:49,2017-01-21 01:24:32
contrib,heroic,https://github.com/kubernetes/contrib/issues/1517,https://api.github.com/repos/kubernetes/contrib/issues/1517,HTTPS Ingress allows http,"I have a inges.yaml defined like so:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: my-app
  annotations:
    kubernetes.io/ingress.allow-http: ""false""
spec:
  tls:
  - secretName: my-app
  rules:
  - host: t.my-app.com
    http:
      paths:
      - backend:
          serviceName: my-app
          servicePort: 80
```

This is as per the readme in the GCE https load balancer example. However, I still see an open http and https port. As seen here:

```
Name:           my-app
Namespace:      default
Address:        130.xxx.xxx.217
Default backend:    default-http-backend:80 (10.4.0.4:8080)
TLS:
  tls-cert terminates 
Rules:
  Host      Path    Backends
  ----      ----    --------
  t.my-app.com  
                backendSrv:80 (<none>)
Annotations:
  https-forwarding-rule:    k8s-fws-default-my-app--3332235f0ec32c08
  https-target-proxy:       k8s-tps-default-my-app--3332235f0ec32c08
  static-ip:            k8s-fw-default-my-app--3332235f0ec32c08
  target-proxy:         k8s-tp-default-my-app--3332235f0ec32c08
  url-map:          k8s-um-default-my-app--3332235f0ec32c08
  backends:         {""k8s-be-31461--3332235f0ec32c08"":""HEALTHY"",""k8s-be-32348--3332235f0ec32c08"":""HEALTHY""}
  forwarding-rule:      k8s-fw-default-my-app--3332235f0ec32c08
No events.
```
",closed,False,2016-08-11 17:37:34,2018-02-15 05:35:23
contrib,eparis,https://github.com/kubernetes/contrib/pull/1518,https://api.github.com/repos/kubernetes/contrib/issues/1518,Use 'do-not-merge' instead of removing 'lgtm' with the release-note process,"This should keep the bot from fighting itself to add and remove LGTM.

We also just stop removing LGTM when people (badly) retrigger a flaky test. That was just punitive...
",closed,True,2016-08-11 20:55:16,2016-08-16 16:53:50
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1519,https://api.github.com/repos/kubernetes/contrib/issues/1519,Added Total PR Count to Header,,closed,True,2016-08-11 21:18:52,2016-08-15 15:23:10
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1520,https://api.github.com/repos/kubernetes/contrib/issues/1520,track flakes in more presubmit jobs,"This turns on flake filing for presubmit jobs that are or soon will be tolerant of flakes.
",closed,True,2016-08-11 23:49:40,2016-08-12 00:13:54
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1521,https://api.github.com/repos/kubernetes/contrib/issues/1521,Submit queue having trouble checking mergability,"causing # in queue to oscillate wildly.

@eparis @apelisse 
",closed,False,2016-08-12 00:17:46,2018-02-14 23:22:56
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1522,https://api.github.com/repos/kubernetes/contrib/issues/1522,Some presubmit test jobs are taking more than 30 minutes,"The difference between taking 38 minutes and 28 minutes is more than 15 merges per day.
",closed,False,2016-08-12 00:20:52,2017-12-20 07:13:53
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1523,https://api.github.com/repos/kubernetes/contrib/issues/1523,Strip Reviewable footer from issue body when merging.,"This keeps commit messages clean.

An example of the current state: https://github.com/kubernetes/kubernetes/commit/78356b53b0c0e8471478e4bdd71c142605d3a196
",closed,True,2016-08-12 05:00:00,2016-08-12 17:03:56
contrib,piosz,https://github.com/kubernetes/contrib/pull/1524,https://api.github.com/repos/kubernetes/contrib/issues/1524,[rescheduler] Bumped CA godeps,,closed,True,2016-08-12 07:44:31,2016-08-12 08:09:00
contrib,scheeles,https://github.com/kubernetes/contrib/issues/1525,https://api.github.com/repos/kubernetes/contrib/issues/1525,Nginx ingress with https,"The nginx ingress don't accept the same tls format as the GCE.

For nginx the host must be every time specified min the tls definition:

```
spec:
  tls:
  - hosts:
    - foo.bar.com
    secretName: foo-secret
```

this ingress don't serve https on nginx:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: no-rules-map
spec:
  tls:
    - secretName: testsecret
  backend:
    serviceName: s1
    servicePort: 80
```

but it work for GKE and also mention here http://kubernetes.io/docs/user-guide/ingress/
",closed,False,2016-08-12 09:55:39,2018-02-15 03:26:02
contrib,piosz,https://github.com/kubernetes/contrib/pull/1526,https://api.github.com/repos/kubernetes/contrib/issues/1526,[rescheduler] Implemented main control loop,"ref https://github.com/kubernetes/kubernetes/issues/29023

cc @davidopp @wojtek-t

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1526)

<!-- Reviewable:end -->
",closed,True,2016-08-12 11:45:45,2016-08-17 08:30:04
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1527,https://api.github.com/repos/kubernetes/contrib/issues/1527,Created a top level playbooks dir and scripts dir,"Polish https://github.com/kubernetes/contrib/pull/967 work done by Steve Louie. Thanks for your effort starting the effort.
",closed,True,2016-08-12 12:11:51,2016-08-12 13:18:19
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1528,https://api.github.com/repos/kubernetes/contrib/issues/1528,Make IsMergeable retry up to 5 times with exponential delays. (2-32s),"This should fix the bouncing in the submit queue where the
mergeability of a PR is indeterminate after each round of merging.
I think Github is just taking longer than 2 seconds to respond.
",closed,True,2016-08-12 16:45:39,2016-08-12 17:43:57
contrib,yujuhong,https://github.com/kubernetes/contrib/issues/1529,https://api.github.com/repos/kubernetes/contrib/issues/1529,merge bot skipping many PRs: Unable to determine is PR is mergeable,"http://submit-queue.k8s.io/#/history shows tens of PRs with the ""Unable to determine is PR is mergeable"" message.

E.g.,

```
#30401: Let load and density e2e tests use GC if it's on
Unable to determine is PR is mergeable. Will try again later.
Aug 12, 2016 10:55:24 AM
deads2k
#30383: speed up RC scaler
Unable to determine is PR is mergeable. Will try again later.
Aug 12, 2016 10:55:20 AM
bbreslauer
#30372: Add user-specified kubectl arguments to addons start script
Unable to determine is PR is mergeable. Will try again later.
Aug 12, 2016 10:55:17 AM
dims
#30366: Validate SHA/Tag when checking docker images
Unable to determine is PR is mergeable. Will try again later.
Aug 12, 2016 10:55:14 AM
janetkuo
#30327: Use unversioned client in scheduledjobs and set group version to batch/v2alpha1
Unable to determine is PR is mergeable. Will try again later.
Aug 12, 2016 10:55:10 AM
ardnaxelarak
#30247: Make more messages respect --quiet flag
Unable to determine is PR is mergeable. Will try again later.
Aug 12, 2016 10:55:07 AM
liggitt
#30246: Set user info in CertificateSigningRequest.Spec on create
Unable to determine is PR is mergeable. Will try again later.
Aug 12, 2016 10:55:03 AM
```

The submit queue ended up having only one p3 PR in the queue.

/cc @eparis 
",closed,False,2016-08-12 18:01:03,2016-08-12 18:46:56
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1530,https://api.github.com/repos/kubernetes/contrib/issues/1530,Add new way to match events and comments,"Each mungers is looking for comments and events all the time. Let's try
to create an easier way to find these.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1530)

<!-- Reviewable:end -->
",closed,True,2016-08-12 18:54:55,2016-08-17 21:30:13
contrib,timothysc,https://github.com/kubernetes/contrib/pull/1531,https://api.github.com/repos/kubernetes/contrib/issues/1531,Add exclusion for HA.yaml files as they are part of core tests that need to be editable.,"My regex is rusty, hopefully I got it right. 
/cc @eparis 
",closed,True,2016-08-12 20:06:56,2016-08-15 17:06:05
contrib,foxish,https://github.com/kubernetes/contrib/pull/1532,https://api.github.com/repos/kubernetes/contrib/issues/1532,Updated readme with instructions to run locally.,"Added instructions to run/test locally as several people seem to be having trouble with testing their changes.
",closed,True,2016-08-12 20:20:47,2016-08-12 20:44:03
contrib,silasbw,https://github.com/kubernetes/contrib/pull/1533,https://api.github.com/repos/kubernetes/contrib/issues/1533,service-loadbalancer Dockerfile: replace wget usage with curl,"wget looks like it's missing from gcr.io/google_containers/haproxy:0.3

```
[service-loadbalancer:master]$ docker run -ti gcr.io/google_containers/haproxy:0.3 wget
exec: ""wget"": executable file not found in $PATH
Error response from daemon: Cannot start container 5a99d4f99495c74f57045b7926e7211e1f1da1d2db7d7b79be6d69f055d00c04: [8] System error: exec: ""wget"": executable file not found in $PATH
[service-loadbalancer:master]$ docker run -ti gcr.io/google_containers/haproxy:0.3 bash
root@3437d156cc7d:/# find . -name wget
root@3437d156cc7d:/#
```
",closed,True,2016-08-12 23:43:36,2016-08-13 00:03:35
contrib,silasbw,https://github.com/kubernetes/contrib/pull/1534,https://api.github.com/repos/kubernetes/contrib/issues/1534,service-loadbalancer Makefile tweaks,"- change `server` target to `service_loadbalancer` and add prerequisites
- conditionally assign TAG, PREFIX, and GCLOUD
- make `container` and `push` PHONY
- `rm -f service_loadbalancer` in `clean`

Original `make` behavior is the same, except that `make` will not build
`service_loadbalancer` unless there are updates to service_loadbalancer.go or
loadbalancer_log.go since the last build.

Facilitates usage by non GCP users. E.g., `PREFIX=silasbw/servicelb GCLOUD= make`
",closed,True,2016-08-13 05:12:19,2016-08-13 05:53:36
contrib,tyranron,https://github.com/kubernetes/contrib/issues/1535,https://api.github.com/repos/kubernetes/contrib/issues/1535,Is --default-backend-service really required for Nginx Ingress Controller?,"Why `--default-backend-service` flag for `nginx-ingress-controller` is required?
Couldn't we just serve nginx static 404 page by default if no default backend is provided?
Idea to deploy and expose one more service in cluster just for simple 404 page is pretty strange. Do I miss something?
",closed,False,2016-08-13 13:17:29,2019-04-01 13:03:05
contrib,foxish,https://github.com/kubernetes/contrib/pull/1536,https://api.github.com/repos/kubernetes/contrib/issues/1536,Specifying aliases in OWNERs files,"--alias-file option can specify team aliases of the following kind:

```
aliases:
  team/t1:
    - team/t2
    - u2
  team/t2:
    - team/t3
    - u3
  team/t3:
    - u4
```

This should let us use OWNERs files better with an extra level of indirection.
Things that need further thought:
1. Do we want to support nested team definitions like above? (If yes, what do we do if there are cyclic loops?)
2. Should the yaml file containing aliases be within the repository, or be globally consistent and supplied to mungegithub at the time of invocation?

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1536)

<!-- Reviewable:end -->
",closed,True,2016-08-13 19:01:26,2016-08-18 23:49:11
contrib,roberthbailey,https://github.com/kubernetes/contrib/pull/1537,https://api.github.com/repos/kubernetes/contrib/issues/1537,Update the scale-demo readme for Kubernetes 1.2.0.,"A bit out of date, now that 1.3 has been out for a while, but had this change sitting around so I thought I'd send it. 
",closed,True,2016-08-14 18:22:09,2016-08-16 02:23:12
contrib,roberthbailey,https://github.com/kubernetes/contrib/pull/1538,https://api.github.com/repos/kubernetes/contrib/issues/1538,Add a simple script to scale up a cluster,"Also change the html headings to ""Scale Demo"" since we now go well beyond 1M QPS.
",closed,True,2016-08-14 19:08:00,2016-08-16 02:35:13
contrib,hongchaodeng,https://github.com/kubernetes/contrib/pull/1539,https://api.github.com/repos/kubernetes/contrib/issues/1539,lgtm handler: auto add/remove lgtm label,"This is to follow-up on #1428.

First of all, the previous race has been fixed in https://github.com/kubernetes/contrib/pull/1518. Thus this PR can add back the functionality of adding lgtm label. A reviewer commenting with only ""/lgtm"" will do it.
Second, this PR also adds the ability to remove lgtm label. A reviewer commenting with only ""/lgtm cancel"" will do it.

When both ""/lgtm"" and ""/lgtm cancel"" exist, it will find which is the latest created comment.
",closed,True,2016-08-14 19:56:10,2016-08-16 17:05:53
contrib,yogev-stratoscale,https://github.com/kubernetes/contrib/pull/1540,https://api.github.com/repos/kubernetes/contrib/issues/1540,Yogev/add strato makefile,"@shay-stratoscale
",closed,True,2016-08-15 12:06:32,2016-08-15 12:07:26
contrib,caesarxuchao,https://github.com/kubernetes/contrib/pull/1541,https://api.github.com/repos/kubernetes/contrib/issues/1541,Add a publisher that publishes the staging client to its own repository daily,"You can see the sample commits here: https://github.com/caesarxuchao/client-go/commits/master (see last few commits)
@lavalamp @foxish

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1541)

<!-- Reviewable:end -->
",closed,True,2016-08-15 17:03:12,2016-08-16 22:50:02
contrib,Pensu,https://github.com/kubernetes/contrib/issues/1542,https://api.github.com/repos/kubernetes/contrib/issues/1542,Not able to build nginx controller,"Hi, I am trying to build nginx controller and here is the error I am getting:

```
# make controller
rm -f nginx-ingress-controller
CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags \
    ""-s -w -X main.version=git-d192df3 -X main.gitRepo=https://github.com/kubernetes/contrib.git"" \
    -o nginx-ingress-controller
# _/root/contrib/ingress/controllers/nginx
./controller.go:139: cannot use kubeClient (type *""k8s.io/kubernetes/pkg/client/unversioned"".Client) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/unversioned"".Client in argument to nginx.NewManager
./controller.go:451: cannot use cfg (type *""k8s.io/kubernetes/pkg/api"".ConfigMap) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/api"".ConfigMap in argument to lbc.nginx.ReadConfig
./controller.go:697: cannot use lbc.client (type *""k8s.io/kubernetes/pkg/client/unversioned"".Client) as type ""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/unversioned"".Interface in argument to auth.ParseAnnotations:
    *""k8s.io/kubernetes/pkg/client/unversioned"".Client does not implement ""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/unversioned"".Interface (wrong type for Autoscaling method)
        have Autoscaling() ""k8s.io/kubernetes/pkg/client/unversioned"".AutoscalingInterface
        want Autoscaling() ""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/unversioned"".AutoscalingInterface
./controller.go:697: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to auth.ParseAnnotations
./controller.go:703: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to ""k8s.io/contrib/ingress/controllers/nginx/nginx/ratelimit"".ParseAnnotations
./controller.go:709: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to secureupstream.ParseAnnotations
./controller.go:714: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to rewrite.ParseAnnotations
./controller.go:719: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to ipwhitelist.ParseAnnotations
./controller.go:818: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to healthcheck.ParseAnnotations
./main.go:97: cannot use flags (type *""github.com/spf13/pflag"".FlagSet) as type *""k8s.io/kubernetes/vendor/github.com/spf13/pflag"".FlagSet in argument to util.DefaultClientConfig
./main.go:97: too many errors
make: *** [controller] Error 2

```

Any idea what could be wrong here? Can you please help me resolve it? TIA. 
",closed,False,2016-08-16 08:56:35,2016-08-18 10:23:20
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1543,https://api.github.com/repos/kubernetes/contrib/issues/1543,Fix relative paths to init directory and k8s build,"When deploying locally built kubernetes, systemd service files and binaries are one level up.
Due to changes of top directory layout another level needs to be added.
",closed,True,2016-08-16 10:25:31,2016-08-16 12:41:23
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/1544,https://api.github.com/repos/kubernetes/contrib/issues/1544,Validate SSL certificates when downloading binaries,"Kubernetes, flannel, etcd and possibly other component have their corresponding binaries downloaded without SSL certs validation. The validation should be at least configurable. Opening the issue so we can talk about it and solve it eventually. E.g.

``` yaml
- name: Download Kubernetes binaries
  get_url:
    url: ""{{ kube_download_url_base }}/{{ item }}""
    dest: ""{{ kube_current_release_directory }}/{{ item }}""
    mode: 0755
    validate_certs: False
```
",closed,False,2016-08-16 16:24:58,2018-02-15 04:27:03
contrib,foxish,https://github.com/kubernetes/contrib/pull/1545,https://api.github.com/repos/kubernetes/contrib/issues/1545,Turning up lgtm-handler on contrib,"Now that https://github.com/kubernetes/contrib/pull/1539 has been merged, we can run the lgtm handler as a test on contrib before turning it up on the main repository.
",closed,True,2016-08-16 17:17:12,2016-08-16 17:44:32
contrib,foxish,https://github.com/kubernetes/contrib/issues/1546,https://api.github.com/repos/kubernetes/contrib/issues/1546,Potential race: lgtm_handler and lgtm_after_commit,"Before deploying https://github.com/kubernetes/contrib/pull/1539, I want to ensure that we have no races with the bot fighting against itself and exhausting quota.

It appears that it could happen in the following situation:
- Someone comments `/lgtm`.
- The bot adds the `lgtm` label.
- The author pushes new commits.

What happens:
- The lgtm_handler munger continues to see the comment and tries to apply the label. 
- The lgtm_after_commit munger tries to remove the lgtm label.

cc @eparis @apelisse @lavalamp 
",closed,False,2016-08-16 17:50:56,2016-08-17 23:16:11
contrib,foxish,https://github.com/kubernetes/contrib/pull/1547,https://api.github.com/repos/kubernetes/contrib/issues/1547,"Revert ""Turning up lgtm-handler on contrib""","Reverts kubernetes/contrib#1545

Issue https://github.com/kubernetes/contrib/issues/1546 needs to be addressed first before we can do this.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1547)

<!-- Reviewable:end -->
",closed,True,2016-08-16 18:42:50,2016-08-17 19:00:09
contrib,foxish,https://github.com/kubernetes/contrib/issues/1548,https://api.github.com/repos/kubernetes/contrib/issues/1548,Fix the way we fetch last modified time,"We should be fetching the time of the last push, [not the last commit](https://github.com/kubernetes/contrib/blob/aaec42bbee650c5714faa47b70d63be450cd884d/mungegithub/github/github.go#L573).
",closed,False,2016-08-16 21:29:35,2018-02-15 02:25:04
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1549,https://api.github.com/repos/kubernetes/contrib/issues/1549,Pre-making the feature freeze settings change PR,"We will merge & push this on Monday (8/22) morning. At that point, only things with the v1.4 milestone on them will be allowed in the queue.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1549)

<!-- Reviewable:end -->
",closed,True,2016-08-16 21:46:37,2016-08-23 16:53:11
contrib,foxish,https://github.com/kubernetes/contrib/pull/1550,https://api.github.com/repos/kubernetes/contrib/issues/1550,Fixes race between lgtm-handler and lgtm-after-commit & adds tests.,"Fixes #1546
- Adds tests for lgtm_handler.
- Change the message posted by lgtm-after-commit when removing LGTM.

cc @lavalamp @eparis

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1550)

<!-- Reviewable:end -->
",closed,True,2016-08-17 00:06:19,2016-08-17 23:16:12
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1551,https://api.github.com/repos/kubernetes/contrib/issues/1551,Temporarily bring bring back workaround agains changed gce config format.,"It was accidentally removed by #1399. 

cc: @fgrzadkowski @piosz @jszczepkowski

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1551)

<!-- Reviewable:end -->
",closed,True,2016-08-17 09:45:37,2016-08-17 10:20:06
contrib,andrewsykim,https://github.com/kubernetes/contrib/pull/1552,https://api.github.com/repos/kubernetes/contrib/issues/1552,AWS Cluster Autoscaler README,"under https://github.com/kubernetes/contrib/issues/1311

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1552)

<!-- Reviewable:end -->
",closed,True,2016-08-17 20:05:08,2016-08-25 10:43:05
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1553,https://api.github.com/repos/kubernetes/contrib/issues/1553,Add mungegithub to kubernetes/test-infra,"Make sure the mungegithub is running for test-infra repository.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1553)

<!-- Reviewable:end -->
",closed,True,2016-08-17 20:52:03,2016-08-19 21:29:20
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1554,https://api.github.com/repos/kubernetes/contrib/issues/1554,Add needs-rebase munger to contrib,"This is missing. And frustrating.
",closed,True,2016-08-17 21:36:47,2016-08-17 21:43:17
contrib,foxish,https://github.com/kubernetes/contrib/pull/1555,https://api.github.com/repos/kubernetes/contrib/issues/1555,Turning up the lgtm-handler munger on contrib.,"Finally, it is time to turn it on and see if it works right. :)
",closed,True,2016-08-17 22:13:48,2016-08-17 23:10:15
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1556,https://api.github.com/repos/kubernetes/contrib/issues/1556,Add Feedback to Issue Categorizer,"Detect when a label added by the bot was later changed by a human being.  Use that information to update the model.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1556)

<!-- Reviewable:end -->
",closed,True,2016-08-17 22:42:46,2016-08-23 23:53:15
contrib,foxish,https://github.com/kubernetes/contrib/pull/1557,https://api.github.com/repos/kubernetes/contrib/issues/1557,Adding instructions to turn up a new submit queue,"These steps are required when starting up a new submit-queue instance.
",closed,True,2016-08-18 00:00:46,2016-08-19 02:39:12
contrib,foxish,https://github.com/kubernetes/contrib/pull/1558,https://api.github.com/repos/kubernetes/contrib/issues/1558,[Do-not-merge] Testing /lgtm comments,"Testing the various mungers.
Please ignore this PR.
",closed,True,2016-08-18 00:03:17,2016-08-18 00:48:39
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/1559,https://api.github.com/repos/kubernetes/contrib/issues/1559,Ingress nginx controller fail to start after upgrading k8s from 1.2 to 1.3.2,"When I create the ingress rc, I get the error below from describing the nginx controller pod:
Error syncing pod, skipping: failed to ""StartContainer"" for ""nginx-ingress-lb"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-k8nmm_default(510d6c6f-6458-11e6-9396-00505697649c)""

My new k8s version info:
Client Version: version.Info{Major:""1"", Minor:""3"", GitVersion:""v1.3.2"", GitCommit:""9bafa3400a77c14ee50782bb05f9efc5c91b3185"", GitTreeState:""clean"", BuildDate:""2016-07-17T18:30:39Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""3"", GitVersion:""v1.3.2"", GitCommit:""9bafa3400a77c14ee50782bb05f9efc5c91b3185"", GitTreeState:""clean"", BuildDate:""2016-07-17T18:23:58Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}

This issue is similar to #752.
",closed,False,2016-08-18 05:27:22,2018-02-15 03:26:03
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1560,https://api.github.com/repos/kubernetes/contrib/issues/1560,Cluster-autoscaler: fix belongs check in gce cloud provider,,closed,True,2016-08-18 08:16:22,2016-08-18 08:36:13
contrib,Pensu,https://github.com/kubernetes/contrib/issues/1561,https://api.github.com/repos/kubernetes/contrib/issues/1561,"Ingress is up, but not able to connect to it.","Hi,

I am running nginx ingress controller as given in the example:

```

# kubectl get ing
NAME      RULE          BACKEND   ADDRESS           AGE
echomap   -                       192.168.122.120   3h
          foo.bar.com   
          /foo          echoheaders-x:80
          bar.baz.com   
          /bar          echoheaders-y:80
          /foo          echoheaders-x:80
```

But when I try to access it, I get the following response:

```

# curl -v http://192.168.122.120:80/foo -H 'Host: foo.bar.com'
* About to connect() to 192.168.122.120 port 80 (#0)
*   Trying 192.168.122.120...
* Connected to 192.168.122.120 (192.168.122.120) port 80 (#0)
> GET /foo HTTP/1.1
> User-Agent: curl/7.29.0
> Accept: */*
> Host: foo.bar.com
> 
< HTTP/1.1 404 Not Found
< Server: nginx/1.10.1
< Date: Thu, 18 Aug 2016 10:25:20 GMT
< Content-Type: text/html
< Content-Length: 169
< Connection: keep-alive
< 
<html>
<head><title>404 Not Found</title></head>
<body bgcolor=""white"">
<center><h1>404 Not Found</h1></center>
<hr><center>nginx/1.10.1</center>
</body>
</html>
* Connection #0 to host 192.168.122.120 left intact
```

All the pods are up and running in my cluster:

```
# kubectl get pods
NAME                             READY     STATUS    RESTARTS   AGE
default-http-backend-25vlo       1/1       Running   0          3h
echoheaders-tn8l4                1/1       Running   0          2h
nginx-ingress-controller-igs9i   1/1       Running   0          2h
```

Any idea what am I doing wrong here? Or how to resolve it? TIA.
",closed,False,2016-08-18 10:23:40,2016-10-24 06:02:57
contrib,piosz,https://github.com/kubernetes/contrib/pull/1562,https://api.github.com/repos/kubernetes/contrib/issues/1562,[rescheduler] Added Dockerfile,,closed,True,2016-08-18 11:41:18,2016-08-18 12:03:45
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1563,https://api.github.com/repos/kubernetes/contrib/issues/1563,switch queue tie-break to first LGTM time,"Fixes #1286
",closed,True,2016-08-18 22:16:58,2016-08-19 03:09:13
contrib,philk,https://github.com/kubernetes/contrib/pull/1564,https://api.github.com/repos/kubernetes/contrib/issues/1564,autoscaler: Remove minimum size requirement,"There are use cases (data processing nodes) where an instance group
would need to be zero most of the time and only scale up on demand.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1564)

<!-- Reviewable:end -->
",closed,True,2016-08-18 22:30:55,2016-09-16 09:24:00
contrib,harryge00,https://github.com/kubernetes/contrib/pull/1565,https://api.github.com/repos/kubernetes/contrib/issues/1565,fix bug in fluentd-es-logging sidecar,"`filename=$(basename $filepath)` will cause problem if multiple files are of the same base name, 
e.g. 
`""/mnt/log1/synthetic-dates.log /mnt/log2/synthetic-dates.log""` are two different files, but there will be only one `/etc/td-agent/files/synthetic-dates.log` indicating fluentd to collect logs from  `/mnt/log2/synthetic-dates.log`.

In my case, the config under /etc/td-agent/files become like `/etc/td-agent/files/_mnt_log1_synthetic-dates.log`

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1565)

<!-- Reviewable:end -->
",closed,True,2016-08-18 23:34:21,2017-12-29 02:38:39
contrib,timstclair,https://github.com/kubernetes/contrib/issues/1566,https://api.github.com/repos/kubernetes/contrib/issues/1566,retest-not-required-docs-only should count images as docs,"E.g. the label was removed from [this PR](https://github.com/kubernetes/kubernetes/pull/30840) since it includes several png's referenced in the docs.
",closed,False,2016-08-18 23:52:10,2017-02-10 17:29:47
contrib,harryge00,https://github.com/kubernetes/contrib/pull/1567,https://api.github.com/repos/kubernetes/contrib/issues/1567,allow logroate when fluentd-sidecar collecting logs,"When collecting logs, we may want logs be rotated to avoid becoming too large.
Users can defile env variables ""FILES_TO_ROTATE"" to declare files to be rotated, and their ""SIZE_LIMIT"", ""ROTATE_TIMES"".

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1567)

<!-- Reviewable:end -->
",closed,True,2016-08-19 01:49:18,2017-12-29 02:38:56
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1568,https://api.github.com/repos/kubernetes/contrib/issues/1568,Consider .png files as doc and don't rerun tests,"Fixes #1566 
",closed,True,2016-08-19 02:58:52,2016-08-19 21:10:32
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1569,https://api.github.com/repos/kubernetes/contrib/issues/1569,Merge queue is a bit aggressive about removing comments,"```
I0819 03:14:24.369792       1 github.go:1664] Removing comment 240911058 from Issue 26926. Author:k8s-bot Body:""GCE e2e build/test **failed** for commit c55e20e0a29ff116c85289dfc932500eea7f783e.\n* [Test Results](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/26926/kubernetes-pull-build-test-e2e-gce/54561)\n* [Build Log](http://pr-test.k8s.io/26926/kubernetes-pull-build-test-e2e-gce/54561/build-log.txt)\n* [Test Artifacts](https://console.developers.google.com/storage/browser/kubernetes-jenkins/pr-logs/pull/26926/kubernetes-pull-build-test-e2e-gce/54561/artifacts/)\n* [Internal Jenkins Results](http://goto.google.com/prkubekins/job/kubernetes-pull-build-test-e2e-gce//54561)\n\nPlease reference the [list of currently known flakes](https://github.com/kubernetes/kubernetes/issues?q=is:issue+label:kind/flake+is:open) when examining this failure. If you request a re-test, you must reference the issue describing the flake.\n\n""
```

Seen in logs, why did it delete this comment?
",closed,False,2016-08-19 03:17:27,2016-08-19 17:00:19
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1570,https://api.github.com/repos/kubernetes/contrib/issues/1570,Remove most common log message.,,closed,True,2016-08-19 03:35:57,2016-08-19 03:59:14
contrib,kayrus,https://github.com/kubernetes/contrib/pull/1571,https://api.github.com/repos/kubernetes/contrib/issues/1571,ingress: use POD_NAMESPACE as a namespace in cli parameters,"When you deploy ingress not into `default` namespace, ingress RC fails with the `no service with name default/default-http-backend found: services ""default-http-backend"" not found` error message.

This fix uses `POD_NAMESPACE` which we already pass into the pod ENV.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1571)

<!-- Reviewable:end -->
",closed,True,2016-08-19 08:28:20,2016-08-26 23:11:52
contrib,kayrus,https://github.com/kubernetes/contrib/issues/1572,https://api.github.com/repos/kubernetes/contrib/issues/1572,ingress: update an nginx ingress image version,"This PR https://github.com/kubernetes/contrib/pull/1467 changed default healthz port. So now examples for Ingress don't work because of incorrect healthz checks.

There are two workarounds:
- build a new  `gcr.io/google_containers/nginx-ingress-controller` image  with new controller version which has a new default healthz port and set its tag to 0.8.3
- explicitly set `--healthz-port=10254` in all ingress examples

/cc @aledbf @bprashanth
",closed,False,2016-08-19 11:25:59,2016-08-22 18:33:31
contrib,valentin-krasontovitsch,https://github.com/kubernetes/contrib/issues/1573,https://api.github.com/repos/kubernetes/contrib/issues/1573,Write etcd config file fails: no ipv4 address for eth1,"Using contrib/ansible, trying to setup a k8s cluster
-  with Vagrant (1.8.5), 
- using centos7 as OS, 
- VirtualBox (Manager 5.0.26) as provider and 
- running on OS X El Capitan, 

running `vagrant up` produces the following error message:

```
TASK [etcd : Write etcd config file] *******************************************
fatal: [kube-master-1]: FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""msg"": ""AnsibleUndefinedVariable: {{ etcd_peer_url_scheme }}:// \
        {{ etcd_machine_address }}:{{ etcd_peer_port }}: \
        {{ hostvars[inventory_hostname]['ansible_' + etcd_interface].ipv4.address }}: \
        'dict object' has no attribute 'ipv4'""}
```

Upon more precise inspection of the task, it turns out that a template file is used, which tries to access `hostvars[kube-master-1][eth1].ipv4.address`. Upon debugging the corresponding `hostvars`, I observed that that interface does not have an ipv4 section.

To get this far I had to do the following edits of `contrib/ansible/vagrant/Vagrantfile`:
- comment out `# require 'vagrant-aws'`
- add `source_type: 'packageManager',` in `ansible.extra_vars = { ... }`
- configure virtual machine to use local proxy by adding line `config.proxy.http = ENV['http_proxy']` (using vagrant-proxyconf plugin)

Hence, in order to reproduce, the last and first step might be unnecessary.
",closed,False,2016-08-19 13:52:28,2018-02-19 17:14:00
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1574,https://api.github.com/repos/kubernetes/contrib/issues/1574,[ubuntu-slim] Update ubuntu image,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1574)

<!-- Reviewable:end -->
",closed,True,2016-08-19 14:37:14,2016-08-22 18:46:03
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1575,https://api.github.com/repos/kubernetes/contrib/issues/1575,[nginx-slim] Update nginx to 1.11.3,,closed,True,2016-08-19 14:37:55,2016-08-19 16:39:19
contrib,jmhodges,https://github.com/kubernetes/contrib/issues/1576,https://api.github.com/repos/kubernetes/contrib/issues/1576,add HTTPS to backend services to GCE Ingress contoller,"It would be nice to support HTTPS from the GCE Ingress controller to the backend services it fronts. The Google Load Balancers currently support HTTPS healthchecks and traffic.

I didn't see a ticket for tracking this work already, and so made this one.
",closed,False,2016-08-19 14:40:32,2018-02-15 04:27:03
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1577,https://api.github.com/repos/kubernetes/contrib/issues/1577,[nginx-ingress-controller] Release 0.8.3,"fixes #1572

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1577)

<!-- Reviewable:end -->
",closed,True,2016-08-19 14:45:00,2016-08-22 19:00:01
contrib,piosz,https://github.com/kubernetes/contrib/pull/1578,https://api.github.com/repos/kubernetes/contrib/issues/1578,[rescheduler] Fixed few issues in rescheduler,"1. Wait 2 minutes after rescheduler is created in order to
   make sure that all addons has a chance to start.
2. Operate on copy of a node (without taint) while doing simulations.
3. Correctly handle iteration over PodList.
",closed,True,2016-08-19 15:09:25,2016-08-22 07:23:49
contrib,tmrts,https://github.com/kubernetes/contrib/issues/1579,https://api.github.com/repos/kubernetes/contrib/issues/1579,Submit-Queue race condition,"See the comments by the bot for context in kubernetes/kubernetes#30996

After further examination, it seems to me that the problem is `retest-not-required`. When used, there is a race condition between merging and checking the PR. Here, the commit was merged immediately, but the PR wasn't closed so the bot saw a conflict because the commit was already merged and applied the label `needs-rebase`

cc @lavalamp @spxtr @kubernetes/test-infra-maintainers
",closed,False,2016-08-19 20:15:05,2017-12-17 21:05:01
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1580,https://api.github.com/repos/kubernetes/contrib/issues/1580,Consider .dia and .svg as documentation,"Follow-up on #1566

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1580)

<!-- Reviewable:end -->
",closed,True,2016-08-19 20:47:27,2016-08-19 22:19:22
contrib,foxish,https://github.com/kubernetes/contrib/issues/1581,https://api.github.com/repos/kubernetes/contrib/issues/1581,Bot does not post message when /lgtm is not honored,"https://github.com/kubernetes/contrib/pull/1568

@eparis commented with /LGTM twice but the bot didn't do anything since he was not an assignee. 
We need to make sure that the bot posts a message when such an event occurs and notifies the current assignee of it.
",closed,False,2016-08-19 21:13:27,2016-11-11 21:14:11
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1582,https://api.github.com/repos/kubernetes/contrib/issues/1582,Create Human/Bot interaction tools,"This is defining what will be used to parse commands (message from human
to robot) and notifications (message from robot to human).

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1582)

<!-- Reviewable:end -->
",closed,True,2016-08-19 23:47:08,2016-08-25 02:38:10
contrib,ixdy,https://github.com/kubernetes/contrib/issues/1583,https://api.github.com/repos/kubernetes/contrib/issues/1583,"""tests pending"" checks are buggy","Two separate issues that I've noticed the munger getting confused by:
- tests were broken at some point a few days ago. [I request a re-test of just that particular build](https://github.com/kubernetes/kubernetes/pull/30551#issuecomment-241163328), and then the munger [thinks tests have been pending for over 24h](https://github.com/kubernetes/kubernetes/pull/30551#issuecomment-241164346).
- someone LGTM's a PR before all tests have finished running for the first time. The munger will comment ""
  pr builder appears to be missing, activating due to 'lgtm' label."" and retest, which may cause some builds to needlessly rerun.

cc @lavalamp @eparis @apelisse 
",closed,False,2016-08-20 00:48:15,2018-01-16 23:35:37
contrib,mjbright,https://github.com/kubernetes/contrib/issues/1584,https://api.github.com/repos/kubernetes/contrib/issues/1584,Attempt to use ansible/vagrant fails: AttributeError: 'module' object has no attribute 'selinux_snapperd_contexts_path',"I'm trying to use https://github.com/kubernetes/contrib/blob/master/ansible/vagrant/Vagrantfile
I commented out requires:

```
    #require 'vagrant-openstack-provider'
    #require 'vagrant-aws'
```

Attempts to use ""vagrant up"" or ""vagrant provision"" finish with an selinux error - see below.

**Note:** I am running VirtualBox 5.1.4r110228, Vagrant 1.8.1 on Fedora 24.

> vagrant provision
> ==> kube-node-2: Running provisioner: ansible...
>     kube-node-2: Running ansible-playbook...
> ERROR! Unexpected Exception: 'module' object has no attribute 'selinux_snapperd_contexts_path'
> the full traceback was:

Traceback (most recent call last):
  File ""/usr/bin/ansible-playbook"", line 79, in <module>
    mycli = getattr(**import**(""ansible.cli.%s"" % sub, fromlist=[myclass]), myclass)
  File ""/usr/lib/python2.7/site-packages/ansible/cli/playbook.py"", line 30, in <module>
    from ansible.executor.playbook_executor import PlaybookExecutor
  File ""/usr/lib/python2.7/site-packages/ansible/executor/playbook_executor.py"", line 27, in <module>
    from ansible.executor.task_queue_manager import TaskQueueManager
  File ""/usr/lib/python2.7/site-packages/ansible/executor/task_queue_manager.py"", line 28, in <module>
    from ansible.executor.play_iterator import PlayIterator
  File ""/usr/lib/python2.7/site-packages/ansible/executor/play_iterator.py"", line 29, in <module>
    from ansible.playbook.block import Block
  File ""/usr/lib/python2.7/site-packages/ansible/playbook/**init**.py"", line 25, in <module>
    from ansible.playbook.play import Play
  File ""/usr/lib/python2.7/site-packages/ansible/playbook/play.py"", line 27, in <module>
    from ansible.playbook.base import Base
  File ""/usr/lib/python2.7/site-packages/ansible/playbook/base.py"", line 35, in <module>
    from ansible.parsing.dataloader import DataLoader
  File ""/usr/lib/python2.7/site-packages/ansible/parsing/dataloader.py"", line 35, in <module>
    from ansible.parsing.yaml.loader import AnsibleLoader
  File ""/usr/lib/python2.7/site-packages/ansible/parsing/yaml/loader.py"", line 30, in <module>
    from ansible.parsing.yaml.constructor import AnsibleConstructor
  File ""/usr/lib/python2.7/site-packages/ansible/parsing/yaml/constructor.py"", line 25, in <module>
    from ansible.vars.unsafe_proxy import wrap_var
  File ""/usr/lib/python2.7/site-packages/ansible/vars/**init**.py"", line 39, in <module>
    from ansible.inventory.host import Host
  File ""/usr/lib/python2.7/site-packages/ansible/inventory/**init**.py"", line 33, in <module>
    from ansible.inventory.dir import InventoryDirectory, get_file_parser
  File ""/usr/lib/python2.7/site-packages/ansible/inventory/dir.py"", line 32, in <module>
    from ansible.inventory.script import InventoryScript
  File ""/usr/lib/python2.7/site-packages/ansible/inventory/script.py"", line 33, in <module>
    from ansible.module_utils.basic import json_dict_bytes_to_unicode
  File ""/usr/lib/python2.7/site-packages/ansible/module_utils/basic.py"", line 147, in <module>
    import selinux
  File ""/usr/lib64/python2.7/site-packages/selinux/**init**.py"", line 1441, in <module>
    selinux_snapperd_contexts_path = _selinux.selinux_snapperd_contexts_path
AttributeError: 'module' object has no attribute 'selinux_snapperd_contexts_path'
Ansible failed to complete successfully. Any error output should be
visible above. Please fix these errors and try again.
",closed,False,2016-08-20 21:41:07,2016-09-15 17:29:36
contrib,itajaja,https://github.com/kubernetes/contrib/issues/1585,https://api.github.com/repos/kubernetes/contrib/issues/1585,brittle rolling update,"I'd like to raise some doubts about [this line](https://github.com/kubernetes/contrib/blob/master/continuousdelivery/deploy/deploy-service.sh#L82):

```
~/.kube/kubectl rolling-update ${SERVICENAME} --image=${DOCKER_REGISTRY}/${CONTAINER1}:latest || true
```

1) why use latest instead of `${BUILD}`? The [docs actually say](http://kubernetes.io/docs/user-guide/rolling-updates/#updating-the-container-image) that this approach shouldn't even work
2) If rolling-update is done this way, the actual rc and template [will contain a wrong build number](https://github.com/kubernetes/contrib/blob/master/continuousdelivery/kubeyaml/myserviceexample.yaml#L26), and I think it's confusing

Am I missing something? Thanks for the awesome work :)
",closed,False,2016-08-21 21:21:17,2018-02-15 04:27:04
contrib,piosz,https://github.com/kubernetes/contrib/pull/1586,https://api.github.com/repos/kubernetes/contrib/issues/1586,[rescheduler] Fixed taint format,"Kubernetes doesn't support `/` in annotations.
",closed,True,2016-08-22 09:29:31,2016-08-22 09:53:50
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1587,https://api.github.com/repos/kubernetes/contrib/issues/1587,make insecure-bind-address of apiserver template file configurable,"SSIA
",closed,True,2016-08-22 10:11:02,2016-08-22 10:51:12
contrib,tommywo,https://github.com/kubernetes/contrib/issues/1588,https://api.github.com/repos/kubernetes/contrib/issues/1588,Cluster-autoscaler: problem with scaling down on gce,"After having multiple unused nodes, autoscaler tries to delete multiple vm, but only deletes one,
then time unneeded is reset and after 10 min does the same thing over again
In logs i found these warnings:

```
12:06:28.000 - Cluster is not ready for autoscaling: wrong number of nodes for mig: https://content.googleapis.com/compute/v1/projects/XXXXX/zones/europe-west1-d/instanceGroups/kubernetes-minion-group expected: 9 actual: 10
12:17:19.000 - Cluster is not ready for autoscaling: wrong number of nodes for mig: https://content.googleapis.com/compute/v1/projects/XXXXX/zones/europe-west1-d/instanceGroups/kubernetes-minion-group expected: 8 actual: 9
12:28:20.000 - Cluster is not ready for autoscaling: wrong number of nodes for mig: https://content.googleapis.com/compute/v1/projects/XXXXX/zones/europe-west1-d/instanceGroups/kubernetes-minion-group expected: 7 actual: 8
12:39:21.000 - Cluster is not ready for autoscaling: wrong number of nodes for mig: https://content.googleapis.com/compute/v1/projects/XXXXX/zones/europe-west1-d/instanceGroups/kubernetes-minion-group expected: 6 actual: 7
```

kubernetes: 1.3.5
cluster-autoscaler:v0.2.2

@mwielgus @fgrzadkowski @piosz can you help? 
",closed,False,2016-08-22 11:35:32,2016-09-02 13:21:25
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1589,https://api.github.com/repos/kubernetes/contrib/issues/1589,quote additional script arguments,"If not quoted, `--extra-vars=""source_type=packageManager flannel_subnet=10.253.0.0 flannel_network=16""` gets translated into

`ansible ... --extra-vars source_type=packageManager flannel_subnet=10.253.0.0 flannel_prefix=16`

instead of expected

`ansible ... --extra-vars ""source_type=packageManager flannel_subnet=10.253.0.0 flannel_prefix=16""`
",closed,True,2016-08-22 11:53:59,2016-08-22 12:22:31
contrib,wernight,https://github.com/kubernetes/contrib/issues/1590,https://api.github.com/repos/kubernetes/contrib/issues/1590,Nginx Ingress controller default backend,"I noticed on https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx that you advice to explicitely specify the default backend. Why do that instead of using the Ingress defined default backend?

```
kind: Ingress
spec:
  backend:
    serviceName: default-http-backend
    servicePort: 80
```

Also, why is returning an HTTP 404 a requirement? Shouldn't that be up to the website to decide how to respond to unknown subdomains?
",closed,False,2016-08-22 12:03:09,2018-02-16 23:09:01
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1591,https://api.github.com/repos/kubernetes/contrib/issues/1591,"[nginx-ingress-controller] Always listen on port 443, even without ingress rules","Currently is required at least one ingress rule to listen on port 443
",closed,True,2016-08-22 13:52:45,2016-08-22 15:44:22
contrib,wernight,https://github.com/kubernetes/contrib/issues/1592,https://api.github.com/repos/kubernetes/contrib/issues/1592,Nginx Ingress Controller not enabling TLS/HTTPS,"Following https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/tls and other examples there I my Nginx Ingress works on HTTP but refuses to connect on port 443 even when I'm directly on that Pod. The generated `nginx.conf` does not contain any `443`, and my Ingress definition worked on GCE Ingress.

Another strange thing is that https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/tls describes the _standard_ `tls.key` and `tls.crt` (also used by GCE Ingress) but then https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/examples/tls/dhparam.sh generates a `dhparam.pem` and I also get a matching warning from Nginx Ingress Controller:

```
ssl.go:132] no file dhparam.pem found in secrets
```

Possibly related to https://github.com/kubernetes/contrib/issues/1525
",closed,False,2016-08-22 15:01:23,2018-02-15 04:27:03
contrib,eparis,https://github.com/kubernetes/contrib/pull/1593,https://api.github.com/repos/kubernetes/contrib/issues/1593,Only show 512 characters of comments in the logs,"When creating new issues or adding comments to existing issue we can
create VERY long comments which make the logs really hard to read. We
don't need all of that in the logs.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1593)

<!-- Reviewable:end -->
",closed,True,2016-08-22 15:21:13,2016-08-23 18:23:12
contrib,wernight,https://github.com/kubernetes/contrib/pull/1594,https://api.github.com/repos/kubernetes/contrib/issues/1594,Update controller.go,"Better error message when there is no secretName provided.

Fixes #1592

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1594)

<!-- Reviewable:end -->
",closed,True,2016-08-22 15:36:37,2018-02-17 12:22:01
contrib,DanielHeckrath,https://github.com/kubernetes/contrib/issues/1595,https://api.github.com/repos/kubernetes/contrib/issues/1595,Ingress instance group has wrong count with autoscaling enabled,"I recently enabled autoscaling on my GKE cluster running Kubernetes 1.3.5

After the autoscaler increased the instance count number of instances in our k8s-ig instance group are missing the recently created instances.

How can i further debug this problem?
",closed,False,2016-08-22 21:21:38,2018-02-15 04:27:05
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1596,https://api.github.com/repos/kubernetes/contrib/issues/1596,[nginx-ingress-controller]: Adapt nginx hash sizes to the number of ingress,"This change allows the tuning of 2 important NGINX variables:
- server_names_hash_max_size
- server_names_hash_bucket_size

The default values should be enough for most of the users but after +300 Ingress rules or long hostnames as FQDN NGINX requires tuning of this values or it will not start.

The introduced change allows the self-tuning using the Ingress information
Using `--v=3` it's possible to see the changes:

```
...
I0822 21:42:10.517778       1 template.go:84] adjusting ServerNameHashMaxSize variable from 4096 to 16384
...
```

fixes #1487

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1596)

<!-- Reviewable:end -->
",closed,True,2016-08-22 21:50:09,2016-09-01 21:11:48
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1597,https://api.github.com/repos/kubernetes/contrib/issues/1597,Stop processing the pull-request if Github fails,"Today we keep going because we don't properly handle errors, and we may
end-up posting a comment everytime we can't read the comments.
Let's check if we can fetch the comment and fail if it's broken.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1597)

<!-- Reviewable:end -->
",closed,True,2016-08-23 00:21:59,2016-08-23 20:33:14
contrib,thockin,https://github.com/kubernetes/contrib/pull/1598,https://api.github.com/repos/kubernetes/contrib/issues/1598,Kill git sync,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1598)

<!-- Reviewable:end -->
",closed,True,2016-08-23 05:07:11,2016-08-23 05:33:05
contrib,keontang,https://github.com/kubernetes/contrib/issues/1599,https://api.github.com/repos/kubernetes/contrib/issues/1599,[conrib/ansible]: Dose it support async roles?,"It seems ansible is not able to support async roles: [https://github.com/ansible/ansible/issues/10588](https://github.com/ansible/ansible/issues/10588), so it will deploy masters/nodes one by one. It will take a long time to finish it.

How do you think of it?
",closed,False,2016-08-23 07:58:24,2018-02-22 00:07:44
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/1600,https://api.github.com/repos/kubernetes/contrib/issues/1600,[ansible] make all source types camel-case,"Use the same naming convention for all source types (and other enumerated values).

For instance `packageManager`, `localBuild`, `github-release`, `distribution-rpm` change to `package-manager`, `local-build`, `github-release`, `distribution-rpm``
",closed,False,2016-08-23 08:51:21,2018-02-15 04:27:06
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1601,https://api.github.com/repos/kubernetes/contrib/issues/1601,move kubernetes-accounting.conf under node roles,"After recent refactoring of kubernetes role,
enablement of node accounting got move under node role.
However, without the kubernetes-accounting.conf file.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1601)

<!-- Reviewable:end -->
",closed,True,2016-08-23 13:12:13,2016-08-23 13:43:40
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/1602,https://api.github.com/repos/kubernetes/contrib/issues/1602,Link to contrib submit queue is incorrect,"The link points to `http://submit-queue.k8s.io` instead of `http://contrib.submit-queue.k8s.io`.
",closed,False,2016-08-23 13:40:59,2018-02-15 05:28:03
contrib,wernight,https://github.com/kubernetes/contrib/issues/1603,https://api.github.com/repos/kubernetes/contrib/issues/1603,Security and API access,"One part I didn't fully understand after using a Nginx Ingress Controller, is what part of the Kubernetes API is actually exposed without authentication from within the cluster.

From http://kubernetes.io/docs/user-guide/accessing-the-cluster/#programmatic-access-to-the-api is looks like a token is always required.

However the Nginx Ingress Controller is visibly capable of at least reading the list of Ingress rules set up, and probably also secrets, and that without token. How is that possible?
",closed,False,2016-08-23 17:13:56,2016-08-24 08:16:57
contrib,asifhj,https://github.com/kubernetes/contrib/issues/1604,https://api.github.com/repos/kubernetes/contrib/issues/1604,Nginx Ingress Controller performance,"Hey Guys -

Observed significant difference in performance when we access the service from nginx-ingress-controller.

Accessing service via NodePort is faster than accessing via nginx-ingress-controller

nginx-ingress-controller:0.8.2

Kubernetes version

Client Version: version.Info{Major:""1"", Minor:""3"", GitVersion:""v1.3.4"", GitCommit:""dd6b458ef8dbf24aff55795baa68f83383c9b3a9"", GitTreeState:""clean"", BuildDate:""2016-08-01T16:45:16Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""3"", GitVersion:""v1.3.5"", GitCommit:""b0deb2eb8f4037421077f77cb163dbb4c0a2a9f5"", GitTreeState:""clean"", BuildDate:""2016-08-11T20:21:58Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}

Ubuntu 14.04

Any pointer to look into ?
",closed,False,2016-08-23 21:10:34,2016-09-06 22:57:19
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1605,https://api.github.com/repos/kubernetes/contrib/issues/1605,Bot To Handle Approvers AND Reviewers In Owners File,"Here's the proposed workflow as described by @bgrant0607  

Proposed workflow:
a reviewer, who isn't necessarily an approver, would be assigned
the reviewer would eventually type ""LGTM"" into a comment
the author would (often) squash commits and/or rebase
the reviewer would then type I approve if they were an approver, or would notify an approver to do that if they were not; the author could self-approve if they were an approver

I believe @hongchaodeng is working on tooling to automatically generate an owners file with two components, an Reviewers List and an Owners List. 

We will need to implement our own ACLs using the bot to verify that only approvers can make the final ""approvals"".

@apelisse @foxish let me know if this sounds reasonable, and if so, I'll start working on it.
",closed,False,2016-08-23 21:29:45,2018-02-15 08:31:03
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1606,https://api.github.com/repos/kubernetes/contrib/issues/1606,copy inventory group_vars to vagrant inventory directory,"Vagrant creates its own inventory file without predefined ansible group_vars.
Thus, it is necessary to copy the directory to vagrant inventory as well.

<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1606)

<!-- Reviewable:end -->
",closed,True,2016-08-24 08:11:52,2016-08-24 18:33:20
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1607,https://api.github.com/repos/kubernetes/contrib/issues/1607,"add playbook for kubernetes-addons role, decrease dependency","Introduce new playbook for kubernetes-addons role.

At the same time decrease dependency of the role from master to kubernetes. With the master dependency everytime the addons is run (e.g. to update addons), master role is run as well (resulting in update or re-configuration) which is not always desired. On the other hand, if the addons role is run, functioning master is required in advance.

Fixes: #1280

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1607)

<!-- Reviewable:end -->
",closed,True,2016-08-24 09:49:10,2016-08-24 18:33:23
contrib,m1093782566,https://github.com/kubernetes/contrib/pull/1608,https://api.github.com/repos/kubernetes/contrib/issues/1608,fix typo in 404-server README,"fix typo in 404-server README :)

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1608)

<!-- Reviewable:end -->
",closed,True,2016-08-24 10:19:50,2016-08-24 10:43:18
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/1609,https://api.github.com/repos/kubernetes/contrib/issues/1609,[ansible] minimize dependencies of roles on another roles,"Lot a roles depends on `common` role which defines facts as `is_atomic`, `is_coreos` and others. Which make dependent roles less reusable.
",closed,False,2016-08-24 11:53:35,2018-02-15 06:29:03
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1610,https://api.github.com/repos/kubernetes/contrib/issues/1610,Add docker source type,"Continuing with effort in https://github.com/kubernetes/contrib/pull/456 and https://github.com/kubernetes/contrib/pull/291. All credits to @rsmitty and @furikake for bringing the topic up. I have just put crumbs together.

With this PR docker can be installed in addtion to the usual way (via distribution package manager) from custom docker repository or from distribution rpm.

Introducing separate playbook and script so it is easier to test the docker role separately.

Fixes: #455

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1610)

<!-- Reviewable:end -->
",closed,True,2016-08-24 12:01:59,2016-08-24 18:33:27
contrib,aledbf,https://github.com/kubernetes/contrib/issues/1611,https://api.github.com/repos/kubernetes/contrib/issues/1611,[nginx-ingress-controller] Clarify the controller uses endpoints and not services,"This is not present in the readme.
",closed,False,2016-08-24 17:56:54,2016-09-21 21:26:34
contrib,jlebon,https://github.com/kubernetes/contrib/pull/1612,https://api.github.com/repos/kubernetes/contrib/issues/1612,iptables: adapt for new iptables mode,"In the new ""iptables"" mode of kube-proxy, packets addressed for a node
port will get DNAT'ed to one of the endpoints. If that endpoint does not
reside on the node, then flannel needs to be able to do the proxying to
the endpoint. We generalize and add flannel rules for this.

If the endpoint does reside on the node, then the packet will be
addressed to the docker bridge, in which case we need to allow it. We
modify the node rules for this. The previous node rules there are no
longer needed (they were only relevant in the ""userspace"" mode).

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1612)

<!-- Reviewable:end -->
",closed,True,2016-08-24 20:12:19,2016-08-30 08:31:53
contrib,ixdy,https://github.com/kubernetes/contrib/pull/1613,https://api.github.com/repos/kubernetes/contrib/issues/1613,Add kubernetes-cross-build to list of non-blocking builds,"I'd prefer this to be a blocking build, but we aren't doing a full crossbuild on PRs, so I guess we don't want to make it blocking?

Broken crossbuild does already block cutting releases, though.

cc @luxas @david-mcmahon @spxtr 

x-ref https://github.com/kubernetes/test-infra/issues/393

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1613)

<!-- Reviewable:end -->
",closed,True,2016-08-24 21:50:21,2016-08-25 19:53:11
contrib,caesarxuchao,https://github.com/kubernetes/contrib/pull/1614,https://api.github.com/repos/kubernetes/contrib/issues/1614,Add newlines in publisher bot's commit message,"To make the commit message present nicely in github.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1614)

<!-- Reviewable:end -->
",closed,True,2016-08-24 22:29:26,2016-08-25 00:02:58
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1615,https://api.github.com/repos/kubernetes/contrib/issues/1615,That could be nil,"I'm pushing this right now because the merge queue is hitting it and crashlooping.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1615)

<!-- Reviewable:end -->
",closed,True,2016-08-25 01:10:07,2016-08-25 01:33:00
contrib,foxish,https://github.com/kubernetes/contrib/pull/1616,https://api.github.com/repos/kubernetes/contrib/issues/1616,Fix incorrect deployment config,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1616)

<!-- Reviewable:end -->
",closed,True,2016-08-25 01:30:15,2016-08-25 02:23:02
contrib,foxish,https://github.com/kubernetes/contrib/pull/1617,https://api.github.com/repos/kubernetes/contrib/issues/1617,Requesting blunderbuss feature to read aliases.,"Turn on the aliases feature in the blunderbuss munger.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1617)

<!-- Reviewable:end -->
",closed,True,2016-08-25 01:40:28,2016-09-01 03:11:37
contrib,mdshuai,https://github.com/kubernetes/contrib/issues/1618,https://api.github.com/repos/kubernetes/contrib/issues/1618,[ansible]Can't get latest kube from storage.googleapis.com,"When install latest kube using ansilbe, always failed with can't get file.

```
TASK [master : Download Kubernetes binaries] ***********************************
failed: [ec2-54-152-124-227.compute-1.amazonaws.com] (item=kube-apiserver) => {""dest"": ""/opt/kubernetes/1.4.0-alpha.2/kube-apiserver"", ""failed"": true, ""item"": ""kube-apiserver"", ""msg"": ""Request failed"", ""response"": ""HTTP Error 404: Not Found"", ""state"": ""absent"", ""status_code"": 404, ""url"": ""https://storage.googleapis.com/kubernetes-release/release/v1.4.0-alpha.2/bin/linux/amd64/kube-apiserver""}
failed: [ec2-54-152-124-227.compute-1.amazonaws.com] (item=kube-controller-manager) => {""dest"": ""/opt/kubernetes/1.4.0-alpha.2/kube-controller-manager"", ""failed"": true, ""item"": ""kube-controller-manager"", ""msg"": ""Request failed"", ""response"": ""HTTP Error 404: Not Found"", ""state"": ""absent"", ""status_code"": 404, ""url"": ""https://storage.googleapis.com/kubernetes-release/release/v1.4.0-alpha.2/bin/linux/amd64/kube-controller-manager""}
failed: [ec2-54-152-124-227.compute-1.amazonaws.com] (item=kube-scheduler) => {""dest"": ""/opt/kubernetes/1.4.0-alpha.2/kube-scheduler"", ""failed"": true, ""item"": ""kube-scheduler"", ""msg"": ""Request failed"", ""response"": ""HTTP Error 404: Not Found"", ""state"": ""absent"", ""status_code"": 404, ""url"": ""https://storage.googleapis.com/kubernetes-release/release/v1.4.0-alpha.2/bin/linux/amd64/kube-scheduler""}
failed: [ec2-54-152-124-227.compute-1.amazonaws.com] (item=kubectl) => {""dest"": ""/opt/kubernetes/1.4.0-alpha.2/kubectl"", ""failed"": true, ""item"": ""kubectl"", ""msg"": ""Request failed"", ""response"": ""HTTP Error 404: Not Found"", ""state"": ""absent"", ""status_code"": 404, ""url"": ""https://storage.googleapis.com/kubernetes-release/release/v1.4.0-alpha.2/bin/linux/amd64/kubectl""}
```

I check the latest version is v1.4.0-alpha.2, but there is no related files.

```
[root@ip-172-18-12-44 dma]# wget https://storage.googleapis.com/kubernetes-release/release/latest.txt
--2016-08-25 01:59:11--  https://storage.googleapis.com/kubernetes-release/release/latest.txt
Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.217.112, 2607:f8b0:4004:80e::2010
Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.217.112|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 15 [application/octet-stream]
Saving to: ‘latest.txt.1’

100%[===================================================================================================================================================================================================>] 15          --.-K/s   in 0s      

2016-08-25 01:59:12 (3.89 MB/s) - ‘latest.txt.1’ saved [15/15]

[root@ip-172-18-12-44 dma]# cat latest.txt
v1.4.0-alpha.2
[root@ip-172-18-12-44 dma]# wget https://storage.googleapis.com/kubernetes-release/release/v1.4.0-alpha.2/bin/linux/amd64/kube-apiserver
--2016-08-25 01:59:28--  https://storage.googleapis.com/kubernetes-release/release/v1.4.0-alpha.2/bin/linux/amd64/kube-apiserver
Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.217.112, 2607:f8b0:4004:80e::2010
Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.217.112|:443... connected.
HTTP request sent, awaiting response... 404 Not Found
2016-08-25 01:59:28 ERROR 404: Not Found.
```
",closed,False,2016-08-25 06:00:41,2016-08-26 01:50:37
contrib,mdshuai,https://github.com/kubernetes/contrib/issues/1619,https://api.github.com/repos/kubernetes/contrib/issues/1619,[ansible]ERROR! The requested handler 'restart daemons' was not found in any of the known handlers,"Install with ansible, at last failed with error

```
TASK [kubernetes : Write the global config file] *******************************
ERROR! The requested handler 'restart daemons' was not found in any of the known handlers
```

Whole logs is : http://pastebin.test.redhat.com/405933

@ingvagabund 
",closed,False,2016-08-25 06:49:59,2016-08-25 12:23:08
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1620,https://api.github.com/repos/kubernetes/contrib/issues/1620,define fake 'restart deamons' handler for kubernetes-addons role,"The kubernetes role invokes restart daemons. It must be defined here
since the kubernetes-addons playbook depends on that role to setup basic
variables such as the kubernetes configuration file directory.

Fixes: #1619

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1620)

<!-- Reviewable:end -->
",closed,True,2016-08-25 08:38:44,2016-08-25 12:23:08
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1621,https://api.github.com/repos/kubernetes/contrib/issues/1621,WIP: replace some resources get via get_url with local copy,"Minimize reliance on remote resources, use local ones instead.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1621)

<!-- Reviewable:end -->
",closed,True,2016-08-25 10:16:14,2018-02-27 17:22:51
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1622,https://api.github.com/repos/kubernetes/contrib/issues/1622,Cluster-autoscaler: handle nil interfaces in CheckGroupsAndNodes,"Workaround for nil check semantics in go.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1622)

<!-- Reviewable:end -->
",closed,True,2016-08-25 10:25:37,2016-08-25 11:33:06
contrib,foxish,https://github.com/kubernetes/contrib/issues/1623,https://api.github.com/repos/kubernetes/contrib/issues/1623,"cluster-autoscaler, ingress, keepalived-vip: update vendored work-queue","The fix in https://github.com/kubernetes/kubernetes/pull/31396 needs to be updated:
- k8s.io/contrib/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/util/workqueue
- k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/workqueue
- k8s.io/contrib/keepalived-vip/vendor/k8s.io/kubernetes/pkg/util/workqueue

cc @pwittrock @bprashanth @fgrzadkowski
",closed,False,2016-08-25 16:43:50,2017-04-19 10:40:39
contrib,ravihansa3000,https://github.com/kubernetes/contrib/pull/1624,https://api.github.com/repos/kubernetes/contrib/issues/1624,Support SSL bridge mode with HAProxy and ACLs per ServicePort,"This pull request adds support for SSL Bridge mode (re-encrypt client side traffic using LB cert) with HAProxy and ACLs per ServicePort.
Instructions are added to README. A test case is added with full regression checks.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1624)

<!-- Reviewable:end -->
",closed,True,2016-08-25 17:47:16,2016-12-23 19:32:57
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1625,https://api.github.com/repos/kubernetes/contrib/issues/1625,[keepalived-vip] Release 0.8,"Update required because of #1623

<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1625)

<!-- Reviewable:end -->
",closed,True,2016-08-25 18:59:28,2016-09-09 17:30:51
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1626,https://api.github.com/repos/kubernetes/contrib/issues/1626,Ping people when flaky issues don't get attention,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1626)

<!-- Reviewable:end -->
",closed,True,2016-08-25 21:00:22,2016-08-26 23:31:53
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1627,https://api.github.com/repos/kubernetes/contrib/issues/1627,Change owners,"This is WIP.

<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1627)

<!-- Reviewable:end -->
",closed,True,2016-08-25 22:29:41,2016-10-14 18:32:48
contrib,keontang,https://github.com/kubernetes/contrib/issues/1628,https://api.github.com/repos/kubernetes/contrib/issues/1628,kube-down should not delete multi-user.target.wants directory on centos,"```
- name: Remove remaining files
      file: path={{ item }} state=absent
      with_items:
          ...
          - /etc/systemd/system/multi-user.target.wants
          ...
```

Because in multi-user.target.wants directory, there are some other useful services.
For example sshd.service, once deleted this direcotry, we will fail to access the machine by ssh.
",closed,False,2016-08-26 03:50:01,2016-08-26 04:33:20
contrib,keontang,https://github.com/kubernetes/contrib/pull/1629,https://api.github.com/repos/kubernetes/contrib/issues/1629,Fixed #1628: kube-down should not delete multi-user.target.wants dir,"On Centos, there are some other service in multi-user.target.wants directory, so we should not just delete it.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1629)

<!-- Reviewable:end -->
",closed,True,2016-08-26 04:05:43,2016-08-26 04:33:14
contrib,piosz,https://github.com/kubernetes/contrib/pull/1630,https://api.github.com/repos/kubernetes/contrib/issues/1630,[rescheduler] actually wait for pod to be scheduled,"https://github.com/kubernetes/kubernetes/issues/29023

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1630)

<!-- Reviewable:end -->
",closed,True,2016-08-26 06:15:17,2016-08-26 08:43:17
contrib,piosz,https://github.com/kubernetes/contrib/pull/1631,https://api.github.com/repos/kubernetes/contrib/issues/1631,[rescheduler] added exporting events,"ref #https://github.com/kubernetes/kubernetes/issues/29023

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1631)

<!-- Reviewable:end -->
",closed,True,2016-08-26 06:38:22,2016-08-26 08:43:21
contrib,mdshuai,https://github.com/kubernetes/contrib/issues/1632,https://api.github.com/repos/kubernetes/contrib/issues/1632,[ansible]kube-dns and logging CrashLoopBackOff after install,"After install from github-release, dns and logging is CrashLoopBackOff
kube_version: 1.4.0-alpha.3
kube_source_type: github-release

```
[root@ip-172-18-0-91 ~]# kubectl get pod --namespace=kube-system
NAME                                                               READY     STATUS             RESTARTS   AGE
elasticsearch-logging-v1-3we4j                                     1/1       Running            0          30m
elasticsearch-logging-v1-eod2n                                     1/1       Running            0          30m
fluentd-elasticsearch-ec2-52-91-111-130.compute-1.amazonaws.com    1/1       Running            1          15m
fluentd-elasticsearch-ec2-54-197-207-211.compute-1.amazonaws.com   1/1       Running            0          15m
heapster-v1.0.2-2780992708-63e7w                                   4/4       Running            0          30m
kibana-logging-v1-0nryf                                            0/1       CrashLoopBackOff   5          30m
kube-dns-v11-0691y                                                 3/4       CrashLoopBackOff   7          30m
kubedash-mts5v                                                     1/1       Running            0          20m
kubernetes-dashboard-v1.1.0-3fh5w                                  1/1       Running            0          20m
monitoring-influxdb-grafana-v3-wv58o                               2/2       Running            0          30m

```

describe pod info: http://pastebin.test.redhat.com/406374
@ingvagabund 
",closed,False,2016-08-26 07:28:01,2018-02-19 17:14:02
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1633,https://api.github.com/repos/kubernetes/contrib/issues/1633,Cluster-autoscaler: fix node group nil issues,"cc: @fgrzadkowski @piosz @jszczepkowski

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1633)

<!-- Reviewable:end -->
",closed,True,2016-08-26 08:35:05,2016-08-26 09:03:22
contrib,piosz,https://github.com/kubernetes/contrib/pull/1634,https://api.github.com/repos/kubernetes/contrib/issues/1634,[rescheduler] bumped version,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1634)

<!-- Reviewable:end -->
",closed,True,2016-08-26 09:15:02,2016-08-26 09:53:24
contrib,mdshuai,https://github.com/kubernetes/contrib/pull/1635,https://api.github.com/repos/kubernetes/contrib/issues/1635,[ansible]remove kube_master_api_port from group_vars,"kube_master_api_port has defined in roles/kubernetes/defaults/main.yml
In [README doc](https://github.com/kubernetes/contrib/tree/master/ansible#kubernetes-source-type) also describe that if we want change secure port , just change kube_master_api_port under roles/kubernetes/defaults/main.yml

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1635)

<!-- Reviewable:end -->
",closed,True,2016-08-26 10:02:13,2016-08-29 15:22:07
contrib,kayrus,https://github.com/kubernetes/contrib/pull/1636,https://api.github.com/repos/kubernetes/contrib/issues/1636,ansible: added etcd cert auth support,"subj

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1636)

<!-- Reviewable:end -->
",closed,True,2016-08-26 10:16:20,2016-08-29 15:22:10
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/1637,https://api.github.com/repos/kubernetes/contrib/issues/1637,AddonResizer: Watch nodes instead of constantly listing them,"@piosz - if you have any feedback on it

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1637)

<!-- Reviewable:end -->
",closed,True,2016-08-26 12:34:17,2016-08-29 12:42:05
contrib,foxish,https://github.com/kubernetes/contrib/pull/1638,https://api.github.com/repos/kubernetes/contrib/issues/1638,First step for FSM Munger. [WIP],"Author:    Anirudh Ramanathana ramanathana@google.com

@apelisse PTAL, this is still a WIP but I'd like to know what's good and bad about the approach. Just wanted to check in before everything gets rethought. Will revert changes to mungegithub/mungers/matchers/comment/*.go after your https://github.com/kubernetes/contrib/pull/1626 goes through.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1638)

<!-- Reviewable:end -->
",closed,True,2016-08-26 12:37:42,2016-09-13 17:44:02
contrib,sgsits,https://github.com/kubernetes/contrib/issues/1639,https://api.github.com/repos/kubernetes/contrib/issues/1639,Clarify docs around running nginx ingress controller without serviceaccounts,"I am running a Kubernetes cluster without TLS (no ca/tokens etc)
I am unable to run nginx-ingress-controller due to following errror

I0825 16:53:47.191547       1 main.go:99] Using build: https://github.com/bprashanth/contrib.git - git-b195d9b
F0825 16:53:47.191966       1 main.go:121] failed to create client: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory

Why is config.go>InClusterConfig() is forcing presence of serviceaccount and ca files.
I should be able to run it in unsecure environment.
",closed,False,2016-08-26 13:35:48,2018-02-15 21:44:02
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1640,https://api.github.com/repos/kubernetes/contrib/issues/1640,Ensure that all Github handles in Owners File Are In The Org,"We should add some validation that ensures that all github users listed as reviewers or approvers are in the k8s organization.

https://github.com/philips/kubernetes/commit/7936f777e8c56eb6417fa2c969d76ead2af01890
",closed,False,2016-08-26 18:39:53,2018-02-15 07:30:03
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1641,https://api.github.com/repos/kubernetes/contrib/issues/1641,service loadbalancer unittests fail because of stray whitespace,"Should just strip out whitespace when it doesn't matter. 
",closed,False,2016-08-26 20:54:00,2018-02-15 07:30:05
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1642,https://api.github.com/repos/kubernetes/contrib/issues/1642,block pre-1.5-general,"(already pushed)

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1642)

<!-- Reviewable:end -->
",closed,True,2016-08-26 22:27:22,2016-08-26 22:51:51
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1643,https://api.github.com/repos/kubernetes/contrib/issues/1643,Document the Commands that the Bot Accepts,"We need a comprehensive list of the commands that humans can use to communicate with the bot.  (e.g /lgtm /lgtm-cancel etc...)  
",closed,False,2016-08-26 22:34:21,2016-08-30 00:00:30
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1644,https://api.github.com/repos/kubernetes/contrib/issues/1644,added documentation for commands that the bot accepts,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1644)

<!-- Reviewable:end -->
",closed,True,2016-08-26 22:58:59,2016-08-29 22:02:18
contrib,eparis,https://github.com/kubernetes/contrib/pull/1645,https://api.github.com/repos/kubernetes/contrib/issues/1645,Consistent bool returns on exposed methods for err handling,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1645)

<!-- Reviewable:end -->
",closed,True,2016-08-27 01:04:11,2017-01-11 21:40:19
contrib,foxish,https://github.com/kubernetes/contrib/pull/1646,https://api.github.com/repos/kubernetes/contrib/issues/1646,Adding cla-munger.,"Adding the CLA munger to work with the linux foundation's CLA bot.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1646)

<!-- Reviewable:end -->
",closed,True,2016-08-27 01:21:49,2016-08-29 23:01:50
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1647,https://api.github.com/repos/kubernetes/contrib/issues/1647,Enable nag-flake-issues on Kubernetes repo,"And add an extra-test to make sure everything is as expected :-)

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1647)

<!-- Reviewable:end -->
",closed,True,2016-08-27 04:21:36,2016-08-27 04:41:55
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1648,https://api.github.com/repos/kubernetes/contrib/issues/1648,Parse github user mentions from strings,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1648)

<!-- Reviewable:end -->
",closed,True,2016-08-27 21:27:13,2017-02-17 01:07:32
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/1649,https://api.github.com/repos/kubernetes/contrib/issues/1649,Push new addon resizer version,"The update to the code was done in:
https://github.com/kubernetes/contrib/pull/1637

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1649)

<!-- Reviewable:end -->
",closed,True,2016-08-29 14:37:52,2016-08-29 17:22:14
contrib,derekwaynecarr,https://github.com/kubernetes/contrib/pull/1650,https://api.github.com/repos/kubernetes/contrib/issues/1650,Stop referencing deprecated admission plugins,"/cc @eparis @ncdc

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1650)

<!-- Reviewable:end -->
",closed,True,2016-08-29 15:01:23,2016-08-29 15:32:12
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1651,https://api.github.com/repos/kubernetes/contrib/issues/1651,Automatically Generate Webpage Describing New Features,"Either scrape the features repo and autogenerate an html file or use github requests to generate the html.
",closed,False,2016-08-29 18:12:39,2018-02-15 08:31:02
contrib,pdoreau,https://github.com/kubernetes/contrib/issues/1652,https://api.github.com/repos/kubernetes/contrib/issues/1652,Nginx Ingress LivenessProbe fails,"Hello. I'm trying the [nginx ingress http example](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx#http).

The livenessProbe fails : 
`Liveness probe failed: Get http://...:10254/healthz: dial tcp ....:10254: getsockopt: connection refused`

Using nmap on the pod I can't see any usage of this port (nmap).
",closed,False,2016-08-29 18:25:07,2016-08-29 19:52:16
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1653,https://api.github.com/repos/kubernetes/contrib/issues/1653,[nginx-ingress-controller] Update image version,"fixes #1652
#1571 introduced the error because it was merged after the `0.8.3` release

<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1653)

<!-- Reviewable:end -->
",closed,True,2016-08-29 18:47:39,2016-08-29 19:52:16
contrib,foxish,https://github.com/kubernetes/contrib/issues/1654,https://api.github.com/repos/kubernetes/contrib/issues/1654,Clean up github mockserver used in testing,"https://github.com/kubernetes/contrib/blob/master/mungegithub/github/testing/github.go#L325
`InitServer` takes far too many arguments. We should put that into a struct and clean up.
",closed,False,2016-08-29 21:34:27,2017-01-21 01:40:21
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1655,https://api.github.com/repos/kubernetes/contrib/issues/1655,matchers: Fix matching on multi-lines comments,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1655)

<!-- Reviewable:end -->
",closed,True,2016-08-29 22:46:50,2016-08-30 00:11:51
contrib,quinton-hoole-zz,https://github.com/kubernetes/contrib/issues/1656,https://api.github.com/repos/kubernetes/contrib/issues/1656,Ingress: Support downtimeless LoadBalancer UID changes in Google Load Balancer Controller,"Capturing a design discussion that @bprashanth and I just had, so that it doesn't get lost:
# Summary:

Currently, changing the GCE L7 Load Balancer fronting the Ingresses in a cluster involves a cluster outage for ~10 minutes, and the manual deletion and recreation of all Ingresses in the cluster.  This causes problems for Federated Ingress (where multiple clusters need to be updated to use the same GCE L7 Load Balancer, which currently causes a cluster outage).

This proposal describes how this update could be achieved with zero downtime.  I recommend that this be done before releasing Ingress to GA/stable. The implementation is pretty straightforward.  I'd be happy to volunteer to do it, unless someone else is interested.
# Detail:

Currently it's possible to change the so-called ""UID"" configmap entry for the Google Load Balancer Controller (GLBC).  See https://gist.github.com/bprashanth/52648b2a0b6a5b637f843e7efb2abc97 for details.

This ""UID"" is actually the identifier of the GCE L7 load balancer that will front all Ingresses created from that point forward. For clarity, I'll refer to it as the LB-UID henceforth.  

There is a problem though.  Each created Ingress is associated with the LB-UID that the GLBC was configured with at the time it was created.   And the GLBC always tries to ensure that there is precisely one GCE LB for the cluster, with a well-known name. So switching from one GCE LB to another involves deleting one and creating another (both with the same name).  Given that this involves a 10 minute delay, that does not work well if there are existing Ingress's, which will see a 10 minute outage.  For this reason, the GLBC does the safe thing, and refuses to delete the old GCE LB if it sees existing Ingresses in the cluster that rely on it.   So it never configures the new GCE LB (by e.g. adding all the nodes in the cluster as backends for the new GCE LB - note that nodes cannot be added as backends of multiple GCE LB's).  This means that in order to change the GCE LB UID configured in the Ingress Controller, it's necessary to first delete all Ingresses in the Cluster, reconfigure the GCE LB UID in the configmap, and then re-create all the existing ingresses.

But there is a better, no-downtime way:
1. Migrate ~half of the nodes in the cluster over to the new GCE LB (identified by the new LB-UID in the configmap)
2. flip the old external IP(s) over to the new GCE LB.
3. Wait for all traffic to drain over to the new GCE LB.  In this state, both GCE LB's are able to balance to the cluster (each via half of the nodes in the cluster).
4. Update all Ingresses to reflect the successful migration to the new GCE LB (only updating the UID annotation, as I understand).
5. Delete the old GCE-LB.

All of the above could/should be performed by the GLBC in reponse to changing it's configured LB-UID.

I just realized that I should move this to the separate repo where the GLBC lives.  Will do shortly.

cc @kubernetes/sig-network @bprashanth @thockin FYI
",closed,False,2016-08-29 23:03:01,2018-02-17 09:19:02
contrib,pdoreau,https://github.com/kubernetes/contrib/issues/1657,https://api.github.com/repos/kubernetes/contrib/issues/1657,Ingress Nginx / GLBC conflict,"Hello. I'm trying the basic [HTTP example](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx#http) for configuring an Nginx Ingress Contrroller

It looks like both Nginx RC and GLBC are enabled. Here is my describe ingress command output :

```
Name:           echomap
Namespace:      default
Address:        XXX.XXX.XX.XXX,YYY.YYY.YYY.YYY
Default backend:    default-http-backend:80 (XX.X.X.X:8080)
Rules:
  Host      Path    Backends
  ----      ----    --------
  foo.bar.com   
            /foo    echoheaders-x:80 (<none>)
  bar.baz.com   
            /bar    echoheaders-y:80 (<none>)
            /foo    echoheaders-x:80 (<none>)
Annotations:
  forwarding-rule:  k8s-fw-default-echomap--...
  target-proxy:     k8s-tp-default-echomap--...
  url-map:      k8s-um-default-echomap--...
  backends:     {""k8s-be-..."":""Unknown""}
Events:
  FirstSeen LastSeen    Count   From                SubobjectPath   Type        Reason  Message
  --------- --------    -----   ----                -------------   --------    ------  -------
  24m       24m     1   {nginx-ingress-controller }         Normal      CREATE  default/echomap
  24m       23m     2   {nginx-ingress-controller }         Normal      CREATE  ip: XXX.XXX.XX.XXX
  23m       23m     1   {loadbalancer-controller }          Normal      CREATE  ip: YYY.YYY.YYY.YYY
  23m       23m     1   {loadbalancer-controller }          Warning     Status  Operation cannot be fulfilled on ingresses.extensions ""echomap"": the object has been modified; please apply your changes to the latest version and try again
  24m       23m     4   {nginx-ingress-controller }         Normal      UPDATE  default/echomap

```

I've added the [annotation to disable](https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md#disabling-glbc) GLBC with ""nginx"".
I got a 502 response with the first IP (GLBC I guess) and no response from the second.

Is there something else to do to disable GLBC / enable Nginx ?
",closed,False,2016-08-29 23:03:09,2018-02-15 15:38:04
contrib,timstclair,https://github.com/kubernetes/contrib/pull/1658,https://api.github.com/repos/kubernetes/contrib/issues/1658,AppArmor loader DaemonSet,"This PR adds a proof-of-concept DaemonSet for loading AppArmor profiles from a ConfigMap onto a cluster.

The DaemonSet runs a small go program that is mostly a wrapper around `apparmor_parser`, which actually loads the profiles. The additional features the loader provides are:
- Polling
- Sourcing from multiple directories
- Better handling of symlinks (necessary for ConfigMap volumes)
- Better logging and messaging

This is meant to provide an example of using AppArmor, and be a starting point for users trying out AppArmor or building there own solutions.

/cc @jfrazelle @thockin

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1658)

<!-- Reviewable:end -->
",closed,True,2016-08-30 00:21:17,2016-09-01 23:01:31
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1659,https://api.github.com/repos/kubernetes/contrib/issues/1659,matchers: make them more generic,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1659)

<!-- Reviewable:end -->
",closed,True,2016-08-30 04:00:35,2017-02-07 23:30:51
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1660,https://api.github.com/repos/kubernetes/contrib/issues/1660,Update kubernetes addons scripts,"When deploying kube-apiserver listening on secure port, kubernetes addons are not deployed as the script calling kubectl does not posses kubeconfig.

The first commits cherry picks changes from [1].

[1] https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1660)

<!-- Reviewable:end -->
",closed,True,2016-08-30 07:38:57,2018-03-12 03:14:33
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/1661,https://api.github.com/repos/kubernetes/contrib/issues/1661,Add kubemark-scale to non-blocking SQ suites,"Fix kubernetes/kubernetes#31105

@gmarek

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1661)

<!-- Reviewable:end -->
",closed,True,2016-08-30 07:43:05,2016-09-01 12:12:12
contrib,pdoreau,https://github.com/kubernetes/contrib/issues/1662,https://api.github.com/repos/kubernetes/contrib/issues/1662,Nginx Ingress RC handles all namespace ingress,"Hello. I noticied that an nginx RC defined with namespace applies rules of an ingress defined in another namespace.

Is that nomral and desirable ? 
",closed,False,2016-08-30 15:21:12,2016-09-06 23:03:31
contrib,Q-Lee,https://github.com/kubernetes/contrib/pull/1663,https://api.github.com/repos/kubernetes/contrib/issues/1663,Changes to pull configuration from GCE metadata for master component …,"…monitoring.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1663)

<!-- Reviewable:end -->
",closed,True,2016-08-31 05:14:04,2016-09-01 23:07:26
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/1664,https://api.github.com/repos/kubernetes/contrib/issues/1664,Fix pod_nanny regression from #1637,,closed,True,2016-08-31 12:24:31,2016-08-31 12:52:00
contrib,aledbf,https://github.com/kubernetes/contrib/issues/1665,https://api.github.com/repos/kubernetes/contrib/issues/1665,[nginx-ingress-controller] Add cloud detection,"In order to remove invalid settings, like `use-proxy-protocol` in GCE/GKE
",closed,False,2016-08-31 13:22:18,2016-10-31 19:45:19
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1666,https://api.github.com/repos/kubernetes/contrib/issues/1666,Precise node count estimations in Cluster Autoscaler,"Calculate the precise number of needed nodes when there is just a single type of pods available. Currently we do a rough estimation which usually result in providing too few nodes and another round of scale up. This mechanism will not work if there is more than one type of pending pods.
",closed,False,2016-08-31 15:42:00,2016-12-08 14:00:23
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1667,https://api.github.com/repos/kubernetes/contrib/issues/1667,nag-flaky-issue: Do not ping if no assignee,"also improve the comment very slightly.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1667)

<!-- Reviewable:end -->
",closed,True,2016-08-31 17:06:12,2016-08-31 17:42:04
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1668,https://api.github.com/repos/kubernetes/contrib/issues/1668,issue-triager: Temporarily deactivate update,"This is doing a lot of uncached request to ListIssueEvents. Let's
deactivate it until we have a better solution.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1668)

<!-- Reviewable:end -->
",closed,True,2016-08-31 17:06:21,2016-08-31 17:32:03
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/1669,https://api.github.com/repos/kubernetes/contrib/issues/1669,[ansible] extend kube-addons pyyaml workaround to fedora atomic,"The script that sets up  kube-addons requires PyYAML, which
ansible installs on distros capable of package installation.
For CoreOS, this is worked-around by running python in a container.

This commit extends the same workaround to Fedora Atomic (the
workaround isn't needed for CentOS Atomic, because its image
already includes PyYAML.)

The commit changes the check for whether the workaround is required
from ""is python here"" to ""is pyyaml importable."" This is necessary
because Fedora does include python, so just checking for python
isn't sufficient to trigger the workaround.

I tested this on coreos and on fedora atomic.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1669)

<!-- Reviewable:end -->
",closed,True,2016-08-31 19:47:54,2016-09-05 12:56:01
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1670,https://api.github.com/repos/kubernetes/contrib/issues/1670,Cluster-autoscaler: pick one of the matching node groups at random,"Currently the first matching node group is picked for scale up. CA should select it at random so that all node groups grow in a similar way.

cc: @fgrzadkowski @piosz @jszczepkowski

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1670)

<!-- Reviewable:end -->
",closed,True,2016-08-31 21:52:01,2016-09-01 10:12:51
contrib,jnorment-q2,https://github.com/kubernetes/contrib/issues/1671,https://api.github.com/repos/kubernetes/contrib/issues/1671,[ansible] kube-dns container absent and kibana-logging CrashLoopBackOff,"Traced this to here:

contrib/ansible/roles/kubernetes-addons/templates/dns/skydns-rc.yaml.j2

When attempting to create the rc from the file, I got these two errors:

error validating ""skydns-rc.yaml"": error validating data: [found invalid field successThreshold for v1.Probe, found invalid field failureThreshold for v1.Probe]; if you choose to ignore these errors, turn validation off with --validate=false

I think this resulted in the silent failure of the kube-dns RC, but the success of the kube-dns svc.

After deployment, the behavior observed was that every service resolved to the same ( random? ) IP. ( And that there was no container for kube-dns running. )
",closed,False,2016-08-31 22:29:34,2018-02-15 17:40:04
contrib,pdoreau,https://github.com/kubernetes/contrib/pull/1672,https://api.github.com/repos/kubernetes/contrib/issues/1672,Add firewall rules and ing class clarifications,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1672)

<!-- Reviewable:end -->
",closed,True,2016-09-01 09:21:11,2016-09-02 21:39:33
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1673,https://api.github.com/repos/kubernetes/contrib/issues/1673,K8S removes LGTM label without a reason,"See: https://github.com/kubernetes/contrib/pull/1670
",closed,False,2016-09-01 10:14:36,2017-01-20 22:03:52
contrib,mkumatag,https://github.com/kubernetes/contrib/issues/1674,https://api.github.com/repos/kubernetes/contrib/issues/1674,[ansible] uninstall playbook doesn't work,"Modified the inventory file and installed the cluster. Later tried uninstalling using `adhoc/uninstall.yml` playbook but failed with below error:

```
[root@jupiter-vm536 ansible]# pwd
/root/contrib/ansible
[root@jupiter-vm536 ansible]# ansible-playbook -i inventory/inventory playbooks/adhoc/uninstall.yml 

```

fails with following error:

```
TASK [Remove remaining files] **************************************************
[DEPRECATION WARNING]: Skipping task due to undefined Error, in the future this will be a fatal error.: 'ansible_ssh_user' is undefined.
This feature will be removed 
in a future release. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.
[DEPRECATION WARNING]: Skipping task due to undefined Error, in the future this will be a fatal error.: 'ansible_ssh_user' is undefined.
This feature will be removed 
in a future release. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.
fatal: [jupiter-vm1055.pok.stglabs.ibm.com]: FAILED! => {""failed"": true, ""msg"": ""the field 'args' has an invalid value, which appears to include a variable that is undefined. The error was: 'item' is undefined\n\nThe error appears to have been in '/root/contrib/ansible/playbooks/adhoc/uninstall.yml': line 114, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n    - name: Remove remaining files\n      ^ here\n""}
fatal: [jupiter-vm1194.pok.stglabs.ibm.com]: FAILED! => {""failed"": true, ""msg"": ""the field 'args' has an invalid value, which appears to include a variable that is undefined. The error was: 'item' is undefined\n\nThe error appears to have been in '/root/contrib/ansible/playbooks/adhoc/uninstall.yml': line 114, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n    - name: Remove remaining files\n      ^ here\n""}

NO MORE HOSTS LEFT *************************************************************
    to retry, use: --limit @playbooks/adhoc/uninstall.retry

PLAY RECAP *********************************************************************
jupiter-vm1055.pok.stglabs.ibm.com : ok=15   changed=2    unreachable=0    failed=1   
jupiter-vm1194.pok.stglabs.ibm.com : ok=14   changed=2    unreachable=0    failed=1   

[root@jupiter-vm536 ansible]#
```
",closed,False,2016-09-01 11:27:01,2016-09-06 11:26:18
contrib,kgelinas,https://github.com/kubernetes/contrib/issues/1675,https://api.github.com/repos/kubernetes/contrib/issues/1675,[Ansible] CoreOS pypy libtinfo.so.5 not found,"CoreOS 1153 VMWare image.
Made clean install twice

ansible-playbook --version
ansible-playbook 2.1.1.0
  config file = /home/gelke01/contrib/ansible/scripts/ansible.cfg
  configured module search path = Default w/o overrides

```
TASK [setup] *******************************************************************
<10.0.0.50> ESTABLISH SSH CONNECTION FOR USER: core
<10.0.0.50> SSH: EXEC ssh -C -q -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=core -o ConnectTimeout=10 -o ControlPath=/home/gelke01/.ansible/cp/ansible-ssh-%h-%p-%r 10.0.0.50 '/bin/sh -c '""'""'( umask 77 && mkdir -p ""` echo $HOME/.ansible/tmp/ansible-tmp-1472733511.21-153745518274568 `"" && echo ansible-tmp-1472733511.21-153745518274568=""` echo $HOME/.ansible/tmp/ansible-tmp-1472733511.21-153745518274568 `"" ) && sleep 0'""'""''
<10.0.0.50> PUT /tmp/tmpZQSEJQ TO /home/core/.ansible/tmp/ansible-tmp-1472733511.21-153745518274568/setup
<10.0.0.50> SSH: EXEC sftp -b - -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=core -o ConnectTimeout=10 -o ControlPath=/home/gelke01/.ansible/cp/ansible-ssh-%h-%p-%r '[10.0.0.50]'
<10.0.0.50> ESTABLISH SSH CONNECTION FOR USER: core
<10.0.0.50> SSH: EXEC ssh -C -q -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=core -o ConnectTimeout=10 -o ControlPath=/home/gelke01/.ansible/cp/ansible-ssh-%h-%p-%r -tt 10.0.0.50 '/bin/sh -c '""'""'sudo -H -S -n -u root /bin/sh -c '""'""'""'""'""'""'""'""'echo BECOME-SUCCESS-zzpeleiojpelxrzvpvcjdforqqoqhyie; LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 PATH=/opt/bin:$PATH python /home/core/.ansible/tmp/ansible-tmp-1472733511.21-153745518274568/setup; rm -rf ""/home/core/.ansible/tmp/ansible-tmp-1472733511.21-153745518274568/"" > /dev/null 2>&1'""'""'""'""'""'""'""'""' && sleep 0'""'""''
fatal: [IAmCoreMaster1]: FAILED! => {""changed"": false, ""failed"": true, ""invocation"": {""module_name"": ""setup""}, ""module_stderr"": """", ""module_stdout"": ""/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n/opt/bin/pypy/bin/pypy: /lib64/libssl.so.1.0.0: no version information available (required by /opt/bin/pypy/bin/pypy)\r\n/opt/bin/pypy/bin/pypy: /lib64/libcrypto.so.1.0.0: no version information available (required by /opt/bin/pypy/bin/pypy)\r\n/bin/sh: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\nTraceback (most recent call last):\r\n  File \""app_main.py\"", line 75, in run_toplevel\r\n  File \""/home/core/.ansible/tmp/ansible-tmp-1472733511.21-153745518274568/setup\"", line 120, in <module>\r\n    exitcode = invoke_module(module, zipped_mod, ZIPLOADER_PARAMS)\r\n  File \""/home/core/.ansible/tmp/ansible-tmp-1472733511.21-153745518274568/setup\"", line 28, in invoke_module\r\n    p = subprocess.Popen(['PATH=/opt/bin:$PATH python', module], env=os.environ, shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\r\n  File \""/opt/bin/pypy/lib-python/2.7/subprocess.py\"", line 710, in __init__\r\n    errread, errwrite)\r\n  File \""/opt/bin/pypy/lib-python/2.7/subprocess.py\"", line 1331, in _execute_child\r\n    raise child_exception\r\nOSError: [Errno 2] No such file or directory\r\n"", ""msg"": ""MODULE FAILURE"", ""parsed"": false}
```

```
ldd /opt/bin/pypy/bin/pypy
/opt/bin/pypy/bin/pypy: /lib64/libssl.so.1.0.0: no version information available (required by /opt/bin/pypy/bin/pypy)
/opt/bin/pypy/bin/pypy: /lib64/libcrypto.so.1.0.0: no version information available (required by /opt/bin/pypy/bin/pypy)
        linux-vdso.so.1 (0x00007ffd375eb000)
        libdl.so.2 => /lib64/libdl.so.2 (0x00007fb891a6a000)
        libm.so.6 => /lib64/libm.so.6 (0x00007fb891766000)
        libz.so.1 => /lib64/libz.so.1 (0x00007fb89154e000)
        libssl.so.1.0.0 => /lib64/libssl.so.1.0.0 (0x00007fb8912ce000)
        libcrypto.so.1.0.0 => /lib64/libcrypto.so.1.0.0 (0x00007fb890e4d000)
        libffi.so.6 => /lib64/libffi.so.6 (0x00007fb890c44000)
        libexpat.so.1 => /lib64/libexpat.so.1 (0x00007fb890a16000)
        librt.so.1 => /lib64/librt.so.1 (0x00007fb89080e000)
        libbz2.so.1.0 => /lib64/libbz2.so.1.0 (0x00007fb8905fd000)
        libcrypt.so.1 => /lib64/libcrypt.so.1 (0x00007fb8903c5000)
        libutil.so.1 => /lib64/libutil.so.1 (0x00007fb8901c2000)
        libtinfo.so.5 => not found
        libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fb88ffa5000)
        libc.so.6 => /lib64/libc.so.6 (0x00007fb88fbf5000)
        /lib64/ld-linux-x86-64.so.2 (0x00007fb891c6e000)

```
",closed,False,2016-09-01 12:59:11,2018-07-27 11:01:44
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1676,https://api.github.com/repos/kubernetes/contrib/issues/1676,Fix fetch comments,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1676)

<!-- Reviewable:end -->
",closed,True,2016-09-01 17:59:59,2016-09-01 20:12:46
contrib,mumoshu,https://github.com/kubernetes/contrib/issues/1677,https://api.github.com/repos/kubernetes/contrib/issues/1677,[nginx-ingress-controller] Rate limiting requests to a specific path/requestor ip,"Normally we use something like `limit_req` and `limit_req_zone` for Nginx to rate limit requests to specific path for an requestor ip, so that we can rate limit access to specific path e.g. `/api` not to fire up or web servers until they scale out.

Is there any recommended way to deal with an use-case like this with today's nginx-ingress-controller?
",closed,False,2016-09-02 03:40:26,2018-05-07 17:55:08
contrib,apelisse,https://github.com/kubernetes/contrib/issues/1678,https://api.github.com/repos/kubernetes/contrib/issues/1678,mungegithub/github: Tests are very slow,"Tests are much slower since 06d9dd190. One of the test checks what happens when mergeability flag isn't there. The new code is blocking for up to 62 seconds (2 + 4 + 8 + 16 + 32), this may be a problem on different levels.

Test time basically went from 5 seconds to 65 seconds.

cc @foxish 
",closed,False,2016-09-02 03:59:56,2016-09-02 22:21:07
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/1679,https://api.github.com/repos/kubernetes/contrib/issues/1679,Fix bug in addon resizer,"Ref https://github.com/kubernetes/kubernetes/issues/31950

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1679)

<!-- Reviewable:end -->
",closed,True,2016-09-02 08:18:41,2016-09-02 09:12:30
contrib,dominikschulz,https://github.com/kubernetes/contrib/issues/1680,https://api.github.com/repos/kubernetes/contrib/issues/1680,Ingress GLBC: Please provide a way to enable Cloud CDN,"The GLBC Ingress Resource should provide a way to enable the Google CloudCDN property on managed L7 LBs.

The CloudCDN flag is a property of an backend that can be enabled using the gcloud command line tool afterwards but since it's not clear how the GLBC ingress interacts with that on service updates it's not really practical.
",closed,False,2016-09-02 08:32:34,2019-01-03 21:20:05
contrib,dominikschulz,https://github.com/kubernetes/contrib/issues/1681,https://api.github.com/repos/kubernetes/contrib/issues/1681,Ingress GLBC: Default healthcheck used if readiness probe for different port,"For a service/deployments exposing multiple ports the GLBC ingress may create the GCE backend healthchecks using the default path (""/"") instead of the configured readiness probe.

We deployed a service exposing two ports (80 and 81) with a readiness probe configured as follows:
`readinessProbe:
          httpGet:
            path: /healthz
            port: 81
          initialDelaySeconds: 10
          timeoutSeconds: 1
`

The GLBC ingress was configured to target the service port 80 and this led to the auto-created GCE healthcheck using ""/"" instead of ""/healthz"".

This fact should at least be mentioned in the documentation.
",closed,False,2016-09-02 08:36:57,2016-09-06 15:41:53
contrib,valentin-krasontovitsch,https://github.com/kubernetes/contrib/issues/1682,https://api.github.com/repos/kubernetes/contrib/issues/1682,starting etcd service fails,"In `contrib/ansible`, using `deploy-cluster.sh` to setup a cluster of 3 Ubuntu 16.04.1 servers (virtual machines provisioned on hyper-v), the playbook fails at role `etcd`, at `etcd-start.yml`, at task `Enable etcd`.

The error message returned by ansible is:

```
fatal: [docker1]: FAILED! => {""changed"": false, ""failed"": true, \
""msg"": ""Error when trying to enable etcd: rc=1 Failed to execute operation: No such file or directory\n""}
```

If any more info is required, pls let me know.
",closed,False,2016-09-02 11:06:00,2016-09-13 07:50:40
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1683,https://api.github.com/repos/kubernetes/contrib/issues/1683,Nil pointer in cluster autoscaler simulator in 0.3.0-beta5,"cc: @piosz @fgrzadkowski @jszczepkowski 

```
I0902 18:04:57.444449       5 cluster.go:189] Looking for place for default/nginx-3358184317-bjw2p
panic: runtime error: invalid memory address or nil pointer dereference
[signal 0xb code=0x1 addr=0x0 pc=0x52b8a8]
goroutine 36 [running]:
panic(0x1b648a0, 0xc820010070)
        /usr/local/google/home/mwielgus/.gvm/gos/go1.6.2/src/runtime/panic.go:481 +0x3e6
k8s.io/contrib/cluster-autoscaler/simulator.findPlaceFor.func2(0xc82097b200, 0x1a, 0xc82094d900, 0x1a)
        /usr/local/google/home/mwielgus/ca-0.3/src/k8s.io/contrib/cluster-autoscaler/simulator/cluster.go:160 +0x778
k8s.io/contrib/cluster-autoscaler/simulator.findPlaceFor(0xc820ab8a40, 0x1a, 0xc8200209b8, 0x1, 0x1, 0xc820c413f0, 0x13, 0x16, 0xc82096db00, 0xc
820020370, ...)
        /usr/local/google/home/mwielgus/ca-0.3/src/k8s.io/contrib/cluster-autoscaler/simulator/cluster.go:193 +0x833
k8s.io/contrib/cluster-autoscaler/simulator.FindNodesToRemove(0xc82086ad00, 0x13, 0x20, 0xc820c413f0, 0x13, 0x16, 0xc820a00400, 0x6d, 0x80, 0x0,
 ...)
        /usr/local/google/home/mwielgus/ca-0.3/src/k8s.io/contrib/cluster-autoscaler/simulator/cluster.go:95 +0x977
main.FindUnneededNodes(0xc820c413f0, 0x13, 0x16, 0xc820ad1140, 0x3fe0000000000000, 0xc820a00400, 0x6d, 0x80, 0xc820020370, 0xc820c201e0, ...)
        /usr/local/google/home/mwielgus/ca-0.3/src/k8s.io/contrib/cluster-autoscaler/scale_down.go:85 +0x363
main.run(0xc8203613e0)
        /usr/local/google/home/mwielgus/ca-0.3/src/k8s.io/contrib/cluster-autoscaler/cluster_autoscaler.go:283 +0x2360
created by k8s.io/contrib/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/leaderelection.(*LeaderElector).Run
        /usr/local/google/home/mwielgus/ca-0.3/src/k8s.io/contrib/cluster-autoscaler/vendor/k8s.io/kubernetes/pkg/client/leaderelection/leaderel
ection.go:177 +0x91
```
",closed,False,2016-09-02 18:07:56,2016-09-02 19:29:32
contrib,rmmh,https://github.com/kubernetes/contrib/pull/1684,https://api.github.com/repos/kubernetes/contrib/issues/1684,Fix #1678 by speeding up IsMergeable retries when testing.,"before:
ok      k8s.io/contrib/mungegithub/mungers      65.728s
after:
ok      k8s.io/contrib/mungegithub/mungers      3.886s

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1684)

<!-- Reviewable:end -->
",closed,True,2016-09-02 18:43:27,2016-09-02 22:21:01
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1685,https://api.github.com/repos/kubernetes/contrib/issues/1685,Cluster-autoscaler: skip nodeinfos with nil nodes,"Fixes #1683.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1685)

<!-- Reviewable:end -->
",closed,True,2016-09-02 19:07:52,2016-09-02 19:29:32
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1686,https://api.github.com/repos/kubernetes/contrib/issues/1686,Update The Auto-prioritization of Flakes,"p0 Flakes are currently defined as 3x per week.  That results in a lot of p0 flakes.  Let's recalibrate
",closed,False,2016-09-02 22:00:42,2016-10-12 20:26:08
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1687,https://api.github.com/repos/kubernetes/contrib/issues/1687,updated the flake autoprioritization to reflect histogram,"Fixes #1686

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1687)

<!-- Reviewable:end -->
",closed,True,2016-09-02 22:04:14,2016-10-12 20:26:08
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1688,https://api.github.com/repos/kubernetes/contrib/issues/1688,Submit queue: don't assign reviews/bugs to people that are on vacation,"Not sure how to make this happen exactly. Last time I tried, people were unhappy at the idea of their OOO status being public knowledge, and so my solution (add it to your github bio) didn't fly. So may have to provide some way of telling only the merge queue and not the world.

Or you could have the merge queue try to guess by looking at their github activity? This is probably too coarse-grained to work.

It's super frustrating to have something assigned to someone that's not around and you have no idea when or if they'll ever be back.
",closed,False,2016-09-02 23:58:33,2017-04-10 23:37:47
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1689,https://api.github.com/repos/kubernetes/contrib/issues/1689,Add playbook to install 1-node cluster from within running host machine,"The playbook is useful for starting users of kubernetes. It just runs through etcd, docker and kubernetes roles. So user can run&play without need to generate certificates or tokens.

It skips configuration of etcd and docker in case a user has already any of them installed.
On the other hand, kubernetes configuration is always carried as tokens/certs require config files update.

**TODO**
- [x] refactor `docker` role (at least introduce `configure` tag) https://github.com/kubernetes/contrib/pull/1690

Fixes: #1325

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1689)

<!-- Reviewable:end -->
",closed,True,2016-09-05 11:16:48,2016-09-07 14:22:57
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1690,https://api.github.com/repos/kubernetes/contrib/issues/1690,tag various tasks of docker/main.yml,"Tag various tasks so one can skip blocks of tasks. For instance configuration.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1690)

<!-- Reviewable:end -->
",closed,True,2016-09-05 12:08:25,2016-09-05 12:39:41
contrib,valentin-krasontovitsch,https://github.com/kubernetes/contrib/issues/1691,https://api.github.com/repos/kubernetes/contrib/issues/1691,Credentials for gcr.io container image repository,"Trying to deploy k8s using the ansible setups in contrib/ansible, I have gotten quite far, only to notice that the proxy settings ain't working for docker (might make own issue), and that I can't pull any images from `gcr.io/google_containers/` - apparently one always need to login / authenticate oneself? If so, is this mentioned anywhere in the docs of kubernets/contrib?

For context, I'm deploying on Ubuntu 16.4.1 virtual machines running on top of hyper-v, and sitting behind a proxy.
",closed,False,2016-09-05 14:22:51,2016-09-06 10:41:15
contrib,coufon,https://github.com/kubernetes/contrib/pull/1692,https://api.github.com/repos/kubernetes/contrib/issues/1692,Add new project Node-Performance-Dashboard,"This PR contains the node performance dashboard. It is based on perfdash project to visualize test data of node performance tests.

The projects contains the following components:
1. Web server implemented in Golang based on Perfdash project;
2. Parser to extract tracing data from kubelet.log;
3. Webpages implemented in Javascript/html using AngularJS, ChartJS, and Angular Material.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1692)

<!-- Reviewable:end -->
",closed,True,2016-09-05 22:04:19,2016-09-15 20:50:06
contrib,vyshane,https://github.com/kubernetes/contrib/pull/1693,https://api.github.com/repos/kubernetes/contrib/issues/1693,Allow customisation of the nginx proxy_buffer_size directive via ConfigMap,"When using nginx as a proxy we can run into the following error:

```
upstream sent too big header while reading response header from upstream
```

In order to fix this, we need to be able to configure the `proxy_buffer_size` nginx directive to increase its value. This PR updates the nginx-ingress-controller to allow that.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1693)

<!-- Reviewable:end -->
",closed,True,2016-09-06 05:54:32,2016-09-15 15:22:36
contrib,mkumatag,https://github.com/kubernetes/contrib/pull/1694,https://api.github.com/repos/kubernetes/contrib/issues/1694,Fix uninstall playbook,"Fixes - https://github.com/kubernetes/contrib/issues/1674

<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1694)

<!-- Reviewable:end -->
",closed,True,2016-09-06 08:51:00,2016-09-06 10:49:47
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/1695,https://api.github.com/repos/kubernetes/contrib/issues/1695,[Ingress] How to use regexp in nginx.tmpl?,"Hi,

According to our application requirement, I need to add some lua code block in nginx.tmpl. To limit the impact of these lua code, I want to achieve some limitation: 
1. If the upstream name matches some specific string, add balancer_by_lua_block
2. If the location patch matches some specific string, add rewrite_by_lua_block and header_filter_by_lua_block.

How can I implement these 2 limitations in nginx.tmpl?

Thanks
",closed,False,2016-09-06 08:55:15,2016-09-26 04:03:33
contrib,RobinsChens,https://github.com/kubernetes/contrib/pull/1696,https://api.github.com/repos/kubernetes/contrib/issues/1696,add nginx controller,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1696)

<!-- Reviewable:end -->
",closed,True,2016-09-06 13:41:20,2017-05-15 09:04:57
contrib,kovvik,https://github.com/kubernetes/contrib/issues/1697,https://api.github.com/repos/kubernetes/contrib/issues/1697,kube-apiserver service cannot open port below 1024 on Ubuntu 16.04.1,"On Ubuntu with systemd, the kube-apiserver service started by the kube user. But this user cannot open port below 1024:

 kube-apiserver[8482]: E0906 13:19:29.895480    8482 genericapiserver.go:716] Unable to listen for secure (listen tcp 0.0.0.0:443: bind: permission denied); will try again.

My workaround is to set capability on kube-apiserver binary:
`setcap 'cap_net_bind_service=+ep'  /opt/kubernetes/1.2.4/kube-apiserver`

(You cannot set capability on symlinks, so it must not be set directly on /usr/bin/kubeapiserver)
",closed,False,2016-09-06 13:43:41,2017-10-29 22:15:10
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1698,https://api.github.com/repos/kubernetes/contrib/issues/1698,update etcd role to deploy etcd service on Ubuntu-16.04,"SSIA

Fixes: #1682

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1698)

<!-- Reviewable:end -->
",closed,True,2016-09-06 13:52:41,2016-09-13 07:50:40
contrib,RobinsChens,https://github.com/kubernetes/contrib/pull/1699,https://api.github.com/repos/kubernetes/contrib/issues/1699,add bulk-lgtm tool.,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1699)

<!-- Reviewable:end -->
",closed,True,2016-09-06 13:52:46,2017-11-18 20:05:14
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1700,https://api.github.com/repos/kubernetes/contrib/issues/1700,Update kubernetes role to deploy on Ubuntu-16.04,"SSIA

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1700)

<!-- Reviewable:end -->
",closed,True,2016-09-06 15:23:08,2016-09-07 08:09:53
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1701,https://api.github.com/repos/kubernetes/contrib/issues/1701,extend the list of supported libvirt OSes to Ubuntu-14.04 and Ubuntu-16.04,"SSIA

depends on:
- https://github.com/kubernetes/contrib/pull/1698
- https://github.com/kubernetes/contrib/pull/1700

Once merged, both version of Ubuntu can be used.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1701)

<!-- Reviewable:end -->
",closed,True,2016-09-06 18:03:28,2016-11-16 14:03:59
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1702,https://api.github.com/repos/kubernetes/contrib/issues/1702,Expand cluster autoscaler documentation,"More details on when and how scale up/down work are needed.
",closed,False,2016-09-06 21:52:14,2017-04-19 10:40:56
contrib,eksavoy,https://github.com/kubernetes/contrib/issues/1703,https://api.github.com/repos/kubernetes/contrib/issues/1703,[Ansible] flannel package,"Hello,

I try to deploy kubernetes cluster with 1 master and 3 nodes.
All my server are on Fedora 24 4.5.7-300.

I have this error :

```
TASK [flannel : Force to use github-release when packages are not available] ***
FAILED! => {""failed"": true, ""msg"": ""The conditional check 'inventory_hostname in groups['masters'] + groups['nodes']' failed. The error was: error while evaluating conditional (inventory_hostname in groups['masters'] + groups['nodes']): 'dict object' has no attribute 'nodes'\n\nThe error appears to have been in '/home/elerion/Ansible/contrib/ansible/roles/flannel/tasks/main.yml': line 5, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n- name: Install Flannel\n  ^ here\n""}
```

Have you any idea ?
",closed,False,2016-09-06 22:47:41,2016-09-07 21:19:54
contrib,lavalamp,https://github.com/kubernetes/contrib/pull/1704,https://api.github.com/repos/kubernetes/contrib/issues/1704,allow pre-1.5-general PRs to merge,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1704)

<!-- Reviewable:end -->
",closed,True,2016-09-06 23:11:01,2016-09-07 02:29:51
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/1705,https://api.github.com/repos/kubernetes/contrib/issues/1705,Enable protobufs in rescheduler,"Ref https://github.com/kubernetes/kubernetes/issues/32098

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1705)

<!-- Reviewable:end -->
",closed,True,2016-09-07 07:46:13,2016-09-07 08:59:54
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/1706,https://api.github.com/repos/kubernetes/contrib/issues/1706,Fail to build nginx controller,"Hi,

Several weeks ago I success to build the nginx controller at k8s.io/contrib/ingress/controllers/nginx. Today I use git pull to update the source code and try to build again. This time I fail. The output is as follow:

make controller
rm -f nginx-ingress-controller
CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags \
        ""-s -w -X main.version=git-316e640 -X main.gitRepo=https://github.com/kubernetes/contrib.git"" \
        -o nginx-ingress-controller
# k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/certificates
../../vendor/k8s.io/kubernetes/pkg/util/certificates/csr.go:96: undefined: elliptic.P224

Thanks
",closed,False,2016-09-07 10:03:43,2016-09-08 02:17:08
contrib,ingvagabund,https://github.com/kubernetes/contrib/issues/1707,https://api.github.com/repos/kubernetes/contrib/issues/1707,"Remove ""All rights reserved"" from all the headers","It looks like it is applicable for contrib as well [1]. Any thoughts?

[1] https://github.com/kubernetes/kubernetes/pull/26755
",closed,False,2016-09-07 14:12:37,2016-09-08 12:30:46
contrib,sekka1,https://github.com/kubernetes/contrib/pull/1708,https://api.github.com/repos/kubernetes/contrib/issues/1708,Solr petset,"Signed-off-by: gar garlandk@gmail.com

Adding in a Solr Petset

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1708)

<!-- Reviewable:end -->
",closed,True,2016-09-07 16:38:50,2016-09-07 17:05:56
contrib,Philmod,https://github.com/kubernetes/contrib/issues/1709,https://api.github.com/repos/kubernetes/contrib/issues/1709,[service-loadbalancer] Routed services,"Hey!

Why does the service-loadbalancer automatically open routes for heapster, kube-dns:53, kubernetes-dashboard, kubernetes:443, and one for my redis service I created?
![openroutes](https://dl.dropboxusercontent.com/u/45971143/service-loadbalancer.png)

I clearly don't want to open these. Is there a way to avoid it and specify the services I want to route through service-loadbalancer?

Thanks,
Philmod
",closed,False,2016-09-07 19:39:27,2016-09-07 20:13:34
contrib,Philmod,https://github.com/kubernetes/contrib/issues/1710,https://api.github.com/repos/kubernetes/contrib/issues/1710,[service-loadbalancer] Acl Match,"Hey,

I tried to set `serviceloadbalancer/lb.aclMatch: ""-i /""` for one of my service. 
But in the haproxy frontend configuration, it still uses the service name: ""/service-name"" (not ""/"").

Is it a bug?

Thanks,
Philmod
",closed,False,2016-09-07 19:41:17,2018-02-15 21:44:04
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1711,https://api.github.com/repos/kubernetes/contrib/issues/1711,[nginx-ingress-controller]: Add function helpers to nginx template,"fixes #1695

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1711)

<!-- Reviewable:end -->
",closed,True,2016-09-07 20:22:24,2016-09-26 04:03:33
contrib,eksavoy,https://github.com/kubernetes/contrib/issues/1712,https://api.github.com/repos/kubernetes/contrib/issues/1712,webService Unauthorized,"Hi,

I have install a new cluster with ansible and after the installation I try access to ui like https://my-host/ui for example. I have always the same error Unauthorized
When I go see the security request, I can see that https certificats are expired :
![errorkube](https://cloud.githubusercontent.com/assets/14874472/18329460/cd50d7ea-7553-11e6-8c6b-e5e818cf3d49.PNG)

Have you got any idea ?

Thank's
",closed,False,2016-09-07 21:39:00,2016-09-12 14:06:54
contrib,gssbzn,https://github.com/kubernetes/contrib/pull/1713,https://api.github.com/repos/kubernetes/contrib/issues/1713,Update kubectl to 1.3.6,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1713)

<!-- Reviewable:end -->
",closed,True,2016-09-07 21:39:25,2018-02-18 12:46:00
contrib,pwittrock,https://github.com/kubernetes/contrib/pull/1714,https://api.github.com/repos/kubernetes/contrib/issues/1714,Open up submit queue,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1714)

<!-- Reviewable:end -->
",closed,True,2016-09-07 22:04:16,2016-09-07 22:33:33
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/1715,https://api.github.com/repos/kubernetes/contrib/issues/1715,What cause starting nginx controll failure? ,"I build ingress nginx controller and fail to start it. env: kubernetes 1.3.2, docker 1.11.2. CentOS 7.2.1

The output of kubectl describe is:
 7m    7m      1       {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Normal  Created         Created container with docker id e1d3670bad8c
  7m    7m      1       {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Normal  Started         Started container with docker id e1d3670bad8c
  7m    7m      1       {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Normal  Killing         Killing container with docker id e1d3670bad8c: pod ""nginx-ingress-controller-du1gp_default(ac3b025c-75a8-11e6-9cd1-00505697649c)"" container ""nginx-ingress-lb"" is unhealthy, it will be killed and re-created.
  2m    2m      1       {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Normal  Created         Created container with docker id 6e71d8c77211
  2m    2m      1       {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Normal  Started         Started container with docker id 6e71d8c77211
  1m    1m      1       {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Normal  Started         (events with common reason combined)
  1m    1m      1       {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Normal  Created         (events with common reason combined)
  17m   1m      10      {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Normal  Pulled          Container image ""nginx-ingress:0.8.3"" already present on machine
  1m    1m      1       {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Normal  Killing         Killing container with docker id 6e71d8c77211: pod ""nginx-ingress-controller-du1gp_default(ac3b025c-75a8-11e6-9cd1-00505697649c)"" container ""nginx-ingress-lb"" is unhealthy, it will be killed and re-created.
  16m   40s     12      {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Warning Unhealthy       Liveness probe failed: Get http://172.77.63.3:10249/healthz: dial tcp 172.77.63.3:10249: getsockopt: connection refused
  37s   37s     1       {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Normal  Killing         (events with common reason combined)
  7m    8s      27      {kubelet 16.186.74.94}                                          Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""nginx-ingress-lb"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-du1gp_default(ac3b025c-75a8-11e6-9cd1-00505697649c)""

  12m   8s      47      {kubelet 16.186.74.94}  spec.containers{nginx-ingress-lb}       Warning BackOff Back-off restarting failed docker container

The output of kubectl logs is:
2016/09/08 09:52:38 [debug] 22#22: epoll add event: fd:16 op:1 ev:00002001
2016/09/08 09:52:38 [debug] 23#23: epoll add event: fd:17 op:1 ev:00002001
2016/09/08 09:52:38 [debug] 24#24: epoll add event: fd:18 op:1 ev:00002001
2016/09/08 09:52:38 [debug] 22#22: epoll add event: fd:14 op:1 ev:00002001
2016/09/08 09:52:38 [debug] 22#22: epoll add event: fd:15 op:1 ev:00002001
2016/09/08 09:52:38 [debug] 22#22: epoll add event: fd:20 op:1 ev:00002001
2016/09/08 09:52:38 [debug] 25#25: epoll add event: fd:19 op:1 ev:00002001
2016/09/08 09:52:47 [debug] 24#24: epoll add event: fd:14 op:1 ev:00002001
2016/09/08 09:52:47 [debug] 22#22: epoll del event: fd:14 op:2 ev:00000000
",closed,False,2016-09-08 10:01:28,2016-09-18 01:53:34
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1716,https://api.github.com/repos/kubernetes/contrib/issues/1716,"Remove ""All rights reserved"" from all the headers.","Fixes: #1707

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1716)

<!-- Reviewable:end -->
",closed,True,2016-09-08 11:01:04,2016-09-08 12:30:46
contrib,micheleorsi,https://github.com/kubernetes/contrib/issues/1717,https://api.github.com/repos/kubernetes/contrib/issues/1717,[nginx ingress controller] possibility to specify nginx properties different for each ingress,"Hello,
the problem we are facing is that adding more and more different applications (one for each namespace) in our kubernetes cluster brings different needs in terms of configuration (timeouts for example).

I am already aware of the --watch-namespace property and I followed this discussion #847, but I think that probably a solution like this (https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/customization#using-annotations) would have a couple of benefits:
- the external load balancer (F5 for example) doesn't have to change in order to redirect proper traffic to the proper ingress controller marked with --watch-namespace
- the level of isolation of different namespaces for different applications is strictly linked to different needs (and that's fine in my opinion, without the need of a service loadbalancer)

Probably an intermediate solution could be to just specify a personalized nginx.tmpl.

Thanks
",closed,False,2016-09-08 11:11:59,2016-11-02 11:59:04
contrib,wernight,https://github.com/kubernetes/contrib/issues/1718,https://api.github.com/repos/kubernetes/contrib/issues/1718,Nginx Ingress Controller frequently giving HTTP 503,"For unknown reasons to me, the [Nginx Ingress Controller](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx) is frequently (that is something like every other day with 1-2 deployments a day of Kubernetes Service updates) returning HTTP error 503 for some of the Ingress rules (which point to running working Pods).

Note: Running also [`kube-lego`](https://github.com/jetstack/kube-lego).

Below are logs of Nginx Ingress Controller:

```
$ kubectl logs nginx-ingress
10.10.10.10 - [10.10.10.10, 10.10.10.10] - - [08/Sep/2016:11:17:26 +0000] ""GET / HTTP/1.1"" 503 615 ""-"" ""agent here"" 592 0.000 - - - -
10.10.10.10 - [10.10.10.10] - - [08/Sep/2016:11:17:26 +0000] ""GET / HTTP/2.0"" 503 730 ""-"" ""agent here"" 18 0.001 127.0.0.1:8181 615 0.001 503
10.10.10.10 - [10.10.10.10, 10.10.10.10] - - [08/Sep/2016:11:17:26 +0000] ""GET /favicon.ico HTTP/1.1"" 503 615 ""https://foo.example.com/"" ""agent here"" 510 0.000 - - - -
10.10.10.10 - [10.10.10.10] - - [08/Sep/2016:11:17:26 +0000] ""GET /favicon.ico HTTP/2.0"" 503 730 ""https://foo.example.com/"" ""agent here"" 24 0.001 127.0.0.1:8181 615 0.001 503
```

Looking at `/etc/nginx/nginx.conf` of that nginx-ingress:

```
$ kubectl exec nginx-ingress-controller grep -A 5 foo /etc/nginx/nginx.conf
upstream foo-frontend-80 {
    least_conn;
    server 20.20.0.37:80 max_fails=0 fail_timeout=0;

}
```

And checking that service actual IP of the Pod (because it's bypassing the service visibly):

```
$ kubectl --namespace=foo describe po frontend | grep IP:
IP:             20.20.0.37
```

IP matches, so visibly the reload failed, and doing this fixes it:

```
$ curl -I https://foo.example.com/
HTTP/1.1 503 Service is no longer available

$ kubectl exec nginx-ingress-controller -- nginx -s reload

$ curl -I https://foo.example.com/
HTTP/1.1 200 OK
```

So it looks like there are cases where the reload didn't pick up changes for some reason, or didn't happen, or some concurrency.
",closed,False,2016-09-08 11:17:55,2018-03-10 16:40:47
contrib,caesarxuchao,https://github.com/kubernetes/contrib/pull/1719,https://api.github.com/repos/kubernetes/contrib/issues/1719,Publisher for multiple sources,"Rewrite the publisher to allow gathering contents from multiple sources to the destination repository. For example, `kubernetes/client-go/1.5` gets its content from the `staging/.../1.5` in the **_master**_ branch of the main repository, and `kubernetes/client-go/1.4` gets its content from the `staging/.../1.4` in the **_release-1.4**_ branch of the main repository

Example: https://github.com/caesarxuchao/client-go/commit/a0c3ac3da0ab929aba25d0cdb6cf42885309b06b

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1719)

<!-- Reviewable:end -->
",closed,True,2016-09-08 22:56:13,2016-09-21 19:46:32
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/1720,https://api.github.com/repos/kubernetes/contrib/issues/1720,Add a mungegithub tool for bulk LGTM of PRs,"This adds a tool which lists all small-ish (user configurable) PRs along with their diffs in a condensed format so that a reviewer can quickly skim through a large number of PRs and LGTM them if appropriate.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1720)

<!-- Reviewable:end -->
",closed,True,2016-09-09 05:16:29,2017-01-30 01:41:27
contrib,andrewsykim,https://github.com/kubernetes/contrib/pull/1721,https://api.github.com/repos/kubernetes/contrib/issues/1721,AWS Cluster Autoscaler: NodeGroupForNode should return nil if instance is not in any known ASG,"https://github.com/kubernetes/contrib/issues/1311

We ran into a bug where the `CheckGroupsAndNodes` check fails once we added a second ASG to our cluster that was not maintained by CA. And that's because in `GetAsgForInstance` we should be returning a nil group if the instance doesn't belong to any of the groups that the CA is aware of.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1721)

<!-- Reviewable:end -->
",closed,True,2016-09-09 15:47:14,2016-09-12 18:15:12
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1722,https://api.github.com/repos/kubernetes/contrib/issues/1722,More aggressive node estimation in ClusterAutoscaler,"The current code tries to create nodes with super high utilization. More precisely 

```
new_nodes_count = ceil(sum_of_pending_cpu_requests / capacity) 
```

This leads to multiple iterations as, with many pods, it is unlikely to get the right node count in the first try. We would get better results and faster reaction time if the formula was more like:

```
new_nodes_count = min(pending_pod_count, max(1, round(sum_of_pending_cpu/mem_requests / capacity * 1.3)))
```

With this:
- we will have more healthy cluster than when we try to pack everything super-tight.
- there will be a control of how overprovisioned the cluster can be (1.3 -> max ~30%)
- we wont crete more nodes than the number of pending pods.
- there will be some risk that 10 min after the scale-up something will be scaled-down.
",closed,False,2016-09-09 17:43:19,2016-12-08 13:59:24
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1723,https://api.github.com/repos/kubernetes/contrib/issues/1723,Delete empty nodes in bulk in cluster autoscaler,"Currently we delete only one node at a time, even if there are couple completely empty nodes that can easily be deleted together.
",closed,False,2016-09-09 17:45:10,2017-04-19 10:41:22
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1724,https://api.github.com/repos/kubernetes/contrib/issues/1724,GCE ingress should work in large clusters,"If there are > max nodes allowed in an IG, we should overflow to another IG. Currently this only works if the user also spreads cluster across zones, because each zone gets its own IG. 
",closed,False,2016-09-09 20:27:11,2018-02-15 16:39:17
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1725,https://api.github.com/repos/kubernetes/contrib/issues/1725,Clarify gce ingress limitation around large clusters.,"Just docs

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1725)

<!-- Reviewable:end -->
",closed,True,2016-09-09 20:29:42,2016-09-09 23:10:52
contrib,arnaldopereira,https://github.com/kubernetes/contrib/pull/1726,https://api.github.com/repos/kubernetes/contrib/issues/1726,"Fix cluster autoscaler deployment spec, AWS implementation","Fix verbose flag to `cluster-autoscaler` command line, so it actually runs.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1726)

<!-- Reviewable:end -->
",closed,True,2016-09-09 21:40:39,2016-09-12 10:01:03
contrib,hongchaodeng,https://github.com/kubernetes/contrib/pull/1727,https://api.github.com/repos/kubernetes/contrib/issues/1727,implement approval handler,"ref: https://github.com/kubernetes/contrib/issues/1389

What?
This PR adds ApprovalHandler that will try to add ""approved"" label once all files of change has been approved by approvers.

```
// The algorithm goes as:
// - Initially, we set up approverMap
//   - Go through all comments after latest commit. If any approver said ""/approve"", add him to approverMap.
// - For each file, we see if any approver of this file is in approverMap.
//   - An approver of a file is defined as:
//     - It's known that each dir has a list of approvers. (This might not hold true. For usability, current situation is enough.)
//     - Approver of a dir is also the approver of child dirs.
//   - We look at only top 3 dir level's approvers .
// - Iff all files has been approved, the bot will add ""approved"" label.
```

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1727)

<!-- Reviewable:end -->
",closed,True,2016-09-10 03:25:05,2016-09-15 22:29:30
contrib,k3vinha,https://github.com/kubernetes/contrib/issues/1728,https://api.github.com/repos/kubernetes/contrib/issues/1728,missing proxy environment settings in ansible roles,"missing proxy configuration for yum module in the following files:
ansible/roles/docker/tasks/install-rpms.yml
ansible/roles/master/tasks/install_rpms.yml
ansible/roles/master/tasks/pkgMgrInstallers/centos-install.yml
ansible/roles/node/tasks/install_rpms.yml
ansible/roles/node/tasks/pkgMgrInstallers/centos-install.yml

need to add the following lines for yum in files mentioned:

```
  environment:
    http_proxy: ""{{ http_proxy|default('') }}""
    https_proxy: ""{{ https_proxy|default('') }}""
    no_proxy: ""{{ no_proxy|default('') }}""
```
",closed,False,2016-09-11 09:55:57,2016-09-12 07:11:01
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1729,https://api.github.com/repos/kubernetes/contrib/issues/1729,Add proxy environment settings in yum modules,"Fixes: #1728

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1729)

<!-- Reviewable:end -->
",closed,True,2016-09-12 06:41:27,2016-09-12 07:11:01
contrib,oilbeater,https://github.com/kubernetes/contrib/pull/1730,https://api.github.com/repos/kubernetes/contrib/issues/1730,Fix typo,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1730)

<!-- Reviewable:end -->
",closed,True,2016-09-12 10:32:08,2016-09-12 16:01:35
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1731,https://api.github.com/repos/kubernetes/contrib/issues/1731,Cluster-autoscaler: more readme,"This PR adds some documentation about:
- what is the goal of Cluster Autoscaler
- when and how ScaleUp is executed
- when and how ScaleDown is executed

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1731)

<!-- Reviewable:end -->
",closed,True,2016-09-12 12:54:19,2016-09-29 15:18:23
contrib,eksavoy,https://github.com/kubernetes/contrib/issues/1732,https://api.github.com/repos/kubernetes/contrib/issues/1732,[Ansible] deployement with contiv,"Hi,

I try to create a new kube cluster with ansible and contiv (Network) But I have an error :
`FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Error when trying to enable openvswitch: rc=1 Failed to execute operation: No such file or directory\n""}`

Have you got an idea to solve this problem ?
Thank's
",closed,False,2016-09-12 14:11:30,2018-02-15 16:39:17
contrib,bacongobbler,https://github.com/kubernetes/contrib/issues/1733,https://api.github.com/repos/kubernetes/contrib/issues/1733,GCE missing l7 ingress controller?,"Right now if you deploy or upgrade a GCE cluster to 1.3.6, the l7 ingress controller that is mentioned in http://kubernetes.io/docs/user-guide/ingress/#prerequisites is not present, but instead you get:

```
$ kubectl get pods --namespace=kube-system -l k8s-app=glbc
NAME                            READY     STATUS    RESTARTS   AGE
l7-default-backend-v1.0-bym3n   1/1       Running   0          3d
```

This is not mentioned anywhere in the [GCE release notes](https://cloud.google.com/container-engine/release-notes). Is this an unintentional bug in GCE or are the docs out of date and the ingress controller is held elsewhere?
",closed,False,2016-09-12 15:19:06,2016-09-12 23:07:10
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1734,https://api.github.com/repos/kubernetes/contrib/issues/1734,ClusterAutoscaler: set version to 0.3,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1734)

<!-- Reviewable:end -->
",closed,True,2016-09-12 15:46:18,2016-09-12 15:47:31
contrib,MrHohn,https://github.com/kubernetes/contrib/issues/1735,https://api.github.com/repos/kubernetes/contrib/issues/1735,Check multiple commands on multiple URLs in Exechealthz,"Refer kubernetes/kubernetes#30633.

Would it be possible to support checking multiple commands on multiple URLs in one Exechealthz process? I am going to run two Exechealthz processes within one pod but consider it would be better if Exechealthz supports this intuitively.

@luxas 
",closed,False,2016-09-12 16:55:50,2016-09-22 04:26:37
contrib,foxish,https://github.com/kubernetes/contrib/pull/1736,https://api.github.com/repos/kubernetes/contrib/issues/1736,Added token file to ignore list.,"The secrets generation depends on a file named `token` containing the github token, but that file should be ignored.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1736)

<!-- Reviewable:end -->
",closed,True,2016-09-12 18:27:43,2016-09-12 19:01:37
contrib,foxish,https://github.com/kubernetes/contrib/pull/1737,https://api.github.com/repos/kubernetes/contrib/issues/1737,Adding labels to denote PR state.,"State machine for https://github.com/kubernetes/kubernetes/issues/31260
In order to figure out whose court a PR is in, we currently only use the last IssueComment on the PR. 

Future work:
- Also switch state based on `push` and `reviewComment` additions.
- Creating the relevant labels in the repository.
- Turn up the state machine in contrib.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1737)

<!-- Reviewable:end -->
",closed,True,2016-09-12 20:17:13,2016-10-13 19:06:17
contrib,edana-dev,https://github.com/kubernetes/contrib/issues/1738,https://api.github.com/repos/kubernetes/contrib/issues/1738,How to configure cube-master-url when runing nginx-ingress-controller?,"I got some errors as bellow when I run then command ""$ kubectl create -f examples/default/rc-default.yaml"". I think it because my cube-master server is on the other server. May I ask how to configure cube-master-url?

2016-09-13T12:26:37.040520000Z I0913 12:26:37.039996       1 main.go:94] Using build: https://github.com/bprashanth/contrib.git - git-92b2bac

2016-09-13T12:27:07.043020000Z W0913 12:27:07.042596       1 main.go:118] unexpected error getting runtime information: timed out waiting for the condition

2016-09-13T12:27:07.043166000Z F0913 12:27:07.042840       1 main.go:121] no service with name default/default-http-backend found: Get http://localhost:8080/api/v1/namespaces/default/services/default-http-backend: dial tcp [::1]:8080: getsockopt: connection refused
",closed,False,2016-09-13 12:38:18,2016-09-18 04:05:13
contrib,MrHohn,https://github.com/kubernetes/contrib/pull/1739,https://api.github.com/repos/kubernetes/contrib/issues/1739,Support multiple commands on multiple URLs in exechealthz,"Fixes #1735.

Adding support for multiple commands on URLs for [exec-healthz](https://github.com/kubernetes/contrib/tree/master/exec-healthz).

Example as below:

```
$ ./exechealthz --cmd=""ls /tmp/test1"" --url=""/healthz1"" --cmd=""ls /tmp/test2"" --url=""/healthz2""
```

The --url flag indicates the path healthz server needs to serve on.
Notes: Number of cmds and URLs have to be the same (if more than 1). URL need to start with ""/"". URLs and cmds match up based on their orders (first URL to first cmd).

The previous behavior remains the same (as the previous examples in README).

@thockin @bprashanth @luxas

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1739)

<!-- Reviewable:end -->
",closed,True,2016-09-13 19:13:29,2016-09-22 04:27:06
contrib,andrewsykim,https://github.com/kubernetes/contrib/pull/1740,https://api.github.com/repos/kubernetes/contrib/issues/1740,wattpad makefile,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1740)

<!-- Reviewable:end -->
",closed,True,2016-09-13 20:24:45,2016-09-13 20:39:26
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1741,https://api.github.com/repos/kubernetes/contrib/issues/1741,ClusterAutoscaler: first fit decreasing binpacking-based estimate algorithm,"This should help with #1722.

The goal of the PR is to provide a better, immediate estimation of how many nodes should be added to a cluster to handle a given set of pods. The idea is to run an approximate bin-packing of pending pods to the cluster using FFD heuristic. See 

https://en.wikipedia.org/wiki/Bin_packing_problem

for details.

cc: @piosz @fgrzadkowski

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1741)

<!-- Reviewable:end -->
",closed,True,2016-09-13 21:02:42,2016-10-04 09:59:29
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1742,https://api.github.com/repos/kubernetes/contrib/issues/1742,ClusterAutoscaler may build a node-group template on half-populated node.,"It is possible that some daemon sets fail to be scheduled on a node. If cluster autoscaler picks such a node and use it for building a node-group template in scaleup then it will think that a new node from that nodepgroup will have more capacity than it actually will.  
",closed,False,2016-09-14 19:07:32,2018-02-15 18:41:04
contrib,vyshane,https://github.com/kubernetes/contrib/pull/1743,https://api.github.com/repos/kubernetes/contrib/issues/1743,Allow customisation of the nginx proxy_buffer_size directive via ConfigMap,"I'm opening a new PR with the same changes as #1693 because I pushed the latter with an email address that can't be used to sign the CLA. Description from the previous PR:

When using nginx as a proxy we can run into the following error:

```
upstream sent too big header while reading response header from upstream
```

In order to fix this, we need to be able to configure the proxy_buffer_size nginx directive to increase its value. This PR updates the nginx-ingress-controller to allow that.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1743)

<!-- Reviewable:end -->
",closed,True,2016-09-15 15:19:58,2016-09-15 16:49:58
contrib,suyogbarve,https://github.com/kubernetes/contrib/pull/1744,https://api.github.com/repos/kubernetes/contrib/issues/1744,three card poker game to demo n-tier/microservice architecture,"This is an interesting n-tier/microservice based three-card-poker game. It also contains a demo-client application which dynamically creates a view of pods (in each layer) as they are deployed. 
This works as an interesting demo to showcase the ease of deploying a heterogenous application environment.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1744)

<!-- Reviewable:end -->
",closed,True,2016-09-15 17:10:51,2016-10-15 01:21:40
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1745,https://api.github.com/repos/kubernetes/contrib/issues/1745,Create webhooks-listener to listen to webhooks,"Listens for webhooks, verify the signature, and pushes them (along with
event type) into a pub-sub queue.

All of this can be configured through a configmap: special path to push
along with github secret-key and topic to use.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1745)

<!-- Reviewable:end -->
",closed,True,2016-09-15 18:23:54,2016-09-21 17:46:28
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1746,https://api.github.com/repos/kubernetes/contrib/issues/1746,Document and consider exposing backend timeouts,"The timeout from proxy -> backend, eg: https://cloud.google.com/compute/docs/load-balancing/http/backend-service#create_a_backend_service, at least needs per lb documentation, and probably can be surfaced better through annotations/fields since it's a fairly common concept in l7 proxies
",closed,False,2016-09-15 18:39:33,2018-02-15 18:41:05
contrib,timstclair,https://github.com/kubernetes/contrib/pull/1747,https://api.github.com/repos/kubernetes/contrib/issues/1747,Update Apparmor examples to beta annotations,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1747)

<!-- Reviewable:end -->
",closed,True,2016-09-15 18:42:14,2016-09-20 01:14:02
contrib,ixdy,https://github.com/kubernetes/contrib/issues/1748,https://api.github.com/repos/kubernetes/contrib/issues/1748,test flake shame mailer should not send emails for P2/P3 flakes ,"Currently, the test flake shame mailer emails everyone who has a `kind/flake` issue assigned to them, but it only lists P0/P1 bugs in the email body. We should reconcile this.
",closed,False,2016-09-15 23:04:47,2018-01-16 18:45:40
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1749,https://api.github.com/repos/kubernetes/contrib/issues/1749,[nginx-ingress-controller] Readiness probe that works behind a CP lb,"fixes #1507

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1749)

<!-- Reviewable:end -->
",closed,True,2016-09-16 21:24:47,2016-09-26 22:44:13
contrib,edouardKaiser,https://github.com/kubernetes/contrib/issues/1750,https://api.github.com/repos/kubernetes/contrib/issues/1750,[Ingress] Service/Upstream name in the log,"Is there a way to get the service (or upstream) name logged in the NGINX log_format ?

We are trying to export the NGINX logs into Elasticsearch, but we are only interested by the logs of particular applications. Right now, it's all mixed, there's no way to differentiate which pod/service is targeted. Sure, the targeted upstream address gets logged, but that does not help to process the log.
",closed,False,2016-09-18 12:44:26,2016-09-19 21:26:50
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1751,https://api.github.com/repos/kubernetes/contrib/issues/1751,[nginx-ingress-controller] Add the name of the upstream in the log,"fixes #1750

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1751)

<!-- Reviewable:end -->
",closed,True,2016-09-19 04:38:41,2016-09-19 21:26:51
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1752,https://api.github.com/repos/kubernetes/contrib/issues/1752,node-perf-dash: Deployed v0.2,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1752)

<!-- Reviewable:end -->
",closed,True,2016-09-19 16:45:01,2016-09-19 17:37:17
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1753,https://api.github.com/repos/kubernetes/contrib/issues/1753,mungegithub: Fix flaky issue nagger timings,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1753)

<!-- Reviewable:end -->
",closed,True,2016-09-19 17:41:34,2016-09-19 18:36:49
contrib,aledbf,https://github.com/kubernetes/contrib/issues/1754,https://api.github.com/repos/kubernetes/contrib/issues/1754,[nginx-ingress-controller] Avoid Ingress with same path that healthz url,,closed,False,2016-09-19 21:44:51,2016-10-31 19:46:03
contrib,Q-Lee,https://github.com/kubernetes/contrib/pull/1755,https://api.github.com/repos/kubernetes/contrib/issues/1755,Refactoring kubelet-to-gcm and adding kube-controller.,"This pulls the node evictions metric the kube-controller's prometheus endpoint, and pushes it to Stackdriver. I've repurposed the kubelet monitoring agent for this, and refactored it to make further extensions easy.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1755)

<!-- Reviewable:end -->
",closed,True,2016-09-19 22:42:57,2016-09-30 20:15:00
contrib,MrHohn,https://github.com/kubernetes/contrib/pull/1756,https://api.github.com/repos/kubernetes/contrib/issues/1756,Redirect STOPSIGNAL for kube-dns graceful termination,"Fixes kubernetes/kubernetes#33050.

Current `dnsmasq` container exit immediately after docker sends STOPSIGNAL. In order to support graceful termination as kubernetes/kubernetes#31807 indicated, we need to delay this operation.

Similar to that we do in `kubedns` --- ignoring the SIGTERM and exit when SIGKILL from docker occurs, @thockin's solution is to redirect docker's STOPSIGNAL to SIGCONT. I did a couple rounds of test and it works as expected.

The tag should be bumped up when we need to get the graceful termination feature in.

@girishkalele @ArtfulCoder @luxas

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1756)

<!-- Reviewable:end -->
",closed,True,2016-09-20 00:40:10,2016-09-20 06:06:23
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1757,https://api.github.com/repos/kubernetes/contrib/issues/1757,[ubuntu-slim] Release 0.5,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1757)

<!-- Reviewable:end -->
",closed,True,2016-09-20 13:19:49,2016-10-14 17:46:28
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1758,https://api.github.com/repos/kubernetes/contrib/issues/1758,[nginx-slim] Release 0.10,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1758)

<!-- Reviewable:end -->
",closed,True,2016-09-20 13:20:41,2016-10-14 17:46:32
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1759,https://api.github.com/repos/kubernetes/contrib/issues/1759,[nginx-ingress-controller] Add support for default backend in Ingress rule,"fixes #1590

<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1759)

<!-- Reviewable:end -->
",closed,True,2016-09-20 15:07:19,2016-10-03 19:29:31
contrib,cescoferraro,https://github.com/kubernetes/contrib/pull/1760,https://api.github.com/repos/kubernetes/contrib/issues/1760,deployments option for nginx,"add Deployment alternative the Replication Controller when deploying the Nginx Ingress Controller

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1760)

<!-- Reviewable:end -->
",closed,True,2016-09-20 15:08:44,2017-04-04 15:13:34
contrib,alph486,https://github.com/kubernetes/contrib/issues/1761,https://api.github.com/repos/kubernetes/contrib/issues/1761,Name-based Vhosts in Nginx Ingress Errors out,"When using the current `nginx-alpha` ingress controller and the following ingress:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
  namespace: my-ns
spec:
  rules:
  - host: myservice.domaon.name
    http:
      paths:
      - backend:
          serviceName: svc-planner
          servicePort: 3000
```

The following error occurs in the ingress controller:

```
2016/09/20 16:48:14 Failed to execute nginx -s reload: nginx: [emerg] invalid number of arguments in ""location"" directive in /etc/nginx/nginx.conf:13
, err: exit status 1
```

This also occurs with multiple backends.
",closed,False,2016-09-20 17:45:08,2016-09-21 20:35:23
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1762,https://api.github.com/repos/kubernetes/contrib/issues/1762,[nginx-ingress-controller] Add cloud detection,"fixes #1665

<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1762)

<!-- Reviewable:end -->
",closed,True,2016-09-20 23:18:13,2016-10-07 23:56:20
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/1763,https://api.github.com/repos/kubernetes/contrib/issues/1763,Is possible to run 2 ingress controller for different usage in a cluster,"Hi,

We plan to run 2 ingress controller: one for front end proxy and load balancing; another is for backend load balancing. For this kind of requirement, how to config the ingress controller yaml? As we want to avoid the single point of failure issue, we need 2 controllers. 

For the front end controller, it can run on a specific node which is labelled as ""role=loadbalancer""
For the backend controller, we want it to run as a service whose type is NodePort.

Should I build different ingress controller image for these 2 objectives? If yes, when create the Ingress resource, how to specify which controller we should use?

Thanks
",closed,False,2016-09-21 10:01:30,2016-12-12 05:37:52
contrib,foxish,https://github.com/kubernetes/contrib/pull/1764,https://api.github.com/repos/kubernetes/contrib/issues/1764,Updating CLA labels to CNCF ones.,"This will **not** remove the `cla:yes/no` labels, it will simply add two new ones.
The plan is to later remove (manually) the `cla:yes/no` labels on contrib, which are set by the Google CLA bot and let mungegithub set `cncf-cla: yes` and `cncf-cla: no` instead.

cc @bgrant0607

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1764)

<!-- Reviewable:end -->
",closed,True,2016-09-21 17:09:40,2016-09-21 20:35:22
contrib,foxish,https://github.com/kubernetes/contrib/pull/1765,https://api.github.com/repos/kubernetes/contrib/issues/1765,Modifications to allow submit-queue to honor cncf labels.,"This change lets the submit-queue merge PRs when it sees the cncf-cla:yes **or** cla:yes labels.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1765)

<!-- Reviewable:end -->
",closed,True,2016-09-21 17:34:07,2016-10-03 17:38:18
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1766,https://api.github.com/repos/kubernetes/contrib/issues/1766,[nginx-ingress-controller] Clarify the controller uses endpoints and not services,"fixes #1611

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1766)

<!-- Reviewable:end -->
",closed,True,2016-09-21 20:54:10,2016-09-21 21:26:34
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1767,https://api.github.com/repos/kubernetes/contrib/issues/1767,[ingress] Update godeps,"Against tag v1.4.0-beta.9

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1767)

<!-- Reviewable:end -->
",closed,True,2016-09-21 23:06:23,2017-01-31 18:07:41
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1768,https://api.github.com/repos/kubernetes/contrib/issues/1768,[keepalived-vip] Release 0.9,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1768)

<!-- Reviewable:end -->
",closed,True,2016-09-22 00:44:53,2016-10-14 17:46:47
contrib,gouyang,https://github.com/kubernetes/contrib/issues/1769,https://api.github.com/repos/kubernetes/contrib/issues/1769,[ansible] Update flanneld config file,"On RHEL, the flannel-0.5.5-1 is using environment vairables

```
ExecStart=/usr/bin/flanneld -etcd-endpoints=${FLANNEL_ETCD_ENDPOINTS} -etcd-prefix=${FLANNEL_ETCD_PREFIX}
```

On Fedora 24, flannel-0.5.5-6.fc24.x86_64 is still using 

```
ExecStart=/usr/bin/flanneld -etcd-endpoints=${FLANNEL_ETCD} -etcd-prefix=${FLANNEL_ETCD_KEY}
```

Should update `contrib/ansible/roles/flannel/templates/flanneld.j2` accordingly, no rush.
",closed,False,2016-09-22 06:43:18,2018-01-17 04:02:02
contrib,kayrus,https://github.com/kubernetes/contrib/issues/1770,https://api.github.com/repos/kubernetes/contrib/issues/1770,ingress: implement --watch-namespaces and --ignore-namespaces in nginx controller,"current `--watch-namespace` implementation doesn't work well.
- first of all you can specify only one namespace or all namespaces.
- when you already have system namespaces (i.e. monitoring, kube-system, etc.) along with several production namespaces, it is not possible to schedule ingress controller which will serve only production namespaces (because we can schedule only one ingress controller per node)

`--ignore-namespaces` will be suitable for system namespaces (blacklist). `--watch-namespaces` will be suitable for production namespaces (whitelist).

Also I suppose this could be implemented inside configmaps and won't require ingress controller restart (since daemonsets don't support rolling update).
",closed,False,2016-09-22 12:39:40,2018-03-02 19:35:17
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1771,https://api.github.com/repos/kubernetes/contrib/issues/1771,Bump glbc to 0.8.0,"Don't think this hits the bar for 1.4.0, but hopefully it can make 1.4.1. The version bump is for the godep update that fixes an issue with the throttling workqueue (https://github.com/kubernetes/kubernetes/pull/31396). I should've done this sooner, dropped it.

Also fixes https://github.com/kubernetes/contrib/issues/1776 and https://github.com/kubernetes/contrib/issues/1783

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1771)

<!-- Reviewable:end -->
",closed,True,2016-09-22 15:23:53,2016-09-27 21:39:04
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1772,https://api.github.com/repos/kubernetes/contrib/issues/1772,[nginx-ingress-controller] Avoid replacing nginx.conf file with invalid content,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1772)

<!-- Reviewable:end -->
",closed,True,2016-09-22 17:09:25,2016-09-29 01:08:20
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1773,https://api.github.com/repos/kubernetes/contrib/issues/1773,[nginx-ingress-controller] Add annotation to add CORS support,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1773)

<!-- Reviewable:end -->
",closed,True,2016-09-22 18:01:04,2016-12-01 15:11:49
contrib,foxish,https://github.com/kubernetes/contrib/pull/1774,https://api.github.com/repos/kubernetes/contrib/issues/1774,Enabling cla functionality in mungegithub for contrib.,"This PR contains:
- Modifications to the CLA munger to enable ping notifications
- Fixes to the deployment, and addition of a new parameter required by the CLA munger.
- Turning on the mungegithub handling of CLA on contrib.

cc @sarahnovotny @bgrant0607

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1774)

<!-- Reviewable:end -->
",closed,True,2016-09-22 19:36:22,2016-09-22 21:36:41
contrib,beeradb,https://github.com/kubernetes/contrib/pull/1775,https://api.github.com/repos/kubernetes/contrib/issues/1775,Fix link to auth example,"The link to the authentication example is incorrect. This fixes it.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1775)

<!-- Reviewable:end -->
",closed,True,2016-09-22 20:36:29,2016-09-22 21:06:40
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1776,https://api.github.com/repos/kubernetes/contrib/issues/1776,gce ingress is too eager in converting probes to health checks,"the gce ingress controller goes looking in `AllNamespaces` for pods matching selectors of a given service, to convert their readiness probes into http health checks. It should only adopt readiness probes from pods in the same namespace as the service. 
",closed,False,2016-09-22 22:01:00,2016-09-27 21:39:04
contrib,caesarxuchao,https://github.com/kubernetes/contrib/issues/1777,https://api.github.com/repos/kubernetes/contrib/issues/1777,Add upgrade tests as non-blocking tests in submit-queue,"...so that the bot will create issues for flaky upgrade tests.

Do we need all these tests?
- https://k8s-testgrid.appspot.com/google-1.2-1.4-upgrade
- https://k8s-testgrid.appspot.com/google-1.3-1.4-upgrade
- https://k8s-testgrid.appspot.com/google-kubectl-skew (except the tests for 1.2-1.3 compatibility)

If we need all of these tests, we'll need a script to populate the configMap key: https://github.com/kubernetes/contrib/blob/master/mungegithub/submit-queue/deployment/kubernetes/configmap.yaml#L19

@pwittrock @bprashanth @lavalamp @foxish 
",closed,False,2016-09-22 23:23:40,2017-01-21 01:40:06
contrib,MrHohn,https://github.com/kubernetes/contrib/pull/1778,https://api.github.com/repos/kubernetes/contrib/issues/1778,Adopt go-build-template and push images as 1.2,"The latest version(1.2) exechealthz docker images have been pushed to gcr.io. Listed as below:
- gcr.io/google_containers/exechealthz:1.2
- gcr.io/google_containers/exechealthz-amd64:1.2
- gcr.io/google_containers/exechealthz-arm:1.2
- gcr.io/google_containers/exechealthz-arm64:1.2
- gcr.io/google_containers/exechealthz-ppc64le:1.2

Previous Makefile is broken due to the new appear vendor folder. It is fixed now.

I tried [go-build-template](https://github.com/thockin/go-build-template) but found it not very friendly to set multiple BASEIMAGEs for different ARCHs. So I switch back.

@thockin @bprashanth

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1778)

<!-- Reviewable:end -->
",closed,True,2016-09-22 23:33:44,2016-09-26 20:41:33
contrib,Max-bazinga,https://github.com/kubernetes/contrib/issues/1779,https://api.github.com/repos/kubernetes/contrib/issues/1779,"create a loadbalancer, the container's status always be CrashLoopBackOff","Hi, teamer:
I download the latest contrb and follow the example to create a  loadbalancer, using the cmd ""kubectl create -f ./rc.yaml"". But the status of the service-loadbalancer is always CrashLoopBackOff. The container's log are displayed as below:

  12m   12m 1   {kubelet 192.168.0.123} spec.containers{haproxy}    Normal  Created Created container with docker id 48b955197dc5
  12m   12m 1   {kubelet 192.168.0.123} spec.containers{haproxy}    Warning Failed  Failed to start container with docker id 48b955197dc5 with error: API error (500): Cannot start container 48b955197dc5e26813c9f5618352f220a5e553db34f9936bdc8398bc739b8a1d: [9] System error: exec format error
I want to now what's wrong with this?
",closed,False,2016-09-23 08:16:48,2016-09-23 08:32:49
contrib,kayrus,https://github.com/kubernetes/contrib/pull/1780,https://api.github.com/repos/kubernetes/contrib/issues/1780,ansible: Netmaster interface typo fixed,"subj

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1780)

<!-- Reviewable:end -->
",closed,True,2016-09-23 09:06:35,2016-09-23 09:49:26
contrib,spxtr,https://github.com/kubernetes/contrib/pull/1781,https://api.github.com/repos/kubernetes/contrib/issues/1781,Remove rebuild-request munger.,"Rather than add k8s-merge-robot to the list of OK bot names, lets just delete this munger. We can get this behavior back properly this time by adding it to ciongke.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1781)

<!-- Reviewable:end -->
",closed,True,2016-09-23 17:29:32,2016-09-23 18:12:21
contrib,mtaufen,https://github.com/kubernetes/contrib/pull/1782,https://api.github.com/repos/kubernetes/contrib/issues/1782,Add GCI HEAD and 1.4 jobs to submit queue non-blocking list,"So issues in these jobs are automatically filed.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1782)

<!-- Reviewable:end -->
",closed,True,2016-09-23 18:20:59,2016-09-23 21:22:23
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1783,https://api.github.com/repos/kubernetes/contrib/issues/1783,gce ingress should handle health check reconciliation better,"Currently the gce controller doesn't reconcile health checks periodically. This was so users could modify the health check. Unfortunately it also doesn't snapshot, so on restart, it comes up, checks for health check, checks the path, and ends up clobbering what a user put in. 

We should just create the health check and backoff from reconciliation till we have a better story around how we're going to expose it through the ingress. 

In 0.7.1 and later, we already adopt the users readiness probe through a cloud provider health check, maybe that's enough for the end goal here. 
",closed,False,2016-09-23 21:38:17,2018-02-15 20:43:05
contrib,mtaufen,https://github.com/kubernetes/contrib/pull/1784,https://api.github.com/repos/kubernetes/contrib/issues/1784,Add more 1.4 and head GCI jobs to non-blocking submit queue,"/cc @vishh

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1784)

<!-- Reviewable:end -->
",closed,True,2016-09-23 23:59:36,2016-09-27 21:54:26
contrib,mtaufen,https://github.com/kubernetes/contrib/pull/1785,https://api.github.com/repos/kubernetes/contrib/issues/1785,Add GCI kubernetes-pull tests to submit-queue.presubmit-jobs,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1785)

<!-- Reviewable:end -->
",closed,True,2016-09-24 00:45:29,2016-09-27 22:29:06
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1786,https://api.github.com/repos/kubernetes/contrib/issues/1786,[nginx-ingress-controller] Add docs about go template,"Address https://github.com/kubernetes/contrib/pull/1711#issuecomment-249474385

<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1786)

<!-- Reviewable:end -->
",closed,True,2016-09-26 14:41:45,2016-09-26 22:44:20
contrib,thockin,https://github.com/kubernetes/contrib/pull/1787,https://api.github.com/repos/kubernetes/contrib/issues/1787,Prep for 1.2.0 release,"Run container as non-root
Add a --version flag

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1787)

<!-- Reviewable:end -->
",closed,True,2016-09-26 19:54:20,2016-09-26 22:44:16
contrib,pschiffe,https://github.com/kubernetes/contrib/issues/1788,https://api.github.com/repos/kubernetes/contrib/issues/1788,[ansible] add variable to configure KUBELET_ARGS in kubelet.j2 template of node role,"Currently it's not possible to provide custom options for `KUBELET_ARGS` in `/etc/kubernetes/kubelet` file. So for example, if I want my pods to run with host network (`KUBELET_ARGS=""--host-network-sources=*""`), I have to manually edit `/etc/kubernetes/kubelet` file on every node.
",closed,False,2016-09-27 15:10:45,2016-11-18 11:07:24
contrib,brendandburns,https://github.com/kubernetes/contrib/issues/1789,https://api.github.com/repos/kubernetes/contrib/issues/1789,mungegithub old-doc-bot doesn't allow delete only changes,"We should allow delete-only changes since they are inherently making the world a better place, e.g.:

https://github.com/kubernetes/kubernetes/pull/33200
",closed,False,2016-09-27 15:30:38,2017-01-21 01:36:27
contrib,smarterclayton,https://github.com/kubernetes/contrib/issues/1790,https://api.github.com/repos/kubernetes/contrib/issues/1790,Ingress controller reporting concurrent map writes,"Reported by @ichekrygin against v1.3.6 in https://github.com/openshift/origin/issues/10826#issuecomment-249714898

Issue below is https://github.com/kubernetes/kubernetes/pull/32125

---

I am hitting this issue in v1.3.6:

```
fatal error: concurrent map read and map write

goroutine 140812 [running]:
runtime.throw(0x1b9f100, 0x21)
        /usr/local/go/src/runtime/panic.go:547 +0x90 fp=0xc82059eaf8 sp=0xc82059eae0
runtime.mapaccess1_faststr(0x148a4a0, 0xc820317f50, 0xc8208fd0e0, 0x15, 0x1)
        /usr/local/go/src/runtime/hashmap_fast.go:202 +0x5b fp=0xc82059eb58 sp=0xc82059eaf8
k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache.(*DeltaFIFO).queueActionLocked(0xc8200e5b80, 0x1a3e8c8, 0x4, 0x19dde80, 0xc820d01348, 0x0, 0x0)
        /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache/delta_fifo.go:305 +0x1dd fp=0xc82059ecc0 sp=0xc82059eb58
k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache.(*DeltaFIFO).Resync(0xc8200e5b80, 0x0, 0x0)
        /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache/delta_fifo.go:511 +0x4f7 fp=0xc82059ee38 sp=0xc82059ecc0
k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache.(*Reflector).ListAndWatch.func1(0xc8200c01e0, 0xc82008ec00, 0xc820612000, 0xc820cf04e0, 0xc8200c01e8)
        /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache/reflector.go:289 +0x252 fp=0xc82059ef88 sp=0xc82059ee38
runtime.goexit()
        /usr/local/go/src/runtime/asm_amd64.s:1998 +0x1 fp=0xc82059ef90 sp=0xc82059ef88
created by k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache.(*Reflector).ListAndWatch
```

This issue impacts ingress-controller resulting in frequent restarts

```
nginx-ingress-4280m             1/1       Running   25         5d
nginx-ingress-5dv2y             1/1       Running   25         5d
nginx-ingress-6pwtz             1/1       Running   16         3d
nginx-ingress-vt8cr             1/1       Running   24         5d
```
",closed,False,2016-09-27 16:02:27,2018-02-17 12:22:00
contrib,svda,https://github.com/kubernetes/contrib/issues/1791,https://api.github.com/repos/kubernetes/contrib/issues/1791,Allow mixed http / https GCLB Ingress rules,"I have an ingress fanout running with two https services, but I would like to add a third one using http only. This is impossible in the current config, which is as follows:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: load-balancer
  # namespace: production
  annotations:
    kubernetes.io/ingress.allow-http: ""false""
spec:
  tls:
  - secretName: myservice-tls
  backend:
    serviceName: api
    servicePort: 80
  rules:
  - host: api.myservice.com
    http:
      paths:
      - path: /*
        backend:
          serviceName: api
          servicePort: 80
  - host: developer.myservice.com
    http:
      paths:
      - path: /*
        backend:
          serviceName: developer
          servicePort: 80
```

Something more flexible should be implemented, for example allowing annotations inside fanout rules. Not sure if this is possible, but this would also open the door to add http > https redirects, multi-tls (like https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/examples/multi-tls/multi-tls.yaml), etc.
",closed,False,2016-09-27 16:13:05,2018-06-17 19:03:55
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1792,https://api.github.com/repos/kubernetes/contrib/issues/1792,WIP: [nginx-ingress-controller] Release 0.9,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1792)

<!-- Reviewable:end -->
",closed,True,2016-09-27 17:13:34,2016-10-17 17:49:12
contrib,mtaufen,https://github.com/kubernetes/contrib/pull/1793,https://api.github.com/repos/kubernetes/contrib/issues/1793,Add more gci jobs to the non-blocking list,"So issues will be auto-filed for test failures in these jobs.

/cc @vishh

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1793)

<!-- Reviewable:end -->
",closed,True,2016-09-27 18:01:52,2016-09-27 18:29:03
contrib,mtaufen,https://github.com/kubernetes/contrib/pull/1794,https://api.github.com/repos/kubernetes/contrib/issues/1794,"Add non-blocking jobs for ""hidden"" GCI tests","These are test jobs that were switched to GCI but don't have gci in the job name.

I think this should be it for today.

/cc @vishh

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1794)

<!-- Reviewable:end -->
",closed,True,2016-09-27 18:35:46,2016-09-27 22:29:07
contrib,emmanuel,https://github.com/kubernetes/contrib/issues/1795,https://api.github.com/repos/kubernetes/contrib/issues/1795,[nginx-ingress-controller] Proxy protocol example has bug in health check,"It appears that the proxy protocol setting (set via configmap) breaks the default health check. Nginx is expecting PROXY protocol on the default HTTP port, but the kubelet is just speaking plain HTTP. The 18080 port looks expressly designed for this purpose, so I was able to get things working merely by switching the readiness/liveness probe config to look like the following:

``` yaml
          readinessProbe:
            httpGet:
              path: /healthz
              port: 18080
              scheme: HTTP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 18080
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
```
",closed,False,2016-09-28 02:11:10,2017-12-18 03:34:41
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1796,https://api.github.com/repos/kubernetes/contrib/issues/1796,[nginx-ingress-controller] Add external authentication using auth_request,"fixes #1492

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1796)

<!-- Reviewable:end -->
",closed,True,2016-09-28 02:25:44,2016-10-03 15:48:14
contrib,gg7,https://github.com/kubernetes/contrib/pull/1797,https://api.github.com/repos/kubernetes/contrib/issues/1797,ingress/controllers/README.md: Fix a link,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1797)

<!-- Reviewable:end -->
",closed,True,2016-09-28 15:34:12,2016-09-28 16:08:18
contrib,jack0,https://github.com/kubernetes/contrib/pull/1798,https://api.github.com/repos/kubernetes/contrib/issues/1798,Added more pod buckets for basic rescheduling priorization,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1798)

<!-- Reviewable:end -->
",closed,True,2016-09-28 16:14:58,2016-09-28 16:16:15
contrib,metral,https://github.com/kubernetes/contrib/issues/1799,https://api.github.com/repos/kubernetes/contrib/issues/1799,[nginx-ingress-controller] Ingress address counts don't match ingress-controller replica count,"I'm running into an anomaly where the Ingress resource for my nginx-ingress-controller has a count of addresses for a host that **do not** actively match the number of replicas of the nginx-ingress-controller itself.

I'm running nginx-ingress-controller v0.8.3 in a Deployment with no hostPort, but instead a NodePort-typed Service fronting the Deployment, and the Ingress resource references this Service for a single host on a single path, in its rules.

This nginx-ingress-controller + Ingress + NodePort Service setup was chosen based on docs read, and mainly done to allow each namespace to run its own nginx-ingress-controller. Each controller is configured in its own namespace, with its own default backend as well, and a `--watch-namespace=$(POD_NAMESPACE)` to only watch for Ingresses in the namespace it operates in.

By default, the ingress-controller Deployment only has 1 replica.

The nginx-ingress-controller is working and I have external access to my app, but in a spurt of curiosity, I scaled the ingress-controller Deployment from 1 replica to 4 (I have 3 Nodes in my cluster), and the address count reported by the ingress resource shows that I have 3 addresses for the Ingress host (one for each of the 3 Nodes). It seemed logical to only have 3 addresses for the Ingress after scaling up as I only had 3 Nodes in the cluster. I also figured that by using a NodePort-typed Service, iptables was already handling the forwarding of the packets for the 4th replica, wherever it ended up living in the set of 3 Nodes, given that I can't physically have a 4th address.

But when I scaled the Deployment back down to 1 replica, the ingress resource address count changed to 2 addresses, instead of 1.

So questions:
1. In this setup, how many addresses _should_ the Ingress resource be reporting? Is it supposed to match the number of replicas of the ingress-controller itself?
   - Better said, how come when I scale down to 1 replica, down from 4, do I have any number of addresses other than a single one being reported by the Ingress? Is this logically incorrect?
2. Given that the ingress-controller's Service is a NodePort type, which opens the same random port on **ALL** nodes, shouldn't the addresses listed for the ingress **always** be the address for each Node, or is there something strange going on that I haven't caught onto given the nature of the nginx-ingress-controller's role?
   - I primarily ask this because even in cases where only one single address is reported for the Ingress, the host:NodePort for **all** Nodes still works in an external curl command to the app, proxying to the app just fine, no matter the Node IP used - the Service used for the Ingress is a NodePort Service after all, so this should be the case, regardless. Is it possible that something to do with the logic of which addresses the Ingress reports to satisfy the host, is off?

Thanks!

/cc @aledbf @bprashanth 
",closed,False,2016-09-28 21:46:04,2018-02-15 23:46:02
contrib,kiranmantri,https://github.com/kubernetes/contrib/pull/1800,https://api.github.com/repos/kubernetes/contrib/issues/1800,building for s390x architecture (aka IBM mainframe),"building support for s390x architecture, as already supported by golang1.7

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1800)

<!-- Reviewable:end -->
",closed,True,2016-09-29 00:26:16,2016-09-30 19:02:22
contrib,kayrus,https://github.com/kubernetes/contrib/issues/1801,https://api.github.com/repos/kubernetes/contrib/issues/1801,"ingress: uninitialized ""proxy_upstream_name"" variable while logging request","I tried the latest nginx.tmpl with the  https://github.com/kubernetes/contrib/pull/1751 change and get lots of error logs in nginx pod:

```
2016/09/29 14:49:36 [warn] 24#24: *454 using uninitialized ""proxy_upstream_name"" variable while logging request, client: 1.2.3.4, server: example.com, request: ""HEAD / HTTP/1.0"", host: ""example.com""
```

these logs relate to    newrelic:

```
1.2.3.4 example.com [1.2.3.4] - - [29/Sep/2016:14:54:58 +0000] ""HEAD / HTTP/1.0"" 301 0 ""-"" ""NewRelicPinger/1.0 (767762)"" 219 0.000 [] - - - -
```
",closed,False,2016-09-29 14:52:58,2016-09-29 18:28:25
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1802,https://api.github.com/repos/kubernetes/contrib/issues/1802,[nginx-ingress-controller] Initialize proxy_upstream_name variable,"fixes #1801

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1802)

<!-- Reviewable:end -->
",closed,True,2016-09-29 15:03:15,2016-09-29 19:38:24
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1803,https://api.github.com/repos/kubernetes/contrib/issues/1803,mungegithub: Update go-github dependency,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1803)

<!-- Reviewable:end -->
",closed,True,2016-09-29 16:54:29,2016-09-29 17:39:24
contrib,MrHohn,https://github.com/kubernetes/contrib/pull/1804,https://api.github.com/repos/kubernetes/contrib/issues/1804,Bump up dnsmasq to 1.4 for graceful termination,"Below images are built and pushed:
- gcr.io/google_containers/kube-dnsmasq-amd64:1.4
- gcr.io/google_containers/kube-dnsmasq-arm:1.4
- gcr.io/google_containers/kube-dnsmasq-arm64:1.4
- gcr.io/google_containers/kube-dnsmasq-ppc64le:1.4

I will soon send out another PR to bump up kube-dns addon in the kubernetes repo.

@thockin @bprashanth

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1804)

<!-- Reviewable:end -->
",closed,True,2016-09-29 17:36:23,2016-09-29 17:37:39
contrib,kayrus,https://github.com/kubernetes/contrib/issues/1805,https://api.github.com/repos/kubernetes/contrib/issues/1805,ingress: controller doesn't reread template when it was modified inside the pod,"I use this example https://github.com/kubernetes/contrib/blob/6bc22addcdf29c531ebd1fa449aa8de2d840e242/ingress/controllers/nginx/examples/custom-template/custom-template.yaml#L52..L55
And when I update configmap with the new template I don't see any changes or related logs inside the ingress pod. I suppose it is related to inotify, but I'm not sure.
",closed,False,2016-09-29 18:02:51,2018-02-16 03:50:02
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1806,https://api.github.com/repos/kubernetes/contrib/issues/1806,[nginx-ingress-controller] Add docs about the log format,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1806)

<!-- Reviewable:end -->
",closed,True,2016-09-29 19:41:50,2016-09-30 17:58:29
contrib,emmanuel,https://github.com/kubernetes/contrib/issues/1807,https://api.github.com/repos/kubernetes/contrib/issues/1807,[nginx-ingress-controller] Feature request: enable session stickiness per-ingress,"We are currently using the ingress-global session stickiness, but have apps with different needs and would like to be able to utilize stickiness on specific ingresses. Based on the other annotation-based configurations that are available per-ingress, it looks like this is not a huge change, since the capability to configure upstreams per-ingress is already present. WDYT?
",closed,False,2016-09-29 20:06:08,2018-02-19 12:08:58
contrib,whitlockjc,https://github.com/kubernetes/contrib/pull/1808,https://api.github.com/repos/kubernetes/contrib/issues/1808,ingress/controllers/nginx: WebSocket documentation,"For those that do not understand the default way in which nginx proxies
requests not containing a ""Connection"" header, the approach for enabling
WebSocket support might not make sense.  This commit adds documentation
that explains why things are done this way.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1808)

<!-- Reviewable:end -->
",closed,True,2016-09-29 21:46:52,2016-10-05 17:09:18
contrib,odacremolbap,https://github.com/kubernetes/contrib/issues/1809,https://api.github.com/repos/kubernetes/contrib/issues/1809,HAProxy Ingress Controller,"**FEATURE REQUEST /  CONTRIBUTION**

We want to contribute our **HAProxy** Ingress controller.

It is simple enough to get going easily, only Ingress spec are supported.
That limits the features of the Ingress (no configuration management through config maps, no advanced configuration using annotations, not a lot of parameters).

Although backends (kubernetes watchers, stores, ...) are mostly the same for every ingress controller, unifying all ingresses is a WIP that will require an agreement on arguments, annotations, config maps ...

That won't probably happen very soonish, so we would like to contribute this HAProxy controller for anyone who wants to use it.
",closed,False,2016-09-29 22:24:30,2018-02-15 22:45:07
contrib,odacremolbap,https://github.com/kubernetes/contrib/pull/1810,https://api.github.com/repos/kubernetes/contrib/issues/1810,HAProxy Ingress Controller,"Related to #1809 

Simple Ingress spec only HAProxy ingress controller

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1810)

<!-- Reviewable:end -->
",closed,True,2016-09-29 22:28:17,2017-09-07 15:29:46
contrib,foxish,https://github.com/kubernetes/contrib/pull/1811,https://api.github.com/repos/kubernetes/contrib/issues/1811,Honor lgtm label applied manually.,"Should prevent issues like https://github.com/kubernetes/contrib/pull/1719#issuecomment-248719633
This teaches the bot to allow human users to override the `/lgtm` and `/lgtm cancel` behaviors by directly adding/removing the label.

This is the last blocker to rolling out `/lgtm` to the main repository.

cc @kubernetes/contributor-experience @bgrant0607 @anguslees @eparis @grodrigues3 

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1811)

<!-- Reviewable:end -->
",closed,True,2016-09-30 01:47:59,2016-09-30 21:38:31
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1812,https://api.github.com/repos/kubernetes/contrib/issues/1812,make some kubelet args configurable,"Fixes: #1788

As long as user specified args does not conflict with automatically specified.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1812)

<!-- Reviewable:end -->
",closed,True,2016-09-30 10:10:06,2016-11-16 14:07:44
contrib,kayrus,https://github.com/kubernetes/contrib/pull/1813,https://api.github.com/repos/kubernetes/contrib/issues/1813,Added domain name and server port into nginx logs,"I replaced dashes so it shouldn't brake log parsers.
In addition here are the [fluentd rules](https://github.com/kayrus/elk-kubernetes/blob/a3727fdf1411114da7e2fe921cf693afa97ea478/docker/fluentd/td-agent.conf#L262..L269) which parse these logs (requires https://github.com/tagomoris/fluent-plugin-parser plugin)

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1813)

<!-- Reviewable:end -->
",closed,True,2016-09-30 10:35:03,2016-10-25 19:55:24
contrib,cristi-,https://github.com/kubernetes/contrib/issues/1814,https://api.github.com/repos/kubernetes/contrib/issues/1814,[peer-finder] Allow dns domain to be configured ,"Hello,

 We are using peer finder for zookeeper auto discovery. However since we are using a custom dns domain with kube-dns, peer-finder is not working.

This is the parameter that's causing the issue:
https://github.com/kubernetes/contrib/blob/master/pets/peer-finder/peer-finder.go#L35

Thanks!
",closed,False,2016-09-30 13:30:16,2018-02-17 08:18:02
contrib,lerencao,https://github.com/kubernetes/contrib/pull/1815,https://api.github.com/repos/kubernetes/contrib/issues/1815,ansible: fix docker insecure registry issue,"This fixes #550 and also flannel in ubuntu < 15(as flannel will change docker opts)

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1815)

<!-- Reviewable:end -->
",closed,True,2016-09-30 14:47:31,2018-01-25 12:52:08
contrib,kayrus,https://github.com/kubernetes/contrib/issues/1816,https://api.github.com/repos/kubernetes/contrib/issues/1816,ansible: kube-proxy: Error looking for path of conntrack,"I've provisioned k8s using ansible scripts into Ubuntu Xenial VM and noticed the following logs:

```
Sep 30 14:45:11 ubuntu2 kube-proxy[1111]: E0930 14:45:11.595540    1111 proxier.go:604] conntrack return with error: Error looking for path of conntrack: exec: ""conntrack"": executable file not found in $PATH
``

should we check `conntrack` availability in provisioning rules?
```
",closed,False,2016-09-30 15:11:57,2016-11-18 11:03:51
contrib,kayrus,https://github.com/kubernetes/contrib/issues/1817,https://api.github.com/repos/kubernetes/contrib/issues/1817,ingress: nginx - could not build map_hash,"Same issue I faced months ago when I provided new nginx config in this PR https://github.com/kubernetes/contrib/pull/1310.

```
E0930 16:07:51.073733       1 command.go:87] failed to execute nginx -s reload: 2016/09/30 16:07:51 [emerg] 16#16: could not build map_hash, you should increase map_hash_bucket_size: 32
nginx: [emerg] could not build map_hash, you should increase map_hash_bucket_size: 32
W0930 16:07:51.073971       1 utils.go:91] requeuing prod-uu/icp-media, err error reloading nginx: exit status 1
```

Today I faced it again. But the only difference - I use docker 1.11.2 and HDD instead of 1.12 and SSD. Same Ubuntu Xenial VMs with the same amount of RAM.

I successfully reproduced this error message on pure docker container with this config:
https://gist.github.com/kayrus/dcc092fe2b80b268d01dc591cfcec89a

for some reason increasing `map_hash_bucket_size` didn't help me months ago. But today it helped (`map_hash_bucket_size 64;`). Most probably it's worth to make `map_hash_bucket_size` option configurable in ingress.
",closed,False,2016-09-30 16:12:41,2016-11-15 13:17:08
contrib,wstrange,https://github.com/kubernetes/contrib/issues/1818,https://api.github.com/repos/kubernetes/contrib/issues/1818,nginx   examples/default/rc-default.yaml has healthz errors ,"I am finding the latest rc-default config does not work anymore. The container is being restarted because it is failing the liveness / readiness probes. See errors below.

If I comment out those checks, it works fine.

This is minikube  /  k8s 1.4.0 
- [30/Sep/2016:20:56:16 +0000] ""GET /ingress-controller-healthz HTTP/1.1"" 404 21 ""-"" ""Go-http-client/1.1"" 139 0.001 172.17.0.4:8080 21 0.001 404
  172.17.0.1 - [172.17.0.1] - - [30/Sep/2016:20:56:26 +0000] ""GET /ingress-controller-healthz HTTP/1.1"" 404 21 ""-"" ""Go-http-client/1.1"" 139 0.000 172.17.0.4:8080 21 0.000 404
  172.17.0.1 - [172.17.0.1] - - [30/Sep/2016:20:56:26 +0000] ""GET /ingress-controller-healthz HTTP/1.1"" 404 21 ""-"" ""Go-http-client/1.1"" 139 0.000 172.17.0.4:8080 21 0.000 404
  I0930 20:56:27.108540       1 main.go:187] Received SIGTERM, shutting down
",closed,False,2016-09-30 21:08:00,2016-09-30 21:15:16
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1819,https://api.github.com/repos/kubernetes/contrib/issues/1819,Munge pr since last loop,"I've tentatively explain what this does in the commit messages, please read those instead.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1819)

<!-- Reviewable:end -->
",closed,True,2016-09-30 21:25:55,2017-01-21 05:38:50
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/1820,https://api.github.com/repos/kubernetes/contrib/issues/1820,Don't index ingress.status.ip if empty.,"This can happen in some odd situations where the user has asked for invalid ingress/secrets. It results in a nil pointer deref.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1820)

<!-- Reviewable:end -->
",closed,True,2016-10-01 01:47:45,2016-10-03 17:38:15
contrib,yolkov,https://github.com/kubernetes/contrib/issues/1821,https://api.github.com/repos/kubernetes/contrib/issues/1821,[kube-keepalived-vip] don't start error,"```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.0"", GitCommit:""a16c0a7f71a6f93c7e0f222d961f4675cd97a46b"", GitTreeState:""clean"", BuildDate:""2016-09-26T18:16:57Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.0"", GitCommit:""a16c0a7f71a6f93c7e0f222d961f4675cd97a46b"", GitTreeState:""clean"", BuildDate:""2016-09-26T18:10:32Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: 3 baremetal nodes
- **OS** (e.g. from /etc/os-release): debian jessie
- **Kernel** (e.g. `uname -a`): Linux avi-app68 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2+deb8u3 (2016-07-02) x86_64 GNU/Linux
- **Install tools**:  kubelet as systemd service, others from manifests pods
- **Others**: calico network

vip-daemonset.yaml

``` yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-keepalived-vip
spec:
  template:
    metadata:
      labels:
        name: kube-keepalived-vip
    spec:
      hostNetwork: true
      #nodeSelector:
      #  type: worker
      volumes:
        - name: modules
          hostPath:
            path: /lib/modules
        - name: dev
          hostPath:
            path: /dev
      containers:
        - image: gcr.io/google_containers/kube-keepalived-vip:0.8
          name: kube-keepalived-vip
          imagePullPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /lib/modules
              name: modules
              readOnly: true
            - mountPath: /dev
              name: dev
          # use downward API
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          # to use unicast
          args:
          - --services-configmap=kube-system/vip-configmap
```

vip-configmap.yaml

``` yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vip-configmap
data:
  10.9.1.70: demos/service-example
```

10.9.1.70 - free ip from host network

kubectl logs po/kube-keepalived-vip-8xjnx:

```
F1002 20:05:34.704159       1 controller.go:299] Error getting POD information: timed out waiting to observe own status as Running
goroutine 1 [running]:
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.stacks(0x33b6500, 0x0, 0x0, 0x0)
        /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:766 +0xb8
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).output(0x3396180, 0xc800000003, 0xc82050ef00, 0x3360868, 0xd, 0x12b, 0x0)
        /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:717 +0x259
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).printf(0x3396180, 0x3, 0x26159a0, 0x21, 0xc82042fc48, 0x1, 0x1)
        /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:655 +0x1d4
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.Fatalf(0x26159a0, 0x21, 0xc82042fc48, 0x1, 0x1)
        /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:1145 +0x5d
main.newIPVSController(0xc8203f0a80, 0x0, 0x0, 0x1ce0700, 0x7ffcd19a2c48, 0x19, 0xc8203eb628)
        /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/controller.go:299 +0x2a7
main.main()
        /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/main.go:115 +0x812
```
",closed,False,2016-10-02 20:20:45,2016-10-11 20:17:52
contrib,perezje,https://github.com/kubernetes/contrib/issues/1822,https://api.github.com/repos/kubernetes/contrib/issues/1822,No Package matching 'etcd' found available,"I have a small AWS cluster of 3 EC2 redhat 7.2 instances. I'm trying to deploy kubernetes using this ansible playbook. It connected and install k8s master and nodes fine but fails to install etcd with following error...
TASK [etcd : Install etcd via package manager] *********************************
fatal: [52.38.167.59]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""No Package matching 'etcd' found available, installed or updated"", ""rc"": 0, ""results"": []}

Any help/advice would be appreciated.
",closed,False,2016-10-03 00:50:37,2018-02-16 21:07:01
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1823,https://api.github.com/repos/kubernetes/contrib/issues/1823,Cluster autoscaler may block if max node group size is beyond quota,"On GCE/GKE (and possibly on AWS) if node group scaling goes beyond quota then CA blocks indefinitely as the node group size in Kubernetes doesn't match to the node group size on cloud provider side. 

We should check why node group scaling didn't succeed and roll-back the change.
",closed,False,2016-10-03 10:01:52,2017-04-19 11:13:55
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1824,https://api.github.com/repos/kubernetes/contrib/issues/1824,install conntrack (kube-proxy's runtime dependency),"SSIA

Fixes: #1816

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1824)

<!-- Reviewable:end -->
",closed,True,2016-10-03 11:38:09,2016-11-18 11:03:52
contrib,foxish,https://github.com/kubernetes/contrib/pull/1825,https://api.github.com/repos/kubernetes/contrib/issues/1825,Create test-pr,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1825)

<!-- Reviewable:end -->
",closed,True,2016-10-03 16:51:56,2016-10-03 16:52:06
contrib,micheleorsi,https://github.com/kubernetes/contrib/issues/1826,https://api.github.com/repos/kubernetes/contrib/issues/1826,[nginx-ingress-controller] metrics on reload,"Hello,
we are adding more and more services/ingresses/pods to our bare metal cluster. 
Since the reload operation is _heavy_ and affects the balancing of our requests we would like to keep track of this event.

I developed a simple endpoint with [expvar](https://golang.org/pkg/expvar/) library to expose such information. 

Are you interested in this feature? If you want I can submit a merge request and discuss about it. 

Thanks 
",closed,False,2016-10-03 17:03:45,2016-11-02 11:58:51
contrib,micheleorsi,https://github.com/kubernetes/contrib/issues/1827,https://api.github.com/repos/kubernetes/contrib/issues/1827,[nginx-ingress-controller] wrong balancing of requests for 20-30 seconds after configuration reload,"Hello,
we are in a situation where we have hundreds of pods and dozens of nginx ingress controllers for high-availability.

We noticed that when the readiness of a pod changes (and the configuration of _failureThreshold_ and _successThreshold_ is a low number) the sort applied in the [following lines](https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/controller.go#L806) creates a huge problem in balancing requests.
A single nginx serves **more traffic** to the **first servers (in lexicographic order)** for a slice of 20-30 seconds. After that everything becomes normal. 
The real problem arises when you have dozens of nginx (in our case 20) because everyone of them forwards more requests to the first (lexicographic order) IPs that causes crashes on the application on those IPs, since they cannot sustain such high traffic.
This event triggers a probe failing (readiness or liveness) that triggers another reload on all of the nginx, creating **a self-sustaining spiral** that brings down the whole cluster.

Internally we modified this nginx ingress controller to reshuffle IPs before configuration reload. And it's almost a week that works like a charm.
Are you interested in such improvements? Probably it's more related to nginx itself that doesn't internally reshuffle IPs, but for us it was way simpler to modify this project instead of the whole nginx one.
If you want I can submit a merge request and discuss about it.

Thanks!
",closed,False,2016-10-03 17:29:29,2018-02-16 11:58:02
contrib,foxish,https://github.com/kubernetes/contrib/pull/1828,https://api.github.com/repos/kubernetes/contrib/issues/1828,Adding /lgtm syntax for the kubernetes/kubernetes repo.,"cc @kubernetes/contributor-experience

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1828)

<!-- Reviewable:end -->
",closed,True,2016-10-03 18:00:20,2016-10-03 18:25:09
contrib,eparis,https://github.com/kubernetes/contrib/pull/1829,https://api.github.com/repos/kubernetes/contrib/issues/1829,submit queue: truncate comments at 64k,"Github gets mad at us when we do this:

github.go:1664] POST https://api.github.com/repos/kubernetes/kubernetes/issues/33381/comments: 422 Validation Failed [{Resource:IssueComment Field:body Code:custom Message:body is too long (maximum is 65536 characters)}]

and then soon:

github.go:1664] POST https://api.github.com/repos/kubernetes/kubernetes/issues/33381/comments: 403 You have triggered an abuse detection mechanism and have been temporarily blocked from content creation. Please retry your request again later. []

Which can cause abuse failures for other calls to the API.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1829)

<!-- Reviewable:end -->
",closed,True,2016-10-03 19:02:22,2016-10-03 19:35:11
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1830,https://api.github.com/repos/kubernetes/contrib/issues/1830,[nginx-ingress-controller] Add support for default backend in Ingress rule,"replaces #1759

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1830)

<!-- Reviewable:end -->
",closed,True,2016-10-03 19:32:05,2016-10-06 21:28:10
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/1831,https://api.github.com/repos/kubernetes/contrib/issues/1831,Add etcd3 suite to SubmitQueue jobs,"Ref https://github.com/kubernetes/kubernetes/issues/20504

@timothysc - FYI
",closed,True,2016-10-04 12:19:12,2016-10-05 06:49:15
contrib,foxish,https://github.com/kubernetes/contrib/issues/1832,https://api.github.com/repos/kubernetes/contrib/issues/1832,The CLA nag munger alerting the PR author and not the commit authors,"It is simple enough to check the individual commits and their status, (at the cost of a few extra API calls).
e.g. https://github.com/kubernetes/contrib/pull/1276#issuecomment-250811324

However, when we have a large number of distinct authors, perhaps it would still make sense to ping the PR author (on a cherrypick PR for example).
This issue is to discuss whether it makes sense to always ping everyone who doesn't have a CLA among all commits in a PR.
",closed,False,2016-10-04 14:40:42,2016-11-29 09:49:54
contrib,micheleorsi,https://github.com/kubernetes/contrib/pull/1833,https://api.github.com/repos/kubernetes/contrib/issues/1833,[nginx-ingress-controller] Added reload metrics,,closed,True,2016-10-04 14:44:12,2016-10-04 16:04:48
contrib,foxish,https://github.com/kubernetes/contrib/issues/1834,https://api.github.com/repos/kubernetes/contrib/issues/1834,CLA munger in each repository,"Currently, the model we operate under is one instance of the mungegithub bot per repository (test-infra, contrib, kubernetes).
However, with the CNCF CLA, message posting and applying CLA labels are tied to the bot. We would need to have one instance per repository running, which doesn't seem scalable (or efficient) as we have so many different repositories. 

The alternatives include:
- Separating the CLA functionality into a separate single instance of mungegithub which which can easily run on a list of repos.
- Adding the label functionality at the CNCF end.

I think the first option is reasonable. As I understand, the latter is not preferable since we want to have control of how/who we notify. 
cc @kubernetes/contributor-experience @apelisse @grodrigues3 
",closed,False,2016-10-04 15:05:27,2016-11-15 18:22:16
contrib,micheleorsi,https://github.com/kubernetes/contrib/pull/1835,https://api.github.com/repos/kubernetes/contrib/issues/1835,[nginx-ingress-controller] Added reload metrics,"relates to #1826

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1835)

<!-- Reviewable:end -->
",closed,True,2016-10-04 16:11:15,2016-10-31 15:17:51
contrib,zmerlynn,https://github.com/kubernetes/contrib/pull/1836,https://api.github.com/repos/kubernetes/contrib/issues/1836,AWS: aws and kops-aws-updown to non-blocking list,"So we start getting flakiness and testrun failures.
",closed,True,2016-10-04 23:46:49,2016-10-05 00:30:42
contrib,justinsb,https://github.com/kubernetes/contrib/issues/1837,https://api.github.com/repos/kubernetes/contrib/issues/1837,nginx controller crashed when master went down,"No real logs yet, but

```
docker logs a8bd529bd143
I1005 15:02:43.678320       1 main.go:94] Using build: https://github.com/bprashanth/contrib.git - git-92b2bac
```

From docker inspect...

```
        ""Args"": [
            ""--default-backend-service=default/nginx-default-backend"",
            ""--nginx-configmap=default/ingress-nginx""
        ],
        ""State"": {
            ""Status"": ""exited"",
            ""Running"": false,
            ""Paused"": false,
            ""Restarting"": false,
            ""OOMKilled"": false,
            ""Dead"": false,
            ""Pid"": 0,
            ""ExitCode"": 2,
            ""Error"": """",
            ""StartedAt"": ""2016-10-05T15:02:43.666671138Z"",
            ""FinishedAt"": ""2016-10-05T15:03:23.294107321Z""
        },
```
",closed,False,2016-10-05 15:09:13,2016-10-05 15:48:17
contrib,andrewrutter,https://github.com/kubernetes/contrib/issues/1838,https://api.github.com/repos/kubernetes/contrib/issues/1838,Blocking issues trying to install on Fedora,"I have been following through the process defined on http://kubernetes.io/docs/getting-started-guides/fedora/fedora_ansible_config/ which is failing for me in a couple of places.

First, any reference to firewalld is failing. For example

```
TASK [flannel : Save firewalld vxlan port for flanneld] ************************
fatal: [k8-master]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""firewalld and its python 2 module are required for this module""}
```

Second, the script is failing completely trying to generate certificates:

```
TASK [kubernetes : Run create cert script on master] ***************************
fatal: [k8-master]: FAILED! => {""changed"": true, ""cmd"": [""/usr/libexec/kubernetes/make-ca-cert.sh""], ""delta"": ""0:00:00.285986"", ""end"": ""2016-10-05 13:40:59.489791"", ""failed"": true, ""rc"": 2, ""start"": ""2016-10-05 13:40:59.203805"", ""stderr"": ""=== Failed to generate certificates: Aborting ==="", ""stdout"": ""/usr/libexec/kubernetes/make-ca-cert.sh: line 104: ./easyrsa: Permission denied\n/usr/libexec/kubernetes/make-ca-cert.sh: line 106: ./easyrsa: Permission denied\n/usr/libexec/kubernetes/make-ca-cert.sh: line 107: ./easyrsa: Permission denied\n/usr/libexec/kubernetes/make-ca-cert.sh: line 108: ./easyrsa: Permission denied\n/usr/libexec/kubernetes/make-ca-cert.sh: line 109: ./easyrsa: Permission denied"", ""stdout_lines"": [""/usr/libexec/kubernetes/make-ca-cert.sh: line 104: ./easyrsa: Permission denied"", ""/usr/libexec/kubernetes/make-ca-cert.sh: line 106: ./easyrsa: Permission denied"", ""/usr/libexec/kubernetes/make-ca-cert.sh: line 107: ./easyrsa: Permission denied"", ""/usr/libexec/kubernetes/make-ca-cert.sh: line 108: ./easyrsa: Permission denied"", ""/usr/libexec/kubernetes/make-ca-cert.sh: line 109: ./easyrsa: Permission denied""], ""warnings"": []}
```
",closed,False,2016-10-05 17:46:55,2018-02-17 00:10:01
contrib,miend,https://github.com/kubernetes/contrib/issues/1839,https://api.github.com/repos/kubernetes/contrib/issues/1839,"GLBC ingress controller troubleshooting information is out of date, and unclear after v1.3 changes","The troubleshooting section of [the readme for the GLBC ingress controller](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/gce#troubleshooting) states that one should look at the controller pod's logs for troubleshooting information. According to the information in #1733, as of v1.3, the L7 controller runs on master, not as a regular pod. Google, of course, does not allow access to master on GKE clusters.

First, this means that the GLBC ingress controller documentation should be updated to reflect its current status. And secondly, how am I recommended to get logs from the ingress controller to troubleshoot potential issues with it if I cannot access it? Or is there some other method now recommended for troubleshooting it? I'm currently receiving a 502 bad gateway error when trying to access services/pods via ingress which are, as far as I can tell, conforming exactly to the standards set in documentation. I thought getting these logs would help me rule out any issues on the controller's end, but I've found I can't get to it at all.
",closed,False,2016-10-05 18:37:00,2018-02-16 01:48:01
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/1840,https://api.github.com/repos/kubernetes/contrib/issues/1840,Docker Micro Benchmark: Add dockerd and containerd support.,"@timstclair Can you help me review this? I remember you made similar change locally before.

I used this to get the result https://github.com/kubernetes/kubernetes/issues/28698#issuecomment-244514829, but forgot to submit the PR at that time.
",closed,True,2016-10-05 21:36:12,2016-10-06 17:18:35
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/1841,https://api.github.com/repos/kubernetes/contrib/issues/1841,CRI: Add cri benchmark test in node perf dash.,"For kubernetes/kubernetes#31459.

This depends on https://github.com/kubernetes/test-infra/pull/761 and https://github.com/kubernetes/kubernetes/pull/34141.

After this gets merged, I'll restart node perf dash. Then we should be able to monitor benchmark result with cri enabled.

@yujuhong @feiskyer 
/cc @kubernetes/sig-node 
",closed,True,2016-10-05 21:48:09,2016-10-06 18:28:07
contrib,calebamiles,https://github.com/kubernetes/contrib/pull/1842,https://api.github.com/repos/kubernetes/contrib/issues/1842,Expand owners franchise,"Updates owners files using git commit history. Updates were produced using the same tooling as in https://github.com/kubernetes/kubernetes/pull/31752. PTAL, @grodrigues3.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1842)

<!-- Reviewable:end -->
",closed,True,2016-10-06 02:11:08,2018-03-30 00:15:56
contrib,ixdy,https://github.com/kubernetes/contrib/issues/1843,https://api.github.com/repos/kubernetes/contrib/issues/1843,godep restore failing for mungegithub,"``` console
.../k8s.io/contrib/mungegithub $ git describe --long --always --dirty 
cc75761
.../k8s.io/contrib/mungegithub $ godep restore
# cd /tmp/godep/src/google.golang.org/cloud; git checkout 12aa462581208c155e498dc13e14cabe6da24dc3
fatal: reference is not a tree: 12aa462581208c155e498dc13e14cabe6da24dc3
godep: error restoring dep (google.golang.org/cloud): exit status 128
# cd /tmp/godep/src/google.golang.org/cloud; git checkout 12aa462581208c155e498dc13e14cabe6da24dc3
fatal: reference is not a tree: 12aa462581208c155e498dc13e14cabe6da24dc3
godep: error restoring dep (google.golang.org/cloud/internal): exit status 128
# cd /tmp/godep/src/google.golang.org/cloud; git checkout 12aa462581208c155e498dc13e14cabe6da24dc3
fatal: reference is not a tree: 12aa462581208c155e498dc13e14cabe6da24dc3
godep: error restoring dep (google.golang.org/cloud/internal/transport): exit status 128
# cd /tmp/godep/src/google.golang.org/cloud; git checkout 12aa462581208c155e498dc13e14cabe6da24dc3
fatal: reference is not a tree: 12aa462581208c155e498dc13e14cabe6da24dc3
godep: error restoring dep (google.golang.org/cloud/pubsub): exit status 128
# cd /tmp/godep/src/k8s.io/contrib; git checkout 567964f08af526755e9494ebe308da2a8c1dfe8a
fatal: reference is not a tree: 567964f08af526755e9494ebe308da2a8c1dfe8a
godep: error restoring dep (k8s.io/contrib/test-utils/utils): exit status 128
godep: Error restoring some deps. Aborting check.
```

cc @apelisse @lavalamp @krzyzacy 
",closed,False,2016-10-06 19:26:54,2017-01-21 01:16:52
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1844,https://api.github.com/repos/kubernetes/contrib/issues/1844,mungegithub: Adjust flakiness nagger sensitivity,,closed,True,2016-10-06 19:49:52,2016-10-06 20:28:08
contrib,mtaufen,https://github.com/kubernetes/contrib/pull/1845,https://api.github.com/repos/kubernetes/contrib/issues/1845,Remove flaky test job from list of jobs to auto-file issues for,"Specifically, this removes the `kubernetes-e2e-gci-gke-flaky` job from the list.
",closed,True,2016-10-06 20:19:45,2016-10-06 21:39:25
contrib,krzyzacy,https://github.com/kubernetes/contrib/pull/1846,https://api.github.com/repos/kubernetes/contrib/issues/1846,Make TestOwner can parse globs from test names,"Also added a test for glob parsing
",closed,True,2016-10-06 20:47:57,2016-10-08 20:31:25
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1847,https://api.github.com/repos/kubernetes/contrib/issues/1847,[nginx-ingress-controller] Change structure of packages,"Goals of this PR:
- change the layout of the packages to be agnostic to the backend (first step)
- remove most of the references to nginx
- remove unnecessary function exports
- add test 
- refactor how the status of the Ingress rules are keep up to date (using leader election)
- move main inside cmd package

My wish is that to implement a new controller (like caddy) you just need to write a template for the tool, calling the generic code and a Dockerfile

replaces #1762

fixes #1665
fixes #1799

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1847)

<!-- Reviewable:end -->
",closed,True,2016-10-06 23:48:42,2016-10-25 11:28:46
contrib,kuroneko25,https://github.com/kubernetes/contrib/issues/1848,https://api.github.com/repos/kubernetes/contrib/issues/1848,Ingress can leverage healthCheckNodePort,"Hi! I'm looking for a way to make Ingress/GLBC work when a different port is desired for the health check. Here is the context:
We run containers that contain Java gRPC services on GKE. When I set up GLBC I noticed that health checks are failing because the load balancer is making HTTP1.1 requests to the pods and our gRPC servers cannot handle HTTP1.1 requests. So I came up with a work-around by adding a side-car container that runs a HTTP server and proxies the health check requests from the LB to the gRPC server container. My service looks like this:

```
Name:           test-service
Namespace:      test
Labels:         name=test-service
Selector:               app=test-service
Type:           NodePort
IP:                 10.47.242.69
Port:           grpc    9000/TCP
NodePort:       grpc    31986/TCP
Endpoints:      10.44.0.17:1337
Port:           health-check    9001/TCP
NodePort:       health-check    32231/TCP
Endpoints:      10.44.0.17:1338
Session Affinity:   None
```

This works but requires manual configuration after the Ingress is created. Specifically I have to:
- Create a new health check in GCE that points to the node port 32231
- Create a new firewall rule that allows the LB to talk to the node port 32231
- Update the backend to use this new health check

Is it possible to make GLBC use a different port for health checking? Is there a better way to achieve what I want to do here?
",closed,False,2016-10-07 04:28:19,2018-02-17 09:19:01
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/1849,https://api.github.com/repos/kubernetes/contrib/issues/1849,Block PR merge on kubemark presubmit and add kubemark-500 to blocking suites.,"Ref https://github.com/kubernetes/kubernetes/issues/29897

@gmarek - FYI
",closed,True,2016-10-07 11:16:45,2016-10-11 17:53:05
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1850,https://api.github.com/repos/kubernetes/contrib/issues/1850,Cluster-autoscaler: Add a flag to enable more aggressive new node count nt estimation via binpacking estimator,"Binpacking estimator was added here: #1741

cc: @piosz @fgrzadkowski

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1850)

<!-- Reviewable:end -->
",closed,True,2016-10-07 15:46:29,2016-11-04 16:39:29
contrib,justinsb,https://github.com/kubernetes/contrib/pull/1851,https://api.github.com/repos/kubernetes/contrib/issues/1851,WIP: Loadbalancers with nginx,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1851)

<!-- Reviewable:end -->
",closed,True,2016-10-07 18:00:26,2017-04-10 03:32:38
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1852,https://api.github.com/repos/kubernetes/contrib/issues/1852,Fix godeps,,closed,True,2016-10-07 21:38:31,2016-10-08 20:52:53
contrib,lavalamp,https://github.com/kubernetes/contrib/issues/1853,https://api.github.com/repos/kubernetes/contrib/issues/1853,"submit queue: allow ""squash merges""","Github has added an option to the merge button that humans can press that will allow changes to get automatically squashed and then merged. It would be great if we could enable that for the submit queue. I don't think we want to do it for all PRs, but it'd be great if the reviewer could say e.g. `/lgtm+squash` so that the author doesn't have to do a squash.
",closed,False,2016-10-07 23:54:17,2018-02-16 02:49:01
contrib,sandys,https://github.com/kubernetes/contrib/issues/1854,https://api.github.com/repos/kubernetes/contrib/issues/1854,SSL passthrough for Ingress container,"the current ingress container cannot do L4 routing. I dont want to terminate my ssl at the ingress container because my ssl config is complicated (with lua rules and all) and i would rather not port them over to annotations.

I see that there is a readme for [proxy protocol](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/proxy-protocol) as well plans to use [Caddy](https://github.com/kubernetes/kubernetes/issues/19899) as the ingress container.

 Is there a long term plan to add SSL pass through for the ingress container ? 
",closed,False,2016-10-08 12:50:29,2018-12-10 13:11:47
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1855,https://api.github.com/repos/kubernetes/contrib/issues/1855,mungegithub: Fix Godeps pointing to non-exisitng commit,,closed,True,2016-10-08 21:12:56,2016-10-11 00:06:26
contrib,crigor,https://github.com/kubernetes/contrib/pull/1856,https://api.github.com/repos/kubernetes/contrib/issues/1856,Add aws to allowed values on cluster autoscaler cloud-provider flag,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1856)

<!-- Reviewable:end -->
",closed,True,2016-10-09 04:56:25,2016-10-12 09:06:35
contrib,javiercr,https://github.com/kubernetes/contrib/issues/1857,https://api.github.com/repos/kubernetes/contrib/issues/1857,Custom IP with GLBC doesn't work,"I'm trying to create an ingress with an IP previously created from the Google Cloud console, following the [instructions](https://gist.github.com/bprashanth/545f00b3b5488109c727) by @bprashanth :

``` yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: another-no-rules-map
  annotations:
    kubernetes.io/ingress.global-static-ip-name: ""staticip""
spec:
  tls:
  - secretName: testsecret
  backend:
    serviceName: haproxy
    servicePort: 80
```

Where `staticip` it's the name of the IP I created from the console. The annotation works (it gets added to the Ingress resource) however kubernetes totally ignores it and allocates a new IP. I also tried with `ingress.kubernetes.io/global-static-ip-name` as the annotation name but it didn't work.

Am I missing something or is this behavior broken?

In case it helps, here is the version I'm running:

```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.0"", GitCommit:""a16c0a7f71a6f93c7e0f222d961f4675cd97a46b"", GitTreeState:""clean"", BuildDate:""2016-09-26T18:16:57Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.0"", GitCommit:""a16c0a7f71a6f93c7e0f222d961f4675cd97a46b"", GitTreeState:""clean"", BuildDate:""2016-09-26T18:10:32Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

This behavior was introduced in PR #563.
",closed,False,2016-10-09 14:38:50,2016-10-10 09:36:02
contrib,gmarek,https://github.com/kubernetes/contrib/issues/1858,https://api.github.com/repos/kubernetes/contrib/issues/1858,Submit Queue should print timezones by dates.,"Without it I have no idea what I'm looking at. cc @kubernetes/test-infra-maintainers @eparis 
",closed,False,2016-10-10 06:42:28,2018-02-16 02:49:02
contrib,cheyang,https://github.com/kubernetes/contrib/pull/1859,https://api.github.com/repos/kubernetes/contrib/issues/1859,make kubelet option configurable,"for #1812 , please review it. I've done the test locally.

Fixes: #1788

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1859)

<!-- Reviewable:end -->
",closed,True,2016-10-10 09:57:19,2016-11-18 11:07:14
contrib,krzyzacy,https://github.com/kubernetes/contrib/pull/1860,https://api.github.com/repos/kubernetes/contrib/issues/1860,Make TestOwner match globs,"Also let's see if it fixes godeps...

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1860)

<!-- Reviewable:end -->
",closed,True,2016-10-10 17:56:50,2016-10-12 22:06:40
contrib,treyhyde,https://github.com/kubernetes/contrib/pull/1861,https://api.github.com/repos/kubernetes/contrib/issues/1861,"AWS is in fact, a valid option.",,closed,True,2016-10-10 23:40:56,2016-10-10 23:54:15
contrib,treyhyde,https://github.com/kubernetes/contrib/pull/1862,https://api.github.com/repos/kubernetes/contrib/issues/1862,"AWS is, in fact a valid option","<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1862)

<!-- Reviewable:end -->
",closed,True,2016-10-10 23:57:09,2016-11-04 17:00:07
contrib,treyhyde,https://github.com/kubernetes/contrib/pull/1863,https://api.github.com/repos/kubernetes/contrib/issues/1863,Target linux binaries by default so you can follow the build instruct…,"…ions on a Mac and still build a valid image.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1863)

<!-- Reviewable:end -->
",closed,True,2016-10-11 00:20:49,2016-10-14 14:56:26
contrib,rrati,https://github.com/kubernetes/contrib/issues/1864,https://api.github.com/repos/kubernetes/contrib/issues/1864,AnsibleUndefinedVariable for networking when running deploy-cluster.sh,"Attempting to run the deploy-cluster.sh script like this:

cd scripts
INVENTORY=../inventory.kube ./deploy-cluster.sh 

Results in an error:
fatal: [192.10.0.4] => {'msg': ""AnsibleUndefinedVariable: One or more undefined variables: 'networking' is undefined"", 'failed': True}
",closed,False,2016-10-11 12:42:59,2016-11-16 14:17:49
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/1865,https://api.github.com/repos/kubernetes/contrib/issues/1865,state a custom inventory file must be at the same directory as group_vars,"SSIA

Fixes: #1864

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1865)

<!-- Reviewable:end -->
",closed,True,2016-10-11 13:39:01,2016-11-16 14:17:49
contrib,seeekr,https://github.com/kubernetes/contrib/pull/1866,https://api.github.com/repos/kubernetes/contrib/issues/1866,fix typo in ingress/controllers/README.md,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1866)

<!-- Reviewable:end -->
",closed,True,2016-10-11 16:25:27,2016-10-11 16:56:30
contrib,kyessenov,https://github.com/kubernetes/contrib/issues/1867,https://api.github.com/repos/kubernetes/contrib/issues/1867,[gce ingress] Unable to use custom health checks,"We have a service that provides an API management interface. As is, it does not have ""/"" endpoint defined since that is dictated by OpenAPI interface specification that our service abides to. I have tried to specify a custom ""liveness probe"" on the pods backing the service:

livenessProbe:
        httpGet:
          path: /shelves
          port: 80

However, this is not picked up by the GCE ingress controller, and I see lots of 400s on / in the access log from my pods. I have also tried using a custom port (8090) since we have a status endpoint defined for it, but that also does not work. 

livenessProbe:
        httpGet:
          path: /nginx_status
          port: 8090

I have exposed both ports 80 and 8090 in my service spec.

Can you help me figure out why GCE ingress is not working? I cannot view the logs from GCE ingress pod since I'm using GKE and the load balancer seems to be running on the master.
",closed,False,2016-10-11 17:17:30,2016-10-27 18:15:25
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1868,https://api.github.com/repos/kubernetes/contrib/issues/1868,"Revert ""Block PR merge on kubemark presubmit and add kubemark-500 to blocking suites.""","Reverts kubernetes/contrib#1849

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1868)

<!-- Reviewable:end -->
",closed,True,2016-10-11 17:28:12,2016-10-11 17:33:48
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1869,https://api.github.com/repos/kubernetes/contrib/issues/1869,"Revert ""Revert ""Block PR merge on kubemark presubmit and add kubemark-500 to blocking suites.""""","Reverts kubernetes/contrib#1868

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1869)

<!-- Reviewable:end -->
",closed,True,2016-10-11 20:02:09,2016-10-13 03:27:29
contrib,dshmelev,https://github.com/kubernetes/contrib/issues/1870,https://api.github.com/repos/kubernetes/contrib/issues/1870,TLS auth for nginx-controller,"How can I implement mutual tls auth on nginx-controller?
Example:

```
ssl_certificate /etc/nginx-ssl/ca.crt;
ssl_certificate_key /etc/nginx-ssl/api-api.pem;
ssl_client_certificate /etc/nginx-ssl/api-api.pem;
ssl_verify_client on;
```

I put ca.crt to ingress secret:

```
apiVersion: v1
kind: Secret
metadata:
  name: api-search
  namespace: api-search
data:
  tls.crt: base64 string
  tls.key: base64 string
  ca.crt: base64 string
```

but `/etc/nginx-ssl/api-api.pem` file not contain ca.crt.
Thank you.
",closed,False,2016-10-11 20:51:39,2017-12-18 07:33:23
contrib,bowei,https://github.com/kubernetes/contrib/pull/1871,https://api.github.com/repos/kubernetes/contrib/issues/1871,Dnsmasq metrics,"Posting the dnsmasq-metrics daemon. This still needs to be done:

This commit is towards resolving issue https://github.com/kubernetes/kubernetes/issues/29060
- e2e test with a dnsmasq server running in a container
- readme.md instructions

I'm posting this early to get some feedback.

Note: for the build, I used thockin's template.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1871)

<!-- Reviewable:end -->
",closed,True,2016-10-11 21:19:59,2016-11-04 20:32:57
contrib,alfirin,https://github.com/kubernetes/contrib/issues/1872,https://api.github.com/repos/kubernetes/contrib/issues/1872,"[nginx-ingress-lb] Error syncing pod, skipping: failed to ""StartContainer"" for ""nginx-ingress-lb""","I deployed a new v1.4.1 kubernetes cluster.

I follow the tutorial to create nginx-ingress-lb.

When I do the containers always restarting with some errors: 
`Readiness probe failed: HTTP probe failed with statuscode: 404`
or: `Liveness probe failed: HTTP probe failed with statuscode: 404`
or: `Back-off restarting failed docker container`
or: `Error syncing pod, skipping: failed to ""StartContainer"" for ""nginx-ingress-lb"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-8ahmm_kube-system(d252e1a9-8fcc-11e6-8e94-028069efceb3)""`

With a lot of restarts.

<img width=""1104"" alt=""screen shot 2016-10-12 at 00 11 27"" src=""https://cloud.githubusercontent.com/assets/2813035/19290624/e8a97524-9010-11e6-9c96-00ac7ce0aa13.png"">
",closed,False,2016-10-11 22:15:11,2016-10-13 07:23:55
contrib,alfirin,https://github.com/kubernetes/contrib/issues/1873,https://api.github.com/repos/kubernetes/contrib/issues/1873,[nginx-ingress-lb] Need to have one nginx-ingress-controller per host,"I created a new kubernetes v1.4.1 with two minion hosts.

Then I created the `default-http-backend` and one RC `nginx-ingress-controller` following [the contrib example](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx) as follow:

```
$ kubectl create -f examples/default-backend.yaml
$ kubectl expose rc default-http-backend --port=80 --target-port=8080 --name=default-http-backend
$ kubectl create -f examples/default/rc-default.yaml
```

Everything is ok for the first host:

```
curl http://<MINION_HOST_1>/healthz
ok%
```

However, for the second host I have an error:

```
curl http://<MINION_HOST_2>/healthz
curl: (7) Failed to connect to <MINION_HOST_2> port 80: Connection refused
```

In order to make this work for the two hosts, I have to scale the `nginx-ingress-controller` to 2 (one per host).

How could I make this work without doing this manual scale / without having one nginx-ingress-controller per host?

Because I have an auto-scaling group for my hosts and I will have more hosts added dynamically and I don't want to scale by hand the `nginx-ingress-controller`.

Thanks a lot for your help.
",closed,False,2016-10-12 09:24:05,2016-10-13 07:24:03
contrib,spxtr,https://github.com/kubernetes/contrib/pull/1874,https://api.github.com/repos/kubernetes/contrib/issues/1874,"Remove ""ok to test"" munger.","https://github.com/kubernetes/test-infra/issues/812

You don't need to say ""ok to test"" anymore, ""@k8s-bot test this"" works. The only case we don't hit anymore is if you apply the lgtm label without saying anything, which we can fix elsewhere. In the meantime, lets fix the world.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1874)

<!-- Reviewable:end -->
",closed,True,2016-10-12 23:08:23,2016-10-25 18:47:07
contrib,lewislyou,https://github.com/kubernetes/contrib/issues/1875,https://api.github.com/repos/kubernetes/contrib/issues/1875,logging/Dockerfile CMD /usr/local/sbin/config_generator.sh && /usr/sbin/td-agent -qq --use-v1-config --suppress-repeated-stacktrace > /var/log/td-agent/td-agent.log,,closed,False,2016-10-13 03:08:32,2016-10-13 03:53:00
contrib,bowei,https://github.com/kubernetes/contrib/pull/1876,https://api.github.com/repos/kubernetes/contrib/issues/1876,Add --verbose to boilerplate check to show what does not match,"--verbose will print out detailed information about why a given
file did not match the boilerplate match

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1876)

<!-- Reviewable:end -->
",closed,True,2016-10-13 06:08:54,2016-10-13 20:53:17
contrib,mkobetic,https://github.com/kubernetes/contrib/issues/1877,https://api.github.com/repos/kubernetes/contrib/issues/1877,Problems using zookeeper petset with latest stable version,"I'm using the petset yaml pretty much as is, just minimal tweaks to bump it down to the latest stable (2.4.9 atm), which should be something like this IIUC:

```
@@ -36,7 +43,7 @@
                 ""name"": ""install"",
                 ""image"": ""gcr.io/google_containers/zookeeper-install:0.1"",
                 ""imagePullPolicy"": ""Always"",
-                ""args"": [""--version=3.5.0-alpha"", ""--install-into=/opt"", ""--work-dir=/work-dir""],
+                ""args"": [""--version=3.4.9"", ""--install-into=/opt"", ""--work-dir=/work-dir""],
                 ""volumeMounts"": [
                     {
                         ""name"": ""opt"",
@@ -52,7 +59,7 @@
                 ""name"": ""bootstrap"",
                 ""image"": ""java:openjdk-8-jre"",
                 ""command"": [""/work-dir/peer-finder""],
-                ""args"": [""-on-start=\""/work-dir/on-start.sh\"""", ""-service=zk""],
+                ""args"": [""-on-start=\""/work-dir/on-change.sh\"""", ""-service=zk""],
                 ""env"": [
                   {
                       ""name"": ""POD_NAMESPACE"",
```

Just running this modified yaml through `kubectl create`, starts the first pet, then moves on to the second and stops. Looking at the logs it seems that the first pet just decides to run in standalone mode and then the second fails trying to connect to the election port.

Here's the start of the zoo-0 log (note the WARN line about the standalone mode):

```
ZooKeeper JMX enabled by default
Using config: /opt/zookeeper/bin/../conf/zoo.cfg
2016-10-13 17:26:24,020 [myid:] - INFO  [main:QuorumPeerConfig@124] - Reading configuration from: /opt/zookeeper/bin/../conf/zoo.cfg
2016-10-13 17:26:24,034 [myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: zoo-0.zk.default.svc.cluster.local to address: zoo-0.zk.default.svc.cluster.local/10.0.2.5
2016-10-13 17:26:24,034 [myid:] - ERROR [main:QuorumPeerConfig@301] - Invalid configuration, only one server specified (ignoring)
2016-10-13 17:26:24,036 [myid:] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
2016-10-13 17:26:24,036 [myid:] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
2016-10-13 17:26:24,036 [myid:] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.
2016-10-13 17:26:24,036 [myid:] - WARN  [main:QuorumPeerMain@113] - Either no config or no quorum defined in config, running  in standalone mode
2016-10-13 17:26:24,047 [myid:] - INFO  [main:QuorumPeerConfig@124] - Reading configuration from: /opt/zookeeper/bin/../conf/zoo.cfg
2016-10-13 17:26:24,048 [myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: zoo-0.zk.default.svc.cluster.local to address: zoo-0.zk.default.svc.cluster.local/10.0.2.5
2016-10-13 17:26:24,048 [myid:] - ERROR [main:QuorumPeerConfig@301] - Invalid configuration, only one server specified (ignoring)
2016-10-13 17:26:24,048 [myid:] - INFO  [main:ZooKeeperServerMain@96] - Starting server
2016-10-13 17:26:24,054 [myid:] - INFO  [main:Environment@100] - Server environment:zookeeper.version=3.4.9-1757313, built on 08/23/2016 06:50 GMT
2016-10-13 17:26:24,054 [myid:] - INFO  [main:Environment@100] - Server environment:host.name=zoo-0.zk.default.svc.cluster.local
2016-10-13 17:26:24,054 [myid:] - INFO  [main:Environment@100] - Server environment:java.version=1.8.0_102
2016-10-13 17:26:24,054 [myid:] - INFO  [main:Environment@100] - Server environment:java.vendor=Oracle Corporation
2016-10-13 17:26:24,054 [myid:] - INFO  [main:Environment@100] - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2016-10-13 17:26:24,056 [myid:] - INFO  [main:Environment@100] - Server environment:java.class.path=/opt/zookeeper/bin/../build/classes:/opt/zookeeper/bin/../build/lib/*.jar:/opt/zookeeper/bin/../lib/slf4j-log4j12-1.6.1.jar:/opt/zookeeper/bin/../lib/slf4j-api-1.6.1.jar:/opt/zookeeper/bin/../lib/netty-3.10.5.Final.jar:/opt/zookeeper/bin/../lib/log4j-1.2.16.jar:/opt/zookeeper/bin/../lib/jline-0.9.94.jar:/opt/zookeeper/bin/../zookeeper-3.4.9.jar:/opt/zookeeper/bin/../src/java/lib/*.jar:/opt/zookeeper/bin/../conf:
2016-10-13 17:26:24,056 [myid:] - INFO  [main:Environment@100] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2016-10-13 17:26:24,056 [myid:] - INFO  [main:Environment@100] - Server environment:java.io.tmpdir=/tmp
2016-10-13 17:26:24,056 [myid:] - INFO  [main:Environment@100] - Server environment:java.compiler=<NA>
2016-10-13 17:26:24,056 [myid:] - INFO  [main:Environment@100] - Server environment:os.name=Linux
2016-10-13 17:26:24,056 [myid:] - INFO  [main:Environment@100] - Server environment:os.arch=amd64
2016-10-13 17:26:24,056 [myid:] - INFO  [main:Environment@100] - Server environment:os.version=4.4.14+
2016-10-13 17:26:24,056 [myid:] - INFO  [main:Environment@100] - Server environment:user.name=root
2016-10-13 17:26:24,056 [myid:] - INFO  [main:Environment@100] - Server environment:user.home=/root
2016-10-13 17:26:24,056 [myid:] - INFO  [main:Environment@100] - Server environment:user.dir=/
2016-10-13 17:26:24,063 [myid:] - INFO  [main:ZooKeeperServer@815] - tickTime set to 2000
2016-10-13 17:26:24,063 [myid:] - INFO  [main:ZooKeeperServer@824] - minSessionTimeout set to -1
2016-10-13 17:26:24,063 [myid:] - INFO  [main:ZooKeeperServer@833] - maxSessionTimeout set to -1
2016-10-13 17:26:24,072 [myid:] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2181
```

The second pet set starts ok but gets ECONREFUSED when trying to reach the election port of the first pet

```
ZooKeeper JMX enabled by default
Using config: /opt/zookeeper/bin/../conf/zoo.cfg
2016-10-13 17:27:20,983 [myid:] - INFO  [main:QuorumPeerConfig@124] - Reading configuration from: /opt/zookeeper/bin/../conf/zoo.cfg
2016-10-13 17:27:21,001 [myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: zoo-0.zk.default.svc.cluster.local to address: zoo-0.zk.default.svc.cluster.local/10.0.2.5
2016-10-13 17:27:21,001 [myid:] - INFO  [main:QuorumPeer$QuorumServer@149] - Resolved hostname: zoo-1.zk.default.svc.cluster.local to address: zoo-1.zk.default.svc.cluster.local/10.0.0.6
2016-10-13 17:27:21,002 [myid:] - WARN  [main:QuorumPeerConfig@305] - No server failure will be tolerated. You need at least 3 servers.
2016-10-13 17:27:21,002 [myid:] - INFO  [main:QuorumPeerConfig@352] - Defaulting to majority quorums
2016-10-13 17:27:21,005 [myid:2] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
2016-10-13 17:27:21,006 [myid:2] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
2016-10-13 17:27:21,006 [myid:2] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.
2016-10-13 17:27:21,016 [myid:2] - INFO  [main:QuorumPeerMain@127] - Starting quorum peer
2016-10-13 17:27:21,025 [myid:2] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2181
2016-10-13 17:27:21,031 [myid:2] - INFO  [main:QuorumPeer@1019] - tickTime set to 2000
2016-10-13 17:27:21,032 [myid:2] - INFO  [main:QuorumPeer@1039] - minSessionTimeout set to -1
2016-10-13 17:27:21,032 [myid:2] - INFO  [main:QuorumPeer@1050] - maxSessionTimeout set to -1
2016-10-13 17:27:21,032 [myid:2] - INFO  [main:QuorumPeer@1065] - initLimit set to 10
2016-10-13 17:27:21,047 [myid:2] - INFO  [ListenerThread:QuorumCnxManager$Listener@534] - My election bind port: zoo-1.zk.default.svc.cluster.local/10.0.0.6:3888
2016-10-13 17:27:21,054 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:QuorumPeer@774] - LOOKING
2016-10-13 17:27:21,056 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@818] - New election. My id =  2, proposed zxid=0x0
2016-10-13 17:27:21,061 [myid:2] - WARN  [WorkerSender[myid=2]:QuorumCnxManager@400] - Cannot open channel to 1 at election address zoo-0.zk.default.svc.cluster.local/10.0.2.5:3888
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:589)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:381)
    at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:354)
    at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:452)
    at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:433)
    at java.lang.Thread.run(Thread.java:745)
```

@bprashanth, did this work before? Hopefully there's a way to force the first pet out of standalone mode despite being lonely?

/cc @ibawt
",closed,False,2016-10-13 19:34:17,2016-10-24 14:34:08
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1878,https://api.github.com/repos/kubernetes/contrib/issues/1878,Cluster-autoscaler: clean up debugs,"Clean up debug messages and add missing but useful ones.
",closed,False,2016-10-13 20:38:56,2017-04-19 11:58:00
contrib,r2d4,https://github.com/kubernetes/contrib/pull/1879,https://api.github.com/repos/kubernetes/contrib/issues/1879,Make map_hash_bucket_size configurable,"I was getting an error while trying to run the nginx controller in minikube.  This allows this nginx configuration option to be passed in through a configmap.  

The default value depends on the processor's cache line size (32 | 64 | 128), however ServerNameHashBucketSize is determined similarly, so I've set it to the same default (64).

Fixes #1817

ref https://github.com/kubernetes/minikube/issues/611

cc @bprashanth

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1879)

<!-- Reviewable:end -->
",closed,True,2016-10-14 18:28:28,2016-11-15 22:28:14
contrib,roboll,https://github.com/kubernetes/contrib/pull/1880,https://api.github.com/repos/kubernetes/contrib/issues/1880,nginx-ingress: configurable ingress class,"Add --ingress-class flag for configurable ingress class. Default
behavior is preserved, but adds flexibility to support, for example,
internal and external ELBs attached to nginx ingress controllers.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1880)

<!-- Reviewable:end -->
",closed,True,2016-10-14 20:12:58,2017-02-02 18:37:28
contrib,acca,https://github.com/kubernetes/contrib/issues/1881,https://api.github.com/repos/kubernetes/contrib/issues/1881,[vagrant] tty required to run sudo on Vagrant OpenStack provider for certain images,"Deploying with vagrant on OpenStack using centos7 base image I encountered the below error on the ""Rsyncing folder"" step due to tty missing from Vagrant env but required in order to run sudo on certain OS images (those with `Defaults requiretty` in `/etc/sudoers`).

This is related to https://goo.gl/fC5sPW and a fix is proposed within this PR ready for submit: https://github.com/kubernetes/contrib/compare/master...acca:fix.vagrant

```
==> kube-master-1: Rsyncing folder: PATH/contrib/ansible/vagrant/ => /vagrant
An unknown error happened in Vagrant OpenStack provider

To easily debug what happened, we recommend to set the environment
variable VAGRANT_OPENSTACK_LOG to debug

    $ export VAGRANT_OPENSTACK_LOG=debug

If doing this does not help fixing your issue, there may be a bug
in the provider. Please submit an issue on Github at
https://github.com/ggiamarchi/vagrant-openstack-provider
with the stracktrace and the logs.

We are looking for feedback, so feel free to ask questions or
describe features you would like to see in this provider.
Catched Error: Catched Error: The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

mkdir -p '/vagrant'

Stdout from the command:

Stderr from the command:

sudo: sorry, you must have a tty to run sudo
```
",closed,False,2016-10-16 21:14:29,2016-11-16 09:13:48
contrib,aledbf,https://github.com/kubernetes/contrib/issues/1882,https://api.github.com/repos/kubernetes/contrib/issues/1882,Proposal: remove nginx ingress controller from contrib,"Hi everyone,

I would like to propose moving the nginx ingress controller located in https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx.

I'm not sure if this qualifies for the incubation process or just split it out straight into a repo.
This Ingress controller has been maintained for more than 10 months, with active support in the contrib issues and slack.
(I don't have metrics to indicate the numbers of users)

This will help to:
- remove code from contrib
- give more visibility in the organization (so people can actually find it)
- pick up a few more contributors (hopefully)

_Releases:_

```
gcr.io/google_containers/nginx-ingress-controller    0.8.3    3f8119a98476    7 weeks ago
gcr.io/google_containers/nginx-ingress-controller    0.8.2    485ae35fc9f4    11 weeks ago
gcr.io/google_containers/nginx-ingress-controller    0.8.1    d272ea9a1e92    3 months ago
gcr.io/google_containers/nginx-ingress-controller    0.8      0496d9e01ef6    3 months ago
gcr.io/google_containers/nginx-ingress-controller    0.7      98e6b4e68a67    4 months ago
gcr.io/google_containers/nginx-ingress-controller    0.62     9972dee1b956    4 months ago
gcr.io/google_containers/nginx-ingress-controller    0.61     edeb93850b71    5 months ago
gcr.io/google_containers/nginx-ingress-controller    0.6      8cbcaaf34045    5 months ago
gcr.io/google_containers/nginx-ingress-controller    0.5      1ae65a65a987    6 months ago
gcr.io/google_containers/nginx-ingress-controller    0.4      7ca49d81084b    6 months ago
gcr.io/google_containers/nginx-third-party           0.4      6ece3bb745ea    6 months ago
gcr.io/google_containers/nginx-third-party           0.3      b33a4ba965e7    7 months ago
aledbf/nginx-third-party                             0.2      97561252656d    8 months ago
aledbf/nginx-third-party                             0.1      3fb253ced003    8 months ago
aledbf/nginx-third-party                             0.0      1fcb6571e70d    10 months ago
```

Please let me know your thoughts.

Thanks,
Alejandro
",closed,False,2016-10-17 19:12:57,2019-01-30 23:41:51
contrib,Q-Lee,https://github.com/kubernetes/contrib/pull/1883,https://api.github.com/repos/kubernetes/contrib/issues/1883,Use host name for instance.,"It turns out alarming is inaccurate without this (due to IP conflicts).

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1883)

<!-- Reviewable:end -->
",closed,True,2016-10-17 22:36:12,2016-10-18 06:54:33
contrib,MaikuMori,https://github.com/kubernetes/contrib/issues/1884,https://api.github.com/repos/kubernetes/contrib/issues/1884,[NGINIX Ingress Controller] Support for 301 redirects.,"Would be nice to be able to setup 301 redirects. As services come and go, certain things in the cluster get deprecated and require a redirect to a new location. 

Two current solutions that I'm aware of now:
- Use custom template, but this is overkill and makes upgrading controller harder/error prone;
- Put another proxy in front of the ingress controller, also seems overkill, but maybe not a bad idea in longterm
- Spin up container which only does redirection for configurable domains. (This is what I'll currently use until there is a better solution)

The general issue is to be able to add custom configuration sections in global and per ingress controller scope. Otherwise I feel the project will end up slowly implementing each NGINX config option as flag.

I am willing to contributing the redirect code.
",closed,False,2016-10-18 00:49:09,2018-06-10 08:01:06
contrib,alpe,https://github.com/kubernetes/contrib/issues/1885,https://api.github.com/repos/kubernetes/contrib/issues/1885,Incompatible health check path in `ingress/controllers/nginx` image/ examples,"With https://github.com/kubernetes/contrib/pull/1749 a new healthcheck path was introduced and the path in all the example yamls updated. Unfortunately the examples still point to the old image`gcr.io/google_containers/nginx-ingress-controller:0.8.3` which does not include the updated `nginx.tmpl`. 
This results in a restart loop as the new `/ingress-controller-healthz` path is served by the 404 default backend now.
",closed,False,2016-10-18 05:04:06,2016-10-31 21:42:16
contrib,andrewsykim,https://github.com/kubernetes/contrib/issues/1886,https://api.github.com/repos/kubernetes/contrib/issues/1886,Cluster Autoscaler should delete nodes that fail to register with the api server,"In the rare occasion that workers fail to register with the api server, the cluster autoscaler gets ""stuck"":
`W1018 14:50:46.022562       1 cluster_autoscaler.go:191] Cluster is not ready for autoscaling: wrong number of nodes for node group: k8s-worker expected: 3 actual: 2`
until there's manual intervention. 

It would be awesome if the cluster autoscaler could detect new nodes in the cloud provider that haven't been registered for a long period of time (10 mins?) and terminate them to let the cloud provider create a new instance. 

Alternatively you can add your own health checks to your nodes but the implementation would be cleaner if the cluster autoscaler was responsible for that logic. If this is a common issue and a feature that the cluster autoscaler should have, I can get a PR open for it. 
",closed,False,2016-10-18 15:00:54,2017-04-19 11:21:06
contrib,Q-Lee,https://github.com/kubernetes/contrib/pull/1887,https://api.github.com/repos/kubernetes/contrib/issues/1887,Bumping kubelet-to-gcm to 1.2.1,"I'd like to bump the minor version so I can distinguish the latest version.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1887)

<!-- Reviewable:end -->
",closed,True,2016-10-18 20:18:46,2016-10-19 09:04:39
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1888,https://api.github.com/repos/kubernetes/contrib/issues/1888,allow both approvers and assignees in owners files,"In preparation for the upcoming owners change, let's allow both assignees (old way) and approvers (new way) to lgtm code.  This does not affect the lgtm functionality.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1888)

<!-- Reviewable:end -->
",closed,True,2016-10-18 21:46:42,2016-10-20 21:06:00
contrib,apelisse,https://github.com/kubernetes/contrib/pull/1889,https://api.github.com/repos/kubernetes/contrib/issues/1889,mungegithub: Make approval label mimick lgtm label,"Our goal is to move from **LGTM** to **approved** as the gating label for the submit-queue.
In order to transition smoothly, we will first begin by having both LGTM and Approved
labels that behave identically.  Initially, only LGTM will be needed for merge, though the approved label will be present, and eventually only the approved label will be needed.  

Notes: this implementation always adds and removes the two labels synchronously

The plan is to start warning about LGTM use later, indicating that its use will be phased out in favor of approved.  Then, it will be downgraded to be the prerequisite for approved.  

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1889)

<!-- Reviewable:end -->
",closed,True,2016-10-18 22:39:06,2016-11-12 00:11:40
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1890,https://api.github.com/repos/kubernetes/contrib/issues/1890,allow either lgtm or approved for merge,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1890)

<!-- Reviewable:end -->
",closed,True,2016-10-18 22:55:37,2016-10-19 20:34:43
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1891,https://api.github.com/repos/kubernetes/contrib/issues/1891,Modify Approvers Mechanism (Notify for Cancels),"When canceling an approve, we should have the bot add a comment /approve cancel.
We also need to remove the approvedLabel when an approver cancels the approve request
",closed,False,2016-10-19 00:43:07,2017-01-17 23:31:15
contrib,saad-ali,https://github.com/kubernetes/contrib/issues/1892,https://api.github.com/repos/kubernetes/contrib/issues/1892,Incorrect PR Size label,"In PR https://github.com/kubernetes/kubernetes/pull/33944 the bot correctly applied the `size/XXL` label to the PR that touches 72 files and adds 6936 lines.

It later removed that label and replaced it with `size/XS` <-- Buggy behavior
",closed,False,2016-10-19 03:26:18,2016-10-20 20:24:54
contrib,foxish,https://github.com/kubernetes/contrib/issues/1893,https://api.github.com/repos/kubernetes/contrib/issues/1893,Errors when creating some issues from mungegithub,"This from the kubectl logs. 

```
E1019 18:03:56.373107       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:56.417936       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:56.476660       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:56.477031       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:56.534581       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:56.534957       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:56.591905       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:56.592241       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:56.659927       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:56.660282       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:56.734570       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:56.734943       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:56.790573       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:56.790931       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:56.852857       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:56.853210       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:56.914895       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:56.915232       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:56.982470       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:56.982807       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.048214       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.048581       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.113114       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.113485       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.184051       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.184412       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.244986       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.245350       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.303771       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.304088       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.362153       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.362486       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.423115       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.423484       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.481527       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.481872       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.538040       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.538368       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.598095       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.598433       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.662140       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.662488       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.722342       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.722681       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.788377       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.788709       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.845227       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.845582       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.902249       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.902593       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:57.960458       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:57.960850       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.018606       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.019034       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.075526       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.075892       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.138200       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.138604       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.194242       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.194609       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.258727       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.259129       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.324371       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.324770       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.379470       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.379937       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.435667       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.436025       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.497263       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.497616       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.551912       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.552247       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.613618       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.613982       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.673884       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.674244       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.735793       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.736130       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.852386       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.852761       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.906454       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.906834       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:58.966258       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:58.966622       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.027831       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.028168       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.084744       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.085132       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.148537       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.148910       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.208157       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.208516       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.268042       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.268388       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.327683       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.328071       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.387486       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.387903       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.466114       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.466686       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.547494       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.547856       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.609409       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.609896       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.668676       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.669092       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.732677       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.733201       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.796628       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.796971       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.850965       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.851447       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.921894       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.922275       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:03:59.990815       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:03:59.991191       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:04:00.050198       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:04:00.050581       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:04:00.120749       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:04:00.121140       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:04:00.184245       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:04:00.184723       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:04:00.247557       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:04:00.247974       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:04:00.319718       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:04:00.320181       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:04:00.378510       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:04:00.378981       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:04:00.459327       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:04:00.459771       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:04:00.532921       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:04:00.533282       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:04:00.617286       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
I1019 18:04:00.617764       1 github.go:535] Creating an issue: ""[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers {Kubernetes e2e suite}""
E1019 18:04:00.681010       1 github.go:550] createIssue: POST https://api.github.com/repos/kubernetes/kubernetes/issues: 422 Validation Failed [{Resource:Issue Field:assignee Code:invalid Message:}]
```
",closed,False,2016-10-19 18:12:00,2017-01-21 01:16:20
contrib,foxish,https://github.com/kubernetes/contrib/issues/1894,https://api.github.com/repos/kubernetes/contrib/issues/1894,Mungegithub loop takes more than 10 minutes,"https://github.com/kubernetes/kubernetes/pull/34980#issuecomment-254890383

There is a delay of around 19 minutes between the `/lgtm` comment and the application of the label. 
Clearly looping over the issues serially is taking much too long. Maybe we should take a look at the way the mungers are structured, find ones that can execute in parallel and allow for parallel execution to cut down on the time it takes per loop.

/cc @kubernetes/contributor-experience @apelisse @grodrigues3 @lavalamp 
",closed,False,2016-10-19 18:18:06,2016-11-29 09:48:45
contrib,caesarxuchao,https://github.com/kubernetes/contrib/pull/1895,https://api.github.com/repos/kubernetes/contrib/issues/1895,Don't make subdirs when publishing client-go,"Part of the client-go restructuring effort. This PR make publisher not to create subdirectories in client-go.

The latest commit in https://github.com/caesarxuchao/client-go/commits/master is a sample commit.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1895)

<!-- Reviewable:end -->
",closed,True,2016-10-19 18:18:42,2016-10-20 21:34:56
contrib,dmitriyvolk,https://github.com/kubernetes/contrib/issues/1896,https://api.github.com/repos/kubernetes/contrib/issues/1896,Is there a way to autoscale groups down to empty?,"Usecase: keeping an `x1.32xlarge` instance idle quickly becomes very expensive, so it would be nice to have a way to start one when it's needed, and kill it afterwards.  
",closed,False,2016-10-19 19:10:18,2018-02-16 06:53:02
contrib,balajismaniam,https://github.com/kubernetes/contrib/pull/1897,https://api.github.com/repos/kubernetes/contrib/issues/1897,Fix to respect CLA ping timeperiod.,"- Updated PingNotication call in CLA munger with the correct startDate.

cc @foxish See https://github.com/kubernetes-incubator/node-feature-discovery/pull/20 for details. 
",closed,True,2016-10-19 20:59:36,2016-11-21 17:08:30
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1898,https://api.github.com/repos/kubernetes/contrib/issues/1898,Cluster autoscaler: make sure that all pods present on the deleted node had a chance to be scheduled elsewhere,"Otherwise next node deletion may be premature. We are not very likely to hit this issue right now although it is theoretically possible.

cc: @fgrzadkowski @piosz 
",closed,False,2016-10-19 22:58:14,2018-02-21 12:57:00
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1899,https://api.github.com/repos/kubernetes/contrib/issues/1899,Cluster-autoscaler: Add events for scale down,"Today there are only logs.

cc: @fgrzadkowski @piosz 
",closed,False,2016-10-19 22:59:10,2016-10-24 11:51:51
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1900,https://api.github.com/repos/kubernetes/contrib/issues/1900,Cluster-Autoscaler: Integrate with Disruption Budget,"When we move DB out of alpha.

cc: @fgrzadkowski @piosz @davidopp 
",closed,False,2016-10-19 23:00:14,2017-04-19 10:41:59
contrib,foxish,https://github.com/kubernetes/contrib/issues/1901,https://api.github.com/repos/kubernetes/contrib/issues/1901,Bot-name should be parameterizable,"Context: https://github.com/kubernetes/contrib/pull/1897#issuecomment-255005200
Intel is running their own instance of the mergebot under a different name and the CLA-ping is not recognized by the matchers because we assume that they must be made by `k8s-merge-bot`. We should ideally be able to pass in the name of the bot as a commandline argument.

cc @kubernetes/contributor-experience @lavalamp @grodrigues3 
",closed,False,2016-10-20 04:21:01,2018-01-22 18:48:44
contrib,chamilad,https://github.com/kubernetes/contrib/issues/1902,https://api.github.com/repos/kubernetes/contrib/issues/1902,"""Exec format error"" when running customized service_loadbalancer ","I tried doing some minor changes to the `service_loadbalancer.go` file and building it in a custom Docker image to be run. However when done so, the RC goes in to CrashLoopBack and `kubectl logs <pod>` shows the only the following error.

```
Exec format error
```

I used both Go 1.7 and 1.6 versions to build the customizations and also tried specifying the target architecture as `GOARCH=386` and `GOARCH=amd64` in the Makefile, without success. 
",closed,False,2016-10-20 11:19:12,2016-12-09 01:29:39
contrib,eparis,https://github.com/kubernetes/contrib/pull/1903,https://api.github.com/repos/kubernetes/contrib/issues/1903,Fix size check,"fixes #1892 

The github api lies about the size of diffs if they are large. If we
then subtract out the size of generated files that it is not lieing
about we end up with negative numbers. So huge PRs can get XS size.

Instead of believing the API and deleting those things we don't care
about just sum the things we do care about and use that.
",closed,True,2016-10-20 19:05:36,2016-10-20 20:24:54
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1904,https://api.github.com/repos/kubernetes/contrib/issues/1904,"Allow ""Approvers"" group in md files","And take the union of approvers and assignees for transitory cases, when/if not all files have been changed.
",closed,True,2016-10-20 21:19:15,2016-10-20 23:24:28
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1905,https://api.github.com/repos/kubernetes/contrib/issues/1905,Cluster-autoscaler: publish event on node deletion,"Fixes: #1899

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1905)

<!-- Reviewable:end -->
",closed,True,2016-10-21 14:23:43,2016-10-24 11:51:51
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1906,https://api.github.com/repos/kubernetes/contrib/issues/1906,[nginx-ingress-controller]: Sync yaml probes with published image,,closed,True,2016-10-21 14:59:48,2016-10-21 19:14:56
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1907,https://api.github.com/repos/kubernetes/contrib/issues/1907,Cluster-autoscaler: respect pod termination notice,"Currently pods are not terminated gracefully. 

cc: @fgrzadkowski 
",closed,False,2016-10-21 15:07:37,2017-04-19 10:42:24
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/1908,https://api.github.com/repos/kubernetes/contrib/issues/1908,Prioritize contributor inconveniences,"@tmrts @pires @fabianofranz @pipejakob @idvoretskyi @bdbauer @feiskyer @chrislovecnm @wonderfly @hongchaodeng

You have requested to be added to kubernetes-maintainers in the past 9 months. 

Why?

What exactly were you trying to accomplish such that you thought write access to the kubernetes repo would make it possible or help?
- Set lgtm label?
- Set release-note labels?
- Set milestone on PR or issue?
- Self-assign PR or issue?
- Assign PR to someone else?
- Assign issue to someone else?
- Set some other labels on issues (which ones)?
- Set some other labels on PRs (which ones)?
- Edit wiki
- Something else (what)?

Once I have a reasonable list of items I'll send a questionnaire to contributors more broadly to help prioritize work in contributor experience.

cc @kubernetes/contributor-experience @grodrigues3 
",closed,False,2016-10-21 16:19:07,2017-12-27 20:17:31
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1909,https://api.github.com/repos/kubernetes/contrib/issues/1909,Cluster-autoscaler: own drain,"This PR adds an own drain logic to ClusterAutoscaler. Previously we were using the code from kubectl drain command but:
- due to refactorings it was hard to update the the CA dependencies. 
- we had to add some extra logic on top of it in fast drain evaluation
- a recent feature request to check the number of replicas in rs/rc was impossible to fulfill. 

cc: @fgrzadkowski @davidopp @piosz

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1909)

<!-- Reviewable:end -->
",closed,True,2016-10-21 18:29:24,2016-10-27 15:31:11
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1910,https://api.github.com/repos/kubernetes/contrib/issues/1910,Cluster-autoscaler: update scale down preconditions in readme.md,"cc: @fgrzadkowski @davidopp

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1910)

<!-- Reviewable:end -->
",closed,True,2016-10-21 19:25:06,2016-10-24 12:21:53
contrib,caesarxuchao,https://github.com/kubernetes/contrib/pull/1911,https://api.github.com/repos/kubernetes/contrib/issues/1911,Don't remove examples,"Marked as WIP because github is down, I cannot test the fix.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1911)

<!-- Reviewable:end -->
",closed,True,2016-10-21 19:35:22,2016-10-24 19:50:27
contrib,ncdc,https://github.com/kubernetes/contrib/issues/1912,https://api.github.com/repos/kubernetes/contrib/issues/1912,Add a way for the bot to avoid flake pings for issues requiring external fixes,"I have a few flakes assigned to me that can't be fixed until we move to a newer version of docker. It would be nice if there was some way, perhaps a label, to keep the bot from adding flake pings when we know we're dependent on external changes.

cc @ixdy @apelisse @lavalamp @spxtr @bgrant0607 @eparis 
",closed,False,2016-10-21 20:59:51,2018-02-16 07:54:00
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1913,https://api.github.com/repos/kubernetes/contrib/issues/1913,Cluster-autoscaler: Unschedulable (and unready) nodes are blocking CA,"When debugging customer issue we've seen the following scenario:
1. Customer drained 2 nodes to force CA to remove them
2. CA started reporting that cluster is not ready for autoscaling as number of expected and current nodes does not exist. This was caused by listing only ready, schedulable nodes. See [code](https://github.com/kubernetes/contrib/blob/d519cddaa0b85d37cc912fa6cf2c0592c2b8cf74/cluster-autoscaler/utils/kubernetes/listers.go#L98-L116)
3. CA couldn't do anything.

@davidopp @roberthbailey @mwielgus @piosz @jszczepkowski 
",closed,False,2016-10-21 22:05:01,2016-10-28 13:54:45
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1914,https://api.github.com/repos/kubernetes/contrib/issues/1914,test,"I'm using this PR to test the functionality of the approver-handler label.  Ignore it

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1914)

<!-- Reviewable:end -->
",closed,True,2016-10-21 22:37:10,2016-10-25 20:02:48
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1915,https://api.github.com/repos/kubernetes/contrib/issues/1915,Create Owners,,closed,True,2016-10-21 23:23:26,2017-11-18 20:05:30
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1916,https://api.github.com/repos/kubernetes/contrib/issues/1916,fixed the not in-place union issue,,closed,True,2016-10-22 00:59:58,2016-10-22 01:28:30
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1917,https://api.github.com/repos/kubernetes/contrib/issues/1917,Fix The Approval Handler,"In the `hasApprove` function there is a bug in the for loop

```
for { 
fileOwners := h.features.Repos.LeafAssignees(p)
        if fileOwners.Len() == 0 {
            glog.Warningf(""Couldn't find an owner for path (%s)"", p)
            continue
        }
...

p = path.Join(p, paths[i])

}
```

The continue statement never allows the handler to look deeper in the tree after it runs into the `fileOwners.Len() == 0`.  The continue statement means the path never gets appended to.
",closed,False,2016-10-22 01:04:26,2017-01-17 23:30:16
contrib,acca,https://github.com/kubernetes/contrib/pull/1918,https://api.github.com/repos/kubernetes/contrib/issues/1918,Vagrant script improvements,"- Solve issue described in #1881 
- Parametrise user and ssh public key for OpenStack provider from the configuration file. This avoid to modify `Vagrantfile` for custom deployments

I'm currently not a contributor but I will follow the procedure if the PR will be reviewed positively.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1918)

<!-- Reviewable:end -->
",closed,True,2016-10-22 10:15:26,2016-11-16 01:08:08
contrib,Simperfit,https://github.com/kubernetes/contrib/issues/1919,https://api.github.com/repos/kubernetes/contrib/issues/1919,Error Trying to run ansible on Fedora 21 / 23 / 24,"Is there something i'm doing wrong ? 

http://XX.XX.XX.XX:2379 is returning 404

```
fatal: [XX.XX.XX.XX -> XX.XX.XX.XX]: FAILED! => {""changed"": true, ""cmd"": ""/usr/bin/etcdctl --no-sync --peers=http://XX.XX.XX.XX:2379 set /k8s/network/config < /tmp/flannel-conf.json"", ""delta"": ""0:00:01.019966"", ""end"": ""2016-10-23 15:23:35.705567"", ""failed"": true, ""rc"": 4, ""start"": ""2016-10-23 15:23:34.685601"", ""stderr"": ""Error:  client: etcd cluster is unavailable or misconfigured\nerror #0: client: endpoint http://XX.XX.XX.XX:2379 exceeded header timeout"", ""stdout"": """", ""stdout_lines"": [], ""warnings"": []}
```
",closed,False,2016-10-23 13:26:19,2018-02-16 07:54:01
contrib,karolyi,https://github.com/kubernetes/contrib/issues/1920,https://api.github.com/repos/kubernetes/contrib/issues/1920,nginx ingress controller compilation error,"Hey,

I want to compile the nginx ingress controller in a google cloud shell.

Running `go get .`, I get the following error:

```
laszlo_karolyi@free-tier-test:~/kubernetes-contrib/ingress/controllers/nginx$ go get -v .
Fetching https://k8s.io/kubernetes/pkg/controller/framework?go-get=1
Parsing meta tags from https://k8s.io/kubernetes/pkg/controller/framework?go-get=1 (status code 200)
get ""k8s.io/kubernetes/pkg/controller/framework"": found meta tag main.metaImport{Prefix:""k8s.io/kubernetes"", VCS:""git"", RepoRoot:""https://github.com/kubernetes/kubernetes""} at https://k8s.io/kubernetes/pkg/controller/framework?go-get=1
get ""k8s.io/kubernetes/pkg/controller/framework"": verifying non-authoritative meta tag
Fetching https://k8s.io/kubernetes?go-get=1
Parsing meta tags from https://k8s.io/kubernetes?go-get=1 (status code 200)
k8s.io/kubernetes (download)
package k8s.io/kubernetes/pkg/controller/framework: cannot find package ""k8s.io/kubernetes/pkg/controller/framework"" in any of:
        /usr/local/go/src/k8s.io/kubernetes/pkg/controller/framework (from $GOROOT)
        /home/laszlo_karolyi/gopath/src/k8s.io/kubernetes/pkg/controller/framework (from $GOPATH)
        /google/gopath/src/k8s.io/kubernetes/pkg/controller/framework
```
",closed,False,2016-10-24 14:17:05,2017-01-16 12:52:20
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1921,https://api.github.com/repos/kubernetes/contrib/issues/1921,Cluster-autoscaler: add a way to specify which node pool to expand,"Right now if there are two node pools that expanding which would make a pod schedulable a random one is chosen. This may result creating a 32core+ssd node for a single pod. 
",closed,False,2016-10-24 21:13:29,2017-06-21 21:16:57
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1922,https://api.github.com/repos/kubernetes/contrib/issues/1922,Fixed OWNERS file in issue-labeler,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1922)

<!-- Reviewable:end -->
",closed,True,2016-10-24 22:13:06,2016-10-24 22:17:47
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1923,https://api.github.com/repos/kubernetes/contrib/issues/1923,fixed the hasApproval implementation,"Now we allow for approval for any approver in path
Also allow recognize approve cancel comment

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1923)

<!-- Reviewable:end -->
",closed,True,2016-10-24 23:22:08,2016-11-29 22:31:56
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1924,https://api.github.com/repos/kubernetes/contrib/issues/1924,Only Approved PRs to Be Merged After Feature Freeze,"No new features should be merged after Feature Freeze (see the exceptions doc), and we should enforce this using the mungebot, so that it does not have to be manually monitored.  

I propose we do the following:
- [ ] Add a new flag that indicates the current release
- [ ] Add a new flag that indicates if we're in feature freeze period
- [ ] Add a new label (releaseCzarApproved) that indicates Saad (and future releaseCzars) have given the ok for it to get merged
- [ ] Create a new munger that checks if we're in a feature freezer period.  If so, check if the milestone set on the PR matches the current one and if the releaseCzarApproved label has been applied.  Only merge the PR if both conditions are met

WDYT?

@apelisse @saad-ali @pwittrock 
",closed,False,2016-10-25 00:27:02,2018-02-17 13:23:01
contrib,pschiffe,https://github.com/kubernetes/contrib/issues/1925,https://api.github.com/repos/kubernetes/contrib/issues/1925,[ansible] kube-dash and kube-ui are invalid ansible variable names,"Ansible variable name can't contain dash
http://docs.ansible.com/ansible/playbooks_variables.html#what-makes-a-valid-variable-name

https://github.com/kubernetes/contrib/blob/master/ansible/inventory/group_vars/all.yml#L86
",closed,False,2016-10-25 13:54:55,2016-11-16 01:08:12
contrib,maxnowack,https://github.com/kubernetes/contrib/pull/1926,https://api.github.com/repos/kubernetes/contrib/issues/1926,fix link to sticky session documentation,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1926)

<!-- Reviewable:end -->
",closed,True,2016-10-25 14:59:35,2016-12-10 18:08:07
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1927,https://api.github.com/repos/kubernetes/contrib/issues/1927,"Add ""/dibs"" to available bot commands","Allow potential reviewers to comment ""/dibs"" on PRs that they would like assigned to them.   Also allow ""/nack"" from reviewers who have been assigned that want the PR to be assigned to someone else. 

cc
@bgrant0607 @apelisse @foxish 
",closed,False,2016-10-25 20:05:45,2016-12-09 22:53:21
contrib,Conky5,https://github.com/kubernetes/contrib/pull/1928,https://api.github.com/repos/kubernetes/contrib/issues/1928,[service-loadbalancer]: readme tweak,"Change haproxy stats command to be inline code markdown and use
ASCII quotes.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1928)

<!-- Reviewable:end -->
",closed,True,2016-10-25 20:46:59,2017-09-08 22:49:08
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/1929,https://api.github.com/repos/kubernetes/contrib/issues/1929,Have the bot automatically apply release-note-* labels,"We would like to require that PR authors correctly fill out the release note section in the PR template.  That means, if the PR requires no release note, they enter `NONE` in the release block section.  

And if the PR requires a release note, the block is filled in with text that adequately describes the bugfix/feature.  I also think we should remove the functionality that automatically adds the PR title as the release note entirely; this will ensure that if a release note is included, it was **intentional**.

The current problem is that a human being is still required to add one of the `release-note-*` labels manually.  Instead, I propose that the bot read the body of the PR and automatically remove the `release-note-needed` label, and add one of `release-note`, `release-note-none`, or `release-note-action-required` 

Does this make sense to everyone?

cc

@apelisse @foxish @bgrant0607 @david-mcmahon @eparis @roberthbailey 
",closed,False,2016-10-25 22:24:51,2016-11-16 01:32:43
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1930,https://api.github.com/repos/kubernetes/contrib/issues/1930,Automatically Add Release Note Label Based on PR Template,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1930)

<!-- Reviewable:end -->

Based on https://github.com/kubernetes/kubernetes/edit/master/.github/PULL_REQUEST_TEMPLATE.md

we can figure out which release-note-\* label to apply
",closed,True,2016-10-26 00:26:32,2016-11-10 18:39:12
contrib,a-robinson,https://github.com/kubernetes/contrib/issues/1931,https://api.github.com/repos/kubernetes/contrib/issues/1931,peer-finder gets stuck in retry loop of DNS errors when no peers are found,"While working on an init container to help with a potential edge case in cockroachdb's petset config (https://github.com/cockroachdb/cockroach/issues/10140), I spun up an init container with the following config as part of a petset:

``` yaml
        pod.alpha.kubernetes.io/init-containers: '[
            {
                ""name"": ""bootstrap"",
                ""image"": ""gcr.io/google_containers/peer-finder:0.1"",
                ""args"": [
                  ""-on-start=\""readarray PEERS;
                               if [ ${#PEERS[@]} -eq 0 ]; then
                                 mkdir -p /cockroach/cockroach-data && touch /cockroach/cockroach-data/cluster_exists_marker
                               fi\"""",
                  ""-service=cockroachdb""],
                ""env"": [
                  {
                      ""name"": ""POD_NAMESPACE"",
                      ""valueFrom"": {
                          ""fieldRef"": {
                              ""apiVersion"": ""v1"",
                              ""fieldPath"": ""metadata.namespace""
                          }
                      }
                   }
                ],
                ""volumeMounts"": [
                    {
                        ""name"": ""datadir"",
                        ""mountPath"": ""/cockroach/cockroach-data""
                    }
                ]
            }
        ]'
```

The first pet has been stuck in the init state for more than 10 minutes, and the peer-finder is clearly having a bad time, with its logs containing the same DNS error over and over:

``` console
$ kc logs cockroachdb-0 bootstrap
2016/10/26 16:49:53 lookup cockroachdb on 10.3.240.10:53: server misbehaving
2016/10/26 16:49:54 lookup cockroachdb on 10.3.240.10:53: server misbehaving
2016/10/26 16:49:55 lookup cockroachdb on 10.3.240.10:53: server misbehaving
...
```

It's expected that no peers would be found, but not that DNS errors would be returned. https://github.com/golang/go/issues/12712 looks like a potential cause, although it was supposedly fixed in 1.6 if you trust the milestone attached to it.

If I open a shell in the init container and play around with similar DNS lookups, this is what I see:

``` console
root@cockroachdb-0:/# dig cockroachdb

; <<>> DiG 9.10.3-P4-Ubuntu <<>> cockroachdb
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 18282
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;cockroachdb.           IN  A

;; Query time: 1 msec
;; SERVER: 10.3.240.10#53(10.3.240.10)
;; WHEN: Wed Oct 26 16:54:24 UTC 2016
;; MSG SIZE  rcvd: 29

root@cockroachdb-0:/# nslookup cockroachdb
Server:     10.3.240.10
Address:    10.3.240.10#53

** server can't find cockroachdb: SERVFAIL

root@cockroachdb-0:/# dig cockroachdb.default.svc.cluster.local

; <<>> DiG 9.10.3-P4-Ubuntu <<>> cockroachdb.default.svc.cluster.local
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 53063
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0

;; QUESTION SECTION:
;cockroachdb.default.svc.cluster.local. IN A

;; AUTHORITY SECTION:
cluster.local.      60  IN  SOA ns.dns.cluster.local. hostmaster.cluster.local. 1477501200 28800 7200 604800 60

;; Query time: 1 msec
;; SERVER: 10.3.240.10#53(10.3.240.10)
;; WHEN: Wed Oct 26 17:12:26 UTC 2016
;; MSG SIZE  rcvd: 148

root@cockroachdb-0:/# nslookup cockroachdb.default.svc.cluster.local
Server:     10.3.240.10
Address:    10.3.240.10#53

** server can't find cockroachdb.default.svc.cluster.local: NXDOMAIN

```

The SERVFAIL for `nslookup cockroachdb` is fairly damning, considering the peer-finder doesn't qualify its lookups with the namespace/suffix (I'd be interested to know why, but that's orthogonal). The kubedns containers in the cluster don't have any logs, but might be interesting with verbose logging enabled?

In case it matters, the cluster is at 1.4.0 on GKE using the alpha cluster option.

``` console
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.0"", GitCommit:""a16c0a7f71a6f93c7e0f222d961f4675cd97a46b"", GitTreeState:""clean"", BuildDate:""2016-09-26T18:16:57Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.0"", GitCommit:""a16c0a7f71a6f93c7e0f222d961f4675cd97a46b"", GitTreeState:""clean"", BuildDate:""2016-09-26T18:10:32Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

@bprashanth - is this a known issue, or something that needs further investigation?
",closed,False,2016-10-26 17:17:55,2018-01-17 18:04:48
contrib,Conky5,https://github.com/kubernetes/contrib/issues/1932,https://api.github.com/repos/kubernetes/contrib/issues/1932,"[service-loadbalancer] haproxy server check ""inter"" too short","The backend server port check `inter` value is set to 5 milliseconds: https://github.com/kubernetes/contrib/blob/master/service-loadbalancer/template.cfg#L143

This results in servers being marked as down quite frequently since they cannot always complete the check within 5ms (this is on a gcp kubernetes 2 node cluster).

Example of haproxy logs from service-loadbalancer:

```
servicelb [ALERT] Server graphite/10.0.0.9:80 is DOWN, reason: Layer4 timeout, check duration: 5ms. 0 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
servicelb [ALERT] Server graphite/10.0.0.9:80 is DOWN, reason: Layer4 timeout, check duration: 5ms. 0 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.
servicelb [EMERG] backend graphite has no server available!
servicelb [EMERG] backend graphite has no server available!
```

High frequency:

```
$ kubectl logs service-load-balancer-3qyxb --since 5m | grep 'Server graphite.* is DOWN, reason: Layer4 timeout' | wc -l
     935
```

Increasing `inter` to 2000ms (the default http://cbonte.github.io/haproxy-dconv/1.6/configuration.html#inter) results in much more stable backends.

This behavior results in 503 responses from service-loadbalancer thinking backends are down.

I'm thinking the default time should be increased or should be configurable.
",closed,False,2016-10-26 19:48:20,2018-02-17 23:33:00
contrib,wincus,https://github.com/kubernetes/contrib/issues/1933,https://api.github.com/repos/kubernetes/contrib/issues/1933,[Nginx Ingress Controller] errors during deployments,"When a k8s deployment takes place some pods are terminated by the scheduler using whatever deployment strategy is defined. As the nginx controller pulls the k8s state in time intervals, a  small time window takes place where the pod is not longer running ( has been terminated ) but it is still present as backend in the nginx controller configuration file.

This has as consequence that the client might get an error from the service that is being deployed if his request is routed to the terminated pod. 

We have been able to lower down the amount of errors a client gets during deployments setting the `upstream-fail-timeout` and `upstream-max-fails` annotations to the ingress object.
Unfortunately this has two problems:
1. as stated here [1] nginx will do ""Passive"" health checks, meaning that we can't reduce the amount of errors to `0` during deployments.
2. We can't use a fast rolling update strategy for deployments ( aka `maxSure:100%` ) cause we could have a nginx configuration with no valid backends during n seconds if all the old pods are terminated before the nginx controller updates its configuration.

Is there a best practice / known way to avoid the errors described above? 

[1] http://nginx.org/en/docs/http/load_balancing.html#nginx_load_balancing_health_checks
",closed,False,2016-10-27 00:12:29,2016-11-04 00:00:46
contrib,TerraTech,https://github.com/kubernetes/contrib/pull/1934,https://api.github.com/repos/kubernetes/contrib/issues/1934,README.md: fix typo for echoheaders-y deployment,"  This now matches up to:
  https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx#http

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1934)

<!-- Reviewable:end -->
",closed,True,2016-10-27 10:37:05,2016-12-03 03:13:12
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1935,https://api.github.com/repos/kubernetes/contrib/issues/1935,Cluster-autoscaler: allow unschedulable nodes,"Unschedulable nodes that belong to some autoscaling-enabled node pool are perfectly fine.

cc: @fgrzadkowski @piosz

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1935)

<!-- Reviewable:end -->
",closed,True,2016-10-27 10:49:06,2016-11-05 18:32:03
contrib,aledbf,https://github.com/kubernetes/contrib/pull/1936,https://api.github.com/repos/kubernetes/contrib/issues/1936,[nginx-slim]: update nginx to add feature SSL passthrough,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1936)

<!-- Reviewable:end -->
",closed,True,2016-10-27 12:13:17,2016-11-29 16:53:03
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1937,https://api.github.com/repos/kubernetes/contrib/issues/1937,Cluster-autoscaler: publish CA status as events in kube-system namespace,"More insight into what CA is doing was requested. Currently there is no good place to publish events about CA, especially in GKE where master node and CA pod is not visible. So we decided to publish them under kube-system namespace object as a temporary solution.

cc: @fgrzadkowski @piosz

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1937)

<!-- Reviewable:end -->
",closed,True,2016-10-27 12:18:47,2016-11-30 20:26:19
contrib,jokogr,https://github.com/kubernetes/contrib/pull/1938,https://api.github.com/repos/kubernetes/contrib/issues/1938,Use underscore in kube-* Ansible variables,"According to the Ansible documentation [1](http://docs.ansible.com/ansible/playbooks_variables.html#what-makes-a-valid-variable-name), variable names with dash are
considered invalid. This commit replaces kube-ui and kube-dash with
kube_ui and kube_dash, respectively.

Fixes #1925.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1938)

<!-- Reviewable:end -->
",closed,True,2016-10-27 12:24:36,2016-11-16 01:08:12
contrib,wojtek-t,https://github.com/kubernetes/contrib/pull/1939,https://api.github.com/repos/kubernetes/contrib/issues/1939,Block PRs on etcd3,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1939)

<!-- Reviewable:end -->
",closed,True,2016-10-27 14:16:05,2016-10-31 18:16:21
contrib,kayrus,https://github.com/kubernetes/contrib/pull/1940,https://api.github.com/repos/kubernetes/contrib/issues/1940,Increase kubernetes security in ansible scripts,"Inspired by https://github.com/kubernetes/kubernetes/issues/11816
By default the following locations are available through the internet:
- http://host:10251/configz
- http://host:10251/debug/pprof/heap
- http://host:10252/configz
- http://host:10252/debug/pprof/heap
- http://host:10255/metrics
- http://host:4194
- https://host:10250
- http://host:10255

Let's reduce probability to access them.

Forcing kubelet to listen the localhost interface results in inaccessible `kubectl exec` and `kubectl logs`. I'm trying to resolve this issue within https://github.com/kubernetes/kubernetes/pull/35693

Anyway it is recommended to set the following custom variables:
- `private_interface: ""eth0""`
- `kubelet_bind_address: ""{{ hostvars[inventory_hostname]['ansible_'+private_interface]['ipv4']['address'] }}""`
- `has_iptables: true`

And then `kubectl exec` and `kubectl logs` will work flawlessly.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1940)

<!-- Reviewable:end -->
",closed,True,2016-10-27 20:01:22,2018-02-17 13:23:01
contrib,ericuldall,https://github.com/kubernetes/contrib/issues/1941,https://api.github.com/repos/kubernetes/contrib/issues/1941,Ingress whitelist doesn't seem to be working,"I'm trying to restrict my ingress to a certain IP using the following doc (https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/whitelist) and it doesn't seem to be working. 

I'm running in a gke 1.4 cluster.

Here's my ingress yaml:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.kubernetes.io/backends: '{""k8s-be-30148--d986a7842f3541a8"":""HEALTHY"",""k8s-be-30419--d986a7842f3541a8"":""HEALTHY"",""k8s-be-31619--d986a7842f3541a8"":""HEALTHY""}'
    ingress.kubernetes.io/forwarding-rule: k8s-fw-default-site-ingress--d986a7842f3541a8
    ingress.kubernetes.io/target-proxy: k8s-tp-default-site-ingress--d986a7842f3541a8
    ingress.kubernetes.io/url-map: k8s-um-default-site-ingress--d986a7842f3541a8
    ingress.kubernetes.io/whitelist-source-range: 1.2.3.4/32
  creationTimestamp: 2016-10-26T19:20:14Z
  generation: 3
  name: [REDACTED]
  namespace: default
  resourceVersion: ""76990""
  selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/[REDACTED]
  uid: [REDACTED]
spec:
  rules:
  - host: a.site.com
    http:
      paths:
      - backend:
          serviceName: a
          servicePort: 80
  - host: b.site.com
    http:
      paths:
      - backend:
          serviceName: b
          servicePort: 80
status:
  loadBalancer:
    ingress:
    - ip: [REDACTED]
```

The ingress is routing traffic to the correct hosts, but the whitelist does not seem to take affect.
Does the whitelist even apply to hosts?
",closed,False,2016-10-27 20:07:43,2017-03-17 09:06:05
contrib,mtaufen,https://github.com/kubernetes/contrib/pull/1942,https://api.github.com/repos/kubernetes/contrib/issues/1942,Swap submit queue blockers for GCI equivalents,"Ref: https://github.com/kubernetes/kubernetes/issues/34335

/cc @vishh

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1942)

<!-- Reviewable:end -->
",closed,True,2016-10-27 22:00:46,2016-11-01 15:57:52
contrib,kushmansingh,https://github.com/kubernetes/contrib/pull/1943,https://api.github.com/repos/kubernetes/contrib/issues/1943,Fix bug where paths were being url decoded during rewrites,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1943)

<!-- Reviewable:end -->
",closed,True,2016-10-27 22:18:40,2017-03-15 12:34:35
contrib,calebcase,https://github.com/kubernetes/contrib/pull/1944,https://api.github.com/repos/kubernetes/contrib/issues/1944,ingress nginx: Should not interpret path as regex.,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1944)

<!-- Reviewable:end -->
",closed,True,2016-10-27 22:24:41,2017-04-04 15:13:23
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/1945,https://api.github.com/repos/kubernetes/contrib/issues/1945,Add assign and unassign functionality,"Fixes #1927 

Allow people to add self-assign PRs or decline PRs that have been assigned to them.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1945)

<!-- Reviewable:end -->
",closed,True,2016-10-27 22:43:48,2016-12-09 22:53:21
contrib,Ryan-Dmello,https://github.com/kubernetes/contrib/pull/1946,https://api.github.com/repos/kubernetes/contrib/issues/1946,Adding s390x support for dnsmasq and exec-healthz,"<!-- Reviewable:start -->

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1946)

<!-- Reviewable:end -->
",closed,True,2016-10-28 13:05:16,2017-02-23 23:14:58
contrib,pesho,https://github.com/kubernetes/contrib/issues/1947,https://api.github.com/repos/kubernetes/contrib/issues/1947,[Nginx ingress controller] TCP/UDP upstreams need to be sorted,"cc @aledbf 

I needed to expose an FTP service (don't ask :scream_cat:). FTP is a horrible protocol which requires exposing multiple ports for ""passive mode"" to work. I managed quickly to achieve that using the Nginx ingress controller's support for exposing TCP services, using a ConfigMap like this:

``` yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-ingress-conf-tcp
  namespace: kube-system
data:
  21: ""mynamespace/upload:21""
  2100: ""mynamespace/upload:2100""
  2101: ""mynamespace/upload:2101""
  2102: ""mynamespace/upload:2102""
  2103: ""mynamespace/upload:2103""
  2104: ""mynamespace/upload:2104""
  2105: ""mynamespace/upload:2105""
  2106: ""mynamespace/upload:2106""
  2107: ""mynamespace/upload:2107""
  2108: ""mynamespace/upload:2108""
  2109: ""mynamespace/upload:2109""
  2110: ""mynamespace/upload:2110""
  2111: ""mynamespace/upload:2111""
  2112: ""mynamespace/upload:2112""
  2113: ""mynamespace/upload:2113""
  2114: ""mynamespace/upload:2114""
  2115: ""mynamespace/upload:2115""
  2116: ""mynamespace/upload:2116""
  2117: ""mynamespace/upload:2117""
  2118: ""mynamespace/upload:2118""
  2119: ""mynamespace/upload:2119""
```

(the `mynamespace/upload` service exposes the same ports, of course)

A couple of hours later I started receiving alerts that the service is down, and investigation showed that Nginx was constantly reloading (every sync interval), which eventually lead to memory exhaustion due to the many old processes. Upon examining the generated `nginx.conf` within pods, it turned out that the order of TCP servers/upstreams is random and changes on each sync, which leads to Nginx reloading every time.

I think that TCP and UDP upstreams need to be sorted in order to have deterministic result, as is already done with HTTP ingress resources.

P.S. A mildly related question: Early versions of the Nginx ingress controller had support for updating the endpoints without reloading, using Lua scripts. Why was this approach abandoned? Looks like it was dropped in #570, but no rationale was given for the reason.
",closed,False,2016-10-28 19:54:09,2018-02-16 09:56:04
contrib,PierrickI3,https://github.com/kubernetes/contrib/issues/1948,https://api.github.com/repos/kubernetes/contrib/issues/1948,Create a scheduled job using the API,"Using minikube 0.12.0

I am trying to create a scheduled job using the API (via kubectl proxy):

`curl -X POST http://localhost:8001/apis/batch/v2alpha1/scheduledjobs -d @testscheduledjob.yaml -H 'Content-type:application/yaml'`

Contents of testscheduledjob.yaml:

```
apiVersion: batch/v2alpha1
kind: ScheduledJob
metadata:
  name: hello
spec:
  schedule: 0/1 * * * ?
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
```

I get this error:

```
{
  ""kind"": ""Status"",
  ""apiVersion"": ""v1"",
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""the server does not allow this method on the requested resource"",
  ""reason"": ""MethodNotAllowed"",
  ""details"": {},
  ""code"": 405
}
```

Is there a way to create a scheduled job other than using `kubectl` at the moment? 
",closed,False,2016-10-28 20:29:36,2016-10-28 21:01:13
contrib,pesho,https://github.com/kubernetes/contrib/issues/1949,https://api.github.com/repos/kubernetes/contrib/issues/1949,[Nginx ingress controller] Additional SSL options,"A couple of suggestions for additional SSL options:
- An option to enable OCSP stapling. Should be as easy as adding the [ssl_stapling](http://nginx.org/en/docs/http/ngx_http_ssl_module.html#ssl_stapling) directive. I also think it should be on by default, as it has no significant drawbacks and can speed up the first connection.
- An option to specify the [secret key](http://nginx.org/en/docs/http/ngx_http_ssl_module.html#ssl_session_ticket_key) used to encrypt the TLS session tickets. This is desirable when running multiple instances of the load balancer. The option should accept a file path, which could be provided using a Secret volume mount.

Of course, both of these could be achieved using a custom Nginx template.
",closed,False,2016-10-28 20:36:19,2018-02-19 22:18:59
contrib,kop,https://github.com/kubernetes/contrib/issues/1950,https://api.github.com/repos/kubernetes/contrib/issues/1950,[Nginx ingress controller] Ability to exclude routes from auth,"A little proposal for the authentication system.

It would be very helpful to have an ability to set a list of URI's (or a list of regular expressions) that should not use authentication system (both basic and external).

For example, in my current scenario, I need to limit access to our internal cluster services for users, but still be able to accept webhooks from third party systems.
",closed,False,2016-10-29 08:05:07,2016-10-31 21:41:46
contrib,ghost,https://github.com/kubernetes/contrib/pull/1951,https://api.github.com/repos/kubernetes/contrib/issues/1951,Allow custom configuration for apiserver host,"Currently your system uses the kubectl_util.DefaultClientConfig() to retrieve the IP for the api-server, which may not work outside GCE in customized setups.

This PR provides optional flags to use a non-default api-server. It uses the same flags and borrows code from the kubernetes/dashboard project:
https://github.com/kubernetes/dashboard/blob/master/src/app/backend/client/apiserverclient.go
https://github.com/kubernetes/dashboard/blob/master/src/app/backend/dashboard.go#L31

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1951)

<!-- Reviewable:end -->
",closed,True,2016-10-29 23:37:53,2018-02-27 17:22:51
contrib,hectorj2f,https://github.com/kubernetes/contrib/pull/1952,https://api.github.com/repos/kubernetes/contrib/issues/1952,Remove service should remove the deployments,"I was trying to use this package with any newer Kubernetes version. But it always fails to cleanup the services cause it tries to remove the replica controllers.  It should remove the deployments instead when using the current version.

I wanted to add some other improvements but I want to start with something smaller :).

ping @mikedanese

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1952)

<!-- Reviewable:end -->
",closed,True,2016-10-30 12:45:09,2016-12-07 21:53:36
contrib,bronger,https://github.com/kubernetes/contrib/issues/1953,https://api.github.com/repos/kubernetes/contrib/issues/1953,Upgrade Ansible installation to Kubernetes 1.4,"Currently, the installation via Ansible results in a Kubernetes-1.2-based cluster.  This should be upgraded to the current (at the time of this posting) version 1.4.",closed,False,2016-10-31 10:01:18,2016-12-09 21:58:59
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1954,https://api.github.com/repos/kubernetes/contrib/issues/1954,Cluster-autoscaler: more explicit logging on scale down.,"* Return pods to be rescheduled from simulations. First step towards graceful node deletion.
* Print node utilization and pods to be rescheduled on node scale down. 
* Consistent messages on scale down and scale up.

cc: @fgrzadkowski @piosz

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1954)
<!-- Reviewable:end -->
",closed,True,2016-10-31 15:03:05,2016-11-05 09:14:13
contrib,micheleorsi,https://github.com/kubernetes/contrib/pull/1955,https://api.github.com/repos/kubernetes/contrib/issues/1955,[nginx-ingress-controller] Added reload metrics,"relates to #1826

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1955)
<!-- Reviewable:end -->
",closed,True,2016-10-31 15:16:40,2016-11-02 11:56:49
contrib,Raffo,https://github.com/kubernetes/contrib/issues/1956,https://api.github.com/repos/kubernetes/contrib/issues/1956,Autoscaler on AWS should discover automatically the name of the ASG,"In the current implementation of the AutoScaler for the AWS CloudProvider, we need to pass the name of the autoscaling group (ASG). There's no need for this as it is possible to determine the ASG name from the instance itself via the AWS API. ",closed,False,2016-10-31 15:39:18,2017-05-10 09:07:59
contrib,micheleorsi,https://github.com/kubernetes/contrib/pull/1957,https://api.github.com/repos/kubernetes/contrib/issues/1957,[nginx ingress controller] possibility to specify nginx timeout via annotations for each namespace,"relates to this #1717

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1957)
<!-- Reviewable:end -->
",closed,True,2016-10-31 18:48:41,2016-11-02 11:57:16
contrib,yissachar,https://github.com/kubernetes/contrib/issues/1958,https://api.github.com/repos/kubernetes/contrib/issues/1958,Cluster-autoscaler: Scale up before pod becomes unschedulable,"Currently the cluster autoscaler waits for a pod to become unschedulable for it will start bringing new nodes up. Since nodes can take a long time to bring up, the unschedulable pod will remain unschedulable for a while. The cluster has basically entered a bad state and is now working on recovering from it.

Instead of only reactively bringing up nodes, the cluster autoscaler should also proactively bring up nodes when possible. It can do this on a best effort basis, following some heuristics (possible configurable), e.g. when no node has 1GB of memory available bring up another node.

Unschedulable pods would still trigger a scale up, since it is not possible to guarantee that a node will always be started proactively based on the heuristics.",closed,False,2016-10-31 18:53:44,2017-04-19 11:27:19
contrib,kayrus,https://github.com/kubernetes/contrib/pull/1959,https://api.github.com/repos/kubernetes/contrib/issues/1959,ansible: Don't upgrade key cluster components on every playbook run,"Latest software is good, but it should be under control.

Otherwise docker/etcd/flannel will be updated simultaneously on all nodes.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1959)
<!-- Reviewable:end -->
",closed,True,2016-10-31 20:47:35,2018-02-17 14:24:00
contrib,wattsteve,https://github.com/kubernetes/contrib/issues/1960,https://api.github.com/repos/kubernetes/contrib/issues/1960,[ansible] github-release option failing due to flannel timeouts,"I have 4 nodes. 1 Master and 3 Workers. I am running Fedora 24 on all of them and have run dnf -y update on all of them as well. I ran the same experiment earlier today with the packageManager option and everything worked and I had a working Kube 1.2.2 cluster. However, I want a Kube 1.4 cluster, so I used the github-release option on an identical setup but new cluster and it fails with the exception below:

TASK [flannel : Start flannel] *************************************************
changed: [leader.kube]
fatal: [node-2.kube]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Job for flanneld.service failed because a timeout was exceeded. See \""systemctl status flanneld.service\"" and \""journalctl -xe\"" for details.\n""}
fatal: [node-1.kube]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Job for flanneld.service failed because a timeout was exceeded. See \""systemctl status flanneld.service\"" and \""journalctl -xe\"" for details.\n""}
fatal: [node-3.kube]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Job for flanneld.service failed because a timeout was exceeded. See \""systemctl status flanneld.service\"" and \""journalctl -xe\"" for details.\n""}",closed,False,2016-10-31 21:53:44,2016-12-09 20:50:24
contrib,bowei,https://github.com/kubernetes/contrib/pull/1961,https://api.github.com/repos/kubernetes/contrib/issues/1961,Add -v (verbose) flag to verify-boilerplate,"Note: this change is copied from the main kubernetes repository

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1961)
<!-- Reviewable:end -->
",closed,True,2016-10-31 22:26:50,2016-11-01 06:12:18
contrib,coltonmorris,https://github.com/kubernetes/contrib/pull/1962,https://api.github.com/repos/kubernetes/contrib/issues/1962,Documentation fix,"Fixed some spelling mistakes.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1962)
<!-- Reviewable:end -->
",closed,True,2016-10-31 22:56:25,2016-11-01 00:31:38
contrib,hsyed,https://github.com/kubernetes/contrib/issues/1963,https://api.github.com/repos/kubernetes/contrib/issues/1963,DaemonSet not exposing hostPort correctly.,"I'm trying to get the DaemonSet example working. port 80 and 443 cause collisions on 4/6 nodes resulting in the controllers not comping up. Switching to 81 and 844 brings the controllers up but the ports aren't available on the nodes. Switching to `hostNetwork` does lead to 2/6 working controllers -- if I could find a way of mapping the ports I'd be happy.

The example leads me to believe DaemonSet should be mapping the ports. Is this a bug ?

I'm running on AWS in a 3x AZ HA config with weavenet. 

```
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: dev-services-lb
  namespace: dev-services
spec:
  template:
    metadata:
      labels:
        name: dev-services-lb
    spec:
#      hostNetwork: true
      terminationGracePeriodSeconds: 60
      containers:
      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3
        name: dev-services-lb
        imagePullPolicy: Always
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 1
        # use downward API
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        ports:
        - containerPort: 80
          hostPort: 81
        - containerPort: 443
          hostPort: 444
        args:
        - /nginx-ingress-controller
        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
```",closed,False,2016-11-01 01:03:52,2018-04-15 15:16:51
contrib,kayrus,https://github.com/kubernetes/contrib/pull/1964,https://api.github.com/repos/kubernetes/contrib/issues/1964,ansible: Enabled on-the-fly docker reconfiguration,"Since 1.12 docker supports [live-restore](https://docs.docker.com/engine/admin/live-restore/) option. Which can restart docker daemon without killing the containers unless you made storage driver reconfiguration or another significant configuration change.

[More info](https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file) about docker daemon configuration file.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1964)
<!-- Reviewable:end -->
",closed,True,2016-11-01 09:31:12,2016-11-16 16:18:13
contrib,mtaufen,https://github.com/kubernetes/contrib/pull/1965,https://api.github.com/repos/kubernetes/contrib/issues/1965,Remove gci jobs that are now blocking from the list of non-blocking jobs,"/cc @apelisse

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1965)
<!-- Reviewable:end -->
",closed,True,2016-11-01 15:55:33,2016-11-01 16:26:03
contrib,dgoujard,https://github.com/kubernetes/contrib/issues/1966,https://api.github.com/repos/kubernetes/contrib/issues/1966,[Nginx ingress controller] file open limit,"Hi,

I created a cluster with kubeadm on ubuntu 16.04.1 lts (really awesome tool), all pods works great except the nginx ingress controller.
I got this error : 
` [alert] 5#5: setrlimit(RLIMIT_NOFILE, 131072) failed (1: Operation not permitted)`

On a clean ubuntu image with docker i can reproduce this issue with this config : 

```
worker_processes 1;

pid /run/nginx.pid;

worker_rlimit_nofile 131072;

pcre_jit on;

events {
    multi_accept        on;
    worker_connections  16384;
    use                 epoll; 
}

http {
    real_ip_header      X-Forwarded-For;
    set_real_ip_from    0.0.0.0/0;

    real_ip_recursive   on;
    sendfile            on;
    aio                 threads;
    tcp_nopush          on;
    tcp_nodelay         on;

    log_subrequest      on;

    reset_timedout_connection on;

    keepalive_timeout 75s;

    types_hash_max_size 2048;
    server_names_hash_max_size 512;
    server_names_hash_bucket_size 64;

    include /etc/nginx/mime.types;
    default_type text/html;

    client_max_body_size ""1m"";

    error_log  /var/log/nginx/error.log notice;

    # Map a response error watching the header Content-Type
    map $http_accept $httpAccept {
        default          html;
        application/json json;
        application/xml  xml;
        text/plain       text;
    }

    map $httpAccept $httpReturnType {
        default          text/html;
        json             application/json;
        xml              application/xml;
        text             text/plain;
    }

    server_name_in_redirect off;
    port_in_redirect off;


    # default server for services without endpoints
    server {
        listen 8181;
        set $proxy_upstream_name ""-"";

        location / {
            return 503;
        }
    }
}
```

`docker run -v /home/ubuntu/nginx.conf:/etc/nginx/nginx.conf:ro gcr.io/google_containers/nginx-slim:0.9`

It's a limit with default ubuntu settings on file open. I didn't find how to bypass this limitation, i tried this : 
```
cat /etc/security/limits.d/nofile.conf
* soft nofile 10000
* hard nofile 1000000
```

How can i get this image running correctly ? i think i will not be this onlyone this the issue, it affect all ubuntu user with nginx-slim image (so Nginx ingress controller) ",closed,False,2016-11-01 18:23:10,2017-01-19 06:46:50
contrib,foxish,https://github.com/kubernetes/contrib/issues/1967,https://api.github.com/repos/kubernetes/contrib/issues/1967,Mungegithub loop takes too long,"As we go over each issue in the bot's main loop, we go over issues like https://github.com/kubernetes/kubernetes/issues/33388 which has some 2500 comments, it spends some 20 seconds or so (and lots of API calls) to fetch comments.
Do we really need to look at these issues completely? 

cc @eparis ",closed,False,2016-11-01 20:07:02,2017-01-21 01:15:43
contrib,saad-ali,https://github.com/kubernetes/contrib/issues/1968,https://api.github.com/repos/kubernetes/contrib/issues/1968,Super flaky tests hit GitHub comment limit,"The `Jenkins GCI GKE smoke e2e` test failed for my PR https://github.com/kubernetes/kubernetes/pull/36003 (see https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/36003/pull-kubernetes-e2e-gke-gci/2580/), because `Something went wrong: starting e2e cluster: error running up: exit status 1`.

This is an issue tracked by https://github.com/kubernetes/kubernetes/issues/33388. 

Looking for a status update in that issue, however, I noticed that the test flaked so many times, that GitHub disabled commenting:
```
Commenting has been disabled.
This conversation has reached the maximum number of allowed comments: 2,500
```

The issue has therefore not been updated for 21 days ago.

Something seems broken here. Should we be consolidating comments from the k8s-merge bot so that it doesn't hit the limit so quickly? Should we be opening up another issue?",closed,False,2016-11-01 21:40:04,2018-02-16 11:58:02
contrib,bowei,https://github.com/kubernetes/contrib/pull/1969,https://api.github.com/repos/kubernetes/contrib/issues/1969,Add healthz/ url for health checking,"URL for HTTP-based health checks

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1969)
<!-- Reviewable:end -->
",closed,True,2016-11-01 21:43:20,2016-11-05 00:18:36
contrib,caiyixiang,https://github.com/kubernetes/contrib/pull/1970,https://api.github.com/repos/kubernetes/contrib/issues/1970,"add condition,fix an error in ubuntu14.04","[ansible] An error occurred while installing on the ubuntu14.04, increasing the judgment condition, modifying the error，and the error is as follows:
RUNNING HANDLER [common : reload systemd] **************************************
fatal: [10.114.51.55]: FAILED! => {""changed"": false, ""cmd"": ""systemctl --system daemon-reload"", ""failed"": true, ""msg"": ""[Errno 2] No such file or directory"", ""rc"": 2}

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1970)
<!-- Reviewable:end -->
",closed,True,2016-11-02 09:30:46,2017-03-21 11:03:49
contrib,andrejvanderzee,https://github.com/kubernetes/contrib/issues/1971,https://api.github.com/repos/kubernetes/contrib/issues/1971,[nginx ingress ctrl] Timeout caused by TLS handshake error,"The nginx controller times out after 30 sec (while trying to watch its own status condition):

```main.go:125] unexpected error getting run-time information: timed out waiting for the condition```

The apiserver logs got many entries like this:

```http: TLS handshake error from 10.111.46.19:43942: remote error: bad certificate```

Service accounts are enabled and I can see the mount in `/var/run/secrets/kubernetes.io` for the correct namespace. Moreover, Heapster is running inCluster correctly. ",closed,False,2016-11-02 16:09:48,2018-02-16 11:58:04
contrib,ryanj,https://github.com/kubernetes/contrib/issues/1972,https://api.github.com/repos/kubernetes/contrib/issues/1972,Corporate contrib stats: CLA signature logs needed from CNCF,"In order to quantify the amount of impact that various corporate groups are making, we need an effective way to establish and maintain a map of corporate affiliations per each individual contributor.

The CLA signing process is an excellent forcing function that will help ensure that this mapping is accurate and up-to-date.

CNCF should provide logs with the following information to help make these calculations independently verifiable by different corporate groups:

1. cla_signature_timestamp
2. github_id
3. corporate_affiliation (or ""individual contributor"" for CLA signatures without any corporate affiliation)

The CLA signature timestamp is needed to keep track of changes in corporate affiliations over time.",closed,False,2016-11-02 17:10:02,2018-01-17 23:31:13
contrib,atomantic,https://github.com/kubernetes/contrib/pull/1973,https://api.github.com/repos/kubernetes/contrib/issues/1973,Cluster Autoscaler Value for Node Not Needed Time (10 minutes) is Not Configurable,"According to the docs page, ""If a node is not needed for more than 10 min (configurable) then it can be deleted.""
Where would I configure that 10 minute value. Since it will wait for 10 minutes per node it needs to delete, it could take a while to remove all the cruft if I have a lot of overage.

""I believe the value is not configurable."" - David Oppenheimer from GCP.

If this value is configurable, please close this PR and document how to configure it. If it really isn't configurable, this is a missing feature.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1973)
<!-- Reviewable:end -->
",closed,True,2016-11-02 17:58:19,2016-11-02 22:12:31
contrib,dhawal55,https://github.com/kubernetes/contrib/issues/1974,https://api.github.com/repos/kubernetes/contrib/issues/1974,Latest changes to nginx ingress controller is not pushed as a docker image,Trying to deploy the latest rc.yaml file on master branch fails the liveness/readiness probe. Looks like the /healthz port was changed 12 days ago but the docker image is couple months old,closed,False,2016-11-02 21:27:18,2017-01-23 17:49:58
contrib,foxish,https://github.com/kubernetes/contrib/pull/1975,https://api.github.com/repos/kubernetes/contrib/issues/1975,Turn on the check-labels munger,"Looks like https://github.com/kubernetes/kubernetes/pull/33947 never took effect because the munger was not turned on.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1975)
<!-- Reviewable:end -->
",closed,True,2016-11-02 21:27:57,2016-11-02 21:52:59
contrib,saad-ali,https://github.com/kubernetes/contrib/issues/1976,https://api.github.com/repos/kubernetes/contrib/issues/1976,k8s bot did not open test for flaking test,"The test `[k8s.io] [Feature:Example] [k8s.io] Spark should start spark master, driver and workers` has been consistently failing in the `kubernetes-e2e-gce-examples` test suite for a long time. However I don't see any issue opened for it.

Why did the k8s-merge-robot not open an issue for it. I see other tests in that suite have triggered automated issue creation in the past: e.g. https://github.com/kubernetes/kubernetes/issues/33106

I manually opened up an issue in the meantime: https://github.com/kubernetes/kubernetes/issues/36102",closed,False,2016-11-02 22:15:20,2018-02-16 11:58:03
contrib,dhawal55,https://github.com/kubernetes/contrib/issues/1977,https://api.github.com/repos/kubernetes/contrib/issues/1977,nginx ingress controller performance issues,"I'm running the latest nginx ingress controller (v0.8.3) on a Kubernetes (v1.4.4) cluster on AWS (m4.2xlarge instances). When I use [hey](https://github.com/rakyll/hey) to performance test my service, I see that nginx is adding 100-200 ms to response time.

Here's the output when running 500 requests against 4 nginx pods (so 125 requests each) for 2 mins:
```
Summary:
  Total:	121.0365 secs
  Slowest:	1.0351 secs
  Fastest:	0.0015 secs
  Average:	0.1553 secs
  Requests/sec:	495.7182
  Total data:	1200000 bytes
  Size/request:	20 bytes

Status code distribution:
  [200]	60000 responses

Response time histogram:
  0.001 [1]	|
  0.105 [4734]	|∎∎∎∎
  0.208 [48646]	|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
  0.312 [6618]	|∎∎∎∎∎
  0.415 [0]	|
  0.518 [0]	|
  0.622 [0]	|
  0.725 [0]	|
  0.828 [0]	|
  0.932 [0]	|
  1.035 [1]	|

Latency distribution:
  10% in 0.1091 secs
  25% in 0.1288 secs
  50% in 0.1530 secs
  75% in 0.1824 secs
  90% in 0.2106 secs
  95% in 0.2260 secs
  99% in 0.2485 secs
```

Now, when I run the same test against the service endpoint (by-passing nginx), I see much better performance:
```
Summary:
  Total:	120.0107 secs
  Slowest:	0.2032 secs
  Fastest:	0.0003 secs
  Average:	0.0048 secs
  Requests/sec:	499.9555
  Total data:	1200000 bytes
  Size/request:	20 bytes

Status code distribution:
  [200]	60000 responses

Response time histogram:
  0.000 [1]	|
  0.021 [59376]	|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
  0.041 [142]	|
  0.061 [416]	|
  0.081 [16]	|
  0.102 [0]	|
  0.122 [0]	|
  0.142 [0]	|
  0.163 [0]	|
  0.183 [0]	|
  0.203 [49]	|

Latency distribution:
  10% in 0.0015 secs
  25% in 0.0023 secs
  50% in 0.0034 secs
  75% in 0.0047 secs
  90% in 0.0098 secs
  95% in 0.0122 secs
  99% in 0.0309 secs
```

FYI, the service I'm hitting is not doing much, just printing ""hello-world"" and returning 200

What could be causing the 100-200 ms delay? Is it normal for nginx to add this kind of delay?
I have tried tuning the basic suspects but no luck. Here's my nginx config
```

daemon off;

worker_processes 8;

pid /run/nginx.pid;

worker_rlimit_nofile 131072;

pcre_jit on;

events {
    multi_accept        on;
    worker_connections  16384;
    use                 epoll; 
}

http {      
    real_ip_header      X-Forwarded-For;
    set_real_ip_from    0.0.0.0/0;
    real_ip_recursive   on;
 
    geoip_country       /etc/nginx/GeoIP.dat;
    geoip_city          /etc/nginx/GeoLiteCity.dat;
    geoip_proxy_recursive on;# lua section to return proper error codes when custom pages are used
    lua_package_path '.?.lua;./etc/nginx/lua/?.lua;/etc/nginx/lua/vendor/lua-resty-http/lib/?.lua;';
    init_by_lua_block {        
        require(""error_page"")
    }

    sendfile            on;
    aio                 threads;
    tcp_nopush          on;
    tcp_nodelay         on;
    
    log_subrequest      on;

    reset_timedout_connection on;

    keepalive_timeout 75s;

    types_hash_max_size 2048;
    server_names_hash_max_size 512;
    server_names_hash_bucket_size 256;

    include /etc/nginx/mime.types;
    default_type text/html;
    gzip on;
    gzip_comp_level 5;
    gzip_http_version 1.1;
    gzip_min_length 256;
    gzip_types application/atom+xml application/javascript aplication/x-javascript application/json application/rss+xml application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/svg+xml image/x-icon text/css text/plain text/x-component;    
    gzip_proxied any;

    client_max_body_size ""1m"";

    log_format upstreaminfo '$remote_addr - '
        '[$proxy_add_x_forwarded_for] - $remote_user [$time_local] ""$request"" $status $body_bytes_sent ""$http_referer"" ""$http_user_agent"" '
        '$request_length $request_time $upstream_addr $upstream_response_length $upstream_response_time $upstream_status';

    map $request $loggable {
        default 1;
    }

    access_log /var/log/nginx/access.log upstreaminfo if=$loggable;
    error_log  /var/log/nginx/error.log notice;

    # Custom dns resolver.
    resolver 25.0.0.10 valid=30s;

    map $http_upgrade $connection_upgrade {
        default upgrade;
        ''      close;
    }

    # trust http_x_forwarded_proto headers correctly indicate ssl offloading
    map $http_x_forwarded_proto $pass_access_scheme {
      default $http_x_forwarded_proto;
      ''      $scheme;
    }

    # Map a response error watching the header Content-Type
    map $http_accept $httpAccept {
        default          html;
        application/json json;
        application/xml  xml;
        text/plain       text;
    }

    map $httpAccept $httpReturnType {
        default          text/html;
        json             application/json;
        xml              application/xml;
        text             text/plain;
    }

    server_name_in_redirect off;
    port_in_redirect off;

    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

    # turn on session caching to drastically improve performance
   
    ssl_session_cache builtin:1000 shared:SSL:10m;
    ssl_session_timeout 10m;
    
    # allow configuring ssl session tickets
    ssl_session_tickets on;

    # slightly reduce the time-to-first-byte
    ssl_buffer_size 4k;
   
    # allow configuring custom ssl ciphers
    ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA';
    ssl_prefer_server_ciphers on;
   

    # In case of errors try the next upstream server before returning an error
    proxy_next_upstream                     error timeout invalid_header http_502 http_503 http_504;

    
    upstream default-hello-world-8080 {
        least_conn;
        server 25.1.5.10:8080 max_fails=0 fail_timeout=0;
        server 25.1.71.13:8080 max_fails=0 fail_timeout=0;
    
    }
        
    upstream upstream-default-backend {
        least_conn;
        server 25.1.5.11:8080 max_fails=0 fail_timeout=0;
       
    }

    server {
        server_name _;
        listen 80;
        listen 443  ssl spdy http2;
        
        # PEM sha: 4f450338155afd5929cd0194bd4fbcb825e5531c        
        ssl_certificate /etc/nginx-ssl/system-snake-oil-certificate.pem;
        ssl_certificate_key /etc/nginx-ssl/system-snake-oil-certificate.pem;

        more_set_headers                            ""Strict-Transport-Security: max-age=15724800; includeSubDomains; preload"";
 
        location / {
             proxy_set_header Host                   $host;

            # Pass Real IP
            proxy_set_header X-Real-IP              $remote_addr;

            # Allow websocket connections
            proxy_set_header                        Upgrade           $http_upgrade;
            proxy_set_header                        Connection        $connection_upgrade;

            proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Host       $host;
            proxy_set_header X-Forwarded-Port       $server_port;
            proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

            # mitigate HTTPoxy Vulnerability
            # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
            proxy_set_header Proxy                  """";

            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;

            proxy_redirect                          off;
            proxy_buffering                         off;
            proxy_http_version                      1.1;
    
            proxy_pass http://upstream-default-backend;
        }
        
        # this is required to avoid error if nginx is being monitored
        # with an external software (like sysdig)
        location /nginx_status {
            allow 127.0.0.1;
            deny all;

            access_log off;
            stub_status on;
        }
    }
        
    server {
        server_name hello-world.default.xxx.example.com;
        listen 80;
        location / {
            
            proxy_set_header Host                   $host;

            # Pass Real IP
            proxy_set_header X-Real-IP              $remote_addr;

            # Allow websocket connections
            proxy_set_header                        Upgrade           $http_upgrade;
            proxy_set_header                        Connection        $connection_upgrade;

            proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Host       $host;
            proxy_set_header X-Forwarded-Port       $server_port;
            proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

            # mitigate HTTPoxy Vulnerability
            # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
            proxy_set_header Proxy                  """";

            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;

            proxy_redirect                          off;
            proxy_buffering                         off;

            proxy_http_version                      1.1;

            
            proxy_pass http://default-hello-world-8080;
        }
    }
    
    # default server, used for NGINX healthcheck and access to nginx stats
    server {
        # Use the port 18080 (random value just to avoid known ports) as default port for nginx.
        # Changing this value requires a change in:
        # https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/nginx/command.go#L104
        listen 18080 default_server reuseport backlog=511;

        location /healthz {
            access_log off;
            return 200;
        }
       
        location /nginx_status {
            access_log off;
            stub_status on;
        }

        location / {
            proxy_pass             http://upstream-default-backend;
        }
    }

    # default server for services without endpoints
    server {
        listen 8181;
        location / {
           return 503;
       }        
    }    
}

stream {
# TCP services


# UDP services

}
```

",closed,False,2016-11-02 23:18:04,2018-02-16 12:59:03
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/1978,https://api.github.com/repos/kubernetes/contrib/issues/1978,How to use ingress controller for multiple namespaces in a K8s cluster?,"Hi, K8s guys,

We plan to deploy multiple namespaces in a K8s cluster. At present we have an nginx ingress controller for default namespace and one nginx ingress controller service which replica number is 2 for each namespaces. We check the nginx.conf in the controllers, and we find that the nginx.conf includes configuration(upstream, locations and proxy_pass, etc) for the service in other namespaces. 

How can we isolate the ingress controllers for each namespace? Or we do it in a wrong way.

Here I want to mention that the content of the yamls are the same except the namespace is different.

Thanks",closed,False,2016-11-03 04:32:20,2018-02-27 05:25:51
contrib,rudolfv,https://github.com/kubernetes/contrib/issues/1979,https://api.github.com/repos/kubernetes/contrib/issues/1979,Baking Google pagespeed into the nginx-ingress-controller,"I am trying to build an nginx-ingress-controller with the Google pagespeed module built in. We are currently using the following version of the controller in our Kubernetes cluster: gcr.io/google_containers/nginx-ingress-controller:0.8.3.

Therefore I started off by modifying the build for this image gcr.io/google_containers/nginx-slim:0.9. I checked out the commit ""Update nginx to 1.11.3 (4a467bfafb3e8e5b34e1fc47fa45b6e7d468d9ae)"" because that seems to be when the 0.9 release was cut (this is also the image used by version 0.8.3 of the ingress controller). I then added the pagespeed module to the build.sh file and built successfully. Basically merging the instructions found here into your build.sh: https://developers.google.com/speed/pagespeed/module/build_ngx_pagespeed_from_source. I also managed to start up nginx in the built docker container and verified that pagespeed was indeed added. I then pushed this to our repo - the tag is exactly the same except for the repo.

After that I checked out the ""Release 0.8.3 (add0235d60ec86b6c8e52de7be95c183de8cfe26)"" commit of the nginx-ingress-controller. I modified the build to use my custom nginx-slim image and managed to build successfully.

I had to make one change in the code due to dependencies that had changed in the meantime:

```2  ingress/controllers/nginx/nginx/template/template.go
 @@ -271,7 +271,7 @@ func buildRateLimit(input interface{}) []string {
  // maximum number of connections that can be queued for acceptance
  // http://nginx.org/en/docs/http/ngx_http_core_module.html#listen
  func sysctlSomaxconn() int {
 -	maxConns, err := sysctl.GetSysctl(""net/core/somaxconn"")
 +	maxConns, err := sysctl.New().GetSysctl(""net/core/somaxconn"")
  	if err != nil || maxConns < 512 {
  		glog.Warningf(""system net.core.somaxconn=%v. Using NGINX default (511)"", maxConns)
  		return 511
```

The nginx-ingress-controller executable and image built successfully (the executable runs in a standalone docker container and displays the help options). I tried to execute the dry run 

`./nginx-ingress-controller --running-in-cluster=false --default-backend-service=kube-system/default-http-backend`

but the --running-in-cluster option does not seem to be supported in that version. 

I then deployed the ingress controller to our Kubernetes cluster, but it fails with a 255 termination code. 

I then reverted all my changes and built the same way as above, without pagespeed, only changing the image repository name to ours. Essentially it should be exactly the same as your 0.8.3, only with the image repository names changed in the image tag (for both nginx-slim and nginx-ingress-controller). I got exactly the same problem, Kubernetes container terminating with a 255 exit code.

```kubectl describe pod nginx-ingress-controller-hcbcn
Name:		nginx-ingress-controller-hcbcn
Namespace:	default
Node:		myserver.corp/10.101.11.0
Start Time:	Thu, 03 Nov 2016 08:27:49 +0200
Labels:		k8s-app=nginx-ingress-lb
		name=nginx-ingress-lb
Status:		Running
IP:		10.32.0.52
Controllers:	ReplicationController/nginx-ingress-controller
Containers:
  nginx-ingress-lb:
    Container ID:	docker://312b46af5b03de0ba35c209be55a386bed25d709f008d406dee272ca5d415e1a
    Image:		my-artifactory.corp/google_containers/nginx-ingress-controller:0.8.3
    Image ID:		docker://sha256:d8f08c58f2338d334cd678201bfff5dbcb2a341690f584a800fad78c18ffff37
    Ports:		80/TCP, 443/TCP, 18080/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=$(POD_NAMESPACE)/default-http-backend
    Limits:
      cpu:	1
      memory:	2Gi
    Requests:
      cpu:		200m
      memory:		1Gi
    State:		Waiting
      Reason:		CrashLoopBackOff
    Last State:		Terminated
      Reason:		Error
      Exit Code:	255
      Started:		Thu, 03 Nov 2016 08:27:59 +0200
      Finished:		Thu, 03 Nov 2016 08:27:59 +0200
    Ready:		False
    Restart Count:	1
    Liveness:		http-get http://:10254/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment Variables:
      POD_NAME:		nginx-ingress-controller-hcbcn (v1:metadata.name)
      POD_NAMESPACE:	default (v1:metadata.namespace)
Conditions:
  Type		Status
  Initialized 	True 
  Ready 	False 
  PodScheduled 	True 
Volumes:
  default-token-qos4q:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-qos4q
QoS Tier:	Burstable
Events:
  FirstSeen	LastSeen	Count	From				SubobjectPath				Type		Reason		Message
  ---------	--------	-----	----				-------------				--------	------		-------
  27s		27s		1	{default-scheduler }							Normal		Scheduled	Successfully assigned nginx-ingress-controller-hcbcn to myserver.corp
  22s		22s		1	{kubelet myserver.corp}	spec.containers{nginx-ingress-lb}	Normal		Created		Created container with docker id cc284364d89e; Security:[seccomp=unconfined]
  22s		22s		1	{kubelet myserver.corp}	spec.containers{nginx-ingress-lb}	Normal		Started		Started container with docker id cc284364d89e
  24s		19s		2	{kubelet myserver.corp}	spec.containers{nginx-ingress-lb}	Normal		Pulling		pulling image ""my-artifactory.corp/google_containers/nginx-ingress-controller:0.8.3""
  24s		19s		2	{kubelet myserver.corp}	spec.containers{nginx-ingress-lb}	Normal		Pulled		Successfully pulled image ""my-artifactory.corp/google_containers/nginx-ingress-controller:0.8.3""
  17s		17s		1	{kubelet myserver.corp}	spec.containers{nginx-ingress-lb}	Normal		Created		Created container with docker id 312b46af5b03; Security:[seccomp=unconfined]
  17s		17s		1	{kubelet myserver.corp}	spec.containers{nginx-ingress-lb}	Normal		Started		Started container with docker id 312b46af5b03
  16s		7s		4	{kubelet myserver.corp}	spec.containers{nginx-ingress-lb}	Warning		BackOff		Back-off restarting failed docker container
  16s		7s		4	{kubelet myserver.corp}						Warning		FailedSync	Error syncing pod, skipping: failed to ""StartContainer"" for ""nginx-ingress-lb"" with CrashLoopBackOff: ""Back-off 10s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-hcbcn_default(a3fcf750-a18e-11e6-b66c-408d5ce19a83)""
```

BTW I have changed the server names in the above to protect sensitive information.

I even tried manually copying the nginx-ingress-controller executable from your 0.8.3 image and into my custom image. I ended up with the same result - 255 error code and same events as above.

`kubectl logs nginx-ingress-controller-hcbcn`

or

`kubectl logs nginx-ingress-controller-hcbcn --previous`

does not provide any log entries for any of the scenarios above.

This was all built on Ubuntu 15.10 (64-bit).

Any assistance will be appreciated. Also, is it a good idea to provide an image like this so that others can use it too? Or is there a better way to accomplish the same goal?",closed,False,2016-11-03 06:41:01,2016-11-03 16:46:56
contrib,taimir,https://github.com/kubernetes/contrib/pull/1980,https://api.github.com/repos/kubernetes/contrib/issues/1980,[WIP] OpenStack Heat cluster autoscaler,"Hi there, I've been working on an autoscaler for OpenStack for a while now and I thought I should share it. 

It  is entirely based on Heat. It utilizes Heat ResourceGroups (or AutoScalingGroups) to resize groups of instances that form a cluster of minions.

The rough idea is that the user specifies the name of the resource group (and Heat stack it is contained in) that should be auto-scaled when the autoscaler is started, like this:

```
min:max:resource-group:stack-name:size-param-name
```

The OpenStack-heat cloudprovider implementation then takes care of updating the parameters of the Heat stack in order to resize the ResourceGroup, based on the already existing auto-scaler policies (unschedulable pods, etc...).

I'll be happy to contribute this if you are interested :)

The commit contains no vendored dependencies, to ease up the review.

P.S.: I realize the new [Senlin](https://wiki.openstack.org/wiki/Senlin) service is better suited to autoscaling, but it is quite new and there are still no golang SDKs for it. I'd be happy to introduce a Senlin autoscaler later on, once it becomes more wide-spread (as there are currently many OpenStack installations that support Heat but not Senlin).

@kenan435

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1980)
<!-- Reviewable:end -->
",closed,True,2016-11-03 09:26:09,2018-02-24 03:58:50
contrib,craigwillis85,https://github.com/kubernetes/contrib/pull/1981,https://api.github.com/repos/kubernetes/contrib/issues/1981,Fixed a couple of typos in cluster-autoscaler,"This pull request fixes a couple of typos/grammatical errors in the logging output of the cluster-autoscaler addon

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1981)
<!-- Reviewable:end -->
",closed,True,2016-11-03 12:14:28,2016-11-17 08:18:07
contrib,Raffo,https://github.com/kubernetes/contrib/pull/1982,https://api.github.com/repos/kubernetes/contrib/issues/1982,AWS Autoscaler autodiscover ASG names and sizes,"This is just a possible idea on how to implement #1956. It's not yet enough generic and I would love to take the chance to discuss it. 

I implemented automatic discovery of autoscaling groups in case the CLI flag is not set.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1982)
<!-- Reviewable:end -->
",closed,True,2016-11-03 17:28:14,2017-04-14 11:37:49
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1983,https://api.github.com/repos/kubernetes/contrib/issues/1983,Create a troubleshooting guide for Cluster Autoscaler.,"And explain how various failures in the cluster affect CA.

cc: @fgrzadkowski @piosz ",closed,False,2016-11-03 19:12:57,2017-04-19 10:42:43
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1984,https://api.github.com/repos/kubernetes/contrib/issues/1984,Cluster-autoscaler: add profiles so that CA can support multiple consistent strategies.,"Currently you need to play with multiple flags with hard to predict results.

The profiles would be:
* Super aggressive - fast scale up, down, aggressive estimator etc.
* Conservative - slow scale ups, down, ""calm"" estimator etc
* Regular

cc: @fgrzadkowski @piosz  ",closed,False,2016-11-03 19:19:05,2017-06-22 01:13:43
contrib,michaelscheetz,https://github.com/kubernetes/contrib/issues/1985,https://api.github.com/repos/kubernetes/contrib/issues/1985,Not able to configure mungegithub to watch GHE servers.,,closed,False,2016-11-03 20:06:52,2016-11-16 00:28:39
contrib,michaelscheetz,https://github.com/kubernetes/contrib/pull/1986,https://api.github.com/repos/kubernetes/contrib/issues/1986,Add arg to allow mungegithub to watch GHE Enterprise server. Fixes #1985,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1986)
<!-- Reviewable:end -->
",closed,True,2016-11-03 20:07:45,2016-11-16 00:28:34
contrib,spxtr,https://github.com/kubernetes/contrib/pull/1987,https://api.github.com/repos/kubernetes/contrib/issues/1987,Remove /lgtm munger.,"Should go in around the same time as https://github.com/kubernetes/test-infra/pull/985.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1987)
<!-- Reviewable:end -->
",closed,True,2016-11-03 21:21:58,2016-11-03 23:39:50
contrib,renaudguerin,https://github.com/kubernetes/contrib/pull/1988,https://api.github.com/repos/kubernetes/contrib/issues/1988,Fix typo in mime types list for gzip in nginx controller,"Fixed typo in ""aplication/x-javascript"" -> ""application/x-javascript"" and updated corresponding documentation.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1988)
<!-- Reviewable:end -->
",closed,True,2016-11-04 14:24:30,2016-11-04 14:25:16
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1989,https://api.github.com/repos/kubernetes/contrib/issues/1989,Cluster-autoscaler: restrict one-time cluster scale up to 100,"So that a small mistake doesn't result in requesting 700 nodes.

cc: @piosz @fgrzadkowski @jsz

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1989)
<!-- Reviewable:end -->
",closed,True,2016-11-04 16:19:55,2016-11-07 14:11:40
contrib,foxish,https://github.com/kubernetes/contrib/pull/1990,https://api.github.com/repos/kubernetes/contrib/issues/1990,Feature freeze 1.5 PR,"To merge & push this sometime before Tuesday (11/8) morning. 
After that, only PRs with the v1.5 milestone set on them will be allowed in the queue.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1990)
<!-- Reviewable:end -->
",closed,True,2016-11-04 17:34:29,2016-11-08 06:08:25
contrib,renaudguerin,https://github.com/kubernetes/contrib/pull/1991,https://api.github.com/repos/kubernetes/contrib/issues/1991,Fix typo in gzip mime types list for nginx controller,"Fixed typo in default gzipTypes list :  ""aplication/x-javascript"" -> ""application/x-javascript"".
Updated corresponding documentation.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1991)
<!-- Reviewable:end -->
",closed,True,2016-11-04 17:46:26,2017-02-23 15:50:19
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1992,https://api.github.com/repos/kubernetes/contrib/issues/1992,Cluster-autoscaler: delete empty nodes in bulk,"Regular nodes are deleted one by one. However, if a nodes are completely empty (only daemonsets or manifest-run pods) then they can easily be deleted in bulk. With this PR in scale down we will first try to delete empty nodes in bulk and if that fails, we will proceed with regular node deletion.

cc: @fgrzadkowski @piosz @jszczepkowski

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1992)
<!-- Reviewable:end -->
",closed,True,2016-11-05 22:43:48,2016-11-07 17:45:20
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1993,https://api.github.com/repos/kubernetes/contrib/issues/1993,Cluster-autoscaler: bump version to 0.4.0-alpha,"cc: @fgrzadkowski @piosz @jszczepkowski 

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1993)
<!-- Reviewable:end -->
",closed,True,2016-11-06 17:10:15,2016-11-07 17:45:23
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/1994,https://api.github.com/repos/kubernetes/contrib/issues/1994,Cluster-autoscaler: reduce the number of HA-related logs,"Currently there is lots of renewed lease logs.

cc: @davidopp @fgrzadkowski @jszczepkowski @piosz ",closed,False,2016-11-06 23:13:49,2016-11-08 08:29:37
contrib,bprashanth,https://github.com/kubernetes/contrib/issues/1995,https://api.github.com/repos/kubernetes/contrib/issues/1995,Cluster-autoscaler: scale on schedule,"if I know a set of events that might produce higher load (dates/times, eg: sports events), I can specify a timetable and have the scaler preemptively scale instead of taking a hit",closed,False,2016-11-07 04:46:55,2017-04-19 12:00:20
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/1996,https://api.github.com/repos/kubernetes/contrib/issues/1996,Is it possible to generate nginx.conf which the server in upstream uses K8s FQDN instead of IP Address in the ingress controller?,"Hi, 

With the ingress controller, in the upstream of the nginx.conf, the server is IP address as follow;
upstream xxxx-svc-8080 {
        #least_conn;
        sticky;
        server 172.77.52.7:8080 max_fails=0 fail_timeout=0;
        server 172.77.57.6:8080 max_fails=0 fail_timeout=0;
}

Is it possible to generate the server with FQDN, such as [pod-name].[namespace].pod.cluster.local instead of IP Address?

Thanks",closed,False,2016-11-07 05:28:17,2016-11-07 13:34:23
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/1997,https://api.github.com/repos/kubernetes/contrib/issues/1997,How to config the nginx.tmpl for session sticky according to cookie JSESSIONID in nginx ingress controller?,"Hi,

In the default nginx.tmpl, the session sticky setting for upstream is as follow:

{{ if $cfg.enableStickySessions -}}
sticky hash=sha1 httponly;

Does this setting support the session sticky? If yes, what is the session sticky by?  cookie or others?

In our environment, we try to access http://xxx.xxx.xxx.xxx/webtier, and I config the upstream as follow:
upstream itsma-sm-webtier-svc-8080 {
        sticky name=JSESSIONID hash=sha1 httponly;
        server 172.77.52.7:8080;
        server 172.77.57.6:8080;
}

Then I can't login http://xxx.xxx.xxx.xxx/webtier, it always show the login page. I check the request and response, the cookie name is JSESSIONID. If the configuration is as follow:
upstream itsma-sm-webtier-svc-8080 {
        sticky;
        server 172.77.52.7:8080;
        server 172.77.57.6:8080;
}

Then I login successfully.

What's wrong with my configuration?

Thanks
Liu Peng
",closed,False,2016-11-07 08:51:46,2018-01-18 03:15:14
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/1998,https://api.github.com/repos/kubernetes/contrib/issues/1998,cluster-autoscaler: wrong comments in drain.go,"https://github.com/kubernetes/contrib/blob/master/cluster-autoscaler/simulator/drain.go

1. Comments to both functions mention `force` flag which doesn't exist
2. First comment seems have part of the last sentence missing",closed,False,2016-11-07 14:58:37,2017-06-22 01:02:26
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/1999,https://api.github.com/repos/kubernetes/contrib/issues/1999,Cluster-autoscaler: use binpacking estimation algorithm by default,"Should actually be better than basic for most of the users:
* If only 1 type of pod is pending CA is guaranteed to provide the best estimate.
* Only 1 iteration is needed to grow cluster to correct state.
* It should not be much worse than multi-phase iterative algorithm for mixed pending pods.

cc: @fgrzadkowski @piosz @jszczepkowski

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/1999)
<!-- Reviewable:end -->
",closed,True,2016-11-07 16:21:29,2016-11-07 16:55:14
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2000,https://api.github.com/repos/kubernetes/contrib/issues/2000,Cluster-autoscaler: get rid of -alpha in version,"Small follow-up fix after premature lgtm in #1993

cc: @fgrzadkowski

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2000)
<!-- Reviewable:end -->
",closed,True,2016-11-07 17:50:14,2016-11-07 18:15:25
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/2001,https://api.github.com/repos/kubernetes/contrib/issues/2001,Merge on Approved Only,"Merge PRs only if they have the ""approved"" label",closed,False,2016-11-07 18:17:13,2017-02-10 17:30:08
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2002,https://api.github.com/repos/kubernetes/contrib/issues/2002,Gate merge on exclusively approved label,"Will merge after

- [ ] ""/approved"" command works
- [ ] blunderbuss/bot read reviewers and approvers
- [ ] OWNERS files up to date
- [ ] approved label after ALL files have been approved by valid approver
- [ ] notification explaining why approved label has not yet been merged

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2002)
<!-- Reviewable:end -->
",closed,True,2016-11-07 18:48:25,2016-11-23 18:42:03
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2003,https://api.github.com/repos/kubernetes/contrib/issues/2003,mungegithub: Read alias files from repositories,"Add command line flag so that we parse the files properly.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2003)
<!-- Reviewable:end -->
",closed,True,2016-11-07 19:21:00,2016-11-08 16:29:19
contrib,andrewsykim,https://github.com/kubernetes/contrib/issues/2004,https://api.github.com/repos/kubernetes/contrib/issues/2004,AWS Cluster Autoscaler E2E Tests,I'd like to help add some E2E tests for the cluster autoscaler in AWS. Where would be a good place to start? Are there examples for the GCE E2E test suite I can reference? ,closed,False,2016-11-07 21:46:33,2018-03-11 02:50:32
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/2005,https://api.github.com/repos/kubernetes/contrib/issues/2005,Can least_conn and sticky work together in the nginx controller?,"Hi,

In my environment, one upstream in nginx.conf include both

least_conn;
sticky hash=sha1 httponly;

Then in the nginx log I found the error message: 
E1108 04:33:07.380779       1 command.go:92] failed to execute nginx -s reload: 2016/11/08 04:33:07 [emerg] 156#156: You can't use sticky with another upstream module in /etc/nginx/nginx.conf:254

Is it possible to make least_conn and sticky work together?

Thanks
Liu Peng",closed,False,2016-11-08 05:29:15,2018-01-18 03:14:39
contrib,apelisse,https://github.com/kubernetes/contrib/issues/2006,https://api.github.com/repos/kubernetes/contrib/issues/2006,Mungegithub doesn't handle identical blocking and non-blocking jobs,"If a job is both in the non-blocking queue and the blocking queue, everything starts acting funny (can someone say what's actually broken?).

We should probably:
- [ ] Validate/test that the configuration is sane as early as possible
- [ ] Act properly even if the configuration is slightly misleading: consider jobs in both lists to be blocking.",closed,False,2016-11-08 16:48:02,2017-01-21 01:35:35
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/2007,https://api.github.com/repos/kubernetes/contrib/issues/2007,Clarify ingress docs,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2007)
<!-- Reviewable:end -->
",closed,True,2016-11-08 18:01:06,2016-11-08 18:59:22
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2008,https://api.github.com/repos/kubernetes/contrib/issues/2008,mungegithub: Fixes #422,"At least attempts to fix one or two of the problems in #422.

Refresh the pull-request after we've done the tests to:
- Make sure it is still mergeable (doesn't have do-no-merge label or
still has lgtm label)
- The SHA hasn't changed, so that we don't commit something we haven't
tested

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2008)
<!-- Reviewable:end -->
",closed,True,2016-11-08 22:22:20,2016-11-10 17:17:10
contrib,andyxning,https://github.com/kubernetes/contrib/pull/2009,https://api.github.com/repos/kubernetes/contrib/issues/2009,fix typos,"Fix typos in README.

<!-- Reviewable:start -->

---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2009)
<!-- Reviewable:end -->
",closed,True,2016-11-09 06:18:10,2016-11-16 18:08:54
contrib,r2d4,https://github.com/kubernetes/contrib/issues/2010,https://api.github.com/repos/kubernetes/contrib/issues/2010,[Ingress] 0.8.4 Release?,"I understand ingress is being moved to kubernetes/ingress so if issues are being move there feel free to close this and I'll reopen there.  

Could we push a 0.8.4 release of the nginx-ingress controller?  The last one was released awhile ago

```
$ gcloud alpha container images list-tags gcr.io/google_containers/nginx-ingress-controller

820c338dc22e  0.8.3  2016-08-22T11:59:22
```",closed,False,2016-11-09 22:51:31,2017-02-08 22:36:08
contrib,SchwarzM,https://github.com/kubernetes/contrib/issues/2011,https://api.github.com/repos/kubernetes/contrib/issues/2011,Peer-finder unable to cope with non standard cluster domain,"Currently the peer-finder is hardcoded to ""svc.cluster.local"" which is the default but cannot be overridden.

https://github.com/kubernetes/contrib/blob/master/pets/peer-finder/peer-finder.go#L35

myName constructed here is faulty for us:
https://github.com/kubernetes/contrib/blob/master/pets/peer-finder/peer-finder.go#L86

Ending in an endless loop here:
https://github.com/kubernetes/contrib/blob/master/pets/peer-finder/peer-finder.go#L98

Would it be okay if I submit a patch reading the cluster domain from ENV ?

Kind regards
Marian Schwarz",closed,False,2016-11-10 10:23:22,2016-12-05 13:29:41
contrib,mrmm,https://github.com/kubernetes/contrib/issues/2012,https://api.github.com/repos/kubernetes/contrib/issues/2012,Ingress Controller - external address value,"Hi, 
I have been using Nginx Ingress Controller with Kubernetes cluster on AWS and i have noticed that the address field in the ingress object is updated automatically based on which host the Pod of IngressController is running on.
I think it would be better if the external IP or maybe a service name (will be LoadBalancer Service) to extract the external IP from.
Thanks.",closed,False,2016-11-10 13:29:11,2018-02-17 03:13:03
contrib,SchwarzM,https://github.com/kubernetes/contrib/pull/2013,https://api.github.com/repos/kubernetes/contrib/issues/2013,Introduce domain flag for non standard cluster domains,"This would address #2011 added this to facilitate discussion.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2013)
<!-- Reviewable:end -->
",closed,True,2016-11-10 14:19:49,2016-12-02 22:23:07
contrib,gambol99,https://github.com/kubernetes/contrib/issues/2014,https://api.github.com/repos/kubernetes/contrib/issues/2014,Ingress Controller - whitelisting,"We're running 0.8.3 ingress at present, the controller doesn't appear to order the whitelisted ip addresses, this is causing the controller to constantly reload on a 10s interval timer.

```shell
-            allow 1.1.2.2/32;
-            allow 10.98.0.0/16;
-            allow 2.2.2.2/32;
             allow x.x.x.196/32;
             allow x.x.x.91/32;
             allow x.x.x.164/32;
             allow x.x.x.139/32;
+            allow 1.1.2.2/32;
+            allow 10.98.0.0/16;
+            allow 2.2.2.2/32;
             deny all;
             # enforce ssl on server side
```
```shell
$ kubectl --namespace=ingress logs ingress-1936225639-bthcn 2>&1 | grep ""NGINX configuration""
I1110 16:08:17.772854       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I1110 16:08:27.772171       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I1110 16:08:37.773965       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I1110 16:08:47.771990       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I1110 16:08:57.772153       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I1110 16:09:07.777353       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I1110 16:09:17.678405       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I1110 16:09:27.774084       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I1110 16:09:37.770101       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I1110 16:09:47.770506       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
I1110 16:09:57.775239       1 utils.go:190] NGINX configuration diff a//etc/nginx/nginx.conf b//etc/nginx/nginx.conf
```

Hence every 10s i'm seeing
```
nobody   29510  0.0  0.0 387212 60160 ?        Sl   16:12   0:00 nginx: worker process is shutting down
nobody   29511  0.0  0.0 387212 60160 ?        Sl   16:12   0:00 nginx: worker process is shutting down
nobody   29512  0.0  0.0 387212 60160 ?        Sl   16:12   0:00 nginx: worker process is shutting down
nobody   29513  0.0  0.0 387212 60160 ?        Sl   16:12   0:00 nginx: worker process is shutting down
nobody   29514  0.0  0.0 387212 60160 ?        Sl   16:12   0:00 nginx: worker process is shutting down
nobody   29515  0.0  0.0 387212 60160 ?        Sl   16:12   0:00 nginx: worker process is shutting down
nobody   29516  0.0  0.0 387212 60160 ?        Sl   16:12   0:00 nginx: worker process is shutting down
nobody   29517  0.0  0.0 387212 60160 ?        Sl   16:12   0:00 nginx: worker process is shutting down
nobody   29783  0.0  0.0 387212 60116 ?        Sl   16:12   0:00 nginx: worker process
nobody   29784  0.0  0.0 387212 60116 ?        Sl   16:12   0:00 nginx: worker process
nobody   29785  0.0  0.0 387212 60116 ?        Sl   16:12   0:00 nginx: worker process
nobody   29786  0.0  0.0 387212 60116 ?        Sl   16:12   0:00 nginx: worker process
nobody   29787  0.0  0.0 387212 60116 ?        Sl   16:12   0:00 nginx: worker process
nobody   29788  0.0  0.0 387212 60116 ?        Sl   16:12   0:00 nginx: worker process
nobody   29789  0.0  0.0 387212 60116 ?        Sl   16:12   0:00 nginx: worker process
nobody   29790  0.0  0.0 387212 60116 ?        Sl   16:12   0:00 nginx: worker process
```",closed,False,2016-11-10 16:13:48,2017-03-09 16:18:59
contrib,r2d4,https://github.com/kubernetes/contrib/pull/2015,https://api.github.com/repos/kubernetes/contrib/issues/2015,Upgrade ingress controller image to 0.8.4,"ref https://github.com/kubernetes/minikube/issues/611#issuecomment-259756556
ref https://github.com/kubernetes/contrib/issues/2010

cc @bprashanth

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2015)
<!-- Reviewable:end -->
",closed,True,2016-11-10 18:15:05,2016-12-07 18:25:33
contrib,chrislovecnm,https://github.com/kubernetes/contrib/issues/2016,https://api.github.com/repos/kubernetes/contrib/issues/2016,Developer office hours for kops,"We need to setup a zoom based office hours every week this is for developers to come ask questions about what they are working on.

@calebamiles HELP :)  Label and assign to me @kris-nova and @justinsb ",closed,False,2016-11-10 18:34:47,2017-06-13 02:58:46
contrib,test-foxish,https://github.com/kubernetes/contrib/pull/2017,https://api.github.com/repos/kubernetes/contrib/issues/2017,Create do-not-merge.md,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2017)
<!-- Reviewable:end -->
",closed,True,2016-11-10 18:46:48,2016-11-10 19:10:30
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2018,https://api.github.com/repos/kubernetes/contrib/issues/2018,Add reviewers to owners parsing,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2018)
<!-- Reviewable:end -->
",closed,True,2016-11-10 22:17:42,2016-11-23 18:41:14
contrib,timstclair,https://github.com/kubernetes/contrib/issues/2019,https://api.github.com/repos/kubernetes/contrib/issues/2019,OWNERS for vendored dependencies,"Once https://github.com/kubernetes/contrib/issues/1389 (OWNERS implementation) is complete, we should consider adding OWNERS (specifically approvers) for each vendored (godep) dependency. I'm concerned that dependencies are not sufficiently reviewed right now, and version-bumps are typically blind sync-to-head operations.

Since a thorough review (audit) of vendored code is probably off the table (especially for large projects), reviewers should be responsible for:

- Sanity checking (e.g. anything iffy in `init` methods?), skim the code changes
- Check whether the update makes sense (should we sync to master or a release branch/tag?)
- Check for significant known issues / open bugs that may be introduced by the update

Additionally, it would be great if approvers (or some new role, ""maintainers""?) would be responsible for monitoring issues/PRs in the upstream, and be responsible for updating with critical fixes.

/cc @apelisse @kubernetes/contributor-experience ",closed,False,2016-11-10 23:49:43,2018-02-16 15:01:03
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2020,https://api.github.com/repos/kubernetes/contrib/issues/2020,fixing misapplication of do not merge label,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2020)
<!-- Reviewable:end -->
",closed,True,2016-11-11 00:19:04,2016-11-11 00:49:17
contrib,brendandburns,https://github.com/kubernetes/contrib/pull/2021,https://api.github.com/repos/kubernetes/contrib/issues/2021,Add a github munger that knows how to map files (or dirs) to PRs,"@eparis @philips

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2021)
<!-- Reviewable:end -->
",closed,True,2016-11-11 06:08:54,2017-04-07 20:12:11
contrib,jayprakashji,https://github.com/kubernetes/contrib/issues/2022,https://api.github.com/repos/kubernetes/contrib/issues/2022,"[ansible] kube2sky crashing, unable to connect to api","HI,
I'm fighting with an issue with (near) default install of ansible/kube.
Many logs an outputs in attached file.

It all comes down to :
I1111 10:17:40.832838       1 kube2sky.go:604] Ignoring error while waiting for service default/kubernetes: Get https://10.254.0.1:443/api/v1/namespaces/default/services/kubernetes: dial tcp 10.254.0.1:443: getsockopt: connection refused. Sleeping 1s before retrying.

virtualbox Centos 7.2
1 master 1 node

I tryed also 2-3 nodes, kube_source_type: github-release (which installed 1.2.7), in many different combinations with tweaking config values (each deploy-cluster.sh run on clean base snapshot of master and nodes).


Please, what am I missing ?",closed,False,2016-11-11 10:37:13,2018-02-16 21:07:02
contrib,wittrock,https://github.com/kubernetes/contrib/issues/2023,https://api.github.com/repos/kubernetes/contrib/issues/2023,[GCE L7LB controller] Can't create new Google Cloud Load Balancer with ingresses,"When trying to create an ingress with `kubectl create -f ingress.yaml` and watching the ingress description as it's created, I see an error from the Google API:

```  7m		2s		23	{loadbalancer-controller }			Warning		GCE	googleapi: Error 400: Invalid value for field 'resource.timeoutSec': '200'. TimeoutSec should be less than checkIntervalSec., invalid```

My intuition here is that the ingress controller is trying to create a health check via the GCE API, and sending an invalid value. 

The ingress file contains no timeout specifications of any kind.

Is there a way I can override this value with a flag or another setting? I'd like to avoid creating load balancers manually to point at my kubernetes cluster, and this leaves me completely unable to create new ingresses. ",closed,False,2016-11-11 20:25:47,2016-11-11 20:51:35
contrib,thockin,https://github.com/kubernetes/contrib/pull/2024,https://api.github.com/repos/kubernetes/contrib/issues/2024,Ignore autogen files in sizer,"This is an action item from dev summit.  It builds but is not tested.  PTAL and help me test?

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2024)
<!-- Reviewable:end -->
",closed,True,2016-11-11 22:03:51,2016-11-18 15:08:46
contrib,devlinmr,https://github.com/kubernetes/contrib/pull/2025,https://api.github.com/repos/kubernetes/contrib/issues/2025,SSL-enabled Nginx Ingress Controller integrated Hashicorp Vault,"Based on the nginx-alpha controller, this adds functionality to configure SSL nginx servers for ingresses requiring it, pulling in certificates from Vault.

This is the controller demonstrated at KubeCon 2016 by Michael Ward (devoperandi.com) of Pearson.

See README.md.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2025)
<!-- Reviewable:end -->
",closed,True,2016-11-12 13:01:12,2016-11-18 14:58:32
contrib,ObviusOwl,https://github.com/kubernetes/contrib/issues/2026,https://api.github.com/repos/kubernetes/contrib/issues/2026,[ansible] ansible fails with 'kube_binaries' is undefined when using packageManager on fedora 24,"Hi,
I'm on a fully updated fresh install of fedora 24. The configuration is almost default except enabled kube-ui and kube-dash.

Ansible fails with:

```
TASK [master : Set a list of kube master binaries to download] *****************
skipping: [kube-master.example.com] => {""changed"": false, ""skip_reason"": ""Conditional check failed"", ""skipped"": true}

TASK [master : Extend the list of kube master binaries with kubelet] ***********
skipping: [kube-master.example.com] => {""changed"": false, ""skip_reason"": ""Conditional check failed"", ""skipped"": true}

TASK [master : Download Kubernetes binaries] ***********************************
fatal: [kube-master.example.com]: FAILED! => {""failed"": true, ""msg"": ""'kube_binaries' is undefined""}
	to retry, use: --limit @/root/contrib/ansible/playbooks/deploy-cluster.retry
```

Deleting the content in `roles/master/tasks/download_bins.yml` helps.

The same applies for `roles/master/tasks/install_rpms.yml` where ansible complains about undefined `kube_rpms` (logs not included for brevity)

Deleting the content of the files still provides a working set-up, since the packages are in fact installed correctly and thus don't need external rpm's or github release.

Maybe this is a issue with ansible not ignoring correctly the tasks, unfortunately I'm completely new to ansible.
",closed,False,2016-11-12 19:15:26,2017-01-03 07:10:22
contrib,soudy,https://github.com/kubernetes/contrib/issues/2027,https://api.github.com/repos/kubernetes/contrib/issues/2027,[ingress] Path regex matching,"I was wondering about regex matching in Ingress paths like nginx does with `~`. For example, I'd like to be able to do something like this:
```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: api
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /user/*/tiles/
        backend:
          serviceName: specific
          servicePort: 80
      - path: /
        backend:
          serviceName: default
          servicePort: 80

```
And have /user/test/tiles/ go to the `specific` service. At the moment it will only literally match `/user/*/tiles`. I see [here](https://github.com/kubernetes/contrib/blob/494064ba4e685a3d9432411bcd72dc5d49641316/ingress/controllers/nginx/nginx.tmpl#L212) that `location =` is used, is it perhaps an idea to make this configurable? I found [this](http://stackoverflow.com/a/40026443) stackoverflow answer from 2013 in which the `~ path` syntax is used, which I assume was removed. 

Are there any plans on implementing this, and also, if it was removed, why so?",closed,False,2016-11-14 13:10:47,2018-03-11 17:04:33
contrib,gmarek,https://github.com/kubernetes/contrib/pull/2028,https://api.github.com/repos/kubernetes/contrib/issues/2028,Create a deployment file for perfdash,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2028)
<!-- Reviewable:end -->
",closed,True,2016-11-14 16:02:13,2016-11-15 08:01:31
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2029,https://api.github.com/repos/kubernetes/contrib/issues/2029,Update README.md,"Document PR workflow

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2029)
<!-- Reviewable:end -->
",closed,True,2016-11-14 22:58:10,2017-11-18 20:05:34
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2030,https://api.github.com/repos/kubernetes/contrib/issues/2030,Describe New PR Workflow for Contrib in README,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2030)
<!-- Reviewable:end -->
",closed,True,2016-11-15 00:23:51,2017-11-18 20:05:36
contrib,discostur,https://github.com/kubernetes/contrib/issues/2031,https://api.github.com/repos/kubernetes/contrib/issues/2031,contrib/keepalived-vip can't create multiple VIPs,"Hi,

im trying to setup multiple VIPs inside of one Kubernetes cluster. One VIP is running smooth, but keepalived-vip doesn't setup another VIP:


**vip-configmap.yaml**

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: vip-configmap
data:
  192.168.10.100: default/rc-ngx
  192.168.10.110: default/rc-ngx-dev
```

**kubectl exec kube-keepalived-vip-8lkjf cat /etc/keepalived/keepalived.conf**
```
global_defs {
  vrrp_version 3
  vrrp_iptables KUBE-KEEPALIVED-VIP
}

vrrp_instance vips {
  state BACKUP
  interface eth0
  virtual_router_id 50
  priority 102
  nopreempt
  advert_int 1

  track_interface {
    eth0
  }

  virtual_ipaddress { 
    192.168.10.100
  }
}

# Service: default/rc-ngx
virtual_server 192.168.10.100 443 {
  delay_loop 5
  lvs_sched wlc
  lvs_method NAT
  persistence_timeout 1800
  protocol TCP

  
  real_server 10.36.0.1 443 {
    weight 1
    TCP_CHECK {
      connect_port 443
      connect_timeout 3
    }
  }

  real_server 10.44.0.9 443 {
    weight 1
    TCP_CHECK {
      connect_port 443
      connect_timeout 3
    }
  }

  real_server 10.47.0.1 443 {
    weight 1
    TCP_CHECK {
      connect_port 443
      connect_timeout 3
    }
  }

} 
```

I expected keepalived-vip to read the second configuration from my configmap, raise an additional IP address and map the traffic to my second service.

Thanks
Greets
Kilian",closed,False,2016-11-15 08:21:30,2018-02-21 20:04:04
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/2032,https://api.github.com/repos/kubernetes/contrib/issues/2032,"Cluster Autoscaler - cluster autoscaler thinks that a pod can scheduler, when it cannot ","I have created the cluster using these commands:

https://github.com/kubernetes/kubernetes/issues/34772#issuecomment-260594667

I see many pending pods and CA got blocked with the following logs:

```
W1115 14:40:17.711911       7 utils.go:96] Pod my-app-2781155866-p03b8 marked as unschedulable can be scheduled on gke-test-cluster-1-defa
ult-pool-a1904744-c141. Ignoring in scale up.
```

There are many of those lines for different pods.

I checked one of the nodes and it seems that scheduler is correct - it has 4 cores and 3480m are being used and pods requires 550m. It seems we have a serious bug in cluster autoscaler",closed,False,2016-11-15 16:52:16,2016-12-07 11:14:00
contrib,mkumatag,https://github.com/kubernetes/contrib/pull/2033,https://api.github.com/repos/kubernetes/contrib/issues/2033,ansible : load balance kubernetes api server,"This PR will enables load balancing across multiple kubernetes api servers.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2033)
<!-- Reviewable:end -->
",closed,True,2016-11-15 19:54:22,2017-01-03 08:02:20
contrib,foxish,https://github.com/kubernetes/contrib/pull/2034,https://api.github.com/repos/kubernetes/contrib/issues/2034,Disable the CLA munger in contrib.,"Waiting to turn on the CLA plugin in prow for contrib before merging this.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2034)
<!-- Reviewable:end -->
",closed,True,2016-11-15 20:30:41,2016-11-16 00:14:44
contrib,foxish,https://github.com/kubernetes/contrib/pull/2035,https://api.github.com/repos/kubernetes/contrib/issues/2035,do-not-merge,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2035)
<!-- Reviewable:end -->
",closed,True,2016-11-16 00:24:44,2017-11-18 20:04:11
contrib,fejta,https://github.com/kubernetes/contrib/pull/2036,https://api.github.com/repos/kubernetes/contrib/issues/2036,Use ci- version of test-go and verify,"See https://github.com/kubernetes/test-infra/pull/1076

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2036)
<!-- Reviewable:end -->
",closed,True,2016-11-16 01:09:55,2016-11-16 01:38:46
contrib,fejta,https://github.com/kubernetes/contrib/pull/2037,https://api.github.com/repos/kubernetes/contrib/issues/2037,Follow the bootstrap version of etcd3,"https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-etcd3/1353

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2037)
<!-- Reviewable:end -->
",closed,True,2016-11-16 01:53:02,2016-11-16 18:18:56
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/2038,https://api.github.com/repos/kubernetes/contrib/issues/2038,Why the host of INGRESS must be FQDN instead of IP address?,"Hi,

Could any experts tell me that why the host of INGRESS must be FQDN instead of IP address? In some virtual environment, we can get IP address and the FQDN might not be reached.

Thanks",closed,False,2016-11-16 07:03:17,2018-01-18 03:14:10
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/2039,https://api.github.com/repos/kubernetes/contrib/issues/2039,Why the rewrite directive change the matched URL code?,"Hi, 

We define Ingress rewrite annotation for our service. And get the nginx log below:
`
2016/11/16 08:42:36 [notice] 25#25: *14 rewritten data: ""/action=encrypted&data=MzJ8mNJbn7G5d8dxZBSaBSMBM0dIdzC0Ovzv+TVHrfJi84M="", args: """", client: 15.107.4.151, server: shclitvm0647.hpeswlab.net, request: ""GET /stproxyport/action=encrypted&data=MzJ8mNJbn7G5d8dxZBSaBSMBM0dIdzC0Ovzv%2BTVHrfJi84M%3D HTTP/1.1"", host: ""shclitvm0647.hpeswlab.net""
`
You can see that in the GET URL, the value of parameter ""data"" is MzJ8mNJbn7G5d8dxZBSaBSMBM0dIdzC0Ovzv%2BTVHrfJi84M%3D, but in the rewritten data, it becomes ""MzJ8mNJbn7G5d8dxZBSaBSMBM0dIdzC0Ovzv+TVHrfJi84M="": %2B->+ and %3D ->=.

What happens? How can I keep it not changed?

Thanks
",closed,False,2016-11-16 09:46:26,2016-11-18 01:11:59
contrib,vglazkov,https://github.com/kubernetes/contrib/issues/2040,https://api.github.com/repos/kubernetes/contrib/issues/2040,Can i specify different load balancing algorithms for different services?,"I have two services - index and query. First one in stateless, so i would like to use default least_connection algorithm. Second one is statefull, in my case i want to load balance it with the specific cookie field with nginx Hash algorithm. How can i achieve that?",closed,False,2016-11-16 10:11:09,2018-02-16 21:07:00
contrib,aledbf,https://github.com/kubernetes/contrib/pull/2041,https://api.github.com/repos/kubernetes/contrib/issues/2041,[keepalived-vip]: Add support for proxy protocol,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2041)
<!-- Reviewable:end -->
",closed,True,2016-11-16 13:09:02,2017-05-23 05:01:38
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2042,https://api.github.com/repos/kubernetes/contrib/issues/2042,set live-restore only for docker => 1.12,"Fix the cluster deployment for older version of docker.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2042)
<!-- Reviewable:end -->
",closed,True,2016-11-16 15:18:30,2016-11-18 10:54:15
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2043,https://api.github.com/repos/kubernetes/contrib/issues/2043,mungegithub: Remove extreme verbosity from release-note,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2043)
<!-- Reviewable:end -->
",closed,True,2016-11-16 17:26:15,2016-11-16 17:58:51
contrib,timactive,https://github.com/kubernetes/contrib/issues/2044,https://api.github.com/repos/kubernetes/contrib/issues/2044,[Nginx ingress controller] Annotations configuration in ingress,"Add new anotations for nginx config in ingress

This annotation must override config in configmap.

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
 name: docker-registry
 namespace: devops
 annotations:
   ingress.kubernetes.io/cg-body-size: ""600M""
   ingress.kubernetes.io/cg-server-name-hash-bucket-size: ""64""
   ingress.kubernetes.io/cg-server-name-hash-bucket-size: ""64""
spec:
  tls:
  - hosts:
    - registry.example.com
    secretName: registry-example-tls
  rules:
    - host: registry.example.com
      http:
        paths:
        - path: /
          backend:
            serviceName: docker-registry-sv
            servicePort: 5000
```
",closed,False,2016-11-16 18:03:34,2018-01-17 21:08:13
contrib,zmerlynn,https://github.com/kubernetes/contrib/pull/2045,https://api.github.com/repos/kubernetes/contrib/issues/2045,submit-queue: Add kubernetes-e2e-kops-aws to the blocking list,"Now that https://github.com/kubernetes/test-infra/pull/1015 is in, this can be blocking. (Keeping the `kops-updown` variant in the non-blocking list for deployment health tracking.)

Along the way: Reformat the configmap (commits are split up for easy review).

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2045)
<!-- Reviewable:end -->
",closed,True,2016-11-16 20:20:53,2016-11-16 20:58:59
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2046,https://api.github.com/repos/kubernetes/contrib/issues/2046,mungegithub: Disable issue-triager while refactoring labels,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2046)
<!-- Reviewable:end -->
",closed,True,2016-11-16 21:17:19,2016-11-16 21:49:02
contrib,fejta,https://github.com/kubernetes/contrib/pull/2047,https://api.github.com/repos/kubernetes/contrib/issues/2047,Update kubernetes job list to use ci- prefixes,"Add ci- prefix for jobs we migrated.
Migrate three additional critical jobs:
```
ci-kubernetes-e2e-gci-gce
ci-kubernetes-e2e-gci-gce-slow
ci-kubernetes-kubemark-500-gce
```

See https://github.com/kubernetes/test-infra/pull/1089

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2047)
<!-- Reviewable:end -->
",closed,True,2016-11-16 22:10:57,2016-11-16 22:39:04
contrib,gmarek,https://github.com/kubernetes/contrib/pull/2048,https://api.github.com/repos/kubernetes/contrib/issues/2048,"Remove kubemark-scale from monitored suites, as we use it for testing","cc @wojtek-t @fejta 

@apelisse - can you merge this and push the changes to the SubmitQueue? Thanks.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2048)
<!-- Reviewable:end -->
",closed,True,2016-11-17 08:06:20,2016-11-17 17:43:45
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2049,https://api.github.com/repos/kubernetes/contrib/issues/2049,"Bump kubernetes to 1.4.5, Bump libvirt fedora virtbox to 24-cloud-base","Fixes: #1953

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2049)
<!-- Reviewable:end -->
",closed,True,2016-11-17 12:49:56,2016-12-09 21:58:59
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2050,https://api.github.com/repos/kubernetes/contrib/issues/2050,quote bash arguments when passed to ansible playbook,"If not, value of ``--extra-vars`` is not quoted and the ``ansible-playbook`` ends with en error.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2050)
<!-- Reviewable:end -->
",closed,True,2016-11-17 14:35:14,2016-11-17 14:59:06
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2051,https://api.github.com/repos/kubernetes/contrib/issues/2051,mungegithub: Remove generated file from deployment,"Fixes a bug introduced by: https://github.com/kubernetes/contrib/pull/2024

The file has been removed but is still referenced in the Dockerfile. Let's remove it.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2051)
<!-- Reviewable:end -->
",closed,True,2016-11-17 17:20:26,2016-11-17 19:58:43
contrib,foxish,https://github.com/kubernetes/contrib/pull/2052,https://api.github.com/repos/kubernetes/contrib/issues/2052,Disable check-labels munger temporarily.,"For renaming labels as part of: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/kubernetes-wg-contribex/mI7kuTFa_I4/pn3MIUxVBAAJ
Will restart all 3 mungebot instances after this is merged.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2052)
<!-- Reviewable:end -->
",closed,True,2016-11-17 20:12:38,2016-11-17 20:36:39
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2053,https://api.github.com/repos/kubernetes/contrib/issues/2053,promote groups all to templated variable,"Required by ansible >= 2.2

Fixes: #1960

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2053)
<!-- Reviewable:end -->
",closed,True,2016-11-17 21:11:47,2016-12-09 20:50:24
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/2054,https://api.github.com/repos/kubernetes/contrib/issues/2054,Release Note Auto-Assign Doesn't Recognize If RN On Next Line,"```
noteMatcher := regexp.MustCompile(""Release note.*```(.+)```"")
```

The wildcard matcher does not recognize newlines.  Need to set the (?s) flag.

",closed,False,2016-11-17 21:27:54,2016-11-30 22:53:17
contrib,vishpat,https://github.com/kubernetes/contrib/pull/2055,https://api.github.com/repos/kubernetes/contrib/issues/2055,Iptables kubeproxy,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2055)
<!-- Reviewable:end -->
",closed,True,2016-11-17 21:39:40,2016-11-17 21:40:30
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2056,https://api.github.com/repos/kubernetes/contrib/issues/2056,handle the case where release note not on same line as tag,"The (?s) flag allows the wildcard operator to match newlines as well.

fixes #2054

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2056)
<!-- Reviewable:end -->
",closed,True,2016-11-17 21:39:48,2016-11-30 22:53:17
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/2057,https://api.github.com/repos/kubernetes/contrib/issues/2057,Does the ingress controller monitor the change of nginx.tmpl?,"Hi, 

Ingress controller can monitor the change of the container network environment. However, can I change the content of the nginx.tmpl and the controller regenerate the nginx.conf and reload the Nginx?",closed,False,2016-11-18 01:17:44,2018-02-16 23:09:00
contrib,ged15,https://github.com/kubernetes/contrib/issues/2058,https://api.github.com/repos/kubernetes/contrib/issues/2058,Ingress does not support several TLS certificates,"Firstly, thanks for the fantastic tool that is Kubernetes! Really helps me in a bunch of situations!
Are there any plans to make a single Ingress support several TLS certificates? Currently I can have an Ingress route traffic for several host names but I cannot have a TLS cert per hostname so I cannot configure one Ingress to route traffic for `example.com` and `example.org` with their own certificates.",closed,False,2016-11-18 09:36:59,2018-01-02 01:15:18
contrib,porridge,https://github.com/kubernetes/contrib/pull/2059,https://api.github.com/repos/kubernetes/contrib/issues/2059,Fix markup.,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2059)
<!-- Reviewable:end -->
",closed,True,2016-11-18 10:08:27,2016-11-18 15:14:24
contrib,devlinmr,https://github.com/kubernetes/contrib/pull/2060,https://api.github.com/repos/kubernetes/contrib/issues/2060,SSL-enabled Nginx Ingress Controller integrated Hashicorp Vault ,"Based on the nginx-alpha controller, this adds functionality to configure SSL nginx servers for ingresses requiring it, pulling in certificates from Vault.

This is the controller demonstrated at KubeCon 2016 by Michael Ward (devoperandi.com) of Pearson.

See README.md.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2060)
<!-- Reviewable:end -->
",closed,True,2016-11-18 15:00:06,2018-02-01 19:42:20
contrib,vglazkov,https://github.com/kubernetes/contrib/issues/2061,https://api.github.com/repos/kubernetes/contrib/issues/2061,Nginx ingress controller fails to start with custom template,"Hi,

I'm trying to create ingress controller with custom template like in the basic example. When I use the same nginx.tmpl file i get the following output from nginx controller log:
`
I1118 15:13:31.787977       1 main.go:94] Using build: https://github.com/bprashanth/contrib.git - git-92b2bac
I1118 15:13:31.829894       1 main.go:123] Validated default/default-http-backend as the default backend
W1118 15:13:31.830209       1 ssl.go:132] no file dhparam.pem found in secrets
F1118 15:13:31.832432       1 main.go:71] invalid NGINX template: template: nginx.tmpl:210: function ""buildAuthLocation"" not defined
`
",closed,False,2016-11-18 15:17:42,2017-06-25 16:36:32
contrib,krzyzacy,https://github.com/kubernetes/contrib/pull/2062,https://api.github.com/repos/kubernetes/contrib/issues/2062,add ci- to converted bootstrap jobs for SQ,"ref
https://github.com/kubernetes/test-infra/pull/1111
https://github.com/kubernetes/test-infra/pull/1122
https://github.com/kubernetes/test-infra/pull/1123

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2062)
<!-- Reviewable:end -->
",closed,True,2016-11-18 22:28:39,2016-11-18 22:54:26
contrib,fejta,https://github.com/kubernetes/contrib/pull/2063,https://api.github.com/repos/kubernetes/contrib/issues/2063,Update SQ job list to include more bootstrap-migrated jobs,"continuous-e2e-docker-validation-gci is not the name of a valid job so dropping it.

ref kubernetes/test-infra#1021 https://github.com/kubernetes/test-infra/issues/1022 https://github.com/kubernetes/test-infra/issues/1108

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2063)
<!-- Reviewable:end -->
",closed,True,2016-11-19 00:19:08,2016-11-19 18:20:30
contrib,mkumatag,https://github.com/kubernetes/contrib/issues/2064,https://api.github.com/repos/kubernetes/contrib/issues/2064,[ansible] deploy-cluster fails with newer flannel (flannel-0.5.5-1.el7.x86_64),"deploy-clusteer.sh has been run on RHEL7.3 x86 arch machine.

```
TASK [flannel : Start flannel] *************************************************
task path: /root/fix_defect/contrib/ansible/roles/flannel/tasks/main.yml:21
Using module file /usr/lib/python2.7/site-packages/ansible/modules/core/system/systemd.py
<master.example.com> ESTABLISH SSH CONNECTION FOR USER: None
<master.example.com> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/ansible-ssh-%h-%p-%r master.example.com '/bin/sh -c '""'""'( umask 77 && mkdir -p ""` echo $HOME/.ansible/tmp/ansible-tmp-1479564837.51-100973652713456 `"" && echo ansible-tmp-1479564837.51-100973652713456=""` echo $HOME/.ansible/tmp/ansible-tmp-1479564837.51-100973652713456 `"" ) && sleep 0'""'""''
Using module file /usr/lib/python2.7/site-packages/ansible/modules/core/system/systemd.py
<node.example.com> ESTABLISH SSH CONNECTION FOR USER: None
<node.example.com> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/ansible-ssh-%h-%p-%r node.example.com '/bin/sh -c '""'""'( umask 77 && mkdir -p ""` echo $HOME/.ansible/tmp/ansible-tmp-1479564837.53-80742592107946 `"" && echo ansible-tmp-1479564837.53-80742592107946=""` echo $HOME/.ansible/tmp/ansible-tmp-1479564837.53-80742592107946 `"" ) && sleep 0'""'""''
<master.example.com> PUT /tmp/tmpoxtsVw TO /root/.ansible/tmp/ansible-tmp-1479564837.51-100973652713456/systemd.py
<master.example.com> SSH: EXEC sftp -b - -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/ansible-ssh-%h-%p-%r '[master.example.com]'
<node.example.com> PUT /tmp/tmpAcJbXd TO /root/.ansible/tmp/ansible-tmp-1479564837.53-80742592107946/systemd.py
<node.example.com> SSH: EXEC sftp -b - -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/ansible-ssh-%h-%p-%r '[node.example.com]'
<master.example.com> ESTABLISH SSH CONNECTION FOR USER: None
<master.example.com> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/ansible-ssh-%h-%p-%r master.example.com '/bin/sh -c '""'""'chmod u+x /root/.ansible/tmp/ansible-tmp-1479564837.51-100973652713456/ /root/.ansible/tmp/ansible-tmp-1479564837.51-100973652713456/systemd.py && sleep 0'""'""''
<node.example.com> ESTABLISH SSH CONNECTION FOR USER: None
<master.example.com> ESTABLISH SSH CONNECTION FOR USER: None
<node.example.com> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/ansible-ssh-%h-%p-%r node.example.com '/bin/sh -c '""'""'chmod u+x /root/.ansible/tmp/ansible-tmp-1479564837.53-80742592107946/ /root/.ansible/tmp/ansible-tmp-1479564837.53-80742592107946/systemd.py && sleep 0'""'""''
<master.example.com> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/ansible-ssh-%h-%p-%r -tt master.example.com '/bin/sh -c '""'""'sudo -H -S -n -u root /bin/sh -c '""'""'""'""'""'""'""'""'echo BECOME-SUCCESS-aegkpfcazxmkyimouvtmkttbyaqjxxzh; /usr/bin/python /root/.ansible/tmp/ansible-tmp-1479564837.51-100973652713456/systemd.py; rm -rf ""/root/.ansible/tmp/ansible-tmp-1479564837.51-100973652713456/"" > /dev/null 2>&1'""'""'""'""'""'""'""'""' && sleep 0'""'""''
<node.example.com> ESTABLISH SSH CONNECTION FOR USER: None
<node.example.com> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/ansible-ssh-%h-%p-%r -tt node.example.com '/bin/sh -c '""'""'sudo -H -S -n -u root /bin/sh -c '""'""'""'""'""'""'""'""'echo BECOME-SUCCESS-znxlsrsekfgkeerishqxeyabahdlcdto; /usr/bin/python /root/.ansible/tmp/ansible-tmp-1479564837.53-80742592107946/systemd.py; rm -rf ""/root/.ansible/tmp/ansible-tmp-1479564837.53-80742592107946/"" > /dev/null 2>&1'""'""'""'""'""'""'""'""' && sleep 0'""'""''



fatal: [master.example.com]: FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""invocation"": {
        ""module_args"": {
            ""daemon_reload"": false, 
            ""enabled"": null, 
            ""masked"": null, 
            ""name"": ""flanneld"", 
            ""state"": ""started"", 
            ""user"": false
        }
    }, 
    ""msg"": ""Unable to start service flanneld: Job for flanneld.service failed because a timeout was exceeded. See \""systemctl status flanneld.service\"" and \""journalctl -xe\"" for details.\n""
}
fatal: [node.example.com]: FAILED! => {
    ""changed"": false, 
    ""failed"": true, 
    ""invocation"": {
        ""module_args"": {
            ""daemon_reload"": false, 
            ""enabled"": null, 
            ""masked"": null, 
            ""name"": ""flanneld"", 
            ""state"": ""started"", 
            ""user"": false
        }
    }, 
    ""msg"": ""Unable to start service flanneld: Job for flanneld.service failed because a timeout was exceeded. See \""systemctl status flanneld.service\"" and \""journalctl -xe\"" for details.\n""
}

RUNNING HANDLER [flannel : restart flannel] ************************************
	to retry, use: --limit @/root/fix_defect/contrib/ansible/playbooks/deploy-cluster.retry

PLAY RECAP *********************************************************************
node.example.com : ok=45   changed=12   unreachable=0    failed=1   
master.example.com : ok=70   changed=20   unreachable=0    failed=1   
```


Failure is happening because in flannel-0.5.5-1 environment variables names for etcd got changed, due to this flannel is not able to start and whole cluster deploy is failing.

ExecStart=/usr/bin/flanneld -etcd-endpoints=**${FLANNEL_ETCD_ENDPOINTS}** -etcd-prefix=**${FLANNEL_ETCD_PREFIX}** $FLANNEL_OPTIONS

FLANNEL_ETCD_ENDPOINTS, FLANNEL_ETCD_PREFIX are changed variables and we are passing FLANNEL_ETCD and FLANNEL_ETCD_KEY, refer https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/templates/flanneld.j2#L4 for more information.

We can fix this issue in two ways:

1. Update flanneld.j2 with modified environment variable names - This may brakes if script uses older version flannel.
2. Overwrite the service file with old environment variable names. - We may need to maintain our own flannel.service file

Any suggestion?",closed,False,2016-11-19 14:30:57,2016-12-06 13:09:40
contrib,mkumatag,https://github.com/kubernetes/contrib/pull/2065,https://api.github.com/repos/kubernetes/contrib/issues/2065,ansible: Set empty array by default in kubernetes master role if tasks skipped,"Fix for https://github.com/kubernetes/contrib/issues/2026

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2065)
<!-- Reviewable:end -->
",closed,True,2016-11-19 17:32:42,2016-11-21 14:16:06
contrib,mumoshu,https://github.com/kubernetes/contrib/issues/2066,https://api.github.com/repos/kubernetes/contrib/issues/2066,Cluster-autoscaler: AWS EC2 Spot Fleets support,"Hi, thanks for developing cluster-autoscaler!

Is this something you'd like to add to cluster-autoscaler?

With spot fleets, we can easily mix various types of spot instances into a single logical group managed by AWS to benefit from reduced infrastructure cost and reduced operational burden including bidding, choosing which type of instance type to bid, increased availability via mixing mutliple spot instance types(assuming it will be rare that we lose to all the bids to all the different spot types at once).

A spot fleet has the notion of ""target capacity"" where ""capacity"" means number of units(a.k.a InstanceWeight) required to handle your workload e.g. 1 for r3.2xlarge, 4 for r3.8xlarge.
Basically I guess we can treat 1 target capacity = 1 instance weight as unit of autoscaling in cluster-autoscaler so that we can reuse a lot of current codebase to support spot fleets.

I'm both interested in using it and if necessary implementing it.
If you have any comments, advices, questions, etc., please let me know!",closed,False,2016-11-21 06:34:34,2018-11-05 14:41:37
contrib,pabloguerrero,https://github.com/kubernetes/contrib/pull/2067,https://api.github.com/repos/kubernetes/contrib/issues/2067,Updated sed command to work on macOS,"A similar issue as fixed in kubernetes/kubernetes#20147

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2067)
<!-- Reviewable:end -->
",closed,True,2016-11-21 08:19:22,2017-09-09 06:03:29
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/2068,https://api.github.com/repos/kubernetes/contrib/issues/2068,Is it possible for nginx ingress controller to use our own cookie for session sticky?,"Hi,

At present nginx ingress controller uses a third-party session sticky module. But this module seems not support using our own cookie, such as JSESSIONID. It adds its own cookie route. Is it possible for nginx ingress controller to use our own cookie for session sticky?

Thanks",closed,False,2016-11-21 09:27:40,2018-01-22 23:53:09
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2069,https://api.github.com/repos/kubernetes/contrib/issues/2069,WIP: Extend flanneld with canonicalized envs,"Fixes: #2064, #1769

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2069)
<!-- Reviewable:end -->
",closed,True,2016-11-21 13:55:53,2016-12-06 13:09:40
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2070,https://api.github.com/repos/kubernetes/contrib/issues/2070,mungegithub: Export metrics through Prometheus,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2070)
<!-- Reviewable:end -->
",closed,True,2016-11-21 19:20:41,2016-11-21 21:36:09
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2071,https://api.github.com/repos/kubernetes/contrib/issues/2071,mungegithub: Prometheus should not use gzipHandler,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2071)
<!-- Reviewable:end -->
",closed,True,2016-11-22 00:09:30,2016-11-22 00:36:11
contrib,chrismoos,https://github.com/kubernetes/contrib/pull/2072,https://api.github.com/repos/kubernetes/contrib/issues/2072,Add health checking from NGINX to individual endpoints.,"This change adds the [nginx_upstream_check_module](https://github.com/yaoweibin/nginx_upstream_check_module) into NGINX and configures upstreams with health checks based on the `readinessProbe` in the service's pod(s).

It is configurable with ConfigMap (default disabled):

**use-upstream-health-checks**

If the upstream health checking is enabled there is also an additional endpoint at `/upstream_status`.

This is related to #953.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2072)
<!-- Reviewable:end -->",closed,True,2016-11-22 08:16:00,2017-04-04 15:13:14
contrib,afoninsky,https://github.com/kubernetes/contrib/issues/2073,https://api.github.com/repos/kubernetes/contrib/issues/2073,[nginx ingress controller] config reload question,"1) Im getting errors in logs like `upstream default-someingressname-8080 does not have any active endpoints. Using default backend`. Such ingresses was deleted long ago, but nginx din't know it. Is there way to reload config by hands?

2) External loadbalancer ip is changed every time then ingress pod restarts (example: cluster update). In case of ""gce ingress"" this can be fixed by writing annotation something like ""keep ip permanent"" (sorry, didn't remember configuration name exactly). Is there something similar in nginx ingress?

Thank you.",closed,False,2016-11-22 12:52:06,2018-02-17 19:29:02
contrib,sigxcpu76,https://github.com/kubernetes/contrib/issues/2074,https://api.github.com/repos/kubernetes/contrib/issues/2074,[keepalived-vip],"I'm using https://github.com/kubernetes/contrib/tree/master/keepalived-vip and keepalived crashes with SIGSEGV. I'm not 100% sure, but the unset ""track_interface"" can be the problem?

The interface should be ```ens3```.


Kernel log:
```
Nov 22 14:30:44 kube-minion-11.sgu kernel: keepalived[5973]: segfault at 0 ip 00007f07bf928e1a sp 00007ffdee5a7548 error 4 in libc-2.23.so[7f07bf7db000+1c0000]
```

```
Starting Keepalived v1.2.24 (10/17,2016)
Opening file '/etc/keepalived/keepalived.conf'.
Starting Healthcheck child process, pid=26
Initializing ipvs
Starting VRRP child process, pid=27
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Opening file '/etc/keepalived/keepalived.conf'.
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Using LinkWatch kernel netlink reflector...
Opening file '/etc/keepalived/keepalived.conf'.
Using LinkWatch kernel netlink reflector...
Opening file '/etc/keepalived/keepalived.conf'.
Got SIGHUP, reloading checker configuration
Initializing ipvs
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Opening file '/etc/keepalived/keepalived.conf'.
Using LinkWatch kernel netlink reflector...
Activating healthchecker for service [172.31.16.135]:80
Activating healthchecker for service [172.31.16.135]:443
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Opening file '/etc/keepalived/keepalived.conf'.
pid 27 exited due to segmentation fault (SIGSEGV).
  Please report a bug at https://github.com/acassen/keepalived/issues
  and include this log from when keepalived started, what happened
  immediately before the crash, and your configuration file.
VRRP child process(27) died: Respawning
```

Configuration generated by the controller:

```
global_defs {
  vrrp_version 3
  vrrp_iptables KUBE-KEEPALIVED-VIP
}

vrrp_instance vips {
  state BACKUP
  interface
  virtual_router_id 50
  priority 99
  nopreempt
  advert_int 1

  track_interface {

  }



  virtual_ipaddress {
    10.129.128.210
  }
}


# Service: default/nginx-ingress
virtual_server 10.129.128.210 80 {
  delay_loop 5
  lvs_sched wlc
  lvs_method NAT
  persistence_timeout 1800
  protocol TCP


  real_server 172.31.16.135 80 {
    weight 1
    TCP_CHECK {
      connect_port 80
      connect_timeout 3
    }
  }

}

# Service: default/nginx-ingress
virtual_server 10.129.128.210 443 {
  delay_loop 5
  lvs_sched wlc
  lvs_method NAT
  persistence_timeout 1800
  protocol TCP


  real_server 172.31.16.135 443 {
    weight 1
    TCP_CHECK {
      connect_port 443
      connect_timeout 3
    }
  }

}
```
",closed,False,2016-11-22 14:31:51,2018-03-01 17:09:51
contrib,saad-ali,https://github.com/kubernetes/contrib/pull/2075,https://api.github.com/repos/kubernetes/contrib/issues/2075,Code freeze everything except 1.4,"This PR is intended to institute a ""complete merge freeze"" as outlined in https://groups.google.com/forum/#!topic/kubernetes-dev/n-vqlX-HHSM

It blocks the Kubernetes submit queue from merging PRs with `v1.5`, `v1.6`, `next-candidate`, and no milestones. `v1.4` will still be allowed to enable merges to the 1.4 branch.

Ok to review. But PR shouldn't be merged until morning of Wed Nov 23 (marking `do-not-merge` until then).

CC @dchen1107

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2075)
<!-- Reviewable:end -->
",closed,True,2016-11-22 22:52:00,2016-11-28 17:55:47
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2076,https://api.github.com/repos/kubernetes/contrib/issues/2076,Approval process,"Implements https://github.com/kubernetes/contrib/issues/1389

Please do not remove the `do-not-merge` label, unless you know what you're doing.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2076)
<!-- Reviewable:end -->
",closed,True,2016-11-22 23:02:37,2016-12-16 19:17:39
contrib,craigbox,https://github.com/kubernetes/contrib/issues/2077,https://api.github.com/repos/kubernetes/contrib/issues/2077,Ingress only supports :80. GCLB supports :80 and :8080.,"[GCLB docs](https://cloud.google.com/compute/docs/load-balancing/http/) says ""HTTP requests can be load balanced based on port 80 or port 8080. HTTPS requests can be load balanced on port 443.""

[Kubernetes source](https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/extensions/v1beta1/types.go) says 
```
 	// 2. The `:` delimiter is not respected because ports are not allowed.
	//	  Currently the port of an Ingress is implicitly :80 for http and
	//	  :443 for https.

```
Ingress can't be configured on port 8080.  (Low priority FR, I'll just change my endpoint, but documenting for search results.)",open,False,2016-11-23 14:31:57,2017-12-19 11:56:45
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2078,https://api.github.com/repos/kubernetes/contrib/issues/2078,Implement batched merging for the submit queue.,"A commandline option specifying a Prow data endpoint is used to collect
results for batch tests. If a batch has passed all necessary tests and
is applicable given the current state of the repo, it will merge all
included PRs at once, without individually testing them.

This batch goroutine runs in parallel with the normal goroutine that
processes PRs sequentially. A mutex guards the critical section for merges
to avoid conflicts.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2078)
<!-- Reviewable:end -->
",closed,True,2016-11-23 21:04:37,2016-11-28 07:14:30
contrib,ctrlaltdel,https://github.com/kubernetes/contrib/pull/2079,https://api.github.com/repos/kubernetes/contrib/issues/2079,Fix a typo,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2079)
<!-- Reviewable:end -->
",closed,True,2016-11-27 18:04:24,2016-12-01 22:23:23
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2080,https://api.github.com/repos/kubernetes/contrib/issues/2080,Deploy kubernetes/kubernetes munger with batched submits.,"This was integration tested with Prow against the kubernetes/pr-bot repo.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2080)
<!-- Reviewable:end -->
",closed,True,2016-11-28 08:49:34,2016-11-28 15:45:04
contrib,saad-ali,https://github.com/kubernetes/contrib/pull/2081,https://api.github.com/repos/kubernetes/contrib/issues/2081,Lift code freeze,"Lift code freeze as outlined in https://groups.google.com/forum/#!topic/kubernetes-dev/n-vqlX-HHSM

CC @foxish

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2081)
<!-- Reviewable:end -->
",closed,True,2016-11-28 17:56:04,2016-11-28 18:22:02
contrib,erictune,https://github.com/kubernetes/contrib/pull/2082,https://api.github.com/repos/kubernetes/contrib/issues/2082,Update peer-finder docs.,"More detail on options.
Rename PetSet to StatefulSet

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2082)
<!-- Reviewable:end -->
",closed,True,2016-11-28 19:35:50,2016-12-02 18:05:08
contrib,dims,https://github.com/kubernetes/contrib/issues/2083,https://api.github.com/repos/kubernetes/contrib/issues/2083,milestones should rank higher in the submit queue,"For example if we have 2 PR(s) with [""P3"",""v1.5""] or [""P2"",""v1.6""]

Currently we are prioritizing the 1.6 above the 1.5... Can we please do the opposite?",closed,False,2016-11-28 19:59:20,2017-01-21 01:19:59
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2084,https://api.github.com/repos/kubernetes/contrib/issues/2084,Rank PRs with milestones with due dates higher than all other PRs.,"Note that this does not affect milestones without due dates, (v1.6), so
normal priority ordering is maintained until we are in the release cycle.

Fixes kubernetes/contrib#2083.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2084)
<!-- Reviewable:end -->
",closed,True,2016-11-28 20:15:49,2017-01-05 20:43:19
contrib,krzyzacy,https://github.com/kubernetes/contrib/pull/2085,https://api.github.com/repos/kubernetes/contrib/issues/2085,Add ci- prefix for gce-enormous jobs in SQ,"https://github.com/kubernetes/test-infra/pull/1165

kubernetes/kubernetes#37156
kubernetes/kubernetes#37157

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2085)
<!-- Reviewable:end -->
",closed,True,2016-11-28 20:16:39,2016-11-29 02:14:42
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2086,https://api.github.com/repos/kubernetes/contrib/issues/2086,Wait longer (2 hours) for jobs to start.,"The wait period should be longer than the job run time, to avoid cycles
of queueing jobs and failing, keeping a FIFO queue always full to the
point that it will never complete.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2086)
<!-- Reviewable:end -->
",closed,True,2016-11-28 20:52:37,2016-11-28 21:24:37
contrib,davidopp,https://github.com/kubernetes/contrib/issues/2087,https://api.github.com/repos/kubernetes/contrib/issues/2087,Create a bot that does server-side PR email filtering,"AKA ""make Github email useful again.""

See proposal here.

https://docs.google.com/a/google.com/document/d/1-w75_yMjRGVcP-nQEZpPar3AV9w8sE1uwp0nVbBWRq0/edit?usp=sharing

@bgrant0607 @lavalamp 
",closed,False,2016-11-28 21:29:19,2018-02-17 01:11:03
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2088,https://api.github.com/repos/kubernetes/contrib/issues/2088,turn off the flaker nagger,"Temporary disable the code that nags owners of test flakes

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2088)
<!-- Reviewable:end -->
",closed,True,2016-11-28 21:55:07,2016-11-28 22:54:39
contrib,fejta,https://github.com/kubernetes/contrib/pull/2089,https://api.github.com/repos/kubernetes/contrib/issues/2089,Update SQ to use ci-kubernetes-build,"Also update ci-kubernetes-cross-build in the bug filing list and sort both lists.

ref https://github.com/kubernetes/test-infra/issues/1021 and https://github.com/kubernetes/test-infra/issues/1109

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2089)
<!-- Reviewable:end -->
",closed,True,2016-11-28 22:36:04,2016-11-28 23:03:09
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2090,https://api.github.com/repos/kubernetes/contrib/issues/2090,Show why reading from a batch URL is failing.,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2090)
<!-- Reviewable:end -->
",closed,True,2016-11-29 03:20:49,2016-11-29 03:22:19
contrib,lvnilesh,https://github.com/kubernetes/contrib/issues/2091,https://api.github.com/repos/kubernetes/contrib/issues/2091,/usr/lib/tmpfiles.d not writable,"three coreos instances accessible via ssh

inventory is like this:

```
[masters]
192.168.122.119 ansible_python_interpreter=/opt/bin/python

[etcd:children]
masters

[nodes]
192.168.122.226 ansible_python_interpreter=/opt/bin/python
192.168.122.231 ansible_python_interpreter=/opt/bin/python
192.168.122.119 ansible_python_interpreter=/opt/bin/python

```

I run ` ./deploy-cluster.sh` and run into this error. 

```
TASK [node : Get Systemd tmpfile from Kubernetes repository] *******************
skipping: [192.168.122.226]
skipping: [192.168.122.231]
fatal: [192.168.122.119]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Destination /usr/lib/tmpfiles.d not writable""}
```",closed,False,2016-11-29 03:35:30,2017-01-03 06:53:51
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2092,https://api.github.com/repos/kubernetes/contrib/issues/2092,Use http instead of https for the batch URL.,"Error reading batch jobs from Prow URL https://prow.k8s.io/data.js: Get
https://prow.k8s.io/data.js: x509: certificate signed by unknown
authority

We'll dig into why this is happening tomorrow.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2092)
<!-- Reviewable:end -->
",closed,True,2016-11-29 03:37:24,2016-11-29 08:26:03
contrib,dims,https://github.com/kubernetes/contrib/pull/2093,https://api.github.com/repos/kubernetes/contrib/issues/2093,Mark kops-aws as nonblocking job,"Build has been broken for 8+ hours now. Let's mark it
as non-blocking to get 1.5 queue unblocked temporarily.

https://k8s-testgrid.appspot.com/google-aws#kops-aws

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/contrib/2093)
<!-- Reviewable:end -->
",closed,True,2016-11-29 11:25:57,2016-11-29 21:31:13
contrib,krzyzacy,https://github.com/kubernetes/contrib/pull/2094,https://api.github.com/repos/kubernetes/contrib/issues/2094,Add ci- prefix for kops-aws jobs,"ref https://github.com/kubernetes/test-infra/pull/1209

Also fix the renamed garbagecollector job

@fejta @zmerlynn 

btw does this configmap change require a deployment of SQ or it's automatic?",closed,True,2016-11-29 20:25:50,2016-11-29 21:34:51
contrib,jrynyt,https://github.com/kubernetes/contrib/issues/2095,https://api.github.com/repos/kubernetes/contrib/issues/2095,GLBC Ingress - allow using existing SSL Certificates,"The GCE Ingress [requires](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/gce#tls) the actual SSL private key and certificate be defined inside of Kubernetes [via secrets], which the controller then applies to the GCE L7 load balancer. In order to protect the private key, we would like to instead refer to an existing certificate in the GCP Project _by name_ in the Ingress definition.

Example configuration:
```
kind: Ingress
spec:
  tls:
    - certName: my-cert
```
Or an annotation like for AWS is fine: http://kubernetes.io/docs/user-guide/services/#ssl-support-on-aws

GCP-equivalent example:
```
gcloud compute ssl-certificates create my-cert \
  --certificate my.crt --private-key my.pem

gcloud compute target-https-proxies create my-proxy \
  --ssl-certificate my-cert --url-map my-map
```

It appears you are already storing certs in the Project and referring to them by name: https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/loadbalancers/loadbalancers.go#L354 -- we would just like to specify the name instead.

This allows the separation of roles: a certificate administrator can upload the key/cert into the GCP Project, where the key is not accessible to users, then a k8s/GKE administrator can use it without needing to have that very valuable secret.

Thanks.

",closed,False,2016-11-29 20:41:50,2016-12-07 19:13:18
contrib,tamalsaha,https://github.com/kubernetes/contrib/issues/2096,https://api.github.com/repos/kubernetes/contrib/issues/2096,GLBC ingress: only handle annotated ingress,"Hi,
We are running a HAProxy based ingress in our clusters. But for a few service, we would like to run GLBC ingress. I did not see any way to tell ingress controllers, which Ingress resource they can handle. Can Ingress controllers can only handle ingress that has a specific annotation applied to it (similar to how schedulers do it):

`""ingress.alpha.kubernetes.io/controller"": glbc`

Thanks.",closed,False,2016-11-29 22:52:29,2016-12-14 21:49:39
contrib,UlaganathanNamachivayam,https://github.com/kubernetes/contrib/issues/2097,https://api.github.com/repos/kubernetes/contrib/issues/2097,Ansible rerun is not picking the latest variable changes.,"Hi Team,
I have created a cluster with flannel subnet range and it got created successfully. Now i want to update flannel with the new subnet range but ansible re-run is not applying the latest change. I can see only the old subnet range still. 

Also i tried using the github-release instead of package manager to update etcd & flanneld. Both are trying to fetch the tar file from github release URL but the same is failing with 404 error. 

Some help/pointers would be much appreciated.
",closed,False,2016-11-30 03:40:13,2018-02-17 04:14:01
contrib,AlexRRR,https://github.com/kubernetes/contrib/issues/2098,https://api.github.com/repos/kubernetes/contrib/issues/2098,[Nginx ingress controller] Downtime of service exposed via Ingress while doing upgrade.,"Test System:
- Baremetal CoreOS
- Network: Flannel
- K8s 1.4.6
- nginx-ingress-controller:0.8.3 

## Steps to reproduce
create a `Deployment` consisting of a simple service like a bare nginx
```yaml
apiVersion: extensions/v1beta1                                                      
kind: Deployment                                                                    
metadata:                                                                           
  name: weather-app                                                                 
spec:                                                                               
  replicas: 3                                                                       
  strategy:                                                                         
    rollingUpdate:                                                                  
      maxSurge: 1                                                                   
      maxUnavailable: 1                                                             
  template:                                                                         
    metadata:                                                                       
      labels:                                                                       
        app: weather-app                                                            
        track: stable                                                               
        attempt: ""1""                                                                
    spec:                                                                           
      containers:                                                                   
      - name: weather-app                                                           
        lifecycle:                                                                  
            preStop:                                                                
                exec:                                                               
                    command: [""/usr/sbin/nginx"",""-s"",""quit""]                        
        image: nginx:latest                                                         
        ports:                                                                      
        - containerPort: 80                                                         
        imagePullPolicy: Always                                                     
        readinessProbe:                                                             
          httpGet:                                                                  
            path: /                                                                 
            port: 80                                                                
          failureThreshold: 3                                                       
          successThreshold: 1                                                       
          periodSeconds: 5                                                          
          timeoutSeconds: 1                                                         
          initialDelaySeconds: 0                                                    
        livenessProbe:                                                              
          httpGet:                                                                  
            path: /                                                                 
            port: 80                                                                
          failureThreshold: 3                                                       
          successThreshold: 1                                                       
          periodSeconds: 10                                                         
          timeoutSeconds: 1                                                         
          initialDelaySeconds: 15 
``` 
Service
```yaml
apiVersion: v1                                                       
kind: Service                                                        
metadata:                                                                            
  labels:                                                            
    app: weather-app                                                 
  name: weather-app                                                  
  namespace: k8-demo                                                 
  clusterIP: 10.3.0.47                                               
  ports:                                                             
  - port: 80                                                         
    protocol: TCP                                                    
    targetPort: 80                                                   
  selector:                                                          
    app: weather-app                                                 
  sessionAffinity: None                                              
  type: ClusterIP                                                    
```

Ingress Resource
```yaml
apiVersion: v1                                                                           
items:                                                                                   
- apiVersion: extensions/v1beta1                                                         
  kind: Ingress                                                                          
  metadata:                                                                                                                                             
    name: weather-demo                                                                   
    namespace: k8-demo                                                                            
  spec:                                                                                  
    rules:                                                                               
    - host: weather-demo.internal.ch                                                
      http:                                                                              
        paths:                                                                           
        - backend:                                                                       
            serviceName: weather-app                                                     
            servicePort: 80                                                              
          path: /                                                                                                                                                                                           
```

Perform update by simply increasing the `attempt`  label, and `kubectl apply -f deploy.yaml`
while in a terminal fetching the NGINX web page every second with `watch -n ""curl http://weather-demo.internal.ch""`

## Expected results

The Deployment is updated with no downtime


## Actual results

- New pods are created
- Old pods are deleted once new ones have `Running` state 
- New pods answer 
- URL returns a *504* Error from NGINX resource controller for a few seconds
- Service reestablished.

If at the same time I expose the service via NodePort and curl there, the service is *always* available.









                                               ",closed,False,2016-11-30 15:26:45,2018-10-16 11:22:44
contrib,erie149,https://github.com/kubernetes/contrib/issues/2099,https://api.github.com/repos/kubernetes/contrib/issues/2099,Nginx Ingress Controller ConfigMap support for header size,I was wondering if we can add large_client_header_buffers and 	client_header_buffer_size to the ConfigMap.  We have a JWT that gets passed with information larger than 8K in some cases,closed,False,2016-11-30 17:52:27,2016-11-30 21:08:10
contrib,zmerlynn,https://github.com/kubernetes/contrib/pull/2100,https://api.github.com/repos/kubernetes/contrib/issues/2100,Add ci-kubernetes-e2e-kops-aws back to blocking jobs,"The job is stable again after kubernetes/kops#1011 & kubernetes/test-infra#1213, and per request, I also fixed logging in kubernetes/kubernetes#37646 before proposing this.

Along the way: Sort the submit queue job list (separate commit)

TBD: It'd be nice if `cmd.Flags().StringSliceVar` were lenient about a trailing comma, but it doesn't look like it is.",closed,True,2016-11-30 19:03:45,2016-11-30 21:54:45
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2101,https://api.github.com/repos/kubernetes/contrib/issues/2101,Do Not Merge docs\design devel or proposal,Those have been moved to the community repository and the changes should be made there instead.,closed,True,2016-11-30 21:11:33,2016-12-05 22:45:48
contrib,ixdy,https://github.com/kubernetes/contrib/issues/2102,https://api.github.com/repos/kubernetes/contrib/issues/2102,SQ should indicate which commit statuses are preventing merge,"It's not always obvious why SQ says ""Github CI tests are not green."" - e.g., it might be because a required status is missing, it might be because SQ is behind on its sync loop, or it might be a bug in SQ.

If we added the list of failing status(es), it would make it easier to understand how to fix a PR.",closed,False,2016-11-30 21:33:52,2017-01-21 01:45:46
contrib,krzyzacy,https://github.com/kubernetes/contrib/pull/2103,https://api.github.com/repos/kubernetes/contrib/issues/2103,add all upgrade-to-1.5 jobs to configmap,"https://github.com/kubernetes/kubernetes/issues/37717

cc @fejta ",closed,True,2016-11-30 21:40:29,2017-02-11 02:11:54
contrib,krzyzacy,https://github.com/kubernetes/contrib/pull/2104,https://api.github.com/repos/kubernetes/contrib/issues/2104,fix the space,"MAKE SQ HAPPY AGAIN!

@rmmh @fejta ",closed,True,2016-12-01 01:28:09,2016-12-01 01:30:46
contrib,apelisse,https://github.com/kubernetes/contrib/issues/2105,https://api.github.com/repos/kubernetes/contrib/issues/2105,"SQ: ""Currently Running"" should show current batch","The submit-queue [shows](http://submit-queue.k8s.io/#/queue) ""Currently running"" PR. But we have currently running PR **AND** currently running batched PRs. Those are not shown. It'd be nice to show them.

@spxtr ",closed,False,2016-12-01 05:32:35,2017-01-11 22:36:40
contrib,piosz,https://github.com/kubernetes/contrib/pull/2106,https://api.github.com/repos/kubernetes/contrib/issues/2106,Removed ci-kubernetes-e2e-kops-aws from blocking suites,"It's failing all the time after it was added in #2100.

cc @zmerlynn @eparis @lavalamp ",closed,True,2016-12-01 10:54:38,2016-12-01 12:10:41
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2107,https://api.github.com/repos/kubernetes/contrib/issues/2107,Submit queue batch fixes,"- Reveal BaseRef so batches can be filtered to just ""master""
- Stop polling for status if the PR is closed underneath us.",closed,True,2016-12-01 19:44:29,2016-12-01 22:01:46
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2108,https://api.github.com/repos/kubernetes/contrib/issues/2108,add stderrthreshold to mungegithub readme,,closed,True,2016-12-01 22:03:49,2016-12-01 22:33:54
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2109,https://api.github.com/repos/kubernetes/contrib/issues/2109,Remove unpublished builds from nonblocking jobs list.,,closed,True,2016-12-01 22:48:10,2016-12-01 23:13:57
contrib,verb,https://github.com/kubernetes/contrib/pull/2110,https://api.github.com/repos/kubernetes/contrib/issues/2110,Fix bug in scale-demo loader port specification,The port argument isn't being dereferenced,closed,True,2016-12-01 23:06:21,2017-05-17 18:06:48
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2111,https://api.github.com/repos/kubernetes/contrib/issues/2111,Show batch status in submit queue,"This shows the currently running batch in the submit queue, and also leaves better comments when batches are merged.",closed,True,2016-12-02 02:16:18,2016-12-02 17:20:26
contrib,gmarek,https://github.com/kubernetes/contrib/pull/2112,https://api.github.com/repos/kubernetes/contrib/issues/2112,Remove kops from sq blockers,Somehow it just breaks after we start blocking on it...,closed,True,2016-12-02 13:25:32,2016-12-02 17:35:11
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2113,https://api.github.com/repos/kubernetes/contrib/issues/2113,Make stale-pending-ci not segfault when cleaning comments while disabled.,,closed,True,2016-12-02 17:42:39,2016-12-02 18:40:31
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2114,https://api.github.com/repos/kubernetes/contrib/issues/2114,Fix check when following merge commits.,The message is actually stored in commit.Commit.Message.,closed,True,2016-12-02 23:55:36,2016-12-03 00:18:45
contrib,krzyzacy,https://github.com/kubernetes/contrib/pull/2115,https://api.github.com/repos/kubernetes/contrib/issues/2115,ci- for all upgrade jobs,"ref https://github.com/kubernetes/test-infra/pull/1263

@fejta",closed,True,2016-12-03 00:40:41,2016-12-05 22:40:32
contrib,krzyzacy,https://github.com/kubernetes/contrib/issues/2116,https://api.github.com/repos/kubernetes/contrib/issues/2116,SQ: add presubmit test for configmap.yaml,"make sure all test entries are valid, no extra spaces, etc.",closed,False,2016-12-03 00:48:06,2017-01-21 01:45:30
contrib,andrewsykim,https://github.com/kubernetes/contrib/pull/2117,https://api.github.com/repos/kubernetes/contrib/issues/2117,Cluster Autoscaler: Support other scale down poilcies,"Seems like cluster autoscaler terminates the first valid candidate which works for most cases. There are situations where we might want to terminate the oldest nodes (like gradually upgrading cluster to new kubernetes version) or the least utilized node (to reduce rescheduling overhead). 

I wrote some preliminary code to get some initial feedback. After approval I will attempt to add some unit/integration/e2e tests for the different scaling policies (possibly in another PR).",closed,True,2016-12-04 04:38:21,2017-11-01 09:11:04
contrib,thebigjc,https://github.com/kubernetes/contrib/pull/2118,https://api.github.com/repos/kubernetes/contrib/issues/2118,Provide multiple strategies for choosing which Node Group to increase,"As per #1921, the current cluster-autoscaler chooses a node group to increase at random among the groups that can fit at least one pod. This expands this to two additional strategies:

- Most Pods - choose the node group that can schedule the most pods, usually due to predicate rules
- Least Waste - choose the node group that has the least remaining CPU resources after a scale up, and if that is tied, the least remaining Memory

In the event of a tie, both of these strategies fall back to the current Random method.

This probably needs test coverage, to move to its own package, and there could be some weirdness in the algorithms as it needs manual testing.

Looking mostly to see if I'm on the right track and will proceed after.

",closed,True,2016-12-04 14:50:19,2016-12-19 19:46:47
contrib,brancz,https://github.com/kubernetes/contrib/pull/2119,https://api.github.com/repos/kubernetes/contrib/issues/2119,prometheus: update choices,"The previously described setup only does declarative blackbox
monitoring, which is a very small subset of what Prometheus is able to
do. Furthermore it is incorrect as Prometheus expects everything to be
whitebox monitored as `instance` labels are otherwise incorrectly set,
and therefore results in an ambiguous source of a metric.

Prometheus has a built-in Kubernetes Service Discovery mechanism which
can be used to discover targets to perform whitebox monitoring. However,
even though all targets can be discovered with that mechanism, it is not
a one-size-fits-all solution and thus still requires deep knowledge of
the way Prometheus is configured.

The Prometheus Operator manages Prometheus instances and integrates its
configuration natively into Kubernetes using third party resources.

The Prometheus Helm Chart provides another alternative.

@fabxc @matthiasr ",closed,True,2016-12-05 11:04:32,2016-12-28 18:02:57
contrib,bjornl,https://github.com/kubernetes/contrib/issues/2120,https://api.github.com/repos/kubernetes/contrib/issues/2120,Ansible setup broken on Fedora 25,"Red Hat Fedora project have changed the name of configuration parameters for Flannel in release 25. They are now FLANNEL_ETCD_ENDPOINTS and FLANNEL_ETCD_PREFIX instead of the previous FLANNEL_ETCD and FLANNEL_ETCD_KEY.

Then performing the setup on Fedora 25 the flanneld.service from the OS package is used and the /etc/sysconfig/flanneld from playbook. So flannel service will fail to start due to not being able to expand the configuration parameters for Etcd.

I believe the solution is to update the files in ansible/ to use the new Fedora parameters.",closed,False,2016-12-05 14:30:48,2016-12-06 13:50:46
contrib,eparis,https://github.com/kubernetes/contrib/pull/2121,https://api.github.com/repos/kubernetes/contrib/issues/2121,Sync what the bot thinks are required status with github,"This will sync required contexts, retest required contexts, and extra
contexts (possibly used for things like the cla context) between the bot
and github automatically.",closed,True,2016-12-05 16:55:07,2016-12-06 01:33:25
contrib,eparis,https://github.com/kubernetes/contrib/pull/2122,https://api.github.com/repos/kubernetes/contrib/issues/2122,Relax OWNERSHIP rules if none found,"It was possible that a PR changing files whose only ""leaf"" owner was the
PR author would result in 1 OWNER. The PR author. That OWNER would be
eliminated. Thus the PR would get no owner.

The fix is to look for ALL owners of files if we can't find any.",closed,True,2016-12-05 16:55:44,2016-12-08 23:03:32
contrib,eparis,https://github.com/kubernetes/contrib/pull/2123,https://api.github.com/repos/kubernetes/contrib/issues/2123,Specific ci failure status,,closed,True,2016-12-05 16:56:58,2016-12-05 20:23:45
contrib,bjornl,https://github.com/kubernetes/contrib/issues/2124,https://api.github.com/repos/kubernetes/contrib/issues/2124,Ansible setup using deprecated and unavailable images,"Ansible dns manifest references skydns image that is deprecated and not available to pull from Google Container Registry any more.

Error syncing pod, skipping: failed to ""StartContainer"" for ""skydns"" with ErrImagePull: ""image pull failed for gcr.io/google_containers/skydns:2015-10-13-8c72f8c, this may be because there are no credentials on this request.  details: (manifest unknown: Failed to fetch \""2015-10-13-8c72f8c\"" from request \""/v2/google_containers/skydns/manifests/2015-10-13-8c72f8c\"".)""

https://gcr.io/google_containers/skydns:2015-10-13-8c72f8c
",closed,False,2016-12-05 17:25:10,2017-05-26 08:57:54
contrib,smenon78,https://github.com/kubernetes/contrib/issues/2125,https://api.github.com/repos/kubernetes/contrib/issues/2125,Ansible aws ubuntu instance Deploy etcd fails,"I have a Ubuntu EC2 instance and it fails 

PLAY [Deploy etcd] *************************************************************

18:03:20 
18:03:20 TASK [setup] *******************************************************************
18:03:21 fatal: [35.164.0.226]: FAILED! => {""changed"": false, ""failed"": true, ""module_stderr"": ""/bin/sh: 1: /usr/bin/python: not found\n"", ""module_stdout"": """", ""msg"": ""MODULE FAILURE"", ""parsed"": false}",closed,False,2016-12-05 18:16:25,2016-12-13 14:02:24
contrib,eparis,https://github.com/kubernetes/contrib/pull/2126,https://api.github.com/repos/kubernetes/contrib/issues/2126,Delete cached github responses we know are bad,"Github seems to screw up their conditional responses when the last entry
is exactly on a page size boundary. Today, when we get the last page
which is exactly full we try the next page anyway even though github
says there isn't a next page.

This patch attempts to delete our cached entry when we find that case.",closed,True,2016-12-05 20:01:01,2016-12-05 21:43:50
contrib,eparis,https://github.com/kubernetes/contrib/pull/2127,https://api.github.com/repos/kubernetes/contrib/issues/2127,Fix accidental test limit merged when using make test,"I accidentally merged a patch which restricted which tests were run by
make test. Run them all.",closed,True,2016-12-05 20:45:34,2016-12-05 21:13:48
contrib,eparis,https://github.com/kubernetes/contrib/pull/2128,https://api.github.com/repos/kubernetes/contrib/issues/2128,Keep looking up the tree if we found an empty set,"If we find an OWNERS file with only reviewers, but no assignee or
approvers we will insert an empty set into the assignees. If we find
that empty set later we should ignore it and look for the closest
assignable person.",closed,True,2016-12-06 00:40:54,2016-12-06 02:13:29
contrib,foxish,https://github.com/kubernetes/contrib/issues/2129,https://api.github.com/repos/kubernetes/contrib/issues/2129,Turn on 2FA for k8s-merge-robot,k8s-merge-robot needs to be an admin for https://github.com/kubernetes/contrib/pull/2121 to work correctly.,closed,False,2016-12-06 00:50:29,2016-12-06 01:26:56
contrib,containscafeine,https://github.com/kubernetes/contrib/pull/2130,https://api.github.com/repos/kubernetes/contrib/issues/2130,fix spelling for the word 'everything' in ingress controller example,Fix spelling of the word *everything*,closed,True,2016-12-06 07:35:51,2016-12-06 07:37:23
contrib,spxtr,https://github.com/kubernetes/contrib/issues/2131,https://api.github.com/repos/kubernetes/contrib/issues/2131,Batch results should use https.,"From https://github.com/kubernetes/contrib/blob/master/mungegithub/submit-queue/deployment/kubernetes/configmap.yaml#L176
```
  submit-queue.batch-url: http://prow.k8s.io/data.js
```

This should be https. Something strange happened last time we tried this.",closed,False,2016-12-06 21:44:47,2017-01-21 01:15:12
contrib,bowei,https://github.com/kubernetes/contrib/pull/2132,https://api.github.com/repos/kubernetes/contrib/issues/2132,Adds --probe to export health checks and latency metrics for dns servers,"`--probe <label>,<server>,<dns name>,<interval>` will query the given
DNS server for `<dns name>` every `<interval>` seconds. The results will
be posted to a health check URL and DNS latencies and errors reported
via the /metrics URL.

Multple `--probe`s may be specified.

This replaces the functionality of the exec-healthz for the DNS pod.",closed,True,2016-12-07 22:55:48,2016-12-13 05:18:59
contrib,gregbkr,https://github.com/kubernetes/contrib/issues/2133,https://api.github.com/repos/kubernetes/contrib/issues/2133,service-loadbalancer : udp support,"Hello,

I am using rc.yaml to redirect tcp and udp query to ELK.
tpc seems fine: I can access kibana and send logs via tcp
udp doesn't foward. I can send logs ok in the container, or to the svc_ip. But not throught the loadbalancer.

Which arg should I use in the lb?
      args:
        - --udp-services=logstash:5000 ?

I tried but not luck:
        args:
        - --tcp-services=kibana:5601,logstash:5001,logstash:5000 


I test with:
```
kubectl exec logstash -- bash -c ""echo test | socat -t 0 - UDP:10.0.252.166:5000""   <-- svc_ip : ok
kubectl exec logstash -- bash -c ""echo test | socat -t 0 - TPC:10.0.252.166:5001""   <-- svc_ip: ok

echo testudp | socat -t 0 - UDP:185.19.30.194:5000   <-- nok
echo testtcp | socat -t 0 - TCP:185.19.30.194:5001    <-- ok
```

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: service-loadbalancer
  labels:
    app: service-loadbalancer
    version: v1
spec:
  replicas: 1
  selector:
    app: service-loadbalancer
    version: v1
  template:
    metadata:
      labels:
        app: service-loadbalancer
        version: v1
    spec:
      nodeSelector:
        role: loadbalancer
      containers:
      - image: gcr.io/google_containers/servicelb:0.4
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        name: haproxy
        ports:
        ## All http services
        # kibana
        - containerPort: 5601
          hostPort: 5601
          protocol: TCP
        # logstash UDP (syslog)
        - containerPort: 5000
          hostPort: 5000
          protocol: UDP
        # logstash TCP (filebeat)
        - containerPort: 5001
          hostPort: 5001
          protocol: TCP
        resources: {}
        args:
        - --tcp-services=kibana:5601,logstash:5001
        - --udp-services=logstash:5000    #<-- I guessed here, I couldn't find any arg reference....
```

Thank you for your help!
Greg.",closed,False,2016-12-08 18:04:33,2018-02-17 07:17:01
contrib,kow3ns,https://github.com/kubernetes/contrib/pull/2134,https://api.github.com/repos/kubernetes/contrib/issues/2134,Initial  commit of zookeeper example and statefulsets dir,,closed,True,2016-12-08 22:19:23,2016-12-14 22:07:23
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/2135,https://api.github.com/repos/kubernetes/contrib/issues/2135,[Nginx ingress controller] what use of hostPort in the nginx ingress yaml file?,"I thought the hostPort should be the listening port of server context in the generated nginx.conf. But according to the nginx.tmpl, the listening port is harden-coded to 80 and 443. I change the hostPort for containerPort 80 and 443 to some specific ports. But I can't find the port in the netstat output.

So, what usage of hostPort for the nginx ingress controller?",closed,False,2016-12-09 07:24:44,2018-02-17 12:22:02
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2136,https://api.github.com/repos/kubernetes/contrib/issues/2136,Fix panic in release-note-label when Issue.Body is empty.,,closed,True,2016-12-09 11:26:50,2016-12-09 11:26:55
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2137,https://api.github.com/repos/kubernetes/contrib/issues/2137,Cluster-Autoscaler: add node drain logic,"Currently a node is removed without any notice and pods  that live on the node killed without graceful termination. This PR adds drain logic to scale down.

cc: @fgrzadkowski @piosz @jszczepkowski ",closed,True,2016-12-09 15:29:52,2016-12-16 14:47:34
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2138,https://api.github.com/repos/kubernetes/contrib/issues/2138,Make release-note-label work with newlines and ```release-note format.,"There's an even split (109 ""Release note**"", 113 ""```release-note"") between the two styles of release note marking, so we might as well support both.

It didn't work with newlines at all before, so it was messing up in almost every case.",closed,True,2016-12-09 23:31:49,2016-12-10 00:04:22
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/2139,https://api.github.com/repos/kubernetes/contrib/issues/2139,Nginx Ingress controller crash issue (golang panic),"Hi,

In one of my colleague test K8s env, the nginx ingress controller can't start. The trace is as follow:
```
[root@petercentos7 output]# kubectl logs nginx-ingress-controller-26hys
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x40922c]

goroutine 101 [running]:
panic(0x19a2800, 0xc420014030)
               /usr/local/go/src/runtime/panic.go:500 +0x1a1
main.(*loadBalancerController).getUpstreamServers(0xc420196a00, 0x1b6f3ff, 0x2, 0x1b8bd89, 0x1b, 0x1000101, 0x1b6f77a, 0x3, 0x0, 0x1b725c8, ...)
               /home/itsma/service-lb/nginx-ingress-controller/src/k8s.io/contrib/ingress/controllers/nginx/controller.go:746 +0x1cdc
main.(*loadBalancerController).sync(0xc420196a00, 0xc420012d60, 0x16, 0xc420153ea0, 0xc420153e00)
               /home/itsma/service-lb/nginx-ingress-controller/src/k8s.io/contrib/ingress/controllers/nginx/controller.go:466 +0x176
main.(*loadBalancerController).(main.sync)-fm(0xc420012d60, 0x16, 0xc420153ea0, 0xc42004fe00)
               /home/itsma/service-lb/nginx-ingress-controller/src/k8s.io/contrib/ingress/controllers/nginx/controller.go:161 +0x3e
main.(*taskQueue).worker(0xc4203a26a0)
               /home/itsma/service-lb/nginx-ingress-controller/src/k8s.io/contrib/ingress/controllers/nginx/utils.go:90 +0x152
main.(*taskQueue).(main.worker)-fm()
               /home/itsma/service-lb/nginx-ingress-controller/src/k8s.io/contrib/ingress/controllers/nginx/utils.go:64 +0x2a
k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/wait.JitterUntil.func1(0xc42081ff48)
               /home/itsma/service-lb/nginx-ingress-controller/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/wait/wait.go:84 +0x19
k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/wait.JitterUntil(0xc42081ff48, 0x3b9aca00, 0x0, 0x1, 0xc4203c5da0)
               /home/itsma/service-lb/nginx-ingress-controller/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/wait/wait.go:85 +0xad
k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/wait.Until(0xc42081ff48, 0x3b9aca00, 0xc4203c5da0)
               /home/itsma/service-lb/nginx-ingress-controller/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/util/wait/wait.go:47 +0x4d
main.(*taskQueue).run(0xc4203a26a0, 0x3b9aca00, 0xc4203c5da0)
               /home/itsma/service-lb/nginx-ingress-controller/src/k8s.io/contrib/ingress/controllers/nginx/utils.go:64 +0x55
created by main.(*loadBalancerController).Run
               /home/itsma/service-lb/nginx-ingress-controller/src/k8s.io/contrib/ingress/controllers/nginx/controller.go:1231 +0x1b6
```
@aledbf , the crash happens at the line 746 of controller.go in your commit [761ab92c081fc64a4f78](https://github.com/kubernetes/contrib/commit/761ab92c081fc64a4f789c424ad63723f6d0c2b0) Add support for default backend in Ingress rule.  It seems that at least one of ing.GetNamespace(), ing.Spec.Backend.ServiceName, ing.Spec.Backend.ServicePort is null so that the crash happends. But I check the ingress yaml in our env, they are all set already.

Thanks
Liu Peng",closed,False,2016-12-12 05:07:59,2018-01-07 02:41:40
contrib,gmarek,https://github.com/kubernetes/contrib/pull/2140,https://api.github.com/repos/kubernetes/contrib/issues/2140,Fix compare tool,,closed,True,2016-12-12 10:01:03,2016-12-12 12:37:38
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2141,https://api.github.com/repos/kubernetes/contrib/issues/2141,Handle release notes when the commented instructions are present.,,closed,True,2016-12-12 21:34:28,2016-12-13 01:17:43
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2142,https://api.github.com/repos/kubernetes/contrib/issues/2142,Don't open flakes for run failures without specific errors.,,closed,True,2016-12-13 01:14:56,2016-12-13 01:47:46
contrib,iamnewspecies,https://github.com/kubernetes/contrib/issues/2143,https://api.github.com/repos/kubernetes/contrib/issues/2143,Sample still not working ,"I followed the instructions on how to configure config-map for TCP routing. But I am still not successful!

[https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/tcp](instructions)

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: tcp-configmap-example
data:
  2776: ""default/example-go:8080""
```

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-ingress-controller
  labels:
    k8s-app: nginx-ingress-lb
spec:
  replicas: 1
  selector:
    k8s-app: nginx-ingress-lb
  template:
    metadata:
      labels:
        k8s-app: nginx-ingress-lb
        name: nginx-ingress-lb
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3
        name: nginx-ingress-lb
        imagePullPolicy: Always
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 1
        # use downward API
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        ports:
        - containerPort: 80
          hostPort: 80
        - containerPort: 443
          hostPort: 443
        # service echoheaders as TCP service default/echoheaders:9000
        # 9000 indicates the port used to expose the service
        - containerPort: 2776
          hostPort: 2776
        args:
        - /nginx-ingress-controller
        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-configmap-example
```

Following is the ouput of **kubectl describe svc kubernetes**:

Name:           kubernetes
Namespace:      default
Labels:         component=apiserver
            provider=kubernetes
Selector:       <none>
Type:           NodePort
IP:         10.0.0.1
Port:           https   443/TCP
NodePort:       https   31261/TCP
Endpoints:      10.0.2.15:443
Port:           telco-smpp  2776/TCP
NodePort:       telco-smpp  31483/TCP
Endpoints:      10.0.2.15:2776
Port:           telco-https 8444/TCP
NodePort:       telco-https 31225/TCP
Endpoints:      10.0.2.15:8444
Session Affinity:   ClientIP

Following is the ouput of **kubectl describe endpoints**:

Name:       kubernetes
Namespace:  default
Labels:     <none>
Subsets:
  Addresses:        10.0.2.15
  NotReadyAddresses:    <none>
  Ports:
    Name    Port    Protocol
    ----    ----    --------
    https   443 TCP
    telco-https 8444    TCP
    telco-smpp  2776    TCP


What am I doing wrong?",closed,False,2016-12-13 01:53:21,2018-02-17 09:19:00
contrib,srihakum,https://github.com/kubernetes/contrib/issues/2144,https://api.github.com/repos/kubernetes/contrib/issues/2144,nginx-ingress-conroller: Liness/Readiness probe - connection refused,"Trying to run nginx-ingress-controller on by kubernetes cluster with version:
```
root@k8smaster:/k8s/common# kubectl version
Client Version: version.Info{Major:"""", Minor:"""", GitVersion:""pr32107"", GitCommit:""9dfe8df7cd1765b06108e4c996ecfdfc3113e3cf"", GitTreeState:""clean"", BuildDate:""2016-09-08T21:34:39Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:16:27Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
root@k8smaster:/k8s/common# 
```

default-http-backend is running:
```
root@k8smaster:/k8s/common# kubectl describe pod default-http-backend-3vrpw
Name:		default-http-backend-3vrpw
Namespace:	default
Node:		5.10.105.214/5.10.105.214
Start Time:	Tue, 13 Dec 2016 08:55:20 +0000
Labels:		app=default-http-backend
Status:		Running
IP:		172.16.208.82
Controllers:	ReplicationController/default-http-backend
Containers:
  default-http-backend:
    Container ID:	docker://0eaa9c7e36909c2dcb5a33cf1cb3501e6e7f4c1e32f01e07e522f893e6282135
    Image:		gcr.io/google_containers/defaultbackend:1.0
    Image ID:		docker://sha256:279ab580ac70642e42b993224984632bd81b5e4d09b951dd5dd62003abcd290a
    Port:		8080/TCP
    Limits:
      cpu:	10m
      memory:	20Mi
    Requests:
      cpu:		10m
      memory:		20Mi
    State:		Running
      Started:		Tue, 13 Dec 2016 08:55:27 +0000
    Ready:		True
    Restart Count:	0
    Liveness:		http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hxiuy (ro)
    Environment Variables:	<none>
Conditions:
  Type		Status
  Initialized 	True 
  Ready 	True 
  PodScheduled 	True 
Volumes:
  default-token-hxiuy:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-hxiuy
QoS Class:	Guaranteed
Tolerations:	<none>
No events.
```

When I try to start the nginx-ingress-controller I see liveness and readiness probe if failing with connection refused. I was able to get this working my minikube however when I try on cloud vm I see this issue. Any help here 
```
root@k8smaster:/k8s/common# kubectl describe pod nginx-ingress-controller-ugki0
Name:		nginx-ingress-controller-ugki0
Namespace:	default
Node:		5.10.105.210/5.10.105.210
Start Time:	Tue, 13 Dec 2016 12:45:40 +0000
Labels:		k8s-app=nginx-ingress-lb
		name=nginx-ingress-lb
Status:		Running
IP:		172.16.253.190
Controllers:	ReplicationController/nginx-ingress-controller
Containers:
  nginx-ingress-lb:
    Container ID:	docker://2eab659b7404ff3256ed14a9d2d6a44e85e17e384ae471709742c3cb8216d51a
    Image:		gcr.io/google_containers/nginx-ingress-controller:0.8.3
    Image ID:		docker://sha256:3f8119a98476b97e806057a72e8e836a2d3e65d56e9b4c790e75874e586b8db0
    Ports:		80/TCP, 443/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=$(POD_NAMESPACE)/default-http-backend
    State:		Running
      Started:		Tue, 13 Dec 2016 12:46:15 +0000
    Last State:		Terminated
      Reason:		Error
      Exit Code:	255
      Started:		Tue, 13 Dec 2016 12:45:43 +0000
      Finished:		Tue, 13 Dec 2016 12:46:13 +0000
    Ready:		False
    Restart Count:	1
    Liveness:		http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:		http-get http://:10254/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hxiuy (ro)
    Environment Variables:
      POD_NAME:		nginx-ingress-controller-ugki0 (v1:metadata.name)
      POD_NAMESPACE:	default (v1:metadata.namespace)
Conditions:
  Type		Status
  Initialized 	True 
  Ready 	False 
  PodScheduled 	True 
Volumes:
  default-token-hxiuy:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-hxiuy
QoS Class:	BestEffort
Tolerations:	<none>
Events:
  FirstSeen	LastSeen	Count	From			SubobjectPath				Type		Reason		Message
  ---------	--------	-----	----			-------------				--------	------		-------
  41s		41s		1	{default-scheduler }						Normal		Scheduled	Successfully assigned nginx-ingress-controller-ugki0 to 5.10.105.210
  39s		39s		1	{kubelet 5.10.105.210}	spec.containers{nginx-ingress-lb}	Normal		Created		Created container with docker id 43afc6367e05; Security:[seccomp=unconfined]
  38s		38s		1	{kubelet 5.10.105.210}	spec.containers{nginx-ingress-lb}	Normal		Started		Started container with docker id 43afc6367e05
  21s		11s		2	{kubelet 5.10.105.210}	spec.containers{nginx-ingress-lb}	Warning		Unhealthy	Liveness probe failed: Get http://172.16.253.190:10254/healthz: dial tcp 172.16.253.190:10254: getsockopt: connection refused
  39s		7s		2	{kubelet 5.10.105.210}	spec.containers{nginx-ingress-lb}	Normal		Pulling		pulling image ""gcr.io/google_containers/nginx-ingress-controller:0.8.3""
  39s		7s		2	{kubelet 5.10.105.210}	spec.containers{nginx-ingress-lb}	Normal		Pulled		Successfully pulled image ""gcr.io/google_containers/nginx-ingress-controller:0.8.3""
  6s		6s		1	{kubelet 5.10.105.210}	spec.containers{nginx-ingress-lb}	Normal		Created		Created container with docker id 2eab659b7404; Security:[seccomp=unconfined]
  6s		6s		1	{kubelet 5.10.105.210}	spec.containers{nginx-ingress-lb}	Normal		Started		Started container with docker id 2eab659b7404
  37s		1s		5	{kubelet 5.10.105.210}	spec.containers{nginx-ingress-lb}	Warning		Unhealthy	Readiness probe failed: Get http://172.16.253.190:10254/healthz: dial tcp 172.16.253.190:10254: getsockopt: connection refused
```",closed,False,2016-12-13 13:08:06,2018-02-17 21:31:00
contrib,uthark,https://github.com/kubernetes/contrib/issues/2145,https://api.github.com/repos/kubernetes/contrib/issues/2145,Cluster AutoScaler - Azure support,"Hi all,

I'm working on azure support in cluster-autoscaler and main functionality is ready.

Code needs some polishing, though, I'm working on it.

Also, I'd like to get feedback for the code.",closed,False,2016-12-13 16:25:16,2017-01-20 19:24:21
contrib,uthark,https://github.com/kubernetes/contrib/pull/2146,https://api.github.com/repos/kubernetes/contrib/issues/2146,Support for Azure in cluster-autoscaler,See issue #2145 ,closed,True,2016-12-13 16:26:43,2017-01-20 19:19:12
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2147,https://api.github.com/repos/kubernetes/contrib/issues/2147,Cluster-autoscaler: update Kubernetes in CA to 1.5,"The PR doesn't contain any new feature code. It should be mostly refactoring.

cc: @fgrzadkowski @piosz @jszczepkowski ",closed,True,2016-12-13 16:28:00,2016-12-14 12:34:32
contrib,TWood67,https://github.com/kubernetes/contrib/issues/2148,https://api.github.com/repos/kubernetes/contrib/issues/2148,nginx-ingress-controller: Route By Query String ,"I'm looking for the ability to route traffic via query string. We have a bucketed system that supports session persistence with either a query string or cookie. We are currently able to achieve this with HAProxy and ACL url_param. The idea is that I specify a field and a value in my ingress resource and any requests with the matching field and value are routed to the specified service.
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  rules:
  - host: foo.bar.com
    http:
      fields:
      - <fieldKey>: <fieldValue>
        backend:
          serviceName: s1
          servicePort: 80
```
Is this something nginx can support? I would be more than happy to work on a PR for this change if so. ",closed,False,2016-12-13 17:28:30,2018-02-17 09:19:01
contrib,mml,https://github.com/kubernetes/contrib/issues/2149,https://api.github.com/repos/kubernetes/contrib/issues/2149,merge bot spam,"For flakes that happen a lot or are very old, the merge bot adds so many updates that it is impossible to see what the humans are doing.

Would it be possible for the merge bot to add only a single comment to each issue and then edit that comment to include new information?  This would let that comment get quite large, but it would be easier to scroll past it, and a human could easily edit it to reduce its size.  Right now, any attempt to groom an issue is out of the question.

A mild example: kubernetes/kubernetes#33380",closed,False,2016-12-13 22:12:29,2017-01-26 22:53:31
contrib,ppwfx,https://github.com/kubernetes/contrib/issues/2150,https://api.github.com/repos/kubernetes/contrib/issues/2150,[Nginx ingress controller] OpenResty support,"Hey,

we would like to provide OpenResty / Custom Lua stuff support. Maybe it's already working and if it doesn't but we may thinking of providing an additional Dockerfile.

If you would require an additional Dockerfile, would you merge it or is it too specific?

Best regards",closed,False,2016-12-14 12:42:09,2016-12-14 13:46:20
contrib,ppwfx,https://github.com/kubernetes/contrib/issues/2151,https://api.github.com/repos/kubernetes/contrib/issues/2151,[Nginx ingress controller] Custom reloading command,"Hey,

we would like to run the ingress controller in a dedicated container that triggers a config reload within an nginx container. To do so, we will need to call a custom command for this cross container operation (sth different than ""nginx reload"") 

We are thinking of adding the functionality to inject a custom command via a config flag. 

Is this feature a conceptual fit? Would you merge it?

Best regards",closed,False,2016-12-14 12:47:36,2016-12-14 13:41:38
contrib,ppwfx,https://github.com/kubernetes/contrib/issues/2152,https://api.github.com/repos/kubernetes/contrib/issues/2152,[Nginx ingress controller] Proposal: minimize loadbalancer downtime,"Hey,

we really like your nginx ingress controller implementation. Unfortunately, it doesn't support our use case to the extend we would wish to, but we would be very happy to contribute to this project with some generic features.

Our vision is to have an ingress controller that feeds a loadbalancer continuously with hosts, endpoints and paths with as little downtime as possible (e.g. having to reload the configration, drain connection pools etc).

We already built an OpenResty based loadbalancer that spawns a http endpoint that accepts service pod's endpoints and adds them to the service's upstream pool without any downtime. We did not find a solution to change hosts and paths definitions in memory yet, so we would like to go the classy way, by simply rendering the configs and reloading nginx. 
We also strongly believe in the one container per process approach and would like to run the ingress controller within a dedicated container in the long term.

To support our use case and keep the nginx ingress controller flexible we would like to provide some features like:
- provide a way to react to ingress events in a more granular way like, e.g. whenever a new host or path is added config shoud be rendered and a command should be trigger and whenever a service's endpoints change a http request is send
- being able to inject a custom command that will be trigger whenever the configuration will be rendered
- run the ingress controller in a dedicated container (low short term priority, nice to have in the long term)

We would be very happy to hear your opinion on our thoughts and ideas how this can be realized with the greatest benefit for everybody :)

Best regards",closed,False,2016-12-14 13:38:28,2017-06-01 09:36:52
contrib,soudy,https://github.com/kubernetes/contrib/issues/2153,https://api.github.com/repos/kubernetes/contrib/issues/2153,[ingress] Multi TLS working incorrectly,"I was trying to achieve SNI using the example described [here](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/multi-tls). However, with 2 domains, only one of them worked, because all domains were using the same certificate. The ingress config I used looks like this:

```yaml
kind: Ingress
apiVersion: extensions/v1beta1
metadata:
  name: frontend-proxy
spec:
  tls:
  - hosts:
    - foo.com
    secretName: foo.com
  - hosts:
    - foo.nl
    secretName: foo.nl

  rules:
  - host: foo.com
    http:
      paths:
      - backend:
          serviceName: frontend-proxy
          servicePort: 80

  - host: foo.nl
    http:
      paths:
      - backend:
          serviceName: frontend-proxy
          servicePort: 80
```

This resulted in both `foo.com` and `foo.nl` using the `foo.com` certificate. Is there anything I'm doing wrong that I'm missing, or is this a bug? ",closed,False,2016-12-14 15:00:13,2018-03-01 09:01:53
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2154,https://api.github.com/repos/kubernetes/contrib/issues/2154,Handle yet another release-note case.,It mislabeled kubernetes/kubernetes#38672,closed,True,2016-12-14 22:51:36,2016-12-15 23:57:29
contrib,hiroshi,https://github.com/kubernetes/contrib/issues/2155,https://api.github.com/repos/kubernetes/contrib/issues/2155,Cannot see logs from fluentd-sidecar-gcp in stackdriver logging,"- Google Container Engine (master: 1.4.6 node 1.4.6).
- Cloud API access scopes:
  - Stackdriver Logging API: Write Only  (edit)

I followed
https://github.com/kubernetes/contrib/blob/master/logging/fluentd-sidecar-gcp/README.md
However cannot see logs in stackdriver logging.

I confirmed the log is generated in the `synthetic-logger` container and can see in the `sidecar-log-collector` container.
```
$ kubectl exec logging-sidecar-example -c sidecar-log-collector tail -- -f /mnt/log/synthetic-count.log
logging-sidecar-example: 918 
logging-sidecar-example: 919 
logging-sidecar-example: 920 
logging-sidecar-example: 921 
...
```

Regular stdouts of containers can see in stackdriver logging.
",closed,False,2016-12-15 06:42:59,2018-02-17 10:20:00
contrib,hiroshi,https://github.com/kubernetes/contrib/pull/2156,https://api.github.com/repos/kubernetes/contrib/issues/2156,Fix fluentd-sidecar-gcp yaml file name in README,,closed,True,2016-12-15 06:52:23,2016-12-20 22:43:46
contrib,bjornl,https://github.com/kubernetes/contrib/issues/2157,https://api.github.com/repos/kubernetes/contrib/issues/2157,Ansible setup fails on master nodes with multiple interfaces,"Setup fails then setting up a small three node cluster with two masters and one slave node, I get this error regarding the second physical interface.

All nodes have two physical interfaces, the first one is on the server network which is the one the hostname and name in DNS is bound to and is the one Etcd should use. The secondary interfaces are on DMZ network exposing the cluster to the public internet.

All nodes are Fedora 25 Server with latest updates (as of today).

TASK [etcd : Write etcd config file] *******************************************
fatal: [kube1]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""AnsibleUndefinedVariable: 'dict object' has no attribute u'ansible_enp7s0f1'""}
fatal: [kube2]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""AnsibleUndefinedVariable: 'dict object' has no attribute u'ansible_ens33'""}
        to retry, use: --limit @/root/contrib/ansible/playbooks/deploy-cluster.retry

PLAY RECAP *********************************************************************
kube1                      : ok=20   changed=4    unreachable=0    failed=1   
kube2                      : ok=20   changed=4    unreachable=0    failed=1   
kube3                      : ok=5    changed=3    unreachable=0    failed=0  ",closed,False,2016-12-15 15:22:24,2016-12-15 17:15:59
contrib,kow3ns,https://github.com/kubernetes/contrib/pull/2158,https://api.github.com/repos/kubernetes/contrib/issues/2158,zk updates for helm integration,"Updates the configuration script to work off of the number of replicas instead of the ensemble membership 
Updates version to v2",closed,True,2016-12-16 00:27:48,2016-12-19 23:55:44
contrib,mohammedzee1000,https://github.com/kubernetes/contrib/issues/2159,https://api.github.com/repos/kubernetes/contrib/issues/2159,Fluentd setup broken in ansible setup.,"**Kubectl version**: 
Client Version: version.Info{Major:""1"", Minor:""2"", GitVersion:""v1.2.0"", GitCommit:""5e723f67f1e36d387a8a7faa6aa8a7f40cc9ca46"", GitTreeState:""clean""}
Server Version: version.Info{Major:""1"", Minor:""2"", GitVersion:""v1.2.0"", GitCommit:""5e723f67f1e36d387a8a7faa6aa8a7f40cc9ca46"", GitTreeState:""clean""}

**Environment**:
 - **Hardware configuration**: Lenevo Thinkpad T450s
- **OS** (e.g. from /etc/os-release): 
Only machines running the kubernetes (vagrant kvm vms)
NAME=Fedora
VERSION=""24 (Cloud Edition)""
ID=fedora
VERSION_ID=24
PRETTY_NAME=""Fedora 24 (Cloud Edition)""
ANSI_COLOR=""0;34""
CPE_NAME=""cpe:/o:fedoraproject:fedora:24""
HOME_URL=""https://fedoraproject.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_BUGZILLA_PRODUCT=""Fedora""
REDHAT_BUGZILLA_PRODUCT_VERSION=24
REDHAT_SUPPORT_PRODUCT=""Fedora""
REDHAT_SUPPORT_PRODUCT_VERSION=24
PRIVACY_POLICY_URL=https://fedoraproject.org/wiki/Legal:PrivacyPolicy
VARIANT=""Cloud Edition""
VARIANT_ID=cloud

- **Kernel** (e.g. `uname -a`): 

Linux localhost 4.5.5-300.fc24.x86_64 #1 SMP Thu May 19 13:05:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

- **Install tools**:
 dnf, ansible
ansible rpm version : 2.0.2.0-1.fc24

**What happened**:

```
...
TASK [node : Setting the kubelet_modified fact to true] ************************
ok: [192.168.121.128]
ok: [192.168.121.138]

TASK [node : Install fluentd pod into each node] *******************************
fatal: [192.168.121.128]: FAILED! => {""changed"": false, ""dest"": ""/etc/kubernetes/manifests"", ""failed"": true, ""gid"": 0, ""group"": ""root"", ""mode"": ""0755"", ""msg"": ""Request failed"", ""owner"": ""root"", ""response"": ""HTTP Error 404: Not Found"", ""secontext"": ""unconfined_u:object_r:etc_t:s0"", ""size"": 4096, ""state"": ""directory"", ""status_code"": 404, ""uid"": 0, ""url"": ""https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/saltbase/salt/fluentd-es/fluentd-es.yaml""}
fatal: [192.168.121.138]: FAILED! => {""changed"": false, ""dest"": ""/etc/kubernetes/manifests"", ""failed"": true, ""gid"": 0, ""group"": ""root"", ""mode"": ""0755"", ""msg"": ""Request failed"", ""owner"": ""root"", ""response"": ""HTTP Error 404: Not Found"", ""secontext"": ""unconfined_u:object_r:etc_t:s0"", ""size"": 4096, ""state"": ""directory"", ""status_code"": 404, ""uid"": 0, ""url"": ""https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/saltbase/salt/fluentd-es/fluentd-es.yaml""}
	to retry, use: --limit @/home/moahmed/Work/kubernetes/contrib/ansible/playbooks/deploy-cluster.retry

```

**What you expected to happen**:

The ansible should not have failed at this point.

**How to reproduce it**:

Run the ansible script with appropriate inventory and setting ""cluster_logging: true"" in inventory/group_vars/all.yml

**Anything else do we need to know**:

I found that fluend-es is no longer available in kubernetes repository, instead being replaced by fluentd-gcp, located at the below url:

https://github.com/kubernetes/kubernetes/blob/master/cluster/saltbase/salt/fluentd-gcp/fluentd-gcp.yaml",closed,False,2016-12-16 10:09:37,2017-01-02 13:41:44
contrib,ged15,https://github.com/kubernetes/contrib/issues/2160,https://api.github.com/repos/kubernetes/contrib/issues/2160,GCE Ingress controller could use one GLB,"Currently when I create a new Ingress on GKE, GCE Ingress controller provisions a new Google Load Balancer. This means that it also creates a new IP and that leads to the need to change DNS entries.
Would it be wiser to use the same Google Load Balancer and just add routing rules to the GLB for all Ingress resources?
Of course that means that path and hostname collisions should not occur, but that is a reasonable trade-off IMHO.",closed,False,2016-12-16 13:57:10,2018-02-17 11:21:00
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2161,https://api.github.com/repos/kubernetes/contrib/issues/2161,ClusterAutoscaler: context object to keep non-changing objects,"+ flag with max graceful termination for scale down.

cc: @fgrzadkowski @piosz @jszepkowski",closed,True,2016-12-16 15:28:46,2016-12-16 16:37:36
contrib,lvnilesh,https://github.com/kubernetes/contrib/pull/2162,https://api.github.com/repos/kubernetes/contrib/issues/2162,Update retrieve-systemd-files.yml,"Thanks @lucas-cegatti  for https://github.com/kubernetes/contrib/issues/2091#issuecomment-265144052

Fixes: #2091",closed,True,2016-12-16 17:10:18,2017-01-03 06:53:51
contrib,rimusz,https://github.com/kubernetes/contrib/issues/2163,https://api.github.com/repos/kubernetes/contrib/issues/2163,cluster-autoscaler: multiple instances in different AZs are in fight for leader,"I have three AWS ASG, did setup three separate cluster-autoscaler deployments with different names and namespaces, even then I see errors from two scalers:
```
cluster-scaler-azc-126014070-t3ky3 cluster-scaler-azc I1216 17:18:27.235227       1 leaderelection.go:210] failed to renew lease kube-system/cluster-autoscaler
cluster-scaler-azc-126014070-t3ky3 cluster-scaler-gen-azc I1216 17:18:27.235205       1 leaderelection.go:295] lock is held by cluster-scaler-azb-3625505393-a3n2b and has not yet expired
```

Why all of them checking for the lock `kube-system/cluster-autoscaler`?
Is any extra parameter needs to be used there?
",closed,False,2016-12-16 17:28:14,2017-04-19 09:11:30
contrib,spxtr,https://github.com/kubernetes/contrib/pull/2164,https://api.github.com/repos/kubernetes/contrib/issues/2164,Set the bazel build to be blocking.,"It's been green, and we plan to remove unit tests from the test-go job since they're much more suited to the bazel job.",closed,True,2016-12-16 19:30:26,2016-12-16 19:57:40
contrib,bprashanth,https://github.com/kubernetes/contrib/pull/2165,https://api.github.com/repos/kubernetes/contrib/issues/2165,Add more ingress tests to post merge blockers,"Presubmit is already running: https://github.com/kubernetes/kubernetes/pull/38141
Add the more extensive suite as a post merge blocker",closed,True,2016-12-16 20:11:04,2016-12-20 22:43:41
contrib,bgrant0607,https://github.com/kubernetes/contrib/issues/2166,https://api.github.com/repos/kubernetes/contrib/issues/2166,Investigate Azure's github tools,"https://azure.microsoft.com/en-us/blog/enterprise-scale-github-at-azure/

cc @kubernetes/sig-contributor-experience-misc @brendandburns @grodrigues3 @fejta ",closed,False,2016-12-16 21:01:35,2018-01-22 18:22:02
contrib,jessfraz,https://github.com/kubernetes/contrib/issues/2167,https://api.github.com/repos/kubernetes/contrib/issues/2167,feature: asking k8s-bot to rerun all failed tests,"Something like ""k8s-bot rerun failed tests"" so that you dont have to copy paste all the commands for each test individually or rerun all.

",closed,False,2016-12-16 21:19:50,2018-02-17 11:21:00
contrib,jessfraz,https://github.com/kubernetes/contrib/issues/2168,https://api.github.com/repos/kubernetes/contrib/issues/2168,feature: when pull requests are opened to a specific branch add the milestone,"I find myself adding the milestone v1.4 to all the cherry-pick PRs and it would be nice if one of the bots could check the branch it is open on and automatically add the milestone.

since all branches are `release-x.x` and milestones are `vx.x` it shouldnt be too hard

BONUS: if the bot can ping the release person on that PR too :) but thats extra",closed,False,2016-12-16 21:22:13,2018-02-17 14:24:03
contrib,eparis,https://github.com/kubernetes/contrib/pull/2169,https://api.github.com/repos/kubernetes/contrib/issues/2169,Turn on branch protection in contrib,,closed,True,2016-12-16 21:49:19,2016-12-19 21:51:02
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2170,https://api.github.com/repos/kubernetes/contrib/issues/2170,Test approvers,A test PR.  Adding do not merge,closed,True,2016-12-16 21:55:26,2016-12-19 18:16:59
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2171,https://api.github.com/repos/kubernetes/contrib/issues/2171,Remove all the things.,A PR to test approvers.  Ignore this,closed,True,2016-12-16 21:56:42,2017-01-09 21:59:46
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2172,https://api.github.com/repos/kubernetes/contrib/issues/2172,temporarily disable approvers,"There's a strange infinite loop that occurs we think because of old PRs that change files in directories that no longer exist (and therefore have no OWNERS files).  We have to handle that edge case, so disabling in the meantime.",closed,True,2016-12-16 22:51:34,2016-12-16 23:57:45
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2173,https://api.github.com/repos/kubernetes/contrib/issues/2173,handling the . case,The output of the call to `filepath.Dir()` for an empty string is `.` .  This results in an infinitely loop in the code.,closed,True,2016-12-17 00:02:16,2016-12-19 22:56:54
contrib,lucas-cegatti,https://github.com/kubernetes/contrib/pull/2174,https://api.github.com/repos/kubernetes/contrib/issues/2174,Support Fluentd migration to DaemonSet,"Fluentd was recently migrated to DaemonSet on kubernetes master on this [commit](https://github.com/kubernetes/kubernetes/commit/a52637f09f77e1d91bd79b33ddef7ba17542174f) breaking the current node playbook, related issue #2159.

My suggestion here on this PR is to move the new fluentd DaemonSet to `kubernetes-addons` where it will be automatically created on all pods. The difference from the fluentd from kubernetes is that I removed the NodeSelector, I don't know if one would be necessary here on the ansible script.

Fixes: #2159, #2183
",closed,True,2016-12-17 13:17:21,2017-01-02 13:41:44
contrib,sigxcpu76,https://github.com/kubernetes/contrib/issues/2175,https://api.github.com/repos/kubernetes/contrib/issues/2175,nginx-ingress-controller failure to retrieve page relative resources,"Maybe I'm getting this wrong, but the base-url should be constructed from matched path instead of rewrite target. 

Problem description:

Ingress rule:
```
path = ""/p""
rewrite-target = ""/""
add-base-url = true
```

How it works:
```
base-url=protocol://host/target => http(s)://some.host/
```

- ```css/test.css``` becomes ```http(s)://some.host/css/test.css``` (ingress rule is not matched anymore)

How it should work:
```
base-url=protocol://host/target => http(s)://some.host/p
```
- ```css/test.css``` becomes ```http(s)://some.host/p/css/test.css``` (matches the ingress rule and URL is rewritten to ```/css/test.css``` for the backend to be loaded correctly)



If a path ""/p"" is matched by the ingress rule, the backend is rewritten to ```/```, but the resources should still be pulled by ```/p/``` from the browser point of view, otherwise there is no matching path in ingress rule.",closed,False,2016-12-19 08:26:17,2018-02-18 16:50:00
contrib,sebbtl,https://github.com/kubernetes/contrib/issues/2176,https://api.github.com/repos/kubernetes/contrib/issues/2176,keepalived-vip - multiple services with one IP,"Actually, we have only one service per IP with kube-keepalived-vip. One solution is to accept a configmap entry like this:

192.168.0.1: default/svc1 default/svc2 default/svc3

I need this so I write this trivial patch for controller.go 0.7.0:
[keepalived-vip-multiple-services.txt](https://github.com/kubernetes/contrib/files/660752/keepalived-vip-multiple-services.txt)

There is no verification if svc1 and svc2 use same port, it's a trivial patch. 


",closed,False,2016-12-19 10:36:44,2018-02-25 13:31:50
contrib,bjornl,https://github.com/kubernetes/contrib/issues/2177,https://api.github.com/repos/kubernetes/contrib/issues/2177,kube-apiserver fails to start on secondary master node,"Then setting up a cluster using Ansible with two masters on Fedora 25, the kube-apiserver service fails to start on the secondary master node.

There look to me like there is a certificate that is missing on the second node.

++ /usr/bin/kube-apiserver --logtostderr=true --v=0 --etcd-servers=http://kubelab1:2379,http://kubelab2:2379 --insecure-bind-address=127.0.0.1 --secure-port=443 --allow-privileged=true --service-cluster-ip-range=10.254.0.0/16 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota --tls-cert-file=/etc/kubernetes/certs/server.crt --tls-private-key-file=/etc/kubernetes/certs/server.key --client-ca-file=/etc/kubernetes/certs/ca.crt --token-auth-file=/etc/kubernetes/tokens/known_tokens.csv --service-account-key-file=/etc/kubernetes/certs/server.crt --bind-address=0.0.0.0
I1219 16:02:45.330796    9876 genericapiserver.go:629] Will report 192.168.20.39 as public IP address.
F1219 16:02:45.331033    9876 server.go:238] Invalid Authentication Config: open /etc/kubernetes/certs/server.crt: no such file or directory

Certificates on first master node:
[root@kubelab1 ~]# ls -l /etc/kubernetes/certs/
total 40
-r--r-----. 1 kube kube-cert 1338 Dec 19 15:51 ca.crt
-r--r-----. 1 kube kube-cert 4557 Dec 19 15:51 kubecfg.crt
-r--r-----. 1 kube kube-cert 1708 Dec 19 15:51 kubecfg.key
-rw-rw----. 1 root kube-cert 4557 Dec 19 15:51 kubelet.crt
-rw-rw----. 1 root kube-cert 1704 Dec 19 15:51 kubelet.key
-r--r-----. 1 kube kube-cert 5051 Dec 19 15:51 server.crt
-r--r-----. 1 kube kube-cert 1704 Dec 19 15:51 server.key

Certificates on second master node:
[root@kubelab2 ~]# ls -l /etc/kubernetes/certs/
total 16
-rw-r--r--. 1 root root 1338 Dec 19 15:51 ca.crt
-rw-r--r--. 1 root root 4557 Dec 19 15:51 kubecfg.crt
-rw-r--r--. 1 root root 1708 Dec 19 15:51 kubecfg.key

My inventory file look like this:

[masters]
kubelab1
kubelab2

[etcd:children]
masters

[nodes]
kubelab1
kubelab2",closed,False,2016-12-19 15:11:04,2017-03-15 15:27:19
contrib,MrHohn,https://github.com/kubernetes/contrib/issues/2178,https://api.github.com/repos/kubernetes/contrib/issues/2178,[ansible] Insure PyYAML module is installed for the master role.,"From kubernetes/kubernetes#37352.

A user was using the ansible repo but failed to start `kube-dns` using Addon Manager. Turned out his cluster was based on CentOS 7 and did not have PyYAML module installed (this module is needed by Addon Manager to parse out objects from yaml files). Addon manager's logging might not be good so it was not obviously showing what was wrong.

We may insure PyYAML module is installed for the master role to make sure `kube-dns` could be managed by the Addon Manager. Manually create `kube-dns` works but it is not desired.

@eparis @ingvagabund",closed,False,2016-12-19 18:05:30,2018-03-11 03:51:33
contrib,cobookman,https://github.com/kubernetes/contrib/issues/2179,https://api.github.com/repos/kubernetes/contrib/issues/2179,[Typo] Its GCLB not GLBC,"The following documentation uses the acronym of GLBC to represent the GCP Compute Engine Layer 7 Load Balancer:

https://github.com/kubernetes/contrib/tree/master/ingress/controllers/gce

However the correct acronym is GCLB.",closed,False,2016-12-20 00:30:20,2018-02-17 22:32:04
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2180,https://api.github.com/repos/kubernetes/contrib/issues/2180,Try to update comments for flakes instead of making new ones.,"Conditions where it will update a comment:
- It is referring to a flake with the ""same reason"" (fuzzy equality)
- The comment is less than 2 weeks old.
- Nobody other than the bot has made a comment after that.

This should cut down on a large volume of flakes comments and respect
people's inboxes much more.",closed,True,2016-12-20 00:37:38,2016-12-22 21:52:45
contrib,mumoshu,https://github.com/kubernetes/contrib/issues/2181,https://api.github.com/repos/kubernetes/contrib/issues/2181,cluster-autoscaler: Dynamic reconfiguration via ConfigMap,"> **Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

No.

> **What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

I've searched for:

* cluster-autoscaler
* dynamic reconfiguration
* configmap
* config map
* cluster autoscaler

and their combinations but found nothing.

---

> **Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

FEATURE REQUEST

> If this is a FEATURE REQUEST, please:
>  - Describe *in detail* the feature/behavior/change you'd like to see.

As a maintainer of [kube-aws](https://github.com/coreos/kube-aws), I'm planning to add out-of-box support for/integration with cluster-autoscaler into [kube-aws](https://github.com/coreos/kube-aws) and got to want a feature to reconfigure cluster-autoscaler without deleting it and then creating it with new `--nodes` parameters, hence ""dynamic reconfiguration"".

cc @andrewsykim @mwielgus 

## My use-case

My use-case is to reconfigure cluster-autoscaler automatically when a kube-aws user added/removed node pool(s) associated to a k8s cluster.

More complete context can be seen in [an corresponding issue](https://github.com/coreos/kube-aws/issues/148) of kube-aws.

### What could be done WITHOUT the requested feature

Currently, an user has to delete and then create cluster-autoscaler completely out of band from a k8s cluster created by deployment tools like kops and kube-aws, to reconfigure cluster-autoscaler to:

* stop auto-scaling on already removed node pool(s)
* start auto-scaling on newly created node pool(s)

With cluster-autoscaler as of today, we can achieve dynamic reconfiguration via writing e.g.:

* a shell script to `inotify-wait` on a configmap volume,
  * read the latest configuration from it,
  * and then updating cluster-autoscaler by restarting it while updating `--nodes` parameters,
     * via `kubectl` or inside the container running cluster-autoscaler.

Though it is technically possible, doing so in that way seems to involve more moving parts than necessary:
* a correct init process(a shell script as pid=0 may result in zombie processes)
* process supervisor(to keep cluster-autoscaler up and running)

and makes testing harder.

### What could be done WITH the requested feature

* cluster-autoscaler can be integrated to k8s deployment tools in easier ways/cluster-scaler can be deployed more widely
   * For example, for each k8s deployment tool like kops, kube-aws, implement a `configmap updater` application to periodically update the configmap read by cluster-autoscaler
   * and then our user is freed from manually recreating a cluster-autoscaler deployment object for each time he/she updated a cluster, thus less work to utilize cluster-autoscaler

### Thoughts on implementation

After looking around the k8s ecosystem, I've noticed that an existing k8s add-on like [cluster-proportional-autoscaler](https://github.com/kubernetes-incubator/cluster-proportional-autoscaler) seems to have a well thought support for dynamic configuration via `--configmap` and `--default-params` flags, hence I'm ""suggesting"" dynamic reconfiguration ""via ConfigMap"".
It won't necessary be utilizing ConfigMap if dynamic reconfiguration is achieved.

FYI [cluster-proportional-autoscaler's code was helpful to imagine how it would work to achieve dynamic reconfiguration](https://github.com/kubernetes-incubator/cluster-proportional-autoscaler/blob/8ee4bd79b553fa21e0b7e420a6ba7148fdb946ca/pkg/autoscaler/autoscaler_server.go#L103). This matched my expectation on how I'd like cluster-autoscaler to support dynamic reconfiguration.

Btw, cluster-autoscaler seems to have [a similar control-loop](https://github.com/kubernetes/contrib/blob/master/cluster-autoscaler/cluster_autoscaler.go#L227-L229) to periodically update node groups.
If it is ok to extend the loop to periodically check a configmap for an update while changing a part of cluster-autoscaler's state from immutable to mutable, I guess I can submit a pull request to achieve what is requested in this feature request with it.
",closed,False,2016-12-20 04:45:42,2017-04-14 02:39:55
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2182,https://api.github.com/repos/kubernetes/contrib/issues/2182,Cluster-autoscaler: store scale down state in a struct.,cc: @fgrzadkowski @piosz @jszczepkowski,closed,True,2016-12-20 11:29:11,2016-12-20 17:37:49
contrib,Tedezed,https://github.com/kubernetes/contrib/issues/2183,https://api.github.com/repos/kubernetes/contrib/issues/2183,Error 404 to install fluentd,"Hi, today use a playbook of Kubernetes and I have the next error:

```
 TASK [node : Install fluentd pod into each node] *******************************
fatal: [192.168.33.11]: FAILED! => {""changed"": false, ""dest"": ""/etc/kubernetes/manifests"", ""failed"": true, ""gid"": 0, ""group"": ""root"", ""mode"": ""0755"", ""msg"": ""Request failed"", ""owner"": ""root"", ""response"": ""HTTP Error 404: Not Found"", ""size"": 4096, ""state"": ""directory"", ""status_code"": 404, ""uid"": 0, ""url"": ""https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/saltbase/salt/fluentd-es/fluentd-es.yaml""}
	to retry, use: --limit @/home/administrador/vagrant_machines/kubernetes_test/contrib/ansible/playbooks/deploy-cluster.retry
```
My solution, edit `fluentd-install.yml` route `contrib/ansible/roles/node/tasks/fluentd-install.yml`

Remplace: `https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/saltbase/salt/fluentd-es/fluentd-es.yaml`

Use: `https://raw.githubusercontent.com/kubernetes/kops/master/vendor/k8s.io/kubernetes/cluster/saltbase/salt/fluentd-es/fluentd-es.yaml`",closed,False,2016-12-20 14:48:25,2017-01-02 13:42:24
contrib,eparis,https://github.com/kubernetes/contrib/pull/2184,https://api.github.com/repos/kubernetes/contrib/issues/2184,Fix configMaps,"contrib configMap was out of date.
some configMaps didn't have protected-branches
update some help text",closed,True,2016-12-20 19:18:24,2016-12-20 21:39:23
contrib,eparis,https://github.com/kubernetes/contrib/pull/2185,https://api.github.com/repos/kubernetes/contrib/issues/2185,Remove zoidbergwill from OWNERS,@zoidbergwill is not in the org and thus the bot cannot assign them. So remove them from OWNERS,closed,True,2016-12-20 19:31:06,2016-12-30 09:50:23
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2186,https://api.github.com/repos/kubernetes/contrib/issues/2186,Use i-- instead of i++ for reverse loop.,,closed,True,2016-12-20 19:35:43,2016-12-20 19:37:09
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2187,https://api.github.com/repos/kubernetes/contrib/issues/2187,Fix comment editing to use the right API.,"It should be Issues.EditComment, not Repository.UpdateComment.",closed,True,2016-12-20 20:02:21,2016-12-20 21:29:21
contrib,rutsky,https://github.com/kubernetes/contrib/issues/2188,https://api.github.com/repos/kubernetes/contrib/issues/2188,[Nginx Ingress Controller] fails if IPv6 addresses are used for DNS,"I'm running trying to deploy K8s cluster on Ubuntu 16.04 on DigitalOcean with enabled IPv6.

When I'm trying to deploy `nginx-ingress-controller` (using [this config](https://gist.github.com/rutsky/acff82531bc4af3de65eb749b9c4f786)) I'm getting crashing `nginx-ingress-controller`:

```
$ kubectl --kubeconfig admin.conf get pods
NAME                             READY     STATUS             RESTARTS   AGE
default-http-backend-mlr35       1/1       Running            0          16m
nginx-ingress-controller-c9vtm   0/1       CrashLoopBackOff   6          10m
```

With error about incorrect configuration of nginx:

```
$ kubectl --kubeconfig admin.conf logs nginx-ingress-controller-c9vtm
I1220 21:46:42.046335       1 main.go:94] Using build: https://github.com/bprashanth/contrib.git - git-92b2bac
I1220 21:46:42.186986       1 main.go:123] Validated default/default-http-backend as the default backend
W1220 21:46:42.187975       1 ssl.go:132] no file dhparam.pem found in secrets
I1220 21:46:42.193668       1 controller.go:1126] starting NGINX loadbalancer controller
I1220 21:46:42.194230       1 command.go:35] Starting NGINX process...
W1220 21:46:43.235862       1 utils.go:91] requeuing default/kubernetes, err deferring sync till endpoints controller has synced
E1220 21:46:43.405212       1 command.go:87] failed to execute nginx -s reload: 2016/12/20 21:46:43 [emerg] 17#17: invalid port in resolver ""2001:4860:4860::8844"" in /etc/nginx/nginx.conf:76
nginx: [emerg] invalid port in resolver ""2001:4860:4860::8844"" in /etc/nginx/nginx.conf:76
W1220 21:46:43.405682       1 utils.go:91] requeuing kube-system/kube-controller-manager, err error reloading nginx: exit status 1
I1220 21:46:59.545275       1 main.go:187] Received SIGTERM, shutting down
I1220 21:46:59.545402       1 controller.go:1074] removing IP address 82.196.1.53 from ingress rules
I1220 21:46:59.545424       1 controller.go:1091] updating 0 Ingress rule/s
I1220 21:46:59.545433       1 controller.go:1077] Shutting down controller queues.
I1220 21:46:59.545495       1 main.go:145] Handled quit, awaiting pod deletion
I1220 21:47:29.545682       1 main.go:145] Handled quit, awaiting pod deletion
I1220 21:47:52.189014       1 main.go:195] Exiting with 0
```

Specifically nginx is not happy about this line in `/etc/nginx/nginx.conf`:

```
    # Custom dns resolver.
    resolver 2001:4860:4860::8844 2001:4860:4860::8888 8.8.8.8 valid=30s;
```

Host's `/etc/resolv.conf` contains:

```
# cat /etc/resolv.conf 
# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN
nameserver 2001:4860:4860::8844
nameserver 2001:4860:4860::8888
nameserver 8.8.8.8
```

which probably leads to generation of that incorrect config.

`nginx-ingress-controller` should check IP address type and properly wrap IPv6 address before pasting to config (I think it should be like `resolver [2001:4860:4860::8844] [2001:4860:4860::8888] 8.8.8.8 valid=30s;`, but I haven't checked that).
",closed,False,2016-12-20 21:53:35,2017-01-02 04:21:53
contrib,eparis,https://github.com/kubernetes/contrib/pull/2189,https://api.github.com/repos/kubernetes/contrib/issues/2189,Fix docs and !GateApproval,"The main change is the ""docs"" to show that approval either is or isn't
required. These are displayed on the info tab of the bot.

While in here it became clear that we were hiding such simple logic
oddly and incorrectly. Even if `GateApproved` was false, if a PR had the
approved label and had been changed since we would still reject it.

This PR attempts to make the logic a bit more clear. Although we still
do not have any tests for the GateApproved=false case.",closed,True,2016-12-20 22:00:03,2016-12-21 02:03:49
contrib,eparis,https://github.com/kubernetes/contrib/pull/2190,https://api.github.com/repos/kubernetes/contrib/issues/2190,contrib isn't running the approval munger so turn off approvals,,closed,True,2016-12-20 22:04:14,2016-12-20 22:39:25
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/2191,https://api.github.com/repos/kubernetes/contrib/issues/2191,Cluster-autoscaler: increase unit test coverage in core codebase,,closed,False,2016-12-20 22:51:03,2017-04-19 11:46:49
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2192,https://api.github.com/repos/kubernetes/contrib/issues/2192,Cluster-autoscaler: test cloud provider for unit tests,"Ref: #2191

cc: @fgrzadkowski @piosz @jszczepkowski ",closed,True,2016-12-20 22:51:52,2016-12-21 10:43:35
contrib,rutsky,https://github.com/kubernetes/contrib/issues/2193,https://api.github.com/repos/kubernetes/contrib/issues/2193,[Nginx Ingress Controller] Setting DNS resolver through ConfigMap doesn't work,"In order to workaround #2188 I tried to set custom DNS resolver as specified in [Customization docs](https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/configuration.md#allowed-parameters-in-configuration-config-map).

I created ConfigMap:

```
apiVersion: v1                                                                                                                                   
data:                                                                                                                                            
  resolver: ""8.8.8.8""                                                                                                                            
kind: ConfigMap                                                                                                                                  
metadata:                                                                                                                                        
  name: nginx-load-balancer-conf
```

added `https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/configuration.md#allowed-parameters-in-configuration-config-map` to list of `/nginx-ingress-controller`arguments (as specified [here](https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/examples/custom-configuration/rc-custom-configuration.yaml)).

And recreated `nginx-ingress-controller` replication controller.

Still pod contains incorrect resolver (and fails):

```
$ kubectl --kubeconfig admin.conf exec nginx-ingress-controller-s3lhg -- cat /etc/nginx/nginx.conf | grep resolver
    # Custom dns resolver.
    resolver 2001:4860:4860::8844 2001:4860:4860::8888 8.8.8.8 valid=30s;
```

I believe ConfigMap is properly applied in my configuration: when I add to ConfigMap other parameters, e.g. `body-size: 10m`, they are being properly applied --- I see changes in `/etc/nginx/nginx.conf`.
",closed,False,2016-12-20 23:26:23,2017-01-02 13:29:30
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2194,https://api.github.com/repos/kubernetes/contrib/issues/2194,[mungegithub]: Improve Approval Process,"- Add link to OWNERS file in notification
- Show minimum set of OWNERS files needed
  for approval
- Make notification clearer
- Call LeafApprovers on full file, not just
  path, so correct owners file used
- Added tests for repo updates and removed ""/""
  convention for root so files have same prefix
- Added canonical way to get filepath and
  tests for it",closed,True,2016-12-21 02:08:36,2016-12-28 21:52:34
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2195,https://api.github.com/repos/kubernetes/contrib/issues/2195,Cluster-autoscaler: add stateful set support in drain.,"This change will have to be cherry-picked to 0.4.0 release of CA (and K8S 1.5).

cc: @fgrzadkowski @piosz @jszczepkowski",closed,True,2016-12-21 10:37:57,2016-12-22 09:34:06
contrib,olvesh,https://github.com/kubernetes/contrib/pull/2196,https://api.github.com/repos/kubernetes/contrib/issues/2196,Changed kind and apiVersion to reflect PetSet->StatefulSet,"Should be changed to 
```
apiVersion: apps/v1beta1
kind: StatefulSet
``` 
reflecting changes in Kubernetes 1.5.

This pull request does that.
",closed,True,2016-12-21 11:47:17,2018-03-19 10:14:06
contrib,dustymabe,https://github.com/kubernetes/contrib/pull/2197,https://api.github.com/repos/kubernetes/contrib/issues/2197,Ingress: doc updates/image updates,,closed,True,2016-12-21 20:43:42,2016-12-22 18:41:46
contrib,dbrian,https://github.com/kubernetes/contrib/pull/2198,https://api.github.com/repos/kubernetes/contrib/issues/2198,service-loadbalancer fails to build,Curl doesn't follow the redirect to S3 when downloading from Github releases.,closed,True,2016-12-21 21:08:47,2017-04-16 03:28:06
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/2199,https://api.github.com/repos/kubernetes/contrib/issues/2199,[mungegithub] Autoclose Issues Without Routing Labels,"A Two Part Issue
Allow people to suggest area/labels or sig/labels when creating issues using issue template.  Something like 
`area-label: area/kubelet` or `sig-label:sig-cli` in the template could allow us to auto-detect routing labels

If issue opened, by member of org without routing label, auto-close and explain why.",closed,False,2016-12-22 00:04:08,2017-01-21 01:19:20
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/2200,https://api.github.com/repos/kubernetes/contrib/issues/2200,[mungebot] Autoclose PRs Without Release Note Label Set,"If PR opened without release note label set and PR author is member of org, auto close PR and explain how label can be set with PR template",closed,False,2016-12-22 00:04:45,2017-01-21 01:45:01
contrib,ysolt,https://github.com/kubernetes/contrib/pull/2201,https://api.github.com/repos/kubernetes/contrib/issues/2201,SSL documentation improvements,Minor documentation touch-up to add missing ```kubectl create``` option and synchronise secret objects spread-around in the documentation.,closed,True,2016-12-22 07:51:25,2016-12-22 08:00:17
contrib,GheRivero,https://github.com/kubernetes/contrib/pull/2202,https://api.github.com/repos/kubernetes/contrib/issues/2202,Use default etcd port (2379) in systemd service,"etcd changed its default port to 2379. Update apiserver configuration
for systemd to add a etcd node listening in that port, and not removing
the previous one for a smooth transition.",closed,True,2016-12-22 07:56:13,2016-12-28 18:03:01
contrib,ysolt,https://github.com/kubernetes/contrib/pull/2203,https://api.github.com/repos/kubernetes/contrib/issues/2203,SSL documentation improvements,Minor documentation touch-up to add missing kubectl create option and synchronise secret objects spread-around in the documentation.,closed,True,2016-12-22 08:06:13,2016-12-22 18:01:43
contrib,ysolt,https://github.com/kubernetes/contrib/pull/2204,https://api.github.com/repos/kubernetes/contrib/issues/2204,Detailed NGINX-Ingress-Controller status page config,Add detailed example for NGINX-Ingress-Controller status page configuration,closed,True,2016-12-22 10:40:07,2017-04-04 15:13:08
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2205,https://api.github.com/repos/kubernetes/contrib/issues/2205,Cluster autoscaler: more unit tests for scale down,cc: @fgrzadkowski @piosz @jszczepkowski,closed,True,2016-12-22 13:20:26,2016-12-22 19:39:11
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2206,https://api.github.com/repos/kubernetes/contrib/issues/2206,Cluster-Autoscaler: fix logging in scale down,"This will have to be cherry-picked to 1.5. 

cc: @fgrzadkowski @piosz @jszczepkowski",closed,True,2016-12-22 13:21:29,2016-12-22 16:14:16
contrib,tim-zju,https://github.com/kubernetes/contrib/pull/2207,https://api.github.com/repos/kubernetes/contrib/issues/2207,Spell error,"fix symbol errors: change ‘ to '
Signed-off-by: tim-zju <21651152@zju.edu.cn>",closed,True,2016-12-22 14:12:30,2016-12-22 14:56:15
contrib,tim-zju,https://github.com/kubernetes/contrib/pull/2208,https://api.github.com/repos/kubernetes/contrib/issues/2208,symbol errors,"fix symbol errors: change ‘ to ' 
Signed-off-by: tim-zju <21651152@zju.edu.cn>",closed,True,2016-12-22 14:55:26,2017-02-07 23:30:34
contrib,dustymabe,https://github.com/kubernetes/contrib/pull/2209,https://api.github.com/repos/kubernetes/contrib/issues/2209,ansible: temporarily fix skydns issues,"The skydns images aren't able to be pulled for newer docker
clients because they are v1 images vs v2 images as can be
shown by [1]. We should really move to kubedns as is noted
in #2124, but here is a short term workaround.

[1] https://console.cloud.google.com/kubernetes/images/tags/skydns?location=GLOBAL&project=google-containers",closed,True,2016-12-22 18:13:39,2016-12-22 18:41:50
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/2210,https://api.github.com/repos/kubernetes/contrib/issues/2210,Update the PR Template for kubernetes/kubernetes ,"Template can be used to set release-note and release note label automatically, but template is often deleted

Let's try and make the PR template more useful

Note this issue is across repos (the issue is filed in contrib, but the PR that fixes it is going to be in kubernetes)",closed,False,2016-12-22 18:40:10,2018-02-17 14:24:00
contrib,jberkus,https://github.com/kubernetes/contrib/pull/2211,https://api.github.com/repos/kubernetes/contrib/issues/2211,"Fixed issue with ""become"" behavior in checking the local addons direc…","…tory, in cases where the user sets ansible_become in global_vars.",closed,True,2016-12-22 22:18:00,2016-12-23 03:21:17
contrib,alexw23,https://github.com/kubernetes/contrib/pull/2212,https://api.github.com/repos/kubernetes/contrib/issues/2212,Correctly forward X-Forwarded-Port,"It appears this package is assuming the port running on the Ingress Controller as the port.

In the current situation, if my LB sends `X-Forwarded-Port: 443` and the ingress controller is running on port 80. `X-Forwarded-Port: 80` will end up in my app.

Now it's correctly routing.",closed,True,2016-12-22 22:56:47,2016-12-22 23:09:35
contrib,alexw23,https://github.com/kubernetes/contrib/pull/2213,https://api.github.com/repos/kubernetes/contrib/issues/2213,Added default_server option for the default server,If you're using wildcard host names the default backend will no longer be available due to the way nginx has predence on wildcards. By providing `default_server` it indicates a catch-all so regardless of wildcards if there is no direct match they will end up in default server block.,closed,True,2016-12-22 23:05:35,2016-12-22 23:09:44
contrib,tim-zju,https://github.com/kubernetes/contrib/pull/2214,https://api.github.com/repos/kubernetes/contrib/issues/2214,typos errors,"typos errors: misusing an and a, duplicate the
Signed-off-by: tim-zju <21651152@zju.edu.cn>",closed,True,2016-12-23 03:26:14,2017-06-03 21:55:42
contrib,Yancey1989,https://github.com/kubernetes/contrib/issues/2215,https://api.github.com/repos/kubernetes/contrib/issues/2215,[Nginx Ingress Controller] Usage memory kept growing up until the memory limit.,"The memory nginx ingress controller pod  usage memory kept growing up until the memory limit, and later it will reload automatic. Is this a bug?

",closed,False,2016-12-23 13:05:46,2018-02-17 14:24:01
contrib,du2016,https://github.com/kubernetes/contrib/pull/2216,https://api.github.com/repos/kubernetes/contrib/issues/2216,remove the ` at the end of the line,remove the ` at the end of the line,closed,True,2016-12-23 16:19:06,2016-12-24 13:05:35
contrib,omerzach,https://github.com/kubernetes/contrib/pull/2217,https://api.github.com/repos/kubernetes/contrib/issues/2217,Fix typo in ingress/controllers/gce/README.md,,closed,True,2016-12-23 22:28:26,2016-12-24 15:02:48
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2218,https://api.github.com/repos/kubernetes/contrib/issues/2218,Cluster-autoscaler: more unit tests for scale up,cc: @jszczepkowski @fgrzadkowski @piosz,closed,True,2016-12-23 22:29:46,2016-12-28 22:28:04
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2219,https://api.github.com/repos/kubernetes/contrib/issues/2219,Cluster-autoscaler: add self link to BuildTestPod/Node functions,cc: @fgrzadkowski @jszczepkowski,closed,True,2016-12-23 22:45:56,2016-12-28 08:52:51
contrib,eparis,https://github.com/kubernetes/contrib/pull/2220,https://api.github.com/repos/kubernetes/contrib/issues/2220,Turn on protected branches in kubernetes,"This has been running for a couple of days with success in contrib. No
more confusion about what matters and what doesn't!",closed,True,2016-12-24 15:06:57,2016-12-25 21:02:59
contrib,paultiplady,https://github.com/kubernetes/contrib/issues/2221,https://api.github.com/repos/kubernetes/contrib/issues/2221,[Nginx Ingress Controller] Specifying the loadBalancerIP used by the Ingress,"I'd like to use a static IP for my ingress. Currently, the Nginx Ingress Controller allocates a new loadBalancerIP each time it starts.

For Services, it's possible to pin the IP using `loadBalancerIP: 1.2.3.4`. Is this possible in the Nginx Ingress Controller?

If not, is there some other approach that can be used to pin the IP of an Ingress?",closed,False,2016-12-24 22:28:56,2018-02-17 14:24:03
contrib,leshik,https://github.com/kubernetes/contrib/issues/2222,https://api.github.com/repos/kubernetes/contrib/issues/2222,[Nginx Ingress Controller] upstream-default-backend and multiple ingresses,"Hi,
I've configured two ingresses in different namespaces and then found that requests with non-matching  hosts are being redirected to one of my application (instead of the default backend). But everything was working as expected when only one ingress was configured. I looked into ingress controller `nginx.conf` and found that there is no `default_server` option used in the default server block:
```
...
server_name _;
listen 80;
listen 443  ssl spdy http2;
...
```
Wouldn't be better to add `default_server` directive to these `listen` statements?",closed,False,2016-12-25 19:31:23,2018-02-17 14:24:02
contrib,thockin,https://github.com/kubernetes/contrib/pull/2223,https://api.github.com/repos/kubernetes/contrib/issues/2223,Extend docs-no-retest munger to allow OWNERS,"There's no point in running e2e on OWNERS file changes.

Properly, this should be renamed and use a different label - we have both ""retest not required"" and ""...-docs-only"", but the distinction seems silly to me.",closed,True,2016-12-27 18:31:23,2016-12-31 14:12:42
contrib,aledbf,https://github.com/kubernetes/contrib/pull/2224,https://api.github.com/repos/kubernetes/contrib/issues/2224,Release version 0.6,Check https://quay.io/repository/aledbf/ubuntu-slim?tab=tags,closed,True,2016-12-28 13:23:45,2017-01-10 02:01:33
contrib,aledbf,https://github.com/kubernetes/contrib/pull/2225,https://api.github.com/repos/kubernetes/contrib/issues/2225,[nginx-slim]: Release v0.12,,closed,True,2016-12-28 13:45:56,2017-01-13 00:16:36
contrib,mumoshu,https://github.com/kubernetes/contrib/pull/2226,https://api.github.com/repos/kubernetes/contrib/issues/2226,cluster-autoscaler: Dynamic Reconfiguration via ConfigMaps,"This is the first iteration to address #2181.
Please read the discussion in the issue to see more context.

@andrewsykim @Raffo @mwielgus Would you mind reviewing this? :bow:

TODOs:

- [x] Add unit tests

Non-TODOs:

- Automatic creation of configmaps filled with defaults, like cluster-proportional-autoscaler does with `--configmap` with `--default-params`. IMHO doing so is a completely orthogonal issue therefore won't be addressed in this PR.

---

This PR adds a new optional flag named `configmap` to specify the name of a configmap containing node group specs.

The configmap is polled every `scan-interval` seconds to reconfigure cluster-autoscaler dynamically at runtime.

Example usage:

```
./cluster-autoscaler --v=4 --cloud-provider=aws --skip-nodes-with-local-storage=false --logtostderr --leader-elect=false --configmap=cluster-autoscaler --logtostderr
```

The configmap would look like:

```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: cluster-autoscaler
  namespace: kube-system
data:
  settings: |-
    {
      ""nodeGroups"": [
        {
          ""minSize"": 1,
          ""maxSize"": 2,
          ""name"": ""kubeawstest-nodepool1-AutoScaleWorker-1VWD4GAVG35L5""
        }
      ]
    }
 ```",closed,True,2016-12-28 20:36:44,2017-02-28 01:01:41
contrib,bowei,https://github.com/kubernetes/contrib/pull/2227,https://api.github.com/repos/kubernetes/contrib/issues/2227,Removing dnsmasq-metrics; it now lives in kubernetes/dns,@thockin @MrHohn ,closed,True,2016-12-28 20:58:37,2016-12-28 21:02:07
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/2228,https://api.github.com/repos/kubernetes/contrib/issues/2228,Cluster-autoscaler: handle unready nodes,"### Introduction

Currently ClusterAutoscaler stops working when the number of nodes observed on the cloud provider side doesn’t match to the number of ready nodes on the Kubernetes side. This behavior was introduced in the first version of CA in order to prevent CA from breaking more the already broken cluster. However the client reported feedback indicated that this behavior is oftentimes suboptimal and leads to confusion. This document sketches how the problem of unready nodes will be solved in the next release of K8S.

### Use cases

The number of ready nodes can be different than on the mig side in the following situations:

* [UC1] A new node is being added to the cluster. The node group has been increased but the node has not been created/started/registered in K8S yet. On GCP this usually takes couple minutes. 
Indicating factors:
 -- There was a scale up in the last couple minutes.
 -- The number of missing node is at most the size of executed scale-up.
Suggested action: Continue operations, however include all yet-to-arrive nodes in all scale-up considerations.

* [UC2] A new node is being added to the cluster. The node has registered on the cluster but has not yet switched its state to ready. This should be fixed in couple seconds. Indicating factors:
 -- The unready node is new. CreateTime in the last couple minutes.
Suggested action: Continue operations, however include all yet-to-arrive nodes in all scale-up considerations.

* [UC3] A new node was added to the cluster but failed to start within the reasonable time. There is little chance that it will start anytime soon. Indicating factors:
 -- Node is unready
 -- CreateTime == unready NodeCondition.LastTransitionTime
Suggested action: Continue operations, however do not expand this node pool. The probable scenario is that the node will be picked by scale down soon.

* [UC4] A new node is being added to the cluster. However the cloud provider cannot provision the node within the reasonable time due to either no quota or technical problems. Indicating factors:
 -- The target number of nodes on the cloud provider side is greater than the number of nodes in K8S for the prolonged time (more than couple minutes) and the difference doesn’t change.
Suggested action: Reduce the target size of the problematic node group to the current size. 

* [UC5] A node is in an unready state for quite a while (+20min) and the total number of unready/not-present nodes is low (less than XX%). It could either not switched from unready to ready on node registration or something crashed on the node and could not be recovered. Indicating factors:
-- Node condition is unready and last transition time is >= 20 min.
-- The number of TOTAL nodes in K8S is equal to the target number of nodes on the cloud provider side. 
Suggested action: Include the node in scale down, although with greater (configurable) unneeded time.

* [UC6] Some nodes are being removed by cluster autoscaler. Indicating factor:
-- Node is unready and has ToBeRemoved taint.
Suggested action: Continue operations. Nodes should be removed soon.

* [UC7] The number of unjustified (not related to scale-up and scale-down) unready nodes is greater than XX%. Something is broken, possibly due to network partition or generic failure. Indicating factors: 
 -- >XX% of nodes are unready 
Suggested action: halt operations.
",closed,False,2016-12-29 02:13:38,2017-04-19 10:37:51
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2229,https://api.github.com/repos/kubernetes/contrib/issues/2229,Cluster-autoscaler: cluster state registry proposal,"The proposal that addresses #2228.

cc: @fgrzadkowski @jszczepkowski @piosz @mumoshu @andrewsykim @kubernetes/sig-autoscaling-misc",closed,True,2016-12-29 02:46:35,2017-01-03 14:19:28
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2230,https://api.github.com/repos/kubernetes/contrib/issues/2230,Cluster-autoscaler: cluster state registry,"Initial implementation of ClusterStateRegistry.

Ref: #2228 #2229",closed,True,2016-12-29 02:48:09,2016-12-29 14:52:42
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2231,https://api.github.com/repos/kubernetes/contrib/issues/2231,Cluster-autoscaler: move ToBeDeleted taint functions to utils,"Minor cleanup. No much of new code.

cc: @jszczepkowski @fgrzadkowski",closed,True,2016-12-29 19:02:34,2016-12-30 13:42:59
contrib,ixdy,https://github.com/kubernetes/contrib/pull/2232,https://api.github.com/repos/kubernetes/contrib/issues/2232,Remove extra slash from SQ GitHub commit status URL,"The URL currently used by the SQ is subtly wrong: http://submit-queue.k8s.io/#/prs/?prDisplay=39137&historyDisplay=39137 goes to the queue tab. If we remove the slash after ""prs"", then http://submit-queue.k8s.io/#/prs?prDisplay=39137&historyDisplay=39137 goes to the PR details tab.

cc @saad-ali ",closed,True,2016-12-29 19:44:35,2016-12-29 20:12:47
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2233,https://api.github.com/repos/kubernetes/contrib/issues/2233,Cluster-Autoscaler: upcoming nodes in ClusterStateRegistry,Ref: #2228 #2229,closed,True,2016-12-30 01:47:17,2016-12-30 13:12:56
contrib,amnk,https://github.com/kubernetes/contrib/issues/2234,https://api.github.com/repos/kubernetes/contrib/issues/2234,[Election] In certain cases example code does not see changes,"In a 3-pod election, when deleting leader, one of two other nodes becomes a leader. But the 3rd node, which was not a leader and did not become a leader, will still return old leader information:
```
(.venv) vagrant@node1:~/nextgen$ kubectl get pods -o wide
NAME                    READY     STATUS    RESTARTS   AGE       IP               NODE
etcd-4126467301-5frzt   1/3       Running   0          18s       10.233.68.234    node3
etcd-4126467301-fht8r   1/3       Running   0          18s       10.233.72.168    node2
etcd-4126467301-g3k5n   1/3       Running   0          18s       10.233.111.183   node1
(.venv) vagrant@node1:~/nextgen$ for pod in `kubectl get pods -o template --template='{{ range .items }}{{.metadata.name}} {{end}}'`; do kubectl exec -ti $pod -c etcd curl localhost:4040; done
{""name"":""10.233.111.183""}{""name"":""10.233.111.183""}{""name"":""10.233.111.183""}
```

After delete:
```
vagrant@node1:~/nextgen$ kubectl delete pods etcd-4126467301-g3k5n
(.venv) vagrant@node1:~/nextgen$ for pod in `kubectl get pods -o template --template='{{ range .items }}{{.metadata.name}} {{end}}'`; do kubectl exec -ti $pod -c etcd curl localhost:4040; done
{""name"":""10.233.111.183""}{""name"":""10.233.72.168""}{""name"":""10.233.72.168""}
```",closed,False,2016-12-30 11:28:43,2018-02-17 19:29:01
contrib,amnk,https://github.com/kubernetes/contrib/pull/2235,https://api.github.com/repos/kubernetes/contrib/issues/2235,Fixes #2234,,closed,True,2016-12-30 11:30:44,2017-04-19 18:50:51
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2236,https://api.github.com/repos/kubernetes/contrib/issues/2236,Cluster-autoscaler: add coming nodes to estimators,"Ref: #2228 #2229

cc: @fgrzadkowski @jszczepkowski @piosz",closed,True,2016-12-30 14:22:23,2017-01-11 18:18:07
contrib,omerzach,https://github.com/kubernetes/contrib/issues/2237,https://api.github.com/repos/kubernetes/contrib/issues/2237,Annotations in `kubectl describe ing` don't have consistent ordering,I'm running `watch kubectl describe ing` and my annotations keep shuffling around. Would it be reasonable to display them in alphabetical order? Have barely touched Kubernetes source code but happy to work out a pull request if that change sounds good.,closed,False,2016-12-31 10:34:31,2018-08-16 23:39:37
contrib,archerbj,https://github.com/kubernetes/contrib/issues/2238,https://api.github.com/repos/kubernetes/contrib/issues/2238,Using Ingress for spark web-ui can't load the static files,"```
kubernetes ： 1.2 
ingress-controller: gcr.io/google_containers/nginx-ingress-controller:0.8.3
spark: 2.0.0
```
ingress yaml 
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: spark-ingress
  namespace: default
spec:
  rules:
  - host: k8s.spark.com
    http:
      paths:
      - path: /ui
        backend:
          serviceName: spark-master-service
          servicePort: 8080
```


Hi , I was about to using ingress to expose my spark-web-ui from kubernetes at a specific hostname like `http://k8s.spark.com/ui`

previously using service to do so works just fine. 

when i switch to use ingress , it seems the ui tried to load the static files from the host i specified in ingress rules just like below 

![screenshot](https://cloud.githubusercontent.com/assets/14799336/21586061/ea517214-d106-11e6-8395-7b2b0499a4b3.png)



",closed,False,2017-01-02 08:25:13,2017-12-21 02:45:58
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2239,https://api.github.com/repos/kubernetes/contrib/issues/2239,Cluster-autoscaler: support unready nodes in scale down,"Use a different unneeded time threshold for unready nodes (to either allow debugging or make node/cluster healing faster).

Ref: #2228 #2229 

cc: @jszczepkowski @fgrzadkowski @piosz ",closed,True,2017-01-02 16:37:32,2017-01-03 13:53:55
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2240,https://api.github.com/repos/kubernetes/contrib/issues/2240,Cluster-autoscaler: include unregistered nodes in proposal,cc: @jszczepkowski @piosz @fgrzadkowski ,closed,True,2017-01-03 14:33:34,2017-01-04 14:44:08
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2241,https://api.github.com/repos/kubernetes/contrib/issues/2241,Cluster-autoscaler: add get nodes function to cloud provider interface,"This is needed to compare the list of nodes created by the cloud provider with the list of nodes registered in Kubernetes.

Ref: #2228 #2229

cc: @jszczepkowski @andrewsykim @piosz @fgrzadkowski ",closed,True,2017-01-03 16:18:05,2017-01-04 13:24:06
contrib,rikatz,https://github.com/kubernetes/contrib/pull/2242,https://api.github.com/repos/kubernetes/contrib/issues/2242,Updates the TempFile Location from NGINX Ingress Controller,"This commit updates the TempFile Location from SSL Certificates. This is necessary, as the os.Rename directive doesn't work on different mount points.

As this program is being used also on bare metal machines (and not only containers), the temp file should be created on the same directory of the final SSL certificates.",closed,True,2017-01-03 19:31:28,2017-01-04 17:09:32
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/2243,https://api.github.com/repos/kubernetes/contrib/issues/2243,ansible: restore selinux to enforcing,"Rather than setting selinux to permissive on all nodes, leave selinux enforcing and specify the unconfined spc_t container type for the kube-addons containers. For more on spc_t, see: http://danwalsh.livejournal.com/74754.html. See also https://github.com/kubernetes/kubernetes/pull/37327.

I tested this with centos atomic host.",closed,True,2017-01-03 19:48:50,2017-01-04 17:03:43
contrib,jrynyt,https://github.com/kubernetes/contrib/issues/2244,https://api.github.com/repos/kubernetes/contrib/issues/2244,[GLBC] purpose of backend.servicePort,"An Ingress in GKE creates an HTTPS GCE Load Balancer, the Backend Service for which targets the Service's NodePorts. So what is the purpose of `servicePort`?

The only instances in the code are examples, except for the note in `examples/health_checks/README.md`. If it's only used with readiness probes, that should be mentioned here https://github.com/kubernetes/contrib/blame/master/ingress/controllers/gce/README.md#L54.

Otherwise, it seems that the Service name should be all that's needed to locate the NodePort.
```
$ grep -r servicePort ingress/controllers/gce/
ingress/controllers/gce//BETA_LIMITATIONS.md:    servicePort: 80
ingress/controllers/gce//BETA_LIMITATIONS.md:    servicePort: 80
ingress/controllers/gce//examples/health_checks/health_check_app.yaml:          servicePort: 80
ingress/controllers/gce//examples/health_checks/health_check_app.yaml:          servicePort: 80
ingress/controllers/gce//examples/health_checks/health_check_app.yaml:          servicePort: 80
ingress/controllers/gce//examples/health_checks/README.md:* The readiness probe must be exposed on the port matching the `servicePort` specified in the Ingress
ingress/controllers/gce//examples/https/tls-app.yaml:    servicePort: 80
ingress/controllers/gce//ingress-app.yaml:    servicePort: 80
ingress/controllers/gce//ingress-app.yaml:          servicePort: 80
ingress/controllers/gce//ingress-app.yaml:          servicePort: 80
ingress/controllers/gce//ingress-app.yaml:          servicePort: 80
ingress/controllers/gce//README.md:12.          servicePort: 80
ingress/controllers/gce//README.md:          servicePort: 80
ingress/controllers/gce//README.md:          servicePort: 80
ingress/controllers/gce//README.md:    servicePort: 80
ingress/controllers/gce//README.md:    servicePort: 80
ingress/controllers/gce//README.md:    servicePort: 80
```",closed,False,2017-01-03 19:49:37,2017-01-25 21:15:28
contrib,zmerlynn,https://github.com/kubernetes/contrib/pull/2245,https://api.github.com/repos/kubernetes/contrib/issues/2245,SQ: Add kops-aws back to blocking,"Since I attempted this last, we have..
- logging on Up failures as well
- An AWS resource janitor to make sure we don't explode completely

This is a follow-on to kubernetes/test-infra#1471, which reenables reporting of the PR builder. ~~I'm waiting for the latest failure to finish clearing before merging that one, and I'll wait for the PR builder to go (more) green before merging this. (The `kops-aws-updown` job has cleared already, the `kops-aws` job should be green in the next half hour.)~~

I'd like to get this into blocking today, since basically after every weekend I end up fighting the next breakage in `kops-aws`.

Along the way: Reformat `required-retest-contexts` as well (separate commit).",closed,True,2017-01-03 19:55:30,2017-01-03 21:03:59
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2246,https://api.github.com/repos/kubernetes/contrib/issues/2246,Cluster-autoscaler: introduce ClusterStateRegistry to main,"Ref: #2228 #2229 

cc: @jszczepkowski @fgrzadkowski @piosz  ",closed,True,2017-01-03 20:36:59,2017-01-04 15:34:11
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/2247,https://api.github.com/repos/kubernetes/contrib/issues/2247,Cluster-autoscaler: reorganize and reevaluate all constants ,"The effort for #2228 will bring lots of constants/flags to the project. This issue is a reminder to revisit them once the main code is complete and re-validate if they all make sense as a whole.

cc: @fgrzadkowski @jszczepkowski ",closed,False,2017-01-03 20:45:49,2018-02-17 20:30:01
contrib,krzyzacy,https://github.com/kubernetes/contrib/pull/2248,https://api.github.com/repos/kubernetes/contrib/issues/2248,Add a comment for people to be aware of sq-blocking dashboatd in testgrid,"ref https://github.com/kubernetes/test-infra/pull/1459

cc @fejta ",closed,True,2017-01-04 06:44:27,2017-01-20 20:29:15
contrib,Tedezed,https://github.com/kubernetes/contrib/issues/2249,https://api.github.com/repos/kubernetes/contrib/issues/2249,[Error] Getsockopt: connection refused - Kubernetes apiserver,"### Environment
* Vagrant Node master Ubuntu 14 TLS - 192.168.33.10
* Vagrant Node minion Ubuntu 14 TLS - 192.168.33.11
* Deployed with ansible

Hi friends, I need help to solve this error.


#### My error: `Reason: Get https://10.254.0.1:443/version: dial tcp 10.254.0.1:443: getsockopt: connection refused`

Pods
```
NAME                                          READY     STATUS             RESTARTS   AGE
elasticsearch-logging-v1-4721j                1/1       Running            0          1h
elasticsearch-logging-v1-wyukc                1/1       Running            0          1h
fluentd-es-v1.20-5qpw7                        1/1       Running            0          1h
heapster-v1.0.2-2780992708-y0qbz              4/4       Running            0          1h
kibana-logging-v1-z9ffo                       0/1       CrashLoopBackOff   24         1h
kube-dns-v11-4x32n                            3/4       CrashLoopBackOff   32         1h
kubedash-vffq8                                1/1       Running            0          1h
kubernetes-dashboard-v1.1.0-ffgck             0/1       CrashLoopBackOff   26         1h
monitoring-influxdb-grafana-v3-cz6hm          2/2       Running            0          1h
traefik-ingress-controller-2249976834-v2aco   1/1       Running            0          1h
```
Service API Kubernetes
```
kubernetes   10.254.0.1   <none>        443/TCP   1h
```

#### Kube-system Errors
DNS
```getsockopt: connection refused
Readiness probe failed: Get http://172.16.55.6:8081/readiness: dial tcp 172.16.55.6:8081: getsockopt: connection refused
Liveness probe failed: HTTP probe failed with statuscode: 503
{kubelet 192.168.33.11}	spec.containers
(events with common reason combined)
Back-off restarting failed docker container
Error syncing pod, skipping: failed to ""StartContainer"" for ""kube2sky"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=kube2sky pod=kube-dns-v11-4x32n_kube-system(dfe0d4b5-d27a-11e6-b4d9-0800277e8445)""

```

Dashboard
```
Starting HTTP server on port 9090
Creating API server client for https://10.254.0.1:443
Error while initializing connection to Kubernetes apiserver. This most likely means that the cluster is misconfigured (e.g., it has invalid apiserver certificates or service accounts configuration) or the --apiserver-host param points to a server that does not exist. Reason: Get https://10.254.0.1:443/version: dial tcp 10.254.0.1:443: getsockopt: connection refused
```

Traefik-ingress-controller
```
msg=""Error watching kubernetes events: failed to create watch: failed to do version request: GET \""https://10.254.0.1:443/apis/extensions/v1beta1/ingresses\"" : failed to create request: GET \""https://10.254.0.1:443/apis/extensions/v1beta1/ingresses\"" : [Get https://10.254.0.1:443/apis/extensions/v1beta1/ingresses: dial tcp 10.254.0.1:443: getsockopt: connection refused]""
```

#### My tests
```
 kubectl exec test-701078429-3dzbj -- curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt -H  ""Authorization: Bearer $TOKEN_VALUE"" https://10.254.0.1
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to 10.254.0.1 port 443: Connection refused
```
```
curl --cacert /etc/kubernetes/certs/ca.crt https://192.168.33.10
Unauthorized
```
```
curl http://127.0.0.1:8080/version
{
  ""major"": ""1"",
  ""minor"": ""4"",
  ""gitVersion"": ""v1.4.5"",
  ""gitCommit"": ""5a0a696437ad35c133c0c8493f7e9d22b0f9b81b"",
  ""gitTreeState"": ""clean"",
  ""buildDate"": ""2016-10-29T01:32:42Z"",
  ""goVersion"": ""go1.6.3"",
  ""compiler"": ""gc"",
  ""platform"": ""linux/amd64""
}
```

* Errors  in service account, admission-control, flannel or docker net?
",closed,False,2017-01-04 14:20:38,2018-11-16 11:57:22
contrib,mengqiy,https://github.com/kubernetes/contrib/issues/2250,https://api.github.com/repos/kubernetes/contrib/issues/2250,Reviewer assigned by the bot should not be the author,"In kubernetes/kubernetes#39401, the k8s-merge-robot  assigned a reviewer who is the PR author.
",closed,False,2017-01-04 17:34:38,2018-02-18 17:51:01
contrib,rimusz,https://github.com/kubernetes/contrib/issues/2251,https://api.github.com/repos/kubernetes/contrib/issues/2251,Cluster-autoscaler: resource slack for faster pod startup,"hey there, we are happily using https://github.com/kubernetes/contrib/tree/master/cluster-autoscaler on AWS, is any way that scaler can scale two nodes instead of one? sometimes our apps do not get deployed via Deis Workflow which checks for the app deployment status.

Basically to have some spare node in ASG.

cc @mwielgus ",closed,False,2017-01-04 20:31:43,2018-07-13 10:58:13
contrib,spxtr,https://github.com/kubernetes/contrib/pull/2252,https://api.github.com/repos/kubernetes/contrib/issues/2252,Switch to new batch URL in submit queue.,,closed,True,2017-01-04 23:53:39,2017-01-06 22:04:06
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2253,https://api.github.com/repos/kubernetes/contrib/issues/2253,Cluster-autoscaler: check if cluster is healthy and add new node lister,"Ref: #2228 #2229

cc: @jszczepkowski @fgrzadkowski @piosz",closed,True,2017-01-05 10:24:33,2017-01-05 11:23:51
contrib,damiendurant,https://github.com/kubernetes/contrib/pull/2254,https://api.github.com/repos/kubernetes/contrib/issues/2254,Update nginx.tmpl,Disable server_tokens (remove nginx version from error pages and headers),closed,True,2017-01-05 10:41:42,2017-01-05 12:19:53
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2255,https://api.github.com/repos/kubernetes/contrib/issues/2255,Cluster-autoscaler: add stop channel to listers,So that the whole CA loop can be easily stopped and restarted with new settings. This will be handy for ConfigMap PR: https://github.com/kubernetes/contrib/pull/2226,closed,True,2017-01-05 12:01:14,2017-01-05 12:53:54
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2256,https://api.github.com/repos/kubernetes/contrib/issues/2256,[Ansible] Updated the kubernetes addons,"This PR contains the following changes for the Ansible deployment:
* Updated the kubernetes addons
* Added node-problem-detector addon

***

Please let me know if there is anything I can improve in the PR.",closed,True,2017-01-05 17:57:47,2017-01-21 13:48:05
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2257,https://api.github.com/repos/kubernetes/contrib/issues/2257,Cluster-autoscaler: propagate upcoming nodes to estimators in scale-up,"Ref: #2228 #2229

cc: @jszczepkowski @fgrzadkowski @piosz",closed,True,2017-01-05 20:38:41,2017-01-09 11:39:19
contrib,tyranron,https://github.com/kubernetes/contrib/issues/2258,https://api.github.com/repos/kubernetes/contrib/issues/2258,[nginx-ingress-controller] Proxy protocol for TCP services,"There is a possibility to [use Proxy protocol](https://github.com/kubernetes/contrib/tree/99cba026d6c42d08b944aae1fd9fac173d76e3a1/ingress/controllers/nginx#proxy-protocol) when Nginx accepts traffic from L4 proxy.

But is there a possibility to enable Proxy protocol for [exposed TCP services](https://github.com/kubernetes/contrib/tree/99cba026d6c42d08b944aae1fd9fac173d76e3a1/ingress/controllers/nginx#exposing-tcp-services) via Nginx Ingress Controller, so those services will receive a real client IP?

Something like following HAProxy configuration:
```
    frontend ft_pop3_secure
        bind :995
        mode tcp
        log global
        option tcplog
        default_backend bk_pop3_secure_dovecot

    backend bk_pop3_secure_dovecot
        mode tcp
        log global
        option tcplog
        server dovecot server.mail.svc.cluster.local:995 send-proxy-v2
```",closed,False,2017-01-06 08:42:22,2018-01-05 16:32:12
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2259,https://api.github.com/repos/kubernetes/contrib/issues/2259,[WIP] first pass at autoclosing issues without required labels,"To improve project velocity, we can and should required that people can set certain labels and take certain actions when creating an issue or PR. 

For Issues:
- [ ] Make sure people can set an area (routing) label when creating issue (maybe with prow or issue template)
- [ ] Close issues without area label
- [ ] Notify issue creator how they can set the label and reopen the issue

For Issues:
- [ ] Make sure people can set an release-note and release-note-label when creating a PR (possible with feature template and prow)
- [ ] Close PRs without release-note labels set
- [ ] notify PR author how they can set the label and reopen the PR

This PR serves the closing and notifying purposes.  I'm hoping we can also establish a good way to make sure that contributors have the ability to set the appropriate label and are encouraged to do so.

cc @apelisse @foxish @bgrant0607 ",closed,True,2017-01-07 00:20:55,2017-02-07 23:33:52
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2260,https://api.github.com/repos/kubernetes/contrib/issues/2260,Cluster-autoscaler: skip unready node groups in scale up,"Ref: #2228 #2229

cc: @jszczepkowski @fgrzadkowski @piosz",closed,True,2017-01-09 13:46:37,2017-01-09 14:19:22
contrib,Dmitry1987,https://github.com/kubernetes/contrib/issues/2261,https://api.github.com/repos/kubernetes/contrib/issues/2261,[service-loadbalancer] HAproxy restarts all the time,"I've noticed an issue with this pod,
it shows this and restarts haproxy in a loop:
```
service_loadbalancer.go:338] haproxy --
service_loadbalancer.go:536] Sync triggered by service kube-system/kube-controller-manager
service_loadbalancer.go:497] Found service: {Name:nginx-service-1:90 Ep:[10.42.113.224:80] BackendPort:80 FrontendPort:80 Host: SslTerm:false AclMatch: Algorithm:roundrobin SessionAffinity:true CookieStickySession:false}
service_loadbalancer.go:497] Found service: {Name:nginx-service-2:90 Ep:[10.42.38.98:80] BackendPort:80 FrontendPort:80 Host: SslTerm:false AclMatch: Algorithm:roundrobin SessionAffinity:true CookieStickySession:false}
service_loadbalancer.go:497] Found service: {Name:etcdbrowser Ep:[10.42.3.112:8080] BackendPort:8080 FrontendPort:80 Host: SslTerm:false AclMatch: Algorithm:roundrobin SessionAffinity:true CookieStickySession:false}
service_loadbalancer.go:436] No endpoints found for service randomtest, port {Name: Protocol:TCP Port:3000 TargetPort:{Type:0 IntVal:3000 StrVal:} NodePort:31999}
service_loadbalancer.go:436] No endpoints found for service kubernetes, port {Name:https Protocol:TCP Port:443 TargetPort:{Type:0 IntVal:443 StrVal:} NodePort:0}
service_loadbalancer.go:338] haproxy --
```
what might be the reason for constant restarts? (i see in ""ps"" that haproxy process ID changes every moment, so guessed it gets restarted all the time)
kubernetes ingress and services remain the same. I've thrown a few examples and a random empty service definition into it, and that's it. Just to see how it renders HAproxy config file.

The log displays ""Sync triggered by service kube-system/kube-controller-manager""  but I have nothing in ""kube-system"" namespace, because run this on Rancher, so I have all stuff in ""default"" namespace, and an ""empty"" kube-system :)  maybe this what caused restarts?

Also, I don't see hostnames in HAproxy config, only URI, but doesn't it support Ingress host based routing?
",closed,False,2017-01-09 15:08:06,2017-08-02 20:44:13
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2262,https://api.github.com/repos/kubernetes/contrib/issues/2262,Cluster-autoscaler: Add information how long node group incorrect size persisted," Ref: #2228 #2229

cc: @jszczepkowski @fgrzadkowski @piosz
",closed,True,2017-01-09 16:29:16,2017-01-10 13:52:18
contrib,peterj,https://github.com/kubernetes/contrib/issues/2263,https://api.github.com/repos/kubernetes/contrib/issues/2263,[nginx-ingress-controller] Combine upstreams if hosts are the same,"I have two services deployed in two different namespaces:
```
namespace-1   service-a
namespace-2   service-a                
```

I've also deployed Ingress rules to both namespaces to route requests from ""www.example.com"" to ""service-a"": 

```
  ""host"": ""www.example.com"",
                ""http"": {
                    ""paths"": [
                        {
                            ""backend"": {
                                ""serviceName"": ""service-a"",
                                ""servicePort"": 80
                            },
                            ""path"": ""/""
                        }
...
```
I can ping both ""service-a.namespace-1"" and ""service-a.namespace-2"". I've deployed the nginx ingress controller and I get two separate different upstreams: 

```
upstream namespace-1-service-a {
   ...
}

upstream namespace-2-service-a {
....
}
```

And the server block for ""www.example.com"" is doing a proxy_pass to the upstream that was defined last (e.g. if I deployed namespace1 first and then namespace2, I get the ```proxy_pass http://namespace-2-service-a```). 

My expectation was if the host names in the ingress are the same (www.example.com), I would only get 1 upstream that contains endpoints for pods in both namespaces. 

Am I missing something or is this just not supported or I am doing it completely wrong?  ",closed,False,2017-01-09 22:23:39,2018-02-17 23:33:02
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2264,https://api.github.com/repos/kubernetes/contrib/issues/2264,Turn on approvers for contrib,,closed,True,2017-01-09 22:53:44,2017-01-17 19:44:01
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2265,https://api.github.com/repos/kubernetes/contrib/issues/2265,[Mungegithub] Add Additional Mungegithub Documentation,"add documentation for deploying, approval-handler and issue-triager",closed,True,2017-01-09 23:25:15,2017-01-10 01:07:00
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2266,https://api.github.com/repos/kubernetes/contrib/issues/2266,turn off approvers mechanism,,closed,True,2017-01-10 01:25:55,2017-01-10 02:22:12
contrib,aledbf,https://github.com/kubernetes/contrib/pull/2267,https://api.github.com/repos/kubernetes/contrib/issues/2267,[ubuntu-slim]: Remove sh sed replacement,,closed,True,2017-01-10 02:00:43,2017-01-10 02:51:15
contrib,zonorti,https://github.com/kubernetes/contrib/pull/2268,https://api.github.com/repos/kubernetes/contrib/issues/2268,Update peer-finder version,"Required to update peer-finder container and binary.
Follow up for https://github.com/kubernetes/contrib/pull/2013 
By request from https://github.com/kubernetes/charts/commit/a5e1b12e9605de227f330a40a3dedcd5f1e5c6c2#commitcomment-20396288",closed,True,2017-01-10 09:07:22,2017-07-28 14:29:05
contrib,dminkovsky,https://github.com/kubernetes/contrib/pull/2269,https://api.github.com/repos/kubernetes/contrib/issues/2269,Fix StatefulSets: Zookeeper pod affinity,,closed,True,2017-01-10 19:07:36,2017-01-10 19:42:20
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2270,https://api.github.com/repos/kubernetes/contrib/issues/2270,[mungegithub] Edit the approval notification,"- rather than create a new approval notification and potentially spam
  PRs, use edit api functionality to edit original comment",closed,True,2017-01-10 19:54:04,2017-01-10 21:52:23
contrib,ekuefler,https://github.com/kubernetes/contrib/pull/2271,https://api.github.com/repos/kubernetes/contrib/issues/2271,Fix Markdown formatting in README,,closed,True,2017-01-10 23:01:00,2017-01-11 15:32:34
contrib,ixdy,https://github.com/kubernetes/contrib/pull/2272,https://api.github.com/repos/kubernetes/contrib/issues/2272,Always --pull in docker build to ensure recent base images,x-ref https://github.com/kubernetes/kubernetes/pull/39695,closed,True,2017-01-10 23:12:09,2017-01-11 01:42:25
contrib,dminkovsky,https://github.com/kubernetes/contrib/pull/2273,https://api.github.com/repos/kubernetes/contrib/issues/2273,Fix StatefulSets: Zookeeper pod affinity,"According to docs, `requiredDuringSchedulingRequiredDuringExecution`
not yet implemented. Not working in 1.5.1.",closed,True,2017-01-11 02:24:39,2017-01-21 00:59:20
contrib,tangle329,https://github.com/kubernetes/contrib/pull/2274,https://api.github.com/repos/kubernetes/contrib/issues/2274,Fix format error,Signed-off-by: Tang Le <at28997146@163.com>,closed,True,2017-01-11 05:57:07,2017-01-11 06:59:22
contrib,tangle329,https://github.com/kubernetes/contrib/pull/2275,https://api.github.com/repos/kubernetes/contrib/issues/2275,Fix format error,,closed,True,2017-01-11 06:40:35,2017-01-11 07:00:54
contrib,tangle329,https://github.com/kubernetes/contrib/pull/2276,https://api.github.com/repos/kubernetes/contrib/issues/2276,Update README.md for format error.,Signed-off-by: Tang Le <at28997146@163.com>,closed,True,2017-01-11 07:45:24,2017-01-11 15:32:29
contrib,xialonglee,https://github.com/kubernetes/contrib/pull/2277,https://api.github.com/repos/kubernetes/contrib/issues/2277,make docker configuration  more robust,"this commit makes sure the line `. /run/flannel/subnet.env` was written before `DOCKER_OPTS...` in the docker configuration file

**why we need this**
1. People could install k8s without install docker by adding flag `--skip-tags=docker`
So here comes the potential problem, once the docker was installed before and configured, the prefix string `DOCKER_OPTS=` may already exsit , the string ` /run/flannel/subnet.env` may not exsit.
in this situation,  ` /run/flannel/subnet.env`  will written in the end of file after `DOCKER_OPTS=` so that the network configuration not work.

2. this commit will first remove strings that satisfied the regex pattern and then written the line we want and make lines sequence is right.",closed,True,2017-01-11 09:44:07,2017-01-25 13:29:35
contrib,tangle329,https://github.com/kubernetes/contrib/issues/2278,https://api.github.com/repos/kubernetes/contrib/issues/2278,[nginx-ingress-controller] Address delete failed when scale rc from 3 to 0,"When I scale rc from 3 to 0, I can observe update warning below.  Is there any cluster level lock to resolve this issue?
````[root@CDM3F15-209232195 ingress]# kubectl describe ingress
Name:                   echomap
Namespace:              default
Address:                10.209.232.195,10.209.232.194,10.209.232.196
Default backend:        default-http-backend:80 (<none>)
Rules:
  Host          Path    Backends
  ----          ----    --------
  foo.bar.com
                /foo    echoheaders-x:80 (<none>)
  bar.baz.com
                /bar    echoheaders-y:80 (<none>)
                /foo    echoheaders-x:80 (<none>)
Annotations:
Events:
  FirstSeen     LastSeen        Count   From                            SubObjectPath   Type            Reason  Message
  ---------     --------        -----   ----                            -------------   --------        ------  -------
  6m            6m              1       {nginx-ingress-controller }                     Normal          CREATE  default/echomap
  6m            6m              1       {nginx-ingress-controller }                     Normal          CREATE  ip: 10.209.232.194
  6m            6m              1       {nginx-ingress-controller }                     Normal          CREATE  default/echomap
  6m            6m              1       {nginx-ingress-controller }                     Normal          CREATE  default/echomap
  6m            6m              1       {nginx-ingress-controller }                     Normal          CREATE  ip: 10.209.232.196
  6m            6m              2       {nginx-ingress-controller }                     Normal          UPDATE  default/echomap
  6m            6m              1       {nginx-ingress-controller }                     Normal          CREATE  ip: 10.209.232.195
  6m            6m              3       {nginx-ingress-controller }                     Normal          UPDATE  default/echomap
  6m            6m              2       {nginx-ingress-controller }                     Normal          UPDATE  default/echomap
  **3m            3m              1       {nginx-ingress-controller }                     Warning         UPDATE  error: Operation cannot be fulfilled on ingresses.extensions ""echomap"": the object has been modified; please apply your changes to the latest version and try again**
  3m            3m              1       {nginx-ingress-controller }                     Normal          DELETE  ip: 10.209.232.194
  **3m            3m              1       {nginx-ingress-controller }                     Warning         UPDATE  error: Operation cannot be fulfilled on ingresses.extensions ""echomap"": the object has been modified; please apply your changes to the latest version and try again**
",closed,False,2017-01-11 10:39:28,2017-01-24 01:20:23
contrib,dhilipkumars,https://github.com/kubernetes/contrib/pull/2279,https://api.github.com/repos/kubernetes/contrib/issues/2279,Redis cache,"Hi

Initial PR for redis-cache (no persistence) with automatic master-slave promotion.  

If you think this looks okay, i will add the source-code and docker files for other utilities.  

for this petset to work, i have changed the peer-finder little bit, i think it will be better if we factor-out peer-finder as a package instead of a utility.  I could do this as a part of separate PR.

Thanks in Advance,
Dhilip
@erictune @bprashanth ",closed,True,2017-01-11 14:38:06,2017-01-13 20:08:17
contrib,bjoernhaeuser,https://github.com/kubernetes/contrib/pull/2280,https://api.github.com/repos/kubernetes/contrib/issues/2280,s/purgeInteval/purgeInterval,Use correct name for purgeInterval,closed,True,2017-01-11 17:12:15,2017-01-11 17:14:36
contrib,bjoernhaeuser,https://github.com/kubernetes/contrib/pull/2281,https://api.github.com/repos/kubernetes/contrib/issues/2281,s/purgeInteval/purgeInterval,Use correct name for purgeInterval,closed,True,2017-01-11 17:12:34,2017-05-20 18:19:07
contrib,yujuhong,https://github.com/kubernetes/contrib/issues/2282,https://api.github.com/repos/kubernetes/contrib/issues/2282,bot merges closed PRs ,"from https://github.com/kubernetes/kubernetes/issues/39742:
""The following PR had been closed and got merged, #39714. The merge job should make sure the PR is still open before merging.""",closed,False,2017-01-11 17:37:37,2017-12-20 07:15:39
contrib,andrenarchy,https://github.com/kubernetes/contrib/issues/2283,https://api.github.com/repos/kubernetes/contrib/issues/2283,[nginx-slim] update to nginx 1.11.8,nginx-slim is currently building nginx 1.11.6. Current is 1.11.8.,closed,False,2017-01-11 17:50:15,2017-01-13 12:26:47
contrib,andrenarchy,https://github.com/kubernetes/contrib/issues/2284,https://api.github.com/repos/kubernetes/contrib/issues/2284,[nginx-ingress-controller] update nginx-slim,"The nginx ingress controller currently pulls nginx-slim:0.9 (nginx 1.11.3) while nginx-slim:0.11 (nginx 1.11.6) is current. I suggest that nginx-slim is first updated to build nginx 1.11.8 (nginx-slim:0.12) before the ingress controller is updated, see https://github.com/kubernetes/contrib/issues/2283.",closed,False,2017-01-11 17:54:58,2018-02-18 09:43:01
contrib,rithujohn191,https://github.com/kubernetes/contrib/issues/2285,https://api.github.com/repos/kubernetes/contrib/issues/2285,[nginx-ingress-controller] question: Handling Custom error pages,"Is there a way to host custom html error pages without having to update the nginx config and just providing some sort of flag value? 

We are currently using (https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/nginx.tmpl) as the nginx config file.  

According to the documentation at https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx#custom-errors, in case of an error the response is obtained from the default backend. I tried updating this default backend image to a custom image that serves error pages and noticed that the default nginx error pages still get hosted. 
",closed,False,2017-01-11 19:02:55,2018-06-11 07:06:44
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2286,https://api.github.com/repos/kubernetes/contrib/issues/2286,Enabled approvers and turned of cache for contrib,"Looks like the http cache was responsible for the duplicate comments on contrib as ListComments() was returning an empty set of Comments on Issues where the bot had commented before.  This is not a permanent/good solution as we will hit the github rate limit on kubernetes/kubernetes but for the purposes of testing on contrib, it may be okay.

*Verified that the bot is no longer double commenting on PRs",closed,True,2017-01-11 22:48:12,2017-01-17 19:17:01
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2287,https://api.github.com/repos/kubernetes/contrib/issues/2287,Cluster-autoscaler: update unready nodes proposal,cc: @fgrzadkowski @jszczepkowski,closed,True,2017-01-11 23:55:56,2017-01-12 11:59:15
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/2288,https://api.github.com/repos/kubernetes/contrib/issues/2288,Fix http-cache,"For the `ListComments()` function at least, the cache seems to return an incorrect list of IssueComments or no comments at all.  This can result in the k8s-merge-robot spamming an issue since it cannot see that it has already written to the issue.",closed,False,2017-01-12 00:19:47,2017-01-21 01:43:51
contrib,jayme-github,https://github.com/kubernetes/contrib/pull/2289,https://api.github.com/repos/kubernetes/contrib/issues/2289,Allow new/changed scripts to run again,"Small change that allows to change scripts at node runtime and have
them run ""again"" without the need to manually change or remove $CHECKPOINT_PATH.",closed,True,2017-01-12 10:35:10,2017-05-29 10:22:50
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2290,https://api.github.com/repos/kubernetes/contrib/issues/2290,Cluster-autoscaler: return node names in format of gce://proj/zone/name from GceProvider,cc: @jszczepkowski  @fgrzadkowski ,closed,True,2017-01-12 10:45:14,2017-01-12 11:17:15
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2291,https://api.github.com/repos/kubernetes/contrib/issues/2291,Cluster-autoscaler: unregistered nodes in cluster state registry,"Ref: #2228 #2229

cc: @jszczepkowski @fgrzadkowski ",closed,True,2017-01-12 12:28:44,2017-01-12 17:50:08
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2292,https://api.github.com/repos/kubernetes/contrib/issues/2292,Cluster-autoscaler: remove nodes that have been unregistered for a long time,"Ref: #2228

cc: @jszczepkowski @fgrzadkowski",closed,True,2017-01-12 17:54:13,2017-01-13 13:01:37
contrib,rithujohn191,https://github.com/kubernetes/contrib/issues/2293,https://api.github.com/repos/kubernetes/contrib/issues/2293,[nginx-ingress-controller] question: port_in_redirect flag set to off by default,"I was trying to find some documentation as to why the port_in_redirect flag in the nginx.tmpl file is set to off by default when nginx has it as on (http://nginx.org/en/docs/http/ngx_http_core_module.html#port_in_redirect).

We have a use case where all we have to do is turn on port_in_redirect and currently it requires us to mount the entire nginx.tmpl.",closed,False,2017-01-12 17:59:19,2017-01-26 18:20:49
contrib,spxtr,https://github.com/kubernetes/contrib/pull/2294,https://api.github.com/repos/kubernetes/contrib/issues/2294,Add bazel job to nonblocking list.,Purposefully not blocking.,closed,True,2017-01-13 00:05:48,2017-01-13 00:13:19
contrib,tangle329,https://github.com/kubernetes/contrib/pull/2295,https://api.github.com/repos/kubernetes/contrib/issues/2295,"Fix issue when ""ingress.kubernetes.io/limit-rps"" enabled in annotations","For RPS it should use limit_req_zone.

Signed-off-by: Tang Le <at28997146@163.com>",closed,True,2017-01-13 00:45:05,2018-02-19 01:59:00
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2296,https://api.github.com/repos/kubernetes/contrib/issues/2296,Don't cache results across function calls for ListComments,"So good news, I found the problem with ListComments and it wasn't with the httpcache, with github's api, or the approval handler.

Turns out we had implemented an incorrect cache in the ListComments wrapper, and it was returning the result from call to the function with different options.

More Info:
1. The first time `ListComments` gets called in the approvers implementation (via `getCommentsAfterLastModified`) it has the since parameter set in the `IssueListCommentsOptions` 
1. The second time we call` ListComments()` with no parameters expecting ALL Comments on the PR but instead we get the results from the first call",closed,True,2017-01-13 02:22:44,2017-01-13 23:02:43
contrib,qrpike,https://github.com/kubernetes/contrib/issues/2297,https://api.github.com/repos/kubernetes/contrib/issues/2297,[nginx-ingress-controller] question: SSL on root domain and www.,"So I have a SSL wildcard cert and I was wondering do I have to define the routes for both non www and www domains? Or is there a way to define multiple hosts.

eg: 
```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: mysite.com
  annotations:
    kubernetes.io/tls-acme: ""true""
spec:
  tls:
    - secretName: mysite-tls
      hosts:
        - mysite.com
        - www.mysite.com
  rules:
    - host: mysite.com
      http:
        paths:
          - path: /
            backend:
              serviceName: mysite
              servicePort: 80
    - host: www.mysite.com
      http:
        paths:
          - path: /
            backend:
              serviceName: mysite
              servicePort: 80
```

Or is there a way to combine them? This is obviously a simple example but when you have many /routes it can get long and ugly.

Thanks,",closed,False,2017-01-13 04:43:02,2018-02-22 02:10:07
contrib,mqliang,https://github.com/kubernetes/contrib/pull/2298,https://api.github.com/repos/kubernetes/contrib/issues/2298,Ingress admin,,closed,True,2017-01-13 08:01:22,2017-01-13 08:01:36
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2299,https://api.github.com/repos/kubernetes/contrib/issues/2299,Cluster-autoscaler: clear to be deleted taints only at the startup,"We want to clean the taint only at the startup. Currently it will remove the taint also from nodes that have just been deleted but not marked as unready by node controller. 

cc: @jszczepkowski @fgrzadkowski ",closed,True,2017-01-13 13:21:28,2017-01-13 13:51:39
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2300,https://api.github.com/repos/kubernetes/contrib/issues/2300,Cluster-autoscaler: decrease target size function in cloud provider interface,"Sometimes the cloud provider has troubles with providing the requested amount of nodes either due to quota or to (temporary) technical limitations. In such a case CA should be able to change the previously made request without killing any of the existing nodes. 

@andrewsykim - will this PR work on AWS?

cc: @jszczepkowski @andrewsykim @fgrzadkowski ",closed,True,2017-01-13 16:45:30,2017-01-16 14:48:07
contrib,dhilipkumars,https://github.com/kubernetes/contrib/pull/2301,https://api.github.com/repos/kubernetes/contrib/issues/2301,redis cache Statefulset,"Hi

Initial PR for redis-cache statefulset

1) Automatic slave-promotion in case of master failure.
2) Around 3 times faster writes than the current [petset version](https://github.com/kubernetes/contrib/tree/master/pets/redis)
3) Extremely light weight, installs in around 30 - 45 secs
4) over all image size is less than 30MB.
5) Is a statefulset with anti affinity and PodDisruption Budget

#### Quick Benchmark comparing with and without persistence. 
Redis Petset with persistance enabled
```
$ k exec rediscli -- redis-benchmark -q -h rd-0.redis.default.svc.cluster.local -p 6379 -t set,get -n 100000 -d 100 -r 1000000
SET: 22026.43 requests per second
GET: 76161.46 requests per second
```
Redis Statefulset configured without persistance
```
$ k exec rediscli -- redis-benchmark -q -h cache-0.cache.default.svc.cluster.local -p 6379 -t set,get -n 100000 -d 100 -r 1000000
 SET: 60060.06 requests per second
 GET: 89285.71 requests per second
```

Thanks in Advance,
Dhilip

@erictune @kow3ns

PS: Was originall raised as https://github.com/kubernetes/contrib/pull/2279 ",closed,True,2017-01-13 20:06:08,2017-08-01 05:47:45
contrib,timothysc,https://github.com/kubernetes/contrib/issues/2302,https://api.github.com/repos/kubernetes/contrib/issues/2302,Have the bot apply sig label on sig mentions.,"I know folks are working on new submission template but it would likely save a bunch of time if the bot just applied the sig label on the sig mention.  For those that can't label it allows them to reach the right audience most of the time.  

@kubernetes/sig-contributor-experience-misc ",closed,False,2017-01-13 21:49:09,2017-01-21 01:44:24
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2303,https://api.github.com/repos/kubernetes/contrib/issues/2303,Shouldn't modify approvers comment,"Definitely want to modify the last notification from the bot and not the last time a person wrote ""/approve""",closed,True,2017-01-13 23:49:58,2017-01-14 00:32:45
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/2304,https://api.github.com/repos/kubernetes/contrib/issues/2304,Deterministically Choose Approvers,"Right now, choosing an approver is done by picking someone from the set of approvers in a non-deterministic manner.  When we cc people, let's try and do it in a way such the same people are always displayed 

Particularly annoying if the bot edits old comments and changes the people that are cc'd and history is lost
",closed,False,2017-01-13 23:52:04,2017-01-21 01:44:08
contrib,apelisse,https://github.com/kubernetes/contrib/pull/2305,https://api.github.com/repos/kubernetes/contrib/issues/2305,mungegithub: Write consistent approval OWNERS path,"Right now the path to the OWNERS file is not the same if it's been
approved or not (beside the italics vs bold).

One looks like this (with a link to the file):

- **mungegithub/OWNERS**

The other looks like this (without the link):

- ~~mungegithub~~ [apelisse]

Make it consistent by having (always with the link):

- **mungegithub/OWNERS**
- ~~mungegithub/OWNERS~~ [apelisse]",closed,True,2017-01-14 00:32:27,2017-01-15 14:07:52
contrib,warmchang,https://github.com/kubernetes/contrib/pull/2306,https://api.github.com/repos/kubernetes/contrib/issues/2306,link the kubernetes repo.,link the kubernetes repo.,closed,True,2017-01-14 12:32:17,2017-02-14 19:20:54
contrib,pedrosland,https://github.com/kubernetes/contrib/pull/2307,https://api.github.com/repos/kubernetes/contrib/issues/2307,[ingress/controllers/nginx] Fix nginx ingress whitelist in config map,"I was playing with the Nginx Ingress Controller and looking at using the `whitelist-source-range` annotation to restrict access to pods. The idea was that they would be ""secure by default"" by using the `whitelist-source-range` to the config map.

I forgot that source IPs are mangled on AWS and by default on other cloud providers so it didn't matter in the end.

This PR fixes the following error I found in the logs when trying to set the `whitelist-source-range` in the config map:
```
I0112 11:48:02.339121       1 utils.go:123] 1 error(s) decoding:

* 'whitelist-source-range': source data must be an array or slice, got string
```

Link to original feature PR #1141

I looked into the code and I could see why it was failing. I have kept the same style as the also undocumented `skip-access-log-urls`.

I've also documented `whitelist-source-range`. If this goes well, I'd be happy to update the docs for `skip-access-log-urls` and other options.

I hope I've followed the procedure correctly. This is my first contribution here.",closed,True,2017-01-15 16:07:16,2017-01-15 19:57:34
contrib,fate-grand-order,https://github.com/kubernetes/contrib/pull/2308,https://api.github.com/repos/kubernetes/contrib/issues/2308,fix a misspelling of independent in reports.go,,closed,True,2017-01-16 10:59:08,2017-01-16 16:08:10
contrib,roffe,https://github.com/kubernetes/contrib/issues/2309,https://api.github.com/repos/kubernetes/contrib/issues/2309,cannot build nginx-ingress controller,"I'm no pro at GO but i need to compile my own nginx controller to have multiple ones listening under different classes.

I have followed https://github.com/kubernetes/contrib#getting-the-code with no luck:

```
jk@ratch-linux:~/devel/nginx-internall-ingress$ export GOPATH=/home/jk/devel/nginx-internall-ingress
jk@ratch-linux:~/devel/nginx-internall-ingress$ mkdir -p $GOPATH/src/k8s.io
jk@ratch-linux:~/devel/nginx-internall-ingress$ cd $GOPATH/src/k8s.io
jk@ratch-linux:~/devel/nginx-internall-ingress/src/k8s.io$ git clone git@github.com:kubernetes/contrib.git
Cloning into 'contrib'...
remote: Counting objects: 38422, done.
remote: Total 38422 (delta 0), reused 0 (delta 0), pack-reused 38422
Receiving objects: 100% (38422/38422), 41.04 MiB | 8.69 MiB/s, done.
Resolving deltas: 100% (18590/18590), done.
Checking connectivity... done.
Checking out files: 100% (17134/17134), done.
jk@ratch-linux:~/devel/nginx-internall-ingress/src/k8s.io$ cd contrib/ingress
jk@ratch-linux:~/devel/nginx-internall-ingress/src/k8s.io/contrib/ingress$ godep restore
godep: Dep (golang.org/x/oauth2/google) restored, but was unable to load it with error:
	Package (google.golang.org/appengine) not found
godep: Error checking some deps.
jk@ratch-linux:~/devel/nginx-internall-ingress/src/k8s.io/contrib/ingress$ cd ..
jk@ratch-linux:~/devel/nginx-internall-ingress/src/k8s.io/contrib$ cd ..
jk@ratch-linux:~/devel/nginx-internall-ingress/src/k8s.io$ cd ..
jk@ratch-linux:~/devel/nginx-internall-ingress/src$ cd ..
jk@ratch-linux:~/devel/nginx-internall-ingress$ make controller
fatal: Not a git repository (or any of the parent directories): .git
rm -f nginx-ingress-controller
CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags \
	""-s -w -X main.version=git- -X main.gitRepo="" \
	-o nginx-ingress-controller
# _/home/jk/devel/nginx-internall-ingress
./controller.go:149: cannot use kubeClient (type *""k8s.io/kubernetes/pkg/client/unversioned"".Client) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/unversioned"".Client in argument to nginx.NewManager
./controller.go:462: cannot use cfg (type *""k8s.io/kubernetes/pkg/api"".ConfigMap) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/api"".ConfigMap in argument to lbc.nginx.ReadConfig
./controller.go:694: cannot use lbc.client (type *""k8s.io/kubernetes/pkg/client/unversioned"".Client) as type ""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/unversioned"".Interface in argument to auth.ParseAnnotations:
	*""k8s.io/kubernetes/pkg/client/unversioned"".Client does not implement ""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/unversioned"".Interface (wrong type for Apps method)
		have Apps() ""k8s.io/kubernetes/pkg/client/unversioned"".AppsInterface
		want Apps() ""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/unversioned"".AppsInterface
./controller.go:694: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to auth.ParseAnnotations
./controller.go:700: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to ""k8s.io/contrib/ingress/controllers/nginx/nginx/ratelimit"".ParseAnnotations
./controller.go:706: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to secureupstream.ParseAnnotations
./controller.go:711: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to rewrite.ParseAnnotations
./controller.go:716: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to ipwhitelist.ParseAnnotations
./controller.go:722: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to cors.ParseAnnotations
./controller.go:727: cannot use ing (type *""k8s.io/kubernetes/pkg/apis/extensions"".Ingress) as type *""k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/apis/extensions"".Ingress in argument to authreq.ParseAnnotations
./controller.go:727: too many errors
Makefile:14: recipe for target 'controller' failed
make: *** [controller] Error 2
```",closed,False,2017-01-16 13:12:08,2017-01-18 10:32:14
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2310,https://api.github.com/repos/kubernetes/contrib/issues/2310,Cluster-autoscaler: decrease node group target size if no new nodes arrive in reasonable time,cc: @jszczepkowski @andrewsykim @fgrzadkowski ,closed,True,2017-01-16 22:18:32,2017-01-17 09:38:17
contrib,fate-grand-order,https://github.com/kubernetes/contrib/pull/2311,https://api.github.com/repos/kubernetes/contrib/issues/2311,fix misspell initalize in predicates.go,,closed,True,2017-01-17 05:53:35,2017-01-20 19:34:23
contrib,ractive,https://github.com/kubernetes/contrib/issues/2312,https://api.github.com/repos/kubernetes/contrib/issues/2312,Ingress overwrites instead of adds X-Forwarded-* headers,"The ingress proxy sets the X-Forwarded-* headers and hereby overwrites previous values set by other proxies. So the application only sees the X-Forwarded-* header values set by the ingress, but not by the first proxy and can therefore not create URLs pointing to itself. The ingress proxy should either add these headers (using `add_header`) or add its value in a comma separated list to an existing header value. Unfortunately there's no standard describing these X-Forwarded headers, but it seems as if both variants (multiple headers and comma separated values) are seen in the wild.
The changes would be probably needed to be done here: https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/nginx.tmpl#L276

We concretely face this issue because we terminate all https connections on an external loadbalancer (e.g. https://api.example.com) that then routes the traffic via http to an ingress endpoint (e.g. http://internal.example.com). The redirect URI that is created by the internal app is now https://api.example.com:80 - which is wrong. This is because the ingress sets the X-Forwarded-Port header to 80. The X-Forwarded-Port value 443 which is set by the loadbalancer (where ssl is terminated) should be preserved.

This issue shows up in a Spring Boot application with OAuth2 protected resources, where the  app creates the redirect_uri pointing to itself sent to the OAuth provider using a UriComponentsBuilder [1] in the OAuth2ClientContextFilter [2]. The UriComponentsBuilder builds the URL using the X-Forwarded-* headers, picking the first header and the first value (if comma spearated).

[1] https://github.com/spring-projects/spring-framework/blob/master/spring-web/src/main/java/org/springframework/web/util/UriComponentsBuilder.java#L705

[2] https://github.com/spring-projects/spring-security-oauth/blob/master/spring-security-oauth2/src/main/java/org/springframework/security/oauth2/client/filter/OAuth2ClientContextFilter.java#L122
",closed,False,2017-01-17 11:33:06,2018-06-15 11:03:31
contrib,Dmitry1987,https://github.com/kubernetes/contrib/issues/2313,https://api.github.com/repos/kubernetes/contrib/issues/2313,Nginx ingress controller difference,"Hi, what is the difference between nginx controller in this repo, and this one:  https://github.com/nginxinc/kubernetes-ingress  (it's official from nginx company as I understand?)

which one better to use?",closed,False,2017-01-17 15:43:06,2018-02-18 12:46:01
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2314,https://api.github.com/repos/kubernetes/contrib/issues/2314,Cluster-autoscaler: add NodeReadyPredicate and allow unready nodes in CA,"Ref: #2228 

cc: @jszczepkowski @fgrzadkowski",closed,True,2017-01-17 16:19:17,2017-01-18 14:49:30
contrib,andrewsykim,https://github.com/kubernetes/contrib/pull/2315,https://api.github.com/repos/kubernetes/contrib/issues/2315,Cluster Autoscaler: update AWS docs to include expansion options ,"There seems to be some confusion among AWS users regarding how to setup cluster autoscaler with multiple node groups. 

/cc @mwielgus @osxi @mumoshu",closed,True,2017-01-17 16:36:53,2017-01-18 00:13:05
contrib,andrewsykim,https://github.com/kubernetes/contrib/issues/2316,https://api.github.com/repos/kubernetes/contrib/issues/2316,Cluster Autoscaler: Enable instance protection in AWS,"To ensure cluster-autoscaler will only terminate nodes that are valid candidates, we can enable instance protection on all nodes visible to cluster-autoscaler. This will give cluster-autoscaler a lot of flexibility regarding decreasing ASG sizes without accidentally terminating nodes that shouldn't be. 

related issue/PR: https://github.com/kubernetes/contrib/pull/2300
",closed,False,2017-01-17 16:41:56,2018-03-07 05:19:15
contrib,ethernetdan,https://github.com/kubernetes/contrib/pull/2317,https://api.github.com/repos/kubernetes/contrib/issues/2317,Add SIGs to testowners munger parsing,"This adds support for parsing the SIG column added in kubernetes/kubernetes#40031. It should not be merged until that PR is.

Later PRs will label flakes with the appropriate SIG.

",closed,True,2017-01-17 17:44:40,2017-02-03 23:53:39
contrib,andrewsykim,https://github.com/kubernetes/contrib/pull/2318,https://api.github.com/repos/kubernetes/contrib/issues/2318,cluster autoscaler: fix typos,/cc @mwielgus ,closed,True,2017-01-17 18:02:47,2017-01-21 00:47:39
contrib,iameli,https://github.com/kubernetes/contrib/pull/2319,https://api.github.com/repos/kubernetes/contrib/issues/2319,Non-idempotent typo,"Also make hyphenation consistent, if you're into that sort of thing",closed,True,2017-01-17 20:31:26,2017-04-04 15:13:01
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2320,https://api.github.com/repos/kubernetes/contrib/issues/2320,turn on approvers in contrib,,closed,True,2017-01-17 20:55:55,2017-01-18 15:30:41
contrib,andrewsykim,https://github.com/kubernetes/contrib/pull/2321,https://api.github.com/repos/kubernetes/contrib/issues/2321,Cluster Autoscaler should enable node protection for AWS,fixes https://github.com/kubernetes/contrib/issues/2316,closed,True,2017-01-18 02:36:00,2017-06-23 09:40:00
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/2322,https://api.github.com/repos/kubernetes/contrib/issues/2322,"[nginx-ingress-controller] Why is header ""connection: close"" is added when request is passed to upstream in Nginx ingress?","Hi,

With the Nginx ingress controller, the header ""connection: close"" is added when the request is passed to upstream. I read the upstream server log and find that. Why? What I hope is the original header relating to connection status should be passed to upstream server without any change.",closed,False,2017-01-18 14:02:29,2018-08-10 08:04:41
contrib,squat,https://github.com/kubernetes/contrib/pull/2323,https://api.github.com/repos/kubernetes/contrib/issues/2323,ingress/controller/nginx: ensure redirect to HTTPS,"Currently, the Nginx ingress controller will not redirect requests to
HTTPS even if TLS is enabled for that ingress if the ingress has no
annotations. This commit corrects this behavior to ensure that reqests
are redirected to HTTPS as stated in the Nginx ingress controller
README.",closed,True,2017-01-18 21:20:38,2017-01-24 01:10:42
contrib,squat,https://github.com/kubernetes/contrib/pull/2324,https://api.github.com/repos/kubernetes/contrib/issues/2324,ingress/controllers/nginx: fix typos in README.md,This commit fixes some typos and grammar in the nginx ingress controller README,closed,True,2017-01-18 23:13:42,2017-04-04 15:12:54
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2325,https://api.github.com/repos/kubernetes/contrib/issues/2325,Cluster-autoscaler: godeps update,"cc: @jszczepkowski @fgrzadkowski 

ref: https://github.com/kubernetes/contrib/pull/2146",closed,True,2017-01-19 17:36:02,2017-01-20 14:48:18
contrib,uthark,https://github.com/kubernetes/contrib/pull/2326,https://api.github.com/repos/kubernetes/contrib/issues/2326,Add go1.7 to list of supported version for go fmt.,,closed,True,2017-01-19 17:54:22,2017-01-19 18:19:31
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2327,https://api.github.com/repos/kubernetes/contrib/issues/2327,improve the logging and error messages in un/assign handler,"I noticed a few log messages that were wrong.  

Not sure if this is the best way to handle the error from `AssignPR` from the githubapi.  ",closed,True,2017-01-19 23:54:24,2017-01-20 21:39:08
contrib,kitch,https://github.com/kubernetes/contrib/issues/2328,https://api.github.com/repos/kubernetes/contrib/issues/2328,[keepalived-vip] multi-nic systems not supported,If the worker nodes of the cluster have multiple NICs there is currently no way to control where nic the VIP will be attached to. If there is a private (eth0) and public interface (eth1) for the worker there must be a way to choose which adapter to associate the VIP to. ,closed,False,2017-01-20 02:21:49,2019-01-03 05:06:41
contrib,chrismoos,https://github.com/kubernetes/contrib/pull/2329,https://api.github.com/repos/kubernetes/contrib/issues/2329,Add nginx_upstream_check_module.,This will be used by the NGINX ingress controller for performing upstream health checks.,closed,True,2017-01-20 05:21:20,2017-02-08 18:12:02
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2330,https://api.github.com/repos/kubernetes/contrib/issues/2330,Bump golang in travis to 1.7,"Kubernetes (which is a common dependency in contrib) needs go 1.7 to compile. Some files require core golang package ""context"" that is not present in 1.6.  

cc: @gmarek @fgrzadkowski ",closed,True,2017-01-20 11:35:43,2017-01-20 12:57:40
contrib,r0bj,https://github.com/kubernetes/contrib/issues/2331,https://api.github.com/repos/kubernetes/contrib/issues/2331,[nginx-ingress-controller] crash: fatal error: concurrent map writes,"OS: ubuntu 14.04
kubernetes: 1.5.2
docker: 1.12.6

I'm using nginx-ingress-controller as a UDP load balancer in kubernetes pod:
https://gist.github.com/r0bj/ada7be55dd65d2b2376396c42107ba0d

config map:
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: udp-lb-config
  namespace: kube-system
data:
  514: ""logs-ingress/logs-ingress:514""
```

nginx config:
https://gist.github.com/r0bj/cf0f590d2601443cd4bddbe4812dbe90

nginx-ingress-controller frequently (in my use case more or less once every hour) crash with error message:
```
fatal error: concurrent map writes

goroutine 950 [running]:
runtime.throw(0x1b3d330, 0x15)
	/usr/local/go/src/runtime/panic.go:547 +0x90 fp=0xc820226ab0 sp=0xc820226a98
runtime.mapassign1(0x148a4a0, 0xc8204ef110, 0xc820226bf8, 0xc820226c48)
	/usr/local/go/src/runtime/hashmap.go:445 +0xb1 fp=0xc820226b58 sp=0xc820226ab0
k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache.(*DeltaFIFO).queueActionLocked(0xc8200bb340, 0x1a3e8c8, 0x4, 0x19de680, 0xc8203da000, 0x0, 0x0)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache/delta_fifo.go:320 +0x5d1 fp=0xc820226cc0 sp=0xc820226b58
k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache.(*DeltaFIFO).Resync(0xc8200bb340, 0x0, 0x0)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache/delta_fifo.go:511 +0x4f7 fp=0xc820226e38 sp=0xc820226cc0
k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache.(*Reflector).ListAndWatch.func1(0xc8203b6060, 0xc8203b17a0, 0xc820264500, 0xc8203b0a20, 0xc8203b60b0)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache/reflector.go:289 +0x252 fp=0xc820226f88 sp=0xc820226e38
runtime.goexit()
	/usr/local/go/src/runtime/asm_amd64.s:1998 +0x1 fp=0xc820226f90 sp=0xc820226f88
created by k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache.(*Reflector).ListAndWatch
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/ingress/vendor/k8s.io/kubernetes/pkg/client/cache/reflector.go:296 +0xde3
```

Full error trace:
https://gist.github.com/r0bj/9027096fc1a511719cacb282e124398b",closed,False,2017-01-20 14:58:41,2018-02-18 13:47:02
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2332,https://api.github.com/repos/kubernetes/contrib/issues/2332,[mungegithub] AssignPR has same interface as Unassign,Make AssignPR a variadic function like UnassignPR,closed,True,2017-01-20 19:11:05,2017-01-20 21:36:11
contrib,foxish,https://github.com/kubernetes/contrib/issues/2333,https://api.github.com/repos/kubernetes/contrib/issues/2333,Move mungegithub to the test-infra repo,"All the bots can live in one place and we can have documentation for commands and such.

- [x] Preserve history during move
- [x] Open issues against mungegithub must be moved out to test-infra
- [x]  Redirect to new location

This should help new contributors trying to understand our bots and contribute code to them have one place to look.
Long term: eventually unify prow and mungegithub.

/cc @eparis @grodrigues3 @apelisse ",closed,False,2017-01-20 21:55:01,2017-12-20 17:45:26
contrib,grodrigues3,https://github.com/kubernetes/contrib/pull/2334,https://api.github.com/repos/kubernetes/contrib/issues/2334,allow maintainers to add approved label explicitly,"At least temporarily, we should allow people with label-adding privileges to add the approved label.

The process is designed so that only when all files are approved the approved label can be added to the bot but with this ""fix"", human's will be able to manually apply the approved label.",closed,True,2017-01-20 22:22:51,2017-01-24 03:22:39
contrib,foxish,https://github.com/kubernetes/contrib/pull/2335,https://api.github.com/repos/kubernetes/contrib/issues/2335,mungegithub moved to test-infra,"Leaving behind a readme to redirect in case we've linked to it from elsewhere

cc @kubernetes/contrib-maintainers @grodrigues3 @apelisse @spxtr ",closed,True,2017-01-21 01:08:20,2017-06-20 00:27:56
contrib,fate-grand-order,https://github.com/kubernetes/contrib/pull/2336,https://api.github.com/repos/kubernetes/contrib/issues/2336,"fix misspell ""contains"" in nginx/main.go",,closed,True,2017-01-22 13:41:29,2017-01-26 16:21:55
contrib,crassirostris,https://github.com/kubernetes/contrib/issues/2337,https://api.github.com/repos/kubernetes/contrib/issues/2337,[addon-resizer] Set logging levels to reduce amount of logs produced,"Currently, nanny produces a lot of meaningful messages by default, like

```go
log.Infof(""The number of nodes is %d"", num)
```

or

```go
log.Infof(""Resources are within the expected limits."")
```

IMO, logging levels should be set up according to [the developer's guide](https://github.com/kubernetes/community/blob/master/contributors/devel/logging.md) and such messages shouldn't be visible by default.

CC @piosz",closed,False,2017-01-22 15:03:58,2017-02-02 02:35:34
contrib,jrynyt,https://github.com/kubernetes/contrib/pull/2338,https://api.github.com/repos/kubernetes/contrib/issues/2338,Clarify usage of Ingress backend.servicePort,Resolves https://github.com/kubernetes/contrib/issues/2244,closed,True,2017-01-23 14:44:00,2017-01-25 14:54:34
contrib,danielmorlock,https://github.com/kubernetes/contrib/issues/2339,https://api.github.com/repos/kubernetes/contrib/issues/2339,[nginx-ingress-controller] Watch of *api.Endpoints/*.api.Secret ended with: too old resource version,"I created a nginx-ingress controller using the following https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx and get the following errors:

    2017-01-23T17:37:02.662496013Z W0123 17:37:02.662036       1 reflector.go:334] k8s.io/contrib/ingress/controllers/nginx/controller.go:1130: watch of *api.Endpoints ended with: too old resource version: 3579520 (3579966)
    2017-01-23T17:49:32.682430181Z W0123 17:49:32.681931       1 reflector.go:334] k8s.io/contrib/ingress/controllers/nginx/controller.go:1132: watch of *api.Secret ended with: too old resource version: 3135503 (3581872)

Kubernetes version:

    Client Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.1"", GitCommit:""82450d03cb057bab0950214ef122b67c83fb11df"", GitTreeState:""clean"", BuildDate:""2016-12-14T00:57:05Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}
    Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.1"", GitCommit:""82450d03cb057bab0950214ef122b67c83fb11df"", GitTreeState:""clean"", BuildDate:""2016-12-14T00:52:01Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}

Any ideas? How can I further investigate this issue?

",closed,False,2017-01-23 18:05:06,2017-01-23 19:53:30
contrib,christianhuening,https://github.com/kubernetes/contrib/pull/2340,https://api.github.com/repos/kubernetes/contrib/issues/2340,Updated the docs for custom configuration of nginx ingress controller…,"… to reflect changes in the argument name for configmap and ""body-size"" in the upcoming 0.9.0 release",closed,True,2017-01-24 07:21:42,2017-01-26 13:54:55
contrib,bmarks-mylo,https://github.com/kubernetes/contrib/issues/2341,https://api.github.com/repos/kubernetes/contrib/issues/2341,[nginx-ingress-controller] 401 unauthorized ,"I'm unable to get the nginx ingress controller up and running in one of my clusters.  It's getting a 401 unauthorized response when trying to communicate with the API.  I've tried deleting/recreating the default service account token but that has not changed the behavior.

Vitals:
```
$ kubectl version
Client Version: version.Info{Major:"""", Minor:"""", GitVersion:""v0.0.0-master+$Format:%h$"", GitCommit:""$Format:%H$"", GitTreeState:""not a git tree"", BuildDate:""1970-01-01T00:00:00Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""windows/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.7+coreos.0"", GitCommit:""0581d1a5c618b404bd4766544bec479aedef763e"", GitTreeState:""clean"", BuildDate:""2016-12-12T19:04:11Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

ingress controller config: 
```
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress-controller
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    nodePort: 32080
    protocol: TCP
    name: http
  selector:
    app: nginx-ingress-lb
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  labels:
    app: nginx-ingress-lb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-ingress-lb
  template:
    metadata:
      labels:
        app: nginx-ingress-lb
        name: nginx-ingress-lb
    spec:
      terminationGracePeriodSeconds: 60
      nodeSelector:
        nodenum: ""3""
      containers:
      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3
        name: nginx-ingress-lb
        imagePullPolicy: IfNotPresent
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 1
        # use downward API
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        ports:
        - containerPort: 80
          hostPort: 80
        args:
        - /nginx-ingress-controller
        - --default-backend-service=$(POD_NAMESPACE)/nginx-default-backend
        - --v=10
```

pod log
```
...
I0124 15:29:11.644810       1 round_trippers.go:299] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: nginx-ingress-controller/v0.0.0 (linux/amd64) kubernetes/$Format"" -H ""Authorization: Bearer *redacted*"" https://10.3.0.1:443/api/v1/namespaces/default/pods/nginx-ingress-controller-1650409119-4wl99
I0124 15:29:11.646205       1 round_trippers.go:318] GET https://10.3.0.1:443/api/v1/namespaces/default/pods/nginx-ingress-controller-1650409119-4wl99 401 Unauthorized in 1 milliseconds
I0124 15:29:11.646247       1 round_trippers.go:324] Response Headers:
I0124 15:29:11.646259       1 round_trippers.go:327]     Content-Type: text/plain; charset=utf-8
I0124 15:29:11.646267       1 round_trippers.go:327]     X-Content-Type-Options: nosniff
I0124 15:29:11.646275       1 round_trippers.go:327]     Content-Length: 13
I0124 15:29:11.646282       1 round_trippers.go:327]     Date: Tue, 24 Jan 2017 15:29:11 GMT
I0124 15:29:11.646330       1 request.go:891] Response Body: Unauthorized
I0124 15:29:11.646340       1 request.go:973] Response Body: Unauthorized
... <repeats>
```",closed,False,2017-01-24 15:33:36,2017-01-24 17:03:36
contrib,xialonglee,https://github.com/kubernetes/contrib/pull/2342,https://api.github.com/repos/kubernetes/contrib/issues/2342,rename skydns to kubedns,"`skydns` was no longer used, rename to `kubedns`.",closed,True,2017-01-25 02:56:57,2017-01-26 16:07:17
contrib,xialonglee,https://github.com/kubernetes/contrib/pull/2343,https://api.github.com/repos/kubernetes/contrib/issues/2343,support setup etcd cluster with diff interfaces,"Use fact ·ansible_default_ipv4.address· can setup etcd cluster on different network interfaces

for example
setup etcd cluster with 2 nodes A (default_ipv4.interface is `eth0`), B (default_ipv4.interface is `virbr0`, it's a virtual machine)
use fact `['ansible_' + etcd_interface]` will failed, variable `etcd_interface` is related to the host that is executing, and node A does not have `virbr0` and B does not have `eth0` respectively.
use fact `ansible_default_ipv4` can solve this.

\======================

The method solve problem above was outdated.
Adapt @rutsky 's suggestion, simply using `if` statement to solve the problem.",closed,True,2017-01-25 09:50:42,2017-02-15 15:46:04
contrib,ghost,https://github.com/kubernetes/contrib/pull/2344,https://api.github.com/repos/kubernetes/contrib/issues/2344,Add cmdline flag --watch-all-namespaces to kube-keepalived-vip,This is a workaround for clientcmd not allowing --namespace= to override the cluster context namespace (e.g. POD_NAMESPACE) to listen for services on all namespaces.,closed,True,2017-01-25 12:43:30,2017-05-07 21:01:55
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2345,https://api.github.com/repos/kubernetes/contrib/issues/2345,Set verbosity levels for addon-resizer,"Fix https://github.com/kubernetes/contrib/issues/2337

Set verbosity levels according to the [developer's logging guide](https://github.com/kubernetes/community/blob/master/contributors/devel/logging.md)

CC @Q-Lee @piosz ",closed,True,2017-01-25 16:34:36,2017-01-27 22:00:09
contrib,aledbf,https://github.com/kubernetes/contrib/pull/2346,https://api.github.com/repos/kubernetes/contrib/issues/2346,Update nginx to 1.11.9,,closed,True,2017-01-25 18:11:26,2017-01-31 17:17:20
contrib,Capitrium,https://github.com/kubernetes/contrib/issues/2347,https://api.github.com/repos/kubernetes/contrib/issues/2347,Cluster Autoscaler: Add Least-Cost Node Group Expander Strategy,"This expands upon issue #1921 and PR #2118, which added support for node group expansion strategies which allow the cluster autoscaler to determine the ideal node group to expand instead of just selecting a node group at random. Adding a strategy that selects the node group to expand based on the cost of nodes within each configured group allows us to better take advantage of cloud provider discount options (i.e. AWS Spot Instances), thereby minimizing the cost of running clusters.",closed,False,2017-01-26 01:45:59,2017-06-22 01:11:25
contrib,Capitrium,https://github.com/kubernetes/contrib/pull/2348,https://api.github.com/repos/kubernetes/contrib/issues/2348,Add Least-Cost Node Group Expander,"#2118 added support for node group expansion strategies, allowing the cluster autoscaler to determine the best node group to expand instead of just selecting a node group at random. This adds a new expansion strategy that selects the node group with the lowest price per node as the group to be expanded, falling back to random selection if the price per node is equal across all configured node groups.

The main purpose of this PR is to improve support for AWS spot instance nodes; as GCE does not provide a bidding . Consider the following snippet from a cluster-autoscaler deployment manifest:
```
        - ./cluster-autoscaler
        - --v=4
        - --cloud-provider=aws
        - --expander=least-cost
        - --nodes=1:10:spot-node-1
        - --nodes=1:10:spot-node-2
        - --nodes=1:10:spot-node-3
        - --nodes=1:10:spot-node-4
        - --nodes=1:10:on-demand-node:0.10
```
Each of the spot worker groups spans a single Availability Zone on AWS. As instances are added to the spot worker groups, the current market price for spot instances in a given AZ is likely to change. Using the least-cost expander allows the cluster autoscaler to take the market price of spot instances into account when scaling, causing it to select the node group with the cheapest spot price at the time the scale-up was requested.

Note the additional on-demand node group; since there is no simple API for getting the current on-demand price of an AWS instance type, the node cost is manually specified as part of the node group. This way, if the spot price in all AZs is higher than the max bid price configured for the spot node groups, the cluster autoscaler can fall back to the on-demand node group.

Since GCE doesn't use a variable pricing model for their discounted Preemptible VMs, configuring the cluster autoscaler to add support for them should be trivial:
```
        - ./cluster-autoscaler
        - --v=4
        - --cloud-provider=gce
        - --expander=least-cost
        - --nodes=1:10:preemptible-node-1:0.20
        - --nodes=1:10:preemptible-node-2:0.20
        - --nodes=1:10:preemptible-node-3:0.20
        - --nodes=1:10:preemptible-node-4:0.20
        - --nodes=1:10:regular-node:1.00
```",closed,True,2017-01-26 01:48:47,2017-06-23 09:42:10
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/2349,https://api.github.com/repos/kubernetes/contrib/issues/2349,Cluster-autoscaler: Improve introspection,"Currently it is relatively inconvenient to tell what is the current status of Cluster Autoscaler and how the current state was introduced, especially on platforms like GKE without direct access to master node logs. To improve the situation couple things will be introduced:

* ClusterAutoscaler status api https://github.com/kubernetes/kubernetes/pull/40513. - It will provide information about the current overall status of CA like what is the cluster health, whether it is trying to scale up or have some nodes that might be scale down. The api will provide - free text form explanation what is making CA unhealthy or who might be deleted soon.

* CA status api support in kubectl. So that kubectl describe clusterautoscaler will print the information from the API in an easy to consume form. 

* Publish CA events under the CA object. So that whenever the cluster is scaled up or scaled down or some other issue arise the user has a single place where he can go to check what is going on.

* Provide documentation explaining what these items above really mean and what can be done to improve the situation.

cc: @fgrzadkowski @jszczepkowski @piosz 
",closed,False,2017-01-26 14:03:34,2017-04-19 10:44:37
contrib,bmarks-mylo,https://github.com/kubernetes/contrib/issues/2350,https://api.github.com/repos/kubernetes/contrib/issues/2350,nginx-ingress-controller rewrite-target and path regex bad behavior,"I'm trying to use path based routing to create a reverse proxy with ingress.  It looks like my paths get turned into regular expressions and match more than I want them to.  I also get different behavior based on trailing slashes.

My ingress:
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  namespace: ops
  name: ops-reverse-proxy
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: ops.mydomain.com
      http:
        paths:
        - path: /coverage
          backend:
            serviceName: cov-dashboard-service
            servicePort: 3000
        - path: /purecloud
          backend:
            serviceName: purecloud-tools
            servicePort: 3000
        - path: /reconciler
          backend:
            serviceName: reconciler
            servicePort: 3000
```

Resulting Nginx config:
```
...
location ~* /purecloud {
            proxy_set_header Host                   $host;

            # Pass Real IP
            proxy_set_header X-Real-IP              $remote_addr;

            # Allow websocket connections
            proxy_set_header                        Upgrade           $http_upgrade;
            proxy_set_header                        Connection        $connection_upgrade;

            proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Host       $host;
            proxy_set_header X-Forwarded-Port       $server_port;
            proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

            # mitigate HTTPoxy Vulnerability
            # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
            proxy_set_header Proxy                  """";

            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;

            proxy_redirect                          off;
            proxy_buffering                         off;

            proxy_http_version                      1.1;

        rewrite /purecloud/(.*) /$1 break;
        rewrite /purecloud / break;
        proxy_pass http://ops-purecloud-tools-3000;

        }
...
```

 I'm seeing:
ops.mydomain.com/purecloudrandomstring => gets routed to the purecloud service, I would expect this to result in a 404

ops.mydomain.com/purecloud/qstats => gets redirected to ops.mydomain.com/qstats/, I would expect no redirect and for my service to receive /qstats
ops.mydomain.com/purecloud/qstats/ => page opens, no issues 

ops.mydomain.com/purecloud => any root level resources in page don't load ie. style.css tries to load as ops.mydomain.com/style.css not .../purecloud/style.css",closed,False,2017-01-26 16:08:48,2018-04-09 09:20:52
contrib,piosz,https://github.com/kubernetes/contrib/issues/2351,https://api.github.com/repos/kubernetes/contrib/issues/2351,[addon-resizer] Fix resource format in logs,"As for now I can see resources in the following format
```
cpu:{i:{value:88 scale:-3} d:{Dec:<nil>} s:88m Format:DecimalSI}
```
It should be more human readable.

Follow up https://github.com/kubernetes/contrib/pull/2345/files#r97982865.

cc @Crassirostris ",closed,False,2017-01-26 16:15:05,2018-02-18 16:49:59
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/2352,https://api.github.com/repos/kubernetes/contrib/issues/2352,Cluster-autoscaler: provide a mechanism to grow node groups simultaneously.,"We are receiving request to provide an option to grow and shrink node groups more evenly - in many cases the node groups are in different zones/subregions but contain the same type of machines. Having the inbalance in the node groups increase the max severity of a node group outage.

",closed,False,2017-01-27 22:28:30,2017-06-22 01:12:38
contrib,shyamjvs,https://github.com/kubernetes/contrib/pull/2353,https://api.github.com/repos/kubernetes/contrib/issues/2353,Removed 'compare' from this repo as its moved to k8s.io/perf-tests,"Removing compare tool from this repo and adding it to k8s.io/perf-tests (https://github.com/kubernetes/perf-tests/pull/25) as it makes more sense functionally to have this tool there.

cc @kubernetes/sig-scalability-misc @wojtek-t @gmarek ",closed,True,2017-01-28 15:58:23,2017-01-30 13:59:49
contrib,rom-stratoscale,https://github.com/kubernetes/contrib/issues/2354,https://api.github.com/repos/kubernetes/contrib/issues/2354,k8s dashboard is outdated,Is there a plan to upgrade to v1.5.1?,closed,False,2017-01-29 11:12:03,2018-02-18 20:54:00
contrib,styxlab,https://github.com/kubernetes/contrib/issues/2355,https://api.github.com/repos/kubernetes/contrib/issues/2355,Latest nginx-ingress-controller version?,"The latest tag of this contrib is on version 0.7.0, but many examples show the following container image: gcr.io/google_containers/nginx-ingress-controller:0.8.3. However, nowhere did I find version 0.8.3.

So, what is the latest (official) version of the nginx ingress controller?",closed,False,2017-01-30 10:59:57,2018-02-18 20:54:01
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2356,https://api.github.com/repos/kubernetes/contrib/issues/2356,[Ansible] Allows certificate to be generated for multiple masters at once,"This PR ""fixes"" the certificate generation when having multiple masters.

***

If there is anything I can improve, please let me know.",closed,True,2017-01-30 16:44:26,2017-02-27 11:37:13
contrib,jleonar,https://github.com/kubernetes/contrib/issues/2357,https://api.github.com/repos/kubernetes/contrib/issues/2357,Cluster-autoscaler: Update GCR image regularly,"When testing the auto-scaling functionality, specifically the multiple AWS ASG capability.  The latest image (v0.4.0) on gcr does not support the new flag --expander. ",closed,False,2017-01-31 11:40:06,2017-04-19 15:08:05
contrib,mml,https://github.com/kubernetes/contrib/issues/2358,https://api.github.com/repos/kubernetes/contrib/issues/2358,"Say who takes next action when ""PR is NOT APPROVED""","I think it's on the PR author to find and assign appropriate approvers, but that's not made clear.  For more casual contributors, this is the source of confusion.  Please add a message saying `@author please find/assign appropriate approvers`

E.g.

```
[APPROVALNOTIFIER] This PR is NOT APPROVED

Needs approval from an approver in each of these OWNERS Files:

hack/OWNERS
pkg/kubectl/OWNERS
We suggest the following people:
...
```",closed,False,2017-01-31 23:51:42,2018-04-20 20:20:46
contrib,mml,https://github.com/kubernetes/contrib/issues/2359,https://api.github.com/repos/kubernetes/contrib/issues/2359,"Add ""file a bug here"" links to the github user pages of robots","E.g. https://github.com/k8s-merge-robot

Thanks!",closed,False,2017-01-31 23:52:43,2018-01-01 19:37:55
contrib,andrewsykim,https://github.com/kubernetes/contrib/issues/2360,https://api.github.com/repos/kubernetes/contrib/issues/2360,Release an image of cluster-autoscaler that supports --expander flag,/cc @mwielgus ,closed,False,2017-02-01 00:36:52,2017-04-18 13:11:30
contrib,rbtn,https://github.com/kubernetes/contrib/issues/2361,https://api.github.com/repos/kubernetes/contrib/issues/2361,Get http://IP:10254/healthz: dial tcp IP:10254: getsockopt: connection ,"Hi @aledbf ,

Today I tried ""nginx-ingress-conroller"" following the instructions of link https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx, but failed with the following error,

`4m    1m      8       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Warning Unhealthy       Liveness probe failed: Get http://192.168.57.8:10254/healthz: dial tcp 192.168.57.8:10254: getsockopt: connection refused`

I studied in google before seeking for help, and verified the image is ""gcr.io/google_containers/nginx-ingress-controller:0.8.3"", and the port is 10254. Actually I didn't change the configurations in ""https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples"". Please help take a look what is wrong in the steps.

I gathered md5 of the binary in container's namespace as following,

```
root@nginx-ingress-controller-s39ht:/# md5sum nginx-ingress-controller
6dbcc1bcc7b71cca9361e8e4fed4ac6d  nginx-ingress-controller
```

And did similar steps to ""apt-get update && apt-get install net-tools procps -y && netstat -tnlp && ps auxww"" as the link https://github.com/kubernetes/contrib/issues/752 . The result is below,
```
...
Setting up net-tools (1.60-26ubuntu1) ...
Setting up psmisc (22.21-2.1build1) ...
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.2  1.6  41440  8020 ?        Ssl  08:20   0:00 /nginx-ingress-controller --default-backend-service=default/default-http-backend
root        12  0.0  0.3  18216  1900 ?        Ss   08:20   0:00 bash
root       370  0.0  0.5  34424  2924 ?        R+   08:21   0:00 ps auxww
```

Basic Info

```
root@k8s-master:~/kubernetes/ingress/controllers/nginx# kubectl version
Client Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.2"", GitCommit:""08e099554f3c31f6e6f07b448ab3ed78d0520507"", GitTreeState:""clean"", BuildDate:""2017-01-12T04:57:25Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.2"", GitCommit:""08e099554f3c31f6e6f07b448ab3ed78d0520507"", GitTreeState:""clean"", BuildDate:""2017-01-12T04:52:34Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}
root@k8s-master:~/kubernetes/ingress/controllers/nginx# uname -a
Linux k8s-master 4.4.0-59-generic #80-Ubuntu SMP Fri Jan 6 17:47:47 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
root@k8s-master:~/kubernetes/ingress/controllers/nginx# cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=""Ubuntu 16.04.1 LTS""
root@k8s-master:~/kubernetes/ingress/controllers/nginx# cat examples/default/rc-default.yaml | grep image
      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3
        imagePullPolicy: Always
root@k8s-master:~/kubernetes/ingress/controllers/nginx# cat examples/default/rc-default.yaml | grep -C 1 healthz
          httpGet:
            path: /healthz
            port: 10254
--
          httpGet:
            path: /healthz
            port: 10254
```

At initial time

```
root@k8s-master:~/kubernetes/ingress/controllers/nginx# kubectl create -f examples/default-backend.yaml
replicationcontroller ""default-http-backend"" created
root@k8s-master:~/kubernetes/ingress/controllers/nginx# kubectl expose rc default-http-backend --port=80 --target-port=8080 --name=default-http-backend
service ""default-http-backend"" exposed
root@k8s-master:~/kubernetes/ingress/controllers/nginx# kubectl create -f examples/default/rc-default.yaml
replicationcontroller ""nginx-ingress-controller"" created
root@k8s-master:~/kubernetes/ingress/controllers/nginx# date
Tue Jan 31 23:15:29 PST 2017
root@k8s-master:~/kubernetes/ingress/controllers/nginx# kubectl get pods
NAME                             READY     STATUS    RESTARTS   AGE
default-http-backend-41z2h       1/1       Running   0          26s
nginx-ingress-controller-s39ht   0/1       Running   0          10s
```


After several minutes

```
root@k8s-master:~/kubernetes/ingress/controllers/nginx# date; kubectl get pods -o wide; kubectl describe pod/nginx-ingress-controller-s39ht --v=10; kubectl log pod/nginx-ingress-controller-s39ht --v=10
Tue Jan 31 23:19:45 PST 2017
NAME                             READY     STATUS             RESTARTS   AGE       IP             NODE
default-http-backend-41z2h       1/1       Running            0          4m        192.168.9.6    k8s-worker-1
nginx-ingress-controller-s39ht   0/1       CrashLoopBackOff   5          4m        192.168.57.8   k8s-worker-2
I0131 23:19:47.893548   31000 round_trippers.go:299] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" http://localhost:8080/api
I0131 23:19:47.911245   31000 round_trippers.go:318] GET http://localhost:8080/api 200 OK in 17 milliseconds
I0131 23:19:47.911764   31000 round_trippers.go:324] Response Headers:
I0131 23:19:47.911780   31000 round_trippers.go:327]     Content-Type: application/json
I0131 23:19:47.911792   31000 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:47 GMT
I0131 23:19:47.912060   31000 round_trippers.go:327]     Content-Length: 136
I0131 23:19:47.912240   31000 request.go:904] Response Body: {""kind"":""APIVersions"",""versions"":[""v1""],""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]}
I0131 23:19:47.913349   31000 round_trippers.go:299] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" http://localhost:8080/apis
I0131 23:19:47.976712   31000 round_trippers.go:318] GET http://localhost:8080/apis 200 OK in 63 milliseconds
I0131 23:19:47.976748   31000 round_trippers.go:324] Response Headers:
I0131 23:19:47.976774   31000 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:47 GMT
I0131 23:19:47.976822   31000 round_trippers.go:327]     Content-Type: application/json
I0131 23:19:47.976947   31000 request.go:904] Response Body: {""kind"":""APIGroupList"",""groups"":[{""name"":""apps"",""versions"":[{""groupVersion"":""apps/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""apps/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""authentication.k8s.io"",""versions"":[{""groupVersion"":""authentication.k8s.io/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""authentication.k8s.io/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""authorization.k8s.io"",""versions"":[{""groupVersion"":""authorization.k8s.io/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""authorization.k8s.io/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""autoscaling"",""versions"":[{""groupVersion"":""autoscaling/v1"",""version"":""v1""}],""preferredVersion"":{""groupVersion"":""autoscaling/v1"",""version"":""v1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""batch"",""versions"":[{""groupVersion"":""batch/v1"",""version"":""v1""}],""preferredVersion"":{""groupVersion"":""batch/v1"",""version"":""v1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""certificates.k8s.io"",""versions"":[{""groupVersion"":""certificates.k8s.io/v1alpha1"",""version"":""v1alpha1""}],""preferredVersion"":{""groupVersion"":""certificates.k8s.io/v1alpha1"",""version"":""v1alpha1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""extensions"",""versions"":[{""groupVersion"":""extensions/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""extensions/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""policy"",""versions"":[{""groupVersion"":""policy/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""policy/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""rbac.authorization.k8s.io"",""versions"":[{""groupVersion"":""rbac.authorization.k8s.io/v1alpha1"",""version"":""v1alpha1""}],""preferredVersion"":{""groupVersion"":""rbac.authorization.k8s.io/v1alpha1"",""version"":""v1alpha1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""storage.k8s.io"",""versions"":[{""groupVersion"":""storage.k8s.io/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""storage.k8s.io/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]}]}
I0131 23:19:47.977605   31000 cached_discovery.go:112] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/servergroups.json
I0131 23:19:47.977706   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/apps/v1beta1/serverresources.json
I0131 23:19:47.977761   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/authentication.k8s.io/v1beta1/serverresources.json
I0131 23:19:47.977818   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/authorization.k8s.io/v1beta1/serverresources.json
I0131 23:19:47.977881   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/autoscaling/v1/serverresources.json
I0131 23:19:47.977931   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/batch/v1/serverresources.json
I0131 23:19:47.978012   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/certificates.k8s.io/v1alpha1/serverresources.json
I0131 23:19:47.978125   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/extensions/v1beta1/serverresources.json
I0131 23:19:47.978183   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/policy/v1beta1/serverresources.json
I0131 23:19:47.978249   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/rbac.authorization.k8s.io/v1alpha1/serverresources.json
I0131 23:19:47.978302   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/storage.k8s.io/v1beta1/serverresources.json
I0131 23:19:47.978462   31000 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/v1/serverresources.json
I0131 23:19:47.979315   31000 round_trippers.go:299] curl -k -v -XGET  -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" -H ""Accept: application/json, */*"" http://localhost:8080/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht
I0131 23:19:48.023595   31000 round_trippers.go:318] GET http://localhost:8080/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht 200 OK in 44 milliseconds
I0131 23:19:48.023624   31000 round_trippers.go:324] Response Headers:
I0131 23:19:48.023632   31000 round_trippers.go:327]     Content-Type: application/json
I0131 23:19:48.023640   31000 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:48 GMT
I0131 23:19:48.023761   31000 request.go:904] Response Body: {""kind"":""Pod"",""apiVersion"":""v1"",""metadata"":{""name"":""nginx-ingress-controller-s39ht"",""generateName"":""nginx-ingress-controller-"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""resourceVersion"":""470572"",""creationTimestamp"":""2017-02-01T07:15:25Z"",""labels"":{""k8s-app"":""nginx-ingress-lb"",""name"":""nginx-ingress-lb""},""annotations"":{""kubernetes.io/created-by"":""{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicationController\"",\""namespace\"":\""default\"",\""name\"":\""nginx-ingress-controller\"",\""uid\"":\""33877fb1-e84e-11e6-814a-201610140000\"",\""apiVersion\"":\""v1\"",\""resourceVersion\"":\""470261\""}}\n""},""ownerReferences"":[{""apiVersion"":""v1"",""kind"":""ReplicationController"",""name"":""nginx-ingress-controller"",""uid"":""33877fb1-e84e-11e6-814a-201610140000"",""controller"":true}]},""spec"":{""volumes"":[{""name"":""default-token-79m2w"",""secret"":{""secretName"":""default-token-79m2w"",""defaultMode"":420}}],""containers"":[{""name"":""nginx-ingress-lb"",""image"":""gcr.io/google_containers/nginx-ingress-controller:0.8.3"",""args"":[""/nginx-ingress-controller"",""--default-backend-service=$(POD_NAMESPACE)/default-http-backend""],""ports"":[{""hostPort"":80,""containerPort"":80,""protocol"":""TCP""},{""hostPort"":443,""containerPort"":443,""protocol"":""TCP""}],""env"":[{""name"":""POD_NAME"",""valueFrom"":{""fieldRef"":{""apiVersion"":""v1"",""fieldPath"":""metadata.name""}}},{""name"":""POD_NAMESPACE"",""valueFrom"":{""fieldRef"":{""apiVersion"":""v1"",""fieldPath"":""metadata.namespace""}}}],""resources"":{},""volumeMounts"":[{""name"":""default-token-79m2w"",""readOnly"":true,""mountPath"":""/var/run/secrets/kubernetes.io/serviceaccount""}],""livenessProbe"":{""httpGet"":{""path"":""/healthz"",""port"":10254,""scheme"":""HTTP""},""initialDelaySeconds"":10,""timeoutSeconds"":1,""periodSeconds"":10,""successThreshold"":1,""failureThreshold"":3},""readinessProbe"":{""httpGet"":{""path"":""/healthz"",""port"":10254,""scheme"":""HTTP""},""timeoutSeconds"":1,""periodSeconds"":10,""successThreshold"":1,""failureThreshold"":3},""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""Always""}],""restartPolicy"":""Always"",""terminationGracePeriodSeconds"":60,""dnsPolicy"":""ClusterFirst"",""serviceAccountName"":""default"",""serviceAccount"":""default"",""nodeName"":""k8s-worker-2"",""securityContext"":{}},""status"":{""phase"":""Running"",""conditions"":[{""type"":""Initialized"",""status"":""True"",""lastProbeTime"":null,""lastTransitionTime"":""2017-02-01T07:15:26Z""},{""type"":""Ready"",""status"":""False"",""lastProbeTime"":null,""lastTransitionTime"":""2017-02-01T07:15:26Z"",""reason"":""ContainersNotReady"",""message"":""containers with unready status: [nginx-ingress-lb]""},{""type"":""PodScheduled"",""status"":""True"",""lastProbeTime"":null,""lastTransitionTime"":""2017-02-01T07:15:26Z""}],""hostIP"":""192.168.0.18"",""podIP"":""192.168.57.8"",""startTime"":""2017-02-01T07:15:26Z"",""containerStatuses"":[{""name"":""nginx-ingress-lb"",""state"":{""waiting"":{""reason"":""CrashLoopBackOff"",""message"":""Back-off 1m20s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)""}},""lastState"":{""terminated"":{""exitCode"":2,""reason"":""Error"",""startedAt"":""2017-02-01T07:18:12Z"",""finishedAt"":""2017-02-01T07:18:27Z"",""containerID"":""docker://d766b70fc0432a6c47386c8d15bc1a2889df96328adca0105ff3664a4477422f""}},""ready"":false,""restartCount"":5,""image"":""gcr.io/google_containers/nginx-ingress-controller:0.8.3"",""imageID"":""docker-pullable://gcr.io/google_containers/nginx-ingress-controller@sha256:820c338dc22eda7ab6331001da3cccd43b1b7dcd179049d33a62ad6deaef8daf"",""containerID"":""docker://d766b70fc0432a6c47386c8d15bc1a2889df96328adca0105ff3664a4477422f""}]}}
I0131 23:19:48.024865   31000 round_trippers.go:299] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" http://localhost:8080/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht
I0131 23:19:48.030428   31000 round_trippers.go:318] GET http://localhost:8080/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht 200 OK in 5 milliseconds
I0131 23:19:48.030453   31000 round_trippers.go:324] Response Headers:
I0131 23:19:48.030461   31000 round_trippers.go:327]     Content-Type: application/json
I0131 23:19:48.030468   31000 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:48 GMT
I0131 23:19:48.030563   31000 request.go:904] Response Body: {""kind"":""Pod"",""apiVersion"":""v1"",""metadata"":{""name"":""nginx-ingress-controller-s39ht"",""generateName"":""nginx-ingress-controller-"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""resourceVersion"":""470572"",""creationTimestamp"":""2017-02-01T07:15:25Z"",""labels"":{""k8s-app"":""nginx-ingress-lb"",""name"":""nginx-ingress-lb""},""annotations"":{""kubernetes.io/created-by"":""{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicationController\"",\""namespace\"":\""default\"",\""name\"":\""nginx-ingress-controller\"",\""uid\"":\""33877fb1-e84e-11e6-814a-201610140000\"",\""apiVersion\"":\""v1\"",\""resourceVersion\"":\""470261\""}}\n""},""ownerReferences"":[{""apiVersion"":""v1"",""kind"":""ReplicationController"",""name"":""nginx-ingress-controller"",""uid"":""33877fb1-e84e-11e6-814a-201610140000"",""controller"":true}]},""spec"":{""volumes"":[{""name"":""default-token-79m2w"",""secret"":{""secretName"":""default-token-79m2w"",""defaultMode"":420}}],""containers"":[{""name"":""nginx-ingress-lb"",""image"":""gcr.io/google_containers/nginx-ingress-controller:0.8.3"",""args"":[""/nginx-ingress-controller"",""--default-backend-service=$(POD_NAMESPACE)/default-http-backend""],""ports"":[{""hostPort"":80,""containerPort"":80,""protocol"":""TCP""},{""hostPort"":443,""containerPort"":443,""protocol"":""TCP""}],""env"":[{""name"":""POD_NAME"",""valueFrom"":{""fieldRef"":{""apiVersion"":""v1"",""fieldPath"":""metadata.name""}}},{""name"":""POD_NAMESPACE"",""valueFrom"":{""fieldRef"":{""apiVersion"":""v1"",""fieldPath"":""metadata.namespace""}}}],""resources"":{},""volumeMounts"":[{""name"":""default-token-79m2w"",""readOnly"":true,""mountPath"":""/var/run/secrets/kubernetes.io/serviceaccount""}],""livenessProbe"":{""httpGet"":{""path"":""/healthz"",""port"":10254,""scheme"":""HTTP""},""initialDelaySeconds"":10,""timeoutSeconds"":1,""periodSeconds"":10,""successThreshold"":1,""failureThreshold"":3},""readinessProbe"":{""httpGet"":{""path"":""/healthz"",""port"":10254,""scheme"":""HTTP""},""timeoutSeconds"":1,""periodSeconds"":10,""successThreshold"":1,""failureThreshold"":3},""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""Always""}],""restartPolicy"":""Always"",""terminationGracePeriodSeconds"":60,""dnsPolicy"":""ClusterFirst"",""serviceAccountName"":""default"",""serviceAccount"":""default"",""nodeName"":""k8s-worker-2"",""securityContext"":{}},""status"":{""phase"":""Running"",""conditions"":[{""type"":""Initialized"",""status"":""True"",""lastProbeTime"":null,""lastTransitionTime"":""2017-02-01T07:15:26Z""},{""type"":""Ready"",""status"":""False"",""lastProbeTime"":null,""lastTransitionTime"":""2017-02-01T07:15:26Z"",""reason"":""ContainersNotReady"",""message"":""containers with unready status: [nginx-ingress-lb]""},{""type"":""PodScheduled"",""status"":""True"",""lastProbeTime"":null,""lastTransitionTime"":""2017-02-01T07:15:26Z""}],""hostIP"":""192.168.0.18"",""podIP"":""192.168.57.8"",""startTime"":""2017-02-01T07:15:26Z"",""containerStatuses"":[{""name"":""nginx-ingress-lb"",""state"":{""waiting"":{""reason"":""CrashLoopBackOff"",""message"":""Back-off 1m20s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)""}},""lastState"":{""terminated"":{""exitCode"":2,""reason"":""Error"",""startedAt"":""2017-02-01T07:18:12Z"",""finishedAt"":""2017-02-01T07:18:27Z"",""containerID"":""docker://d766b70fc0432a6c47386c8d15bc1a2889df96328adca0105ff3664a4477422f""}},""ready"":false,""restartCount"":5,""image"":""gcr.io/google_containers/nginx-ingress-controller:0.8.3"",""imageID"":""docker-pullable://gcr.io/google_containers/nginx-ingress-controller@sha256:820c338dc22eda7ab6331001da3cccd43b1b7dcd179049d33a62ad6deaef8daf"",""containerID"":""docker://d766b70fc0432a6c47386c8d15bc1a2889df96328adca0105ff3664a4477422f""}]}}
I0131 23:19:48.031308   31000 round_trippers.go:299] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" http://localhost:8080/api/v1/namespaces/default/events?fieldSelector=involvedObject.name%3Dnginx-ingress-controller-s39ht%2CinvolvedObject.namespace%3Ddefault%2CinvolvedObject.uid%3D33bd084c-e84e-11e6-814a-201610140000
I0131 23:19:48.082986   31000 round_trippers.go:318] GET http://localhost:8080/api/v1/namespaces/default/events?fieldSelector=involvedObject.name%3Dnginx-ingress-controller-s39ht%2CinvolvedObject.namespace%3Ddefault%2CinvolvedObject.uid%3D33bd084c-e84e-11e6-814a-201610140000 200 OK in 51 milliseconds
I0131 23:19:48.083152   31000 round_trippers.go:324] Response Headers:
I0131 23:19:48.083167   31000 round_trippers.go:327]     Content-Type: application/json
I0131 23:19:48.083174   31000 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:48 GMT
I0131 23:19:48.083647   31000 request.go:904] Response Body: {""kind"":""EventList"",""apiVersion"":""v1"",""metadata"":{""selfLink"":""/api/v1/namespaces/default/events"",""resourceVersion"":""470699""},""items"":[{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f187ce362825a"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f187ce362825a"",""uid"":""348554d8-e84e-11e6-814a-201610140000"",""resourceVersion"":""470268"",""creationTimestamp"":""2017-02-01T07:15:26Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470263""},""reason"":""Scheduled"",""message"":""Successfully assigned nginx-ingress-controller-s39ht to k8s-worker-2"",""source"":{""component"":""default-scheduler""},""firstTimestamp"":""2017-02-01T07:15:26Z"",""lastTimestamp"":""2017-02-01T07:15:26Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f187de3f19e44"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f187de3f19e44"",""uid"":""373644ca-e84e-11e6-814a-201610140000"",""resourceVersion"":""470537"",""creationTimestamp"":""2017-02-01T07:15:31Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Pulling"",""message"":""pulling image \""gcr.io/google_containers/nginx-ingress-controller:0.8.3\"""",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:15:31Z"",""lastTimestamp"":""2017-02-01T07:18:10Z"",""count"":6,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f187e219ed392"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f187e219ed392"",""uid"":""37b4fc93-e84e-11e6-814a-201610140000"",""resourceVersion"":""470539"",""creationTimestamp"":""2017-02-01T07:15:32Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Pulled"",""message"":""Successfully pulled image \""gcr.io/google_containers/nginx-ingress-controller:0.8.3\"""",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:15:32Z"",""lastTimestamp"":""2017-02-01T07:18:11Z"",""count"":6,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f187e544c8727"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f187e544c8727"",""uid"":""3841d97e-e84e-11e6-814a-201610140000"",""resourceVersion"":""470282"",""creationTimestamp"":""2017-02-01T07:15:33Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Created"",""message"":""Created container with docker id bb819e3e30d4; Security:[seccomp=unconfined]"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:15:32Z"",""lastTimestamp"":""2017-02-01T07:15:32Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f187e67ed2596"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f187e67ed2596"",""uid"":""388528fb-e84e-11e6-814a-201610140000"",""resourceVersion"":""470284"",""creationTimestamp"":""2017-02-01T07:15:33Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Started"",""message"":""Started container with docker id bb819e3e30d4"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:15:33Z"",""lastTimestamp"":""2017-02-01T07:15:33Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f187ec9fde61e"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f187ec9fde61e"",""uid"":""39601d9a-e84e-11e6-814a-201610140000"",""resourceVersion"":""470564"",""creationTimestamp"":""2017-02-01T07:15:34Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Unhealthy"",""message"":""Readiness probe failed: Get http://192.168.57.8:10254/healthz: dial tcp 192.168.57.8:10254: getsockopt: connection refused"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:15:34Z"",""lastTimestamp"":""2017-02-01T07:18:26Z"",""count"":15,""type"":""Warning""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f188190577b25"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f188190577b25"",""uid"":""40b3a891-e84e-11e6-814a-201610140000"",""resourceVersion"":""470565"",""creationTimestamp"":""2017-02-01T07:15:47Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Unhealthy"",""message"":""Liveness probe failed: Get http://192.168.57.8:10254/healthz: dial tcp 192.168.57.8:10254: getsockopt: connection refused"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:15:46Z"",""lastTimestamp"":""2017-02-01T07:18:26Z"",""count"":8,""type"":""Warning""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f18865c896ad3"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f18865c896ad3"",""uid"":""4cd69d0e-e84e-11e6-814a-201610140000"",""resourceVersion"":""470344"",""creationTimestamp"":""2017-02-01T07:16:07Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Killing"",""message"":""Killing container with docker id bb819e3e30d4: pod \""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)\"" container \""nginx-ingress-lb\"" is unhealthy, it will be killed and re-created."",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:16:07Z"",""lastTimestamp"":""2017-02-01T07:16:07Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f1886be5a8b80"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f1886be5a8b80"",""uid"":""4dbc0805-e84e-11e6-814a-201610140000"",""resourceVersion"":""470349"",""creationTimestamp"":""2017-02-01T07:16:09Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Created"",""message"":""Created container with docker id 3e507c62e4f6; Security:[seccomp=unconfined]"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:16:09Z"",""lastTimestamp"":""2017-02-01T07:16:09Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f1886ca5f6ee8"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f1886ca5f6ee8"",""uid"":""4ddb0ebb-e84e-11e6-814a-201610140000"",""resourceVersion"":""470350"",""creationTimestamp"":""2017-02-01T07:16:09Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Started"",""message"":""Started container with docker id 3e507c62e4f6"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:16:09Z"",""lastTimestamp"":""2017-02-01T07:16:09Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f188b1cd08400"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f188b1cd08400"",""uid"":""5f9f1d69-e84e-11e6-814a-201610140000"",""resourceVersion"":""470389"",""creationTimestamp"":""2017-02-01T07:16:39Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Killing"",""message"":""Killing container with docker id 3e507c62e4f6: pod \""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)\"" container \""nginx-ingress-lb\"" is unhealthy, it will be killed and re-created."",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:16:27Z"",""lastTimestamp"":""2017-02-01T07:16:27Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f188ba3df0266"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f188ba3df0266"",""uid"":""60cf8f9a-e84e-11e6-814a-201610140000"",""resourceVersion"":""470394"",""creationTimestamp"":""2017-02-01T07:16:41Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Created"",""message"":""Created container with docker id dee07a54daa3; Security:[seccomp=unconfined]"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:16:30Z"",""lastTimestamp"":""2017-02-01T07:16:30Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f188bb3658dcd"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f188bb3658dcd"",""uid"":""60d7ef70-e84e-11e6-814a-201610140000"",""resourceVersion"":""470395"",""creationTimestamp"":""2017-02-01T07:16:41Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Started"",""message"":""Started container with docker id dee07a54daa3"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:16:30Z"",""lastTimestamp"":""2017-02-01T07:16:30Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f188fa8b6c11a"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f188fa8b6c11a"",""uid"":""66daacfa-e84e-11e6-814a-201610140000"",""resourceVersion"":""470407"",""creationTimestamp"":""2017-02-01T07:16:51Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Killing"",""message"":""Killing container with docker id dee07a54daa3: pod \""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)\"" container \""nginx-ingress-lb\"" is unhealthy, it will be killed and re-created."",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:16:47Z"",""lastTimestamp"":""2017-02-01T07:16:47Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f18901ab1b132"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f18901ab1b132"",""uid"":""6a70ba1e-e84e-11e6-814a-201610140000"",""resourceVersion"":""470418"",""creationTimestamp"":""2017-02-01T07:16:57Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Created"",""message"":""Created container with docker id bcfaa39c1c90; Security:[seccomp=unconfined]"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:16:49Z"",""lastTimestamp"":""2017-02-01T07:16:49Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f189053ff5a6a"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f189053ff5a6a"",""uid"":""6abadd9f-e84e-11e6-814a-201610140000"",""resourceVersion"":""470419"",""creationTimestamp"":""2017-02-01T07:16:57Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Started"",""message"":""Started container with docker id bcfaa39c1c90"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:16:50Z"",""lastTimestamp"":""2017-02-01T07:16:50Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f189459a531d3"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f189459a531d3"",""uid"":""7253b6ed-e84e-11e6-814a-201610140000"",""resourceVersion"":""470436"",""creationTimestamp"":""2017-02-01T07:17:10Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Killing"",""message"":""Killing container with docker id bcfaa39c1c90: pod \""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)\"" container \""nginx-ingress-lb\"" is unhealthy, it will be killed and re-created."",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:17:07Z"",""lastTimestamp"":""2017-02-01T07:17:07Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f1894dd7cb2ff"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f1894dd7cb2ff"",""uid"":""76f9f075-e84e-11e6-814a-201610140000"",""resourceVersion"":""470449"",""creationTimestamp"":""2017-02-01T07:17:18Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Created"",""message"":""Created container with docker id 11c8a74efd81; Security:[seccomp=unconfined]"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:17:09Z"",""lastTimestamp"":""2017-02-01T07:17:09Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f1895004fbf33"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f1895004fbf33"",""uid"":""7706f889-e84e-11e6-814a-201610140000"",""resourceVersion"":""470450"",""creationTimestamp"":""2017-02-01T07:17:18Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Started"",""message"":""Started container with docker id 11c8a74efd81"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:17:10Z"",""lastTimestamp"":""2017-02-01T07:17:10Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f1899104e48c0"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f1899104e48c0"",""uid"":""7e35d991-e84e-11e6-814a-201610140000"",""resourceVersion"":""470470"",""creationTimestamp"":""2017-02-01T07:17:30Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Killing"",""message"":""Killing container with docker id 11c8a74efd81: pod \""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)\"" container \""nginx-ingress-lb\"" is unhealthy, it will be killed and re-created."",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:17:27Z"",""lastTimestamp"":""2017-02-01T07:17:27Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f189910575291"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f189910575291"",""uid"":""7e44bb21-e84e-11e6-814a-201610140000"",""resourceVersion"":""470687"",""creationTimestamp"":""2017-02-01T07:17:30Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""BackOff"",""message"":""Back-off restarting failed docker container"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:17:27Z"",""lastTimestamp"":""2017-02-01T07:19:36Z"",""count"":11,""type"":""Warning""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f1899105f2011"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f1899105f2011"",""uid"":""7d3ccee0-e84e-11e6-814a-201610140000"",""resourceVersion"":""470514"",""creationTimestamp"":""2017-02-01T07:17:28Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267""},""reason"":""FailedSync"",""message"":""Error syncing pod, skipping: failed to \""StartContainer\"" for \""nginx-ingress-lb\"" with CrashLoopBackOff: \""Back-off 40s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)\""\n"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:17:27Z"",""lastTimestamp"":""2017-02-01T07:17:55Z"",""count"":4,""type"":""Warning""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f18a361b2e216"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f18a361b2e216"",""uid"":""970e09c3-e84e-11e6-814a-201610140000"",""resourceVersion"":""470541"",""creationTimestamp"":""2017-02-01T07:18:12Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Created"",""message"":""Created container with docker id d766b70fc043; Security:[seccomp=unconfined]"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:18:12Z"",""lastTimestamp"":""2017-02-01T07:18:12Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f18a36e8bfee6"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f18a36e8bfee6"",""uid"":""972c1e62-e84e-11e6-814a-201610140000"",""resourceVersion"":""470542"",""creationTimestamp"":""2017-02-01T07:18:12Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Started"",""message"":""Started container with docker id d766b70fc043"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:18:12Z"",""lastTimestamp"":""2017-02-01T07:18:12Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f18a6f868b978"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f18a6f868b978"",""uid"":""a05218ee-e84e-11e6-814a-201610140000"",""resourceVersion"":""470568"",""creationTimestamp"":""2017-02-01T07:18:27Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267"",""fieldPath"":""spec.containers{nginx-ingress-lb}""},""reason"":""Killing"",""message"":""Killing container with docker id d766b70fc043: pod \""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)\"" container \""nginx-ingress-lb\"" is unhealthy, it will be killed and re-created."",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:18:27Z"",""lastTimestamp"":""2017-02-01T07:18:27Z"",""count"":1,""type"":""Normal""},{""metadata"":{""name"":""nginx-ingress-controller-s39ht.149f18a6f879eecf"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/events/nginx-ingress-controller-s39ht.149f18a6f879eecf"",""uid"":""a040b1b9-e84e-11e6-814a-201610140000"",""resourceVersion"":""470684"",""creationTimestamp"":""2017-02-01T07:18:27Z""},""involvedObject"":{""kind"":""Pod"",""namespace"":""default"",""name"":""nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""apiVersion"":""v1"",""resourceVersion"":""470267""},""reason"":""FailedSync"",""message"":""Error syncing pod, skipping: failed to \""StartContainer\"" for \""nginx-ingress-lb\"" with CrashLoopBackOff: \""Back-off 1m20s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)\""\n"",""source"":{""component"":""kubelet"",""host"":""k8s-worker-2""},""firstTimestamp"":""2017-02-01T07:18:27Z"",""lastTimestamp"":""2017-02-01T07:19:36Z"",""count"":7,""type"":""Warning""}]}
Name:           nginx-ingress-controller-s39ht
Namespace:      default
Node:           k8s-worker-2/192.168.0.18
Start Time:     Tue, 31 Jan 2017 23:15:26 -0800
Labels:         k8s-app=nginx-ingress-lb
                name=nginx-ingress-lb
Status:         Running
IP:             192.168.57.8
Controllers:    ReplicationController/nginx-ingress-controller
Containers:
  nginx-ingress-lb:
    Container ID:       docker://d766b70fc0432a6c47386c8d15bc1a2889df96328adca0105ff3664a4477422f
    Image:              gcr.io/google_containers/nginx-ingress-controller:0.8.3
    Image ID:           docker-pullable://gcr.io/google_containers/nginx-ingress-controller@sha256:820c338dc22eda7ab6331001da3cccd43b1b7dcd179049d33a62ad6deaef8daf
    Ports:              80/TCP, 443/TCP
    Args:
      /nginx-ingress-controller
      --default-backend-service=$(POD_NAMESPACE)/default-http-backend
    State:              Waiting
      Reason:           CrashLoopBackOff
    Last State:         Terminated
      Reason:           Error
      Exit Code:        2
      Started:          Tue, 31 Jan 2017 23:18:12 -0800
      Finished:         Tue, 31 Jan 2017 23:18:27 -0800
    Ready:              False
    Restart Count:      5
    Liveness:           http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:          http-get http://:10254/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79m2w (ro)
    Environment Variables:
      POD_NAME:         nginx-ingress-controller-s39ht (v1:metadata.name)
      POD_NAMESPACE:    default (v1:metadata.namespace)
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  default-token-79m2w:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-79m2w
QoS Class:      BestEffort
Tolerations:    <none>
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  4m            4m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned nginx-ingress-controller-s39ht to k8s-worker-2
  4m            4m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Created         Created container with docker id bb819e3e30d4; Security:[seccomp=unconfined]
  4m            4m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Started         Started container with docker id bb819e3e30d4
  3m            3m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Killing         Killing container with docker id bb819e3e30d4: pod ""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)"" container ""nginx-ingress-lb"" is unhealthy, it will be killed and re-created.
  3m            3m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Started         Started container with docker id 3e507c62e4f6
  3m            3m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Created         Created container with docker id 3e507c62e4f6; Security:[seccomp=unconfined]
  3m            3m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Killing         Killing container with docker id 3e507c62e4f6: pod ""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)"" container ""nginx-ingress-lb"" is unhealthy, it will be killed and re-created.
  3m            3m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Created         Created container with docker id dee07a54daa3; Security:[seccomp=unconfined]
  3m            3m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Started         Started container with docker id dee07a54daa3
  3m            3m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Killing         Killing container with docker id dee07a54daa3: pod ""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)"" container ""nginx-ingress-lb"" is unhealthy, it will be killed and re-created.
  2m            2m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Created         Created container with docker id bcfaa39c1c90; Security:[seccomp=unconfined]
  2m            2m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Started         Started container with docker id bcfaa39c1c90
  2m            2m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Killing         Killing container with docker id bcfaa39c1c90: pod ""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)"" container ""nginx-ingress-lb"" is unhealthy, it will be killed and re-created.
  2m            2m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Created         Created container with docker id 11c8a74efd81; Security:[seccomp=unconfined]
  2m            2m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Started         Started container with docker id 11c8a74efd81
  2m            2m              1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal          Killing         Killing container with docker id 11c8a74efd81: pod ""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)"" container ""nginx-ingress-lb"" is unhealthy, it will be killed and re-created.
  2m            1m              4       {kubelet k8s-worker-2}                                          Warning         FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""nginx-ingress-lb"" with CrashLoopBackOff: ""Back-off 40s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)""

  4m    1m      6       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal  Pulling         pulling image ""gcr.io/google_containers/nginx-ingress-controller:0.8.3""
  4m    1m      6       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal  Pulled          Successfully pulled image ""gcr.io/google_containers/nginx-ingress-controller:0.8.3""
  1m    1m      1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal  Created         Created container with docker id d766b70fc043; Security:[seccomp=unconfined]
  1m    1m      1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal  Started         Started container with docker id d766b70fc043
  4m    1m      8       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Warning Unhealthy       Liveness probe failed: Get http://192.168.57.8:10254/healthz: dial tcp 192.168.57.8:10254: getsockopt: connection refused
  4m    1m      15      {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Warning Unhealthy       Readiness probe failed: Get http://192.168.57.8:10254/healthz: dial tcp 192.168.57.8:10254: getsockopt: connection refused
  1m    1m      1       {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Normal  Killing         Killing container with docker id d766b70fc043: pod ""nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)"" container ""nginx-ingress-lb"" is unhealthy, it will be killed and re-created.
  2m    12s     11      {kubelet k8s-worker-2}  spec.containers{nginx-ingress-lb}       Warning BackOff         Back-off restarting failed docker container
  1m    12s     7       {kubelet k8s-worker-2}                                          Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""nginx-ingress-lb"" with CrashLoopBackOff: ""Back-off 1m20s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)""

W0131 23:19:48.184354   31023 cmd.go:325] log is DEPRECATED and will be removed in a future version. Use logs instead.
I0131 23:19:48.185459   31023 round_trippers.go:299] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" http://localhost:8080/api
I0131 23:19:48.187725   31023 round_trippers.go:318] GET http://localhost:8080/api 200 OK in 2 milliseconds
I0131 23:19:48.187756   31023 round_trippers.go:324] Response Headers:
I0131 23:19:48.187769   31023 round_trippers.go:327]     Content-Type: application/json
I0131 23:19:48.187783   31023 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:48 GMT
I0131 23:19:48.187790   31023 round_trippers.go:327]     Content-Length: 136
I0131 23:19:48.187843   31023 request.go:904] Response Body: {""kind"":""APIVersions"",""versions"":[""v1""],""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]}
I0131 23:19:48.188116   31023 round_trippers.go:299] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" http://localhost:8080/apis
I0131 23:19:48.190274   31023 round_trippers.go:318] GET http://localhost:8080/apis 200 OK in 1 milliseconds
I0131 23:19:48.190291   31023 round_trippers.go:324] Response Headers:
I0131 23:19:48.190299   31023 round_trippers.go:327]     Content-Type: application/json
I0131 23:19:48.190306   31023 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:48 GMT
I0131 23:19:48.190354   31023 request.go:904] Response Body: {""kind"":""APIGroupList"",""groups"":[{""name"":""apps"",""versions"":[{""groupVersion"":""apps/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""apps/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""authentication.k8s.io"",""versions"":[{""groupVersion"":""authentication.k8s.io/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""authentication.k8s.io/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""authorization.k8s.io"",""versions"":[{""groupVersion"":""authorization.k8s.io/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""authorization.k8s.io/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""autoscaling"",""versions"":[{""groupVersion"":""autoscaling/v1"",""version"":""v1""}],""preferredVersion"":{""groupVersion"":""autoscaling/v1"",""version"":""v1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""batch"",""versions"":[{""groupVersion"":""batch/v1"",""version"":""v1""}],""preferredVersion"":{""groupVersion"":""batch/v1"",""version"":""v1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""certificates.k8s.io"",""versions"":[{""groupVersion"":""certificates.k8s.io/v1alpha1"",""version"":""v1alpha1""}],""preferredVersion"":{""groupVersion"":""certificates.k8s.io/v1alpha1"",""version"":""v1alpha1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""extensions"",""versions"":[{""groupVersion"":""extensions/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""extensions/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""policy"",""versions"":[{""groupVersion"":""policy/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""policy/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""rbac.authorization.k8s.io"",""versions"":[{""groupVersion"":""rbac.authorization.k8s.io/v1alpha1"",""version"":""v1alpha1""}],""preferredVersion"":{""groupVersion"":""rbac.authorization.k8s.io/v1alpha1"",""version"":""v1alpha1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""storage.k8s.io"",""versions"":[{""groupVersion"":""storage.k8s.io/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""storage.k8s.io/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]}]}
I0131 23:19:48.190859   31023 cached_discovery.go:112] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/servergroups.json
I0131 23:19:48.191396   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/apps/v1beta1/serverresources.json
I0131 23:19:48.191651   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/authentication.k8s.io/v1beta1/serverresources.json
I0131 23:19:48.191876   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/authorization.k8s.io/v1beta1/serverresources.json
I0131 23:19:48.192480   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/autoscaling/v1/serverresources.json
I0131 23:19:48.192763   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/batch/v1/serverresources.json
I0131 23:19:48.192846   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/certificates.k8s.io/v1alpha1/serverresources.json
I0131 23:19:48.193155   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/extensions/v1beta1/serverresources.json
I0131 23:19:48.193202   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/policy/v1beta1/serverresources.json
I0131 23:19:48.193570   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/rbac.authorization.k8s.io/v1alpha1/serverresources.json
I0131 23:19:48.193824   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/storage.k8s.io/v1beta1/serverresources.json
I0131 23:19:48.194223   31023 cached_discovery.go:70] returning cached discovery info from /root/.kube/cache/discovery/localhost_8080/v1/serverresources.json
I0131 23:19:48.195433   31023 round_trippers.go:299] curl -k -v -XGET  -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" -H ""Accept: application/json, */*"" http://localhost:8080/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht
I0131 23:19:48.203030   31023 round_trippers.go:318] GET http://localhost:8080/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht 200 OK in 7 milliseconds
I0131 23:19:48.203489   31023 round_trippers.go:324] Response Headers:
I0131 23:19:48.203854   31023 round_trippers.go:327]     Content-Type: application/json
I0131 23:19:48.204256   31023 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:48 GMT
I0131 23:19:48.204366   31023 request.go:904] Response Body: {""kind"":""Pod"",""apiVersion"":""v1"",""metadata"":{""name"":""nginx-ingress-controller-s39ht"",""generateName"":""nginx-ingress-controller-"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht"",""uid"":""33bd084c-e84e-11e6-814a-201610140000"",""resourceVersion"":""470572"",""creationTimestamp"":""2017-02-01T07:15:25Z"",""labels"":{""k8s-app"":""nginx-ingress-lb"",""name"":""nginx-ingress-lb""},""annotations"":{""kubernetes.io/created-by"":""{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicationController\"",\""namespace\"":\""default\"",\""name\"":\""nginx-ingress-controller\"",\""uid\"":\""33877fb1-e84e-11e6-814a-201610140000\"",\""apiVersion\"":\""v1\"",\""resourceVersion\"":\""470261\""}}\n""},""ownerReferences"":[{""apiVersion"":""v1"",""kind"":""ReplicationController"",""name"":""nginx-ingress-controller"",""uid"":""33877fb1-e84e-11e6-814a-201610140000"",""controller"":true}]},""spec"":{""volumes"":[{""name"":""default-token-79m2w"",""secret"":{""secretName"":""default-token-79m2w"",""defaultMode"":420}}],""containers"":[{""name"":""nginx-ingress-lb"",""image"":""gcr.io/google_containers/nginx-ingress-controller:0.8.3"",""args"":[""/nginx-ingress-controller"",""--default-backend-service=$(POD_NAMESPACE)/default-http-backend""],""ports"":[{""hostPort"":80,""containerPort"":80,""protocol"":""TCP""},{""hostPort"":443,""containerPort"":443,""protocol"":""TCP""}],""env"":[{""name"":""POD_NAME"",""valueFrom"":{""fieldRef"":{""apiVersion"":""v1"",""fieldPath"":""metadata.name""}}},{""name"":""POD_NAMESPACE"",""valueFrom"":{""fieldRef"":{""apiVersion"":""v1"",""fieldPath"":""metadata.namespace""}}}],""resources"":{},""volumeMounts"":[{""name"":""default-token-79m2w"",""readOnly"":true,""mountPath"":""/var/run/secrets/kubernetes.io/serviceaccount""}],""livenessProbe"":{""httpGet"":{""path"":""/healthz"",""port"":10254,""scheme"":""HTTP""},""initialDelaySeconds"":10,""timeoutSeconds"":1,""periodSeconds"":10,""successThreshold"":1,""failureThreshold"":3},""readinessProbe"":{""httpGet"":{""path"":""/healthz"",""port"":10254,""scheme"":""HTTP""},""timeoutSeconds"":1,""periodSeconds"":10,""successThreshold"":1,""failureThreshold"":3},""terminationMessagePath"":""/dev/termination-log"",""imagePullPolicy"":""Always""}],""restartPolicy"":""Always"",""terminationGracePeriodSeconds"":60,""dnsPolicy"":""ClusterFirst"",""serviceAccountName"":""default"",""serviceAccount"":""default"",""nodeName"":""k8s-worker-2"",""securityContext"":{}},""status"":{""phase"":""Running"",""conditions"":[{""type"":""Initialized"",""status"":""True"",""lastProbeTime"":null,""lastTransitionTime"":""2017-02-01T07:15:26Z""},{""type"":""Ready"",""status"":""False"",""lastProbeTime"":null,""lastTransitionTime"":""2017-02-01T07:15:26Z"",""reason"":""ContainersNotReady"",""message"":""containers with unready status: [nginx-ingress-lb]""},{""type"":""PodScheduled"",""status"":""True"",""lastProbeTime"":null,""lastTransitionTime"":""2017-02-01T07:15:26Z""}],""hostIP"":""192.168.0.18"",""podIP"":""192.168.57.8"",""startTime"":""2017-02-01T07:15:26Z"",""containerStatuses"":[{""name"":""nginx-ingress-lb"",""state"":{""waiting"":{""reason"":""CrashLoopBackOff"",""message"":""Back-off 1m20s restarting failed container=nginx-ingress-lb pod=nginx-ingress-controller-s39ht_default(33bd084c-e84e-11e6-814a-201610140000)""}},""lastState"":{""terminated"":{""exitCode"":2,""reason"":""Error"",""startedAt"":""2017-02-01T07:18:12Z"",""finishedAt"":""2017-02-01T07:18:27Z"",""containerID"":""docker://d766b70fc0432a6c47386c8d15bc1a2889df96328adca0105ff3664a4477422f""}},""ready"":false,""restartCount"":5,""image"":""gcr.io/google_containers/nginx-ingress-controller:0.8.3"",""imageID"":""docker-pullable://gcr.io/google_containers/nginx-ingress-controller@sha256:820c338dc22eda7ab6331001da3cccd43b1b7dcd179049d33a62ad6deaef8daf"",""containerID"":""docker://d766b70fc0432a6c47386c8d15bc1a2889df96328adca0105ff3664a4477422f""}]}}
I0131 23:19:48.205570   31023 round_trippers.go:299] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" http://localhost:8080/api
I0131 23:19:48.207074   31023 round_trippers.go:318] GET http://localhost:8080/api 200 OK in 1 milliseconds
I0131 23:19:48.207532   31023 round_trippers.go:324] Response Headers:
I0131 23:19:48.207751   31023 round_trippers.go:327]     Content-Type: application/json
I0131 23:19:48.207769   31023 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:48 GMT
I0131 23:19:48.207782   31023 round_trippers.go:327]     Content-Length: 136
I0131 23:19:48.207830   31023 request.go:904] Response Body: {""kind"":""APIVersions"",""versions"":[""v1""],""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]}
I0131 23:19:48.207948   31023 round_trippers.go:299] curl -k -v -XGET  -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" -H ""Accept: application/json, */*"" http://localhost:8080/apis
I0131 23:19:48.228958   31023 round_trippers.go:318] GET http://localhost:8080/apis 200 OK in 20 milliseconds
I0131 23:19:48.228987   31023 round_trippers.go:324] Response Headers:
I0131 23:19:48.228995   31023 round_trippers.go:327]     Content-Type: application/json
I0131 23:19:48.229000   31023 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:48 GMT
I0131 23:19:48.229121   31023 request.go:904] Response Body: {""kind"":""APIGroupList"",""groups"":[{""name"":""apps"",""versions"":[{""groupVersion"":""apps/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""apps/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""authentication.k8s.io"",""versions"":[{""groupVersion"":""authentication.k8s.io/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""authentication.k8s.io/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""authorization.k8s.io"",""versions"":[{""groupVersion"":""authorization.k8s.io/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""authorization.k8s.io/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""autoscaling"",""versions"":[{""groupVersion"":""autoscaling/v1"",""version"":""v1""}],""preferredVersion"":{""groupVersion"":""autoscaling/v1"",""version"":""v1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""batch"",""versions"":[{""groupVersion"":""batch/v1"",""version"":""v1""}],""preferredVersion"":{""groupVersion"":""batch/v1"",""version"":""v1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""certificates.k8s.io"",""versions"":[{""groupVersion"":""certificates.k8s.io/v1alpha1"",""version"":""v1alpha1""}],""preferredVersion"":{""groupVersion"":""certificates.k8s.io/v1alpha1"",""version"":""v1alpha1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""extensions"",""versions"":[{""groupVersion"":""extensions/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""extensions/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""policy"",""versions"":[{""groupVersion"":""policy/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""policy/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""rbac.authorization.k8s.io"",""versions"":[{""groupVersion"":""rbac.authorization.k8s.io/v1alpha1"",""version"":""v1alpha1""}],""preferredVersion"":{""groupVersion"":""rbac.authorization.k8s.io/v1alpha1"",""version"":""v1alpha1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]},{""name"":""storage.k8s.io"",""versions"":[{""groupVersion"":""storage.k8s.io/v1beta1"",""version"":""v1beta1""}],""preferredVersion"":{""groupVersion"":""storage.k8s.io/v1beta1"",""version"":""v1beta1""},""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""192.168.0.125:6443""}]}]}
I0131 23:19:48.253180   31023 round_trippers.go:299] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.5.2 (linux/amd64) kubernetes/08e0995"" http://localhost:8080/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht/log
I0131 23:19:48.291348   31023 round_trippers.go:318] GET http://localhost:8080/api/v1/namespaces/default/pods/nginx-ingress-controller-s39ht/log 200 OK in 37 milliseconds
I0131 23:19:48.291378   31023 round_trippers.go:324] Response Headers:
I0131 23:19:48.291383   31023 round_trippers.go:327]     Content-Type: text/plain
I0131 23:19:48.291387   31023 round_trippers.go:327]     Date: Wed, 01 Feb 2017 07:19:48 GMT
I0131 23:19:48.291390   31023 round_trippers.go:327]     Content-Length: 111
I0201 07:18:12.179379       1 main.go:94] Using build: https://github.com/bprashanth/contrib.git - git-92b2bac

```
```",closed,False,2017-02-01 08:37:32,2018-01-19 23:26:51
contrib,dluc,https://github.com/kubernetes/contrib/pull/2362,https://api.github.com/repos/kubernetes/contrib/issues/2362,Fix some typos in the README,,closed,True,2017-02-02 07:19:28,2017-04-19 06:06:37
contrib,craigwillis85,https://github.com/kubernetes/contrib/issues/2363,https://api.github.com/repos/kubernetes/contrib/issues/2363,Nginx Ingress Controller throwing 502,"
Hi

I'm using the nginx ingress controller DaemonSet example to expose a service of mine.

I have an Ingress, which is below:

    apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      name: my-service
      namespace: default
    spec:
      rules:
      - host: mydomain.com
        http:
          paths:
          - path: /
            backend:
              serviceName: my-service
              servicePort: 80

The problem is, I keep seeing `502` errors when i hit certain pages of mydomain.com, the errors are similar to below:

    2017/02/02 13:44:36 [error] 159#159: *63 upstream prematurely closed connection while reading response header from upstream, client: xx.xx.xx.xx, server: _, request: ""GET /test.html HTTP/1.1"", upstream: ""http://10.123.45.11:80/test.html"", host: ""mydomain.com""

`10.123.45.11` is an IP of one of the pods running the `my-service` service.

I'm unsure as to why this happening as it shouldn't be throwing a `502`

Any ideas?

",closed,False,2017-02-02 13:49:29,2018-05-07 17:54:30
contrib,dashpole,https://github.com/kubernetes/contrib/pull/2364,https://api.github.com/repos/kubernetes/contrib/issues/2364,parser now parses deletion and volume events,"Add deletion events and volume mount/unmount events to the node perf dash
cc @Random-Liu ",closed,True,2017-02-02 21:39:39,2018-06-12 02:50:24
contrib,ixdy,https://github.com/kubernetes/contrib/pull/2365,https://api.github.com/repos/kubernetes/contrib/issues/2365,"Delete ingress/controllers, images/{nginx,ubuntu}-slim, which are now in the kubernetes/ingress repo","Remove duplicated out-of-date code.

I left the `ingress/echoheaders` and `ingress/echoheaders-redirect` directories, since those don't seem to exist in kubernetes/ingress.

cc @bprashanth @aledbf ",closed,True,2017-02-02 22:05:07,2017-02-06 23:13:49
contrib,ixdy,https://github.com/kubernetes/contrib/pull/2366,https://api.github.com/repos/kubernetes/contrib/issues/2366,"Base kubelet-to-gcm on scratch, set user to nobody, and bump tag to 1.2.2","This also fixes changes the prefix back to the correct default.

I haven't pushed 1.2.2 yet, but I don't expect any major changes, besides using an updated base image and being built with go1.7.5.",closed,True,2017-02-02 23:40:16,2017-02-14 01:57:00
contrib,newmanwang,https://github.com/kubernetes/contrib/issues/2367,https://api.github.com/repos/kubernetes/contrib/issues/2367,Can not deploy a nginx ingress controller with a specified ingress source?,"Say I have two ingress resources, how can I create two nginx ingress controller deployment with each one of them use a different ingress resource?",closed,False,2017-02-05 16:14:31,2018-02-20 00:21:00
contrib,lcharkiewicz,https://github.com/kubernetes/contrib/pull/2368,https://api.github.com/repos/kubernetes/contrib/issues/2368,Fix URL typo in nginx ingress configuration,Fixes a wrong URL pointing to Go docs.,closed,True,2017-02-05 21:16:03,2017-02-07 19:29:01
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2369,https://api.github.com/repos/kubernetes/contrib/issues/2369,[Ansible] Fix for #2177,"Fix for issue #2177.

***

This slurps the certs from the first master and puts them on all other masters, so they don't fail starting because of the missing certificate (server.crt and server.key).

(This PR goes well with my other PR for the master cert gen in #2356 :wink:)",closed,True,2017-02-06 09:12:46,2017-02-13 12:48:00
contrib,philk,https://github.com/kubernetes/contrib/pull/2370,https://api.github.com/repos/kubernetes/contrib/issues/2370,Return correct ProviderID format from GetAsgNodes,"This matches the way that the [gce does it][1] and fixes issues with the
[cluster state][2].

[1]: https://github.com/kubernetes/contrib/blob/master/cluster-autoscaler/cloudprovider/gce/gce_manager.go#L247
[2]: https://github.com/kubernetes/contrib/issues/2228#issuecomment-274896211",closed,True,2017-02-06 19:48:38,2017-02-22 18:58:20
contrib,bluecamel,https://github.com/kubernetes/contrib/pull/2371,https://api.github.com/repos/kubernetes/contrib/issues/2371,Default to exact match paths from the beginning.,,closed,True,2017-02-06 21:16:11,2017-02-06 21:17:41
contrib,timstclair,https://github.com/kubernetes/contrib/pull/2372,https://api.github.com/repos/kubernetes/contrib/issues/2372,Hack for debugging FROM scratch containers,"Hacky solution to https://github.com/kubernetes/kubernetes/issues/10834 and https://github.com/kubernetes/kubernetes/issues/27140 to unblock migration of system pods to `scratch` base images (https://github.com/kubernetes/kubernetes/issues/40248). This is not meant to be a long term solution, but rather a temporary workaround until we have native Kubernetes support.

The script works by bringing up a busybox pod on the same node as the debug target, mounting the node's root fs, and calling docker directly to copy busybox into the target container. Once the ""install"" is complete, the target can be debugged through a standard `kubectl exec`.

/cc @verb @thockin ",closed,True,2017-02-07 00:42:14,2017-02-28 02:47:43
contrib,kskalski,https://github.com/kubernetes/contrib/issues/2373,https://api.github.com/repos/kubernetes/contrib/issues/2373,[service-loadbalancer] Support using subdomain to select service name,"Default way of routing requests is http://loadbalancer-ip/service-name -> http://service-name:80
however many services break when you access them with a non-empty url root prefix.

I would like loadbalancer to use subdomain name for redirecting to appropriate service, so the routing would be:
http://service-name.loadbalancer-ip -> http://service-name:80

Any hints on how to achieve this?",open,False,2017-02-07 15:14:57,2019-03-18 09:37:41
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2374,https://api.github.com/repos/kubernetes/contrib/issues/2374,[Ansible] Etcd automatic cert generation,"I played around with automatic certificate generation for etcd.

~~I haven't had time to test this yet~~, but I can already say that switching an etcd cluster from http to https most likely would require some manual work (switching the etcd member urls to https).

I would love to get some feedback on my changes for adding automatic etcd cert generation and hopefully see this merged to make securing etcd simpler using the Ansible deployment simpler.

Currently there aren't that many configurable variables added for the certificate generation, but this could be also implemented later on.",closed,True,2017-02-09 12:59:31,2017-02-21 14:15:50
contrib,noseka1,https://github.com/kubernetes/contrib/issues/2375,https://api.github.com/repos/kubernetes/contrib/issues/2375,[nginx-ingress-controller] Cannot handle host names with more than 46 characters,"Test environment:
Kubernetes v1.6.0-alpha.1
nginx-ingress-controller 0.9.0-beta.1

The definition of the ingress I'm creating, test-ingress.yml:

```
apiVersion: ""extensions/v1beta1""
kind: ""Ingress""
metadata:
  name: ""test""
spec:
  rules:
    - host: ""t123456789.t123456789.t123456789.t123456789.t1""
      http:
        paths:
          - backend:
              serviceName: ""test""
              servicePort: ""test""
```
Create the ingress with:
```
kubectl apply -f test-ingress.yml
```
The log output shows that nginx configuration has been reloaded successfully:

```
I0209 16:44:40.654949       7 event.go:217] Event(api.ObjectReference{Kind:""Ingress"", Namespace:""default"", Name:""test"", UID:""966f2c4d-eee4-11e6-b1ce-fa163e0b0827"", APIVersion:""extensions"", ResourceVersion:""135085"", FieldPath:""""}): type: 'Normal' reason: 'UPDATE' Ingress default/test
I0209 16:44:42.766464       7 controller.go:408] ingress backend successfully reloaded...
```
When I change the host name in my ingress definition to ""t123456789.t123456789.t123456789.t123456789.t12"" (appended one more character) the nginx fails to reload:
```
I0209 16:47:55.547907       7 event.go:217] Event(api.ObjectReference{Kind:""Ingress"", Namespace:""default"", Name:""test"", UID:""966f2c4d-eee4-11e6-b1ce-fa163e0b0827"", APIVersion:""extensions"", ResourceVersion:""135453"", FieldPath:""""}): type: 'Normal' reason: 'UPDATE' Ingress default/test
W0209 16:48:02.733522       7 queue.go:87] requeuing kube-system/ingress-controller-leader, err 
-------------------------------------------------------------------------------
Error: exit status 1
2017/02/09 16:48:02 [emerg] 2323#2323: could not build server_names_hash, you should increase server_names_hash_bucket_size: 64
nginx: [emerg] could not build server_names_hash, you should increase server_names_hash_bucket_size: 64
nginx: configuration file /tmp/nginx-cfg375154267 test failed
```
I would expect to be able to create host names up to 64 characters long.",closed,False,2017-02-09 16:54:28,2017-02-13 08:44:13
contrib,cmluciano,https://github.com/kubernetes/contrib/pull/2376,https://api.github.com/repos/kubernetes/contrib/issues/2376,[404 server] Add prometheus metrics,"Metrics tracked:
- total number of requests
- duration of request
- status codes and method

I am representing duration for the request in milliseconds because my
test request almost always returned way under 1 second.",closed,True,2017-02-10 17:40:42,2017-03-07 18:13:12
contrib,xialonglee,https://github.com/kubernetes/contrib/pull/2377,https://api.github.com/repos/kubernetes/contrib/issues/2377,Copy tokens to other masters,"Before this PR, in multi-masters case, only first master(`groups['masters'][0]`) got the tokens.
This PR will copy tokens to all masters. It's helpful for setup a HA kubernetes cluster.",closed,True,2017-02-11 07:40:23,2017-02-14 16:41:51
contrib,xialonglee,https://github.com/kubernetes/contrib/pull/2378,https://api.github.com/repos/kubernetes/contrib/issues/2378,Fix wrong task tags name,"The later tags name overrides the former, fixed this.",closed,True,2017-02-11 08:04:43,2017-02-11 09:11:47
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2379,https://api.github.com/repos/kubernetes/contrib/issues/2379,set additional ansible extra vars via env when running Vagrantfile,"When ANSIBLE_EXTRA_VARS is set in a form of ""key=value key=value ..."" form,
all key=value pairs are set in extra vars when ansible deployment is run.
Both key and value must be whitespace-free finite string.

E.g. export ANSIBLE_EXTRA_VARS=""kube_source_type=github-release kube_master_api_port=6443 kube_version=1.4.8""

This comes handy when there is a need to change some default vars instead of modifying them manually.",closed,True,2017-02-12 14:25:53,2017-02-14 15:59:15
contrib,xialonglee,https://github.com/kubernetes/contrib/pull/2380,https://api.github.com/repos/kubernetes/contrib/issues/2380,[ansible]fix comment,see https://github.com/kubernetes/contrib/blob/master/init/systemd/environ/config,closed,True,2017-02-13 06:38:24,2017-04-13 01:26:03
contrib,davidopp,https://github.com/kubernetes/contrib/issues/2381,https://api.github.com/repos/kubernetes/contrib/issues/2381,Make cluster autoscaler work with Kubernetes 1.6,"Taints are being moved from annotation to field of Node. See kubernetes/kubernetes#25320 and kubernetes/kubernetes#38957

There's no 1.6 milestone in this repo, but it is needed for 1.6.
",closed,False,2017-02-14 08:47:53,2017-03-04 09:50:08
contrib,davidopp,https://github.com/kubernetes/contrib/issues/2382,https://api.github.com/repos/kubernetes/contrib/issues/2382,Make rescheduler work with Kubernetes 1.6,"Taints are being moved from annotation to field of Node, and tolerations are being moved from annotation to field of PodSpec. See kubernetes/kubernetes#25320 and kubernetes/kubernetes#38957

There's no 1.6 milestone in this repo, but it is needed for 1.6.
",closed,False,2017-02-14 08:48:39,2017-03-15 10:42:49
contrib,1071496910,https://github.com/kubernetes/contrib/issues/2383,https://api.github.com/repos/kubernetes/contrib/issues/2383,flag --ingress-class  not work,"log for ingress-controller:
![image](https://cloud.githubusercontent.com/assets/9696208/22924292/8aa08300-f2df-11e6-8eff-d58b41ed50a2.png)

ingress:
![image](https://cloud.githubusercontent.com/assets/9696208/22924158/10968c6c-f2df-11e6-8b0f-e6c70a70fbdc.png)
nginx.conf
![image](https://cloud.githubusercontent.com/assets/9696208/22924249/61965656-f2df-11e6-8bb2-c4fb8b0ce8ad.png)


only test-ingress2 has location /pear2 

is this a bug?
",closed,False,2017-02-14 10:01:48,2018-02-19 07:03:59
contrib,cdemers,https://github.com/kubernetes/contrib/pull/2384,https://api.github.com/repos/kubernetes/contrib/issues/2384,[keepalived-vip] Support for configurable virtual router id,"Hi all, I submit this small PR.

It adds support for configurable keepalived _virtual router id_.   This functionality is necessary in order to be able to run multiple keepalived ""clusters"" in the same subnet without having them stepping on each other's toes (having one node from one group confusing the other group into thinking it has a master).

The configuration of the _virtual router id_ is done using the `--router-id` command line flag. I considered using the project's _configmap_ but the structure of the existing _configmap_ is not easily expandable, so I had to use a flag (I would have added support for environment variables, but it didn't seems to be in line with the way this application is thought out, so I refrained).

The flag can be specified in a k8s object with the `args` key, for example:
`- args: [""--services-configmap=admin/vip-configmap"", ""--router-id=45""]`

If not specified, it will default to router id _50_ as it was before.

That's pretty much all there is to it, thanks in advance for your time!",closed,True,2017-02-14 17:05:20,2017-03-29 05:04:15
contrib,fate-grand-order,https://github.com/kubernetes/contrib/pull/2385,https://api.github.com/repos/kubernetes/contrib/issues/2385,"fix misspell ""environment"" in helpers.go",,closed,True,2017-02-15 02:12:32,2017-02-15 02:23:33
contrib,timstclair,https://github.com/kubernetes/contrib/pull/2386,https://api.github.com/repos/kubernetes/contrib/issues/2386,Rebase 404-server on scratch & don't run as root,"As part of the effort behind kubernetes/kubernetes#40248, I'm cutting out unnecessary dependencies from system containers by rebasing them on scratch (for containers without any external dependencies). In addition, this does not require any elevated privileges, so run as the nobody user instead of root.

For debugging scratch containers, see kubernetes/contrib#2372

/cc @ixdy",closed,True,2017-02-15 21:51:31,2017-02-28 01:58:04
contrib,fanhaozzu,https://github.com/kubernetes/contrib/issues/2387,https://api.github.com/repos/kubernetes/contrib/issues/2387,[kubernetes ingress] nginx ingress static ip example just can run in gce or aws？,"https://github.com/kubernetes/ingress/tree/master/examples/static-ip/nginx
i assign a unused ip as loadBalancerIP.
when kubectl create -f static-ip-svc.yaml, the EXTERNAL-IP of  nginx-ingress-lb is keeping pending.

## **static-ip-svc.yaml**

apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress-lb
  annotations:
    service.beta.kubernetes.io/external-traffic: OnlyLocal
  labels:
    app: nginx-ingress-lbooooooo
spec:
  type: LoadBalancer
  loadBalancerIP: 192.168.65.64
  ports:
  - port: 8992
    name: http
    targetPort: 80
  selector:
    k8s-app: nginx-ingressclass-lb

## **kubectl-1.3  get service**
NAME                      CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE
nginx-ingress-lb          172.20.163.1     pending        8992/TCP,4434/TCP            13h

## **kubectl-1.3  describe service nginx-ingress-lb**

Name:			nginx-ingress-lb
Namespace:		default
Labels:			app=nginx-ingress-lbooooooo
Selector:		k8s-app=nginx-ingressclass-lb
Type:			LoadBalancer
IP:			172.20.163.1
Port:			http	8992/TCP
NodePort:		http	32568/TCP
Endpoints:		172.24.84.5:80
Port:			https	4434/TCP
NodePort:		https	30380/TCP
Endpoints:		172.24.84.5:443
Session Affinity:	None
No events.

## **kubectl-1.3  get service nginx-ingress-lb -o json**
{
    ""kind"": ""Service"",
    ""apiVersion"": ""v1"",
    ""metadata"": {
        ""name"": ""nginx-ingress-lb"",
        ""namespace"": ""default"",
        ""selfLink"": ""/api/v1/namespaces/default/services/nginx-ingress-lb"",
        ""uid"": ""63f620c1-f380-11e6-91c5-fa163ef2d8bb"",
        ""resourceVersion"": ""3285159"",
        ""creationTimestamp"": ""2017-02-15T13:12:23Z"",
        ""labels"": {
            ""app"": ""nginx-ingress-lbooooooo""
        },
        ""annotations"": {
            ""service.beta.kubernetes.io/external-traffic"": ""OnlyLocal""
        }
    },
    ""spec"": {
        ""ports"": [
            {
                ""name"": ""http"",
                ""protocol"": ""TCP"",
                ""port"": 8992,
                ""targetPort"": 80,
                ""nodePort"": 32568
            },
            {
                ""name"": ""https"",
                ""protocol"": ""TCP"",
                ""port"": 4434,
                ""targetPort"": 443,
                ""nodePort"": 30380
            }
        ],
        ""selector"": {
            ""k8s-app"": ""nginx-ingressclass-lb""
        },
        ""clusterIP"": ""172.20.163.1"",
        ""type"": ""LoadBalancer"",
        ""sessionAffinity"": ""None"",
        ""loadBalancerIP"": ""192.168.65.64""
    },
    ""status"": {
        ""loadBalancer"": {}
    }
}",closed,False,2017-02-16 02:32:59,2018-02-23 16:47:54
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/2388,https://api.github.com/repos/kubernetes/contrib/issues/2388,[ansible] update fedora vagrant box version,,closed,True,2017-02-16 20:19:04,2017-02-23 13:04:02
contrib,calebamiles,https://github.com/kubernetes/contrib/issues/2389,https://api.github.com/repos/kubernetes/contrib/issues/2389,* Add SIG field to `test/test_owners.csv`,* Populate SIG field for each entry,closed,False,2017-02-16 23:08:11,2017-03-09 17:12:01
contrib,ixdy,https://github.com/kubernetes/contrib/pull/2390,https://api.github.com/repos/kubernetes/contrib/issues/2390,Change base image for rescheduler to busybox,"Tested in my own cluster against the `[k8s.io] Rescheduler [Serial] should ensure that critical pod is scheduled in case there is no resources available` test and it passed.

Note that because we write to `/var/log/rescheduler.log` on the master via stdout/stderr redirect, we need busybox and we need to run as root.

cc @timstclair ",closed,True,2017-02-16 23:41:45,2017-02-22 11:08:29
contrib,unicell,https://github.com/kubernetes/contrib/pull/2391,https://api.github.com/repos/kubernetes/contrib/issues/2391,Add Docker upstart dependency on Flannel for Ubuntu 14.04,"Also adding respawn delay to avoid Docker respawning too fast

[1] http://upstart.ubuntu.com/cookbook/#delay-respawn-of-a-job",closed,True,2017-02-17 06:58:40,2018-02-19 12:09:00
contrib,guillaumebreton,https://github.com/kubernetes/contrib/pull/2392,https://api.github.com/repos/kubernetes/contrib/issues/2392,Fix typos in zkGenConfig.sh logs,This patch fix two typos in the zkGenConfig.sh script logs,closed,True,2017-02-17 10:31:38,2017-03-14 17:44:36
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2393,https://api.github.com/repos/kubernetes/contrib/issues/2393,Remove old logging info and replace with a link to the docs,Container images and the general approach are obsolete and duplicating the documentation.,closed,True,2017-02-17 12:21:12,2017-03-01 15:23:04
contrib,metal3d,https://github.com/kubernetes/contrib/issues/2394,https://api.github.com/repos/kubernetes/contrib/issues/2394,Cannot have a valid client certificate,"As in #651 I will explain my problem here with more details.

Installed on CentOS 7, kubernetes is now working. I want to access UI from my computer and give access to other peaple.

I tried this:

```bash
# on my computer
openssl genrsa -out client.key 1024
openssl req -key client.key -new -out client.csr

# Passing my key and csr to kubernetes server, I now
# create a pem file
openssl x509 -req -in client.csr \
    -CA /etc/kubernetes/certs/server.crt \
    -CAkey /etc/kubernetes/certs/server.key \
    -CAcreateserial \
    -out client.pem \
    -days 3650
```

It's OK, I now get back my client.pem file and create a pkcs12 file on my computer:

```bash
openssl pkcs12 -export -out client.pfx -in client.pem -inkey client.key 
```

I finally import that pfx file into Chrome, the given certificate is prompted while I'm connecting to my UI. But I always get ""Unauthorized"" response.

I tried different options, when I generating pkcs12 file I tried to add ""-certfile""  with ca.crt, server.crt, and so on...

Is anyone here can tell me if there is a solution, or if I do something wrong.
",closed,False,2017-02-19 10:08:15,2017-03-14 11:36:24
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2395,https://api.github.com/repos/kubernetes/contrib/issues/2395,generate a list of years from 2014 to the current year,Not the best solution but at least we don't have to extend the list each year.,closed,True,2017-02-20 08:00:02,2017-02-20 15:13:46
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2396,https://api.github.com/repos/kubernetes/contrib/issues/2396,Cluster-autoscaler: api type object for status reporting,As the PR with api (https://github.com/kubernetes/kubernetes/pull/40513) rose some controversy I would like to add it temporarily add it to CA codebase so that CA can use the struct to build the status information. If the PR is approved the api struct will simply be replaced with the one from K8S codebase. If not - a http endpoint using these structs will be exposed in CA.,closed,True,2017-02-20 15:06:08,2017-02-20 16:39:39
contrib,tjliupeng,https://github.com/kubernetes/contrib/issues/2397,https://api.github.com/repos/kubernetes/contrib/issues/2397,Issue of high CPU cost of Nginx-ingress-controller ,"Hi,

Using the TOP to check the cpu and memory status of K8s cluster host, I found that the CPU cost of Nginx-ingress-controller is always high. If I set the cpu limit to 2, then the CPU time would be almost 200%. If without cpu limit, the CPU time would be almost 400% in a 4-core VM.

Is that normal?

Thanks
Liu Peng",closed,False,2017-02-21 06:14:15,2017-02-22 07:14:00
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2398,https://api.github.com/repos/kubernetes/contrib/issues/2398,Cluster-autoscaler: GetState() - health condition,"This PR is the 1st out of 3 providing ClusterAutoscalerStatus object populated with the current CA state. 

cc: @jszczepkowski @fgrzadkowski   ",closed,True,2017-02-21 10:31:29,2017-02-21 12:33:03
contrib,fate-grand-order,https://github.com/kubernetes/contrib/pull/2399,https://api.github.com/repos/kubernetes/contrib/issues/2399,"fix misspell ""inspect"" in config.go",,closed,True,2017-02-21 14:49:49,2017-02-23 13:36:08
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2400,https://api.github.com/repos/kubernetes/contrib/issues/2400,Cluster-autoscaler: getStatus() - scale up,"Follow up on 2398.

cc: @jszczepkowski @fgrzadkowski ",closed,True,2017-02-21 15:23:56,2017-02-21 15:38:09
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2401,https://api.github.com/repos/kubernetes/contrib/issues/2401,Cluster-autoscaler: GetStatus - scaleDown,"Follow up of #2400. 

cc: @jszczepkowski @fgrzadkowski ",closed,True,2017-02-21 20:59:15,2017-02-22 10:53:27
contrib,dhawal55,https://github.com/kubernetes/contrib/issues/2402,https://api.github.com/repos/kubernetes/contrib/issues/2402,cluster-autoscaler assumes kube-system as its namespace,"I'm trying to run cluster-autoscaler in a namespace other than kube-system and get below error:
`leaderelection.go:210] failed to renew lease kube-system/cluster-autoscaler`

The leaderElection section assumes that cluster-autoscaler is running in kube-system namespace. It should lookup the namespace OR at-least update the ReadMe to mention that this will only run in kube-system namespace.",closed,False,2017-02-22 00:09:19,2017-04-14 05:34:14
contrib,evgf,https://github.com/kubernetes/contrib/issues/2403,https://api.github.com/repos/kubernetes/contrib/issues/2403,NGINX cannot access basic auth data in /etc (nginx-ingress-controller:0.9.0-beta.1),"Hello,
When upgrading the NGINX Ingress Controller image from v. 0.8.3 to v. 0.9.0-beta.1 (gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.1) we discovered that all our Ingresses using basic authentication do not work (HTTP 500). 
The reason is that the auth data are stored in /etc/ingress-controller/auth but the privileges of the /etc directory were changed from 755 (in 0.8.3) to 750 (in 0.9.0 beta 1). The NGINX worker processes run as nobody and cannot open these files:
> 2017/02/22 09:45:01 [crit] 341#341: *11 open() ""/etc/ingress-controller/auth/default-kibana.passwd"" failed (13: Permission denied), 

It seems that some work was invested into moving the NGINX-specific files under /ingress-controller but this does not cover the auth data and the new directory also has the 750 privileges.",closed,False,2017-02-22 10:38:03,2017-02-22 12:44:38
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2404,https://api.github.com/repos/kubernetes/contrib/issues/2404,Cluster-autoscaler: skip pods that are being deleted in node drain,"Pods that are still running but are being deleted should not be deleted/moved in CA. 
This also fixes CA against the change in K8S after which the node controller stopped hard-deleting pods (it just sets deletionTimestamp) from unready nodes. With this PR CA will see nodes with all pods under deletion as empty.

cc: @jszczepkowski @gmarek @maciekpytel",closed,True,2017-02-22 13:16:48,2017-02-22 14:19:36
contrib,harobed,https://github.com/kubernetes/contrib/pull/2405,https://api.github.com/repos/kubernetes/contrib/issues/2405,Fix issue #1573 with vagrant shell provisioning,Fix issue [#1573](https://github.com/kubernetes/contrib/issues/1573) with vagrant shell provisioning.,closed,True,2017-02-22 18:55:57,2017-03-01 09:34:48
contrib,harobed,https://github.com/kubernetes/contrib/pull/2406,https://api.github.com/repos/kubernetes/contrib/issues/2406,Rename kube-master => kube-master-1 in README,Rename kube-master => kube-master-1 in README,closed,True,2017-02-23 08:27:56,2017-02-23 11:56:53
contrib,harobed,https://github.com/kubernetes/contrib/pull/2407,https://api.github.com/repos/kubernetes/contrib/issues/2407,Add launch ansible-playbook example in Vagrant README,Add launch ansible-playbook example in Vagrant README.,closed,True,2017-02-23 09:26:43,2018-02-19 13:09:59
contrib,harobed,https://github.com/kubernetes/contrib/issues/2408,https://api.github.com/repos/kubernetes/contrib/issues/2408,Why keep kube-ui in Ansible roles if it is deprecated?,"Why keep kube-ui in [Ansible roles](https://github.com/kubernetes/contrib/blob/master/ansible/roles/kubernetes-addons/tasks/kube-ui.yml) if it is [deprecated](https://github.com/kubernetes/kube-ui) ?

If it is deprecated I suggest to delete it.",closed,False,2017-02-23 09:37:47,2018-02-20 17:38:06
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2409,https://api.github.com/repos/kubernetes/contrib/issues/2409,Cluster-autoscaler: readable status printing,"Example output (github seems to slightly mess up tabs):
```
Health:      Healthy (ready=0 unready=1 notStarted=0 longNotStarted=0 registered=1)
             LastProbeTime:      0001-01-01 00:00:00 +0000 UTC                     
             LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC                     
ScaleUp:     NoActivity (ready=0 registered=1)                                     
             LastProbeTime:      0001-01-01 00:00:00 +0000 UTC                     
             LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC                     
ScaleDown:   NoCandidates (candidates=0)                                           
             LastProbeTime:      0001-01-01 00:00:00 +0000 UTC                     
             LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC                     
                                                                                   
NodeGroups:                                                                        
  Name:        ng1                                                                 
  Health:      Unhealthy (ready=0 unready=1 notStarted=0 longNotStarted=0 registered=1 cloudProviderTarget=5 (min=5, max=5))
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC                   
               LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC                   
  ScaleUp:     NoActivity (ready=0 cloudProviderTarget=5)                          
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC                   
               LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC                   
  ScaleDown:   NoCandidates (candidates=0)                                         
               LastProbeTime:      0001-01-01 00:00:00 +0000 UTC                   
               LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC
```",closed,True,2017-02-23 10:39:46,2017-02-23 14:38:59
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2410,https://api.github.com/repos/kubernetes/contrib/issues/2410,Add generic fluentd image for Stackdriver Logging,"This PR creates a fluentd image which can be configured in any way to export logs to Stackdriver Logging.

CC @igorpeshansky @qingling128",closed,True,2017-02-23 13:20:51,2017-02-26 21:51:17
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2411,https://api.github.com/repos/kubernetes/contrib/issues/2411,Cluster-autoscaler: expand half-deleted pod skip logic in drain,"We should not immediately discard deleting nodes from drain to considering a node as empty immediately after the pod deletion was started.

cc: @jszczepkowski @fgrzadkowski @maciekpytel",closed,True,2017-02-23 13:35:19,2017-02-23 16:44:47
contrib,fate-grand-order,https://github.com/kubernetes/contrib/pull/2412,https://api.github.com/repos/kubernetes/contrib/issues/2412,fix missspell in clusterstate.go,,closed,True,2017-02-23 13:42:17,2017-02-23 13:56:56
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2413,https://api.github.com/repos/kubernetes/contrib/issues/2413,Cluster-autoscaler: implemented LogCollector,,closed,True,2017-02-23 14:45:03,2017-02-23 17:09:06
contrib,piosz,https://github.com/kubernetes/contrib/pull/2414,https://api.github.com/repos/kubernetes/contrib/issues/2414,Intoduced Prometheus to Stackdriver agent,"The agent allows to periodically scrape endpoint in Prometheus format and export metrics to Stackdriver using its Monitoring API.

In the future we may want to introduce the following features:
- scraping multiple endpoints
- support for more metric types (currently it supports Gauge and Cumulative only).",closed,True,2017-02-23 16:11:43,2017-03-14 22:15:44
contrib,grodrigues3,https://github.com/kubernetes/contrib/issues/2415,https://api.github.com/repos/kubernetes/contrib/issues/2415,Add monitoring for test flakes across the project,"Show it as a graph or find a way to present some useful statistics

Data for

- SQ
- PR Builder

Maybe gamify this.

cc @apelisse ",closed,False,2017-02-23 18:26:37,2018-02-19 13:10:00
contrib,andrewstuart,https://github.com/kubernetes/contrib/pull/2416,https://api.github.com/repos/kubernetes/contrib/issues/2416,Fix nil pointer exception when nodegroupfornode is nil,"This fix should prevent a segmentation fault when a NodeGroup comes back as nil from the cloudprovider. I've not been able to track down the root cause, but I figured this might be useful for others.

In case the following is relevant or helpful:

K8s version: 1.5.2
Cluster tool: kops 1.5.1
Cloud: AWS
cluster-autoscaler version: master",closed,True,2017-02-23 22:43:18,2017-03-24 11:08:29
contrib,mmzhou,https://github.com/kubernetes/contrib/issues/2417,https://api.github.com/repos/kubernetes/contrib/issues/2417,Why I can't get the same result with example?,"Hi, I followed the example instructions strictly, but nothing happened after I created the daemonset.

[xxx@xxx]$ kubectl get pods
NAME                READY     STATUS    RESTARTS   AGE
echoheaders-j0t2f   1/1       Running   0          20s
[xxx@xxx]$ kubectl get daemonset
NAME                  DESIRED   CURRENT   READY     NODE-SELECTOR   AGE
kube-keepalived-vip   0         0         0         type=worker     4m

And I didn't found any error in logs. Could you please help me know what's wrong with my try? Thanks!",closed,False,2017-02-24 09:20:05,2018-02-20 17:38:06
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2418,https://api.github.com/repos/kubernetes/contrib/issues/2418,Cluster-autoscaler: bump version to 0.5.0-alpha1 in preparation for the next release,cc: @fgrzadkowski @jszczepkowski @MaciekPytel ,closed,True,2017-02-24 14:10:32,2017-02-24 14:21:26
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2419,https://api.github.com/repos/kubernetes/contrib/issues/2419,Cluster-autoscaler: use listers from ListersRegistry,"Fix after #2226.

cc: @mumoshu @fgrzadkowski @jszczepkowski @MaciekPytel ",closed,True,2017-02-24 19:50:18,2017-02-27 01:05:52
contrib,piosz,https://github.com/kubernetes/contrib/pull/2420,https://api.github.com/repos/kubernetes/contrib/issues/2420,[Rescheduler] Added Prometheus metrics,"ref https://github.com/kubernetes/kubernetes/issues/40736

cc @fabxc @brancz ",closed,True,2017-02-25 12:21:34,2017-03-10 13:47:29
contrib,mhrivnak,https://github.com/kubernetes/contrib/pull/2421,https://api.github.com/repos/kubernetes/contrib/issues/2421,typo fix,,closed,True,2017-02-26 02:50:27,2017-02-27 00:51:08
contrib,fate-grand-order,https://github.com/kubernetes/contrib/pull/2422,https://api.github.com/repos/kubernetes/contrib/issues/2422,fix misspell “inspect” in config.go,,closed,True,2017-02-26 09:10:56,2017-02-27 00:50:19
contrib,acca,https://github.com/kubernetes/contrib/pull/2423,https://api.github.com/repos/kubernetes/contrib/issues/2423,ansible|contiv: Fix tasks PkgMgr CentOS with correct URL,"As can be seen from here: https://repos.fedorapeople.org/repos/openstack/openstack-kilo/ the old url of the rpm package is not available anymore. 
Here is a PR that solves this issue.",closed,True,2017-02-26 14:57:11,2017-02-26 15:10:38
contrib,gkopylov,https://github.com/kubernetes/contrib/pull/2424,https://api.github.com/repos/kubernetes/contrib/issues/2424,Add ability to set multiple services on the same ip,Now you can use single ip for multiple services.,closed,True,2017-02-26 15:01:05,2018-03-02 18:34:15
contrib,ryanwalls,https://github.com/kubernetes/contrib/issues/2425,https://api.github.com/repos/kubernetes/contrib/issues/2425,cluster-autoscaler: Remove minimum ASG size requirement of 1,"I use 3 different ASGs for my cluster.  One of them I have set to a minimum of 3 for my cluster's main components to live on.  The other two ASGs are only for batch jobs that require specific types of instances.  I'd like to be able to have the other two ASGs have a minimum count of 0.  

e.g.  I'd like this configuration to be valid:
--nodes=3:10:primary-asg
--nodes=0:10:batch-job-a-asg
--nodes=0:10:batch-job-b-asg

I specifically use the 2nd two ASGs in conjunction with Kubernetes Jobs, so I don't need those instances sitting around when no jobs are running.",closed,False,2017-02-26 17:48:04,2017-06-22 12:20:15
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2426,https://api.github.com/repos/kubernetes/contrib/issues/2426,Move fluentd-gcp image to contrib repo,"Replacement for https://github.com/kubernetes/contrib/pull/2410, complimentary for https://github.com/kubernetes/kubernetes/pull/42127

Moving to contrib, because it's going to be maintained by Stackdriver team

CC @igorpeshansky, @qingling128 and @dhrupadb",closed,True,2017-02-26 21:50:37,2017-03-02 12:37:10
contrib,bryanbecker,https://github.com/kubernetes/contrib/pull/2427,https://api.github.com/repos/kubernetes/contrib/issues/2427,Change ansible files to respect etcd_client_port variable,"The current implementation is inconsistent, with some roles using `etcd_client_port`, and some roles hard-coding ""2379"" as the port.

This patch fixes those situations.  I also added `etcd_client_port` into `group_vars` so the user only needs to set it once (instead of setting it per role)",closed,True,2017-02-27 01:29:08,2017-02-27 11:18:02
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2428,https://api.github.com/repos/kubernetes/contrib/issues/2428,Cluster-Autoscaler: fix segfault,"StaticAutoscaler.kubeClient was uninitialized,
leading to segfaults when trying to use it. It was
also a duplicate since the client is already available
through AutoscalingContext.",closed,True,2017-02-27 13:19:28,2017-02-27 13:30:37
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2429,https://api.github.com/repos/kubernetes/contrib/issues/2429,Cluster-Autoscaler: Write status to configmap,,closed,True,2017-02-27 16:32:28,2017-02-28 12:22:04
contrib,fate-grand-order,https://github.com/kubernetes/contrib/pull/2430,https://api.github.com/repos/kubernetes/contrib/issues/2430,"fix misspell ""being"" in drain.go",,closed,True,2017-02-28 10:20:20,2017-03-02 01:48:38
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2431,https://api.github.com/repos/kubernetes/contrib/issues/2431,Ca configmap cleanup,,closed,True,2017-02-28 16:26:21,2017-03-02 02:41:32
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2432,https://api.github.com/repos/kubernetes/contrib/issues/2432,Cluster-Autoscaler: add flag to disable status configmap,,closed,True,2017-03-01 10:02:20,2017-03-02 15:36:13
contrib,ryanwalls,https://github.com/kubernetes/contrib/issues/2433,https://api.github.com/repos/kubernetes/contrib/issues/2433,cluster-autoscaler: Add debug logging and flag to help troubleshoot scaling issues,Using release 0.4.0 I have 7 nodes in my ASG and at least 3 of them are running well below 50% utilization and I think should be candidates for scale-in.  It would be great if I could run the autoscaler with a debug flag so it would print out hints as to why each node isn't eligible for scale in.  ,closed,False,2017-03-01 12:02:11,2017-10-16 16:23:44
contrib,ryanwalls,https://github.com/kubernetes/contrib/issues/2434,https://api.github.com/repos/kubernetes/contrib/issues/2434,"cluster-autoscaler: Feature request: publish ""latest"" or ""beta"" docker image ","I don't see any Docker image past 0.4.0 on the registry.  Would be great to try some of the latest additions in a ""latest"" or some sort of ""beta"" tagged Docker image.",closed,False,2017-03-01 12:07:57,2017-04-19 15:30:50
contrib,ryanwalls,https://github.com/kubernetes/contrib/issues/2435,https://api.github.com/repos/kubernetes/contrib/issues/2435,cluster autoscaler: Kubernetes scheduler places Jobs on AWS nodes that are scaling in,"Seems there is a race condition between cluster autoscaler (CA) and the kubernetes job scheduler.  Using CA 0.4.0 and Kubernetes 1.5.2 we are getting Jobs scheduled on nodes that are being terminated by the CA.  This is in AWS.

The sequence of events seems to be:
* CA scales up a node
* Node becomes available for scale in
* CA scales in node
* Kubernetes scheduler schedules a Job on the node that is in the process of terminating.
* Job fails because instance dies
",closed,False,2017-03-01 17:48:13,2017-04-19 16:49:48
contrib,xialonglee,https://github.com/kubernetes/contrib/pull/2436,https://api.github.com/repos/kubernetes/contrib/issues/2436,[Ansible]fix etcd start twice,"There are two register variables use the same name `etcd_started`, the first one was defined in `etcd-start.yml` and the second one was in `etcd2-start.yml`.
The later var will override the previous var even though the task was skipped, see 

http://docs.ansible.com/ansible/playbooks_variables.html#registered-variables

> Note
If a task fails or is skipped, the variable still is registered with a failure or skipped status, the only way to avoid registering a variable is using tags.

In not-coreos case, this will cause etcd restart immediately during its first start process. 
It is not harmless, it will occationally cause etcd unhealthy and could not start normally again , see https://github.com/kubernetes-incubator/kargo/issues/342",closed,True,2017-03-02 05:24:14,2017-03-14 12:17:59
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2437,https://api.github.com/repos/kubernetes/contrib/issues/2437,Cluster-autoscaler: godep update,"Sync with k8s 1.6.

cc: @jszczepkowski @MaciekPytel @piosz ",closed,True,2017-03-02 13:30:15,2017-03-02 14:30:41
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2438,https://api.github.com/repos/kubernetes/contrib/issues/2438,Cluster-Autoscaler events on status configmap,Write events on status configmap on scale up / scale down. Moved writing to status configmap to a separate file (clusterstate/utils/status.go) to clean up and also allow reasonably isolated unittests.,closed,True,2017-03-02 14:06:35,2017-03-06 12:07:19
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2439,https://api.github.com/repos/kubernetes/contrib/issues/2439,Add mwilegus to /hack/OWNERS,Cluster Autoscaler is an actively developer project and from time to time we have to add couple flags to it. It would be good to be able to have an approval rights for verify-flags. ,closed,True,2017-03-02 14:56:16,2017-03-02 15:20:45
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/2440,https://api.github.com/repos/kubernetes/contrib/issues/2440,cluster-autoscaler: leader election problems in GKE,"CA fails to start due to:

```
more /var/log/cluster-autoscaler.log 
I0302 14:36:51.206397       5 cluster_autoscaler.go:353] Cluster Autoscaler 0.4.0
I0302 14:36:51.220822       5 leaderelection.go:210] failed to renew lease kube-system/cluster-autoscaler
I0302 14:36:54.702631       5 leaderelection.go:210] failed to renew lease kube-system/cluster-autoscaler
I0302 14:36:58.984582       5 leaderelection.go:210] failed to renew lease kube-system/cluster-autoscaler
I0302 14:37:02.582302       5 leaderelection.go:210] failed to renew lease kube-system/cluster-autoscaler
I0302 14:37:05.945697       5 leaderelection.go:210] failed to renew lease kube-system/cluster-autoscaler
I0302 14:37:08.989152       5 leaderelection.go:210] failed to renew lease kube-system/cluster-autoscaler

```",closed,False,2017-03-02 16:57:56,2017-04-19 15:26:46
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2441,https://api.github.com/repos/kubernetes/contrib/issues/2441,"[Ansible] Added *_options var to apiserver, controller-manager, scheduler, kubelet (and flannel)","Changes:
* Added *_options var to apiserver, controller-manager, scheduler, kubelet and proxy
* Renamed the flannel_opts to be like the others, flannel_options
* Added the defaults for *_options to the defaults file and not the template",closed,True,2017-03-03 07:26:11,2017-03-15 00:50:54
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2442,https://api.github.com/repos/kubernetes/contrib/issues/2442,[Ansible] Use slurp and copy instead of synchronize,"Ansible synchronize module is a huge trouble for users with a ssh gateway or just extremely strict sshd config(s), not allowing direct server to server, and only playbook-runner to server connections.
Only the know_tokens file is copied, but for **other** masters more isn't needed.

/cc @xialonglee Do you think this is ""okay"" to change to only copy the `know_tokens.csv`?",closed,True,2017-03-03 07:31:57,2017-03-07 06:02:49
contrib,fate-grand-order,https://github.com/kubernetes/contrib/pull/2443,https://api.github.com/repos/kubernetes/contrib/issues/2443,"fix misspell ""attempts"" in exechealthz.go",,closed,True,2017-03-03 09:01:54,2017-09-08 22:50:07
contrib,muffin87,https://github.com/kubernetes/contrib/issues/2444,https://api.github.com/repos/kubernetes/contrib/issues/2444,Nginx-Ingress: Docker Registry behind Ingress - docker push don't work,"Hi everyone,

im running K8s version 1.5.2 on CentOS.
I wanted to deploy a vanilla docker registry and expose the service with Nginx Ingress controller.

# Issue

`docker push` does not work with the nginx-ingress-controller in front of the docker-registry.

It seems to me that something in the nginx-ingress-controller config is blocking the `docker push` payload upload (tar file).

Any idea whats happening here and how I can solve this?

# Error Message:

docker-client:
`Error: Status 404 trying to push repository nginx: ""404 page not found\n""`

dockerd
`Upload failed: EOF`

```
Mar  3 11:29:51 moby root: time=""2017-03-03T11:29:51.177922934Z"" level=debug msg=""Pushing layer: sha256:809c8c0dd73c9081a8ab9795b5fb9beab60c7f602a0886dbbdf48e939d608163""
Mar  3 11:29:51 moby root: time=""2017-03-03T11:29:51.401819584Z"" level=debug msg=""Assembling tar data for d388be1b57d4bbb37c16836b3fafba89e433a413f7448680d6992ecfe4f07089""
Mar  3 11:29:51 moby root: time=""2017-03-03T11:29:51.416382683Z"" level=debug msg=""Assembling tar data for 6780c17ba8c81862656e46d0812c0eaf7fc2eb55447cf1f71b1e3e64e548256b""
Mar  3 11:29:51 moby root: time=""2017-03-03T11:29:51.416555479Z"" level=debug msg=""Assembling tar data for 6c84bf0c3c99e5454f5536068db727174c4dea0add6c7c54052604dbbba4a46a""
Mar  3 11:29:51 moby root: time=""2017-03-03T11:29:51.519050420Z"" level=error msg=""Upload failed, retrying: EOF""
Mar  3 11:29:51 moby root: time=""2017-03-03T11:29:51.537034379Z"" level=error msg=""Upload failed, retrying: EOF""
Mar  3 11:29:51 moby root: time=""2017-03-03T11:29:51.541124148Z"" level=error msg=""Upload failed, retrying: EOF""
Mar  3 11:29:53 moby root: time=""2017-03-03T11:29:53.983465337Z"" level=debug msg=""Pushing layer: sha256:e8fa134cb7b8a5e2943c9ea6e48a90c9f7fc80fd26780ce9d74bbab927a6cfc5""
Mar  3 11:29:54 moby root: time=""2017-03-03T11:29:54.227004087Z"" level=debug msg=""Assembling tar data for b3c45395c4b867925ed531458101d8c5cb8c5462f06a678c8132f06e2db8c588""
Mar  3 11:29:54 moby root: time=""2017-03-03T11:29:54.345157833Z"" level=error msg=""Upload failed, retrying: EOF""
Mar  3 11:30:11 moby root: time=""2017-03-03T11:30:11.520650325Z"" level=debug msg=""Pushing layer: sha256:7cbcbac42c44c6c38559e5df3a494f44987333c8023a40fec48df2fce1fc146b""
Mar  3 11:30:11 moby root: time=""2017-03-03T11:30:11.537773800Z"" level=debug msg=""Pushing layer: sha256:e8d45b8ab3ca8a83ba4e340f4b901c593da00f7f2bb068937a8b32b3beb606af""
Mar  3 11:30:11 moby root: time=""2017-03-03T11:30:11.541707653Z"" level=debug msg=""Pushing layer: sha256:809c8c0dd73c9081a8ab9795b5fb9beab60c7f602a0886dbbdf48e939d608163""
Mar  3 11:30:11 moby root: time=""2017-03-03T11:30:11.744771310Z"" level=debug msg=""Assembling tar data for 6c84bf0c3c99e5454f5536068db727174c4dea0add6c7c54052604dbbba4a46a""
Mar  3 11:30:11 moby root: time=""2017-03-03T11:30:11.756433036Z"" level=debug msg=""Assembling tar data for 6780c17ba8c81862656e46d0812c0eaf7fc2eb55447cf1f71b1e3e64e548256b""
Mar  3 11:30:11 moby root: time=""2017-03-03T11:30:11.760178212Z"" level=debug msg=""Assembling tar data for d388be1b57d4bbb37c16836b3fafba89e433a413f7448680d6992ecfe4f07089""
Mar  3 11:30:11 moby root: time=""2017-03-03T11:30:11.851262730Z"" level=error msg=""Upload failed: EOF""
Mar  3 11:30:11 moby root: time=""2017-03-03T11:30:11.856225668Z"" level=error msg=""Upload failed: EOF""
Mar  3 11:30:11 moby root: time=""2017-03-03T11:30:11.863372727Z"" level=error msg=""Upload failed: EOF""
Mar  3 11:30:11 moby root: time=""2017-03-03T11:30:11.863480799Z"" level=error msg=""Attempting next endpoint for push after error: EOF""
```

nginx-ingress log:

```
<censored> - ucpapp [03/Mar/2017:11:29:20 +0000] ""HEAD /v2/nginx/blobs/sha256:ba1ed986f6a0df1b3fa02fe8cf76e45e7f7ae248c2a2d9f6ce901585c38d5a79 HTTP/1.1"" 404 0 ""-"" ""docker/17.03.0-ce go/go1.7.5 git-commit/3a232c8 kernel/4.9.12-moby os/linux arch/amd64 UpstreamClient(Docker-Client/17.03.0-ce \x5C(darwin\x5C))"" ""-""
<censored> - ucpapp [03/Mar/2017:11:29:20 +0000] ""POST /v2/nginx/blobs/uploads/ HTTP/1.1"" 202 0 ""-"" ""docker/17.03.0-ce go/go1.7.5 git-commit/3a232c8 kernel/4.9.12-moby os/linux arch/amd64 UpstreamClient(Docker-Client/17.03.0-ce \x5C(darwin\x5C))"" ""-""
<censored> - ucpapp [03/Mar/2017:11:29:20 +0000] ""POST /v2/nginx/blobs/uploads/ HTTP/1.1"" 202 0 ""-"" ""docker/17.03.0-ce go/go1.7.5 git-commit/3a232c8 kernel/4.9.12-moby os/linux arch/amd64 UpstreamClient(Docker-Client/17.03.0-ce \x5C(darwin\x5C))"" ""-""
<censored> - ucpapp [03/Mar/2017:11:29:21 +0000] ""POST /v2/nginx/blobs/uploads/ HTTP/1.1"" 202 0 ""-"" ""docker/17.03.0-ce go/go1.7.5 git-commit/3a232c8 kernel/4.9.12-moby os/linux arch/amd64 UpstreamClient(Docker-Client/17.03.0-ce \x5C(darwin\x5C))"" ""-""
<censored> - ucpapp [03/Mar/2017:11:29:21 +0000] ""HEAD /v2/nginx/blobs/sha256:a362ec94dcabb258895c0cde3905dafd6c40df5ffa4a668fdf6b175655774192 HTTP/1.1"" 404 0 ""-"" ""docker/17.03.0-ce go/go1.7.5 git-commit/3a232c8 kernel/4.9.12-moby os/linux arch/amd64 UpstreamClient(Docker-Client/17.03.0-ce \x5C(darwin\x5C))"" ""-""
```

# Setup

dockerd -> nginx-ingress -> k8s-service-object -> nginx-for-auth -> registry

* `docker push` through the nginx results in error.
* Skipping the nginx-ingress and pushing to the `k8s-service-object` via NodePort works fine
* `docker login` works fine
* `docker push` manifest files end up in the registry fine

# Configs

rendered nginx-config

```
upstream tools-docker-registry-registry.<censored>-docker-registry {

  server 10.254.4.9:80;
}

server {
  listen 80;

  server_name registry.<censored>;

  location / {
    proxy_http_version 1.1;

    proxy_connect_timeout 60s;
    proxy_read_timeout 60s;
    client_max_body_size 1m;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Host $host;
    proxy_set_header X-Forwarded-Port $server_port;
    proxy_set_header X-Forwarded-Proto $scheme;

    proxy_buffering on;

    proxy_pass http://tools-docker-registry-registry.<censored>-docker-registry;

  }
}
```

registry ingress conf

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: docker-registry
  namespace: tools
spec:
  rules:
    - host: registry.<censored>
      http:
        paths:
        - backend:
            serviceName: docker-registry
            servicePort: 80
```",closed,False,2017-03-03 12:58:31,2018-05-23 13:30:23
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2445,https://api.github.com/repos/kubernetes/contrib/issues/2445,Cluster-autoscaler: precheck that the api server link is ok,"The logs from leader election are super vague. An explicit check is needed to let the user know that the connection could not be established.

cc: @jszczepkowski @MaciekPytel ",closed,True,2017-03-03 13:07:54,2017-03-03 13:57:07
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2446,https://api.github.com/repos/kubernetes/contrib/issues/2446,Cluster-Autoscaler: fix ignoring node groups config,"This bug makes CA pretty much useless.

@mwielgus please review",closed,True,2017-03-03 16:23:38,2017-03-03 16:50:15
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2447,https://api.github.com/repos/kubernetes/contrib/issues/2447,Fix fluentd-gcp image build files,`run.sh` should be runnable,closed,True,2017-03-03 23:38:31,2017-03-06 18:50:24
contrib,xialonglee,https://github.com/kubernetes/contrib/pull/2448,https://api.github.com/repos/kubernetes/contrib/issues/2448,[Ansible]keep consistent with secret files mode,"All certificate files should keep the same permission mode, as the task `Verify certificate permissions` done.",closed,True,2017-03-04 05:50:22,2017-03-08 13:54:49
contrib,kskalski,https://github.com/kubernetes/contrib/pull/2449,https://api.github.com/repos/kubernetes/contrib/issues/2449,"Pass Namespace information about services to template expansion, such…",… that loadbalancing can be scoped / customized using namespace of a service.,open,True,2017-03-04 09:54:27,2019-03-18 09:37:37
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2450,https://api.github.com/repos/kubernetes/contrib/issues/2450,[Ansible] fix cluster_hostname var not being used in all kubeconfigs,"This should fix the issue that the var `cluster_hostname` wasn't used everywhere where it should has.

/cc @talset",closed,True,2017-03-04 15:33:02,2017-03-08 13:31:45
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2451,https://api.github.com/repos/kubernetes/contrib/issues/2451,Cluster-autoscaler: include PodDisruptionBudget in drain - part 1/2,"In part 1 or 2 we skip nodes that have a pod with 0 poddisruptionallowed. Part 2/2 will delete pods using evict.

cc: @jszczepkowski @MaciekPytel @davidopp @fgrzadkowski ",closed,True,2017-03-06 15:39:45,2017-03-06 17:27:51
contrib,talset,https://github.com/kubernetes/contrib/pull/2452,https://api.github.com/repos/kubernetes/contrib/issues/2452,[ansible] add cluster_public_hostname,"The cluster_hostname is already present (to be able to reach the
private/internal vip kube api).
The goal of cluster_public_hostname is to have the same thing but for
public access to the api.

If the public vip domain is not part of the certificat, when you will
try to access to the api via kubectl you will have something like that :

Unable to connect to the server: x509: certificate is valid for
cluster_hostname, master0, master1 ..., not cluster_public_hostname",closed,True,2017-03-06 17:35:03,2017-03-08 13:30:13
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2453,https://api.github.com/repos/kubernetes/contrib/issues/2453,Cluster-Autoscaler: skip nodes currently under deletion in scale down,"Currently we may try to delete the same node multiple times.

cc: @MaciekPytel @jszczepkowski @fgrzadkowski ",closed,True,2017-03-07 13:47:13,2017-03-07 14:20:39
contrib,timstclair,https://github.com/kubernetes/contrib/pull/2454,https://api.github.com/repos/kubernetes/contrib/issues/2454,Pin fluent-plugin-systemd to 0.0.6,"Fixes https://github.com/kubernetes/kubernetes/issues/42515

I will wait for approval prior to pushing the image.

/cc  @igorpeshansky @qingling128",closed,True,2017-03-07 23:44:34,2017-03-10 17:09:39
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2455,https://api.github.com/repos/kubernetes/contrib/issues/2455,Cluster-Autoscaler: Update timestamps in status configmap,Update LastProbeTime and LastTransitionTime fields in ClusterStateRegistry (previously they weren't used and always showed as epoch in status). Update scale down part of status whenever list of unneeded nodes in CA changes.,closed,True,2017-03-08 10:57:07,2017-03-10 17:28:39
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2456,https://api.github.com/repos/kubernetes/contrib/issues/2456,Cluster-autoscaler: version bump to 0.5.0-beta1,cc: @MaciekPytel @jszczepkowski,closed,True,2017-03-08 13:11:48,2017-03-08 13:56:21
contrib,timstclair,https://github.com/kubernetes/contrib/pull/2457,https://api.github.com/repos/kubernetes/contrib/issues/2457,fluent-plugin-systemd to 0.0.8 & cleanup libc6-dev,"Diff: https://github.com/timstclair/kubernetes/commit/e0b766b63b4ebafb3df8acd2cbc4d36abfe81b1e

Changes:

1. The fluent-plugin-systemd regression was fixed in 0.0.8 (https://github.com/reevoo/fluent-plugin-systemd/issues/32)
2. `libc-dev` is an alias for `libc6-dev`, which works on install but not purge for some reason, so a couple build dependencies were being left behind. Changing to `libc6-dev` solves this.
3. All other gem & package changes are just updates since the last image was built.

/cc @piosz ",closed,True,2017-03-09 00:38:01,2017-03-10 17:34:18
contrib,metal3d,https://github.com/kubernetes/contrib/pull/2458,https://api.github.com/repos/kubernetes/contrib/issues/2458,"Fixup ca-cert, private key needs to be kept","To be able to create user certificates, we need to have ca.key on one server (masters).

Fixes #2394
Fixes #651",closed,True,2017-03-09 13:15:01,2017-03-14 12:16:42
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2459,https://api.github.com/repos/kubernetes/contrib/issues/2459,Cluster-Autoscaler: fix delete taint value format,"Fix a bug, where non-compliant value format prevented CA deletetaint from being created (which in turn caused CA node drain to fail).",closed,True,2017-03-09 14:28:25,2017-03-09 15:43:10
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2460,https://api.github.com/repos/kubernetes/contrib/issues/2460,Cluster-autoscaler: fix typo in pdb listener,cc: @MaciekPytel @jszczepkowski,closed,True,2017-03-09 15:40:40,2017-03-09 15:53:13
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2461,https://api.github.com/repos/kubernetes/contrib/issues/2461,Cluster-autoscaler: ready node lister fix,cc: @MaciekPytel @jszczepkowski  ,closed,True,2017-03-09 16:21:07,2017-03-09 16:32:46
contrib,kenden,https://github.com/kubernetes/contrib/pull/2462,https://api.github.com/repos/kubernetes/contrib/issues/2462,Fix typo in doc,,closed,True,2017-03-09 20:01:47,2017-04-19 01:38:02
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2463,https://api.github.com/repos/kubernetes/contrib/issues/2463,Cluster-Autoscaler: fix delete taint failing,"It was using old node version (which in general is always going to be outdated, as we've likely modified it by adding delete taint).

@mwielgus ",closed,True,2017-03-10 11:05:50,2017-03-10 13:40:10
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2464,https://api.github.com/repos/kubernetes/contrib/issues/2464,Cluster-Autoscaler: evict pods instead of deleting them,This should make CA respect PodDisruptionBudget.,closed,True,2017-03-10 13:39:45,2017-03-20 07:28:13
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/2465,https://api.github.com/repos/kubernetes/contrib/issues/2465,Rescheduler needs unit tests,"This has been postponed over and over. Making P0.

cc: @piosz ",closed,False,2017-03-10 13:53:59,2017-04-28 12:46:06
contrib,newtonkishore,https://github.com/kubernetes/contrib/issues/2466,https://api.github.com/repos/kubernetes/contrib/issues/2466,Can the ansible be modified to bring Kuberenetes with Docker 1.13?,"I am using Kubernetes contrib: git clone https://github.com/kubernetes/contrib.git . can this be modified to start with Docker 1.13? Thanks a lot in advance.

Flannel:
journalctl -o cat --unit flanneld
Starting Flanneld overlay address etcd agent...
I0310 12:14:10.553332 1022 main.go:132] Installing signal handlers
I0310 12:14:10.554032 1022 manager.go:136] Determining IP address of default interface
I0310 12:14:10.554443 1022 manager.go:149] Using interface with name eth0 and address 192.168.122.69
I0310 12:14:10.554472 1022 manager.go:166] Defaulting external address to interface address (192.168.122.69)
flanneld.service start operation timed out. Terminating.
I0310 12:15:40.758926 1022 main.go:172] Exiting...
Failed to start Flanneld overlay address etcd agent.
Unit flanneld.service entered failed state.
flanneld.service failed.
flanneld.service holdoff time over, scheduling restart.

Docker:
Started Docker Application Container Engine.
Stopping Docker Application Container Engine...
time=""2017-03-10T11:39:32.553767154-08:00"" level=info msg=""stopping containerd after receiving terminated""
time=""2017-03-10T11:39:32.553951007-08:00"" level=info msg=""Processing signal 'terminated'""
time=""2017-03-10T11:39:32.555340662-08:00"" level=error msg=""libcontainerd: failed to receive event from containerd: rpc error: code = 13 desc = transport is closing""
Stopped Docker Application Container Engine.

This is fixed in 1.13 -> check this link .

The Flanneld is starting fine with Docker 1.13, but Kubernetes installation conflicts with Docker 1.13 using ansible.

ASK [node : CentOS | Install kubernetes node] *********************************
fatal: [192.168.122.120]: FAILED! => {""changed"": true, ""failed"": true, ""msg"": ""Error: docker-ce-selinux conflicts with 2:container-selinux-2.9-4.el7.noarch\nError: docker-ce conflicts with 2:docker-1.12.6-11.el7.centos.x86_64\n"", ""rc"": 1, ""results"": [""Loaded plugins: fastestmirror, langpacks\nLoading mirror speeds from cached hostfile\n * base: pubmirrors.dal.corespace.com\n * epel: reflector.westga.edu\n * extras: mirrors.sonic.net\n * updates: centos.vwtonline.net\nResolving Dependencies\n--> Running transaction check\n---> Package kubernetes-node.x86_64 0:1.5.2-0.2.gitc55cf2b.el7 will be installed\n--> Processing Dependency: socat for package: kubernetes-node-1.5.2-0.2.gitc55cf2b.el7.x86_64\n--> Processing Dependency: docker for package: kubernetes-node-1.5.2-0.2.gitc55cf2b.el7.x86_64\n--> Processing Dependency: conntrack-tools for package: kubernetes-node-1.5.2-0.2.gitc55cf2b.el7.x86_64\n--> Running transaction check\n---> Package conntrack-tools.x86_64 0:1.4.3-1.el7 will be installed\n--> Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.1)(64bit) for package: conntrack-tools-1.4.3-1.el7.x86_64\n--> Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.0)(64bit) for package: conntrack-tools-1.4.3-1.el7.x86_64\n--> Processing Dependency: libnetfilter_cthelper.so.0(LIBNETFILTER_CTHELPER_1.0)(64bit) for package: conntrack-tools-1.4.3-1.el7.x86_64\n--> Processing Dependency: libnetfilter_queue.so.1()(64bit) for package: conntrack-tools-1.4.3-1.el7.x86_64\n--> Processing Dependency: libnetfilter_cttimeout.so.1()(64bit) for package: conntrack-tools-1.4.3-1.el7.x86_64\n--> Processing Dependency: libnetfilter_cthelper.so.0()(64bit) for package: conntrack-tools-1.4.3-1.el7.x86_64\n---> Package docker.x86_64 2:1.12.6-11.el7.centos will be installed\n--> Processing Dependency: docker-common = 2:1.12.6-11.el7.centos for package: 2:docker-1.12.6-11.el7.centos.x86_64\n--> Processing Dependency: docker-client = 2:1.12.6-11.el7.centos for package: 2:docker-1.12.6-11.el7.centos.x86_64\n--> Processing Dependency: container-selinux >= 2:2.9-4 for package: 2:docker-1.12.6-11.el7.centos.x86_64\n---> Package socat.x86_64 0:1.7.2.2-5.el7 will be installed\n--> Running transaction check\n---> Package container-selinux.noarch 2:2.9-4.el7 will be installed\n---> Package docker-client.x86_64 2:1.12.6-11.el7.centos will be installed\n---> Package docker-common.x86_64 2:1.12.6-11.el7.centos will be installed\n---> Package libnetfilter_cthelper.x86_64 0:1.0.0-9.el7 will be installed\n---> Package libnetfilter_cttimeout.x86_64 0:1.0.0-6.el7 will be installed\n---> Package libnetfilter_queue.x86_64 0:1.0.2-2.el7 will be installed\n--> Processing Conflict: docker-ce-17.03.0.ce-1.el7.centos.x86_64 conflicts docker\n--> Processing Conflict: docker-ce-17.03.0.ce-1.el7.centos.x86_64 conflicts docker\n--> Processing Conflict: docker-ce-17.03.0.ce-1.el7.centos.x86_64 conflicts docker-io\n--> Processing Conflict: docker-ce-17.03.0.ce-1.el7.centos.x86_64 conflicts docker-io\n--> Processing Conflict: docker-ce-selinux-17.03.0.ce-1.el7.centos.noarch conflicts docker-selinux\n--> Processing Conflict: docker-ce-selinux-17.03.0.ce-1.el7.centos.noarch conflicts docker-selinux\n--> Finished Dependency Resolution\n You could try using --skip-broken to work around the problem\n You could try running: rpm -Va --nofiles --nodigest\n""]}
to retry, use: --limit @/home/contrib/ansible/playbooks/deploy-cluster.retry",closed,False,2017-03-10 22:58:39,2018-02-21 05:50:01
contrib,proydakov,https://github.com/kubernetes/contrib/pull/2467,https://api.github.com/repos/kubernetes/contrib/issues/2467,Goreportcard,Added cool icons,closed,True,2017-03-12 13:23:27,2017-03-12 18:12:38
contrib,proydakov,https://github.com/kubernetes/contrib/pull/2468,https://api.github.com/repos/kubernetes/contrib/issues/2468,Goreportcard,Added cool icons,closed,True,2017-03-12 13:28:20,2017-03-12 18:12:38
contrib,proydakov,https://github.com/kubernetes/contrib/pull/2469,https://api.github.com/repos/kubernetes/contrib/issues/2469,Goreportcard,"Sign CLA, force push source and make new request.",closed,True,2017-03-12 18:17:59,2017-05-05 02:00:01
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2470,https://api.github.com/repos/kubernetes/contrib/issues/2470,[Ansible] Add apiserver-count as apiserver option,"Fix the `kube_apiserver_options` variable usage in the template
Removed duplicate variables from the master and node role defaults

/cc @xialonglee @axhoffmann For even better multi master clusters ;)",closed,True,2017-03-13 09:37:41,2017-03-14 15:19:28
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2471,https://api.github.com/repos/kubernetes/contrib/issues/2471,Cluster-Autoscaler: handle nil node group,"In a few place we assumed it's not-nil, leading to segfaults.",closed,True,2017-03-13 13:47:56,2017-03-13 16:14:15
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2472,https://api.github.com/repos/kubernetes/contrib/issues/2472,Cluster-autoscaler: fix NotTriggerScaleUp event,"This should fix a failing e2e test.

Also updated some scale_up unittests to check created events and fixed a typo in variable name.",closed,True,2017-03-13 18:49:11,2017-03-14 18:18:45
contrib,kayrus,https://github.com/kubernetes/contrib/pull/2473,https://api.github.com/repos/kubernetes/contrib/issues/2473,Manage swap on the worker node,Covers https://github.com/kubernetes/kubernetes/pull/31996,closed,True,2017-03-13 20:53:28,2017-03-14 11:57:27
contrib,piosz,https://github.com/kubernetes/contrib/pull/2474,https://api.github.com/repos/kubernetes/contrib/issues/2474,Migrated Rescheduler to new taints,"fix #2382
ref https://github.com/kubernetes/kubernetes/issues/32531

cc @ethernetdan  @marun @k82cn @aveshagarwal",closed,True,2017-03-14 22:11:08,2017-03-15 19:06:25
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2475,https://api.github.com/repos/kubernetes/contrib/issues/2475,Cluster-Autoscaler: consider node with unknown readiness unready,"Node with non-responsive kubelet seems to be marked as NodeReady: Unknown, which is currently considered as ready by CA.",closed,True,2017-03-15 10:20:51,2017-03-15 10:32:26
contrib,shyamjvs,https://github.com/kubernetes/contrib/pull/2476,https://api.github.com/repos/kubernetes/contrib/issues/2476,Add --api-override flag to allow overriding exporter's stackdriver endpoint,"Ref: https://github.com/kubernetes/contrib/pull/2414#issuecomment-285343716

cc @piosz @mwielgus ",closed,True,2017-03-15 10:48:56,2017-03-15 12:26:09
contrib,wangjianghuan,https://github.com/kubernetes/contrib/issues/2477,https://api.github.com/repos/kubernetes/contrib/issues/2477,"In the service_loadbalancer contain(gcr.io/google_containers/servicelb:0.4) , haproxy always restart","![image](https://cloud.githubusercontent.com/assets/16407843/23945420/65efd086-09b1-11e7-81ce-968192e34592.png)

Because the process is always restart, so can't see the haproxy statistical data

",closed,False,2017-03-15 11:01:22,2018-03-10 03:27:33
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2478,https://api.github.com/repos/kubernetes/contrib/issues/2478,Cluster-autoscaler: bump version to 0.5.0-beta2,cc: @MaciekPytel @jszczepkowski @fgrzadkowski ,closed,True,2017-03-15 11:23:09,2017-03-15 11:36:30
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2479,https://api.github.com/repos/kubernetes/contrib/issues/2479,Cluster-Autoscaler: update status configmap on errors,"Previously it would only update after successfully completing the main
loop, meaning the status wouldn't get updated unless cluster was
healthy.",closed,True,2017-03-15 12:17:17,2017-03-15 15:39:40
contrib,tuannvm,https://github.com/kubernetes/contrib/issues/2480,https://api.github.com/repos/kubernetes/contrib/issues/2480,[ansible] Cluster failed to start after installation,"Hi, I follow the ansible scripts to create k8s cluster on 3 coreOS servers (1 master, 2 nodes)
After the installation was finished, I went to check the `pods` status and got these result:
![link](https://i.imgur.com/RmnHokw.jpg)

Further check on specific pod:
![link](https://i.imgur.com/l00bCKB.jpg)

What do i miss though? Thanks for any help!",closed,False,2017-03-16 11:32:17,2018-04-16 19:44:51
contrib,tuannvm,https://github.com/kubernetes/contrib/pull/2481,https://api.github.com/repos/kubernetes/contrib/issues/2481,pre-install python to work with ubuntu 16.04 or newer,,closed,True,2017-03-16 11:40:41,2017-03-27 12:32:57
contrib,FaKod,https://github.com/kubernetes/contrib/issues/2482,https://api.github.com/repos/kubernetes/contrib/issues/2482,[nginx] tls is not recognized,"Hi,

following the example here https://github.com/kubernetes/ingress/tree/master/examples/tls-termination/nginx

Everything looks fine except curl shows:

```
$curl https://x.x.ch
curl: (60) SSL certificate problem: Invalid certificate chain
More details here: https://curl.haxx.se/docs/sslcerts.html
```
nginx.conf shows:
```
$ kubectl -n kube-system exec -it nginx-ingress-controller-557735307-mg5md -- cat /etc/nginx/nginx.conf | grep ssl_certificate
        ssl_certificate /etc/nginx-ssl/system-snake-oil-certificate.pem;
        ssl_certificate_key /etc/nginx-ssl/system-snake-oil-certificate.pem;
```

My Ingress is:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: ""nginx""
  name: my_ingress
spec:
  tls:
    - secretName: tls-secret
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: dashboard
          servicePort: 8080
```

my_ingress is created as shown in the example.
It seems that a wrong cert is used.
(ingress nginx container version 0.8.3)

Any hint?

",closed,False,2017-03-16 12:49:16,2017-03-16 12:56:14
contrib,metal3d,https://github.com/kubernetes/contrib/pull/2483,https://api.github.com/repos/kubernetes/contrib/issues/2483,[Ansible] Forgotten copy of ca.key,"When kube_cert_keep_ca is true, we must copy the certificate private key. That was forgotten in earlier PR acceptance.",closed,True,2017-03-17 00:50:47,2017-03-17 10:53:40
contrib,UlaganathanNamachivayam,https://github.com/kubernetes/contrib/issues/2484,https://api.github.com/repos/kubernetes/contrib/issues/2484,Documentation requirement for kubernetes cluster upgrade.,"Hi Team,
Unable to upgrade properly from a current version to 1.5.X and the ansible scripts fails due to various error. Is there any proper documentation link we can follow to perform cluster up gradation using ansible.

Thanks.",closed,False,2017-03-20 04:57:46,2018-02-22 11:19:04
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2485,https://api.github.com/repos/kubernetes/contrib/issues/2485,Cluster-autoscaler: bump version to 0.5.0,cc: @MaciekPytel @fgrzadkowski @jszczepkowski,closed,True,2017-03-21 14:12:29,2017-03-21 14:25:12
contrib,rocketraman,https://github.com/kubernetes/contrib/pull/2486,https://api.github.com/repos/kubernetes/contrib/issues/2486,Missing `/` in default `ZK_LOG_DIR`,,closed,True,2017-03-21 23:44:03,2018-02-20 14:34:59
contrib,saiyen2002,https://github.com/kubernetes/contrib/issues/2487,https://api.github.com/repos/kubernetes/contrib/issues/2487,SSL redirect on nginx-ingress-controller not working,"looking at doc here

https://github.com/kubernetes/ingress/tree/master/controllers/nginx#https

it says that by Default the controller redirects to HTTPS if TLS is enabled for that ingress

I have TLS defined in my ingress-controller, but HTTPS is not working, only HTTP. Trying HTTPS sends to default backend.

my ingress looks as follows
```
Name:                   cafe-ingress
Namespace:              default
Address:                192.168.20.232
Default backend:        default-http-backend:80 (<none>)
TLS:
  cafe-secret terminates cafe.example.com
Rules:
  Host                  Path    Backends
  ----                  ----    --------
  cafe.example.com
                        /tea            tea-svc:80 (<none>)
                        /coffee         coffee-svc:80 (<none>)
Annotations:
Events:
  FirstSeen     LastSeen        Count   From                            SubObjectPath   Type            Reason  Message
  ---------     --------        -----   ----                            -------------   --------        ------  -------
  34m           34m             1       {nginx-ingress-controller }                     Normal          CREATE  default/cafe-ingress
  34m           34m             1       {nginx-ingress-controller }                     Normal          UPDATE  default/cafe-ingress
  34m           34m             1       {nginx-ingress-controller }                     Normal          CREATE  ip: 192.168.20.232
```
In the actual nginx-ingress container looking at the nginx.conf i can see that Hosts and paths are only created in port 80 host.

```
 server {
        server_name cafe.example.com;
        listen 80;
```

the server block for 443 does not have the example

```
    server {
        server_name _;
        listen 80;
        listen 443  ssl spdy http2;
		
```

I deployed the nginx-ingress-controller with the image gcr.io/google_containers/nginx-ingress-controller:0.8.3 My deployment is as follows. 

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: default
  labels:
    app: nginx-ingress-lb
    group: lb
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-ingress-lb
        name: nginx-ingress-lb
        group: lb
    spec:
      hostNetwork: true
      terminationGracePeriodSeconds: 60
      containers:
      - name: nginx-ingress-lb
        image: gcr.io/google_containers/nginx-ingress-controller:0.8.3
        imagePullPolicy: IfNotPresent
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 1
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        ports:
        - containerPort: 80
        - containerPort: 18080 
        - containerPort: 443
        args:
        - /nginx-ingress-controller
        - --default-backend-service=default/nginx-default-backend
        - --nginx-configmap=default/nginx-ingress-lb-cfg

```

I originally setup this up with the example from here https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/complete-example and everything worked as expected.
but changing the controller to gcr.io/google_containers/nginx-ingress-controller:0.8.3 is what caused the issue

I am using kubernetes version 1.5.3. The cluster is running locally, not on any cloud platform.
",closed,False,2017-03-22 15:57:40,2017-03-22 22:51:13
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2488,https://api.github.com/repos/kubernetes/contrib/issues/2488,Cluster-Autoscaler: reset unneededNodes list on cluster failure,"If the nodes is marked as unneeded and cluster goes to an unhealthy state shortly after the node will likely be deleted immediately on cluster recovery. This is because there is already an entry for it in unnededNodes datastructure and the cluster downtime is counted towards node being unneeded time.

It's not 100% obvious to me what should happen in this case, but I think it's better to play it safe and just wait the full 10 minutes after cluster recovery before we start to delete nodes. After a quick glance at the code I haven't spotted any other stuff that needs to be cleaned up in case of cluster failure, but maybe you have some other ideas @mwielgus?",closed,True,2017-03-22 16:37:52,2017-03-23 13:40:25
contrib,ryanwalls,https://github.com/kubernetes/contrib/issues/2489,https://api.github.com/repos/kubernetes/contrib/issues/2489,Cluster Autoscaler: Documentation request - release notes for 0.5.0,"There is a new version of the cluster autoscaler as of a day or two ago.  But I can't find any release notes or indications of what has changed.  

In my ideal world it would be great if CA could live in it's own repo so that tags, issues, release notes, etc would be more easy to browse.  But if that's not possible, a release log of closed issues, new features, etc in the main CA readme would be a good compromise.  

Thanks!",closed,False,2017-03-23 03:15:56,2017-03-29 21:33:58
contrib,redbaron,https://github.com/kubernetes/contrib/pull/2490,https://api.github.com/repos/kubernetes/contrib/issues/2490,Return error if kube client can't be initialized,,closed,True,2017-03-23 13:02:30,2017-03-23 13:42:57
contrib,ryanwalls,https://github.com/kubernetes/contrib/issues/2491,https://api.github.com/repos/kubernetes/contrib/issues/2491,Cluster Autoscaler: Does not set node to unschedulable when trying to scale down,"Version: CA 0.5.0

Still digging through the code, but here's what I'm seeing in the logs.

```
I0323 14:13:03.952083       1 scale_down.go:292] Scale-down: removing node ip-172-31-45-132.us-west-2.compute.internal, utilization: 0.1, pods to reschedule: default/prometheus-operator-188658439-l50sl,default/parametric-aggregator-2173763796-zggm3,default/part-processor-2731341995-b6qp4
I0323 14:13:03.952628       1 event.go:217] Event(v1.ObjectReference{Kind:""ConfigMap"", Namespace:""kube-system"", Name:""cluster-autoscaler-status"", UID:""81f28c35-0fc1-11e7-a76e-02fda4b511cb"", APIVersion:""v1"", ResourceVersion:""2556450"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' Scale-down: removing node ip-172-31-45-132.us-west-2.compute.internal, utilization: 0.1, pods to reschedule: default/prometheus-operator-188658439-l50sl,default/parametric-aggregator-2173763796-zggm3,default/part-processor-2731341995-b6qp4
I0323 14:13:04.110792       1 delete.go:53] Successfully added toBeDeletedTaint on node ip-172-31-45-132.us-west-2.compute.internal
I0323 14:13:04.110997       1 event.go:217] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""prometheus-operator-188658439-l50sl"", UID:""affad382-0fcf-11e7-a76e-02fda4b511cb"", APIVersion:""v1"", ResourceVersion:""2553091"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' deleting pod for node scale down
I0323 14:13:04.111046       1 event.go:217] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""parametric-aggregator-2173763796-zggm3"", UID:""185ab916-0fd1-11e7-a76e-02fda4b511cb"", APIVersion:""v1"", ResourceVersion:""2554692"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' deleting pod for node scale down
I0323 14:13:04.111075       1 event.go:217] Event(v1.ObjectReference{Kind:""Pod"", Namespace:""default"", Name:""part-processor-2731341995-b6qp4"", UID:""185b2c33-0fd1-11e7-a76e-02fda4b511cb"", APIVersion:""v1"", ResourceVersion:""2554683"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' deleting pod for node scale down
I0323 14:13:04.111414       1 event.go:217] Event(v1.ObjectReference{Kind:""Node"", Namespace:"""", Name:""ip-172-31-45-132.us-west-2.compute.internal"", UID:""5769d390-0f4f-11e7-a5d1-02fda4ca2c4b"", APIVersion:""v1"", ResourceVersion:""2556463"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' marked the node as toBeDeleted/unschedulable
E0323 14:13:04.361220       1 scale_down.go:435] Not deleted yet &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:prometheus-operator-188658439-l50sl,GenerateName:prometheus-operator-188658439-,Namespace:default,SelfLink:/api/v1/namespaces/default/pods/prometheus-operator-188658439-l50sl,UID:affad382-0fcf-11e7-a76e-02fda4b511cb,ResourceVersion:2556482,Generation:0,CreationTimestamp:2017-03-23 13:50:34 +0000 UTC,DeletionTimestamp:2017-03-23 14:15:03 +0000 UTC,DeletionGracePeriodSeconds:*120,Labels:map[string]string{operator: prometheus,pod-template-hash: 188658439,},Annotations:map[string]string{kubernetes.io/created-by: {""kind"":""SerializedReference"",""apiVersion"":""v1"",""reference"":{""kind"":""ReplicaSet"",""namespace"":""default"",""name"":""prometheus-operator-188658439"",""uid"":""6bae9c80-0f70-11e7-8473-02fda4cfdadf"",""apiVersion"":""extensions"",""resourceVersion"":""2550663""}}
```

Basically... what I'm seeing is that CA identifies the node is eligible for scale down.  It tries to mark the node as unschedulable.  It deletes the pods.  Then it checks if the pods are deleted.  

What is happening is the pods are being rescheduled immediately on the same node.  And if I look in my k8s dashboard, it shows `unschedulable = false` on the node details while CA is waiting for the pods to be deleted in its 5s loop.

![image](https://cloud.githubusercontent.com/assets/842653/24251983/a901282a-0fa1-11e7-9530-84ece41ed5c7.png)

So for some reason, CA is not successfully setting `unschedulable` to `true`.  

  ",closed,False,2017-03-23 14:22:15,2017-04-19 15:26:07
contrib,andrewsykim,https://github.com/kubernetes/contrib/issues/2492,https://api.github.com/repos/kubernetes/contrib/issues/2492,Cluster Autoscaler 0.5.0 beta is not compatible with Kubernetes 1.4,"Seems like cluster autoscaler 0.5.0 relies on the `PodDisruptionBudget` which was introduced in 1.5.   Makes it completely unusable in 1.4 which is unfortunate because some of the new features (drain logic + handling unready nodes) is desirable. Is this a known issue? Should it be backwards compatible? 

```
E0323 17:54:56.270355       1 reflector.go:201] k8s.io/contrib/cluster-autoscaler/utils/kubernetes/listers.go:241: Failed to list *v1beta1.PodDisruptionBudget: the server could not find the requested resource
```",closed,False,2017-03-23 17:56:52,2017-03-24 15:33:41
contrib,snoby,https://github.com/kubernetes/contrib/issues/2493,https://api.github.com/repos/kubernetes/contrib/issues/2493,Basic auth for backend ssl service.,"I have an ingress setup with the nginx ingress controller version 0.90beta2 and I can't seem to make basic auth work on this interface.  It seems to go right past  as if no authorization attempt is being made.  i have a docker registry v2 running in the cluster via a real cert ( not self signed).  Works just fine.  It is fronted by an ingress.   

Here is my ingress:
```
   90	---
    91	apiVersion: extensions/v1beta1
    92	kind: Ingress
    93	metadata:
    94	  name: kube-registry
    95	  namespace: kube-system
    96	  annotations:
    97	    kubernetes.io/ingress.class: ""nginx""
    98	    ingress.kubernetes.io/ssl-passthrough: ""true""
    99	    ingress.kubernetes.io/secure-backends: ""true""
   100	    ingress.kubernetes.io/auth-type: basic
   101	    #the auth-secret has to come from a file called ""auth""
   102	    ingress.kubernetes.io/auth-secret: registry-auth
   103	    ingress.kubernetes.io/auth-realm: ""Authentication Required - for the registry""
   104	spec:
   105	  rules:
   106	  - host: registry.ops.example.com
   107	    http:
   108	      paths:
   109	      - path: /
   110	        backend:
   111	          serviceName: kube-registry
   112	          servicePort: 443
   113	  tls:
   114	  - hosts:
   115	    - registry.ops.example.com
```
Logs from the nginx controller look ""normal"" nothing to really report.  I've logged on to the container and this is the generated config.  At least a snippet of it
```
 server {
        server_name registry.ops.tropo.com;
        listen [::]:80 proxy_protocol;
        listen 442  ssl http2;
        # PEM sha: 146d6a1a176900b4af00faf1b177c82d6c7b39ae
        ssl_certificate                         /ingress-controller/ssl/system-snake-oil-certificate.pem;
        ssl_certificate_key                     /ingress-controller/ssl/system-snake-oil-certificate.pem;

        more_set_headers                        ""Strict-Transport-Security: max-age=15724800; includeSubDomains; preload"";
        location / {
            set $proxy_upstream_name ""kube-system-kube-registry-443"";

            port_in_redirect off;

            # enforce ssl on server side
            if ($scheme = http) {
                return 301 https://$host$request_uri;
            }
            auth_basic ""Authentication Required - for the registry"";
            auth_basic_user_file /etc/ingress-controller/auth/kube-system-kube-registry.passwd;
            proxy_set_header Authorization """";

            client_max_body_size                    ""1m"";

            proxy_set_header Host                   $host;

            # Pass Real IP
            proxy_set_header X-Real-IP              $remote_addr;

            # Allow websocket connections
            proxy_set_header                        Upgrade           $http_upgrade;
            proxy_set_header                        Connection        $connection_upgrade;

            proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Host       $host;
            proxy_set_header X-Forwarded-Port       $pass_port;
            proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

            # mitigate HTTPoxy Vulnerability
            # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
            proxy_set_header Proxy                  """";

            # Custom headers

            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;

            proxy_redirect                          off;
            proxy_buffering                         off;
            proxy_buffer_size                       ""4k"";

            proxy_http_version                      1.1;
            proxy_pass https://kube-system-kube-registry-443;
        }
    }
```
",closed,False,2017-03-23 21:07:08,2018-02-21 01:46:00
contrib,andrewsykim,https://github.com/kubernetes/contrib/pull/2494,https://api.github.com/repos/kubernetes/contrib/issues/2494,[Cluster Autoscaler] Use pod's default grace period instead of setting it to maxGracefulTerminationSec,"It seems like we set maxGracefulTermination in the eviction policy for each pod but we wait [maxGracefulTermination for all pods to terminate](https://github.com/kubernetes/contrib/blob/master/cluster-autoscaler/core/scale_down.go#L436). If you have pods that receive SIGTERM and then rely on kubelet to send send SIGKILL, then you're likely to reach the node eviction timeout. We should be defaulting to whatever terminationGracePeriod is set by users for each pod. `maxGracefulTermination` should be the max time it takes for all pods to be evicted (which is the current behaviour). 

Fixes https://github.com/kubernetes/contrib/issues/2491 (at least for me)

cc @mwielgus @davidopp @fgrzadkowski @MaciekPytel @ryanwalls",closed,True,2017-03-23 21:56:01,2017-05-05 23:14:53
contrib,knowhy,https://github.com/kubernetes/contrib/pull/2495,https://api.github.com/repos/kubernetes/contrib/issues/2495,unified Ansible syntax,"- replace inline with block format
- fix incorrect `=` block indicator",closed,True,2017-03-24 18:59:29,2018-03-12 11:22:33
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2496,https://api.github.com/repos/kubernetes/contrib/issues/2496,Cluster-autoscaler: add information about which version is supported in which k8s,cc: @andrewsykim @MaciekPytel @fgrzadkowski ,closed,True,2017-03-24 22:42:28,2017-03-27 08:11:52
contrib,rchicoli,https://github.com/kubernetes/contrib/pull/2497,https://api.github.com/repos/kubernetes/contrib/issues/2497,add opennebula provider to vagrantfile in ansible folder,I guess the title and the changes are clear enough,closed,True,2017-03-25 07:17:08,2017-03-28 06:34:04
contrib,naarani,https://github.com/kubernetes/contrib/issues/2498,https://api.github.com/repos/kubernetes/contrib/issues/2498,service-loadbalancer image weakness,"testing the HAPROXY ssl connection I've found this weaknesses:

1) PROBLEM: DIFFIE-HELLMAN PARAMETER WEAK
""The Diffie-Hellman parameter's size is only 1024 bits . A longer one must be generated to prevent Logjam vulnerability.""

as from [haproxy guide](https://www.haproxy.com/doc/aloha/7.0/haproxy/tls.html)
haproxy config should add:
tune.ssl.default-dh-param 2048

PS: the new image should warn about no java7 client support, to close this weakness]

2) PROBLEM: CVE-2016-2107 
""The server is vulnerable to OpenSSL padding-oracle flaw (CVE-2016-2107), consider upgrading OpenSSL.Non-compliant with PCI DSS requirements""

docker image should change OPENSSL version to the newest 
",closed,False,2017-03-25 11:48:13,2018-02-20 17:38:01
contrib,diwu1989,https://github.com/kubernetes/contrib/pull/2499,https://api.github.com/repos/kubernetes/contrib/issues/2499,fix typo consumig,,closed,True,2017-03-26 20:12:05,2017-04-11 02:12:43
contrib,mashayev,https://github.com/kubernetes/contrib/issues/2500,https://api.github.com/repos/kubernetes/contrib/issues/2500,cluster-autoscaler: v0.5.0 don't send logs,"Hi,

I've upgraded cluster-autoscaler from v0.4.0 to 0.5.0 and the created container not sending logs any more, the logs are empty.

From the documentation I didn't see any change from v0.4 to v0.5 regarding the logs.

```
      containers:
        - image: xplenty/kubernetes-autoscaler:v0.5.0
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --kubernetes=${MASTER_URL}
            - --v=4
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=true
            - --nodes=${ASG_MIN}:${ASG_MAX}:${ASG_NAME}
```",closed,False,2017-03-27 10:38:10,2017-06-22 01:08:28
contrib,Bekt,https://github.com/kubernetes/contrib/issues/2501,https://api.github.com/repos/kubernetes/contrib/issues/2501,[fluentd-gcp-image] add kubernetes_metadata_filter,"Consider adding [kubernetes_metadata_filter](https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter) into the fluentd-gcp-image Gemfile.

Some description of the use cases is described [here](https://github.com/fabric8io/docker-fluentd-kubernetes/blob/master/fluent.conf#L69).",closed,False,2017-03-27 15:58:16,2017-03-31 14:40:34
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2502,https://api.github.com/repos/kubernetes/contrib/issues/2502,Cluster-autoscaler: Fix isNodeStarting,"Fix for: https://github.com/kubernetes/kubernetes/issues/43709

cc: @MaciekPytel @fgrzadkowski ",closed,True,2017-03-27 21:43:39,2017-03-28 08:14:46
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/2503,https://api.github.com/repos/kubernetes/contrib/issues/2503,ClusterAutoscaler: ScaleUp switches to NoActivity in ConfigMap prematurelly,"The cluster is still scaling up but the overall status is NoActivty.

",closed,False,2017-03-27 21:45:14,2017-03-28 10:14:20
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2504,https://api.github.com/repos/kubernetes/contrib/issues/2504,Cluster-Autoscaler: fix scaleup status reporting,This fixes https://github.com/kubernetes/contrib/issues/2503,closed,True,2017-03-28 10:01:59,2017-03-28 10:14:20
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2505,https://api.github.com/repos/kubernetes/contrib/issues/2505,Bump cluster autoscaler to 0.5.1,cc: @MaciekPytel @fgrzadkowski ,closed,True,2017-03-28 10:52:00,2017-03-28 11:22:24
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2506,https://api.github.com/repos/kubernetes/contrib/issues/2506,Cluster-Autoscaler: make status less confusing,"Previously min and max in status were refering to
non-obvious internal variables, which was pretty confusing.",closed,True,2017-03-28 11:33:13,2017-03-29 07:49:30
contrib,yawboateng,https://github.com/kubernetes/contrib/issues/2507,https://api.github.com/repos/kubernetes/contrib/issues/2507,cluster-autoscaler: Unable to look up PVC info ,"cluster-autoscaler works fine but im seeing multiple lines of this error in the logs:
`
E0328 18:14:57.140465       1 predicates.go:234] Unable to look up PVC info for logging/datadir-elasticsearch-data-ss-1, assuming PVC matches predicate when counting limits: persistentvolumeclaim ""datadir-elasticsearch-data-ss-1"" not found `

I've confirmed the pvc datadir-elasticsearch-data-ss-1does exist in the logging namespace",closed,False,2017-03-28 18:19:35,2017-04-26 12:39:25
contrib,cdemers,https://github.com/kubernetes/contrib/pull/2508,https://api.github.com/repos/kubernetes/contrib/issues/2508,Configurable keepalived virtual router id,"Hi all, this PR is an up to date version of this previous PR  #2384, it's the same but squashed and merged with the latest master revision.

It adds support for configurable keepalived virtual router id. This functionality is necessary in order to be able to run multiple keepalived ""clusters"" in the same subnet without having them stepping on each other's toes (having one node from one group confusing the other group into thinking it has a master).

The configuration of the virtual router id is done using the --vrid command line flag. I considered using the project's configmap but the structure of the existing configmap is not easily expandable, so I had to use a flag (I would have added support for environment variables, but it didn't seems to be in line with the way this application is thought out, so I refrained).

The flag can be specified in a k8s object with the args key, for example:
- args: [""--services-configmap=admin/vip-configmap"", ""--vrid=45""]

If not specified, it will default to router id 50 as it was before.

That's pretty much all there is to it, thanks in advance for your time!",closed,True,2017-03-29 05:02:30,2017-04-03 13:05:17
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2509,https://api.github.com/repos/kubernetes/contrib/issues/2509,Cluster-autoscaler: update aws doc with --stderrthreshold=info flag,"cc: @andrewsykim @MaciekPytel

ref:  #2500",closed,True,2017-03-29 21:53:34,2017-03-30 06:31:15
contrib,alxndr13,https://github.com/kubernetes/contrib/pull/2510,https://api.github.com/repos/kubernetes/contrib/issues/2510,[Ansible] removed kubeui,"remove kubeui as proposed in #2408 

",closed,True,2017-03-30 14:20:36,2018-03-09 20:20:33
contrib,wtritch,https://github.com/kubernetes/contrib/pull/2511,https://api.github.com/repos/kubernetes/contrib/issues/2511,Leader Election: Add callback on new leader detection.,"The leader election web service doesn't currently update the advertised leader via:
http://localhost:4040/
if the leader has changed.

This PR adds a callback that updates the advertised leader when a change is detected.",closed,True,2017-03-30 19:58:17,2017-03-30 22:36:00
contrib,arnaudsj,https://github.com/kubernetes/contrib/issues/2512,https://api.github.com/repos/kubernetes/contrib/issues/2512,zookeeper/statefulset example not working,"Hi,

I am trying to run the zookeeper example (as it is also now in the official kubernetes blog: http://blog.kubernetes.io/2016/12/statefulset-run-scale-stateful-applications-in-kubernetes.html)

In short, running: 

` kubectl create -f http://k8s.io/docs/tutorials/stateful-application/zookeeper.yaml`

but I am getting this when the zk-9 pod runs (seems to be a permission issue with the claimed persistent volumes)

```2017-03-30T22:04:11.307268070Z Starting environment validation
2017-03-30T22:04:11.307341748Z ZK_ENSEMBLE=zk-0;zk-1;zk-2
2017-03-30T22:04:11.309637378Z MY_ID=1
2017-03-30T22:04:11.309650569Z ZK_LOG_LEVEL=INFO
2017-03-30T22:04:11.309654116Z ZK_DATA_DIR=/var/lib/zookeeper/data
2017-03-30T22:04:11.309656976Z ZK_DATA_LOG_DIR=/var/lib/zookeeper/log
2017-03-30T22:04:11.309659703Z ZK_LOG_DIR=/var/log/zookeeper
2017-03-30T22:04:11.309662366Z ZK_CLIENT_PORT=2181
2017-03-30T22:04:11.309664850Z ZK_SERVER_PORT=2888
2017-03-30T22:04:11.309667430Z ZK_ELECTION_PORT=3888
2017-03-30T22:04:11.309669961Z ZK_TICK_TIME=2000
2017-03-30T22:04:11.309672481Z ZK_INIT_LIMIT=10
2017-03-30T22:04:11.309675013Z ZK_SYNC_LIMIT=2000
2017-03-30T22:04:11.309677548Z ZK_MAX_CLIENT_CNXNS=60
2017-03-30T22:04:11.309680068Z ZK_MIN_SESSION_TIMEOUT= 4000
2017-03-30T22:04:11.309682652Z ZK_MAX_SESSION_TIMEOUT= 40000
2017-03-30T22:04:11.309685187Z ZK_HEAP_SIZE=1G
2017-03-30T22:04:11.309687730Z ZK_SNAP_RETAIN_COUNT=3
2017-03-30T22:04:11.309690248Z ZK_PURGE_INTERVAL=1
2017-03-30T22:04:11.309692830Z Enviorment validation successful
2017-03-30T22:04:11.312128797Z Creating ZooKeeper configuration in /opt/zookeeper/conf/zoo.cfg
2017-03-30T22:04:11.316764022Z mkdir: cannot create directory '/var/lib/zookeeper': Permission denied
2017-03-30T22:04:11.316892282Z ZooKeeper configuration file written to /opt/zookeeper/conf/zoo.cfg
2017-03-30T22:04:11.316905973Z clientPort=2181
2017-03-30T22:04:11.316910066Z dataDir=/var/lib/zookeeper/data
2017-03-30T22:04:11.316913062Z dataLogDir=/var/lib/zookeeper/log
2017-03-30T22:04:11.316916557Z tickTime=2000
2017-03-30T22:04:11.316919243Z initLimit=10
2017-03-30T22:04:11.316921864Z syncLimit=2000
2017-03-30T22:04:11.316924493Z maxClientCnxns=60```

Any ideas?",closed,False,2017-03-30 22:13:31,2018-03-11 14:01:33
contrib,bowei,https://github.com/kubernetes/contrib/pull/2513,https://api.github.com/repos/kubernetes/contrib/issues/2513,Add bowei to approvers for keepalived,,closed,True,2017-03-31 00:42:08,2017-03-31 00:56:13
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2514,https://api.github.com/repos/kubernetes/contrib/issues/2514,[Ansible] Add missing default var for gen_certs keep ca var,This adds the missing default var to the when condition for `kube_cert_keep_ca` var.,closed,True,2017-03-31 15:48:20,2017-07-13 19:04:06
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2515,https://api.github.com/repos/kubernetes/contrib/issues/2515,[Ansible] Added a var to disable the swap disable tasks,This adds a var named `disable_swap` to allow disabling the swap disabling tasks.,closed,True,2017-03-31 15:54:03,2017-04-03 12:30:33
contrib,fredrik955,https://github.com/kubernetes/contrib/issues/2516,https://api.github.com/repos/kubernetes/contrib/issues/2516,Autoscaling: accept min-nodes value of 0,"**FEATURE REQUEST**

It would be convenient to accept `min-nodes` to be equal to `0` for autoscaling on node-pools.
If you want to run a resource-intensive batch process now and then this will save a lot of costs.
",closed,False,2017-03-31 19:06:54,2017-04-19 15:31:13
contrib,sijhu,https://github.com/kubernetes/contrib/issues/2517,https://api.github.com/repos/kubernetes/contrib/issues/2517,"[ansible] in cluster, master could not find node","Hi , I follow https://github.com/kubernetes/contrib/tree/master/ansible/README.MD to deploy k8s cluster (1 master, 2 nodes) with ansible on openstack.  I set the public ip of master,nodes in ./inventory/inventory
After the installation was finished, I run command: ""kubectl get node""
and get result as : ""No resources found.""
I check the logs with command : ""journalctl -r -a"", it shows me :
x509: certificate is valid for private ip , not for public ip.
it seems like : the public ip of master is invalid to certificate.
Could you please help with : how to configure to certificate valid for public ip ？",closed,False,2017-04-01 09:58:55,2018-02-22 00:07:43
contrib,c-knowles,https://github.com/kubernetes/contrib/issues/2518,https://api.github.com/repos/kubernetes/contrib/issues/2518,[rescheduler] Logs going to /tmp/rescheduler.INFO,"We've recently introduced the rescheduler to [kube-aws](https://github.com/kubernetes-incubator/kube-aws/issues/441).

I've been trying to do the prep for the v1.6 updates, i.e. upgrading to rescheduler v0.3.0. Our setup is a similar to the [salt manifest](https://github.com/kubernetes/kubernetes/blob/master/cluster/saltbase/salt/rescheduler/rescheduler.manifest).

However, when using the same log volume mount as the salt config the logs are not ending up in `/var/log/rescheduler.log` as expected and seem to be going to tmp files like `/tmp/rescheduler.INFO`. I've also tried some of the setup from [glog](https://github.com/golang/glog/blob/master/glog.go#L42), for example setting `logtostderr` to try to get the output to stderr but this seems to fail the flag parsing:

```
unknown flag: --logtostderr
Usage of rescheduler: rescheduler --running-in-cluster=true:
   --housekeeping-interval duration   How often rescheduler takes actions. (default 10s)
   [other flags...]
```",closed,False,2017-04-03 15:40:47,2017-07-14 00:34:39
contrib,Bekt,https://github.com/kubernetes/contrib/issues/2519,https://api.github.com/repos/kubernetes/contrib/issues/2519,[fluentd-gcp-image] constant memory spikes,"Using the [fluentd-gcp-ds](https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-gcp/fluentd-gcp-ds.yaml) (fluentd-gcp:2.0.2), we have been seeing (1) consistently high memory usages when the google-fluentd output is used (500-600mb), and (2) memory spikes when no output plugin is specified.

For (2), the pattern is the same on all nodes with this configuration:

```
containers.input.conf: |-
  <source>
    type tail
    format json
    time_key time
    path /var/log/containers/*.log
    pos_file /var/log/fluentd-containers.log.pos
    time_format %Y-%m-%dT%H:%M:%S.%N%Z
    tag reform.*
    read_from_head true
  </source>

  <filter reform.**>
    type parser
    format /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<log>.*)/
    reserve_data true
    suppress_parse_error_log true
    key_name log
  </filter>

output.conf: |-
  <match **>
    type null
  </match>
```

![](https://cloud.githubusercontent.com/assets/1221480/24634300/c266ef08-1892-11e7-8522-628be82c148b.png)

There are ~30 pods on this node, none of which are heavy load. I can't tell if this is a fluentd-specific issue or not.

/cc @piosz @crassirostris @igorpeshansky",closed,False,2017-04-03 22:27:32,2017-04-05 08:52:25
contrib,discostur,https://github.com/kubernetes/contrib/issues/2520,https://api.github.com/repos/kubernetes/contrib/issues/2520,[keepalived-vip]: Error getting POD information (k8s v1.6.1),"Hi,

just tried out keepalived-vip with the new kubernetes v1.6.1 and i'm getting the following error:

```
F0404 09:52:28.045035       1 controller.go:298] Error getting POD information: timed out waiting to observe own status as Running
goroutine 1 [running]:
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.stacks(0x1d9fd00, 0xc400000000, 0x83, 0xf2)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:766 +0xa5
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).output(0x1d7f800, 0xc400000003, 0xc4203b9b00, 0x1d1a03f, 0xd, 0x12a, 0x0)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:717 +0x337
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).printf(0x1d7f800, 0xc400000003, 0x150ae12, 0x21, 0xc42065bc88, 0x1, 0x1)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:655 +0x14c
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.Fatalf(0x150ae12, 0x21, 0xc42065bc88, 0x1, 0x1)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.newIPVSController(0xc4205a08a0, 0xc42001000e, 0x7, 0x12ffd00, 0x7fff6dde1720, 0x15, 0x3)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/controller.go:298 +0x10ee
main.main()
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/main.go:115 +0x29a
```

I think it may be related to the new RBAC feature introduced in v.1.6.1?",closed,False,2017-04-04 10:01:18,2018-02-07 10:20:19
contrib,bbzg,https://github.com/kubernetes/contrib/issues/2521,https://api.github.com/repos/kubernetes/contrib/issues/2521,"GLBC Ingress TLS - new secret did not get applied because of redundant ""k8s-ssl-1*"" cert","Kubernetes 1.5.4 on GKE

Similar/same as https://github.com/kubernetes/contrib/issues/1396

After replacing the secret holding the TLS cert, nothing happened. Found #1396, that pointed me in the right direction - deleting the extra certificate that was not connected to any ingress. After doing that (about 5-10 minutes) the certificate was swapped out on the ingress and used.",closed,False,2017-04-04 17:25:16,2017-04-15 08:04:01
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2522,https://api.github.com/repos/kubernetes/contrib/issues/2522,Cluster-autoscaler: improve readme,cc: @fgrzadkowski @MaciekPytel ,closed,True,2017-04-04 19:54:23,2017-04-05 08:44:08
contrib,eugene-chow,https://github.com/kubernetes/contrib/pull/2523,https://api.github.com/repos/kubernetes/contrib/issues/2523,Reorganised documentation and added RBAC section,,closed,True,2017-04-05 08:02:20,2017-04-11 07:17:01
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2524,https://api.github.com/repos/kubernetes/contrib/issues/2524,Cluster-autoscaler: Frequently Asked Questions,cc: @MaciekPytel @fgrzadkowski ,closed,True,2017-04-05 09:51:17,2017-04-06 13:57:15
contrib,jorge07,https://github.com/kubernetes/contrib/issues/2525,https://api.github.com/repos/kubernetes/contrib/issues/2525,Autocaler is considering Master/s as schedulable for pods when are not.,"```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
        - image: gcr.io/google_containers/cluster-autoscaler:v0.5.1
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --nodes=1:10:nodes.kubernetes.staging.XXX.com
          env:
            - name: AWS_REGION
              value: us-west-1
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
          imagePullPolicy: ""Always""
      volumes:
        - name: ssl-certs
          hostPath:
            path: ""/etc/ssl/certs/ca-certificates.crt""

```

Logs
```
utils.go:98] Pod es-data-2026409273-4m7ts marked as unschedulable can be scheduled on ip-172-20-42-8.us-west-1.compute.internal. Ignoring in scale up.
W0405 15:34:32.502077       1 utils.go:98] Pod es-data-2026409273-nv68g marked as unschedulable can be scheduled on ip-172-20-42-8.us-west-1.compute.internal. Ignoring in scale up.
W0405 15:34:32.502101       1 utils.go:98] Pod es-data-2026409273-gdgjt marked as unschedulable can be scheduled on ip-172-20-XX-X.us-west-1.compute.internal. Ignoring in scale up.
W0405 15:34:32.502116       1 utils.go:98] Pod es-data-2026409273-wstdq marked as unschedulable can be scheduled on ip-172-20-XX-X.us-west-1.compute.internal. Ignoring in scale up.
W0405 15:34:32.502132       1 utils.go:98] Pod es-data-2026409273-rd6p3 marked as unschedulable can be scheduled on ip-172-20-XX-X.us-west-1.compute.internal. Ignoring in scale up.
W0405 15:34:32.502156       1 utils.go:98] Pod es-data-2026409273-m9p7c marked as unschedulable can be scheduled on ip-172-20-XX-X.us-west-1.compute.internal. Ignoring in scale up.
```",closed,False,2017-04-06 08:38:42,2017-10-09 16:55:35
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2526,https://api.github.com/repos/kubernetes/contrib/issues/2526,Cluster-autoscaler: readme reorg,cc: @MaciekPytel @fgrzadkowski ,closed,True,2017-04-06 09:37:01,2017-04-06 10:02:33
contrib,eugene-chow,https://github.com/kubernetes/contrib/issues/2527,https://api.github.com/repos/kubernetes/contrib/issues/2527,[keepalived-vip] Active/Active HA cluster,"@aledbf Is an active-active configuration supported? If not, are there plans to support it?

Let's say there're 3 public nodes with 3 VIPs. Each node will be a master for a VIP. With DNS RR, incoming traffic will be load-balanced among the nodes, instead of sending all traffic to the current master.
```
vrrp_instance vip0 {
  state MASTER
  interface ens3
  virtual_router_id 100
  priority 102
  nopreempt
  advert_int 1

  track_interface {
    ens3
  }

  virtual_ipaddress {
    172.16.32.110
  }
}

vrrp_instance vip1 {
  state BACKUP
  interface ens3
  virtual_router_id 101
  priority 101
  nopreempt
  advert_int 1

  track_interface {
    ens3
  }

  virtual_ipaddress {
    172.16.32.111
  }
}

vrrp_instance vip2 {
  state BACKUP
  interface ens3
  virtual_router_id 102
  priority 100
  nopreempt
  advert_int 1

  track_interface {
    ens3
  }

  virtual_ipaddress {
    172.16.32.112
  }
}
```",closed,False,2017-04-06 09:37:28,2018-03-01 10:02:51
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2528,https://api.github.com/repos/kubernetes/contrib/issues/2528,Cluster-autoscaler: rename FAQ,For some reason in the previous PR the name was not changed. ,closed,True,2017-04-06 10:06:57,2017-04-06 11:03:21
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2529,https://api.github.com/repos/kubernetes/contrib/issues/2529,Cluster-autoscaler: add HPA and performance questions to FAQ,cc: @MaciekPytel @fgrzadkowski ,closed,True,2017-04-06 11:06:21,2017-04-06 11:41:23
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2530,https://api.github.com/repos/kubernetes/contrib/issues/2530,Cluster-autoscaler: ubuntu slim version bump,cc: @MaciekPytel @fgrzadkowski ,closed,True,2017-04-06 11:42:49,2017-04-07 08:10:32
contrib,c-knowles,https://github.com/kubernetes/contrib/issues/2531,https://api.github.com/repos/kubernetes/contrib/issues/2531,[rescheduler] Trying to reschedule deleted pods,"I'm not sure if this is a bug or expected behaviour but it certainly spams the logs.

Steps to reproduce:
1. Create a state that induces the rescheduler, in my case I scaled a deployment with CPU requests much higher than my cluster could handle but any way should work
1. Delete a critical pod such as heapster
1. Wait for new pod, `<X>`, to appear (but not scheduled)
1. Wait for rescheduler to perform reschedule initialisation and log `Waiting for pod <X> to be scheduled`
1. Before the scheduler has a chance to schedule it, delete pod `<X>`

This is what will be received in the logs:

```
W0406 13:39:21.430383       1 rescheduler.go:182] Error while getting pod kube-system_heapster-v1.3.0-76786035-bsnx1: pods ""heapster-v1.3.0-76786035-bsnx1"" not found
W0406 13:39:22.301473       1 rescheduler.go:182] Error while getting pod kube-system_heapster-v1.3.0-76786035-kltpb: pods ""heapster-v1.3.0-76786035-kltpb"" not found
W0406 13:39:22.430417       1 rescheduler.go:182] Error while getting pod kube-system_heapster-v1.3.0-76786035-bsnx1: pods ""heapster-v1.3.0-76786035-bsnx1"" not found
W0406 13:39:23.301651       1 rescheduler.go:182] Error while getting pod kube-system_heapster-v1.3.0-76786035-kltpb: pods ""heapster-v1.3.0-76786035-kltpb"" not found
W0406 13:39:23.430573       1 rescheduler.go:182] Error while getting pod kube-system_heapster-v1.3.0-76786035-bsnx1: pods ""heapster-v1.3.0-76786035-bsnx1"" not found
W0406 13:39:24.301559       1 rescheduler.go:182] Error while getting pod kube-system_heapster-v1.3.0-76786035-kltpb: pods ""heapster-v1.3.0-76786035-kltpb"" not found
```

In this case I did steps 3-5 twice so there's `bsnx1` and `kltpb`.

After the timeout the rescheduler will stop trying to reschedule these deleted pods and won't log about them any more:
```
W0406 13:43:28.433038       1 rescheduler.go:188] Timeout while waiting for pod kube-system_heapster-v1.3.0-76786035-bsnx1 to be scheduled after 10m0s.
```",closed,False,2017-04-06 13:50:21,2018-02-21 01:46:01
contrib,GFilipek,https://github.com/kubernetes/contrib/pull/2532,https://api.github.com/repos/kubernetes/contrib/issues/2532,Vertical Pod Autoscaler : updater first version,"Updater component for Vertical Pod Autoscaler described in  https://github.com/kubernetes/community/pull/338

Runs in loop. On one iteration performs:
 - Fetching Vertical Pod Autoscaler configuration 
 - Fetching Pods information and resource allocation recommendations for pods. 
 - Calculating if pod update is required and how many replicas can be evicted
 - Evicting pods if recommended resources significantly vary from actual resources allocation.

Mocked parts - to be implemented:
 - Recommendation API for fetching data from Vertical Pod Autoscaler Recommender.
 - Vertical Pod Autoscaler lister for fetching Vertical Pod Autoscaler config.

cc: @mwielgus @kgrygiel @KarolKraskiewicz",closed,True,2017-04-06 14:01:28,2017-04-27 09:56:21
contrib,fgrzadkowski,https://github.com/kubernetes/contrib/issues/2533,https://api.github.com/repos/kubernetes/contrib/issues/2533,Create a separate repo for cluster autoscaler,"We need a separate repo to a) conform to new guidelines of deprecating contrib and more importantly to b) properly do release management (branches, tags, release notes etc)

@MaciekPytel ",closed,False,2017-04-06 17:35:07,2017-04-19 15:31:48
contrib,eugene-chow,https://github.com/kubernetes/contrib/issues/2534,https://api.github.com/repos/kubernetes/contrib/issues/2534,[keepalived-vip] Doesn't detect hostNetwork pods & use of clusterIP VS podIP,"@aledbf I tried pointing keepalived to a Service which points to an Ingress on `hostNetwork`. It failed to pick up the Ingress' `podIP`, which is equivalent to the `hostIP`. I haven't figured it out from the source code but I suspect it's on `hostNetwork`.

Here's the Service defn for the Ingress. I tested that `NodePort` in the original config wasn't necessary.
```
apiVersion: v1
kind: Service
metadata:
  name: traefik
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
  - name: http
    port: 80
    protocol: TCP
  - name: https
    port: 443
    protocol: TCP
```

The keepalived ConfigMap for the Ingress.
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: vip-configmap
data:
  172.16.32.15: kube-system/traefik
```

Also, is there a reason for pointing keepalived to the podIP instead of the Service's `clusterIP`? I think `clusterIP` would be a better choice because its IP more stable than the `podIP` and it's also load-balanced by iptables.

Thanks!",closed,False,2017-04-07 10:41:46,2018-02-21 02:47:02
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2535,https://api.github.com/repos/kubernetes/contrib/issues/2535,Cluster-autoscaler: FAQ improvement,cc: @MaciekPytel @fgrzadkowski ,closed,True,2017-04-07 11:27:04,2017-04-07 12:47:59
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2536,https://api.github.com/repos/kubernetes/contrib/issues/2536,Cluster-autoscaler: other autoscalers in FAQ,cc: @MaciekPytel @fgrzadkowski ,closed,True,2017-04-07 12:59:23,2017-04-07 15:08:40
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2537,https://api.github.com/repos/kubernetes/contrib/issues/2537,Cluster-Autoscaler: Add FAQ entry about e2e,Also removed trailing whitespaces.,closed,True,2017-04-07 15:19:13,2017-04-10 11:29:26
contrib,tonglil,https://github.com/kubernetes/contrib/issues/2538,https://api.github.com/repos/kubernetes/contrib/issues/2538,startup-script no longer seems to run on host for 1.6,"It seems like this daemonset image no longer executes commands on the host w/ K8s 1.6.

https://github.com/kubernetes/contrib/tree/master/startup-script

I have a cluster w/ nodes running 1.6.0 and 1.5.2, and the same daemonset running.

On 1.6.0 (echo is written to /tmp/test inside the container):
```
bash-4.3# ls -la /tmp/
total 12
drwxrwxrwt    1 root     root          4096 Apr  7 21:05 .
drwxr-xr-x    1 root     root          4096 Apr  7 21:05 ..
-rw-r--r--    1 root     root            12 Apr  7 21:05 test
-rw-r--r--    1 root     root             0 Apr  7 21:05 startup-script-v1
```

On 1.5.2 (expected since the commands are executed on the host node):
```
bash-4.3# ls -la /tmp
total 8
drwxrwxrwt    2 root     root          4096 Oct 18 18:58 .
drwxr-xr-x    1 root     root          4096 Apr  7 21:05 ..
```

```yaml
---
kind: DaemonSet
apiVersion: extensions/v1beta1

metadata:
  name: test-ds

spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1

  minReadySeconds: 30

  template:
    metadata:
      labels:
        app: test-ds

    spec:
      hostPID: true
      containers:
        - name: test-ds
          image: gcr.io/google-containers/startup-script:v1
          imagePullPolicy: Always
          resources:
            requests:
              cpu: 10m
            limits:
              cpu: 50m
          securityContext:
            privileged: true
          env:
          # Increment this value to make new DS script apply
          - name: CHECKPOINT_PATH
            value: /tmp/startup-script-v1
          - name: STARTUP_SCRIPT
            value: |
              #!/bin/bash

              set -o nounset
              set -o errexit
              set -o pipefail

              echo ""Start""

              echo ""hello world"" > /tmp/test

              echo ""End""
```",closed,False,2017-04-07 21:13:23,2017-04-12 20:07:21
contrib,MaciekPytel,https://github.com/kubernetes/contrib/pull/2539,https://api.github.com/repos/kubernetes/contrib/issues/2539,Cluster-Autoscaler FAQ entry describing events,"While writing the FAQ I noticed one event is named inconsistently with the rest, so I updated the name. It's a very minor change so I included it in this PR, but I can move it to a separate one if you think it makes more sense @mwielgus ",closed,True,2017-04-10 09:54:15,2017-04-10 13:29:57
contrib,mwielgus,https://github.com/kubernetes/contrib/issues/2540,https://api.github.com/repos/kubernetes/contrib/issues/2540,Move Cluster-Autoscaler to a separate repo,"We would like to have a separate repo for cluster autoscaler to ensure proper branching and versioning.  

cc: @MaciekPytel @fgrzadkowski ",closed,False,2017-04-10 21:46:38,2017-04-11 08:40:47
contrib,SleepyBrett,https://github.com/kubernetes/contrib/issues/2541,https://api.github.com/repos/kubernetes/contrib/issues/2541,Cluster Autoscaler (0.4.0) - Excessive calls to describeautoscalinggroup,"We're still on 1.5.6 here and thus suck on 0.4.0

We are also in a corporate shared AWS account for just a few more weeks but we've been responsible for hammering the describe asg endpoint. At one count we had about 6200 calls an hour while watching just 2 autoscaling groups with a scan interval of 15s.

After reviewing the logs at verbosity 4 I'm seeing what seems like excessive cache regeneration log inc below.

```
I0411 00:18:03.743910       1 aws_manager.go:186] Regenerating ASG information for tf-asg-00451965f3d6dfa3c713f1f196
I0411 00:18:03.767672       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-workers-asg
I0411 00:18:03.814842       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-subnet_workers-asg
I0411 00:18:03.847489       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-flink_workers-asg
I0411 00:18:03.878911       1 aws_manager.go:186] Regenerating ASG information for tf-asg-00451965f3d6dfa3c713f1f196
I0411 00:18:03.909796       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-workers-asg
I0411 00:18:03.948900       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-subnet_workers-asg
I0411 00:18:03.971444       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-flink_workers-asg
I0411 00:18:04.014617       1 aws_manager.go:186] Regenerating ASG information for tf-asg-00451965f3d6dfa3c713f1f196
I0411 00:18:04.062654       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-workers-asg
I0411 00:18:04.112767       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-subnet_workers-asg
I0411 00:18:04.135767       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-flink_workers-asg
I0411 00:18:04.179638       1 aws_manager.go:186] Regenerating ASG information for tf-asg-00451965f3d6dfa3c713f1f196
I0411 00:18:04.208361       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-workers-asg
I0411 00:18:04.255125       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-subnet_workers-asg
I0411 00:18:04.285420       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-flink_workers-asg
I0411 00:18:04.315456       1 aws_manager.go:186] Regenerating ASG information for tf-asg-00451965f3d6dfa3c713f1f196
I0411 00:18:04.338964       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-workers-asg
I0411 00:18:04.391533       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-subnet_workers-asg
I0411 00:18:04.414932       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-flink_workers-asg
I0411 00:18:04.439831       1 aws_manager.go:186] Regenerating ASG information for tf-asg-00451965f3d6dfa3c713f1f196
I0411 00:18:04.475670       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-workers-asg
I0411 00:18:04.532452       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-subnet_workers-asg
I0411 00:18:04.565761       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-flink_workers-asg
I0411 00:18:04.596577       1 aws_manager.go:186] Regenerating ASG information for tf-asg-00451965f3d6dfa3c713f1f196
I0411 00:18:04.628497       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-workers-asg
I0411 00:18:04.689757       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-subnet_workers-asg
I0411 00:18:04.711811       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-flink_workers-asg
I0411 00:18:04.741555       1 aws_manager.go:186] Regenerating ASG information for tf-asg-00451965f3d6dfa3c713f1f196
I0411 00:18:04.767574       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-workers-asg
I0411 00:18:04.809850       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-subnet_workers-asg
I0411 00:18:04.849322       1 aws_manager.go:186] Regenerating ASG information for a0098-p18-flink_workers-asg
I0411 00:18:04.878316       1 aws_manager.go:186] Regenerating ASG information for tf-asg-00451965f3d6dfa3c713f1f196
```

I'm digging through the code trying to track down what's invalidating the cache so rapidly.",closed,False,2017-04-11 00:23:39,2018-02-22 04:12:07
contrib,diwu1989,https://github.com/kubernetes/contrib/pull/2542,https://api.github.com/repos/kubernetes/contrib/issues/2542,fixes to documentation to be more clear,"i was reading through this doc to understand how the CA works, and decided to fix up a few minor typos and grammar mistakes",closed,True,2017-04-11 02:31:39,2017-04-11 20:45:46
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2543,https://api.github.com/repos/kubernetes/contrib/issues/2543,Cluster-Autoscaler: add more info about cpu-based autoscalers to FAQ,cc: @MaciekPytel @fgrzadkowski ,closed,True,2017-04-11 21:35:56,2017-04-12 07:45:16
contrib,mkumatag,https://github.com/kubernetes/contrib/pull/2544,https://api.github.com/repos/kubernetes/contrib/issues/2544,Multi architecture test-webserver image,"Signed-off-by: Manjunath A Kumatagi <mkumatag@in.ibm.com>

This PR will enable to build test-webserver image for different architecture like amd64, ppc64le, arm etc..",closed,True,2017-04-12 08:44:21,2017-07-05 09:29:34
contrib,xialonglee,https://github.com/kubernetes/contrib/pull/2545,https://api.github.com/repos/kubernetes/contrib/issues/2545,[ansible]fix wrong condition,It has to be some mistakes with the last condition...,closed,True,2017-04-13 11:15:02,2018-01-23 07:02:20
contrib,xialonglee,https://github.com/kubernetes/contrib/pull/2546,https://api.github.com/repos/kubernetes/contrib/issues/2546,[ansible]fix var name,"Before #2441 , it called `apiserver_extra_args`, so after refactoring, it should called `kube_apiserver_additional_options` for consistency.

Using `kube_apiserver_options` will override the same named var defined in `roles/master/defaults/main.yml` .
cc @galexrt , I touch your PR.",closed,True,2017-04-13 11:45:48,2017-04-13 12:07:42
contrib,LEW21,https://github.com/kubernetes/contrib/issues/2547,https://api.github.com/repos/kubernetes/contrib/issues/2547,Cluster-Autoscaler: Downscaling even if all the remaining nodes are unschedulable,"I'm using Kubernetes 1.4 with autoscaler 0.4.0.

I had 3 unschedulable nodes in an autoscaling group, then I've started a new pod. This happened:

```
I0413 14:08:12.823425       1 scale_up.go:142] Scale-up: setting group XXX size to 4
I0413 14:08:12.919440       1 event.go:216] Event(api.ObjectReference{Kind:""Pod"", Namespace:""git"", Name:""gitlab-1123350407-69jam"", UID:""9e27db32-2052-11e7-8b84-06b948b7f09f"", APIVersion:""v1"", ResourceVersion:""30546195"", FieldPath:""""}): type: 'Normal' reason: 'TriggeredScaleUp' pod triggered scale-up, group: XXX, sizes (current/new): 3/4

W0413 14:08:23.151784       1 cluster_autoscaler.go:202] Cluster is not ready for autoscaling: wrong number of nodes for node group: XXX expected: 4 actual: 3
(...)
W0413 14:12:29.759816       1 cluster_autoscaler.go:202] Cluster is not ready for autoscaling: wrong number of nodes for node group: XXX expected: 4 actual: 3

I0413 14:18:16.394456       1 scale_down.go:163] No candidates for scale down
(...)
I0413 14:22:31.109053       1 scale_down.go:163] No candidates for scale down

I0413 14:22:41.428253       1 scale_down.go:211] Scale-down: removing node ip-10-136-47-222.eu-west-1.compute.internal, utilization: 0.19139655879874828, pods to reschedule: %!(EXTRA string=git/gitlab-1123350407-69jam)

I0413 14:22:41.594908       1 event.go:216] Event(api.ObjectReference{Kind:""Node"", Namespace:"""", Name:""ip-10-136-47-222.eu-west-1.compute.internal"", UID:""29c5fe70-2053-11e7-a64c-0aad1907bffb"", APIVersion:""v1"", ResourceVersion:""30549184"", FieldPath:""""}): type: 'Normal' reason: 'ScaleDown' node removed by cluster autoscaler

W0413 14:22:51.699649       1 cluster_autoscaler.go:202] Cluster is not ready for autoscaling: wrong number of nodes for node group: XXX expected: 3 actual: 4
(...)
W0413 14:24:13.328638       1 cluster_autoscaler.go:202] Cluster is not ready for autoscaling: wrong number of nodes for node group: XXX expected: 3 actual: 4

I0413 14:24:46.951333       1 scale_up.go:142] Scale-up: setting group XXX size to 4
I0413 14:24:47.036529       1 event.go:216] Event(api.ObjectReference{Kind:""Pod"", Namespace:""git"", Name:""gitlab-1123350407-ize6p"", UID:""e070554b-2054-11e7-8b84-06b948b7f09f"", APIVersion:""v1"", ResourceVersion:""30549570"", FieldPath:""""}): type: 'Normal' reason: 'TriggeredScaleUp' pod triggered scale-up, group: XXX, sizes (current/new): 3/4

W0413 14:24:57.214977       1 cluster_autoscaler.go:202] Cluster is not ready for autoscaling: wrong number of nodes for node group: XXX expected: 4 actual: 3
(...)
```",closed,False,2017-04-13 14:39:53,2017-04-19 14:55:01
contrib,naarani,https://github.com/kubernetes/contrib/pull/2548,https://api.github.com/repos/kubernetes/contrib/issues/2548,fix openssl vulnerability,"the image is based on previous versions, I don't think this should be done in the future or it will add too many layers and all the packages should be updated manually to prevent bugs and vulnerability",closed,True,2017-04-13 22:31:37,2018-03-04 08:10:44
contrib,mumoshu,https://github.com/kubernetes/contrib/pull/2549,https://api.github.com/repos/kubernetes/contrib/issues/2549,cluster-autoscaler: Re: AWS Autoscaler autodiscover ASG names and sizes,"@mwielgus @Raffo @andrewsykim @osxi Would you mind reviewing this?

---

This is an alternative implementation of #1982

Notable differences from the original PR are:

* A new flag named `--node-group-auto-discovery` is introduced for opting in to enable the auto-discovery feature.
  * For example, specifying `--cloud-provider aws --node-group-auto-discovery asg:tag=k8s.io/cluster-autoscaler/enabled` instructs CA to auto-discover ASGs tagged with `k8s.io/cluster-autoscaler/enabled` to be used as target node groups
* The new code path introduced by this PR is executed only when `node-group-auto-discovery` is specified. There is relatively less chance to break existing features by this change

Other notes:

* ~~I thought it might be a good idea to implement this feature on top of the dynamic reconfiguration #2181 initially~~
  * ~~However it turned out it is easier and less verbose to just implement an alternative aws cloud provider like what I've done for this PR~~
* We rely mainly on the `DescribeTags` API rather than `DescribeAutoScalingGroups` so that AWS can filter out unnecessary ASGs which doesn't belong to the k8s cluster, for us.
  * If we relied on `DescribeAutoScalingGroups` here, as it doesn't support `Filter`ing, we'd need to iterate over ALL the ASGs available in an AWS account, which isn't desirable due to unnecessary excessive API calls and network usages

Possible future improvements before recommending this to everyone:

* Cache the result of an auto-discovery for a configurable period, so that we won't invoke DescribeTags and DescribeAutoScalingGroup APIs too many times
",closed,True,2017-04-14 05:43:46,2017-06-20 04:41:13
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2550,https://api.github.com/repos/kubernetes/contrib/issues/2550,Cluster-autoscaler: add default credentials to faq.,cc: @MaciekPytel @fgrzadkowski ,closed,True,2017-04-14 09:06:20,2017-04-14 10:17:06
contrib,derekparker,https://github.com/kubernetes/contrib/issues/2551,https://api.github.com/repos/kubernetes/contrib/issues/2551,[rescheduler] Rescheduler should be leader elected,It is recommended to run the rescheduler as a static manifest on the master node (https://kubernetes.io/docs/concepts/cluster-administration/guaranteed-scheduling-critical-addon-pods/) however for multi-master setups we would have multiple reschedulers running without any sort of leader election.,closed,False,2017-04-14 21:39:21,2018-02-21 06:51:01
contrib,caarlos0,https://github.com/kubernetes/contrib/issues/2552,https://api.github.com/repos/kubernetes/contrib/issues/2552,cluster-autoscaler not scaling down,"The title is a little misleading, but I didn't know how to name it.

Anyway, it does scale down, to a certain point, than it doesn't.

Usually, it stops scaling down at 2 nodes, even if I set the minimum to 1 and there are very few pods running.

Example:

```
$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                                   READY     STATUS    RESTARTS   AGE       IP             NODE
default       ingress-nginx-3737565091-xz68c                         1/1       Running   0          5d        10.39.224.4    ip-172-16-33-40.ec2.internal
default       nginx-default-backend-623339674-z93xc                  1/1       Running   0          5d        10.39.224.2    ip-172-16-33-40.ec2.internal
kube-system   cluster-autoscaler-888265702-ztn61                     1/1       Running   0          4d        10.39.248.1    ip-172-16-53-56.ec2.internal
kube-system   dns-controller-489065303-t1589                         1/1       Running   0          4d        172.16.53.56   ip-172-16-53-56.ec2.internal
kube-system   etcd-server-events-ip-172-16-53-56.ec2.internal        1/1       Running   0          4d        172.16.53.56   ip-172-16-53-56.ec2.internal
kube-system   etcd-server-ip-172-16-53-56.ec2.internal               1/1       Running   0          4d        172.16.53.56   ip-172-16-53-56.ec2.internal
kube-system   heapster-564189836-36l0p                               1/1       Running   0          5d        10.39.160.1    ip-172-16-33-98.ec2.internal
kube-system   kube-apiserver-ip-172-16-53-56.ec2.internal            1/1       Running   0          4d        172.16.53.56   ip-172-16-53-56.ec2.internal
kube-system   kube-controller-manager-ip-172-16-53-56.ec2.internal   1/1       Running   0          4d        172.16.53.56   ip-172-16-53-56.ec2.internal
kube-system   kube-dns-782804071-wgbdm                               4/4       Running   0          5d        10.39.160.4    ip-172-16-33-98.ec2.internal
kube-system   kube-dns-autoscaler-2813114833-k7m8r                   1/1       Running   0          5d        10.39.224.3    ip-172-16-33-40.ec2.internal
kube-system   kube-proxy-ip-172-16-33-40.ec2.internal                1/1       Running   0          5d        172.16.33.40   ip-172-16-33-40.ec2.internal
kube-system   kube-proxy-ip-172-16-33-98.ec2.internal                1/1       Running   0          5d        172.16.33.98   ip-172-16-33-98.ec2.internal
kube-system   kube-proxy-ip-172-16-53-56.ec2.internal                1/1       Running   0          4d        172.16.53.56   ip-172-16-53-56.ec2.internal
kube-system   kube-scheduler-ip-172-16-53-56.ec2.internal            1/1       Running   0          4d        172.16.53.56   ip-172-16-53-56.ec2.internal
kube-system   kubernetes-dashboard-3203831700-nt5s0                  1/1       Running   0          4d        10.39.248.2    ip-172-16-53-56.ec2.internal
kube-system   monitoring-influxdb-grafana-v4-t31zw                   2/2       Running   0          5d        10.39.160.2    ip-172-16-33-98.ec2.internal
kube-system   weave-net-cv3hj                                        2/2       Running   1          5d        172.16.33.98   ip-172-16-33-98.ec2.internal
kube-system   weave-net-gmvnv                                        2/2       Running   0          5d        172.16.33.40   ip-172-16-33-40.ec2.internal
kube-system   weave-net-mvr9n                                        2/2       Running   0          4d        172.16.53.56   ip-172-16-53-56.ec2.internal
```

And yet:

```
$ kubectl get nodes
NAME                           STATUS         AGE       VERSION
ip-172-16-33-40.ec2.internal   Ready          5d        v1.5.6
ip-172-16-33-98.ec2.internal   Ready          5d        v1.5.6
ip-172-16-53-56.ec2.internal   Ready,master   4d        v1.5.6
```

The allocated resources are OK too:

```
Name:			ip-172-16-33-40.ec2.internal
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  330m (16%)	210m (10%)	430Mi (2%)	420Mi (2%)


Name:			ip-172-16-33-98.ec2.internal
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  760m (38%)	400m (20%)	1140Mi (7%)	1220Mi (7%)



Name:			ip-172-16-53-56.ec2.internal
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests	CPU Limits	Memory Requests	Memory Limits
  ------------	----------	---------------	-------------
  1100m (55%)	300m (15%)	750Mi (19%)	700Mi (18%)
```

The cluster autoscaler logs only say `I0417 11:52:11.065164       1 scale_down.go:163] No candidates for scale down`.

Is it intended and I'm not understanding it right? How can I improve this?

PS1: `ip-172-16-53-56.ec2.internal` is the master.
PS2: I'm using this in a dev cluster, that's why I want to downscale a lot ",closed,False,2017-04-17 11:53:33,2017-04-24 11:39:13
contrib,dwatrous,https://github.com/kubernetes/contrib/pull/2553,https://api.github.com/repos/kubernetes/contrib/issues/2553,Update README.md,add note about ingress host and common name.,closed,True,2017-04-17 16:33:14,2018-02-21 07:52:00
contrib,kow3ns,https://github.com/kubernetes/contrib/pull/2554,https://api.github.com/repos/kubernetes/contrib/issues/2554,A Kafka StatefulSet that works with the existing ZooKeeper example.,,closed,True,2017-04-17 19:23:56,2017-06-11 08:44:27
contrib,gouyang,https://github.com/kubernetes/contrib/issues/2555,https://api.github.com/repos/kubernetes/contrib/issues/2555,[Ansible] inconsistent key name in ansible/roles/flannel/templates/flanneld.j2,"Some begin with `FLANNEL` and some begin with `FLANNELD`, it would be better to have them all begin with `FLANNEL`, after this change, the flannel code itself might need to make changes accordingly.

```
FLANNEL_ETCD_KEY=""/{{ cluster_name }}/network""
FLANNEL_ETCD_PREFIX=""/{{ cluster_name }}/network""

{% if etcd_url_scheme is defined and etcd_url_scheme == 'https' %}
FLANNELD_ETCD_CAFILE=""{{ flannel_etcd_ca_file }}""
FLANNELD_ETCD_CERTFILE=""{{ flannel_etcd_cert_file }}""
FLANNELD_ETCD_KEYFILE=""{{ flannel_etcd_key_file }}""
```",closed,False,2017-04-18 03:35:40,2018-01-23 23:33:01
contrib,kenm47,https://github.com/kubernetes/contrib/pull/2556,https://api.github.com/repos/kubernetes/contrib/issues/2556,Fix typo (Kubernets -> Kubernetes),Minor typo change.,closed,True,2017-04-18 22:45:01,2017-04-19 12:02:54
contrib,dluc,https://github.com/kubernetes/contrib/pull/2557,https://api.github.com/repos/kubernetes/contrib/issues/2557,fix typos (re-applying pull/2362),,closed,True,2017-04-19 06:04:57,2017-04-19 06:37:05
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2558,https://api.github.com/repos/kubernetes/contrib/issues/2558,Cluster-Autoscaler: remove all code - the project has moved to https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler,"Ref: #2533

cc: @MaciekPytel @piosz @jszczepkowski @fgrzadkowski @andrewsykim @mumoshu @Raffo",closed,True,2017-04-19 09:38:12,2017-04-19 12:17:14
contrib,nickjbyrne,https://github.com/kubernetes/contrib/issues/2559,https://api.github.com/repos/kubernetes/contrib/issues/2559,[keepalived-vip] build failing,"After godep restore -v running make controller fails with 

`rm -f kube-keepalived-vip
CGO_ENABLED=0 GOOS=linux go build -a -ldflags '-w' -o kube-keepalived-vip
_/path/kubernetes/contrib/keepalived-vip
./main.go:65: cannot use flags (type *""github.com/spf13/pflag"".FlagSet) as type *""k8s.io/kubernetes/vendor/github.com/spf13/pflag"".FlagSet in argument to util.DefaultClientConfig`

These are my versions

`godep v79 (linux/amd64/go1.7.5)
go version go1.7.5 linux/amd64`

I'm not familiar with go dependencies and resolution, any quick fix would be appreciated",closed,False,2017-04-19 11:11:05,2017-04-19 15:56:31
contrib,mkumatag,https://github.com/kubernetes/contrib/pull/2560,https://api.github.com/repos/kubernetes/contrib/issues/2560,Fix the import path,"Part of https://github.com/kubernetes/kubernetes/pull/39475, `k8s.io/kubernetes/pkg/util/sets` moved to `k8s.io/apimachinery/pkg/util/sets` so here import path needs to be fixed.",closed,True,2017-04-24 05:23:41,2017-06-09 17:32:45
contrib,DarkBlaez,https://github.com/kubernetes/contrib/issues/2561,https://api.github.com/repos/kubernetes/contrib/issues/2561,More of a question,"I see the ansible install has Kubernetes 1.4.5 referenced. Is there a plan to move or option this to the current 1.6 release (I think 1.6.2)?

-DB",closed,False,2017-04-24 20:26:43,2018-03-09 13:13:33
contrib,DarkBlaez,https://github.com/kubernetes/contrib/issues/2562,https://api.github.com/repos/kubernetes/contrib/issues/2562,[Ansible] Unable to enable service kubelet,"OS: Ubuntu 16.04 server

Running basic script as is in the contrib/ansible. Just changed master, nodes, etc in the inventory. When it gets to the node(s) this error results

TASK [node : Enable kubelet] *******************************************************************************************
fatal: [192.168.1.20]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Unable to enable service kubelet: Failed to execute operation: Unit file is masked\n""}

This is on a fresh install of Ubuntu 16.04 server, 1 master/etcd and 1 node to test before I expand to all  my servers.

Thoughts? Help?

Thanks
DB
",closed,False,2017-04-24 23:24:32,2017-04-25 17:11:10
contrib,killianbrackey,https://github.com/kubernetes/contrib/pull/2563,https://api.github.com/repos/kubernetes/contrib/issues/2563,Typo fix in custom-configuration readme,Small fix for anybody copy/pasting examples. ,closed,True,2017-04-25 16:00:27,2018-02-22 03:10:34
contrib,smenon78,https://github.com/kubernetes/contrib/issues/2564,https://api.github.com/repos/kubernetes/contrib/issues/2564,Failed to retrieve network config: client: etcd cluster is unavailable or misconfigured,"OS: CentOS 7

Ansible playbook stops at 
TASK [node : Start kubelet] ***********************************************************************************************************************************************************
fatal: [172.22.16.62]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Unable to start service kubelet: A dependency job for kubelet.service failed. See 'journalctl -xe' for details.\n""}

Logs says:
Failed to retrieve network config: client: etcd cluster is unavailable or misconfigured

This was working a week ago. Any ideas?",closed,False,2017-04-25 16:34:34,2017-04-26 19:20:18
contrib,DarkBlaez,https://github.com/kubernetes/contrib/issues/2565,https://api.github.com/repos/kubernetes/contrib/issues/2565,Change to expose hostname rather than localhost,"Surprised a prior issue around this was closed without resolution. Summary of issue

All nodes Ubuntu 16.04

    Inventory containing IP addresses only of each master, node
    A) run ansible, result localhost referenced as indicated in the original issue above. this should be considered a failure of the install. One would expect IPs defined to be used, not localhost

    Inventory containing fully qualified server names
    A) ansible blows chunks when trying to reference the certs with flanneld indicating the actual IPs over fqdn. Might be a cert generation issue

Either way has anyone gone back and checked the current ansible install on fresh VMs. I've already found one issue with the current and Ubuntu 16.04

This issue should be considered still open until resolved as default behavior with FQDN or IP defined in inventory should not default to services on localhost or even referencing localhost/127.0.0.1.

DB",closed,False,2017-04-25 19:38:18,2018-02-21 19:03:03
contrib,githubvick,https://github.com/kubernetes/contrib/issues/2566,https://api.github.com/repos/kubernetes/contrib/issues/2566,[Ansible] Https Master-Node communication not working with Azure,"I had used the ansible scripts to install Kubernetes cluster on VMWare Vcloud director and it worked perfect. I tried to do the same on Azure, but the cluster communication between master and node doesnt happen and gives the below error.

```
Apr 25 21:29:42 hostname kube-scheduler[50716]: E0425 21:29:42.079238   50716 reflector.go:199] k8s.io/kubernetes/plugin/pkg/scheduler/factory/factory.go:457: Failed to list *api.Pod: Get https://13.82.180.84:443/api/v1/pods?fieldSelector=spec.nodeName%3D%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded&resourceVersion=0: x509: certificate is valid for 10.254.0.1, 172.16.29.0, 172.16.29.1, 10.0.1.5, not 13.82.180.84
Apr 25 21:29:42 hostname kube-apiserver[57829]: I0425 21:29:42.099905   57829 logs.go:41] http: TLS handshake error from 13.82.180.84:49176: read tcp 10.0.1.5:443->13.82.180.84:49176: read: connection reset by peer

```
Issue is because the certificate of server.crt on the node is not signed with its own IP Address I guess as shown below where 13.82.180.84 is the public IP Address. 

sudo openssl x509 -in /etc/kubernetes/certs/server.crt -text -noout

```
 X509v3 Subject Alternative Name:
                IP Address:10.254.0.1, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:172.16.29.0, IP Address:172.16.29.1, IP Address:10.0.1.5, DNS:13.82.180.84
```

I'm proceeding to fix the issue on my codebase, I see that the IP Address is not being taken properly during makecert.sh and looking how to include the IP Address., any inputs on how to achieve would be very helpful as Im just a beginner in Ansible. 

Also, if it is fixed generically to be working on AWS, ,Azure Google etc, would be great in the longer term.

Thanks,",closed,False,2017-04-25 21:41:52,2018-02-22 00:07:39
contrib,drcrallen,https://github.com/kubernetes/contrib/pull/2567,https://api.github.com/repos/kubernetes/contrib/issues/2567,Make formatting less bad on zookeeper README.md,,closed,True,2017-04-27 01:45:25,2018-02-26 22:03:49
contrib,huikang,https://github.com/kubernetes/contrib/pull/2568,https://api.github.com/repos/kubernetes/contrib/issues/2568,Ansible vagrant requires the vagrant-aws plugin,"Add a note in the readme

Signed-off-by: Hui Kang <hkang.sunysb@gmail.com>",closed,True,2017-04-27 15:37:30,2017-06-02 21:48:12
contrib,DarkBlaez,https://github.com/kubernetes/contrib/issues/2569,https://api.github.com/repos/kubernetes/contrib/issues/2569,[Ansible] Add-on in all.yml ignored,"Scripts are ignoring some settings in group_vars/all.yml

example:

Turn to false to disable the kube-ui addon for this cluster
kube_ui: false

Turn to false to disable the kube-dash addon for this cluster
kube_dash: false

Regardless of setting false which is the default right from the git repo, the dashboard is still installed and started

DB",closed,False,2017-04-27 18:34:36,2018-02-22 00:07:41
contrib,piosz,https://github.com/kubernetes/contrib/pull/2570,https://api.github.com/repos/kubernetes/contrib/issues/2570,Implemented unit tests for rescheduler,fix #2465,closed,True,2017-04-28 07:44:26,2017-04-28 13:35:01
contrib,crassirostris,https://github.com/kubernetes/contrib/issues/2571,https://api.github.com/repos/kubernetes/contrib/issues/2571,[prometheus-to-sd] Warning about missing process start time metrics should not be logged in loop,"There are processes that lack this metrics, in this case [warning](https://github.com/kubernetes/contrib/blob/3ab70a970b8ab54086383a606f20a5f2e73df66a/prometheus-to-sd/translator/translator.go#L44) gets logged on each cycle, trashing logs",closed,False,2017-04-28 11:21:41,2018-06-24 03:33:29
contrib,DarkBlaez,https://github.com/kubernetes/contrib/issues/2572,https://api.github.com/repos/kubernetes/contrib/issues/2572,[Question] Enabling RBAC in 1.6.0,"In the ansible scripts now that I have set this for v1.6.0 which is provided from the Centos 7.3 repo, how would I enable RBAC. I see that adding --authorization-mode=RBAC to the apiserver ARGS should be all that it takes. But doing so does not result in a functional cluster where other nodes cannot interact with the master. Is there more needed then just adding the above to the apiserver config args?

Thanks
DB",closed,False,2017-05-01 20:42:47,2017-05-23 17:49:08
contrib,codecap,https://github.com/kubernetes/contrib/issues/2573,https://api.github.com/repos/kubernetes/contrib/issues/2573,nginx ingress controller does not handle host headers longer than 47 characters,"__Version__: 0.8.3


__Working__:  Host with 46 characters
```yaml
  - host: kubernetes-dashboard2345678901234567890123456
    http:
      paths:
      - path: /
        backend:
          serviceName: kubernetes-dashboard
          servicePort: 9090
```

curl -k -D- -L -H ""Host: kubernetes-dashboard2345678901234567890123456""  [IP]
```bash
HTTP/1.1 401 Unauthorized
Server: nginx/1.11.3
Date: Tue, 02 May 2017 11:26:52 GMT
Content-Type: text/html
Content-Length: 195
Connection: keep-alive
WWW-Authenticate: Basic realm=""Authentication Required""

<html>
<head><title>401 Authorization Required</title></head>
<body bgcolor=""white"">
<center><h1>401 Authorization Required</h1></center>
<hr><center>nginx/1.11.3</center>
</body>
</html>
```


__Broken__: Host with 50 characters
```yaml
  - host: kubernetes-dashboard23456789012345678901234567890
    http:
      paths:
      - path: /
        backend:
          serviceName: kubernetes-dashboard
          servicePort: 9090
```
curl -k -D- -L -H ""Host: kubernetes-dashboard23456789012345678901234567890"" [IP]
```bash
HTTP/1.1 404 Not Found
Server: nginx/1.11.3
Date: Tue, 02 May 2017 11:27:06 GMT
Content-Type: text/plain; charset=utf-8
Content-Length: 21
Connection: keep-alive
Strict-Transport-Security: max-age=15724800; includeSubDomains; preload

default backend - 404
```",closed,False,2017-05-02 11:38:37,2018-02-22 04:12:04
contrib,gaplyk,https://github.com/kubernetes/contrib/issues/2574,https://api.github.com/repos/kubernetes/contrib/issues/2574,[prometheus-to-sd] Service does not send any metrics without whitelisted-metrics,"In description to this param:
`Comma-separated list of whitelisted metrics. If empty all metrics will be exported.` 
But it does not send any metrics because of this line:
https://github.com/kubernetes/contrib/blob/master/prometheus-to-sd/main.go#L82
the result array will always have at least one element.

",closed,False,2017-05-02 19:27:51,2017-05-08 16:43:05
contrib,gaplyk,https://github.com/kubernetes/contrib/pull/2575,https://api.github.com/repos/kubernetes/contrib/issues/2575,Fix reading metrics with empty whitelisted-metrics flag,"this pr fixes issue when you do launch
```
./monitor
```
without param --whitelisted-metrics

Expected behavior: monitor service will send all metrics.
Actual: Error message - No metrics to send to Stackdriver",closed,True,2017-05-02 20:08:42,2017-05-08 16:42:57
contrib,Q-Lee,https://github.com/kubernetes/contrib/pull/2576,https://api.github.com/repos/kubernetes/contrib/issues/2576,Metadata proxy,Adding a container to proxy the gce metadata service.,closed,True,2017-05-02 22:26:51,2017-05-26 23:28:51
contrib,cfarquhar,https://github.com/kubernetes/contrib/pull/2577,https://api.github.com/repos/kubernetes/contrib/issues/2577,Fix yaml syntax in echoheaders example,"This commit fixes the following error:

```
$ kubectl create -f ./echoheaders.yaml
error: error converting YAML to JSON: yaml: line 17: could not find expected ':'
```",closed,True,2017-05-03 00:11:34,2017-06-02 22:42:15
contrib,gaplyk,https://github.com/kubernetes/contrib/issues/2578,https://api.github.com/repos/kubernetes/contrib/issues/2578,[prometheus-to-sd] Add ca-certificates.crt to docker image,"Getting an error when sending data to stackdriver:
```
1 stackdriver.go:40] Error while sending request to Stackdriver Post https://monitoring.googleapis.com/v3/projects/<project-name>/timeSeries?alt=json: x509: failed to load system roots and no roots provided
```
",closed,False,2017-05-03 14:23:44,2017-05-16 15:54:50
contrib,qingling128,https://github.com/kubernetes/contrib/issues/2579,https://api.github.com/repos/kubernetes/contrib/issues/2579,Update fluent-plugin-google-cloud version in fluentd-gcp-image image to the latest (>= v0.6.1),"Two related `fluent-plugin-google-cloud` issues https://github.com/GoogleCloudPlatform/fluent-plugin-google-cloud/issues/84 and https://github.com/GoogleCloudPlatform/fluent-plugin-google-cloud/issues/85 were fixed in fluent-plugin-google-cloud [`v0.6.1`](https://github.com/GoogleCloudPlatform/fluent-plugin-google-cloud/releases/tag/v0.6.1).

Yet the `fluent-plugin-google-cloud` [version](https://github.com/kubernetes/contrib/blob/1c6a67cb723ceb7fd997f6c3d6a2fe0d780eaff4/fluentd/fluentd-gcp-image/Gemfile#L6) in `fluentd-gcp-image` is still pinned to `v0.5.*`.

A newer image needs to be built with the latest `fluent-plugin-google-cloud` to have the fix applied.",closed,False,2017-05-03 15:47:37,2018-02-24 17:57:36
contrib,genti-t,https://github.com/kubernetes/contrib/issues/2580,https://api.github.com/repos/kubernetes/contrib/issues/2580,[keepalived-vip] - VIP only ?,"Is it possibile to use keepalived-vip, only just as an IP-failover mechanism?
For example, instead of exposing the services on a specific namespace, keeping only the virtual ip in on the keepalived template, like this :

```
{{ $iface := .iface }}{{ $netmask := .netmask }}

global_defs {
  vrrp_version 3
  vrrp_iptables {{ .iptablesChain }}
}

vrrp_instance vips {
  state BACKUP
  interface {{ $iface }}
  virtual_router_id {{ .vrid }}
  priority {{ .priority }}
  nopreempt
  advert_int 1

  track_interface {
    {{ $iface }}
  }

  {{ if .useUnicast }}
  unicast_src_ip {{ .myIP }}
  unicast_peer { {{ range .nodes }}
    {{ . }}{{ end }}
  }
  {{ end }}

  virtual_ipaddress { {{ range .vips }}
    {{ . }}{{ end }}
  }
}
```

I use traefik as loadbalancer and having only virtual ip assigned, is enough for me.

Thanks",closed,False,2017-05-03 16:36:48,2018-03-09 14:14:35
contrib,widgetpl,https://github.com/kubernetes/contrib/pull/2581,https://api.github.com/repos/kubernetes/contrib/issues/2581,added extra parameters to define node for nodeSelector,"Added extra parameters to be able to stick to specific cluster node:

`serverNode` - specify node for netperf server
`clientNode` - specify node for netperf client

example usage:

`./netperf-tester -number 500 -output data.csv -serverNode 10.122.16.118 -clientNode 10.122.16.119`",closed,True,2017-05-05 12:34:10,2018-02-24 11:05:49
contrib,bowei,https://github.com/kubernetes/contrib/issues/2582,https://api.github.com/repos/kubernetes/contrib/issues/2582,Add README to prometheus-to-sd,"It would be good to add a readme to describe how to use this component.

",closed,False,2017-05-05 18:04:21,2017-08-09 04:19:56
contrib,xizhibei,https://github.com/kubernetes/contrib/pull/2583,https://api.github.com/repos/kubernetes/contrib/issues/2583,[Ansible] remove conflicted configure task,"I found the task **Get Systemd config files from Kubernetes repository** is conflicted with following tasks, and it override the correct config, causing the cluster deploy failed.

- https://github.com/kubernetes/contrib/blob/master/ansible/roles/kubernetes/tasks/configure.yml
- https://github.com/kubernetes/contrib/blob/master/ansible/roles/node/tasks/kubelet-configure.yml
- https://github.com/kubernetes/contrib/blob/master/ansible/roles/node/tasks/proxy-configure.yml",closed,True,2017-05-06 05:37:59,2018-02-25 02:20:48
contrib,gintautassulskus,https://github.com/kubernetes/contrib/issues/2584,https://api.github.com/repos/kubernetes/contrib/issues/2584,Ansible for centosatomic fails on kube-apiserver validation - missing.,"Hi,

I am trying to build a local cluster of centosatomic instances, but the Ansible script validation fails with the following error:
```
TASK [master : Enable apiserver] ***********************************************
fatal: [kube-master-1]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Could not find the requested service kube-apiserver: host""}
```
Probably the scripts need to be modified due to the following changes: https://wiki.centos.org/SpecialInterestGroup/Atomic/ContainerizedMaster

Any advice/confirmation would be very welcome.

Best,
Gintas",closed,False,2017-05-07 16:13:00,2018-02-08 03:14:41
contrib,gyliu513,https://github.com/kubernetes/contrib/issues/2585,https://api.github.com/repos/kubernetes/contrib/issues/2585,The latest rescheduler do not include listen address flag,"The test steps are as following:

1) Check out `contrib` code
2) go to `contrib` and `make build`
3) build succeed, but the help info do not include flag `listen-address`.

The source code is here https://github.com/kubernetes/contrib/blob/master/rescheduler/rescheduler.go#L83-L84

My test
```
root@k8s001:~/go/src/k8s.io/contrib/rescheduler# ./rescheduler  --help
Usage of rescheduler: rescheduler --running-in-cluster=true:
      --housekeeping-interval duration   How often rescheduler takes actions. (default 10s)
      --initial-delay duration           How long should rescheduler wait after start to make sure
		 all critical addons had a chance to start. (default 2m0s)
      --kube-api-content-type string     Content type of requests sent to apiserver. (default ""application/vnd.kubernetes.protobuf"")
      --pod-scheduled-timeout duration   How long should rescheduler wait for critical pod to be scheduled
		 after evicting pods to make a spot for it. (default 10m0s)
      --running-in-cluster               Optional, if this controller is running in a kubernetes cluster, use the
		 pod secrets for creating a Kubernetes client. (default true)
      --system-namespace string          Namespace to watch for critical addons. (default ""kube-system"")
```

/cc @piosz  can you help?
",closed,False,2017-05-08 07:08:30,2017-05-08 17:13:42
contrib,gyliu513,https://github.com/kubernetes/contrib/pull/2586,https://api.github.com/repos/kubernetes/contrib/issues/2586,Enabled rescheduler help include listen address.,Fixed #2585,closed,True,2017-05-08 07:38:56,2017-05-08 23:26:19
contrib,mrcrgl,https://github.com/kubernetes/contrib/issues/2587,https://api.github.com/repos/kubernetes/contrib/issues/2587,ErrImagePull at scale-demo: loader (vegeta) unknown tag,"Hi there,

the referenced Tag `0.6` is not accessable in the container registry (see https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/loader).

Either the reference in the code is wrong, what i could fix quickly, or the correct version of vegeta is not published. Does someone know further info about that?

Cheers
Marc",closed,False,2017-05-08 13:22:18,2018-02-22 09:36:07
contrib,gaplyk,https://github.com/kubernetes/contrib/pull/2588,https://api.github.com/repos/kubernetes/contrib/issues/2588,Add ca-certificates to prometheus-to-sd to make https requests,,closed,True,2017-05-08 16:22:21,2017-05-17 08:31:29
contrib,jar349,https://github.com/kubernetes/contrib/pull/2589,https://api.github.com/repos/kubernetes/contrib/issues/2589,allow explicit configuration of the kube namespace,"Production k8s clusters often use multiple namespaces.  `deploy-service.sh` pulls the namespace to use from the first context it finds in `~/.kube/kubectl config view`.  This makes it impossible to create Deployments in a specified namespace.

In addition, those of us who use `kops` to export their `~/.kube/config` have found that `kops` does not include namespace in its exported config file due to the reason given above.  The result is that `kops` users cannot also use the `continuousdelivery` project here.

This pull request makes the k8s namespace a parameter passed to the `deploy-service.sh` script and then updates the three CI systems' example config files to include a new `KUBENAMESPACE` environment variable, which is then passed to the `deploy-service.sh` script.",closed,True,2017-05-08 21:57:29,2018-02-24 03:58:50
contrib,ChenXiaoTemp,https://github.com/kubernetes/contrib/issues/2590,https://api.github.com/repos/kubernetes/contrib/issues/2590,Missing vagrant-aws,"Following error occurs when I run vagrant up in ansible/vagrant folder
There was an error loading a Vagrantfile. The file being loaded
and the error message are shown below. This is usually caused by
a syntax error.

Path: /home/shawn/Develop/WorkSpace/Kubernetes1/contrib/ansible/vagrant/Vagrantfile
Line number: 0
Message: LoadError: cannot load such file -- vagrant-aws",closed,False,2017-05-09 09:26:48,2018-03-10 16:40:59
contrib,smenon78,https://github.com/kubernetes/contrib/issues/2591,https://api.github.com/repos/kubernetes/contrib/issues/2591,Kubeadm vs Kubernetes Ansbile. ,"More of a question. For a production system, what are the advantages/disadvantages of using kubeadm to deploy kubernetes vs contrib/ansible? 
Currently we are using terraform to create the infrastructure to aws/vsphere and use kubeadm to deploy kubernetes. I have also used contrib/ansible to deploy to CentOs vms and now considering  which option to choose.
",closed,False,2017-05-09 17:16:56,2018-03-09 11:11:36
contrib,gmarek,https://github.com/kubernetes/contrib/pull/2592,https://api.github.com/repos/kubernetes/contrib/issues/2592,Fix perfdash after moving performance data to separate files,"FYI @Random-Liu @dchen1107 - I want to wait for main repo changes to go in first, so that I can actually test this one. But this in theory should be enough.",closed,True,2017-05-10 14:10:14,2017-05-12 13:10:36
contrib,dimthe,https://github.com/kubernetes/contrib/issues/2593,https://api.github.com/repos/kubernetes/contrib/issues/2593,"TASK [flannel : Load the flannel config file into etcd] , fails","hello 
i get this error , what do i need to take a look at please?



TASK [flannel : Load the flannel config file into etcd] ******************************************************************************************************************************************************
fatal: [centos-atomic-01 -> None]: FAILED! => {""changed"": true, ""cmd"": ""/usr/bin/etcdctl --no-sync --peers=http://centos-atomic-02:2379 set /cluster.local/network/config < /tmp/flannel-conf.json"", ""delta"": ""0:00:00.046901"", ""end"": ""2017-05-11 14:36:23.550022"", ""failed"": true, ""rc"": 4, ""start"": ""2017-05-11 14:36:23.503121"", ""stderr"": ""Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp: lookup centos-atomic-02: no such host\n\nerror #0: dial tcp: lookup centos-atomic-02: no such host"", ""stderr_lines"": [""Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp: lookup centos-atomic-02: no such host"", """", ""error #0: dial tcp: lookup centos-atomic-02: no such host""], ""stdout"": """", ""stdout_lines"": []}
",closed,False,2017-05-11 14:42:36,2017-05-16 17:10:22
contrib,totahuanocotl,https://github.com/kubernetes/contrib/issues/2594,https://api.github.com/repos/kubernetes/contrib/issues/2594,[addon-resizer] A misconfigured container is still reported as healthy/ready,"I can see in the logs that the resizer is not able to find the target container, due to a spelling mistake:

```
I0511 16:36:42.889365       1 nanny_lib.go:90] The number of nodes is 9
E0511 16:36:42.913472       1 nanny_lib.go:95] Error while querying apiserver for resources: Container mispelled-container-name was not found in deployment mispelled-deployment-name in namespace kube-system.
```
The nanny container itself was reported as ready, but failed to resize the target deployment which resulted in an outage for a cluster where the default resources were not enough for it to work. The pod kept being OOM Killed. 

Currently there is no way to prevent this from happening as this state is not visible outside of the container.
",closed,False,2017-05-11 16:56:18,2018-02-22 06:13:36
contrib,warmchang,https://github.com/kubernetes/contrib/pull/2595,https://api.github.com/repos/kubernetes/contrib/issues/2595,The Ingress controllers have moved to the kubernetes/ingress repository.,,closed,True,2017-05-12 07:31:24,2017-09-08 22:46:42
contrib,soumypau1,https://github.com/kubernetes/contrib/issues/2596,https://api.github.com/repos/kubernetes/contrib/issues/2596,404-image build fails,"Hello,
I was trying to build the 404 image . (The end goal is setting up Ingress). However, i am facing the following issue. I do not have any clue about GO language. Please help.

soumyadeep@angrybird:~/software/kube-master/contrib/404-server$ make container 
# Compile the binary inside a container for reliable builds
docker pull golang:1.7
1.7: Pulling from library/golang
Digest: sha256:036e430b902b1b91070492953992a6784621753c858f7d305f11916b12a5b5b6
Status: Image is up to date for golang:1.7
docker run --rm -it -v /home/soumyadeep/software/kube-master/contrib/404-server:/build golang:1.7 /bin/bash -c ""make -C /build server ARCH=amd64""
make: Entering directory '/build'
CGO_ENABLED=0 GOOS=linux GOARCH=amd64 GOARM=6 go build -a -installsuffix cgo -ldflags '-w -s' -o server ./server.go
server.go:28:2: cannot find package ""github.com/prometheus/client_golang/prometheus"" in any of:
	/usr/local/go/src/github.com/prometheus/client_golang/prometheus (from $GOROOT)
	/go/src/github.com/prometheus/client_golang/prometheus (from $GOPATH)
server.go:29:2: cannot find package ""github.com/prometheus/client_golang/prometheus/promhttp"" in any of:
	/usr/local/go/src/github.com/prometheus/client_golang/prometheus/promhttp (from $GOROOT)
	/go/src/github.com/prometheus/client_golang/prometheus/promhttp (from $GOPATH)
Makefile:30: recipe for target 'server' failed
make: *** [server] Error 1
make: Leaving directory '/build'
make: *** [container] Error 2
soumyadeep@angrybird:~/software/kube-master/contrib/404-server$ 
",closed,False,2017-05-12 11:12:16,2018-02-22 07:14:47
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2597,https://api.github.com/repos/kubernetes/contrib/issues/2597,Fix behavior in case process start metric is missing,,closed,True,2017-05-12 11:45:52,2017-05-16 13:49:59
contrib,xeor,https://github.com/kubernetes/contrib/issues/2598,https://api.github.com/repos/kubernetes/contrib/issues/2598,New release of gcr.io/google_containers/kube-keepalived-vip,"Can we get a build of `gcr.io/google_containers/kube-keepalived-vip`, bumping the container from the current `0.9`?

There are a couple of pull-requests that have been added, example https://github.com/kubernetes/contrib/pull/2344, which might be important for some people.",closed,False,2017-05-12 22:30:59,2017-08-02 00:49:21
contrib,dimthe,https://github.com/kubernetes/contrib/issues/2599,https://api.github.com/repos/kubernetes/contrib/issues/2599,TASK [contiv : Netplugin | Start Netplugin]  fails with no logs,"Hello 
running the ansible role to deploy kubernetes on several atomic centos hosts. The role fails here 

TASK [contiv : Netplugin | Start Netplugin] **********************************************************************************************************
skipping: [atomic02]
skipping: [atomic03]
	to retry, use: --limit @/home/centos/centos-atomic-k8s/third-party/contrib/ansible/playbooks/deploy-cluster.retry

what should i pay attention to please
thanks",closed,False,2017-05-15 11:34:11,2017-05-15 11:50:16
contrib,dimthe,https://github.com/kubernetes/contrib/issues/2600,https://api.github.com/repos/kubernetes/contrib/issues/2600,TASK [atomic-k8s-master-post : Install the k8s dashboard]       fails,"hello 

any tips why this fails and how can i continue?


TASK [atomic-k8s-master-post : Install the k8s dashboard] ****************************************************************************************************************************************************
fatal: [atomic01]: FAILED! => {""changed"": true, ""cmd"": [""/usr/bin/kubectl"", ""create"", ""-f"", ""https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml""], ""delta"": ""0:00:00.962618"", ""end"": ""2017-05-15 11:49:29.138992"", ""failed"": true, ""rc"": 1, ""start"": ""2017-05-15 11:49:28.176374"", ""stderr"": ""Error from server (AlreadyExists): error when creating \""https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml\"": serviceaccounts \""kubernetes-dashboard\"" already exists\nError from server (BadRequest): error when creating \""https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml\"": ClusterRoleBinding in version \""v1beta1\"" cannot be handled as a ClusterRoleBinding: no kind \""ClusterRoleBinding\"" is registered for version \""rbac.authorization.k8s.io/v1beta1\""\nerror validating \""https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml\"": error validating data: found invalid field tolerations for v1.PodSpec; if you choose to ignore these errors, turn validation off with --validate=false"", ""stderr_lines"": [""Error from server (AlreadyExists): error when creating \""https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml\"": serviceaccounts \""kubernetes-dashboard\"" already exists"", ""Error from server (BadRequest): error when creating \""https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml\"": ClusterRoleBinding in version \""v1beta1\"" cannot be handled as a ClusterRoleBinding: no kind \""ClusterRoleBinding\"" is registered for version \""rbac.authorization.k8s.io/v1beta1\"""", ""error validating \""https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml\"": error validating data: found invalid field tolerations for v1.PodSpec; if you choose to ignore these errors, turn validation off with --validate=false""], ""stdout"": """", ""stdout_lines"": []}

RUNNING HANDLER [atomic-k8s-master-post : restart k8sapi] ****************************************************************************************************************************************************
	to retry, use: --limit @/var/home/centos/centos-atomic-k8s/atomic-master-post.retry

PLAY RECAP ***************************************************************************************************************************************************************************************************
atomic01                   : ok=6    changed=2    unreachable=0    failed=1   

",closed,False,2017-05-15 11:51:05,2018-02-24 15:09:50
contrib,piosz,https://github.com/kubernetes/contrib/pull/2601,https://api.github.com/repos/kubernetes/contrib/issues/2601,[prom-to-sd] Added support for histogram,,closed,True,2017-05-15 19:07:01,2017-05-16 09:37:14
contrib,piosz,https://github.com/kubernetes/contrib/pull/2602,https://api.github.com/repos/kubernetes/contrib/issues/2602,Minor renaming in prom-to-sd,You merged #2601 too fast,closed,True,2017-05-16 09:43:46,2017-05-16 10:01:56
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2603,https://api.github.com/repos/kubernetes/contrib/issues/2603,Update google-cloud fluentd plugin version in fluentd-gcp,,closed,True,2017-05-16 14:49:55,2017-05-17 10:06:49
contrib,dimthe,https://github.com/kubernetes/contrib/issues/2604,https://api.github.com/repos/kubernetes/contrib/issues/2604,Ansible  : TASK [master : Create symlinks]  fails,"on ubuntu 16.04 the ansible role fails at the point where it has to create symlinks with the error below. Can someone give me some guidance on this ? thanks

TASK [master : Create symlinks] ***********************************************************************************************************************************************************************************************************
failed: [ubuntu1] (item=kube-apiserver) => {""failed"": true, ""gid"": 0, ""group"": ""root"", ""item"": ""kube-apiserver"", ""mode"": ""0755"", ""msg"": ""refusing to convert between file and link for /usr/bin/kube-apiserver"", ""owner"": ""root"", ""path"": ""/usr/bin/kube-apiserver"", ""size"": 124759446, ""state"": ""file"", ""uid"": 0}
failed: [ubuntu1] (item=kube-controller-manager) => {""failed"": true, ""gid"": 0, ""group"": ""root"", ""item"": ""kube-controller-manager"", ""mode"": ""0755"", ""msg"": ""refusing to convert between file and link for /usr/bin/kube-controller-manager"", ""owner"": ""root"", ""path"": ""/usr/bin/kube-controller-manager"", ""size"": 101623874, ""state"": ""file"", ""uid"": 0}
failed: [ubuntu1] (item=kube-scheduler) => {""failed"": true, ""gid"": 0, ""group"": ""root"", ""item"": ""kube-scheduler"", ""mode"": ""0755"", ""msg"": ""refusing to convert between file and link for /usr/bin/kube-scheduler"", ""owner"": ""root"", ""path"": ""/usr/bin/kube-scheduler"", ""size"": 52878477, ""state"": ""file"", ""uid"": 0}
failed: [ubuntu1] (item=kubectl) => {""failed"": true, ""gid"": 0, ""group"": ""root"", ""item"": ""kubectl"", ""mode"": ""0755"", ""msg"": ""refusing to convert between file and link for /usr/bin/kubectl"", ""owner"": ""root"", ""path"": ""/usr/bin/kubectl"", ""size"": 50351990, ""state"": ""file"", ""uid"": 0}

",closed,False,2017-05-17 00:18:33,2017-05-20 12:46:37
contrib,galexrt,https://github.com/kubernetes/contrib/pull/2605,https://api.github.com/repos/kubernetes/contrib/issues/2605,[Ansible] Use the flanneld env vars for etcd config,"See #2555.

Use the flanneld environment variable configuration for the ""standard"" configuration instead of flags.

/cc @gouyang ",closed,True,2017-05-18 09:35:28,2017-12-14 18:47:04
contrib,bjoernhaeuser,https://github.com/kubernetes/contrib/pull/2606,https://api.github.com/repos/kubernetes/contrib/issues/2606,Use correct setting name for purgeInterval,"Correct setting name for ""purgeInterval"". Otherwise the config will not be picked up and zookeeper is running without purging, eventually causing out of disk.",closed,True,2017-05-20 18:55:08,2017-06-22 07:21:17
contrib,billy2180,https://github.com/kubernetes/contrib/pull/2607,https://api.github.com/repos/kubernetes/contrib/issues/2607,Fix keepalived vip readme nginx broken link,change nginx-alpha to nginx,closed,True,2017-05-22 03:50:23,2017-07-26 01:20:18
contrib,aledbf,https://github.com/kubernetes/contrib/pull/2608,https://api.github.com/repos/kubernetes/contrib/issues/2608,keepalived: Update keepalived to 1.3.5 and use client-go,"replaces #2041
",closed,True,2017-05-23 05:00:35,2017-09-12 10:13:28
contrib,cbarillet,https://github.com/kubernetes/contrib/issues/2609,https://api.github.com/repos/kubernetes/contrib/issues/2609,[Nginx ingress controller] - Error when rule host is too long,"Hello.

I notice that I have this error `E0523 05:05:06.946179       1 command.go:87] failed to execute nginx -s reload: 2017/05/23 05:05:06 [emerg] 240#240: could not build server_names_hash, you should increase server_names_hash_bucket_size: 64` when rule host is too long (in my case : dep-kuber-test-machine5.westeurope.cloudapp.azure.com).

If I try to request my ingress server with `dep-kuber-test-machine5.westeurope.cloudapp.azure.com`, I obtain this error : `75.131.204.163 - [75.131.204.163] - - [23/May/2017:05:06:34 +0000] ""POST / HTTP/1.1"" 502 173 ""-"" ""Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko"" 637 0.000 10.1.10.2:80 0 0.000 502
2017/05/23 05:06:34 [error] 202#202: *878 connect() failed (111: Connection refused) while connecting to upstream, client: 75.131.204.163, server: _, request: ""POST / HTTP/1.1"", upstream: ""http://10.1.10.2:80/"", host: ""13.73.161.177""`.

I confirm my analyze by replacing host 'dep-kuber-test-machine5.westeurope.cloudapp.azure.com' by another shorter 'mm2.cyril-barillet.com'.
So my ingress server responds correctly with this last host.

After some research on Internet, I found this web site which maybe give the solution : http://charles.lescampeurs.org/2008/11/14/fix-nginx-increase-server_names_hash_bucket_size

How can I apply this patch on all my instance of nginx ingress controller ?

Thank you for response.",closed,False,2017-05-23 05:35:27,2017-05-23 09:41:38
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2610,https://api.github.com/repos/kubernetes/contrib/issues/2610,[event-exporter] Add event exporter image sources,Implement tool for exporting events from Kubernetes to Stackdriver Logging,closed,True,2017-05-24 15:46:48,2017-05-31 10:26:52
contrib,smenon78,https://github.com/kubernetes/contrib/issues/2611,https://api.github.com/repos/kubernetes/contrib/issues/2611,Access kubernetes master directly using REST API,"More of a question. 
How can I access the API server directly and not vis proxy? 
following documentation
https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/
APISERVER=$(kubectl config view | grep server | cut -f 2- -d "":"" | tr -d "" "") doesn't return anything. ",closed,False,2017-05-25 22:29:22,2018-02-23 06:37:55
contrib,loburm,https://github.com/kubernetes/contrib/pull/2612,https://api.github.com/repos/kubernetes/contrib/issues/2612,Send timeserieses to stackdriver in batches.,"Stackdriver has a limit of 200 timeserieses per request. To be able to
send more we need to send them in batches.",closed,True,2017-05-26 15:32:40,2017-05-31 13:24:42
contrib,PradeepSingh1988,https://github.com/kubernetes/contrib/issues/2613,https://api.github.com/repos/kubernetes/contrib/issues/2613,Master node can not be pinged after 'flannel enable' step completes.,"Hello,

I am installing kubernetes using ansible on ubuntu 16.04.02 LTS and once the 'flannel enable' step is completed, the master node becomes unreachable.

Could you please help me with it guys?",closed,False,2017-05-29 11:21:50,2018-02-23 12:43:49
contrib,mborsz,https://github.com/kubernetes/contrib/pull/2614,https://api.github.com/repos/kubernetes/contrib/issues/2614,Watch multiple components by one prometheus-to-sd.,"Before the change, one instance of prometheus-to-sd was able to fetch
data from only one component.

Now, --source flag can be provided multiple times to describe multiple
components to be watched by one instance of prometheus-to-sd.

For backward compatibility, combination of --component, --host, --port
and --whitelisted-metrics is still supported as a method of describing
component to watch.",closed,True,2017-05-29 12:51:24,2017-05-31 10:24:01
contrib,crassirostris,https://github.com/kubernetes/contrib/issues/2615,https://api.github.com/repos/kubernetes/contrib/issues/2615,[prometheus-to-sd] Flaky tests,Example: https://travis-ci.org/kubernetes/contrib/builds/237194320,closed,False,2017-05-29 15:26:50,2017-05-30 09:51:32
contrib,mborsz,https://github.com/kubernetes/contrib/pull/2616,https://api.github.com/repos/kubernetes/contrib/issues/2616,Fix flaky TestTranslatePrometheusToStackdriver.,"It assumes particular order of result, which is not guaranteed by
TestTranslatePrometheusToStackdriver as it uses maps internally.

Tested:
```
cd prometheus-to-sd; i=0; while godep go test ./...; do echo $i; i=$((i+1)); done
```
Without fix: It was failing after 3-9 attempts
Now: Not able to reproduce failure (>100 attempts succeeded)


",closed,True,2017-05-30 08:05:41,2017-05-30 09:09:01
contrib,blues-man,https://github.com/kubernetes/contrib/issues/2617,https://api.github.com/repos/kubernetes/contrib/issues/2617,Ansible installation doesn't remove journald Docker log setting which may break EFK on Fedora nodes,"Hello,

using latest (master) [Vagrant Ansible](https://github.com/kubernetes/contrib/tree/master/ansible/vagrant) cluster installation and following [Fedora via Ansible](https://kubernetes.io/docs/getting-started-guides/fedora/fedora_ansible_config/#architecture-of-the-cluster) documentation, I coudn't get containers logging into Kibana using Fedora 25 [boxes](https://atlas.hashicorp.com/fedora/boxes/25-cloud-base) used for Vagrant

I activated logging as per documentation
`cluster_logging: true`
and pods are correctly deployed, but on Fedora kubernetes nodes the problem seems the same mentioned in this other [issue](https://github.com/kubernetes/kubernetes/issues/39225#issuecomment-269588375)

/var/log/containers is empty without any symlinks to /var/lib/docker/containers/ due `--log-driver=journald` setting into /etc/sysconfig/docker:

`OPTIONS='--log-driver=journald'`

Thus containers are writing to system journal log and fluentd agent cannot send anything to Elasticsearch as per its configuration shipped within EFK containers.

Removing this option manually and restarting docker on each kubernetes nodes solved the issue, could be done rather by Ansible?",closed,False,2017-05-30 11:02:14,2018-02-23 12:43:51
contrib,loburm,https://github.com/kubernetes/contrib/pull/2618,https://api.github.com/repos/kubernetes/contrib/issues/2618,Update metric descriptors before pushing metrics.,"Currently prometheus-to-sd ignores description of metrics and as result
stackdriver autogenerates it. Now before pushing metrics it's going to
check if MetricDescriptor exists or has changed and update it if it
neccessary. This check is performed every hour.",closed,True,2017-05-30 13:10:25,2017-06-23 21:52:14
contrib,crassirostris,https://github.com/kubernetes/contrib/issues/2619,https://api.github.com/repos/kubernetes/contrib/issues/2619,[prometheus-to-sd] Cluster name field has a newline character,Here be Trim: https://github.com/kubernetes/contrib/blob/master/prometheus-to-sd/config/gce_config.go#L50,closed,False,2017-05-31 09:33:35,2018-08-09 22:31:37
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2620,https://api.github.com/repos/kubernetes/contrib/issues/2620,Update fluentd-gcp gems,"To address problem from https://github.com/kubernetes/kubernetes/pull/45892

/cc @piosz @igorpeshansky",closed,True,2017-05-31 11:24:01,2017-07-13 20:05:00
contrib,mborsz,https://github.com/kubernetes/contrib/pull/2621,https://api.github.com/repos/kubernetes/contrib/issues/2621,Implement whitelisted metric autodiscovery in prometheus-to-sd.,"Now if --auto-whitelist-metrics is provided (default false) and there
are no whitelisted metrics for component, prometheus-to-sd will query
Stackdriver and fetch them.

The known whitelisted metrics are refreshed each
--auto-whitelist-metrics-resolution (default 10 minutes).",closed,True,2017-05-31 12:23:56,2017-05-31 15:32:51
contrib,loburm,https://github.com/kubernetes/contrib/pull/2622,https://api.github.com/repos/kubernetes/contrib/issues/2622,Remove leading and trailing whitespaces from cluster name.,"It may happen that GCE is going to return cluster-name metadata with
trailing or leading whitespaces, to avoid errors remove them from the
final configuration.

fix #2619",closed,True,2017-05-31 13:25:42,2017-06-12 16:07:42
contrib,x13n,https://github.com/kubernetes/contrib/pull/2623,https://api.github.com/repos/kubernetes/contrib/issues/2623,Rewrite addon-resizer nanny,"There will be a single estimator class, providing two ranges: acceptable
and recommended range. As long as current pod requirements and limits fall into
acceptable range, nothing happens. Once either limits or requirements
fall out of acceptable range, they are both updated to lower (when
upscaling) or higher (when downscaling) end of recommended range. This
approach prevents flapping, which took place in previous implementation,
when cluster size oscillated around certain values.

More details: https://docs.google.com/a/google.com/document/d/1T0A7GHwNU_w6gpq_eCN166aRth6usbRvxgJJqSZeESU/edit?usp=sharing (shared with kubernetes-sig-instrumentation and kubernetes-sig-autoscaling)",closed,True,2017-05-31 14:27:11,2017-06-20 09:42:12
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2624,https://api.github.com/repos/kubernetes/contrib/issues/2624,Fake storage instead of storing events in watch,"@piosz @wojtek-t Could you please take a look? In case of compaction/lost history, re-list will emit an entry about lost events anyway ",closed,True,2017-05-31 16:27:49,2017-07-13 20:04:34
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2625,https://api.github.com/repos/kubernetes/contrib/issues/2625,Fix fluentd-gcp image,To keep fluentd 0.12,closed,True,2017-06-01 15:13:46,2017-06-01 20:38:03
contrib,widgetpl,https://github.com/kubernetes/contrib/pull/2626,https://api.github.com/repos/kubernetes/contrib/issues/2626,Extra node parameter,"Added node selector `serverNode` and `clientNode` to be able to test from specific nodes.
They are based on `kubernetes.io/hostname` label.",closed,True,2017-06-05 06:32:20,2018-02-24 11:05:49
contrib,wzhliang,https://github.com/kubernetes/contrib/issues/2627,https://api.github.com/repos/kubernetes/contrib/issues/2627,proxy-to-service image build error,"`docker build` gives the following error:

```
/bin/sh: can't open '/etc/rc.common'
/bin/sh: can't open '/etc/rc.common'
//usr/lib/opkg/info/socat.postinst: line 4: /etc/init.d/: Permission denied
//usr/lib/opkg/info/socat.postinst: line 4: /etc/init.d/: Permission denied
```",closed,False,2017-06-07 08:05:51,2018-02-24 15:09:50
contrib,yguo0905,https://github.com/kubernetes/contrib/pull/2628,https://api.github.com/repos/kubernetes/contrib/issues/2628,Refactor node-perf-dash to read perf and time series data from json files,"Fixes the node-dash-perf issue in https://github.com/kubernetes/kubernetes/issues/44003, and refactors the code.

- Rename `data.go` to `types.go` and move the global variables from `types.go` to `node-perf-dash.go` so that `types.go` only contains types.
- Move the parsing logic from `downloader.go` to `parser.go`.
- `parser.go` now reads the perf and time series data from json files, generated in  https://github.com/kubernetes/kubernetes/pull/47260.

/assign @Random-Liu ",closed,True,2017-06-09 23:37:30,2017-06-15 23:11:43
contrib,rmmh,https://github.com/kubernetes/contrib/pull/2629,https://api.github.com/repos/kubernetes/contrib/issues/2629,Make echoheaders nginx container escape &<> to prevent XSS.,"Tested locally by doing `make container; docker run --rm -p 8080:8080 gcr.io/google_containers/echoserver:1.4` and then `curl 'localhost:8080/xss-<&>'`.

Intentionally not bumping the image tag because that would require multiple patches against old branches to pick up the newer version, and this change is relatively small and safe!",closed,True,2017-06-10 02:14:25,2017-06-15 00:27:02
contrib,nafets227,https://github.com/kubernetes/contrib/issues/2630,https://api.github.com/repos/kubernetes/contrib/issues/2630,[keepalived-vip] forwarding method DR does not work,"When trying to use forwarding method DR Log says ist not allowed:
`W0611 20:54:02.951613       1 controller.go:176] invalid LVS method. Only NAT and DR are supported: DR`

If I am not wrong, this is because of Line 45 of utils.go that only Supports NAT and not DR:
`lvsRegex      = regexp.MustCompile(`NAT`)`

But I am not sure if there are no other side effects...",closed,False,2017-06-11 20:59:05,2017-07-25 18:19:10
contrib,RTBathula,https://github.com/kubernetes/contrib/issues/2631,https://api.github.com/repos/kubernetes/contrib/issues/2631,"ingress.kubernetes.io/affinity: ""cookie"" is not working","Hi,
I followed this tutorial https://github.com/kubernetes/ingress/tree/master/examples/affinity/cookie/nginx
And yes I have created and running one backend(one pod and service). Then I have added one ingress type
and mentioned like this 
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: haproxy-ingress
  annotations:     
    kubernetes.io/ingress.class: ""gce""  
    ingress.kubernetes.io/affinity: ""cookie""
    ingress.kubernetes.io/session-cookie-name: ""route""
    ingress.kubernetes.io/session-cookie-hash: ""sha1""
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: red //My service name 
          servicePort: 80
```

But as i am doing curl, I am not seeing cookie is set in headers.

",closed,False,2017-06-12 19:55:36,2018-04-21 20:43:51
contrib,carlosedp,https://github.com/kubernetes/contrib/issues/2632,https://api.github.com/repos/kubernetes/contrib/issues/2632,[keepalived-vip] Keepalived does not forward the traffic to correct port set on service,"I have Traefik ingress controller deployed with a service on port 30080, When I create a keepalived-vip instance to forward the traffic from port 80 to 30080, it does not follow my config. Here follows my configurations:

Traefik service:

```
[root@master-1 keepalived-vip]# kubectl get service -n kube-system traefik-ingress-controller -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2017-06-12T20:38:18Z
  labels:
    k8s-app: traefik-ingress-lb
  name: traefik-ingress-controller
  namespace: kube-system
  resourceVersion: ""351177""
  selfLink: /api/v1/namespaces/kube-system/services/traefik-ingress-controller
  uid: 10fc54c6-4faf-11e7-9435-fa163e5e86fb
spec:
  clusterIP: 10.96.118.244
  ports:
  - name: http
    nodePort: 30080
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    k8s-app: traefik-ingress-lb
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
```

My ConfigMap:

```
apiVersion: v1
data:
  10.178.11.241: kube-system/traefik-ingress-controller
kind: ConfigMap
metadata:
  name: vip-configmap
  namespace: default
```

Generated config from keepalived.conf

```
[root@master-1 keepalived-vip]# kubectl exec kube-keepalived-vip-6lx5w cat /etc/keepalived/keepalived.conf

global_defs {
  vrrp_version 3
  vrrp_iptables KUBE-KEEPALIVED-VIP
}

vrrp_script chk_haproxy {
  script ""/haproxy-check.sh""
  interval 1
}

vrrp_instance vips {
  state BACKUP
  interface eth0
  virtual_router_id 50
  priority 100
  nopreempt
  advert_int 1

  track_interface {
    eth0
  }

  virtual_ipaddress {
    10.178.11.241
  }
}

# Service: kube-system-traefik-ingress-controller
virtual_server 10.178.11.241 80 {
  delay_loop 5
  lvs_sched wlc
  lvs_method NAT
  persistence_timeout 1800
  protocol TCP

  real_server 10.178.11.236 80 {
    weight 1
    TCP_CHECK {
      connect_port 80
      connect_timeout 3
    }
  }

  real_server 10.178.11.237 80 {
    weight 1
    TCP_CHECK {
      connect_port 80
      connect_timeout 3
    }
  }

  real_server 10.178.11.238 80 {
    weight 1
    TCP_CHECK {
      connect_port 80
      connect_timeout 3
    }
  }

} 
```

I want to access my ingress in port 80 and have keepalived forward the traffic to my nodes in port 30080.",closed,False,2017-06-12 20:49:30,2018-02-22 12:05:07
contrib,ixdy,https://github.com/kubernetes/contrib/pull/2633,https://api.github.com/repos/kubernetes/contrib/issues/2633,Bump images that use debian-base image,"I recently updated the `debian-base-*` image off upstream with fixes for a number of CVEs.
The downstream images now need to be updated, which I've done in this PR, bumping the patch version for each.

I haven't yet pushed any of these images.
After doing so, I'll need to follow up with additional changes in the manifests.

x-ref https://github.com/kubernetes/kubernetes/issues/47386
cc @Q-Lee @crassirostris ",closed,True,2017-06-13 01:25:22,2017-06-13 18:39:29
contrib,alexef,https://github.com/kubernetes/contrib/pull/2634,https://api.github.com/repos/kubernetes/contrib/issues/2634,Fix formatting of README.md,make it readable in github ,closed,True,2017-06-14 13:13:49,2017-06-14 13:27:31
contrib,alexef,https://github.com/kubernetes/contrib/pull/2635,https://api.github.com/repos/kubernetes/contrib/issues/2635,Fix formatting of README.md,to see it in github,closed,True,2017-06-14 13:27:57,2017-06-19 22:23:09
contrib,thilinapiy,https://github.com/kubernetes/contrib/pull/2636,https://api.github.com/repos/kubernetes/contrib/issues/2636,ZooKeeper README.md file markdown format update.,,closed,True,2017-06-14 14:20:07,2017-06-14 14:20:39
contrib,carlosedp,https://github.com/kubernetes/contrib/issues/2637,https://api.github.com/repos/kubernetes/contrib/issues/2637,[keepalived-vip] Traffic is not forwarded to service when pod not running on MASTER,"I have a fresh deployment of K8s 1.6.3 using Weave net on 1 master and 2 minions. I've set a Traefik ingress controller using a deployment with 2 replicas. The replicas are currently running on both minions.

Then I deployed keepalived-vip and it started a pod on each of my 3 nodes (master+minions). 

The problem is that when I'm on the master node, I can `curl` any of the three IP addresses and get a http response but when I'm on any other machine in the same network, I can't `curl` the VIP. The only ones that answer are the two nodes that have traefik controller listening on port 80.

I checked around and this only occurs when the keepalived MASTER is the master node which don't have the controller pod running (also there's nothing listening on port 80) but when I killed the keepalived processes in my master forcing one of the nodes to assume the VIP, `curl` started working perfectly on any machine.

###Endpoints

```
NAMESPACE     NAME                         ENDPOINTS                               AGE
default       kubernetes                   10.178.11.236:6443                      4h
kube-system   heapster                     10.46.0.2:8082                          4h
kube-system   kube-controller-manager      <none>                                  4h
kube-system   kube-dns                     10.32.0.2:53,10.32.0.2:53               4h
kube-system   kube-scheduler               <none>                                  4h
kube-system   kubernetes-dashboard         10.46.0.1:9090                          4h
kube-system   monitoring-influxdb          10.46.0.3:8086                          4h
kube-system   tiller-deploy                10.46.0.4:44134                         4h
kube-system   traefik-ingress-controller   10.178.11.237:80,10.178.11.238:80       4h
kube-system   traefik-web-ui               10.178.11.237:8081,10.178.11.238:8081   4h
```

### Services
```
NAMESPACE     NAME                         CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
default       kubernetes                   10.96.0.1        <none>        443/TCP         4h
kube-system   heapster                     10.98.175.4      <none>        80/TCP          4h
kube-system   kube-dns                     10.96.0.10       <none>        53/UDP,53/TCP   4h
kube-system   kubernetes-dashboard         10.107.153.235   <none>        80/TCP          4h
kube-system   monitoring-influxdb          10.105.161.7     <none>        8086/TCP        4h
kube-system   tiller-deploy                10.102.54.84     <none>        44134/TCP       4h
kube-system   traefik-ingress-controller   10.110.69.223    <nodes>       80:30080/TCP    4h
kube-system   traefik-web-ui               10.97.196.245    <none>        8081/TCP        4h
```



###Logs from Master node

```
[root@master-1 ~]# kdesc keepal 1
Pod: kube-keepalived-vip-c6fv0

Name:           kube-keepalived-vip-c6fv0
Namespace:      default
Node:           master-1/10.178.11.236
Start Time:     Wed, 14 Jun 2017 14:11:40 +0000
Labels:         name=kube-keepalived-vip
                pod-template-generation=1
Annotations:    kubernetes.io/created-by={""kind"":""SerializedReference"",""apiVersion"":""v1"",""reference"":{""kind"":""DaemonSet"",""namespace"":""default"",""name"":""kube-keepalived-vip"",""uid"":""62e9a9c6-510b-11e7-9be9-fa163e5e86fb""...
Status:         Running
IP:             10.178.11.236
Controllers:    DaemonSet/kube-keepalived-vip
Containers:
  kube-keepalived-vip:
    Container ID:       docker://1b09cfc8e4d5a71b7f938bdca9cca905d894e78e06ee3672b3fcf8aef6ccf15b
    Image:              aledbf/kube-keepalived-vip:0.14
    Image ID:           docker-pullable://docker.io/aledbf/kube-keepalived-vip@sha256:7e451b871a13e931402f91586f96b267a7a8eaa8f7c27a46eb5e82237a029cf8
    Port:
    Args:
      --services-configmap=default/vip-configmap
    State:              Running
      Started:          Wed, 14 Jun 2017 14:12:08 +0000
    Last State:         Terminated
      Reason:           Error
      Exit Code:        255
      Started:          Mon, 01 Jan 0001 00:00:00 +0000
      Finished:         Wed, 14 Jun 2017 14:11:51 +0000
    Ready:              True
    Restart Count:      2
    Environment:
      POD_NAME:         kube-keepalived-vip-c6fv0 (v1:metadata.name)
      POD_NAMESPACE:    default (v1:metadata.namespace)
    Mounts:
      /dev from dev (rw)
      /lib/modules from modules (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-keepalived-vip-token-n9tdh (ro)
Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
Volumes:
  modules:
    Type:       HostPath (bare host directory volume)
    Path:       /lib/modules
  dev:
    Type:       HostPath (bare host directory volume)
    Path:       /dev
  kube-keepalived-vip-token-n9tdh:
    Type:       Secret (a volume populated by a Secret)
    SecretName: kube-keepalived-vip-token-n9tdh
    Optional:   false
QoS Class:      BestEffort
Node-Selectors: <none>
Tolerations:    node.alpha.kubernetes.io/notReady=:Exists:NoExecute
                node.alpha.kubernetes.io/unreachable=:Exists:NoExecute
Events:         <none> 


Pod Logs:

Starting Keepalived v1.3.5 (unknown)
Unable to resolve default script username 'keepalived_script' - ignoring
Opening file '/etc/keepalived/keepalived.conf'.
Starting Healthcheck child process, pid=17
Starting VRRP child process, pid=18
Initializing ipvs
Opening file '/etc/keepalived/keepalived.conf'.
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Opening file '/etc/keepalived/keepalived.conf'.
Using LinkWatch kernel netlink reflector...
I0614 14:12:08.951919       1 keepalived.go:159] reloading keepalived
Opening file '/etc/keepalived/keepalived.conf'.
Got SIGHUP, reloading checker configuration
Initializing ipvs
Opening file '/etc/keepalived/keepalived.conf'.
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Opening file '/etc/keepalived/keepalived.conf'.
Activating healthchecker for service [10.178.11.241]:80
Activating healthchecker for service [10.178.11.241]:80
Using LinkWatch kernel netlink reflector...
VRRP_Instance(vips) Entering BACKUP STATE
VRRP_Instance(vips) Transition to MASTER STATE
VRRP_Instance(vips) Entering MASTER STATE
VRRP_Instance(vips) using locally configured advertisement interval (1000 milli-sec) 


[root@master-1 ~]#  kubectl exec kube-keepalived-vip-c6fv0 cat /etc/keepalived/keepalived.conf

global_defs {
  vrrp_version 3
  vrrp_iptables KUBE-KEEPALIVED-VIP
}

vrrp_script chk_haproxy {
  script ""/haproxy-check.sh""
  interval 1
}

vrrp_instance vips {
  state BACKUP
  interface eth0
  virtual_router_id 50
  priority 100
  nopreempt
  advert_int 1

  track_interface {
    eth0
  }
  virtual_ipaddress {
    10.178.11.241
  }
}

# Service: kube-system-traefik-ingress-controller
virtual_server 10.178.11.241 80 {
  delay_loop 5
  lvs_sched wlc
  lvs_method NAT
  persistence_timeout 1800
  protocol TCP

  real_server 10.178.11.237 80 {
    weight 1
    TCP_CHECK {
      connect_port 80
      connect_timeout 3
    }
  }

  real_server 10.178.11.238 80 {
    weight 1
    TCP_CHECK {
      connect_port 80
      connect_timeout 3
    }
  }
} 


[root@master-1 ~]# netstat -anpl |grep 80
tcp        0      0 127.0.0.1:2380          0.0.0.0:*               LISTEN      4677/etcd
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      800/sshd
tcp        0      0 127.0.0.1:40680         127.0.0.1:2379          ESTABLISHED 5387/kube-apiserver
tcp        0      0 127.0.0.1:40180         127.0.0.1:2379          ESTABLISHED 4677/etcd
tcp        0      0 127.0.0.1:2379          127.0.0.1:40680         ESTABLISHED 4677/etcd
tcp        0      0 127.0.0.1:2379          127.0.0.1:40180         ESTABLISHED 4677/etcd
tcp6       0      0 :::22                   :::*                    LISTEN      800/sshd
tcp6       0      0 :::30080                :::*                    LISTEN      6171/kube-proxy
raw        0      0 0.0.0.0:112             0.0.0.0:*               7           13380/keepalived
raw        0      0 0.0.0.0:112             0.0.0.0:*               7           13380/keepalived
unix  2      [ ACC ]     STREAM     LISTENING     35880    5141/kubelet         /var/run/dockershim.sock
unix  2      [ ]         DGRAM                    7804     1/systemd            /run/systemd/notify
unix  2      [ ]         DGRAM                    7806     1/systemd            /run/systemd/cgroups-agent
unix  3      [ ]         STREAM     CONNECTED     14803    514/dbus-daemon      /var/run/dbus/system_bus_socket
unix  3      [ ]         STREAM     CONNECTED     14802    505/polkitd
unix  3      [ ]         STREAM     CONNECTED     34803    5141/kubelet
unix  2      [ ]         DGRAM                    14805    505/polkitd
unix  3      [ ]         STREAM     CONNECTED     14680    509/systemd-logind    


[root@master-1 ~]# ps aux|grep keepalived
root      4203  0.0  0.0 112648   964 pts/0    S+   18:32   0:00 grep --color=auto keepalived
root     13347  0.1  0.4  36332 17568 ?        Ssl  14:12   0:18 /kube-keepalived-vip --services-configmap=default/vip-configmap
root     13378  0.0  0.0  69372  3032 ?        S    14:12   0:00 keepalived --dont-fork --log-console --release-vips --pid /keepalived.pid
root     13379  0.0  0.0  71568  2584 ?        S    14:12   0:01 keepalived --dont-fork --log-console --release-vips --pid /keepalived.pid
root     13380  0.0  0.0  75944  2316 ?        S    14:12   0:01 keepalived --dont-fork --log-console --release-vips --pid /keepalived.pid
[root@master-1 ~]# kill -9 13347 13378 13379 13380 

```

### The Minion assumed VIP master

```
[root@master-1 ~]# klog keepali 3
Pod: kube-keepalived-vip-t579n

I0614 14:12:58.361202       1 main.go:197] Creating API server client for https://10.96.0.1:443
I0614 14:12:58.371511       1 keepalived.go:186] cleaning ipvs configuration
I0614 14:12:58.534971       1 main.go:121] starting LVS configuration
I0614 14:12:58.600283       1 utils.go:150] network interfaces: [{Index:2 MTU:1500 Name:eth0 HardwareAddr:fa:16:3e:77:46:55 Flags:up|broadcast|multicast} {Index:4 MTU:1376 Name:datapath HardwareAddr:6a:b4:db:11:74:81 Flags:up|broadcast|multicast} {Index:6 MTU:1376 Name:weave HardwareAddr:ca:28:03:56:24:c2 Flags:up|broadcast|multicast} {Index:7 MTU:1500 Name:dummy0 HardwareAddr:0a:c9:e4:8a:35:4c Flags:broadcast} {Index:11 MTU:65470 Name:vxlan-6784 HardwareAddr:aa:69:66:e9:9f:48 Flags:up|broadcast|multicast}]
I0614 14:12:58.604461       1 main.go:131] starting keepalived to announce VIPs
Starting Keepalived v1.3.5 (unknown)
Unable to resolve default script username 'keepalived_script' - ignoring
Opening file '/etc/keepalived/keepalived.conf'.
Starting Healthcheck child process, pid=18
Initializing ipvs
Starting VRRP child process, pid=19
Opening file '/etc/keepalived/keepalived.conf'.
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Opening file '/etc/keepalived/keepalived.conf'.
Using LinkWatch kernel netlink reflector...
I0614 14:12:58.709928       1 keepalived.go:159] reloading keepalived
Opening file '/etc/keepalived/keepalived.conf'.
Got SIGHUP, reloading checker configuration
Initializing ipvs
Opening file '/etc/keepalived/keepalived.conf'.
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Opening file '/etc/keepalived/keepalived.conf'.
Using LinkWatch kernel netlink reflector...
VRRP_Instance(vips) Entering BACKUP STATE
Activating healthchecker for service [10.178.11.241]:80
Activating healthchecker for service [10.178.11.241]:80
VRRP_Instance(vips) Transition to MASTER STATE
VRRP_Instance(vips) Entering MASTER STATE
VRRP_Instance(vips) using locally configured advertisement interval (1000 milli-sec) 
```
",closed,False,2017-06-14 18:43:49,2017-06-14 20:08:44
contrib,thilinapiy,https://github.com/kubernetes/contrib/pull/2638,https://api.github.com/repos/kubernetes/contrib/issues/2638,Update markdown formatting in zookeeper readme.,,closed,True,2017-06-15 07:10:30,2017-08-10 01:05:05
contrib,mwielgus,https://github.com/kubernetes/contrib/pull/2639,https://api.github.com/repos/kubernetes/contrib/issues/2639,Remove addon resizer code after migrating it to kubernetes/autoscaler,cc: @Q-Lee @wojtek-t @gmarek @piosz @x13n @MaciekPytel,closed,True,2017-06-16 13:07:04,2017-07-21 13:22:31
contrib,ixdy,https://github.com/kubernetes/contrib/pull/2640,https://api.github.com/repos/kubernetes/contrib/issues/2640,Bump images that use debian-base image again,"This is a rehash of #2633. After pushing the images and updating Kubernetes, we discovered that the fluentd container was crashing, as the updated `debian-base` image had unexpectedly removed libcap2 (kubernetes/kubernetes#47600).

I've since pushed a new `debian-base` image (kubernetes/kubernetes#47616). I'm guessing that only the fluentd image needs to be bumped, but I'm going to update all of them out of an abundance of caution.

/cc @Q-Lee @crassirostris @timstclair @eparis",closed,True,2017-06-16 20:07:11,2017-06-21 20:13:27
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2641,https://api.github.com/repos/kubernetes/contrib/issues/2641,[event-exporter] Fix event exporter spam,"Watch framework periodically resets the connections (~one every 15 minutes) which causes a lot of messages about lost events in Stackdriver. Though this change is raising the chances that some events will be lost without a note, the probability of this event is extremely low (it requires to lose whole apiserver event history, e.g. in case of a really long network outage). Note, that in case container is restarted, the message will still be generated.

/cc @piosz ",closed,True,2017-06-22 14:34:41,2017-06-22 15:00:50
contrib,Pensu,https://github.com/kubernetes/contrib/pull/2642,https://api.github.com/repos/kubernetes/contrib/issues/2642,Enabling Multi-arch support in Ansible scripts,"This PR enables multi-arch support for ansible scripts when github-release option is chosen. 

 - By default for x86_64 machines the packages have amd64 as arch, so we need to set ansible_architecture as amd64 when it's x86_64. 

 - Flannel new version has a different directory structure compared to older version, so had to change the version too with flannel.
",closed,True,2017-06-27 06:50:09,2017-07-31 11:19:08
contrib,venezia,https://github.com/kubernetes/contrib/issues/2643,https://api.github.com/repos/kubernetes/contrib/issues/2643,"pets/peer-finder could have a better default domain, messaging","Right now, peer-finder will presume `cluster.local` as the domain name of the kubernetes cluster unless manually overridden.  This, in conjunction with some popular helm charts (including mongodb-replicaset) can lead to an odd circumstance where peer-finder will seemingly hang forever without any message to the user if the domain name is different.

Given that peer-finder is relying heavily on DNS, it would seem reasonable to reference `/etc/resolv.conf` to better guess the domain name if not supplied.

Furthermore, instead of just looping without any message to the user while the presumed host entry is not in the service's peer list, it would be nice to notify the user what peers are there and what the presumed host entry is.

Peer-entry is currently used in an init container in [kubernetes' chart repo](https://github.com/kubernetes/charts/tree/master/stable/mongodb-replicaset) - and init containers are a bit of a pain to debug - so the more information provided to the user the better.",closed,False,2017-06-27 18:43:23,2018-02-28 21:50:50
contrib,venezia,https://github.com/kubernetes/contrib/pull/2644,https://api.github.com/repos/kubernetes/contrib/issues/2644,"Improve domain default, add messaging when in loop to help anyone debugging",This is a proposed patch to peer-finder per issue #2643 ,closed,True,2017-06-27 18:44:26,2017-07-17 16:35:04
contrib,MikeSpreitzer,https://github.com/kubernetes/contrib/pull/2645,https://api.github.com/repos/kubernetes/contrib/issues/2645,Fix cni-plugins/to_docker/bin/c2d: return code should be an int,"It was a string.

Resolves #2646 ",closed,True,2017-06-28 01:30:37,2018-02-28 22:51:49
contrib,MikeSpreitzer,https://github.com/kubernetes/contrib/issues/2646,https://api.github.com/repos/kubernetes/contrib/issues/2646,Return code in cni-plugins/to_docker should be an int,"Currently a string is being returned instead of an int, in the error case.",closed,False,2017-06-28 01:31:50,2018-02-28 21:50:50
contrib,loburm,https://github.com/kubernetes/contrib/pull/2647,https://api.github.com/repos/kubernetes/contrib/issues/2647,Replace deprecated oauth2.NoContext by allowed one.,"oauth2.NoContext is deprecated. context.Backgound() should be used
instead.",closed,True,2017-06-28 08:45:56,2017-06-29 13:15:19
contrib,JordanP,https://github.com/kubernetes/contrib/pull/2648,https://api.github.com/repos/kubernetes/contrib/issues/2648,examples/sysctl/change-proc-values-rc.yaml: fix wrong example command,,closed,True,2017-06-29 09:46:51,2018-03-15 14:35:46
contrib,loburm,https://github.com/kubernetes/contrib/pull/2649,https://api.github.com/repos/kubernetes/contrib/issues/2649,Added few more flags with information about pod.,"Add flag pod-id and namespace-id that are used to create correct
monitored resource object for the stackdriver.

Additionally some small refactoring to improve readability.",closed,True,2017-06-30 08:43:44,2017-07-05 11:07:06
contrib,crassirostris,https://github.com/kubernetes/contrib/issues/2650,https://api.github.com/repos/kubernetes/contrib/issues/2650,[kubelet-to-gcm] Glog configured incorrectly and logging is too spammy,"There are messages in the kubelet-to-gcm logs, like

```
ERROR: logging before flag.Parse: I0630 10:01:00.590934 1 poll.go:63] Successfully wrote TimeSeries data for kubelet to GCM v3 API.
```

which show 2 problems:

* Incorrectly configured glog
* Why would it even write something like that?",closed,False,2017-06-30 10:13:48,2017-06-30 18:20:42
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2651,https://api.github.com/repos/kubernetes/contrib/issues/2651,Fix logging in kubelet-to-gcm,Fixes https://github.com/kubernetes/contrib/issues/2650,closed,True,2017-06-30 10:14:23,2017-06-30 18:20:42
contrib,loburm,https://github.com/kubernetes/contrib/pull/2652,https://api.github.com/repos/kubernetes/contrib/issues/2652,Update labels of metric descriptors if they change,"This PR includes few minor changes that fixes issues in the prometheus-to-sd component:

1. Component is not trying to update metric descriptor if this metric was not whitelisted.
2. Updates metric descriptor if new label was added.
3. Doesn't send broken metrics to the stackdriver (for example when name is longer than 100 characters or number of labels bigger than 10)
4. If sending of batch of timeserieses to the Stackdriver fails, correct number of sent timeserieses is logged.
",closed,True,2017-07-05 12:09:22,2017-07-11 09:37:01
contrib,Guirish-Salgaonkar,https://github.com/kubernetes/contrib/issues/2653,https://api.github.com/repos/kubernetes/contrib/issues/2653,exechealth-s390x pod fails on kubernetes local cluster ,"I built exechealthz-s390x:9098dc6-dirty image locally from master, later I used this image for testing kube-dns and kube-dnsmasq add-on features with Kubernetes local cluster setup. I observed that exechealth pod was failing to start. On debugging the exechealth docker image for s390x, I found that it is using **s390x/busybox** image as its BASEIMAGE in which nobody user is associated with **nogroup** group instead of **nobody** group as expected in latest docker file for exechealth. So I modified the BASEIMAGE for s390x to **s390x/alpine** and rebuilt exechealth. On using the latest built image of exechealth everything is working as expected. 

I will be raising a PR to consider the BASEIMAGE change for s390x soon. So after my PR is it possible for you'll to build exechealth-s390x docker image and push it to google repository? So that we will have a working exechealth docker image for s390x.",closed,False,2017-07-05 12:59:17,2017-07-06 12:52:22
contrib,loburm,https://github.com/kubernetes/contrib/issues/2654,https://api.github.com/repos/kubernetes/contrib/issues/2654,Support of Summary metric type in the prometheus-to-sd,"Currently prometheus-to-sd supports only Gauge, Counter and Histogram. We still missing support of Summary. Considering that summaries are not aggregatable I think it should map to Gauge, Distribution type in stackdriver.

/assign @loburm",closed,False,2017-07-06 09:50:02,2018-03-01 09:01:50
contrib,loburm,https://github.com/kubernetes/contrib/issues/2655,https://api.github.com/repos/kubernetes/contrib/issues/2655,Support metric types with floating point in prometheus-to-sd,"If we have Gauge of Counter type then we automatically assign to it int64 value type. Unfortunately there is no value type in the prometheus format, so we need to try to determine it base on the actual metric numbers.

/assign @loburm",closed,False,2017-07-06 09:50:36,2017-10-31 15:05:39
contrib,discostur,https://github.com/kubernetes/contrib/issues/2656,https://api.github.com/repos/kubernetes/contrib/issues/2656,[keepalived-vip] No real Service Loadbalancing,"Hi,

i've 3 k8s worker nodes and 3 pods running one on each node. I have defined a service and a keepalived-vip, which routes the traffic to the service:

keepalived-vip -> k8s service -> k8s pods

I did some tests and figured out, that the whole traffic which comes through the keepalived-vip only hits one pod. There is no loadbalancing across each of the 3 pods, only one does all the work.

Is this an expected behaviour?

Thanks
Greets Kilian",closed,False,2017-07-06 12:55:17,2018-06-03 20:36:18
contrib,f0,https://github.com/kubernetes/contrib/issues/2657,https://api.github.com/repos/kubernetes/contrib/issues/2657,[keepalived-vip] Can not use service in another Namespace,"Hi,

I followed the Readme (with RBAC) and  tested the default manifests (keepalived and echoheaders in the default namespace), it does work.

If i try to use a Service (echoheaders) in another Namespace , keepalived complains that no VIP is avaible and the pod's are crashing.

My Cluster is 1.6.2 with RBAC 

any Ideas? How can i Debug this ? 

My configmap
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: vip-configmap
data:
  10.62.3.168: testecho/echoheaders
```


regards f0",closed,False,2017-07-06 17:16:37,2018-01-01 10:33:18
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/2658,https://api.github.com/repos/kubernetes/contrib/issues/2658,[ansible] swap out deprecated --config argument,We're using the deprecated (https://github.com/kubernetes/kubernetes/pull/40048/) `--confg` argument to pass the manifest dir. this fixes that. Tested on kube 1.6.4 and kube 1.5.3.,closed,True,2017-07-07 20:36:16,2017-07-08 11:06:10
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2659,https://api.github.com/repos/kubernetes/contrib/issues/2659,set default cgroup driver to systemd,Kubernetes 1.6.7 's kubelet does not start without it properly.,closed,True,2017-07-08 12:38:47,2017-07-08 12:50:43
contrib,vboginskey,https://github.com/kubernetes/contrib/pull/2660,https://api.github.com/repos/kubernetes/contrib/issues/2660,Add glog flags to rescheduler,"This PR adds flags from the `glog` package to `rescheduler`. This enables passing `--logtostderr=true` to rescheduler in order to have Docker gather the logs, as opposed to logging them to the filesystem inside the container.

Fixes https://github.com/kubernetes/contrib/issues/2518.",closed,True,2017-07-08 22:49:37,2017-07-14 11:24:08
contrib,guirish,https://github.com/kubernetes/contrib/issues/2661,https://api.github.com/repos/kubernetes/contrib/issues/2661,exechealth-s390x pod fails to start on kubernetes local cluster,"I built **exechealthz-s390x:9098dc6-dirty** image locally from master, later I used this image for testing kube-dns and kube-dnsmasq add-on features with Kubernetes local cluster setup. I observed that exechealth pod was failing to start. On debugging the exechealth docker image for s390x, I found that it is using **s390x/busybox** image as its BASEIMAGE in which nobody user is associated with **nogroup** group instead of **nobody** group as expected in latest docker file for exechealth. So I modified the BASEIMAGE for s390x to **s390x/alpine** and rebuilt exechealth. On using the latest built image of exechealth everything is working as expected.

I will be raising a PR to consider the BASEIMAGE change for s390x soon. So after my PR is it possible for you'll to build exechealth-s390x docker image and push it to google repository? So that we will have a working exechealth docker image for s390x.",closed,False,2017-07-10 06:56:57,2017-07-14 09:10:25
contrib,gytisgreitai,https://github.com/kubernetes/contrib/pull/2662,https://api.github.com/repos/kubernetes/contrib/issues/2662,use publicly available event-exporter image,Change url in examples from private `google.com/vmik-k8s-testing-0` to public `gcr.io/google-containers`,closed,True,2017-07-10 08:14:31,2017-07-13 08:09:24
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/2663,https://api.github.com/repos/kubernetes/contrib/issues/2663,set default cgroup driver to systemd,this addresses the issue that #2659  was meant to address -- that env default was being overwritten here,closed,True,2017-07-10 19:44:28,2017-07-10 19:56:08
contrib,dashpole,https://github.com/kubernetes/contrib/pull/2664,https://api.github.com/repos/kubernetes/contrib/issues/2664,Implement ListFilesInBuild,"In order to use the node-perf-dash on local runs, it needs to be able to find local files.   A previous refactor left the ListFilesInBuild function with a TODO to implement it.
This PR implements the ListFilesInBuild function, which should allow using the node-perf-dash with local runs again.

I was not sure if this would follow symbolic links or not, so I added a max depth of 10 to avoid an infinite loop.

/assign @yguo0905 ",closed,True,2017-07-10 20:10:44,2017-07-13 19:07:03
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2665,https://api.github.com/repos/kubernetes/contrib/issues/2665,Remove Stackdriver-related projects,"There's a [new repo](github.com/GoogleCloudPlatform/k8s-stackdriver) for Stackdriver integration. Moving these project there is a part of deprecation contrib repo.

Fixes https://github.com/kubernetes/contrib/issues/2672

/cc @piosz ",closed,True,2017-07-10 20:44:05,2017-07-12 13:34:38
contrib,yan234280533,https://github.com/kubernetes/contrib/issues/2666,https://api.github.com/repos/kubernetes/contrib/issues/2666,Can it add an example which using DaemonSet to deploy logrotate pod in every nodes of the cluster?,"Can it add an example which using DaemonSet to deploy logrotate pod in every nodes of the cluster ?

So it can give the cluster enhanced ability to collector logs.",closed,False,2017-07-11 08:55:31,2017-07-11 15:15:17
contrib,fate-grand-order,https://github.com/kubernetes/contrib/pull/2667,https://api.github.com/repos/kubernetes/contrib/issues/2667,fix some typos for test-utils/utils/utils.go,,closed,True,2017-07-11 09:21:29,2017-07-11 09:53:46
contrib,ensonic,https://github.com/kubernetes/contrib/issues/2668,https://api.github.com/repos/kubernetes/contrib/issues/2668,ingress/controllers/nginx/examples/tls: wrong ingress.yaml,"In the section ""Finally create a tls Ingress rule:"", the ingress.yaml should be simplified:
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: foo
spec:
  tls:
  - secretName: foo-secret
  rules:
  - host: foo.bar.com
  backend:
    serviceName: echoheaders-x
    servicePort: 80
```

* the namespace does not need to be set
* there is no need to specify the host for spec.tls
* there is no need for the http part under spec.rules.host, even worse this way one won't have a default backend",closed,False,2017-07-11 10:30:03,2017-07-11 18:02:38
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2669,https://api.github.com/repos/kubernetes/contrib/issues/2669,Bump to kubernetes 1.5.2 release,Due to a new --cgroup-driver flag.,closed,True,2017-07-11 11:50:40,2017-07-11 12:36:15
contrib,guirish,https://github.com/kubernetes/contrib/pull/2670,https://api.github.com/repos/kubernetes/contrib/issues/2670,Changes: Updating BASEIMAGE for s390x from s390x/busybox to s390x/alp…,"I have create this PR to resolve below listed issue:
https://github.com/kubernetes/contrib/issues/2661

**Description**: Updated Exechealthz s390x BASEIMAGE to **s390x/alpine** which has **nobody** user in **nobody** group. **s390x/busybox** image has **nobody** user in **nogroup** group.",closed,True,2017-07-11 13:49:47,2017-07-17 17:12:17
contrib,ConradIrwin,https://github.com/kubernetes/contrib/issues/2671,https://api.github.com/repos/kubernetes/contrib/issues/2671,GCE Ingress controller cannot support healthchecks with different paths.,"This is a re-phrasing of https://github.com/jetstack/kube-lego/issues/27 as it seems to be a bug in the GCE controller.

1. Create a service with a readinessCheck with path `/ping`, and set up a GCE ingress.
2. Create another service with a readinessCheck with path `/pong` and add it to the GCE ingress.

The GCE ingress controller creates both health-checks with path`/ping`.",closed,False,2017-07-11 18:15:37,2018-03-01 15:07:49
contrib,crassirostris,https://github.com/kubernetes/contrib/issues/2672,https://api.github.com/repos/kubernetes/contrib/issues/2672,Remove Stackdriver-specific projects from the contrib repo,"To move one step closer to deprecating contrib repo, remove everything and leave stubs to redirect everyone to https://github.com/GoogleCloudPlatform/k8s-stackdriver",closed,False,2017-07-11 20:58:40,2017-07-13 08:12:53
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2673,https://api.github.com/repos/kubernetes/contrib/issues/2673,Stackdriver integration removal step 1: add information to README,"Fixes https://github.com/kubernetes/contrib/issues/2672

/cc @piosz ",closed,True,2017-07-12 13:39:06,2017-07-13 08:12:53
contrib,crassirostris,https://github.com/kubernetes/contrib/pull/2674,https://api.github.com/repos/kubernetes/contrib/issues/2674,Stackdriver integration removal step 2,"Fixes #2672 

/cc @piosz 

Should be merged after https://github.com/kubernetes/contrib/pull/2673",closed,True,2017-07-12 13:43:28,2017-07-13 19:05:16
contrib,MrHohn,https://github.com/kubernetes/contrib/pull/2675,https://api.github.com/repos/kubernetes/contrib/issues/2675,exec-healthz: Use alpine as base image on all platforms,"Fixes #2661. See discussions on #2670.

<s>Some others user:group configurations are imported from kubernetes/dns#31.</s> (This PR does not change user:group anymore.)

/assign @bowei @luxas 
@guirish Could you please build/test s390x image with this? I don't have the correct environment. Thanks :)",closed,True,2017-07-12 17:58:53,2017-07-17 17:10:50
contrib,luotian-git,https://github.com/kubernetes/contrib/issues/2676,https://api.github.com/repos/kubernetes/contrib/issues/2676,simple fanout failed  in ingress,"Use the example in https://kubernetes.io/docs/concepts/services-networking/ingress/#simple-fanout

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginxingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: kubenode1.inspur
      http:
        paths:
        - path: /nginx
          backend:
            serviceName: nginx
            servicePort: 80
        - path: /demoshow
          backend:
            serviceName: demoshowsub
            servicePort: 5000

But falied to generate the fanour rules:
[root@localhost kuby-ingress]# kubectl get ing
NAME           HOSTS              ADDRESS   PORTS     AGE
nginxingress   kubenode1.inspur             80        10s
[root@localhost kuby-ingress]#

In the document, it should be:
$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
test      -
          foo.bar.com
          /foo          s1:80
          /bar          s2:80


How can I generate loadbalance in nginx.conf 's upstream?
I'm using kubenetes 1.5.2  ingress 0.9.0


",closed,False,2017-07-13 09:45:48,2018-03-02 08:24:15
contrib,nafets227,https://github.com/kubernetes/contrib/pull/2677,https://api.github.com/repos/kubernetes/contrib/issues/2677,VIP only with no backend service ,"This Pull Requests enables to let run Keepalived with only attaching the VIP to the node. Then e.g. HAProxy can be run via Daemonset to make use of it.

I use it in order to Forward SMTP via Proxy protocol to postfix. With normal forwarding postfix will njever get the real IP address of the caller.",closed,True,2017-07-13 19:59:49,2017-07-24 21:50:30
contrib,gm42,https://github.com/kubernetes/contrib/issues/2678,https://api.github.com/repos/kubernetes/contrib/issues/2678,GKE: ingress does not get an endpoint address if allow-http is false and secret does not exist,"I am using the ingress.yaml provided in the guide here.
```yaml
# Copyright 2015 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: jenkins
  namespace: jenkins
  annotations:
    kubernetes.io/ingress.allow-http: ""false""
spec:
  tls:
  - secretName: mysecret
  backend:
    serviceName: jenkins-ui
    servicePort: 8080
```

## Problem
If the secret `mysecret` does not exist and `allow-http: false` is specified, no external endpoint is ever assigned.

## Expected behaviour
An error if `mysecret` does not exist.",closed,False,2017-07-14 11:40:56,2018-04-12 09:42:47
contrib,hchenxa,https://github.com/kubernetes/contrib/issues/2679,https://api.github.com/repos/kubernetes/contrib/issues/2679,rescheduler can not started in docker container,"I checkout the latest source code from master branch and try to build the rescheduler.

The build success and the rescheduler can be executed on my local host success like below:

```
root@hchenk8s4:~/GOPATH/src/k8s.io/contrib/rescheduler# make
rm -f rescheduler
go get github.com/tools/godep
GOOS=linux GOARCH=amd64 godep go build ./...
GOOS=linux GOARCH=amd64 godep go build -o rescheduler
```
And here is the output:
```
root@hchenk8s4:~/GOPATH/src/k8s.io/contrib/rescheduler# ./rescheduler --help
Usage of rescheduler: rescheduler --running-in-cluster=true:
      --alsologtostderr                  log to standard error as well as files
      --housekeeping-interval duration   How often rescheduler takes actions. (default 10s)
      --initial-delay duration           How long should rescheduler wait after start to make sure
		 all critical addons had a chance to start. (default 2m0s)
      --kube-api-content-type string     Content type of requests sent to apiserver. (default ""application/vnd.kubernetes.protobuf"")
      --listen-address string            Address to listen on for serving prometheus metrics (default ""localhost:9235"")
      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)
      --log_dir string                   If non-empty, write log files in this directory
      --logtostderr                      log to standard error instead of files (default true)
      --min-replica-count int            Minimum number or replicas that a replica set or replication controller should have to allow their pods deletion in scale down
      --pod-scheduled-timeout duration   How long should rescheduler wait for critical pod to be scheduled
		 after evicting pods to make a spot for it. (default 10m0s)
      --running-in-cluster               Optional, if this controller is running in a kubernetes cluster, use the
		 pod secrets for creating a Kubernetes client. (default true)
      --skip-nodes-with-local-storage    If true cluster autoscaler will never delete nodes with pods with local storage, e.g. EmptyDir or HostPath (default true)
      --skip-nodes-with-system-pods      If true cluster autoscaler will never delete nodes with pods from kube-system (except for DeamonSet or mirror pods) (default true)
      --stderrthreshold severity         logs at or above this threshold go to stderr (default 2)
      --system-namespace string          Namespace to watch for critical addons. (default ""kube-system"")
  -v, --v Level                          log level for V logs
      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging
```

Then I build the docker image with Dockerfile which provided from community, and then start the docker container with the docker image.
```
root@hchenk8s2:~# docker exec -it 056a88842bda sh
/ # sh -c /rescheduler --help
--help: line 1: /rescheduler: not found
```


Seems this issues was caused by:
the current rescheduler binary depend on some dynamic library which did not exist in docker containers:
```
root@hchenk8s4:~/GOPATH/src/k8s.io/contrib/rescheduler# ldd rescheduler
	linux-vdso.so.1 =>  (0x00007ffecd075000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fd85493c000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fd854573000)
	/lib64/ld-linux-x86-64.so.2 (0x0000555c4a9b2000)
```",closed,False,2017-07-17 08:17:53,2017-07-19 13:40:15
contrib,mkumatag,https://github.com/kubernetes/contrib/pull/2680,https://api.github.com/repos/kubernetes/contrib/issues/2680,Move test-webserver to kubernetes/test/images,"PR https://github.com/kubernetes/kubernetes/pull/48494 has been created to move to kubernetes/test/images

Fixes #2689 ",closed,True,2017-07-17 12:50:54,2017-07-21 16:34:03
contrib,foxish,https://github.com/kubernetes/contrib/pull/2681,https://api.github.com/repos/kubernetes/contrib/issues/2681,Update owners,"Remove bprashanth and add current maintainers
xref: https://github.com/kubernetes/contrib/pull/2644",closed,True,2017-07-17 16:00:38,2017-07-17 16:34:01
contrib,foxish,https://github.com/kubernetes/contrib/pull/2682,https://api.github.com/repos/kubernetes/contrib/issues/2682,"Add new top-level owner, remove stale","```
- bprashanth
+ foxish
```",closed,True,2017-07-17 16:47:14,2017-11-18 20:04:09
contrib,foxish,https://github.com/kubernetes/contrib/pull/2683,https://api.github.com/repos/kubernetes/contrib/issues/2683,Contrib cleanup,"pets/ has been largely superseded by statefulsets/
Removing stale and unmaintained examples, except peer-finder which will likely be maintained by @spiffxp and @venezia in the future.",closed,True,2017-07-17 16:53:03,2017-07-17 18:36:57
contrib,farahfa,https://github.com/kubernetes/contrib/issues/2684,https://api.github.com/repos/kubernetes/contrib/issues/2684,404 with Ubuntu upstart config files for Ansible playbook,"""master"" playbook is failing with the following errors:

```
TASK [master : Ubuntu | Get Upstart Config Files from Kubernetes repository] ***
failed: [10.96.3.85] (item=kube-apiserver) => {""dest"": ""/etc/init/kube-apiserver.conf"", ""failed"": true, ""item"": ""kube-apiserver"", ""msg"": ""Request failed"", ""response"": ""HTTP Error 404: Not Found"", ""state"": ""absent"", ""status_code"": 404, ""url"": ""https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/ubuntu/master/init_conf/kube-apiserver.conf""}
failed: [10.96.3.85] (item=kube-controller-manager) => {""dest"": ""/etc/init/kube-controller-manager.conf"", ""failed"": true, ""item"": ""kube-controller-manager"", ""msg"": ""Request failed"", ""response"": ""HTTP Error 404: Not Found"", ""state"": ""absent"", ""status_code"": 404, ""url"": ""https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/ubuntu/master/init_conf/kube-controller-manager.conf""}
failed: [10.96.3.85] (item=kube-scheduler) => {""dest"": ""/etc/init/kube-scheduler.conf"", ""failed"": true, ""item"": ""kube-scheduler"", ""msg"": ""Request failed"", ""response"": ""HTTP Error 404: Not Found"", ""state"": ""absent"", ""status_code"": 404, ""url"": ""https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/ubuntu/master/init_conf/kube-scheduler.conf""}
```",closed,False,2017-07-18 23:16:08,2018-03-02 04:20:17
contrib,kawych,https://github.com/kubernetes/contrib/pull/2685,https://api.github.com/repos/kubernetes/contrib/issues/2685,Link rescheduler statically,Link rescheduler statically to fix #2679.,closed,True,2017-07-19 12:50:14,2017-07-19 13:39:35
contrib,mkumatag,https://github.com/kubernetes/contrib/pull/2686,https://api.github.com/repos/kubernetes/contrib/issues/2686,Mutiarch addon-resizer image,This PR is for creating multi architecture image for addon-resizer,closed,True,2017-07-20 10:40:49,2017-07-24 03:44:16
contrib,guirish,https://github.com/kubernetes/contrib/issues/2687,https://api.github.com/repos/kubernetes/contrib/issues/2687,"Exechealthz pod though an error ""nslookup: can't resolve 'kubernetes.default.svc.cluster.local': Name does not resolve"" for ARCH=s390x","Hi,

I have setup a local cluster using local-up-cluster.sh to verify DNS functionality on s390x ARCH. 
I am observing a issue in the exechealthz pod, when I try to use it along with kube-dns and kube-dnsmasq pods and do a nslookup from an s390x/busybox pod. My DNS functionality is working as expected but I am observing errors in exechealthz container, with similar setup I do not observe any errors in exechealthz container on intel.

Any pointer on this will be very helpful.

Updating YML (dns-addon.yml) used to deploy the pods.
```
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: ""true""
    kubernetes.io/name: ""KubeDNS""
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.0.0.10
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP


---


apiVersion: v1
kind: ReplicationController
metadata:
  name: kube-dns-v20
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    version: v20
    kubernetes.io/cluster-service: ""true""
spec:
  replicas: 1
  selector:
    k8s-app: kube-dns
    version: v20
  template:
    metadata:
      labels:
        k8s-app: kube-dns
        version: v20
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
        scheduler.alpha.kubernetes.io/tolerations: '[{""key"":""CriticalAddonsOnly"", ""operator"":""Exists""}]'
    spec:
      containers:
      - name: kubedns
        image: gcr.io/google_containers/k8s-dns-kube-dns-s390x:1.14.1
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        livenessProbe:
          httpGet:
            path: /healthz-kubedns
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /readiness
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 3
          timeoutSeconds: 5
        args:
        - --domain=cluster.local.
        - --dns-port=10053
        ports:
        - containerPort: 10053
          name: dns-local
          protocol: UDP
        - containerPort: 10053
          name: dns-tcp-local
          protocol: TCP
      - name: dnsmasq
        image: gcr.io/google_containers/k8s-dns-dnsmasq-s390x:1.14.1
        livenessProbe:
          httpGet:
            path: /healthz-dnsmasq
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        args:
        - --cache-size=1000
        - --no-resolv
        - --server=127.0.0.1#10053
        - --log-facility=-
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
      - name: healthz
        image: gcr.io/google_containers/exechealthz-s390x:1.2
        resources:
          limits:
            memory: 50Mi
          requests:
            cpu: 10m
            memory: 50Mi
        args:
        - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null
        - --url=/healthz-dnsmasq
        - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
        - --url=/healthz-kubedns
        - --port=8080
        - --quiet
        ports:
        - containerPort: 8080
          protocol: TCP
      dnsPolicy: Default
```
	  
Updating below all 3 pods (kube-dns,kube-dnsmasq and exechealthz) status
```
[root@XXXX kube]# kubectl get all --all-namespaces
NAMESPACE     NAME                    READY     STATUS    RESTARTS   AGE
kube-system   po/kube-dns-v20-kjmqj   3/3       Running   0          1h

NAMESPACE     NAME              DESIRED   CURRENT   READY     AGE
kube-system   rc/kube-dns-v20   1         1         1         1h

NAMESPACE     NAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
default       svc/kubernetes   10.0.0.1     <none>        443/TCP         1h
kube-system   svc/kube-dns     10.0.0.10    <none>        53/UDP,53/TCP   1h
```

Updating below logs from exhealthz container:
```
[root@XXXX kube]# docker logs 4dd0807c5239
2017/07/20 09:08:28 Healthz probe on /healthz-dnsmasq error: Result of last exec: nslookup: can't resolve 'kubernetes.default.svc.cluster.local': Name does not resolve
, at 2017-07-20 09:08:26.72946 +0000 UTC, error exit status 1
2017/07/20 09:10:48 Healthz probe on /healthz-dnsmasq error: Result of last exec: nslookup: can't resolve 'kubernetes.default.svc.cluster.local': Name does not resolve
2017/07/20 09:52:38 Healthz probe on /healthz-kubedns error: Result of last exec: nslookup: can't resolve 'kubernetes.default.svc.cluster.local': Name does not resolve
, at 2017-07-20 09:52:36.734491 +0000 UTC, error exit status 1
2017/07/20 09:53:18 Healthz probe on /healthz-kubedns error: Result of last exec: nslookup: can't resolve 'kubernetes.default.svc.cluster.local': Name does not resolve
, at 2017-07-20 09:53:16.744527 +0000 UTC, error exit status 1
```

Describing below all pods logs:
```
[root@XXXX kube]# kubectl --namespace=kube-system describe pods
Name:           kube-dns-v19-bcz5t
Namespace:      kube-system
Node:           127.0.0.1/127.0.0.1
Start Time:     Wed, 19 Jul 2017 09:02:45 -0400
Labels:         k8s-app=kube-dns
                kubernetes.io/cluster-service=true
                version=v19
Status:         Running
IP:             172.17.0.2
Controllers:    ReplicationController/kube-dns-v19
Containers:
  kubedns:
    Container ID:       docker://fb936513b932d971514cc17d92cf5b178fd2f7bf1d30ba66a03a15128056d055
    Image:              gcr.io/google_containers/k8s-dns-kube-dns-s390x:1.14.1
    Image ID:           docker-pullable://gcr.io/google_containers/k8s-dns-kube-dns-s390x@sha256:b512b66d5fd9a3ce32f9aaa5e29a3672978d49e66b06f575d9bd0d6adce24594
    Ports:              10053/UDP, 10053/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --v=10
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           70Mi
    State:              Running
      Started:          Wed, 19 Jul 2017 09:03:25 -0400
    Ready:              True
    Restart Count:      0
    Liveness:           http-get http://:8080/healthz delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:          http-get http://:8081/readiness delay=30s timeout=5s period=10s #success=1 #failure=4
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9zmrn (ro)
    Environment Variables:      <none>
  dnsmasq:
    Container ID:       docker://2b6e676ea114ad6d5c3e9b725446c4056828ac367c227827dbd5cbd0c892e208
    Image:              gcr.io/google_containers/k8s-dns-dnsmasq-s390x:1.14.1
    Image ID:           docker-pullable://gcr.io/google_containers/k8s-dns-dnsmasq-s390x@sha256:1eb57c914d85af5a77a9af9632ad144106b3e12f68a8e8a734c5657c917753fd
    Ports:              53/UDP, 53/TCP
    Args:
      --cache-size=1000
      --no-resolv
      --server=127.0.0.1#10053
    State:              Running
      Started:          Wed, 19 Jul 2017 09:03:06 -0400
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9zmrn (ro)
    Environment Variables:      <none>
  healthz:
    Container ID:       docker://66881707f667d565942700a74e9be210fdd513c74127d1060010d4f22ba8fb1d
    Image:              gcr.io/google_containers/exechealthz-s390x:1.2
    Image ID:           docker://sha256:975b7c9bebc616358d0e9cdd60dc59f7579731eb8f48361ca62547e532e8e9db
    Port:               8080/TCP
    Args:
      -cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null && nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
      -port=8080
      -quiet
    Limits:
      cpu:      10m
      memory:   50Mi
    Requests:
      cpu:              10m
      memory:           50Mi
    State:              Running
      Started:          Wed, 19 Jul 2017 09:03:17 -0400
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9zmrn (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
Volumes:
  default-token-9zmrn:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-9zmrn
QoS Class:      Burstable
Tolerations:    CriticalAddonsOnly=:Exists
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                   Type            Reason          Message
  ---------     --------        -----   ----                    -------------                   --------        ------          -------
  2m            2m              1       {default-scheduler }                                    Normal          Scheduled       Successfully assigned kube-dns-v19-bcz5t to 127.0.0.1
  2m            2m              1       {kubelet 127.0.0.1}     spec.containers{dnsmasq}        Normal          Pulled          Container image ""gcr.io/google_containers/k8s-dns-dnsmasq-s390x:1.14.1"" already present on machine
  1m            1m              1       {kubelet 127.0.0.1}     spec.containers{dnsmasq}        Normal          Created         Created container with docker id 2b6e676ea114; Security:[seccomp=unconfined]
  1m            1m              1       {kubelet 127.0.0.1}     spec.containers{dnsmasq}        Normal          Started         Started container with docker id 2b6e676ea114
  1m            1m              1       {kubelet 127.0.0.1}     spec.containers{healthz}        Normal          Pulled          Container image ""gcr.io/google_containers/exechealthz-s390x:1.2"" already present on machine
  1m            1m              1       {kubelet 127.0.0.1}     spec.containers{healthz}        Normal          Created         Created container with docker id 66881707f667; Security:[seccomp=unconfined]
  1m            1m              1       {kubelet 127.0.0.1}     spec.containers{healthz}        Normal          Started         Started container with docker id 66881707f667
  1m            1m              1       {kubelet 127.0.0.1}     spec.containers{kubedns}        Normal          Pulled          Container image ""gcr.io/google_containers/k8s-dns-kube-dns-s390x:1.14.1"" already present on machine
  1m            1m              1       {kubelet 127.0.0.1}     spec.containers{kubedns}        Normal          Created         Created container with docker id fb936513b932; Security:[seccomp=unconfined]
  1m            1m              1       {kubelet 127.0.0.1}     spec.containers{kubedns}        Normal          Started         Started container with docker id fb936513b932
  27s           7s              3       {kubelet 127.0.0.1}     spec.containers{kubedns}        Warning         Unhealthy       Liveness probe failed: Get http://172.17.0.2:8080/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
```",closed,False,2017-07-21 13:34:54,2018-03-02 09:25:18
contrib,githubvick,https://github.com/kubernetes/contrib/issues/2688,https://api.github.com/repos/kubernetes/contrib/issues/2688,[Ansible] How to select version to install,"Hi,

I'm using this codebase extensively and currently it installs 1.5.2 by default, I would like to know if it is possible to install select version as I would like to try out with v1.7 to check if some of my issues are resolved. tried to search some places in code, but wasnt able to figure out.

Thanks,",closed,False,2017-07-21 15:47:22,2017-08-09 20:17:47
contrib,mkumatag,https://github.com/kubernetes/contrib/issues/2689,https://api.github.com/repos/kubernetes/contrib/issues/2689,Remove test-webserver code,Part of https://github.com/kubernetes/kubernetes/pull/48494 test-webserver code has been moved to kubernetes/tests so the code needs to be remove from contrib project.,closed,False,2017-07-21 16:32:40,2017-07-21 16:33:23
contrib,mkumatag,https://github.com/kubernetes/contrib/issues/2690,https://api.github.com/repos/kubernetes/contrib/issues/2690,Remove serve_hostname code,"serve_hostname is already present in https://github.com/kubernetes/kubernetes/tree/master/test/images/serve-hostname and maintained by test team for e2e tests, so no need to have duplicate code in contrib repo as well.",closed,False,2017-07-24 03:54:05,2017-08-18 16:35:33
contrib,mkumatag,https://github.com/kubernetes/contrib/pull/2691,https://api.github.com/repos/kubernetes/contrib/issues/2691,Remove serve_hostname,Fixes #2690 ,closed,True,2017-07-24 04:00:59,2017-08-18 18:17:41
contrib,nicksardo,https://github.com/kubernetes/contrib/pull/2692,https://api.github.com/repos/kubernetes/contrib/issues/2692,[404 server] Update to makefile and owners file and version bump,"A version of the 404 server hasn't been built with @cmluciano's changes including prometheus metric capturing. Along with bumping the version to 1.4, I made the following changes:

- Use golang 1.8.3 instead of 1.7
- Run 'clean' target before building 'server' target
- 'server' target was building ""server.go"" but ignoring the other file ""metrics.go""
- Moved build location within the golang's gopath - now the vendor directory is used.
- Replaced bprashanth with bowei for reviewer/approver

/assign @mikedanese 
/cc @bowei ",closed,True,2017-07-24 23:35:10,2017-08-14 23:47:02
contrib,themistymay,https://github.com/kubernetes/contrib/issues/2693,https://api.github.com/repos/kubernetes/contrib/issues/2693,[Ansible] Docker insecure registry task conditional error and formatting,"The ansible task for ""docker : Add any insecure registrys to docker config"" tries to compare a list to an int.

Error:
```
fatal: [kube-master-1]: FAILED! => {""failed"": true, ""msg"": ""The conditional check 'insecure_registrys is defined and insecure_registrys > 0' failed. The error was: Unexpected templating type error occurred on ({% if insecure_registrys is defined and insecure_registrys > 0 %} True {% else %} False {% endif %}): unorderable types: list() > int()\n\nThe error appears to have been in '/home/themistymay/dev/datamachines/kube-contrib/ansible/roles/docker/tasks/main.yml': line 71, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n- name: Add any insecure registrys to docker config\n  ^ here\n""}
```

It also has a poorly formatted `lineinfile` that does not conform to similar `lineinfile` for this same task set.",closed,False,2017-07-26 14:39:27,2018-01-10 13:14:13
contrib,themistymay,https://github.com/kubernetes/contrib/pull/2694,https://api.github.com/repos/kubernetes/contrib/issues/2694,[Ansible] Docker insecure registry task conditional error and formatting,Fixes #2693,closed,True,2017-07-26 14:40:19,2018-01-10 13:43:20
contrib,dixudx,https://github.com/kubernetes/contrib/pull/2695,https://api.github.com/repos/kubernetes/contrib/issues/2695,fix compiling errors for 404-server,Fix compiling errors that brought by new packages added in #2376.,closed,True,2017-07-31 10:27:39,2017-08-09 05:36:24
contrib,silenceshell,https://github.com/kubernetes/contrib/issues/2696,https://api.github.com/repos/kubernetes/contrib/issues/2696,[kube-keepalived-vip]why reset all ipvs tables when starting?,"kube-keepalived-vip will reset all ipvs tables when starting:

```
	err = resetIPVS()
	if err != nil {
		glog.Fatalf(""unexpected error: %v"", err)
	}
```

Is there any reason for doing this? In my env, there are many kube-keepalived-vip instances for different  services, they may infect each other when container starting, because the later container will clear the lvs policies setted by the former container.",closed,False,2017-08-01 09:34:40,2018-03-09 10:10:34
contrib,ngnmhieu,https://github.com/kubernetes/contrib/issues/2697,https://api.github.com/repos/kubernetes/contrib/issues/2697,Kube-dns can't be installed (Image not found).,"I'm deploying kubernetes using ansible script onto a virtual machine (Centos 7). The kube-dns addon couldn't be installed. After checking the google container registry, I found that the image `gcr.io/google_containers/kube-dnsmasq-x86_64:1.4` doesn't exist - there're only images for amd64. I guess it's because of the new support for Multi-Arch in Ansible scripts. The `ansible_architecture` might have not been correctly assigned to amd64. See  #2642 

Error message: 
```
Failed to pull image ""gcr.io/google_containers/kube-dnsmasq-x86_64:1.4"": image pull failed for gcr.io/google_containers/kube-dnsmasq-x86_64:1.4, this may be because there are no credentials on this request. details: (Error: Status 405 trying to pull repository google_containers/kube-dnsmasq-x86_64: ""v1 Registry API is disabled. If you are not explicitly using the v1 Registry API, it is possible your v2 image could not be found. Verify that your image is available, or retry with `dockerd --disable-legacy-registry`. See https://cloud.google.com/container-registry/docs/support/deprecation-notices"")
```
",closed,False,2017-08-01 09:47:56,2017-09-14 09:37:18
contrib,mikedanese,https://github.com/kubernetes/contrib/pull/2698,https://api.github.com/repos/kubernetes/contrib/issues/2698,release kube-keepalived:0.10,Fixes #2598,closed,True,2017-08-01 22:56:56,2017-08-02 00:49:24
contrib,mjbright,https://github.com/kubernetes/contrib/issues/2699,https://api.github.com/repos/kubernetes/contrib/issues/2699,Running deploy-local-cluster.sh failing - not picking up correct ansible_architecture,"
When running the deploy-etcd.yml playbook referenced by deploy-local-cluster.sh this fails with the error below.

It seems that the ansible_architecture is being incorrectly set (or rather not changed to amd64).

Running on """"Ubuntu 16.04.3 LTS""

TASK [etcd : Download tar file] ************************************************
fatal: [localhost]: FAILED! => {""changed"": false, ""dest"": ""/tmp/.ansible/files"", ""failed"": true, ""gid"": 0, ""group"": ""root"", ""mode"": ""0755"", ""msg"": ""Request failed"", ""owner"": ""root"", ""response"": ""HTTP Error 404: Not Found"", ""size"": 4096, ""state"": ""directory"", ""status_code"": 404, ""uid"": 0, ""url"": ""https://github.com/coreos/etcd/releases/download/v2.2.3/etcd-v2.2.3-linux-x86_64.tar.gz""}

",closed,False,2017-08-02 15:31:57,2018-06-02 18:10:19
contrib,gsaslis,https://github.com/kubernetes/contrib/pull/2700,https://api.github.com/repos/kubernetes/contrib/issues/2700,[service-loadbalancer] Update README on namespaces,"I got really confused by the README stating that the service loadbalancer only watches the `default` namespace by default. My experience was that I was getting errors about services from other namespaces not playing nicely with haproxy. 

#1709 helped me confirm that, by default, all namespaces are watched, not just `default`, so I thought I'd submit a quick PR to fix this in the docs and hopefully save others some head-bashing... : )",closed,True,2017-08-02 21:35:46,2018-03-13 09:44:33
contrib,claytono,https://github.com/kubernetes/contrib/pull/2701,https://api.github.com/repos/kubernetes/contrib/issues/2701,Update examples and manifests for 0.10,,closed,True,2017-08-03 13:05:59,2018-03-25 20:37:50
contrib,spiffxp,https://github.com/kubernetes/contrib/pull/2702,https://api.github.com/repos/kubernetes/contrib/issues/2702,Rename OWNERS assignees: to approvers:,"They are effectively the same, assignees is deprecated

ref: https://github.com/kubernetes/test-infra/issues/3851

I broke this into two commits around `vendor/` because I'm not sure it's kosher to be editing vendored OWNERS directly. ref: https://github.com/kubernetes/test-infra/issues/3694",closed,True,2017-08-03 23:56:16,2017-08-09 00:06:39
contrib,asifdxtreme,https://github.com/kubernetes/contrib/pull/2703,https://api.github.com/repos/kubernetes/contrib/issues/2703,Add goDoc reference badge,,closed,True,2017-08-04 10:26:25,2017-08-07 01:08:33
contrib,mbssaiakhil,https://github.com/kubernetes/contrib/pull/2704,https://api.github.com/repos/kubernetes/contrib/issues/2704,Fixed typo and grammatical error,"Fixed a typo, grammatical error, and slightly rephrased README of peer-finder",closed,True,2017-08-04 11:17:21,2017-08-09 18:31:03
contrib,claytono,https://github.com/kubernetes/contrib/pull/2705,https://api.github.com/repos/kubernetes/contrib/issues/2705,keepalived-vip: fix deprecated NodeLegacyHostIP,"Replace NodeLegacyHostIP with NodeInternalIP.  This is required for
keepalived-vip to work on bare metal, since it has no cloud provider to set
ExternalIP on each node resource.",closed,True,2017-08-04 12:26:38,2017-08-30 22:16:05
contrib,mbssaiakhil,https://github.com/kubernetes/contrib/pull/2706,https://api.github.com/repos/kubernetes/contrib/issues/2706,Fix typo and grammatical error,Fix typo and grammatical error in Kubernetes ZooKeeper,closed,True,2017-08-05 06:51:40,2017-08-09 18:57:37
contrib,claytono,https://github.com/kubernetes/contrib/pull/2707,https://api.github.com/repos/kubernetes/contrib/issues/2707,Fix service-loadbalancer test failures,"The previous test case of http://www.k8s.io is now failing becuase it
redirects to https://www.k8s.io, but the certificate being served for
that site isn't valid for that hostname anymore.  This changes it to use
the main kubenetes page which fixes the test failures.",closed,True,2017-08-05 21:21:48,2017-08-08 19:35:31
contrib,munnerz,https://github.com/kubernetes/contrib/pull/2708,https://api.github.com/repos/kubernetes/contrib/issues/2708,Fallback to node internal IP if external IP is not set,"Ref. this comment: https://github.com/kubernetes/kubernetes/issues/42125#issuecomment-320137328

Right now kube-keepalived does not work if using a cloud provider as the external IP field cannot be set.

I'm using my own cloud provider (https://github.com/munnerz/keepalived-cloud-provider) that automatically configures kube-keepalived-vip for service `LoadBalancer` support. Without this change, it will not work with any version of Kubernetes that doesn't use the old `NodeLegacyHostIP`.",closed,True,2017-08-06 19:59:19,2017-10-26 19:41:24
contrib,asifdxtreme,https://github.com/kubernetes/contrib/pull/2709,https://api.github.com/repos/kubernetes/contrib/issues/2709,Add GoDoc Widget,,closed,True,2017-08-07 01:08:15,2017-08-08 10:44:58
contrib,asifdxtreme,https://github.com/kubernetes/contrib/pull/2710,https://api.github.com/repos/kubernetes/contrib/issues/2710,Add GoDoc Widget,,closed,True,2017-08-08 10:44:44,2018-01-24 08:39:26
contrib,it-svit,https://github.com/kubernetes/contrib/issues/2711,https://api.github.com/repos/kubernetes/contrib/issues/2711,cluster-autoscaler works improperly with several kubernetes clusters in the same region,"I have two clusters in two different VPCs. 
But single cluster autoscaler is trying to affect both clusters.
```
I0808 15:01:14.854588       1 aws_manager.go:190] Regenerating ASG information for kube1-Nodepool2-1BK23XPH5999G-Workers
I0808 15:01:14.888882       1 aws_manager.go:190] Regenerating ASG information for kube2-Nodepool1-1303RAB6NXCBX-Workers
E0808 15:01:14.925243       1 static_autoscaler.go:219] Failed to scale up: failed to find template node for node group kube2-Nodepool1-1303RAB6NXCBX-Workers
W0808 15:01:14.925281       1 clusterstate.go:237] Failed to find readiness information for kube2-Nodepool1-1303RAB6NXCBX-Workers
W0808 15:01:14.925287       1 clusterstate.go:271] Failed to find readiness information for kube2-Nodepool1-1303RAB6NXCBX-Workers
```",closed,False,2017-08-08 15:15:33,2017-08-08 15:36:23
contrib,jsravn,https://github.com/kubernetes/contrib/pull/2712,https://api.github.com/repos/kubernetes/contrib/issues/2712,Use pod graceful termination time in rescheduler,"So rescheduler can be non-disruptive to existing pods.

It currently uses a hardcoded 10s timeout. In our use case, we want to rely on the value specified in each pod so we don't terminate pods abruptly.

This change adds a max grace period argument that will be set on delete if the pod's grace period is greater. It's set to a default of 10s to preserve the previous behaviour, although in my opinion we should default to the pod's value.",closed,True,2017-08-08 16:44:38,2017-12-20 18:53:55
contrib,spiffxp,https://github.com/kubernetes/contrib/issues/2713,https://api.github.com/repos/kubernetes/contrib/issues/2713,Every user in an OWNERS file should be a kubernetes member or outside collaborator,"/kind bug
/sig contributor-experience

Inspired by https://github.com/kubernetes/kubernetes/issues/50048

**What happened**:
OWNERS files contain users who are not collaborators. These users cannot be assigned to PRs or Issues which is what the bots use OWNERS files for.

**What you expected to happen**:
If you are in this list please become a kubernetes member or an outside collaborator.

The following github users are listed in an at least one OWNERS file in this repo, but are not collaborators or members of the kubernetes GitHub org

@3cky: podex/OWNERS
@BenTheElder: netperf-tester/OWNERS
@MikeSpreitzer: cni-plugins/OWNERS
@alekssaul: scale-demo/OWNERS
@igorpeshansky: fluentd/OWNERS
@paulbakker: git-sync/OWNERS
@raggi: go2docker/OWNERS
@sgallagher: init/OWNERS

If you would like to become a an outside collaborator, please see: https://github.com/kubernetes/community/blob/master/community-membership.md#requirements-for-outside-collaborators

If you would like to become a member of the kubernetes GitHub org, please see: https://github.com/kubernetes/community/blob/master/community-membership.md#member

If you would like to know more about OWNERS files, please see: https://github.com/kubernetes/community/blob/master/contributors/devel/owners.md

If you would like to see the horrible bash I wrote for this, please see:
```sh
for u in \
  $(for of in $(find . -name 'OWNERS' | grep -v vendor); do \
      cat $of | grep -v 'approvers\|reviewers:' | awk '{print $2}'; 
    done | sort | uniq \
  ); do \
  if ! grep -iq $u\ ~/w/go/src/k8s.io/kubernetes/members; then \
    echo @$u: $(ag -l -G OWNERS $u .); \
  fi; \ 
done
```

If you would like to hear no more of this nonsense, sorry for the spam, and we'll eventually have a PR out to prune users in OWNERS who aren't members of collaborators.",open,False,2017-08-09 22:35:25,2018-12-25 11:03:29
contrib,mkumatag,https://github.com/kubernetes/contrib/pull/2714,https://api.github.com/repos/kubernetes/contrib/issues/2714,Migrate _workspace to vendor,"This PR will address following two things:

1. Bump go version to `1.8`
2. Migrate _workspace to vendor",closed,True,2017-08-10 14:32:27,2018-01-12 04:58:19
contrib,kerneljack,https://github.com/kubernetes/contrib/issues/2715,https://api.github.com/repos/kubernetes/contrib/issues/2715,keepalived error: cannot find /proc on a kubernetes 1.7.2 GKE cluster,"Hi I'm trying to get keepalived working (using [1]) on a kubernetes 1.7.2 cluster but the pods all enter into a `CrashLoopBackOff` state with the following error:

```
F0810 20:55:48.149657       1 main.go:103] unexpected error: open /proc/sys/net/ipv4/vs/conntrack: no such file or directory
goroutine 1 [running]:
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.stacks(0x1d9fd00, 0xc400000000, 0x7d, 0xd1)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:766 +0xa5
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).output(0x1d7f800, 0xc400000003, 0xc420067980, 0x1d19f9b, 0x7, 0x67, 0x0)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:717 +0x337
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).printf(0x1d7f800, 0x3, 0x14ff936, 0x14, 0xc420655e78, 0x1, 0x1)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:655 +0x14c
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.Fatalf(0x14ff936, 0x14, 0xc420655e78, 0x1, 0x1)
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.main()
	/usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/main.go:103 +0x5a2
```

Do I need to mount /proc into the containers?

[1] - https://github.com/kubernetes/contrib/tree/master/keepalived-vip",closed,False,2017-08-10 21:05:30,2018-06-15 10:41:09
contrib,seeekr,https://github.com/kubernetes/contrib/pull/2716,https://api.github.com/repos/kubernetes/contrib/issues/2716,fix typo in keepalived-vip/README.md,,closed,True,2017-08-12 11:29:59,2017-08-12 19:54:34
contrib,tomplus,https://github.com/kubernetes/contrib/pull/2717,https://api.github.com/repos/kubernetes/contrib/issues/2717,ingress/readme.md: remove redundant line,"I've removed invalid line from the example.
Regards.",closed,True,2017-08-13 00:36:09,2018-03-09 23:23:35
contrib,shdowofdeath,https://github.com/kubernetes/contrib/issues/2718,https://api.github.com/repos/kubernetes/contrib/issues/2718,"while deploying 1.6.8 K8s , we still get 1.5.2 ","while using k8s installed by ansibe 
ansible/roles /kubernetes/defaults/main.yaml 
# Version of Kubernetes binaries
kube_version: 1.5.2 < changed to 1.6.8 
and we still see kube version is Kubernetes v1.5.2
help is needed :) ",closed,False,2017-08-14 13:02:03,2018-03-10 14:38:35
contrib,jonpulsifer,https://github.com/kubernetes/contrib/pull/2719,https://api.github.com/repos/kubernetes/contrib/issues/2719,Remove 404 server,"According to https://github.com/kubernetes/contrib/issues/762 we should be trying to purge this repository of code that has a better home. I can think of none other to absorb the 404-server than https://github.com/kubernetes/ingress/

I adjusted `404-server/README.md` in the same fashion as those in contrib/ingress/controllers/ with a link to the new repository.

xref https://github.com/kubernetes/ingress/issues/1072
",closed,True,2017-08-14 23:03:02,2017-10-24 20:48:03
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/2720,https://api.github.com/repos/kubernetes/contrib/issues/2720,Change maintainer & bump tag for metadata-proxy,For CVE 2016-9063.  I've already pushed the newly built container.,closed,True,2017-08-14 23:15:05,2017-09-07 18:31:49
contrib,dylanjohnsonsfo,https://github.com/kubernetes/contrib/issues/2721,https://api.github.com/repos/kubernetes/contrib/issues/2721,DNS advertised from broker to consumer etc. seems wrong,"Hey - I have simple consumer app (python/flask) , problem is the hostname returned from the kafka-svc/host is not correct even though I have added the right overrides as per this repos Kafka example. This problem crops up a lot for me and all roads lead to 

```--override advertised.host.name=blahblahblah.com```

even though this is set as you can see below the host name ```kafka``` is coming from somewhere which is odd as the kafka host hostnames are kafka-n anyway ? Any help would be appreciated as this has been driving me round the bend for a week. What is the best way to expose the correct broker hostnames from within a k8 configuration file ? 

```socket.gaierror: getaddrinfo failed for kafka:9092, exception was [Errno -5] No address associated with hostname. Is your advertised.listeners (calledadvertised.host.name before Kafka 9) correct and resolvable?```",closed,False,2017-08-14 23:24:42,2018-03-10 02:26:35
contrib,justlooks,https://github.com/kubernetes/contrib/issues/2722,https://api.github.com/repos/kubernetes/contrib/issues/2722,statefulset stop when it create zk-0,"i use statefulset to create 3-node zk cluster

 here is my Dockerfile and script  
Dockerfile  
```
FROM harbor.myrepo.com/library/hch_baseimage:v1.0

ENV ZK_USER=zookeeper \
ZK_DATA_DIR=/data/zookeeper/data \
ZK_DATA_LOG_DIR=/data/zookeeper/log \
ZK_LOG_DIR=/var/log/zookeeper

ARG ZK_DIST=zookeeper-3.4.10

RUN set -x && cd /opt && yum install wget tar java-1.8.0-openjdk.x86_64 -y && wget http://apache.mirrors.tds.net/zookeeper/$ZK_DIST/$ZK_DIST.tar.gz && /bin/tar -xvf ${ZK_DIST}.tar.gz -C /opt \
    && rm -r ""$ZK_DIST.tar.gz"" && ln -s /opt/$ZK_DIST /opt/zookeeper \
    && rm -rf /opt/zookeeper/CHANGES.txt \
    /opt/zookeeper/README.txt \
    /opt/zookeeper/NOTICE.txt \
    /opt/zookeeper/LICENSE.txt \
    /opt/zookeeper/README_packaging.txt \
    /opt/zookeeper/build.xml \
    /opt/zookeeper/conf/* \
    /opt/zookeeper/contrib \
    /opt/zookeeper/dist-maven \
    /opt/zookeeper/docs \
    /opt/zookeeper/ivy.xml \
    /opt/zookeeper/ivysettings.xml \
    /opt/zookeeper/recipes \
    /opt/zookeeper/src \
    /opt/zookeeper/$ZK_DIST.jar.asc \
    /opt/zookeeper/$ZK_DIST.jar.md5 \
    /opt/zookeeper/$ZK_DIST.jar.sha1

COPY zkGenConfig.sh zkOk.sh zkMetrics.sh /opt/zookeeper/bin/

RUN set -x \
	&& useradd $ZK_USER \
    && mkdir -p /usr/share/zookeeper /tmp/zookeeper /etc/zookeeper \
	&& chown -R ""$ZK_USER:$ZK_USER"" /opt/$ZK_DIST /tmp/zookeeper \
        && chmod +x /opt/zookeeper/bin/* \
	&& ln -s /opt/zookeeper/conf/ /etc/zookeeper \
	&& ln -s /opt/zookeeper/bin/* /usr/bin \
	&& ln -s /opt/zookeeper/$ZK_DIST.jar /usr/share/zookeeper/ \
	&& ln -s /opt/zookeeper/lib/* /usr/share/zookeeper/
```

zkGenConfig.sh

```
#!/usr/bin/env bash

ZK_USER=${ZK_USER:-""zookeeper""}
ZK_LOG_LEVEL=${ZK_LOG_LEVEL:-""INFO""}
ZK_DATA_DIR=${ZK_DATA_DIR:-""/var/lib/zookeeper/data""}
ZK_DATA_LOG_DIR=${ZK_DATA_LOG_DIR:-""/var/lib/zookeeper/log""}
ZK_LOG_DIR=${ZK_LOG_DIR:-""var/log/zookeeper""}
ZK_CONF_DIR=${ZK_CONF_DIR:-""/opt/zookeeper/conf""}
ZK_CLIENT_PORT=${ZK_CLIENT_PORT:-2181}
ZK_SERVER_PORT=${ZK_SERVER_PORT:-2888}
ZK_ELECTION_PORT=${ZK_ELECTION_PORT:-3888}
ZK_TICK_TIME=${ZK_TICK_TIME:-2000}
ZK_INIT_LIMIT=${ZK_INIT_LIMIT:-10}
ZK_SYNC_LIMIT=${ZK_SYNC_LIMIT:-5}
ZK_HEAP_SIZE=${ZK_HEAP_SIZE:-1G}
ZK_MAX_CLIENT_CNXNS=${ZK_MAX_CLIENT_CNXNS:-60}
ZK_MIN_SESSION_TIMEOUT=${ZK_MIN_SESSION_TIMEOUT:- $((ZK_TICK_TIME*2))}
ZK_MAX_SESSION_TIMEOUT=${ZK_MAX_SESSION_TIMEOUT:- $((ZK_TICK_TIME*20))}
ZK_SNAP_RETAIN_COUNT=${ZK_SNAP_RETAIN_COUNT:-3}
ZK_PURGE_INTERVAL=${ZK_PURGE_INTERVAL:-0}
ID_FILE=""$ZK_DATA_DIR/myid""
ZK_CONFIG_FILE=""$ZK_CONF_DIR/zoo.cfg""
LOGGER_PROPS_FILE=""$ZK_CONF_DIR/log4j.properties""
JAVA_ENV_FILE=""$ZK_CONF_DIR/java.env""
HOST=`hostname -s`
DOMAIN=`hostname -d`

function print_servers() {
	 for (( i=1; i<=$ZK_REPLICAS; i++ ))
	do
		echo ""server.$i=$NAME-$((i-1)).$DOMAIN:$ZK_SERVER_PORT:$ZK_ELECTION_PORT""
	done
}

function validate_env() {
    echo ""Validating environment""
	if [ -z $ZK_REPLICAS ]; then
		echo ""ZK_REPLICAS is a mandatory environment variable""
		exit 1
	fi

	if [[ $HOST =~ (.*)-([0-9]+)$ ]]; then
		NAME=${BASH_REMATCH[1]}
		ORD=${BASH_REMATCH[2]}
	else
		echo ""Failed to extract ordinal from hostname $HOST""
		exit 1
	fi
	MY_ID=$((ORD+1))
	echo ""ZK_REPLICAS=$ZK_REPLICAS""
    echo ""MY_ID=$MY_ID""
    echo ""ZK_LOG_LEVEL=$ZK_LOG_LEVEL""
    echo ""ZK_DATA_DIR=$ZK_DATA_DIR""
    echo ""ZK_DATA_LOG_DIR=$ZK_DATA_LOG_DIR""
    echo ""ZK_LOG_DIR=$ZK_LOG_DIR""
    echo ""ZK_CLIENT_PORT=$ZK_CLIENT_PORT""
    echo ""ZK_SERVER_PORT=$ZK_SERVER_PORT""
    echo ""ZK_ELECTION_PORT=$ZK_ELECTION_PORT""
    echo ""ZK_TICK_TIME=$ZK_TICK_TIME""
    echo ""ZK_INIT_LIMIT=$ZK_INIT_LIMIT""
    echo ""ZK_SYNC_LIMIT=$ZK_SYNC_LIMIT""
    echo ""ZK_MAX_CLIENT_CNXNS=$ZK_MAX_CLIENT_CNXNS""
    echo ""ZK_MIN_SESSION_TIMEOUT=$ZK_MIN_SESSION_TIMEOUT""
    echo ""ZK_MAX_SESSION_TIMEOUT=$ZK_MAX_SESSION_TIMEOUT""
    echo ""ZK_HEAP_SIZE=$ZK_HEAP_SIZE""
    echo ""ZK_SNAP_RETAIN_COUNT=$ZK_SNAP_RETAIN_COUNT""
    echo ""ZK_PURGE_INTERVAL=$ZK_PURGE_INTERVAL""
    echo ""ENSEMBLE""
    print_servers
    echo ""Environment validation successful""
}

function create_config() {
	rm -f $ZK_CONFIG_FILE
    echo ""Creating ZooKeeper configuration""
    echo ""#This file was autogenerated by k8szk DO NOT EDIT"" >> $ZK_CONFIG_FILE
	echo ""clientPort=$ZK_CLIENT_PORT"" >> $ZK_CONFIG_FILE
    echo ""dataDir=$ZK_DATA_DIR"" >> $ZK_CONFIG_FILE
    echo ""dataLogDir=$ZK_DATA_LOG_DIR"" >> $ZK_CONFIG_FILE
    echo ""tickTime=$ZK_TICK_TIME"" >> $ZK_CONFIG_FILE
    echo ""initLimit=$ZK_INIT_LIMIT"" >> $ZK_CONFIG_FILE
    echo ""syncLimit=$ZK_SYNC_LIMIT"" >> $ZK_CONFIG_FILE
    echo ""maxClientCnxns=$ZK_MAX_CLIENT_CNXNS"" >> $ZK_CONFIG_FILE
    echo ""minSessionTimeout=$ZK_MIN_SESSION_TIMEOUT"" >> $ZK_CONFIG_FILE
    echo ""maxSessionTimeout=$ZK_MAX_SESSION_TIMEOUT"" >> $ZK_CONFIG_FILE
    echo ""autopurge.snapRetainCount=$ZK_SNAP_RETAIN_COUNT"" >> $ZK_CONFIG_FILE
    echo ""autopurge.purgeInteval=$ZK_PURGE_INTERVAL"" >> $ZK_CONFIG_FILE

    if [ $ZK_REPLICAS -gt 1 ]; then
    	print_servers >> $ZK_CONFIG_FILE
    fi
    echo ""Wrote ZooKeeper configuration file to $ZK_CONFIG_FILE""
}

function create_data_dirs() {
	echo ""Creating ZooKeeper data directories and setting permissions""
    if [ ! -d $ZK_DATA_DIR  ]; then
        mkdir -p $ZK_DATA_DIR
        chown -R $ZK_USER:$ZK_USER $ZK_DATA_DIR
    fi

    if [ ! -d $ZK_DATA_LOG_DIR  ]; then
        mkdir -p $ZK_DATA_LOG_DIR
        chown -R $ZK_USER:$ZK_USER $ZK_DATA_LOG_DIR
    fi

    if [ ! -d $ZK_LOG_DIR  ]; then
        mkdir -p $ZK_LOG_DIR
        chown -R $ZK_USER:$ZK_USER $ZK_LOG_DIR
    fi
    if [ ! -f $ID_FILE ]; then
        echo $MY_ID >> $ID_FILE
    fi
    echo ""Created ZooKeeper data directories and set permissions in $ZK_DATA_DIR""
}

function create_log_props () {
	rm -f $LOGGER_PROPS_FILE
    echo ""Creating ZooKeeper log4j configuration""
	echo ""zookeeper.root.logger=CONSOLE"" >> $LOGGER_PROPS_FILE
	echo ""zookeeper.console.threshold=""$ZK_LOG_LEVEL >> $LOGGER_PROPS_FILE
	echo ""log4j.rootLogger=\${zookeeper.root.logger}"" >> $LOGGER_PROPS_FILE
	echo ""log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender"" >> $LOGGER_PROPS_FILE
	echo ""log4j.appender.CONSOLE.Threshold=\${zookeeper.console.threshold}"" >> $LOGGER_PROPS_FILE
	echo ""log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout"" >> $LOGGER_PROPS_FILE
	echo ""log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n"" >> $LOGGER_PROPS_FILE
	echo ""Wrote log4j configuration to $LOGGER_PROPS_FILE""
}

function create_java_env() {
    rm -f $JAVA_ENV_FILE
    echo ""Creating JVM configuration file""
    echo ""ZOO_LOG_DIR=$ZK_LOG_DIR"" >> $JAVA_ENV_FILE
    echo ""JVMFLAGS=\""-Xmx$ZK_HEAP_SIZE -Xms$ZK_HEAP_SIZE\"""" >> $JAVA_ENV_FILE
    echo ""Wrote JVM configuration to $JAVA_ENV_FILE""
}

validate_env && create_config && create_log_props && create_data_dirs && create_java_env
```

zkMetrics.sh
```
#!/usr/bin/env bash

ZK_CLIENT_PORT=${ZK_CLIENT_PORT:-2181}
echo mntr | nc localhost $ZK_CLIENT_PORT >& 1
```

zkOk.sh
```
#!/usr/bin/env bash

ZK_CLIENT_PORT=${ZK_CLIENT_PORT:-2181}
OK=$(echo ruok | nc 127.0.0.1 $ZK_CLIENT_PORT)
if [ ""$OK"" == ""imok"" ]; then
	exit 0
else
	exit 1
fi
```

and here is my deeply yam file

storage1.yaml
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-3g-01
  namespace : default
  labels:
    storetype: nfs
spec:
  capacity:
    storage: 3Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 10.117.186.63
    path: ""/data/kube-pv/zookeeper/node1""
```

storage2.yaml
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-3g-02
  namespace : default
  labels:
    storetype: nfs
spec:
  capacity:
    storage: 3Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 10.117.186.63
    path: ""/data/kube-pv/zookeeper/node2""
```

storage3.yaml
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-3g-03
  namespace : default
  labels:
    storetype: nfs
spec:
  capacity:
    storage: 3Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 10.117.186.63
    path: ""/data/kube-pv/zookeeper/node3""
```

zookeeper.yaml
```
---
apiVersion: v1
kind: Service
metadata:
  name: zk-svc
  labels:
    app: zk-svc
spec:
  ports:
  - port: 2888
    name: server
  - port: 3888
    name: leader-election
  clusterIP: None
  selector:
    app: zk
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: zk-cm
data:
  jvm.heap: ""1G""
  tick: ""2000""
  init: ""10""
  sync: ""5""
  client.cnxns: ""60""
  snap.retain: ""3""
  purge.interval: ""0""
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: zk-pdb
spec:
  selector:
    matchLabels:
      app: zk
  minAvailable: 2
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: zk
spec:
  serviceName: zk-svc
  replicas: 3
  template:
    metadata:
      labels:
        app: zk
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: ""app""
                    operator: In
                    values:
                    - zk
              topologyKey: ""kubernetes.io/hostname""
      imagePullSecrets:
      - name: demo-registry
      containers:
      - name: k8szk
        imagePullPolicy: Always
        image: harbor.myrepo.com/library/zookeeper:v3.4.10
        resources:
          requests:
            memory: ""1Gi""
            cpu: ""500m""
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        env:
        - name : ZK_REPLICAS
          value: ""3""
        - name : ZK_HEAP_SIZE
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: jvm.heap
        - name : ZK_TICK_TIME
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: tick
        - name : ZK_INIT_LIMIT
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: init
        - name : ZK_SYNC_LIMIT
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: tick
        - name : ZK_MAX_CLIENT_CNXNS
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: client.cnxns
        - name: ZK_SNAP_RETAIN_COUNT
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: snap.retain
        - name: ZK_PURGE_INTERVAL
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: purge.interval
        - name: ZK_CLIENT_PORT
          value: ""2181""
        - name: ZK_SERVER_PORT
          value: ""2888""
        - name: ZK_ELECTION_PORT
          value: ""3888""
        command:
        - sh
        - -c
        - zkGenConfig.sh && /opt/zookeeper/bin/zkServer.sh start-foreground
        readinessProbe:
          exec:
            command:
            - ""zkOk.sh""
          initialDelaySeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          exec:
            command:
            - ""zkOk.sh""
          initialDelaySeconds: 10
          timeoutSeconds: 5
        volumeMounts:
        - name: datadir
          mountPath: /data/zookeeper
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 1Gi
```

and when k8s created zk-0 ,it stopped

```
NAMESPACE     NAME                                        READY     STATUS             RESTARTS   AGE       IP              NODE
default       busybox                                     1/1       Running            121        5d        172.16.1.136    k8s-node1
default       http-svc-1262585536-0m99q                   1/1       Running            0          12d       172.16.2.32     k8s-node2
default       http-svc-1262585536-p7nvs                   1/1       Running            0          12d       172.16.1.126    k8s-node1
default       my-nginx-2141566008-8qg2h                   1/1       Running            0          18d       172.16.1.118    k8s-node1
default       nginx-hsd8q                                 1/1       Running            0          18d       172.16.1.117    k8s-node1
default       nginx-p8d72                                 1/1       Running            0          18d       172.16.1.116    k8s-node1
default       nginx-rb4ml                                 1/1       Running            1          18d       172.16.2.22     k8s-node2
default       zk-0                                        0/1       CrashLoopBackOff   6          10m       172.16.1.174    k8s-node1
demo          ftp-server-2250460153-01bn0                 1/1       Running            0          1d        172.16.1.158    k8s-node1
demo          my-ftp-1361328200-qssqx                     1/1       Running            0          1d        172.16.2.48     k8s-node2
kube-system   default-http-backend-726995137-phqw4        1/1       Running            0          12d       172.16.2.38     k8s-node2
kube-system   etcd-k8s-master                             1/1       Running            2          21d       10.132.41.234   k8s-master
kube-system   heapster-2994581613-cx1r7                   1/1       Running            2          21d       172.16.2.21     k8s-node2
kube-system   http-svc-1262585536-pmx6v                   1/1       Running            0          12d       172.16.1.127    k8s-node1
kube-system   http-svc-1262585536-xzq86                   1/1       Running            0          12d       172.16.2.33     k8s-node2
kube-system   kube-apiserver-k8s-master                   1/1       Running            2          21d       10.132.41.234   k8s-master
kube-system   kube-controller-manager-k8s-master          1/1       Running            5          21d       10.132.41.234   k8s-master
kube-system   kube-dns-2425271678-4ck8r                   3/3       Running            6          21d       172.16.0.75     k8s-master
kube-system   kube-flannel-ds-f67j6                       2/2       Running            4          21d       10.161.233.80   k8s-node2
kube-system   kube-flannel-ds-j936w                       2/2       Running            4          21d       10.132.41.234   k8s-master
kube-system   kube-flannel-ds-n36wb                       2/2       Running            2          21d       10.165.97.219   k8s-node1
kube-system   kube-proxy-3nspx                            1/1       Running            2          21d       10.161.233.80   k8s-node2
kube-system   kube-proxy-gsr5t                            1/1       Running            2          21d       10.132.41.234   k8s-master
kube-system   kube-proxy-zqg7j                            1/1       Running            1          21d       10.165.97.219   k8s-node1
kube-system   kube-scheduler-k8s-master                   1/1       Running            4          21d       10.132.41.234   k8s-master
kube-system   kubernetes-dashboard-2209332821-k9wwf       1/1       Running            2          21d       172.16.2.23     k8s-node2
kube-system   monitoring-grafana-982015592-85kbf          1/1       Running            0          14d       172.16.2.26     k8s-node2
kube-system   monitoring-influxdb-1870447071-rvqr5        1/1       Running            1          21d       172.16.1.111    k8s-node1
kube-system   nginx-ingress-controller-3527265638-qvd9j   1/1       Running            0          12d       10.161.233.80   k8s-node2
```

and log output
```
Validating environment
ZK_REPLICAS=3
MY_ID=1
ZK_LOG_LEVEL=INFO
ZK_DATA_DIR=/data/zookeeper/data
ZK_DATA_LOG_DIR=/data/zookeeper/log
ZK_LOG_DIR=/var/log/zookeeper
ZK_CLIENT_PORT=2181
ZK_SERVER_PORT=2888
ZK_ELECTION_PORT=3888
ZK_TICK_TIME=2000
ZK_INIT_LIMIT=10
ZK_SYNC_LIMIT=2000
ZK_MAX_CLIENT_CNXNS=60
ZK_MIN_SESSION_TIMEOUT= 4000
ZK_MAX_SESSION_TIMEOUT= 40000
ZK_HEAP_SIZE=1G
ZK_SNAP_RETAIN_COUNT=3
ZK_PURGE_INTERVAL=0
ENSEMBLE
server.1=zk-0.zk-svc.default.svc.cluster.local:2888:3888
server.2=zk-1.zk-svc.default.svc.cluster.local:2888:3888
server.3=zk-2.zk-svc.default.svc.cluster.local:2888:3888
Environment validation successful
Creating ZooKeeper configuration
Wrote ZooKeeper configuration file to /opt/zookeeper/conf/zoo.cfg
Creating ZooKeeper log4j configuration
Wrote log4j configuration to /opt/zookeeper/conf/log4j.properties
Creating ZooKeeper data directories and setting permissions
Created ZooKeeper data directories and set permissions in /data/zookeeper/data
Creating JVM configuration file
Wrote JVM configuration to /opt/zookeeper/conf/java.env
ZooKeeper JMX enabled by default
Using config: /opt/zookeeper/bin/../conf/zoo.cfg
2017-08-15 03:27:44,421 [myid:] - INFO  [main:QuorumPeer$QuorumServer@167] - Resolved hostname: zk-0.zk-svc.default.svc.cluster.local to address: zk-0.zk-svc.default.svc.cluster.local/172.16.1.174
2017-08-15 03:27:44,421 [myid:] - INFO  [main:QuorumPeerConfig@396] - Defaulting to majority quorums
2017-08-15 03:27:44,426 [myid:1] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
2017-08-15 03:27:44,426 [myid:1] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
2017-08-15 03:27:44,426 [myid:1] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.
2017-08-15 03:27:44,442 [myid:1] - INFO  [main:QuorumPeerMain@127] - Starting quorum peer
2017-08-15 03:27:44,455 [myid:1] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2181
2017-08-15 03:27:44,471 [myid:1] - INFO  [main:QuorumPeer@1134] - minSessionTimeout set to 4000
2017-08-15 03:27:44,471 [myid:1] - INFO  [main:QuorumPeer@1145] - maxSessionTimeout set to 40000
2017-08-15 03:27:44,472 [myid:1] - INFO  [main:QuorumPeer@1419] - QuorumPeer communication is not secured!
2017-08-15 03:27:44,472 [myid:1] - INFO  [main:QuorumPeer@1448] - quorum.cnxn.threads.size set to 20
2017-08-15 03:27:44,485 [myid:1] - INFO  [ListenerThread:QuorumCnxManager$Listener@739] - My election bind port: zk-0.zk-svc.default.svc.cluster.local/172.16.1.174:3888
2017-08-15 03:27:44,494 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumPeer@865] - LOOKING
2017-08-15 03:27:44,497 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@818] - New election. My id =  1, proposed zxid=0x0
2017-08-15 03:27:44,501 [myid:1] - INFO  [WorkerReceiver[myid=1]:FastLeaderElection@600] - Notification: 1 (message format version), 1 (n.leader), 0x0 (n.zxid), 0x1 (n.round), LOOKING (n.state), 1 (n.sid), 0x0 (n.peerEpoch) LOOKING (my state)
2017-08-15 03:24:19,926 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumPeer$QuorumServer@173] - Failed to resolve address: zk-1.zk-svc.default.svc.cluster.local
java.net.UnknownHostException: zk-1.zk-svc.default.svc.cluster.local: Name or service not known
        at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
        at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)
        at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
        at java.net.InetAddress.getAllByName0(InetAddress.java:1276)
        at java.net.InetAddress.getAllByName(InetAddress.java:1192)
        at java.net.InetAddress.getAllByName(InetAddress.java:1126)
        at java.net.InetAddress.getByName(InetAddress.java:1076)
        at org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer.recreateSocketAddresses(QuorumPeer.java:166)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:595)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)
2017-08-15 03:24:19,927 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumCnxManager@588] - Cannot open channel to 3 at election address zk-2.zk-svc.default.svc.cluster.local:3888
java.net.UnknownHostException: zk-2.zk-svc.default.svc.cluster.local
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)
2017-08-15 03:24:19,938 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumPeer$QuorumServer@173] - Failed to resolve address: zk-2.zk-svc.default.svc.cluster.local
java.net.UnknownHostException: zk-2.zk-svc.default.svc.cluster.local: Name or service not known
        at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
        at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)
        at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
        at java.net.InetAddress.getAllByName0(InetAddress.java:1276)
        at java.net.InetAddress.getAllByName(InetAddress.java:1192)
        at java.net.InetAddress.getAllByName(InetAddress.java:1126)
        at java.net.InetAddress.getByName(InetAddress.java:1076)
        at org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer.recreateSocketAddresses(QuorumPeer.java:166)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:595)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)
2017-08-15 03:24:19,938 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@852] - Notification time out: 25600
2017-08-15 03:24:45,539 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumCnxManager@588] - Cannot open channel to 2 at election address zk-1.zk-svc.default.svc.cluster.local:3888
java.net.UnknownHostException: zk-1.zk-svc.default.svc.cluster.local
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)
2017-08-15 03:24:45,551 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumPeer$QuorumServer@173] - Failed to resolve address: zk-1.zk-svc.default.svc.cluster.local
java.net.UnknownHostException: zk-1.zk-svc.default.svc.cluster.local: Name or service not known
        at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
        at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)
        at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
        at java.net.InetAddress.getAllByName0(InetAddress.java:1276)
        at java.net.InetAddress.getAllByName(InetAddress.java:1192)
        at java.net.InetAddress.getAllByName(InetAddress.java:1126)
        at java.net.InetAddress.getByName(InetAddress.java:1076)
        at org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer.recreateSocketAddresses(QuorumPeer.java:166)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:595)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)
2017-08-15 03:24:45,552 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumCnxManager@588] - Cannot open channel to 3 at election address zk-2.zk-svc.default.svc.cluster.local:3888
java.net.UnknownHostException: zk-2.zk-svc.default.svc.cluster.local
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)
2017-08-15 03:24:45,563 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumPeer$QuorumServer@173] - Failed to resolve address: zk-2.zk-svc.default.svc.cluster.local
java.net.UnknownHostException: zk-2.zk-svc.default.svc.cluster.local: Name or service not known
        at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
        at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)
        at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
        at java.net.InetAddress.getAllByName0(InetAddress.java:1276)
        at java.net.InetAddress.getAllByName(InetAddress.java:1192)
        at java.net.InetAddress.getAllByName(InetAddress.java:1126)
        at java.net.InetAddress.getByName(InetAddress.java:1076)
        at org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer.recreateSocketAddresses(QuorumPeer.java:166)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:595)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)
2017-08-15 03:24:45,563 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@852] - Notification time out: 51200
```

i think because zk-0 can not find zk-1 ,zk-2 ,it failed to start, but due to zk-0 fail ,zk-1,zk-2 will not start. any problem here?
",closed,False,2017-08-15 07:30:34,2018-11-19 09:41:57
contrib,justlooks,https://github.com/kubernetes/contrib/issues/2723,https://api.github.com/repos/kubernetes/contrib/issues/2723,the data and log dir will not be created when there are removed,"when i do zk statefulset test ,i find ,when i use "" kubectl delete -f zookeeper.yaml remove statefulset ,and removed pvc manually and remove pv ,and remove zk's data and log dir on nfs ,than recreate them(pv) ,use ""kubectl create -f zookeeper.yaml"" ,i find no data and log dir created on my nfs dir ,why? is it a bug?",closed,False,2017-08-16 02:22:15,2017-09-01 06:00:02
contrib,hchenxa,https://github.com/kubernetes/contrib/issues/2724,https://api.github.com/repos/kubernetes/contrib/issues/2724,replace rescheduler default lister address to 127.0.0.1,"currently, the rescheduler default --listen-address was localhost:9235, but this depend on the dns solve result and will have some issue if dns server have localhost record. 

so suggest to replace the localhost with 127.0.0.1.",closed,False,2017-08-17 05:07:13,2018-03-10 04:28:35
contrib,hchenxa,https://github.com/kubernetes/contrib/pull/2725,https://api.github.com/repos/kubernetes/contrib/issues/2725,replace localhost to 127.0.0.1 as dns resolve localhost may have issue,"related to issue https://github.com/kubernetes/contrib/issues/2724

in some cases, resolve the localhost may have some problems like below:
root@hchenk8s5:/var/log# host localhost
localhost.eng.platformlab.ibm.com has address 9.111.159.90
root@hchenk8s5:/var/log# ping -c 1 localhost
PING localhost.localdomain (127.0.0.1) 56(84) bytes of data.

so we may need to use 127.0.0.1:9235 as default listen address when rescheduler started. ",closed,True,2017-08-17 05:59:42,2017-08-17 12:04:49
contrib,hchenxa,https://github.com/kubernetes/contrib/issues/2726,https://api.github.com/repos/kubernetes/contrib/issues/2726,typo error in rescheduler help output,"```
root@hchenk8s4:~# ./rescheduler --help
Usage of rescheduler: rescheduler --running-in-cluster=true:
      --alsologtostderr                  log to standard error as well as files
      --housekeeping-interval duration   How often rescheduler takes actions. (default 10s)
      --initial-delay duration           How long should rescheduler wait after start to make sure
		 all critical addons had a chance to start. (default 2m0s)
      --kube-api-content-type string     Content type of requests sent to apiserver. (default ""application/vnd.kubernetes.protobuf"")
      --listen-address string            Address to listen on for serving prometheus metrics (default ""localhost:9235"")
      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)
      --log_dir string                   If non-empty, write log files in this directory
      --logtostderr                      log to standard error instead of files (default true)
      --min-replica-count int            Minimum number or replicas that a replica set or replication controller should have to allow their pods deletion in scale down
      --pod-scheduled-timeout duration   How long should rescheduler wait for critical pod to be scheduled
		 after evicting pods to make a spot for it. (default 10m0s)
      --running-in-cluster               Optional, if this controller is running in a kubernetes cluster, use the
		 pod secrets for creating a Kubernetes client. (default true)
      --skip-nodes-with-local-storage    If true cluster autoscaler will never delete nodes with pods with local storage, e.g. EmptyDir or HostPath (default true)
      --skip-nodes-with-system-pods      If true cluster autoscaler will never delete nodes with pods from kube-system (except for DeamonSet or mirror pods) (default true)
      --stderrthreshold severity         logs at or above this threshold go to stderr (default 2)
      --system-namespace string          Namespace to watch for critical addons. (default ""kube-system"")
  -v, --v Level                          log level for V logs
      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging
```

Need to replace the DeamonSet to DaemonSet.

and the fix https://github.com/kubernetes/autoscaler/commit/f64a7342  has addressed this issue, so we need to update the vendor to include the fix.",closed,False,2017-08-17 09:01:49,2018-03-10 05:29:33
contrib,jolestar,https://github.com/kubernetes/contrib/pull/2727,https://api.github.com/repos/kubernetes/contrib/issues/2727,update election document and example for RBAC,update election document for RBAC and add a deployment example config.,closed,True,2017-08-17 11:01:09,2018-03-12 20:31:33
contrib,vbence,https://github.com/kubernetes/contrib/pull/2728,https://api.github.com/repos/kubernetes/contrib/issues/2728,Fixed docker.sock path in scratch-debugger.,,closed,True,2017-08-18 09:23:40,2018-01-23 21:13:25
contrib,ajmulhollan1,https://github.com/kubernetes/contrib/issues/2729,https://api.github.com/repos/kubernetes/contrib/issues/2729,Keepalived-vip pod crashed in kubernetes,"Hello,

I've got keepalived-vip setup as a kubernetes deployment, alongside my Nginx ingress controller.

I'm trying to figure out why I had the following error for about 15 minutes.

```
F0822 07:13:14.932629       1 keepalived.go:122] keepalived error: signal: hangup
goroutine 1 [running]:
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.stacks(0x1d9fd00, 0xc400000000, 0x52, 0x9b)
    /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:766 +0xa5
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).output(0x1d7f800, 0xc400000003, 0xc420318000, 0x1d19fea, 0xd, 0x7a, 0x0)
    /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:717 +0x337
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).printf(0x1d7f800, 0x3, 0x14ff53a, 0x14, 0xc42071fd10, 0x1, 0x1)
    /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:655 +0x14c
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.Fatalf(0x14ff53a, 0x14, 0xc42071fd10, 0x1, 0x1)
    /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.(*keepalived).Start(0xc4200df540)
    /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/keepalived.go:122 +0x249
main.main()
    /usr/local/google/home/beeps/goproj/src/k8s.io/contrib/keepalived-vip/main.go:124 +0x402
```

I also couldn't ping the VIP, I just got `Destination Host Unreachable` after a lot of researching I couldn't find an answer, and I'd like to know how to fix this if it happens again?

It just started working again after a while, it was around 15-30 minutes.

Any help appreciated.",closed,False,2017-08-22 10:53:19,2018-08-23 10:11:36
contrib,Roderick-Jonsson,https://github.com/kubernetes/contrib/issues/2730,https://api.github.com/repos/kubernetes/contrib/issues/2730,Bug: contrib/ansible not finding image path to pull for kubernetes addons,"**Case:** 
Using contrib/ansible project to deploy kubernetes to a Fedora 25 virtual machine in virtualbox running on a server

**Symptom:**
Kubernetes addons like dns and dashboard fail with ErrImagePull
> 	Error syncing pod, skipping: failed to ""StartContainer"" for ""kubernetes-dashboard"" with ErrImagePull: ""image pull failed for gcr.io/google_containers/kubernetes-dashboard-x86_64:v1.1.0, this may be because there are no credentials on this request.  details: (manifest unknown: Failed to fetch \""v1.1.0\"" from request \""/v2/google_containers/kubernetes-dashboard-x86_64/manifests/v1.1.0\"".)""

**Cause:**
When the ansible variable _ansible_architecture_ is x86_64 an invalid path is generated to gcr image repository on
https://console.cloud.google.com/gcr/images/google-containers/GLOBAL

In the case of the kubernetes dashboard addon
> gcr.io/google_containers/kubernetes-dashboard-x86_64:v1.1.0

should be

> gcr.io/google_containers/kubernetes-dashboard-amd64:v1.1.0

Here is the error source where the paths get their wrong name, in this case the dashboard addon
`contrib/ansible/roles/kubernetes-addons/templates/kube-dash/kube-dash-rc.yaml.j2:        image: gcr.io/google_containers/kubernetes-dashboard-{{ ansible_architecture }}:v1.4.2`
`contrib/ansible/roles/kubernetes-addons/templates/kube-ui/dashboard-controller.yaml.j2:        image: gcr.io/google_containers/kubernetes-dashboard-{{ ansible_architecture }}:v1.1.0`",closed,False,2017-08-23 10:36:22,2018-03-10 11:35:32
contrib,dseeley,https://github.com/kubernetes/contrib/pull/2731,https://api.github.com/repos/kubernetes/contrib/issues/2731,Rebase of #2608 with master,"A rebase of the #2608 PR to master, as well as restoring a couple of bits (DR, vrid in template) that were removed, and upgrading keepalived to v1.3.6.   This PR requested in the thread: https://github.com/kubernetes/contrib/pull/2608#issuecomment-324109028 ",closed,True,2017-08-24 11:01:26,2017-08-24 11:11:36
contrib,dseeley,https://github.com/kubernetes/contrib/pull/2732,https://api.github.com/repos/kubernetes/contrib/issues/2732,Rebase of #2608 with master,"A rebase of the #2608 PR to master, as well as restoring a couple of bits (DR, vrid in template) that were removed, and upgrading keepalived to v1.3.6. This PR requested in the thread: https://github.com/kubernetes/contrib/pull/2608#issuecomment-324109028
",closed,True,2017-08-24 12:54:46,2017-08-24 12:56:27
contrib,dseeley,https://github.com/kubernetes/contrib/pull/2733,https://api.github.com/repos/kubernetes/contrib/issues/2733,Rebase of #2608 with master,"A rebase of the #2608 PR to master, as well as restoring a couple of bits (DR, vrid in template) that were removed, and upgrading keepalived to v1.3.6. This PR requested in the thread: https://github.com/kubernetes/contrib/pull/2608#issuecomment-324109028
",closed,True,2017-08-24 12:59:11,2018-01-07 06:09:20
contrib,arthur0,https://github.com/kubernetes/contrib/issues/2734,https://api.github.com/repos/kubernetes/contrib/issues/2734,[kube-keepalived-vip] Exited due to segmentation fault (SIGSEGV),"Kubernetes version: v1.7.4

Following the steps in the documentation, I created a single yaml file to reproduce:
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: echoheaders
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: echoheaders
    spec:
      containers:
      - name: echoheaders
        image: gcr.io/google_containers/echoserver:1.4
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: echoheaders
  labels:
    app: echoheaders
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30302
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: echoheaders
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-keepalived-vip
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: kube-keepalived-vip
rules:
- apiGroups: [""""]
  resources:
  - pods
  - nodes
  - endpoints
  - services
  - configmaps
  verbs: [""get"", ""list"", ""watch""]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: kube-keepalived-vip
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-keepalived-vip
subjects:
- kind: ServiceAccount
  name: kube-keepalived-vip
  namespace: default
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: vip-configmap
data:
  10.4.0.50: default/echoheaders
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-keepalived-vip
spec:
  template:
    metadata:
      labels:
        name: kube-keepalived-vip
    spec:
      hostNetwork: true
      serviceAccount: kube-keepalived-vip
      containers:
        - image: gcr.io/google_containers/kube-keepalived-vip:0.9
          name: kube-keepalived-vip
          imagePullPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /lib/modules
              name: modules
              readOnly: true
            - mountPath: /dev
              name: dev
          # use downward API
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          # to use unicast
          args:
          - --services-configmap=default/vip-configmap
          # unicast uses the ip of the nodes instead of multicast
          # this is useful if running in cloud providers (like AWS)
          #- --use-unicast=true
      volumes:
        - name: modules
          hostPath:
            path: /lib/modules
        - name: dev
          hostPath:
            path: /dev
      nodeSelector:
        type: worker
``` 
```bash
$ kubectl get po
NAME                        READY     STATUS    RESTARTS   AGE
echoheaders-smfnc           1/1       Running   0          5s
kube-keepalived-vip-dk0vj   1/1       Running   0          5s
$ kubectl logs kube-keepalived-vip-dk0vj
VRRP child process(489) died: Respawning
Starting VRRP child process, pid=491
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Opening file '/etc/keepalived/keepalived.conf'.
pid 491 exited due to segmentation fault (SIGSEGV).
  Please report a bug at https://github.com/acassen/keepalived/issues
  and include this log from when keepalived started, what happened
  immediately before the crash, and your configuration file.
VRRP child process(491) died: Respawning
Starting VRRP child process, pid=493
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Opening file '/etc/keepalived/keepalived.conf'.
pid 493 exited due to segmentation fault (SIGSEGV).
  Please report a bug at https://github.com/acassen/keepalived/issues
  and include this log from when keepalived started, what happened
  immediately before the crash, and your configuration file.
[...]
```

",closed,False,2017-08-25 17:59:40,2018-03-11 03:51:32
contrib,oguzy,https://github.com/kubernetes/contrib/issues/2735,https://api.github.com/repos/kubernetes/contrib/issues/2735,worker node deployment is failing because they are not able to connect to etcd service on master node,"I tried installation with 3 nodes. Master node is also working as etcd node. After master node is installed, worker nodes failed at the kubelet service start stage. When i check the log, i got the error on connecting to etcd service. I tried telnet to connect etcd port on master and it was unsecessfull. When i stopped the firewalld on master and worker nodes, it worked. So it seems there is a missing firewalld rule for worker node access to master node.",closed,False,2017-08-29 08:36:37,2018-03-10 16:40:58
contrib,barclaac,https://github.com/kubernetes/contrib/issues/2736,https://api.github.com/repos/kubernetes/contrib/issues/2736,[keepalived-vip] kube-keepalived-vip not watching for service config map changes,"The kube-keepalived-vip daemon is only registering Informers for the service and endpoints. We should add another informer to watch the ConfigMap. This would then allow editing the list of services and VIPs, and the daemon would automatically reconfigure keepalived to match.
",closed,False,2017-08-29 21:56:57,2018-06-03 20:36:18
contrib,sgarap,https://github.com/kubernetes/contrib/issues/2737,https://api.github.com/repos/kubernetes/contrib/issues/2737,Zookeeper Statefulset is not working with Kubernetes 1.6,"Hi,

Zookeeper statefulsets deployment (with 5 replicas) is not working on Kubernetes 1.6. It is working fine with Kubernetes 1.5.
In Kubernetes 1.6, A Zookeeper pod is unable to connect to other zookeeper pod because of DNS Resolution issue.  


```
2017-08-30 14:18:04,876 [myid:1] - WARN  [WorkerSender[myid=1]:QuorumCnxManager@588] - Cannot open channel to 2 at election address zk-1.zk-headless.default.svc.cluster.local:3888
java.net.UnknownHostException: zk-1.zk-headless.default.svc.cluster.local
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:538)
        at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:452)
        at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:433)
        at java.lang.Thread.run(Thread.java:745)
2017-08-30 14:18:04,898 [myid:1] - WARN  [WorkerSender[myid=1]:QuorumPeer$QuorumServer@173] - Failed to resolve address: zk-1.zk-headless.default.svc.cluster.local
java.net.UnknownHostException: zk-1.zk-headless.default.svc.cluster.local: Name or service not known
        at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
        at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)
        at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
        at java.net.InetAddress.getAllByName0(InetAddress.java:1276)
        at java.net.InetAddress.getAllByName(InetAddress.java:1192)
        at java.net.InetAddress.getAllByName(InetAddress.java:1126)
        at java.net.InetAddress.getByName(InetAddress.java:1076)
        at org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer.recreateSocketAddresses(QuorumPeer.java:166)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:595)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:538)
        at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:452)
        at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:433)
        at java.lang.Thread.run(Thread.java:745)

```

In etcd, I noticed that records are created with random generated name.
/ # etcdctl ls /skydns/local/cluster/svc/default/zk-headless
/skydns/local/cluster/svc/default/zk-headless/ccd37d16
/skydns/local/cluster/svc/default/zk-headless/943e6723
/skydns/local/cluster/svc/default/zk-headless/18576dbd
/skydns/local/cluster/svc/default/zk-headless/aa326531
/skydns/local/cluster/svc/default/zk-headless/1ed8054b

[](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)
This documentation states that, CNAME for SRV Record would be auto-generated-name.my-svc.my-namespace.svc.cluster.local.

Where as in 1.5, it has hostnames as keys.
/ # etcdctl ls /skydns/local/cluster/svc/default/zk-headless
/skydns/local/cluster/svc/default/zk-headless/zk-3
/skydns/local/cluster/svc/default/zk-headless/zk-4
/skydns/local/cluster/svc/default/zk-headless/zk-1
/skydns/local/cluster/svc/default/zk-headless/zk-0
/skydns/local/cluster/svc/default/zk-headless/zk-2

From within the pod curl  zk-1.zk-headless.default.svc.cluster.local:3888 is not working where as using randomnumber from etcd is working. Ex: curl 943e6723.zk-headless.default.svc.cluster.local:3888

As Kube DNS implemenation is changed to save random number in Service Record, can somone please let me know, how to get the zookeeper statefulsets implementation to work on Kubernetes 1.6.

Or Is there any other alternative approach?

Regards,
Sindhura
",closed,False,2017-08-30 14:30:43,2018-08-08 13:38:55
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/2738,https://api.github.com/repos/kubernetes/contrib/issues/2738,Add a container & job spec to check metadata concealment correctness,"This checks for some of the fixes made in https://github.com/kubernetes/kubernetes/pull/51302.  Ref #8867.

I'm unsure of where this should live, but this seems like as good a place as any?  @spxtr We discussed, and I'd rather just check it in here now.",closed,True,2017-08-31 00:52:26,2017-09-14 22:48:08
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/2739,https://api.github.com/repos/kubernetes/contrib/issues/2739,Fix nginx.conf to block recursive calls and identity endpoints,"These changes reflect https://github.com/kubernetes/kubernetes/pull/51302.

We should make these changes here and get rid of the configmap in https://github.com/kubernetes/kubernetes/pull/51302, because configmaps have no good update strategy.  Specifically, if the master version changes, the configmap will change, but pods making up the daemonset won't get restarted (unless the version of the container changed as well), so they won't update their config (the configmap volume may change, but nginx won't reload it).

cc @ahmetb @destijl @mikedanese @Q-Lee.",closed,True,2017-08-31 18:22:13,2017-08-31 21:14:03
contrib,barclaac,https://github.com/kubernetes/contrib/pull/2740,https://api.github.com/repos/kubernetes/contrib/issues/2740,[keepalived-vip] Watch the VIP -> service configmap for changes to services.,"This PR fixes issue #2736 
Any update to the config map will now cause kube-keepalived-vip to update and activate the ipvs configuration.
Let me know what you think. Thanks.",closed,True,2017-08-31 19:08:10,2017-08-31 19:33:57
contrib,barclaac,https://github.com/kubernetes/contrib/pull/2741,https://api.github.com/repos/kubernetes/contrib/issues/2741,[keepalived-vip] Watch the VIP->service configmap for changes,"This PR fixes issue #2736 
Any update to the config map will now cause kube-keepalived-vip to update and activate the ipvs configuration. Let me know what you think. Thanks.
",closed,True,2017-08-31 21:16:32,2018-06-03 20:36:17
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/2742,https://api.github.com/repos/kubernetes/contrib/issues/2742,Don't have a default nginx.conf in the metadata proxy,This avoid confusion as to what config the pod is using.  See https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/metadata-proxy/gce/metadata-proxy-configmap.yaml,closed,True,2017-08-31 21:44:25,2017-09-25 22:55:25
contrib,yarilc,https://github.com/kubernetes/contrib/pull/2743,https://api.github.com/repos/kubernetes/contrib/issues/2743,It looks like *x86_64 images are not longer available,This is to work-around an issue I have notificed today. It looks like the x86_64 are all failing the pull while amd64 are working.,closed,True,2017-09-01 10:28:07,2018-05-04 23:53:08
contrib,barclaac,https://github.com/kubernetes/contrib/issues/2744,https://api.github.com/repos/kubernetes/contrib/issues/2744,[keepalived-vip] kube-keepalived-vip sends SIGHUP to keepalived before keepalived has installed signal handler,"The kube-keepalived-vip daemon starts the keepalived daemon and almost immediately sends it a SIGHUP. Because the default action on a SIGHUP is to terminate the process the keepalived process will continually exit and be restarted.

The fix here should be ensure that kube-keepalived-vip waits until keepalived is fully operational with an installed SIGHUP handler.
",closed,False,2017-09-01 21:45:54,2018-07-02 12:31:18
contrib,barclaac,https://github.com/kubernetes/contrib/pull/2745,https://api.github.com/repos/kubernetes/contrib/issues/2745,WIP: Delay after starting keepalived and before reconfiguring,"This fixes #2744 for me.

This isn't a really good fix because almost any time that you fix a race condition by using a sleep there's something wrong.

k.started definitely should not be set to true before the k.cmd.Start() call.

Anyone got a better solution to the sleep? I thought about waiting on the PID file creation by keepalived but unfortunately by code inspection keepalived creates that file before the signal handler is installed so to truly fix the race condition we would need an upstream fix in keepalived.

Open to other suggestions though.",closed,True,2017-09-01 21:49:20,2018-04-14 01:39:54
contrib,walteraa,https://github.com/kubernetes/contrib/issues/2746,https://api.github.com/repos/kubernetes/contrib/issues/2746,[Keepalived VIP] Help about Logs,"How can I read the logs that I have added in the Controller using `glog.Info` statement?

I mean, I have added this logs in the controller, but when I run `kubectl logs pod_name` these logs don't appear.

Is there a way to see this logs?",closed,False,2017-09-02 14:30:51,2018-01-04 13:15:02
contrib,vklonghml,https://github.com/kubernetes/contrib/issues/2747,https://api.github.com/repos/kubernetes/contrib/issues/2747,[QUESTION]how to use k8s node perf dash with e2e test log?,"Hi all,
recently, i use the node perf dash to analyze the Kubernetes node e2e performance tests; however, when i followed the README
(https://github.com/kubernetes/contrib/tree/master/node-perf-dash), i can't get the
results like k8s ci website(http://node-perf-dash.k8s.io/#/builds).

here are my environment:
1) i got a k8s cluster(one master, two nodes),
2)then run command (go run hack/e2e.go -v -test  --test_args=""--host=http://127.0.0.1:8080 --ginkgo.focus=\[Feature:Performance\]"" >>/tmp/log.txt) to get the e2e log, 
3)make directories like the README said, like this
$MY_TEST_RESULT_PATH/
  latest-build.txt
  build_nr_1/
      build-log.txt
      artifacts/
          test_machine_host_name1/
              kubelet.log
          test_machine_host_name2

latest-build.txt has just string ""1"" in it, build-log.txt is copied from the e2e log, and the kubelet.log iscopied from kubelet node
4)finally, run node perf dash with command ""./node-perf-dash --address=0.0.0.0:808 --builds=1 --tracing=true --datasource=local --local-data-dir=/tmp/node-perf-dash"", but in website, i got nothing, but a few icons

what's the problem i did as described, could you tell how to do with it, thank you in
advence",closed,False,2017-09-04 13:52:30,2018-03-11 00:48:35
contrib,Pensu,https://github.com/kubernetes/contrib/pull/2748,https://api.github.com/repos/kubernetes/contrib/issues/2748,Changes for making multi-arch targets,This PR ensure multi-arch images builds for rescheduler.,closed,True,2017-09-06 10:01:36,2017-09-08 12:25:02
contrib,ghost,https://github.com/kubernetes/contrib/issues/2749,https://api.github.com/repos/kubernetes/contrib/issues/2749,scripts/deploy-cluster.sh fail,"following https://kubernetes.io/docs/getting-started-guides/fedora/fedora_ansible_config/
$ pwd
/home/admin/cluster/contrib/ansible
$ ./scripts/deploy-cluster.sh
./scripts/deploy-cluster.sh: line 17: ./init.sh: No such file or directory
./scripts/deploy-cluster.sh: line 20: ansible_playbook: command not found

Edit: it looks like this is related to Fedora since the script works on Ubuntu",closed,False,2017-09-06 15:17:26,2017-09-19 23:09:25
contrib,ihmccreery,https://github.com/kubernetes/contrib/issues/2750,https://api.github.com/repos/kubernetes/contrib/issues/2750,Move metadata-proxy to GoogleCloudPlatform,"... or somewhere else.

Ref #762.",closed,False,2017-09-06 18:02:38,2017-10-12 20:55:44
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/2751,https://api.github.com/repos/kubernetes/contrib/issues/2751,Add OWNERS file to metadata-proxy,"Ref https://github.com/kubernetes/contrib/pull/2738#issuecomment-326685941, #2750.",closed,True,2017-09-06 18:03:14,2017-09-06 18:13:47
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/2752,https://api.github.com/repos/kubernetes/contrib/issues/2752,make arch equal amd64 if ansible_arch is x86_64,"fixes #2697 

@Pensu does this work for you?",closed,True,2017-09-06 18:25:15,2017-09-14 09:37:18
contrib,golisandeep3,https://github.com/kubernetes/contrib/pull/2753,https://api.github.com/repos/kubernetes/contrib/issues/2753,Use correct property name for log.dir,"The correct property name for log.dir  is log.dirs
",closed,True,2017-09-07 05:27:45,2017-09-07 05:50:20
contrib,golisandeep3,https://github.com/kubernetes/contrib/pull/2754,https://api.github.com/repos/kubernetes/contrib/issues/2754,Fix kafka data directory property and version ,The property name for kafka data directory( log.dir) is wrong.It should be log.dirs,closed,True,2017-09-07 06:03:16,2018-07-07 01:36:12
contrib,ghost,https://github.com/kubernetes/contrib/issues/2755,https://api.github.com/repos/kubernetes/contrib/issues/2755,deploy fails with ERROR! 'gentoken_master' is undefined,"TASK [kubernetes : Copy token files to other masters] **************************
fatal: [atomic-node-01]: FAILED! => {""failed"": true, ""msg"": ""ERROR! 'gentoken_master' is undefined""}
fatal: [atomic-node-02]: FAILED! => {""failed"": true, ""msg"": ""ERROR! 'gentoken_master' is undefined""}
OS: ansible Host Ubuntu, kubernetes master and nodes are Fedora Atomic 26
inventory:
[masters]
atomic-master

[etcd:children]
masters

[nodes]
atomic-node-0[1:2]
",closed,False,2017-09-07 11:19:09,2018-03-11 01:49:35
contrib,wjiangjay,https://github.com/kubernetes/contrib/issues/2756,https://api.github.com/repos/kubernetes/contrib/issues/2756,"Got ""AnsibleUndefinedVariable: 'kube_ca_cert' is undefined"" when deploy master via deploy-cluster.sh",,closed,False,2017-09-08 08:25:19,2017-09-08 09:02:01
contrib,piosz,https://github.com/kubernetes/contrib/pull/2757,https://api.github.com/repos/kubernetes/contrib/issues/2757,Bumped Rescheduler version to v0.3.2,,closed,True,2017-09-08 11:20:38,2017-09-08 11:38:22
contrib,cmluciano,https://github.com/kubernetes/contrib/pull/2758,https://api.github.com/repos/kubernetes/contrib/issues/2758,Deprecation notice for service-loadbalancer.,"This project does not have any active maintainers and is supplanted to
enhancements that exist in Ingress.",closed,True,2017-09-12 14:31:25,2017-09-15 15:52:40
contrib,torvitas,https://github.com/kubernetes/contrib/pull/2759,https://api.github.com/repos/kubernetes/contrib/issues/2759,fixes deprecations for sudo -> become and always_run -> check_mode,,closed,True,2017-09-16 10:14:01,2017-09-18 06:06:47
contrib,harmjanblok,https://github.com/kubernetes/contrib/pull/2760,https://api.github.com/repos/kubernetes/contrib/issues/2760,[keepalived-vip] Release 0.11,"Would it be possible to release a new version of keepalived-vip? My nodes don't have an ExternalIP set, which has been fixed in https://github.com/kubernetes/contrib/pull/2705.",closed,True,2017-09-19 19:30:46,2017-09-20 05:28:05
contrib,juliohm1978,https://github.com/kubernetes/contrib/issues/2761,https://api.github.com/repos/kubernetes/contrib/issues/2761,"Kubelet ""Error updating node status"" appears when using keepalived on selected nodes","I'm deploying keepalived on two of my nodes to provide HA for an nginx service that will be also deployed on those nodes. Both keepalived and nginx are independent daemonsets running privileged containers to interface directly with the node's network (hostNetwork: true).

Nginx is deployed successfully, and the issue persists if I remove it. So, I'm running into problems trying to get keepalived to behave well with the node's kubelet service.

While keepalived is working as expected for its HA purpose, kubelet seems to get lost once virtual addresses are assigned to the node. It will fail to update the node status with the api server.

```
Sep 19 17:58:03 infra02-lab kubelet[25341]: E0919 17:58:03.368592   25341 kubelet_node_status.go:357] Error updating node status, will retry: failed to patch status ""{\""status\"":{\""$setElementOrder/addresses\"":[{\""type\"":\""ExternalIP\""},{\""type\"":\""InternalIP\""},{\""type\"":\""ExternalIP\""},{\""type\"":\""InternalIP\""},{\""type\"":\""Hostname\""}],\""$setElementOrder/conditions\"":[{\""type\"":\""OutOfDisk\""},{\""type\"":\""MemoryPressure\""},{\""type\"":\""DiskPressure\""},{\""type\"":\""Ready\""}],\""addresses\"":[{\""address\"":\""172.31.134.142\"",\""type\"":\""ExternalIP\""},{\""address\"":\""172.31.134.112\"",\""type\"":\""ExternalIP\""},{\""address\"":\""172.31.134.142\"",\""type\"":\""InternalIP\""},{\""address\"":\""172.31.134.112\"",\""type\"":\""InternalIP\""}],\""conditions\"":[{\""lastHeartbeatTime\"":\""2017-09-19T20:58:03Z\"",\""lastTransitionTime\"":\""2017-09-19T20:58:03Z\"",\""message\"":\""kubelet has sufficient disk space available\"",\""reason\"":\""KubeletHasSufficientDisk\"",\""status\"":\""False\"",\""type\"":\""OutOfDisk\""},{\""lastHeartbeatTime\"":\""2017-09-19T20:58:03Z\"",\""lastTransitionTime\"":\""2017-09-19T20:58:03Z\"",\""message\"":\""kubelet has sufficient memory available\"",\""reason\"":\""KubeletHasSufficientMemory\"",\""status\"":\""False\"",\""type\"":\""MemoryPressure\""},{\""lastHeartbeatTime\"":\""2017-09-19T20:58:03Z\"",\""lastTransitionTime\"":\""2017-09-19T20:58:03Z\"",\""message\"":\""kubelet has no disk pressure\"",\""reason\"":\""KubeletHasNoDiskPressure\"",\""status\"":\""False\"",\""type\"":\""DiskPressure\""},{\""lastHeartbeatTime\"":\""2017-09-19T20:58:03Z\"",\""lastTransitionTime\"":\""2017-09-19T20:58:03Z\"",\""message\"":\""kubelet is posting ready status. AppArmor enabled\"",\""reason\"":\""KubeletReady\"",\""status\"":\""True\"",\""type\"":\""Ready\""}]}}"" for node ""infra02-lab"": The order in patch list:
Sep 19 17:58:03 infra02-lab kubelet[25341]: [map[address:172.31.134.142 type:ExternalIP] map[address:172.31.134.112 type:ExternalIP] map[address:172.31.134.142 type:InternalIP] map[address:172.31.134.112 type:InternalIP]]
Sep 19 17:58:03 infra02-lab kubelet[25341]:  doesn't match $setElementOrder list:
Sep 19 17:58:03 infra02-lab kubelet[25341]: [map[type:ExternalIP] map[type:InternalIP] map[type:ExternalIP] map[type:InternalIP] map[type:Hostname]]
```

For this particular node, 172.31.134.112 being the node's true IP and 172.31.134.142 is it's virtual IP.

Clearing up all the clutter from the error message, it boils down to

```
The order in patch list: [
  map[address:172.31.134.142 type:ExternalIP]
  map[address:172.31.134.112 type:ExternalIP]
  map[address:172.31.134.142 type:InternalIP]
  map[address:172.31.134.112 type:InternalIP]
]

doesn't match $setElementOrder list:

[
  map[type:ExternalIP]
  map[type:InternalIP]
  map[type:ExternalIP]
  map[type:InternalIP]
  map[type:Hostname]
]
```

Below is the json content created by kubelet prettyfied:

```
{
  ""status"": {
    ""$setElementOrder\/addresses"": [
      {
        ""type"": ""ExternalIP""
      },
      {
        ""type"": ""InternalIP""
      },
      {
        ""type"": ""ExternalIP""
      },
      {
        ""type"": ""InternalIP""
      },
      {
        ""type"": ""Hostname""
      }
    ],
    ""$setElementOrder\/conditions"": [
      {
        ""type"": ""OutOfDisk""
      },
      {
        ""type"": ""MemoryPressure""
      },
      {
        ""type"": ""DiskPressure""
      },
      {
        ""type"": ""Ready""
      }
    ],
    ""addresses"": [
      {
        ""address"": ""172.31.134.142"",
        ""type"": ""ExternalIP""
      },
      {
        ""address"": ""172.31.134.112"",
        ""type"": ""ExternalIP""
      },
      {
        ""address"": ""172.31.134.142"",
        ""type"": ""InternalIP""
      },
      {
        ""address"": ""172.31.134.112"",
        ""type"": ""InternalIP""
      }
    ],
    ""conditions"": [...]
  }
}
```

I'm inclined to understand here that the api server is refusing the node update because of the fields ExternalIP and InternalIP are mixed out of order. Is this correct?

Is there anything I can do to workaround or fix this?

Kind regards.

```
$ kubectl version

Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3+coreos.0"", GitCommit:""42de91f04e456f7625941a6c4aaedaa69708be1b"", GitTreeState:""clean"", BuildDate:""2017-08-07T19:44:31Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3+coreos.0"", GitCommit:""42de91f04e456f7625941a6c4aaedaa69708be1b"", GitTreeState:""clean"", BuildDate:""2017-08-07T19:44:31Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```",closed,False,2017-09-19 21:19:20,2019-01-16 14:26:51
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/2762,https://api.github.com/repos/kubernetes/contrib/issues/2762,"metadata-proxy: fix build problems, bump dependencies, bump version to 0.1.4","Without `--no-cache`, the `apt-get update` commands were being cached and not updating properly.",closed,True,2017-09-20 17:15:42,2017-10-12 22:43:26
contrib,hugocf,https://github.com/kubernetes/contrib/issues/2763,https://api.github.com/repos/kubernetes/contrib/issues/2763,Lua failure in custom backed for custom error pages: aledbf/nginx-error-server:0.6,"I tried following @aledbf’s suggestions in issue https://github.com/kubernetes/contrib/issues/2285#issuecomment-271963070 about how to host custom html error pages via a custom default backend, but the backend is *blowing up* with a `lua` error causing the backend pod to restart.

```
2017/09/21 11:36:46 [error] 6#6: *6713 lua subrequests cycle while processing ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:36:46 [error] 6#6: *6713 lua entry thread aborted: runtime error: content_by_lua(nginx.conf:63):5: failed to issue subrequest: -1
stack traceback:
coroutine 0:
	[C]: in function 'capture'
	content_by_lua(nginx.conf:63):5: in function <content_by_lua(nginx.conf:63):1>, client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
```
*(see full error log below)*

Any idea what might be going on?

I’m using the pod image [`aledbf/nginx-error-server:0.6`](https://hub.docker.com/r/aledbf/nginx-error-server/tags/) but I’m not sure where is the repo corresponding to the latest version `0.6`.

The following repo seems to be related to an older version (not updated since 11 months ago and indicates versin `0.3` in the `Makefile`): https://github.com/aledbf/contrib/tree/nginx-debug-server/images/nginx-error-server

<details><summary>full error log — aledbf/nginx-error-server:0.6</summary>

```
172.20.56.192 - [21/Sep/2017:11:36:25 +0000] ""GET /healthz HTTP/1.1"" ""-.-"" 200 ""-"" ""Go-http-client/1.1"" 122 0.000
172.20.56.192 - [21/Sep/2017:11:36:35 +0000] ""GET /healthz HTTP/1.1"" ""-.-"" 200 ""-"" ""Go-http-client/1.1"" 122 0.000
172.20.56.192 - [21/Sep/2017:11:36:45 +0000] ""GET /healthz HTTP/1.1"" ""-.-"" 200 ""-"" ""Go-http-client/1.1"" 122 0.000
2017/09/21 11:36:46 [error] 6#6: *6713 lua subrequests cycle while processing ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:36:46 [error] 6#6: *6713 lua entry thread aborted: runtime error: content_by_lua(nginx.conf:63):5: failed to issue subrequest: -1
stack traceback:
coroutine 0:
	[C]: in function 'capture'
	content_by_lua(nginx.conf:63):5: in function <content_by_lua(nginx.conf:63):1>, client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:36:46 [error] 6#6: *6713 lua subrequests cycle while processing ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/5xx.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:36:46 [error] 6#6: *6713 lua entry thread aborted: runtime error: content_by_lua(nginx.conf:63):5: failed to issue subrequest: -1
stack traceback:
coroutine 0:
	[C]: in function 'capture'
	content_by_lua(nginx.conf:63):5: in function <content_by_lua(nginx.conf:63):1>, client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/5xx.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:36:46 [error] 6#6: *6713 lua subrequests cycle while processing ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:36:46 [error] 6#6: *6713 lua entry thread aborted: runtime error: content_by_lua(nginx.conf:63):5: failed to issue subrequest: -1
stack traceback:
coroutine 0:
	[C]: in function 'capture'
	content_by_lua(nginx.conf:63):5: in function <content_by_lua(nginx.conf:63):1>, client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:36:46 [error] 6#6: *6713 lua entry thread aborted: runtime error: content_by_lua(nginx.conf:63):5: failed to issue subrequest: -1
stack traceback:
coroutine 0:
	[C]: in function 'capture'
	content_by_lua(nginx.conf:63):5: in function <content_by_lua(nginx.conf:63):1>, client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/5xx.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""

.
.
. Goes on and on for hundreds of lines
.
.

2017/09/21 11:36:46 [error] 6#6: *6713 lua subrequests cycle while processing ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:36:46 [error] 6#6: *6713 lua entry thread aborted: runtime error: content_by_lua(nginx.conf:63):5: failed to issue subrequest: -1
stack traceback:
coroutine 0:
	[C]: in function 'capture'
	content_by_lua(nginx.conf:63):5: in function <content_by_lua(nginx.conf:63):1>, client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:36:46 [error] 6#6: *6713 lua subrequests cycle while processing ""/503.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/5xx.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:36:46 [error] 6#6: *6713 lua entry thread aborted: runtime error: content_by_lua(nginx.conf:63):5: failed to issue subrequest: -1
stack traceback:
coroutine 0:
	[C]: in function 'capture'
	content_by_lua(nginx.conf:63):5: in function <content_by_lua(nginx.conf:63):1>, client: 100.96.3.135, server: , request: ""GET / HTTP/1.1"", subrequest: ""/5xx.text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"", host: ""router.dev.sandbox.example.com""
2017/09/21 11:37:18 [alert] 1#1: worker process 6 exited on signal 9
```
</details>
",closed,False,2017-09-21 13:06:03,2018-03-12 02:13:32
contrib,jasonbrooks,https://github.com/kubernetes/contrib/pull/2764,https://api.github.com/repos/kubernetes/contrib/issues/2764,[ansible] support installation via system containers,"This PR provides the option of installing flannel, etcd, the and the kube master and node components using system containers.

@ingvagabund ",closed,True,2017-09-22 01:23:14,2017-09-26 21:17:47
contrib,dseeley-sky,https://github.com/kubernetes/contrib/pull/2765,https://api.github.com/repos/kubernetes/contrib/issues/2765,Allow vrrp v2 to be specified as an argument in the container specification,"Add ability to specify VRRP V2 (as opposed to default of 3) in the keepalived template. Current and previous keepalived releases have a problem with VRRP V3 when there are more than 2 nodes (https://github.com/acassen/keepalived/issues/642).  This has been fixed in master, but not part of a release yet.  Using VRRP V2 bypasses the issue.",closed,True,2017-09-22 11:48:59,2017-09-29 08:03:06
contrib,strigazi,https://github.com/kubernetes/contrib/pull/2766,https://api.github.com/repos/kubernetes/contrib/issues/2766,Remove api-servers from kubelet environment,"In kubernetes 1.8.x the kubelet parameter ""--api-servers"" has been
removed, remove it from from the systemd environment.",closed,True,2017-09-25 07:13:16,2017-10-02 12:58:09
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2767,https://api.github.com/repos/kubernetes/contrib/issues/2767,make golint happy,SSIA,closed,True,2017-09-26 14:39:39,2017-09-26 14:49:46
contrib,torvitas,https://github.com/kubernetes/contrib/issues/2768,https://api.github.com/repos/kubernetes/contrib/issues/2768,Firewall immediate settings do not work,"I tried to install this receipt on a fedora 26 cluster and noticed the firewall ports weren't opened as expected.

I already prepared a fix for this. MR incoming.",closed,False,2017-09-27 09:24:53,2017-10-04 07:57:47
contrib,torvitas,https://github.com/kubernetes/contrib/pull/2769,https://api.github.com/repos/kubernetes/contrib/issues/2769,"makes firewall settings really apply immediatly, fixes #2768",Fixes: #2768 ,closed,True,2017-09-27 09:29:46,2017-10-04 07:57:47
contrib,PureWhiteWu,https://github.com/kubernetes/contrib/pull/2770,https://api.github.com/repos/kubernetes/contrib/issues/2770,remove space between name and :,,closed,True,2017-09-28 08:03:58,2018-03-30 01:15:41
contrib,bowei,https://github.com/kubernetes/contrib/pull/2771,https://api.github.com/repos/kubernetes/contrib/issues/2771,Add bowei to the contrib OWNERS,Lets me unblock people who update flags,closed,True,2017-09-28 17:32:56,2017-09-28 17:43:53
contrib,hieuhc,https://github.com/kubernetes/contrib/issues/2772,https://api.github.com/repos/kubernetes/contrib/issues/2772,Error deploying kafka 0.11.0.1,"I change the version of Kafka to 0.11.0.1 as specified in [statefulsets/kafka](https://github.com/kubernetes/contrib/tree/master/statefulsets/kafka). The first pod pops up and runs fine but it stuck at second pod when the first can not talk to the second one. And because of that the third is not created.

`[2017-09-29 09:28:49,498] WARN [Controller-0-to-broker-1-send-thread]: Controller 0's connection to broker kafka-dev-1.kafka-svc-dev.timeseries.svc.cluster.local:9093 (id: 1 rack: null) was unsuccessful (kafka.controller.RequestSendThread)
java.io.IOException: Connection to kafka-dev-1.kafka-svc-dev.timeseries.svc.cluster.local:9093 (id: 1 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:68)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:264)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:218)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64)` 

I tried with Zookeeper 3.4.9 and 3.4.10 (which Kafka 0.11.0.1 is built on) and both yield same result.
Anyone has experienced this? I need this version of Kafka for schema registry server",closed,False,2017-09-29 09:49:25,2018-07-01 10:23:41
contrib,Pensu,https://github.com/kubernetes/contrib/pull/2773,https://api.github.com/repos/kubernetes/contrib/issues/2773,Making targets for multi-arch for zookeeper,This PR ensures multi-arch builds for zookeper.,closed,True,2017-10-05 10:28:47,2018-04-16 07:32:55
contrib,teeram,https://github.com/kubernetes/contrib/pull/2774,https://api.github.com/repos/kubernetes/contrib/issues/2774,Unifying the spacing of lines in the Dockerfile and zkGenConfig.sh.,,closed,True,2017-10-10 00:01:27,2017-10-10 00:52:27
contrib,teeram,https://github.com/kubernetes/contrib/pull/2775,https://api.github.com/repos/kubernetes/contrib/issues/2775,Unifying the alignment of text within Dockerfile zkGenConfig.sh,,closed,True,2017-10-10 00:49:56,2017-10-17 22:56:00
contrib,Anthony25,https://github.com/kubernetes/contrib/issues/2776,https://api.github.com/repos/kubernetes/contrib/issues/2776,[Keepalived VIP] Handle IPv6,"Hi,
Keepalived VIP does not seem to handle IPv6 yet. Even if kubernetes internally does not support it, as Keepalived VIP uses the host network, it would be great to provide IPv6 access.

The issue I see right now is, as you use configmap keys to handle the VIPs, you cannot use directly an IPv6 (':' is forbidden). However, you could cheat by replacing ':' by either a dot or a dash. It is not nice, but it would work by keeping the same design as now.",closed,False,2017-10-11 00:30:28,2019-01-04 04:39:32
contrib,mwasilew2,https://github.com/kubernetes/contrib/issues/2777,https://api.github.com/repos/kubernetes/contrib/issues/2777,[Ansible] deploy-cluster.sh script points to wrong inventory file,"ansible/scripts/deploy-cluster.sh is pointing to a file called inventory but it doesn't exist

it should either be pointing to inventory=${INVENTORY:-${INVENTORY_DIR}/localhost.ini}  or the localhost.ini file should be renamed to inventory

happy to submit a PR, can someone let me know which file name is the intended one?",closed,False,2017-10-12 07:55:52,2018-03-13 09:44:37
contrib,ihmccreery,https://github.com/kubernetes/contrib/pull/2778,https://api.github.com/repos/kubernetes/contrib/issues/2778,"Remove metadata-proxy, add README with new location",Fixes #2750. Ref kubernetes/kubernetes#8867.,closed,True,2017-10-12 20:03:42,2017-10-12 20:57:30
contrib,darkprisco,https://github.com/kubernetes/contrib/issues/2779,https://api.github.com/repos/kubernetes/contrib/issues/2779,Kafka restarting continuosly,"Hi guys,
 I have deployed zookeper and kafka as it is, without changing.. but I am facing many problems of restarts after some days.. 


```
Caused by: kafka.common.StateChangeFailedException: No other replicas in ISR 0 for [licenseServer,21] besides shutting down brokers 0
	at kafka.controller.ControlledShutdownLeaderSelector.selectLeader(PartitionLeaderSelector.scala:191)
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:339)
	... 22 more
[2017-10-17 14:06:11,642] INFO [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [event,9] (kafka.controller.PartitionStateMachine)
[2017-10-17 14:06:11,642] TRACE Controller 0 epoch 3 started leader election for partition [event,9] (state.change.logger)
[2017-10-17 14:06:11,643] ERROR Controller 0 epoch 3 encountered error while electing leader for partition [event,9] due to: No other replicas in ISR 0,2 for [event,9] besides shutting down brokers 0. (state.change.logger)
[2017-10-17 14:06:11,643] ERROR Controller 0 epoch 3 initiated state change for partition [event,9] from OnlinePartition to OnlinePartition failed (state.change.logger)
kafka.common.StateChangeFailedException: encountered error while electing leader for partition [event,9] due to: No other replicas in ISR 0,2 for [event,9] besides shutting down brokers 0.
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:362)
	at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:202)
	at kafka.controller.PartitionStateMachine$$anonfun$handleStateChanges$2.apply(PartitionStateMachine.scala:141)
	at kafka.controller.PartitionStateMachine$$anonfun$handleStateChanges$2.apply(PartitionStateMachine.scala:140)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:94)
	at kafka.controller.PartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:140)
	at kafka.controller.KafkaController$$anonfun$shutdownBroker$3$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(KafkaController.scala:268)
	at kafka.controller.KafkaController$$anonfun$shutdownBroker$3$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(KafkaController.scala:263)
	at scala.Option.foreach(Option.scala:257)
	at kafka.controller.KafkaController$$anonfun$shutdownBroker$3$$anonfun$apply$1.apply$mcV$sp(KafkaController.scala:263)
	at kafka.controller.KafkaController$$anonfun$shutdownBroker$3$$anonfun$apply$1.apply(KafkaController.scala:263)
	at kafka.controller.KafkaController$$anonfun$shutdownBroker$3$$anonfun$apply$1.apply(KafkaController.scala:263)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:213)
	at kafka.controller.KafkaController$$anonfun$shutdownBroker$3.apply(KafkaController.scala:262)
	at kafka.controller.KafkaController$$anonfun$shutdownBroker$3.apply(KafkaController.scala:259)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:316)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972)
	at kafka.controller.KafkaController.shutdownBroker(KafkaController.scala:259)
	at kafka.server.KafkaApis.handleControlledShutdownRequest(KafkaApis.scala:224)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:87)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:62)
	at java.lang.Thread.run(Thread.java:745)
Caused by: kafka.common.StateChangeFailedException: No other replicas in ISR 0,2 for [event,9] besides shutting down brokers 0
	at kafka.controller.ControlledShutdownLeaderSelector.selectLeader(PartitionLeaderSelector.scala:191)
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:339)
	... 22 more
```


Shall I do some optimizations to have zk and kafka running without problems?
Tks
Prisco",closed,False,2017-10-17 14:19:57,2018-03-16 16:00:46
contrib,chhetripradeep,https://github.com/kubernetes/contrib/pull/2780,https://api.github.com/repos/kubernetes/contrib/issues/2780,[zookeeper] 3.4.9 -> 3.4.10,"Hi,

zookeeper `3.4.10` has been released as the latest stable. Many frameworks[0] & datastores[1] using zk has upgraded their client library to `3.4.10`, hence it will be good to build an k8szk image of 3.4.10. This PR adds the required changes.

[0] - https://github.com/lagom/lagom/pull/986
[1] - https://github.com/apache/kafka/commit/c2d17c830ca309a4e3989981a47790209955273c",closed,True,2017-10-20 08:33:44,2017-10-27 18:47:48
contrib,bingoarun,https://github.com/kubernetes/contrib/issues/2781,https://api.github.com/repos/kubernetes/contrib/issues/2781,[Ansible] deploy-cluster.sh script fails,"The following task fails

```
TASK [master : Start apiserver] *******************************************************************************************************************************************************************************
fatal: [x.x.x.x]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Unable to start service kube-apiserver: Job for kube-apiserver.service failed because the control process exited with error code. See \""systemctl status kube-apiserver.service\"" and \""journalctl -xe\"" for details.\n""}
```
```
root@kube-master:~# systemctl status kube-apiserver
● kube-apiserver.service - Kubernetes API Server
   Loaded: loaded (/etc/systemd/system/kube-apiserver.service; enabled; vendor preset: enabled)
   Active: inactive (dead) (Result: exit-code) since Sat 2017-10-21 12:21:49 UTC; 2min 41s ago
     Docs: https://github.com/GoogleCloudPlatform/kubernetes
  Process: 9313 ExecStart=/usr/bin/kube-apiserver $KUBE_LOGTOSTDERR $KUBE_LOG_LEVEL $KUBE_ETCD_SERVERS $KUBE_API_ADDRESS $KUBE_API_PORT $KUBELET_PORT $KUBE_ALLOW_PRIV $KUBE_SERVICE_ADDRESSES $KUBE_ADMISSION_
 Main PID: 9313 (code=exited, status=255)

Oct 21 12:21:49 kube-master systemd[1]: kube-apiserver.service: Main process exited, code=exited, status=255/n/a
Oct 21 12:21:49 kube-master systemd[1]: Failed to start Kubernetes API Server.
Oct 21 12:21:49 kube-master systemd[1]: kube-apiserver.service: Unit entered failed state.
Oct 21 12:21:49 kube-master systemd[1]: kube-apiserver.service: Failed with result 'exit-code'.
Oct 21 12:21:49 kube-master systemd[1]: kube-apiserver.service: Service hold-off time over, scheduling restart.
Oct 21 12:21:49 kube-master systemd[1]: Stopped Kubernetes API Server.
Oct 21 12:21:49 kube-master systemd[1]: kube-apiserver.service: Start request repeated too quickly.
Oct 21 12:21:49 kube-master systemd[1]: Failed to start Kubernetes API Server.
```

From the syslog, I can see the following error

```
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: I1021 12:21:49.570952    9313 trace.go:61] Trace ""List *extensions.DaemonSetList"" (started 2017-10-21 12:21:48.832890296 +0000 UTC):
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: [7.283µs] [7.283µs] About to list etcd node
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: [737.955967ms] [737.948684ms] Etcd node listed
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: [738.036773ms] [80.806µs] Node list decoded
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: [738.042293ms] [5.52µs] END
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: I1021 12:21:49.571596    9313 trace.go:61] Trace ""List *rbac.ClusterRoleList"" (started 2017-10-21 12:21:48.90198477 +0000 UTC):
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: [3.151µs] [3.151µs] About to list etcd node
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: [669.579277ms] [669.576126ms] Etcd node listed
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: [669.586567ms] [7.29µs] Node list decoded
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: [669.592325ms] [5.758µs] END
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: I1021 12:21:49.589291    9313 serve.go:93] Serving securely on 0.0.0.0:443
Oct 21 12:21:49 cs-ubuntu-16 kube-apiserver[9313]: F1021 12:21:49.589582    9313 genericapiserver.go:195] failed to listen on 0.0.0.0:443: listen tcp 0.0.0.0:443: bind: permission denied
Oct 21 12:21:49 cs-ubuntu-16 systemd[1]: kube-apiserver.service: Main process exited, code=exited, status=255/n/a
```

The whole installation is performed as a root user (ansible_ssh_user=root). 

",closed,False,2017-10-21 12:30:40,2018-03-23 11:41:52
contrib,abh,https://github.com/kubernetes/contrib/issues/2782,https://api.github.com/repos/kubernetes/contrib/issues/2782,beta.15: duplicate host in Host header,"[whoops, wrong issue tracker]",closed,False,2017-10-26 06:06:21,2018-03-25 07:24:52
contrib,porridge,https://github.com/kubernetes/contrib/pull/2783,https://api.github.com/repos/kubernetes/contrib/issues/2783,Improve graph visibility and minor cleanups.,"When adding timeseries to graph, start with higher percentiles, since their
values are usually strictly higher than lower percentiles, which avoids
obscuring graph data.

It also orders data in the onHover labels more naturally.

Swap grey colors for a more visible 90th percentile.

Add .gitignore, missing whitespace and newlines.",closed,True,2017-10-26 12:24:52,2017-11-06 17:51:25
contrib,metal3d,https://github.com/kubernetes/contrib/issues/2784,https://api.github.com/repos/kubernetes/contrib/issues/2784,Bad test in peer-finder for newPeers.Equal(),"Hi,

at https://github.com/kubernetes/contrib/blob/master/peer-finder/peer-finder.go#L141 the double test is not clear and gives a bad information. If the peer list has not changed, the message says that peer-finder doisen't find ""my hostname"" in list.

We reuse peer-finder at https://github.com/Smile-SA/dagota/blob/master/dagota/main.go#L190-L198 in our project and made that test:

```go
		if newPeers.Equal(peers) {
			// no new peer
			continue
		}

		if !newPeers.Has(myName) {
			log.Printf(""Have not found myself in list yet.\nMy Hostname: %s\nHosts in list: %s"", myName, strings.Join(newPeers.List(), "", ""))
			continue
		}
```

Are you ok with that and should I sent a PR ?

BTW: peer-finder is a blessing for several of our projects, thanks !",closed,False,2017-10-28 09:42:08,2018-03-27 12:16:51
contrib,pahaz,https://github.com/kubernetes/contrib/pull/2785,https://api.github.com/repos/kubernetes/contrib/issues/2785,Use {{ bin_dir }}/etcdctl instead hardcoded /usr/bin/etcdctl,"We have an inconsistency in symbolic links creation. 

1. https://github.com/kubernetes/contrib/blob/master/ansible/roles/flannel/tasks/github-release.yml#L26
```
    dest: ""{{ bin_dir }}/{{ item }}""
```

2. https://github.com/kubernetes/contrib/blob/master/ansible/roles/master/tasks/download_bins.yml#L33
```
    dest: ""{{ bin_dir }}/{{ item }}""
```

And we have hardcoded `bin_dir` in `etcd-install-github-release.yml`: 
```
    dest: /usr/bin/{{ item }}
```
",closed,True,2017-10-29 19:37:26,2017-11-10 13:20:27
contrib,guenhter,https://github.com/kubernetes/contrib/pull/2786,https://api.github.com/repos/kubernetes/contrib/issues/2786,Add deprecation warning for ansible project,The https://github.com/kubernetes-incubator/kubespray seems to be the successor of the `ansible`-folder. So mark this folder as deprecated and link to the new one.,closed,True,2017-10-30 13:24:58,2017-10-30 14:53:03
contrib,porridge,https://github.com/kubernetes/contrib/pull/2787,https://api.github.com/repos/kubernetes/contrib/issues/2787,"Add myself and Shyam, remove Brendan.",Brendan seems to not have touched this repo for well over a year.,closed,True,2017-10-30 16:24:18,2017-11-06 17:51:25
contrib,porridge,https://github.com/kubernetes/contrib/pull/2788,https://api.github.com/repos/kubernetes/contrib/issues/2788,Make clicks work again.,,closed,True,2017-10-30 16:25:20,2017-11-06 17:51:25
contrib,porridge,https://github.com/kubernetes/contrib/pull/2789,https://api.github.com/repos/kubernetes/contrib/issues/2789,Print a message when data is loaded.,,closed,True,2017-10-30 16:27:09,2017-11-06 17:51:25
contrib,porridge,https://github.com/kubernetes/contrib/pull/2790,https://api.github.com/repos/kubernetes/contrib/issues/2790,Show resource usage data.,,closed,True,2017-10-30 16:27:44,2017-11-08 11:18:25
contrib,humectant,https://github.com/kubernetes/contrib/issues/2791,https://api.github.com/repos/kubernetes/contrib/issues/2791,system_container.yml fails in Vagrant Deployer for Kubernetes Ansible,"Following the instructions in Vagrant Deployer for Kubernetes Ansible I get the following:

ERROR! no action detected in task. This often indicates a misspelled module name, or incorrect module path.

The error appears to have been in '/Users/waxberg/k8s-install-contrib-master/ansible/roles/etcd/tasks/system_container.yml': line 6, column 3, but may
be elsewhere in the file depending on the exact syntax problem.

- name: Install or Update Etcd system container package
  ^ here

The offending bit appears to oc_atomic_container:

- name: Install or Update Etcd system container package
  oc_atomic_container:
    name: etcd
    image: ""{{ etcd_system_image }}""
    state: latest

",closed,False,2017-11-02 22:16:35,2018-02-07 15:20:14
contrib,porridge,https://github.com/kubernetes/contrib/pull/2792,https://api.github.com/repos/kubernetes/contrib/issues/2792,Open links in new tab.,,closed,True,2017-11-07 15:08:57,2017-11-08 11:18:25
contrib,porridge,https://github.com/kubernetes/contrib/pull/2793,https://api.github.com/repos/kubernetes/contrib/issues/2793,Add a way to trim outliers.,"This is important for graphs where outliers are more than an order of
magnitude higher than the baseline, and as a result make the whole graph
useless.

/cc @shyamjvs",closed,True,2017-11-08 11:07:20,2017-11-08 11:23:31
contrib,thockin,https://github.com/kubernetes/contrib/pull/2794,https://api.github.com/repos/kubernetes/contrib/issues/2794,Update README for micro-demos,"Also remove kubectl script, which should not have been committed.",closed,True,2017-11-08 17:37:36,2017-11-13 05:01:05
contrib,xiaods,https://github.com/kubernetes/contrib/issues/2795,https://api.github.com/repos/kubernetes/contrib/issues/2795,contrib/pets/mysql/galera  not up to date,origin source is here:  https://github.com/openshift/origin/issues/17211,closed,False,2017-11-09 06:28:40,2018-04-08 21:08:53
contrib,porridge,https://github.com/kubernetes/contrib/pull/2796,https://api.github.com/repos/kubernetes/contrib/issues/2796,Add more data sources and paralellize fetching.,,closed,True,2017-11-10 17:43:30,2017-12-20 09:36:18
contrib,porridge,https://github.com/kubernetes/contrib/pull/2797,https://api.github.com/repos/kubernetes/contrib/issues/2797,Add test phases graph and a TODO.,,closed,True,2017-11-13 13:51:17,2017-11-13 15:46:00
contrib,beatlejuse,https://github.com/kubernetes/contrib/issues/2798,https://api.github.com/repos/kubernetes/contrib/issues/2798,keepalived-vip / cyclic restart,"I have this problem: https://youtu.be/gj8x2_D_G-k
after some time (a few days) working fine, cluster IP started to work intermittently.
this is evident in unstable pings.
in the logs the nodes and the logs of the containers kube-keepalived-vip seen that happen continually re-election of the master. however, it is not migrated to another node, but the cluster IP to the moment it becomes inaccessible.
redeploy does not change the situation.
Configmap is:
```
             apiVersion: v1
              kind: ConfigMap
              metadata:
                name: vip-configmap
                namespace: vip-system
              data:
                10.1.29.10: default/kubernetes
```",closed,False,2017-11-13 14:31:42,2017-12-07 10:00:51
contrib,porridge,https://github.com/kubernetes/contrib/pull/2799,https://api.github.com/repos/kubernetes/contrib/issues/2799,"New release, with adjusted deployment.","The newly added tests increase memory consumption, so bump resource
limits.
Also, the large /api responses make health checking unhappy, so change
to /.",closed,True,2017-11-13 19:18:01,2017-12-20 08:31:57
contrib,harmjanblok,https://github.com/kubernetes/contrib/pull/2800,https://api.github.com/repos/kubernetes/contrib/issues/2800,[keepalived-vip] Document workaround for running `modprobe ip_vs` on CentOS,"When running `keepalived-vip:0.11` on a CentOS 7 the Pod keeps crashing with the following error:

```
F1115 10:26:38.489907       1 main.go:108] unexpected error: exit status 1
goroutine 1 [running]:
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.stacks(0xc4201fdf00, 0xc420091130, 0x4b, 0xa1)
        /usr/local/google/home/mikedanese/go/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:766 +0xa7
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).output(0x1e2a9c0, 0xc400000003, 0xc420090fd0, 0x1d9086d, 0x7, 0x6c, 0x0)
        /usr/local/google/home/mikedanese/go/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:717 +0x348
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).printf(0x1e2a9c0, 0xc400000003, 0x157a081, 0x14, 0xc42057dee8, 0x1, 0x1)
        /usr/local/google/home/mikedanese/go/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:655 +0x14f
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.Fatalf(0x157a081, 0x14, 0xc42057dee8, 0x1, 0x1)
        /usr/local/google/home/mikedanese/go/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.main()
        /usr/local/google/home/mikedanese/go/src/k8s.io/contrib/keepalived-vip/main.go:108 +0x32e
```

Diving in deeper the issue seems to be related to the `modprobe ip_vs` from https://github.com/kubernetes/contrib/blob/8be3a141fd0d753ddcd8cbc23b74cbdd430394ba/keepalived-vip/utils.go#L232

When running this manually inside the container this returns the following error:
```
root@172:/# modprobe ip_vs; echo $?
modprobe: ERROR: could not insert 'ip_vs': Exec format error
1
```
I don't know what exactly causes this error, but it seems to be an incompatibility between the 'container' modprobe and the 'host' modprobe:

```
# container
root@172:/# uname -a
Linux 172.28.128.30 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
root@172:/# modprobe --version
kmod version 22
-XZ -ZLIB -EXPERIMENTAL 
```

```
# host
[root@172 ~]# modprobe --version
kmod version 20
```

So I've tried this workaround using an `initContainer`, which actually works as expected.",open,True,2017-11-15 13:02:22,2019-04-04 21:15:51
contrib,code0x9,https://github.com/kubernetes/contrib/pull/2801,https://api.github.com/repos/kubernetes/contrib/issues/2801,make zk gpg signing future-proof,,closed,True,2017-11-16 10:07:07,2017-11-16 10:08:06
contrib,sebbonnet,https://github.com/kubernetes/contrib/pull/2802,https://api.github.com/repos/kubernetes/contrib/issues/2802,Fix rescheduler docker tag push target,"Fix typo in makefile, so that the default `gcr.io/google-containers/rescheduler:v0.3.2` image is available, not just `gcr.io/google-containers/rescheduler-amd64:v0.3.2`.
",closed,True,2017-11-21 16:10:34,2017-12-12 16:51:31
contrib,niels-s,https://github.com/kubernetes/contrib/issues/2803,https://api.github.com/repos/kubernetes/contrib/issues/2803,scratch-debugger/debug.sh doesn't work anymore,"The `scratch-debugger/debug.sh` script doesn't work anymore, I tried applying the fix suggested in #2728 but without any luck. 
It seems like it doesn't find the docker socket on the host node:

```
sh: /mnt/rootfs/usr/bin/docker: not found
```

I'm running k8s version 1.8.3 on GKE

Besides that the README.md is also outdated to pass along a namespace or a container name it you should use flags instead of positional params but that's a separate issue.",closed,False,2017-11-22 16:32:12,2018-09-25 22:55:56
contrib,shahaf600,https://github.com/kubernetes/contrib/issues/2804,https://api.github.com/repos/kubernetes/contrib/issues/2804,have to run makecache when repo is old,"I had to run ""yum makecache"" on all of the target hosts, so if it will be automatic it would be nice",closed,False,2017-11-25 11:15:55,2018-04-24 12:46:52
contrib,thockin,https://github.com/kubernetes/contrib/pull/2805,https://api.github.com/repos/kubernetes/contrib/issues/2805,Add a demo for init containers,@grodrigues3 ,closed,True,2017-11-28 01:05:48,2017-12-01 22:05:26
contrib,xinxiaogang,https://github.com/kubernetes/contrib/pull/2806,https://api.github.com/repos/kubernetes/contrib/issues/2806,Tolerance data populating error,"If there is error during data populating, we need to skip the wrong build and continue on the next.",closed,True,2017-11-28 23:39:00,2018-06-24 19:49:30
contrib,xingwangc,https://github.com/kubernetes/contrib/issues/2807,https://api.github.com/repos/kubernetes/contrib/issues/2807,"[ansible]: flag ""--api-version"" already be removed from kubectl, but it is stilled used in kube-addon-update.sh which will cause kube-addons.sh failed","As title. I did not check the flag ""--api-version"" was removed from kubectl through which version . But I checked the specification of Kubernetes 1.7, it is not there. I used contrib/ansible to deploy local build 1.7.3 release. Below code in kube-addon-update.sh caused kube-addons.sh complete with error.

`function get-addon-nsnames-from-server() {
    local -r obj_type=$1
    ""${KUBECTL}"" get ""${obj_type}"" --all-namespaces -o go-template=""{{range.items}}{{.metadata.namespace}}/{{.metadata.name}} {{end}}"" --api-version=v1 -l kubernetes.io/cluster-service=true
}`

Error reported as:

`kube-addons.sh[28154]: Error: unknown flag: --api-version`

Fixed:

Remove **--api-version=v1** can fix the issue",closed,False,2017-11-30 06:55:12,2018-02-28 08:02:17
contrib,dkirrane,https://github.com/kubernetes/contrib/issues/2808,https://api.github.com/repos/kubernetes/contrib/issues/2808,keepalived-vip example for nginx,Once `kube-keepalived-vip` has started how does it dish out the virtual IP to NginX Ingress controller service? Any example available?,closed,False,2017-12-05 22:49:49,2018-05-05 00:54:06
contrib,lotheovian,https://github.com/kubernetes/contrib/pull/2809,https://api.github.com/repos/kubernetes/contrib/issues/2809,Update rewrite rule,,closed,True,2017-12-09 15:49:30,2018-05-08 17:21:08
contrib,kivoli,https://github.com/kubernetes/contrib/pull/2810,https://api.github.com/repos/kubernetes/contrib/issues/2810,Extend kube-keepalived-vip readme,"Add a list of parameters supported by kube-keepalived-vip to its README.

Also closes #2657 by adding a note pointing out that, to use services in other namespaces, the right parameter has to be set.",closed,True,2017-12-11 09:02:39,2018-11-29 09:14:13
contrib,mbovo,https://github.com/kubernetes/contrib/issues/2811,https://api.github.com/repos/kubernetes/contrib/issues/2811,etcd installation fails on Fedora Atomic 27,"As in title the installation of etcd fails at the enabling etcd service step on `ansible/roles/etcd/tasks/etcd-start.yml `
The error is the following:
```

TASK [etcd : include] *********************************************************************************************
included: /home/manuel/prjs/k8/contrib/ansible/roles/etcd/tasks/etcd-start.yml for 10.10.10.82, 10.10.10.83

TASK [etcd : Enable etcd] *****************************************************************************************
fatal: [10.10.10.83]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Could not find the requested service etcd: host""}
fatal: [10.10.10.82]: FAILED! => {""changed"": false, ""failed"": true, ""msg"": ""Could not find the requested service etcd: host""}
```

The problem is, starting from fedora atomic 27 kubernetes and etcd are no longer included in the base image (https://www.projectatomic.io/blog/2017/11/migrating-kubernetes-on-fedora-atomic-host-27/)

**A possible workaround:**
` atomic install --system --system-package=no --storage=ostree --name etcd registry.fedoraproject.org/f27/etcd`
on every host where you plan to install etcd

I'll investigate further if there are other issues with this specific os version (in order to submit a pr)",closed,False,2017-12-15 14:04:00,2018-08-31 00:09:55
contrib,porridge,https://github.com/kubernetes/contrib/issues/2812,https://api.github.com/repos/kubernetes/contrib/issues/2812,Automate perfdash configuration,"In https://github.com/kubernetes/contrib/pull/2796#pullrequestreview-76062138 @shyamjvs suggested that having names of sig-scalability CI jobs hard-coded in `perfdash/config.go` doesn't seem too clean.

If we could add some additional metadata to https://github.com/kubernetes/test-infra/blob/master/jobs/config.json then perf-dash could just periodically fetch the configuration from that file (via raw.githubusercontent.com)

What do @kubernetes/sig-testing-misc folks think about adding a `perfDashName` attribute (or something like it) to selected entries in `jobs.config`, and making perf-dash depend on that file?",closed,False,2017-12-20 09:31:45,2018-01-12 10:48:22
contrib,spiffxp,https://github.com/kubernetes/contrib/pull/2813,https://api.github.com/repos/kubernetes/contrib/issues/2813,Add code-of-conduct.md,"Refer to kubernetes/community as authoritative source for code of conduct

ref: kubernetes/community#1527",closed,True,2017-12-20 19:06:35,2018-01-06 00:13:18
contrib,thockin,https://github.com/kubernetes/contrib/pull/2814,https://api.github.com/repos/kubernetes/contrib/issues/2814,Convert registry to k8s.gcr.io,"This PR was auto-generated.  Please apply human expertise to review for correctness.

Followup to https://github.com/kubernetes/kubernetes/pull/54174 and https://github.com/kubernetes/kubernetes/pull/57824

xref https://github.com/kubernetes/release/issues/281",closed,True,2017-12-22 18:00:25,2018-02-02 04:47:25
contrib,liqlin2015,https://github.com/kubernetes/contrib/issues/2815,https://api.github.com/repos/kubernetes/contrib/issues/2815,[Keepalived VIP] keepalived keeps restarting,"I have followed the example to set up keepalived-vip daemonset. But got following error

```
VRRP child process(6794) died: Respawning
Starting VRRP child process, pid=6796
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Opening file '/etc/keepalived/keepalived.conf'.
pid 6796 exited due to segmentation fault (SIGSEGV).
  Please report a bug at https://github.com/acassen/keepalived/issues
  and include this log from when keepalived started, what happened
  immediately before the crash, and your configuration file.
VRRP child process(6796) died: Respawning
Starting VRRP child process, pid=6798
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Opening file '/etc/keepalived/keepalived.conf'.
pid 6798 exited due to segmentation fault (SIGSEGV).
  Please report a bug at https://github.com/acassen/keepalived/issues
  and include this log from when keepalived started, what happened
  immediately before the crash, and your configuration file.
VRRP child process(6798) died: Respawning
Starting VRRP child process, pid=6800
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Opening file '/etc/keepalived/keepalived.conf'.
pid 6800 exited due to segmentation fault (SIGSEGV).
  Please report a bug at https://github.com/acassen/keepalived/issues
  and include this log from when keepalived started, what happened
  immediately before the crash, and your configuration file.
VRRP child process(6800) died: Respawning
```

This is the vip configmap,

```
# kubectl --namespace=kube-system get configmap vip-configmap -o yaml
apiVersion: v1
data:
  192.168.38.3: demo/nginx
  192.168.38.4: demo/tomcat
  192.168.38.5: demo/httpd
kind: ConfigMap
metadata:
  annotations:
    k8s.co/cloud-provider-config: '{""services"":[{""uid"":""d2cce28b-e5fd-11e7-889e-005056adb12a"",""ip"":""192.168.38.1""},{""uid"":""bb01f0ca-e610-11e7-889e-005056adb12a"",""ip"":""192.168.38.2""},{""uid"":""7c748d66-e632-11e7-889e-005056adb12a"",""ip"":""192.168.38.3""},{""uid"":""436b64ee-e9ea-11e7-889e-005056adb12a"",""ip"":""192.168.38.4""},{""uid"":""e6b45cf8-ea42-11e7-889e-005056adb12a"",""ip"":""192.168.38.5""}]}'
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiVersion"":""v1"",""data"":null,""kind"":""ConfigMap"",""metadata"":{""annotations"":{},""name"":""vip-configmap"",""namespace"":""kube-system""}}
  creationTimestamp: 2017-12-21T03:16:33Z
  name: vip-configmap
  namespace: kube-system
  resourceVersion: ""1059007""
  selfLink: /api/v1/namespaces/kube-system/configmaps/vip-configmap
  uid: 5898e43e-e5fd-11e7-8187-005056adb12a
```

And keepalived-vip daemonset,

```
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  labels:
    name: kube-keepalived-vip
  name: kube-keepalived-vip
  namespace: kube-system
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: kube-keepalived-vip
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: kube-keepalived-vip
    spec:
      containers:
      - args:
        - --services-configmap=kube-system/vip-configmap
        - --watch-all-namespaces=true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        image: gcr.io/google_containers/kube-keepalived-vip:0.9
        imagePullPolicy: IfNotPresent
        name: kube-keepalived-vip
        resources: {}
        securityContext:
          privileged: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /lib/modules
          name: modules
          readOnly: true
        - mountPath: /dev
          name: dev
      dnsPolicy: ClusterFirst
      hostNetwork: true
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - hostPath:
          path: /lib/modules
          type: """"
        name: modules
      - hostPath:
          path: /dev
          type: """"
        name: dev
  templateGeneration: 2
  updateStrategy:
    type: OnDelete
```",closed,False,2017-12-27 02:39:13,2018-07-04 03:27:43
contrib,sai-li,https://github.com/kubernetes/contrib/pull/2816,https://api.github.com/repos/kubernetes/contrib/issues/2816,Correct a typo,Correct a typo from kubeclt to kubectl,closed,True,2017-12-28 03:59:06,2018-05-27 05:36:04
contrib,deathcoder,https://github.com/kubernetes/contrib/issues/2817,https://api.github.com/repos/kubernetes/contrib/issues/2817,SSL passthrough,"Hi,
I'm trying to setup secure docker registry inside a kubernetes cluster with self-signed certificates, and im trying to test it on minikube
my end goal is to be able to do a `docker push  kube-registry.com/hello-world`
this is my setup, the registry:
```
apiVersion: v1
kind: ReplicationController
metadata:
  name: kube-registry
  namespace: kube-system
  labels:
    k8s-app: kube-registry
    version: v0
# A minikube issue prevents the following from working
#    kubernetes.io/cluster-service: ""true""
spec:
  replicas: 1
  selector:
    k8s-app: kube-registry
    version: v0
  template:
    metadata:
      labels:
        k8s-app: kube-registry
        version: v0
# A minikube issue prevents the following from working
#        kubernetes.io/cluster-service: ""true""
    spec:
      containers:
      - name: registry
        image: registry:2
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        env:
        - name: REGISTRY_HTTP_ADDR
          value: :5000
        - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
          value: /var/lib/registry
        - name: REGISTRY_HTTP_TLS_CERTIFICATE
          value: /cert/ca.crt
        - name: REGISTRY_HTTP_TLS_KEY
          value: /key/domain.key
        - name: REGISTRY_LOG_LEVEL
          value: debug
        volumeMounts:
        - name: image-store
          mountPath: /var/lib/registry
        - name: registry-cert 
          mountPath: /cert/
          readOnly: true
        - name: registry-key 
          mountPath: /key/
          readOnly: true
        ports:
        - containerPort: 5000
          name: registry
          protocol: TCP
      volumes:
      - name: image-store
        persistentVolumeClaim:
          claimName: kube-registry-volume
      - name: registry-cert
        secret:
          secretName: registry-cert
      - name: registry-key
        secret:
          secretName: registry-key

```

and the service:
```
apiVersion: v1
kind: Service
metadata:
  name: kube-registry
  namespace: kube-system
  labels:
    k8s-app: kube-registry
    kubernetes.io/name: ""KubeRegistry""
spec:
  selector:
    k8s-app: kube-registry
  ports:
  - name: registry
    port: 5000
    nodePort: 31000
    protocol: TCP
  type: NodePort

```

i also generated the self-signed certificates, and created the relative volumes and config-maps and i edited my /etc/hosts

with this setup i am able to do `docker push  kube-registry.com:31000/hello-world`

the last step is to remove that port, and i started playing around with ingress and the solution with ssl passthrough seemed the right way to go so i did this:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kube-registry-ingress
  namespace: kube-system
  annotations:
    ingress.kubernetes.io/ssl-passthrough: ""true""
spec:
  rules:
  - host: kube-registry.com
    http:
      paths:
      - backend:
          serviceName: kube-registry
          servicePort: 5000
  tls:
  - hosts:
    - kube-registry.com
```

from my understanding ssl passthrough should not decrypt traffic on the ingress and just pass along the traffic to the service and back but `docker push  kube-registry.com/hello-world` gives me this error:

> Error response from daemon: Get https://kube-registry.com/v2/: x509: certificate is valid for ingress.local, not kube-registry.com

this is curl output with port (not going through ingress):
```
* Rebuilt URL to: https://kube-registry.com:31000/
*   Trying 192.168.99.100...
* TCP_NODELAY set
* Connected to kube-registry.com (192.168.99.100) port 31000 (#0)
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* ALPN, server accepted to use http/1.1
* Server certificate:
*  subject: C=GB; ST=Test State or Province; L=Test Locality; O=Local Secure Registry for Kubernetes; CN=kube-registry.com; emailAddress=test@email.address
*  start date: Dec 28 23:19:10 2017 GMT
*  expire date: Sep 19 23:19:10 2018 GMT
*  issuer: C=GB; ST=Test State or Province; L=Test Locality; O=Local Secure Registry for Kubernetes; CN=kube-registry.com; emailAddress=test@email.address
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
> GET / HTTP/1.1
> Host: kube-registry.com:31000
> User-Agent: curl/7.52.1
> Accept: */*
> 
< HTTP/1.1 200 OK
< Cache-Control: no-cache
< Date: Fri, 29 Dec 2017 00:31:18 GMT
< Content-Length: 0
< Content-Type: text/plain; charset=utf-8
< 
* Curl_http_done: called premature == 0
* Connection #0 to host kube-registry.com left intact
```

and without port (using ingress):
```
* Rebuilt URL to: https://kube-registry.com/
*   Trying 192.168.99.100...
* TCP_NODELAY set
* Connected to kube-registry.com (192.168.99.100) port 443 (#0)
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* ALPN, server accepted to use http/1.1
* Server certificate:
*  subject: O=Acme Co; CN=Kubernetes Ingress Controller Fake Certificate
*  start date: Dec 28 22:51:53 2017 GMT
*  expire date: Dec 28 22:51:53 2018 GMT
*  issuer: O=Acme Co; CN=Kubernetes Ingress Controller Fake Certificate
*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
> GET / HTTP/1.1
> Host: kube-registry.com
> User-Agent: curl/7.52.1
> Accept: */*
> 
< HTTP/1.1 200 OK
< Server: nginx/1.13.5
< Date: Fri, 29 Dec 2017 00:32:22 GMT
< Content-Type: text/plain; charset=utf-8
< Content-Length: 0
< Connection: keep-alive
< Cache-Control: no-cache
< Strict-Transport-Security: max-age=15724800; includeSubDomains;
< 
* Curl_http_done: called premature == 0
* Connection #0 to host kube-registry.com left intact
```

i don't understand why in the second case i get ingress's own certificate instead of the one from the registry. am i doing something wrong? 

i also have a [repository](https://github.com/deathcoder/registry-tooling) that sets up everything by using the script reg-tool.sh

ps: i know there is an addon that provides an unsecured registry and that for minikube it might not make much sense what i'm trying to do, i'm just using this little project to understand different components of kubernetes.
",closed,False,2017-12-29 00:43:25,2019-02-18 07:28:05
contrib,jstomphorst,https://github.com/kubernetes/contrib/issues/2818,https://api.github.com/repos/kubernetes/contrib/issues/2818,main.go:108] unexpected error: exit status 1,"Hi guys, 

My pod's are restarting with this error.
Kubernetes v1.9.0
Centos 7

goroutine 1 [running]:
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.stacks(0xc420236e00, 0xc42041a280, 0x4b, 0x9d)
        /usr/local/google/home/mikedanese/go/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:766 +0xa7
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).output(0x1e2a9c0, 0xc400000003, 0xc42008e420, 0x1d9086d, 0x7, 0x6c, 0x0)
        /usr/local/google/home/mikedanese/go/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:717 +0x348
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.(*loggingT).printf(0x1e2a9c0, 0xc400000003, 0x157a081, 0x14, 0xc420527ee8, 0x1, 0x1)
        /usr/local/google/home/mikedanese/go/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:655 +0x14f
k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog.Fatalf(0x157a081, 0x14, 0xc420527ee8, 0x1, 0x1)
        /usr/local/google/home/mikedanese/go/src/k8s.io/contrib/keepalived-vip/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.main()
        /usr/local/google/home/mikedanese/go/src/k8s.io/contrib/keepalived-vip/main.go:108 +0x32e
",closed,False,2018-01-04 15:40:04,2018-10-27 18:26:13
contrib,fernandohackbart,https://github.com/kubernetes/contrib/issues/2819,https://api.github.com/repos/kubernetes/contrib/issues/2819,kube-keepalived-vip: (vips): No VIP specified; at least one is required,"```
Starting Keepalived v1.2.24 (08/01,2017)
Opening file '/etc/keepalived/keepalived.conf'.
Starting Healthcheck child process, pid=19
Initializing ipvs
Starting VRRP child process, pid=21
Opening file '/etc/keepalived/keepalived.conf'.
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Opening file '/etc/keepalived/keepalived.conf'.
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Registering gratuitous ARP shared channel
Using LinkWatch kernel netlink reflector...
Got SIGHUP, reloading checker configuration
Initializing ipvs
Registering Kernel netlink reflector
Registering Kernel netlink command channel
Opening file '/etc/keepalived/keepalived.conf'.
Unable to initialise ipsets
Opening file '/etc/keepalived/keepalived.conf'.
Using LinkWatch kernel netlink reflector...
(vips): No VIP specified; at least one is required
Stopped
pid 21 exited with permanent error CONFIG. Terminating
Stopping
Stopped
Stopped Keepalived v1.2.24 (08/01,2017)
```
",closed,False,2018-01-08 19:13:45,2018-09-25 13:40:04
contrib,ahmetb,https://github.com/kubernetes/contrib/pull/2820,https://api.github.com/repos/kubernetes/contrib/issues/2820,scratch-debugger: Fix stderr redirection syntax,"This was causing a file named 'Pod failed:' to be created when debug.sh failed

  ",closed,True,2018-01-09 01:58:48,2018-01-09 22:58:07
contrib,ahmetb,https://github.com/kubernetes/contrib/pull/2821,https://api.github.com/repos/kubernetes/contrib/issues/2821,scratch-debugger: fix mounted path for socket,"The manifest file for the debug container makes the host's docker socket
available on /mnt/rootfs/var/run/docker.sock, but the script still tries to
use non-existing /run/docker.sock on the container's fs.

/cc @tallclair",closed,True,2018-01-09 02:03:06,2018-01-09 22:58:04
contrib,ahmetb,https://github.com/kubernetes/contrib/pull/2822,https://api.github.com/repos/kubernetes/contrib/issues/2822,scratch-container: allow customizing docker path,"scratch-container/debug.sh assumes the docker CLI on the guest OS will always
be at /usr/bin/docker. This is not the case in many distributions like Minikube
or Kubernetes on Docker (Linuxkit).

Making it customizable through HOST_DOCKERPATH variable.

This will conflict with PR #2821 as it makes edits to the same line. But I'll resolve them depending on which one gets merged first.

/cc @tallclair ",closed,True,2018-01-09 02:14:16,2018-01-09 22:59:35
contrib,ahmetb,https://github.com/kubernetes/contrib/issues/2823,https://api.github.com/repos/kubernetes/contrib/issues/2823,scratch-debugger: do not rely on host-docker,"/cc @tallclair 

It looks like `scratch-debugger/debug.sh` is broken on GKE, because the Container-Optimized OS image has decided not to statically link the `docker` builds. Therefore `docker` cannot be exec'd due to missing/mismatching dynamic libraries on the `busybox` image.

I think it can easily be fixed by downloading the docker.tgz release in the busybox image and using that to do `docker cp`. 👍  I'll get to it once #2820, #2821 and #2822 are in.

/assign",closed,False,2018-01-09 20:52:10,2018-11-07 08:35:37
contrib,ahmetb,https://github.com/kubernetes/contrib/pull/2824,https://api.github.com/repos/kubernetes/contrib/issues/2824,staging-container: stop relying on host docker CLI,"This change supersedes #2320, #2321 and #2322:

- Stop relying on host docker: (1) it might be dynamically compiled and
  therefore might not work on busybox (2) it might not be at /usr/bin/docker.
- Removed mounting of entire host root filesystem to /mnt/rootfs as it's not
  needed anymore. Mounting only the /var/run/docker.sock now.
- Fixed a stderr redirection bug that caused log line to be saved as a file.

/assign @tallclair",closed,True,2018-01-09 22:57:40,2018-01-23 21:16:32
contrib,porridge,https://github.com/kubernetes/contrib/pull/2825,https://api.github.com/repos/kubernetes/contrib/issues/2825,Fetch list of jobs from prow config.,"This is a step towards solving https://github.com/kubernetes/contrib/issues/2812
/hold
Needs https://github.com/kubernetes/test-infra/pull/6220 to be merged first.",closed,True,2018-01-10 16:26:06,2018-01-12 07:41:50
contrib,muenchhausen,https://github.com/kubernetes/contrib/pull/2826,https://api.github.com/repos/kubernetes/contrib/issues/2826,typo in IP address,,closed,True,2018-01-10 20:42:43,2018-01-31 19:26:07
contrib,porridge,https://github.com/kubernetes/contrib/pull/2827,https://api.github.com/repos/kubernetes/contrib/issues/2827,Bump perfdash version.,Ref https://github.com/kubernetes/contrib/issues/2812,closed,True,2018-01-12 07:54:35,2018-01-12 10:39:08
contrib,porridge,https://github.com/kubernetes/contrib/pull/2828,https://api.github.com/repos/kubernetes/contrib/issues/2828,Reload configuration periodically.,"Also update version.

Fixes: #2812",closed,True,2018-01-12 08:50:44,2018-01-16 06:30:06
contrib,sebbonnet,https://github.com/kubernetes/contrib/pull/2829,https://api.github.com/repos/kubernetes/contrib/issues/2829,Bump rescheduler version to v0.3.3,To release changes made in #2712 and #2802,closed,True,2018-01-15 14:26:13,2018-03-24 01:14:33
contrib,maltris,https://github.com/kubernetes/contrib/issues/2830,https://api.github.com/repos/kubernetes/contrib/issues/2830,kubelet Error: unknown flag: --api-servers,"OS:
```
Fedora 27 Server
```

```
Jan 20 18:41:13 kube2 systemd[1]: Started Kubernetes Kubelet Server.
Jan 20 18:41:13 kube2 kubelet[18407]: Error: unknown flag: --api-servers
Jan 20 18:41:13 kube2 kubelet[18407]: The kubelet binary is responsible for maintaining a set of containers on a
Jan 20 18:41:13 kube2 kubelet[18407]:   particular node. It syncs data from a variety of sources including a
Jan 20 18:41:13 kube2 kubelet[18407]:   Kubernetes API server, an etcd cluster, HTTP endpoint or local file. It then
Jan 20 18:41:13 kube2 kubelet[18407]:   queries Docker to see what is currently running.  It synchronizes the
Jan 20 18:41:13 kube2 kubelet[18407]:   configuration data, with the running set of containers by starting or stopping
Jan 20 18:41:13 kube2 kubelet[18407]:   Docker containers.
Jan 20 18:41:13 kube2 kubelet[18407]: Usage:
Jan 20 18:41:13 kube2 kubelet[18407]:   kubelet [flags]
Jan 20 18:41:13 kube2 kubelet[18407]: Available Flags:
Jan 20 18:41:13 kube2 kubelet[18407]:       --address 0.0.0.0                                                                              
Jan 20 18:41:13 kube2 kubelet[18407]:       --allow-privileged                                                                             
Jan 20 18:41:13 kube2 kubelet[18407]:       --allow-verification-with-non-compliant-keys                                                   
Jan 20 18:41:13 kube2 kubelet[18407]:       --alsologtostderr                                                                              
Jan 20 18:41:13 kube2 kubelet[18407]:       --anonymous-auth                                                                               
Jan 20 18:41:13 kube2 kubelet[18407]:       --application-metrics-count-limit int                                                          
Jan 20 18:41:13 kube2 kubelet[18407]:       --authentication-token-webhook                                                                 
Jan 20 18:41:13 kube2 kubelet[18407]:       --authentication-token-webhook-cache-ttl duration                                              
Jan 20 18:41:13 kube2 kubelet[18407]:       --authorization-mode string                                                                    
Jan 20 18:41:13 kube2 kubelet[18407]:       --authorization-webhook-cache-authorized-ttl duration                                          
Jan 20 18:41:13 kube2 kubelet[18407]:       --authorization-webhook-cache-unauthorized-ttl duration                                        
Jan 20 18:41:13 kube2 kubelet[18407]:       --azure-container-registry-config string                                                       
Jan 20 18:41:13 kube2 kubelet[18407]:       --boot-id-file string                                                                          
Jan 20 18:41:13 kube2 kubelet[18407]:       --bootstrap-checkpoint-path string                                                             
Jan 20 18:41:13 kube2 kubelet[18407]:       --bootstrap-kubeconfig string                                                                  
Jan 20 18:41:13 kube2 kubelet[18407]:       --cadvisor-port int32                                                                          
Jan 20 18:41:13 kube2 kubelet[18407]:       --cert-dir string 
```

```
kubelet --version
Kubernetes v1.9.1
```

After commenting out the following in /etc/kubernetes/kubelet it was fine:
```
KUBELET_API_SERVER=""--api-servers=https://kube1:443""
```

Found a related issue here: https://github.com/coreos/coreos-kubernetes/issues/785

According to /etc/kubernetes/kubelet the ""--kubeconfig"" parameter is already set anyway:
```
KUBELET_ARGS=""--kubeconfig=/etc/kubernetes/kubelet.kubeconfig [...]""
```",closed,False,2018-01-20 18:17:54,2019-01-16 05:36:17
contrib,gyliu513,https://github.com/kubernetes/contrib/issues/2831,https://api.github.com/repos/kubernetes/contrib/issues/2831,Update change-proc-values-rc.yaml to use `initContainer`,"We have just merged a patch here https://github.com/kubernetes/ingress-nginx/pull/1960/files by using `initContainer` to update sysctl parameters, but in https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/examples/sysctl/change-proc-values-rc.yaml#L71-L91 , it is using a sidecar container, this sometimes may cause the container restart if the sidecar container start up slowly than the main container.

It is better update https://github.com/kubernetes/contrib/blob/master/ingress/controllers/nginx/examples/sysctl/change-proc-values-rc.yaml#L71-L91 use `initContainer` instead.

/cc @aledbf ",closed,False,2018-01-24 09:32:52,2018-06-23 11:17:32
contrib,bsalamat,https://github.com/kubernetes/contrib/pull/2832,https://api.github.com/repos/kubernetes/contrib/issues/2832,Add bsalamat to Rescheduler owners,/assign @piosz ,closed,True,2018-01-25 01:21:31,2018-01-25 01:48:08
contrib,Davidchinacloud,https://github.com/kubernetes/contrib/pull/2833,https://api.github.com/repos/kubernetes/contrib/issues/2833,Modify the sample error description,"virtual_ipaddress  should same as virtual_server  in /etc/keepalived/keepalived.conf


Signed-off-by: LinWengang <linwengang@chinacloud.com.cn>",closed,True,2018-01-25 02:31:11,2018-01-31 08:06:35
contrib,tomasare,https://github.com/kubernetes/contrib/issues/2834,https://api.github.com/repos/kubernetes/contrib/issues/2834,exec-healthz has no TAG parameter support. ,"There is no TAG/VERSION default value, neither git tag determines version.
Where does 1.2 version come from?",closed,False,2018-01-26 09:22:51,2018-06-25 11:04:28
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/2835,https://api.github.com/repos/kubernetes/contrib/issues/2835,Add ci-cri-containerd-node-e2e-benchmark to node-perf-dash.,Add ci-cri-containerd-node-e2e-benchmark to node performance dashboard.,closed,True,2018-01-26 22:39:32,2018-01-30 22:57:05
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/2836,https://api.github.com/repos/kubernetes/contrib/issues/2836,[Do not merge] Test CI,"I don't understand the CI failure in https://github.com/kubernetes/contrib/pull/2835.

Send this to see whether the CI is broken.",closed,True,2018-01-26 23:09:54,2018-01-30 22:48:55
contrib,Random-Liu,https://github.com/kubernetes/contrib/issues/2837,https://api.github.com/repos/kubernetes/contrib/issues/2837,travis test is broken.,"```
$ hack/for-go-proj.sh test
testing hack/../service-loadbalancer/
godep: WARNING: Godep workspaces (./Godeps/_workspace) are deprecated and support for them will be removed when go1.8 is released.
godep: WARNING: Go version (go1.9) & $GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling because a Godep workspace (Godeps/_workspace) exists
ok  	k8s.io/contrib/service-loadbalancer	0.352s
testing hack/../apparmor/loader/
?   	k8s.io/contrib/apparmor/loader	[no test files]
testing hack/../perfdash/
godep: WARNING: Godep workspaces (./Godeps/_workspace) are deprecated and support for them will be removed when go1.8 is released.
godep: WARNING: Go version (go1.9) & $GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling because a Godep workspace (Godeps/_workspace) exists
Fetching prow config from GitHub...
panic: reflect: reflect.Value.Set using unaddressable value
goroutine 1 [running]:
reflect.flag.mustBeAssignable(0x96)
	/home/travis/.gimme/versions/go1.9.linux.amd64/src/reflect/value.go:228 +0x17d
reflect.Value.Set(0x79afe0, 0xc4209ac100, 0x96, 0x79afe0, 0xc420ad1bc8, 0x16)
	/home/travis/.gimme/versions/go1.9.linux.amd64/src/reflect/value.go:1345 +0x2f
github.com/ghodss/yaml.indirect(0x79afe0, 0xc4209ac100, 0x96, 0x0, 0x0, 0x0, 0x0, 0xa43440, 0x80ed80, 0x0, ...)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/fields.go:48 +0x368
github.com/ghodss/yaml.convertToJSONableObject(0x7b03a0, 0xa42781, 0xc420ede7c0, 0x0, 0x79afe0, 0xc4209ac100, 0x96)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:106 +0x12b2
github.com/ghodss/yaml.convertToJSONableObject(0x7d3d00, 0xc4201583f0, 0xc420ee6d80, 0xc4209ac100, 0x99, 0x0, 0x0)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:195 +0xb1b
github.com/ghodss/yaml.convertToJSONableObject(0x7d3d00, 0xc4201583c0, 0xc420ee6ff8, 0x3, 0x7d5320, 0xc420971ff8, 0x95)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:205 +0x713
github.com/ghodss/yaml.convertToJSONableObject(0x7d3d00, 0xc420158390, 0xc420ee7270, 0xc420971fc0, 0x99, 0x0, 0x0)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:195 +0xb1b
github.com/ghodss/yaml.convertToJSONableObject(0x7d3d00, 0xc420158360, 0xc420ee74e8, 0x4, 0x7d52c0, 0xc420092608, 0x195)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:205 +0x713
github.com/ghodss/yaml.convertToJSONableObject(0x7d3d00, 0xc420158330, 0xc420ee7760, 0x7, 0x815bc0, 0xc4200925c8, 0x199)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:195 +0xb1b
github.com/ghodss/yaml.convertToJSONableObject(0x7d3d00, 0xc420158000, 0xc420ee79d8, 0x79f220, 0xc4201e1090, 0x0, 0x0)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:195 +0xb1b
github.com/ghodss/yaml.yamlToJSON(0xc4204ce000, 0xbe1e1, 0xbe3e1, 0xc42005bb78, 0x16, 0xc420000180, 0x530e30, 0xc42000e2d0, 0x0)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:89 +0xa5
github.com/ghodss/yaml.Unmarshal(0xc4204ce000, 0xbe1e1, 0xbe3e1, 0x80bcc0, 0xc420092480, 0x0, 0x0)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:32 +0xa0
k8s.io/test-infra/prow/config.Load(0xc42001f760, 0x19, 0x7ff9ae1e8408, 0xc420221f00, 0xbe1e1)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/k8s.io/test-infra/prow/config/config.go:190 +0x1ea
k8s.io/contrib/perfdash.getProwConfig(0x0, 0x0, 0x0)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/config.go:125 +0x404
k8s.io/contrib/perfdash.getProwConfigOrDie(0x7d5200)
	/home/travis/gopath/src/k8s.io/contrib/perfdash/config.go:102 +0x22
k8s.io/contrib/perfdash.init()
	/home/travis/gopath/src/k8s.io/contrib/perfdash/config.go:98 +0x698
main.init()
	<autogenerated>:1 +0x53
FAIL	k8s.io/contrib/perfdash	0.283s
godep: go exit status 1
```

This even happens to an empty PR. https://github.com/kubernetes/contrib/pull/2836",closed,False,2018-01-26 23:23:27,2018-01-30 18:24:11
contrib,omar-nahhas,https://github.com/kubernetes/contrib/issues/2838,https://api.github.com/repos/kubernetes/contrib/issues/2838,Helm chart on official kuberentes/charts repository,"Hi,

Are there any plans to publish the helm chart on the kuberentes/charts repository? I have searched for an open pull request but I could not find one for this project.",closed,False,2018-01-28 19:23:51,2018-06-27 20:59:43
contrib,SleepyBrett,https://github.com/kubernetes/contrib/issues/2839,https://api.github.com/repos/kubernetes/contrib/issues/2839,Rescheduler needs a readme,Is there a requirement around what version runs on what kubernetes version. What are the commandline flags and how do I use them? Is there an official docker repo out there? etc.,closed,False,2018-01-29 19:40:42,2018-06-28 21:23:45
contrib,porridge,https://github.com/kubernetes/contrib/issues/2840,https://api.github.com/repos/kubernetes/contrib/issues/2840,perfdash broken by incompatible prow config,"@bsalamat noticed perfdash is down.
Indeed:

```
porridge@kielonek:~$ kubectl get pods --show-labels|grep perf
perfdash-958854004-5lrbg                   0/1       CrashLoopBackOff   1209       15d       app=perfdash,pod-template-hash=958854004
porridge@kielonek:~$ kubectl logs perfdash-958854004-5lrbg
Fetching prow config from GitHub...
panic: reflect: reflect.Value.Set using unaddressable value

goroutine 1 [running]:
reflect.flag.mustBeAssignable(0x96)
        /usr/local/google/home/porridge/.gvm/gos/go1.9.2/src/reflect/value.go:228 +0x17d
reflect.Value.Set(0x7b4fc0, 0xc420e9f480, 0x96, 0x7b4fc0, 0xc420432c40, 0x16)
        /usr/local/google/home/porridge/.gvm/gos/go1.9.2/src/reflect/value.go:1351 +0x2f
github.com/ghodss/yaml.indirect(0x7b4fc0, 0xc420e9f480, 0x96, 0x0, 0x0, 0x0, 0x0, 0xa6fd60, 0x82bc60, 0x0, ...)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/fields.go:48 +0x368
github.com/ghodss/yaml.convertToJSONableObject(0x7ca540, 0xa6efa1, 0xc420ff68a0, 0x0, 0x7b4fc0, 0xc420e9f480, 0x96)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:106 +0x12b2
github.com/ghodss/yaml.convertToJSONableObject(0x7ee580, 0xc420174450, 0xc421000d90, 0xc420e9f480, 0x99, 0x0, 0x0)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:195 +0xb1b
github.com/ghodss/yaml.convertToJSONableObject(0x7ee580, 0xc420174420, 0xc421001008, 0x3, 0x7efae0, 0xc420e9f478, 0x95)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:205 +0x713
github.com/ghodss/yaml.convertToJSONableObject(0x7ee580, 0xc4201743f0, 0xc421001280, 0xc420e9f440, 0x99, 0x0, 0x0)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:195 +0xb1b
github.com/ghodss/yaml.convertToJSONableObject(0x7ee580, 0xc4201743c0, 0xc4210014f8, 0x4, 0x7efa80, 0xc420086848, 0x195)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:205 +0x713
github.com/ghodss/yaml.convertToJSONableObject(0x7ee580, 0xc420174390, 0xc421001770, 0x7, 0x832920, 0xc420086808, 0x199)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:195 +0xb1b
github.com/ghodss/yaml.convertToJSONableObject(0x7ee580, 0xc420174000, 0xc4210019e8, 0x7b9480, 0xc420045720, 0x0, 0x0)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:195 +0xb1b
github.com/ghodss/yaml.yamlToJSON(0xc4204f6000, 0xbf9c2, 0xbfbc2, 0xc42003bb88, 0x16, 0x0, 0x4dfab0, 0xc42006c080, 0x0)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:89 +0xa5
github.com/ghodss/yaml.Unmarshal(0xc4204f6000, 0xbf9c2, 0xbfbc2, 0x8287e0, 0xc4200866c0, 0x0, 0x0)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/github.com/ghodss/yaml/yaml.go:32 +0xa0
k8s.io/test-infra/prow/config.Load(0xc420016de0, 0x19, 0x7f845fc5ed20, 0xc420475fa0, 0xbf9c2)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/Godeps/_workspace/src/k8s.io/test-infra/prow/config/config.go:190 +0x1ea
main.getProwConfig(0x0, 0x0, 0x0)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/config.go:125 +0x404
main.getProwConfigOrDie(0x7efc00)
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/config.go:102 +0x22
main.init()
        /usr/local/google/home/porridge/projects/go/src/k8s.io/contrib/perfdash/config.go:98 +0x693
porridge@kielonek:~$ 
```

I guess that the config in github got out of sync with the vendored `k8s.io/test-infra/prow/config/config.go`. I didn't expect the config to change in an incompatible way when I did https://github.com/kubernetes/contrib/issues/2812
@krzyzacy @BenTheElder @kubernetes/sig-testing-misc how do we prevent this from happening in the future?",closed,False,2018-01-30 06:42:53,2018-01-30 18:24:11
contrib,dudigit,https://github.com/kubernetes/contrib/issues/2841,https://api.github.com/repos/kubernetes/contrib/issues/2841,change ansible module oc_atomic_container to atomic_container,"Ansible 2.4.2.0 already has atomic_container build in

diff --git a/ansible/roles/master/tasks/system_container.yml b/ansible/roles/master/tasks/system_container.yml
index 274d3479..7cf7d888 100644
--- a/ansible/roles/master/tasks/system_container.yml
+++ b/ansible/roles/master/tasks/system_container.yml
@@ -14,19 +14,19 @@
   changed_when: ""'Pulling layer' in pull_result.stdout""

 - name: Install or Update kubernetes-apiserver system container package
-  oc_atomic_container:
+  atomic_container:
     name: kube-apiserver
     image: ""{{ kube_apiserver_system_image }}""
     state: latest

 - name: Install or Update kubernetes-controller-manager system container package
-  oc_atomic_container:
+  atomic_container:
     name: kube-controller-manager
     image: ""{{ kube_controller_manager_system_image }}""
     state: latest

 - name: Install or Update kubernetes-scheduler system container package
-  oc_atomic_container:
+  atomic_container:
     name: kube-scheduler
     image: ""{{ kube_scheduler_system_image }}""
     state: latest
diff --git a/ansible/roles/node/tasks/system_container.yml b/ansible/roles/node/tasks/system_container.yml
index 5d2b0e28..50d39598 100644
--- a/ansible/roles/node/tasks/system_container.yml
+++ b/ansible/roles/node/tasks/system_container.yml
@@ -9,13 +9,13 @@
   changed_when: ""'Pulling layer' in pull_result.stdout""

 - name: Install or Update kubernetes-kubelet system container package
-  oc_atomic_container:
+  atomic_container:
     name: kubelet
     image: ""{{ kubelet_system_image }}""
     state: latest

 - name: Install or Update kubernetes-proxy system container package
-  oc_atomic_container:
+  atomic_container:
     name: kube-proxy
     image: ""{{ kube_proxy_system_image }}""
     state: latest
diff --git a/ansible/vagrant/Vagrantfile b/ansible/vagrant/Vagrantfile
index 205c1a6b..87742dcc 100644
--- a/ansible/vagrant/Vagrantfile
+++ b/ansible/vagrant/Vagrantfile
",closed,False,2018-01-30 11:50:00,2018-06-29 13:39:45
contrib,Kyrremann,https://github.com/kubernetes/contrib/pull/2842,https://api.github.com/repos/kubernetes/contrib/issues/2842,isLeader endpoint for leader election,"We are using the leader election containers in my project, but we want to make it as easy as possible for our developers to find out if a pod is the leader. Hence the `/isLeader` endpoint.

This is also based on problems where we have had a new leader for a set of pods, but the other (non-leader) pods reported that the deleted leader was still the leader, and not the newly created pod.

Flow:
- starting 3 pods (A, B, C)
- B is leader
- deleting B
- starting D
- D is selected as new leader
- A and C both report B to be the leader via `localhost:4040` endpoint, but logs (`kubectl logs`)
 the correct leader (D)
- D reports itself to be the leader, both via endpoint and logs

Deleting one or both A and C, creating a new E and/or F, make them report the correct leader from the endpoint.",open,True,2018-01-30 12:14:39,2019-03-10 10:17:12
contrib,porridge,https://github.com/kubernetes/contrib/pull/2843,https://api.github.com/repos/kubernetes/contrib/issues/2843,Stop using k8s.io/test-infra/prow/config for reading prow config.,"Use a custom minimal config definition instead.
Fixes: #2840
Fixes: #2837",closed,True,2018-01-30 15:39:49,2018-01-31 08:20:46
contrib,porridge,https://github.com/kubernetes/contrib/pull/2844,https://api.github.com/repos/kubernetes/contrib/issues/2844,Bump perfash version.,/cc @shyamjvs,closed,True,2018-01-31 08:22:52,2018-02-01 10:10:44
contrib,Davidchinacloud,https://github.com/kubernetes/contrib/pull/2845,https://api.github.com/repos/kubernetes/contrib/issues/2845,[keepalived-vip] typo in IP address and virtual_ipaddress should same as virtual_server,"virtual_ipaddress should same as virtual_server in /etc/keepalived/keepalived.conf  and  typo in IP address

Signed-off-by: LinWengang <linwengang@chinacloud.com.cn>",closed,True,2018-01-31 08:37:02,2018-06-01 17:21:58
contrib,rvkubiak,https://github.com/kubernetes/contrib/pull/2846,https://api.github.com/repos/kubernetes/contrib/issues/2846,Add link to redirect,,closed,True,2018-02-01 23:50:20,2018-02-23 22:52:48
contrib,liubin,https://github.com/kubernetes/contrib/pull/2847,https://api.github.com/repos/kubernetes/contrib/issues/2847,Fix typo,,closed,True,2018-02-07 08:18:17,2018-02-23 22:50:46
contrib,marekaf,https://github.com/kubernetes/contrib/issues/2848,https://api.github.com/repos/kubernetes/contrib/issues/2848,health_checks example failing,"Hi,
I'm trying to create a simple ingress with gce health checks from readiness probes. I tried this example

https://github.com/kubernetes/contrib/tree/master/ingress/controllers/gce/examples/health_checks

and the health checks are not being creates, readiness probes are ignored and the ingress has ""Unknown"" backend svcs

```
~ kubectl create -f health_check_app.yaml
~ kubectl describe ing echomap                                                                                                                                                                   
Name:             echomap
Namespace:        default
Address:          130.211.31.117
Default backend:  default-http-backend:80 (10.20.1.10:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   echoheadersx:80 (<none>)
  bar.baz.com
               /bar   echoheadersy:80 (<none>)
               /foo   echoheadersx:80 (<none>)
Annotations:
  target-proxy:     k8s-tp-default-echomap--17af70295d9b590a
  url-map:          k8s-um-default-echomap--17af70295d9b590a
  backends:         {""k8s-be-31387--17af70295d9b590a"":""Unknown"",""k8s-be-31903--17af70295d9b590a"":""Unknown"",""k8s-be-32710--17af70295d9b590a"":""Unknown""}
  forwarding-rule:  k8s-fw-default-echomap--17af70295d9b590a
Events:
  Type    Reason   Age              From                     Message
  ----    ------   ----             ----                     -------
  Normal  ADD      5m               loadbalancer-controller  default/echomap
  Normal  CREATE   4m               loadbalancer-controller  ip: 130.211.31.117
  Normal  Service  4m (x3 over 5m)  loadbalancer-controller  no user specified default backend, using system default

~ curl 130.211.31.117/foo -H 'Host:foo.bar.com'                                                                                                                                                  
<html><head>
<meta http-equiv=""content-type"" content=""text/html;charset=utf-8"">
<title>502 Server Error</title>
</head>
<body text=#000000 bgcolor=#ffffff>
<h1>Error: Server Error</h1>
<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>
<h2></h2>
</body></html>
~ gcloud compute http-health-checks list                                                                                                                                                         
NAME                              HOST  PORT   REQUEST_PATH
nothing relevant
~ kubectl version                                                                                                                                                                                
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.6"", GitCommit:""6260bb08c46c31eea6cb538b34a9ceb3e406689c"", GitTreeState:""clean"", BuildDate:""2017-12-21T06:34:11Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7+"", GitVersion:""v1.7.12-gke.0"", GitCommit:""f4f0bedb82a57dc32b21b8f15fe01e0b8411356b"", GitTreeState:""clean"", BuildDate:""2018-01-05T03:35:43Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
Any idea what changed? I just need to be able to create a kube ingress with custom gce health checks (url paths and interval at the very least, custom headers would be awesome too...)",closed,False,2018-02-08 09:56:39,2018-02-08 10:28:35
contrib,accek,https://github.com/kubernetes/contrib/pull/2849,https://api.github.com/repos/kubernetes/contrib/issues/2849,zookeeper: fixed no autopurge,,closed,True,2018-02-08 16:37:28,2018-10-05 09:39:06
contrib,ingvagabund,https://github.com/kubernetes/contrib/pull/2850,https://api.github.com/repos/kubernetes/contrib/issues/2850,kubelet: Set --fail-swap-on=false,See https://github.com/kubernetes/kubernetes/pull/47181,closed,True,2018-02-09 11:32:45,2018-02-09 12:03:14
contrib,twz123,https://github.com/kubernetes/contrib/pull/2851,https://api.github.com/repos/kubernetes/contrib/issues/2851,[keepalived-vip] Propagate error when network interface name couldn't be detected,"Be fail-fast in case when the network interface name auto detection fails. Otherwise, an empty interface name would be used, which in turn crashes keepalived with a segfault.",closed,True,2018-02-13 17:43:06,2018-05-17 21:34:51
contrib,oomichi,https://github.com/kubernetes/contrib/issues/2852,https://api.github.com/repos/kubernetes/contrib/issues/2852,The link of images/nginx-slim/README.md is NotFound,"https://github.com/kubernetes/contrib/blob/master/images/nginx-slim/README.md contains the link to nginx-slim image under kubernetes/ingress repository.
However, today the link is not found.
",closed,False,2018-02-13 21:42:22,2018-06-01 20:21:51
contrib,oomichi,https://github.com/kubernetes/contrib/pull/2853,https://api.github.com/repos/kubernetes/contrib/issues/2853,Remove nginx-slim image path,"Since the commit of kubernetes/ingress-nginx[1], the path has been
changed from images/nginx-slim to images/nginx. This commit updates
the path.

[1]: https://github.com/kubernetes/ingress-nginx/commit/811829de60f8e0c2c69e67a0e6044eab4a5b1627

fixes #2852 ",closed,True,2018-02-13 21:45:43,2018-06-01 20:21:32
contrib,dims,https://github.com/kubernetes/contrib/issues/2854,https://api.github.com/repos/kubernetes/contrib/issues/2854,Updating OWNERS files,"It's been an year since the first population of the OWNERS files in https://github.com/kubernetes/contrib/issues/1389. 

* Some folks have moved on and no longer reviewing or approving changes that they are responsible for
* New folks have adding code who may make good reviewers if not approvers
* Can we get some data on which reviewers / approvers are not really doing what they are supposed to do? (so we can request if we can drop them from the file(s))?
* There is a concept of a SIG in https://github.com/kubernetes/kubernetes/blob/master/OWNERS_ALIASES, but very few OWNERS files use these groups from OWNERS_ALIASES. Do we want to move to a model where we favor the sig groupings over individuals? Is that upto sig(s)? Do we want to tell them that they should review code under their purview and make the change?

It's probably time to update reviewers and approvers again...",closed,False,2018-02-14 14:15:59,2018-02-16 13:33:52
contrib,porridge,https://github.com/kubernetes/contrib/pull/2855,https://api.github.com/repos/kubernetes/contrib/issues/2855,Load benchmark metrics.,"Also minor improvement to error handling.

This is for https://github.com/kubernetes/kubernetes/issues/58154#issuecomment-362551793

/hold
Needs to wait for https://github.com/kubernetes/test-infra/pull/6843 to go in.
/cc @shyamjvs",closed,True,2018-02-15 08:15:41,2018-02-15 12:42:35
contrib,porridge,https://github.com/kubernetes/contrib/pull/2856,https://api.github.com/repos/kubernetes/contrib/issues/2856,Bump version.,/cc @shyamjvs,closed,True,2018-02-15 12:53:10,2018-02-15 13:16:40
contrib,grosser,https://github.com/kubernetes/contrib/pull/2857,https://api.github.com/repos/kubernetes/contrib/issues/2857,add option to show actual output to exechealthz,@thockin ,closed,True,2018-02-15 20:30:25,2018-02-15 21:38:40
contrib,sonigv,https://github.com/kubernetes/contrib/issues/2858,https://api.github.com/repos/kubernetes/contrib/issues/2858,"Unable to access app via an ingress, while being on another app behind another ingress","First of all I am newbie K8s user, so I may be doing a wrong setup.

I am running 2 different app stacks on the same cluster on Minikube. Each app stack has its deployment -> pod -> service -> ingress.

The idea is to access app behind Ingress2 from app behind Ingress 1 which doesnt work. 

Here is my ingress 1:
$ kubectl  -n perf-tests describe ing e2e-12837-e2eservice-47920
Name:             e2e-12837-e2eservice-47920
Namespace:        perf-tests
Address:          192.168.99.100
Default backend:  default-http-backend:80 (172.17.0.3:8080)
Rules:
  Host        Path  Backends
  ----        ----  --------
  myminikube  
              /master9   e2e-12837-e2eservice-47920:9999 (<none>)
Annotations:
Events:
  Type    Reason  Age   From                      Message
  ----    ------  ----  ----                      -------
  Normal  CREATE  51m   nginx-ingress-controller  Ingress perf-tests/e2e-12837-e2eservice-47920
  Normal  UPDATE  50m   nginx-ingress-controller  Ingress perf-tests/e2e-12837-e2eservice-47920



Here is my ingress 2:

$ kubectl  -n perf-tests describe ing e2e-12837-e2eservice-47920-perf-slave-0
Name:             e2e-12837-e2eservice-47920-perf-slave-0
Namespace:        perf-tests
Address:          192.168.99.100
Default backend:  default-http-backend:80 (172.17.0.3:8080)
Rules:
  Host        Path  Backends
  ----        ----  --------
  myminikube  
              /slave0   e2e-12837-e2eservice-47920-perf-slave-0:name-9111 (<none>)
Annotations:
Events:
  Type    Reason  Age   From                      Message
  ----    ------  ----  ----                      -------
  Normal  CREATE  44m   nginx-ingress-controller  Ingress perf-tests/e2e-12837-e2eservice-47920-perf-slave-0
  Normal  UPDATE  43m   nginx-ingress-controller  Ingress perf-tests/e2e-12837-e2eservice-47920-perf-slave-0

I am able to access the apps behind Ingress1 & Ingress2 from outside, by hitting - curl http://myminikube/master9 & curl http://myminikube/slave0

But from inside when App1 under Ingress1 tries to access App2 under Ingress2, it says -
curl: (6) Could not resolve host: myminikube

Any clue what might be wrong here?





",closed,False,2018-02-21 13:47:52,2018-02-22 13:13:50
contrib,jayunit100,https://github.com/kubernetes/contrib/pull/2859,https://api.github.com/repos/kubernetes/contrib/issues/2859,README update for prometheus contextualization,,closed,True,2018-02-22 16:59:12,2018-09-16 21:47:16
contrib,tonglil,https://github.com/kubernetes/contrib/pull/2860,https://api.github.com/repos/kubernetes/contrib/issues/2860,Update links,"Link gce examples to the gce ingress repo.

What are people's thoughts about removing the examples here and only keep the link?
I am not a fan of having duplicate docs floating around.",open,True,2018-02-22 21:58:32,2019-03-06 21:56:01
contrib,ramakrishnateja,https://github.com/kubernetes/contrib/issues/2861,https://api.github.com/repos/kubernetes/contrib/issues/2861,NGINX Ingress Controller backend service resolved properly but still returning 404 response,"
[nginx.txt](https://github.com/kubernetes/contrib/files/1754690/nginx.txt)
Hi,

I have configured an NGINX Ingress controller with a path based redirection to a backend service.

here is the ingress definition
![ingress definition](https://user-images.githubusercontent.com/6781607/36632974-7c6c6a62-195b-11e8-82f4-132b9e19223c.JPG)

Now when i try to access the backend service using the <Ip>/config I see that the NGINX Controller is properly resolving the path and requesting the actual pod but still sending back a 404 response

![ingress logs](https://user-images.githubusercontent.com/6781607/36632990-c5179ee4-195b-11e8-9b57-b7f7cd29f7ee.JPG)

I tried to get in to the NGINX controller pod and execute a curl request to the same path its trying to access and I am able to get the response properly
![pod accesss](https://user-images.githubusercontent.com/6781607/36632996-e9062802-195b-11e8-8b44-b69fa3395e53.JPG)

So I am not sure what the problem here is and why the same request to the backend pod is not working through Ingress Controller

Also attaching the nginx.conf file for reference
",closed,False,2018-02-24 17:12:33,2018-07-24 19:34:52
contrib,adelton,https://github.com/kubernetes/contrib/pull/2862,https://api.github.com/repos/kubernetes/contrib/issues/2862,The github.com/GoogleCloudPlatform/kubernetes is no more.,It was renamed to https://github.com/kubernetes/kubernetes but the Kubernetes documentation seems to live at https://kubernetes.io.,closed,True,2018-02-26 14:41:57,2018-02-27 02:29:46
contrib,adelton,https://github.com/kubernetes/contrib/pull/2863,https://api.github.com/repos/kubernetes/contrib/issues/2863,"The `--api-servers` is no longer supported by Kubelet 1.9, use the `kubelet.kubeconfig` approach.",Addressing (partially) https://github.com/kubernetes/website/issues/7521 and https://bugzilla.redhat.com/show_bug.cgi?id=1549151.,closed,True,2018-02-26 14:49:29,2018-05-02 11:35:17
contrib,Redshadow40,https://github.com/kubernetes/contrib/issues/2864,https://api.github.com/repos/kubernetes/contrib/issues/2864,Kafka Dockerfile apache link no longer works,"wget http://www.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz is a bad link.  
Workaround:
edit the following lines to point to 0.10.2.1
ARG KAFKA_VERSION=0.10.2.1
ARG KAFKA_DIST=kafka_2.11-0.10.2.1",closed,False,2018-02-28 20:16:45,2018-07-28 22:11:52
contrib,Redshadow40,https://github.com/kubernetes/contrib/issues/2865,https://api.github.com/repos/kubernetes/contrib/issues/2865,Zookeeper permission issue ,"When you build the docker image from the provided dockerfile a permission issue is shown that isn't in dockerhub:
![screen shot 2018-03-01 at 9 53 19 am](https://user-images.githubusercontent.com/16948921/36860592-901efe56-1d36-11e8-970e-5e831d26e556.png)

workaround:
chmod +x to all the scripts",closed,False,2018-03-01 17:54:44,2018-07-29 20:33:28
contrib,lenalebt,https://github.com/kubernetes/contrib/pull/2866,https://api.github.com/repos/kubernetes/contrib/issues/2866,Directly write on-start and on-change script output to stdout,"We had a problem with a hanging on-start script, and peer-finder waited for the script to terminate before writing it's output to stdout. This change directly writes the output to stdout as it happens, making it way easier to debug hanging startup scripts.",closed,True,2018-03-05 10:25:12,2018-09-05 13:04:28
contrib,biswajitmukherji,https://github.com/kubernetes/contrib/issues/2867,https://api.github.com/repos/kubernetes/contrib/issues/2867,swapoff command issue,"https://github.com/kubernetes/contrib/blob/e08d765973ff7bfd561067b0784364dab6a6a483/ansible/roles/node/tasks/swapoff.yml#L8

not working for RHEL 7
error ""FAILED! => {""changed"": false, ""cmd"": ""swapoff -a"", ""msg"": ""[Errno 2] No such file or directory"", ""rc"": 2""",closed,False,2018-03-10 04:36:12,2018-08-07 06:52:30
contrib,yuvipanda,https://github.com/kubernetes/contrib/pull/2868,https://api.github.com/repos/kubernetes/contrib/issues/2868,[startup-script] Don't cache apk update results,This makes the image smaller.,closed,True,2018-03-11 05:29:15,2018-07-18 19:31:20
contrib,lrolaz,https://github.com/kubernetes/contrib/pull/2869,https://api.github.com/repos/kubernetes/contrib/issues/2869,Upgrade to keepalived 1.4.2,"This Pull Request only upgrade the version ok keepalived to version 1.4.2
It was tested in our environment on baremetal and AWS and work fine.",closed,True,2018-03-12 14:07:33,2018-04-11 07:38:35
contrib,AdamDang,https://github.com/kubernetes/contrib/pull/2870,https://api.github.com/repos/kubernetes/contrib/issues/2870,"Typo ""it's""->""its""","in line 29 and line 61, ""it's"" should be replaced with ""its"".",closed,True,2018-03-12 15:49:32,2018-05-01 00:39:47
contrib,chenhonggc,https://github.com/kubernetes/contrib/pull/2871,https://api.github.com/repos/kubernetes/contrib/issues/2871,fix some mistake,Fix some minor bugs.,closed,True,2018-03-16 15:21:26,2018-08-29 01:09:17
contrib,Dr13am,https://github.com/kubernetes/contrib/issues/2872,https://api.github.com/repos/kubernetes/contrib/issues/2872,Result  is Error,"

After I used your program  docker_micro_benchmark, I found that the result was repeated 。Although the time is correct, the results are repeated two by two. ",closed,False,2018-03-19 02:13:03,2018-03-19 10:17:26
contrib,sunlintong,https://github.com/kubernetes/contrib/pull/2873,https://api.github.com/repos/kubernetes/contrib/issues/2873,correct spell mistake,statistics spell error,closed,True,2018-03-28 07:12:37,2018-12-02 18:33:05
contrib,sunlintong,https://github.com/kubernetes/contrib/pull/2874,https://api.github.com/repos/kubernetes/contrib/issues/2874,small mistakes,,closed,True,2018-03-28 07:50:57,2018-07-18 19:28:20
contrib,abhaynayak24,https://github.com/kubernetes/contrib/issues/2875,https://api.github.com/repos/kubernetes/contrib/issues/2875,[kubelet-check] It seems like the kubelet isn't running or healthy. #Connection refused,"Hi

Im currently working on setting up kubernetes and im following this blog https://www.techrepublic.com/article/how-to-quickly-install-kubernetes-on-ubuntu/

When I write the command kubeadm init, I get an error while pulling the images. Im not sure why Im facing this issue. Kindly help me with this.

root@mylinux:/home# kubeadm init
[init] Using Kubernetes version: v1.10.0
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks.
	[WARNING FileExisting-crictl]: crictl not found in system path
Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl
[preflight] Starting the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [intel-dell-wyse-thin-client-desktop-3290 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.26]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated etcd/ca certificate and key.
[certificates] Generated etcd/server certificate and key.
[certificates] etcd/server serving cert is signed for DNS names [localhost] and IPs [127.0.0.1]
[certificates] Generated etcd/peer certificate and key.
[certificates] etcd/peer serving cert is signed for DNS names [intel-dell-wyse-thin-client-desktop-3290] and IPs [192.168.1.26]
[certificates] Generated etcd/healthcheck-client certificate and key.
[certificates] Generated apiserver-etcd-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/controller-manager.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/scheduler.conf""
[controlplane] Wrote Static Pod manifest for component kube-apiserver to ""/etc/kubernetes/manifests/kube-apiserver.yaml""
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to ""/etc/kubernetes/manifests/kube-controller-manager.yaml""
[controlplane] Wrote Static Pod manifest for component kube-scheduler to ""/etc/kubernetes/manifests/kube-scheduler.yaml""
[etcd] Wrote Static Pod manifest for a local etcd instance to ""/etc/kubernetes/manifests/etcd.yaml""
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory ""/etc/kubernetes/manifests"".
[init] This might take a minute or longer if the control plane images have to be pulled.

******issue starts from next lines******

[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz/syncloop' failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz/syncloop' failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz/syncloop' failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz/syncloop' failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp 127.0.0.1:10255: getsockopt: connection refused.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
	- Either there is no internet connection, or imagePullPolicy is set to ""Never"",
	  so the kubelet cannot pull or find the following control plane images:
		- k8s.gcr.io/kube-apiserver-amd64:v1.10.0
		- k8s.gcr.io/kube-controller-manager-amd64:v1.10.0
		- k8s.gcr.io/kube-scheduler-amd64:v1.10.0
		- k8s.gcr.io/etcd-amd64:3.1.12 (only if no external etcd endpoints are configured)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'
couldn't initialize a Kubernetes cluster
",closed,False,2018-03-29 13:47:47,2018-06-23 07:30:00
contrib,krzysied,https://github.com/kubernetes/contrib/pull/2876,https://api.github.com/repos/kubernetes/contrib/issues/2876,Adding request count metrics to Perfdash ,/cc @shyamjvs ,closed,True,2018-04-03 16:30:33,2018-04-04 12:21:11
contrib,thockin,https://github.com/kubernetes/contrib/pull/2877,https://api.github.com/repos/kubernetes/contrib/issues/2877,Bump to golang-1.10 and update docker base image,Tagged and pushed as v1.3.0,closed,True,2018-04-03 17:05:21,2018-04-09 20:32:10
contrib,krzysied,https://github.com/kubernetes/contrib/pull/2878,https://api.github.com/repos/kubernetes/contrib/issues/2878,Adding metrics for apiserver request count.,,closed,True,2018-04-05 11:39:08,2018-04-09 12:37:09
contrib,ArcticSnowman,https://github.com/kubernetes/contrib/issues/2879,https://api.github.com/repos/kubernetes/contrib/issues/2879,[keepalived vip ]  clusterrole RBAC does not verify with v1.9.0,"With the latest K8S V1.9, the list of resources needs to be quoted

```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-keepalived-vip
rules:
- apiGroups: [""""]
  resources:
    - ""pods""
    - ""nodes""
    - ""endpoints""
    - ""services""
    - ""configmaps""
  verbs: [""get"", ""list"", ""watch""]
```
 or you'll get a validation error..

```
error: error validating ""vip-clusterrole.yaml"": error validating data: the server could not find the requested resource; if you choose to ignore these errors, turn validation off with --validate=false
```",closed,False,2018-04-06 02:22:12,2018-12-02 15:30:01
contrib,raggi,https://github.com/kubernetes/contrib/issues/2880,https://api.github.com/repos/kubernetes/contrib/issues/2880,go2docker stale certs,"go2docker's certs got really stale again. They're now 2 years old, which includes some major CA's being untrusted by most parties and CAB.

This should be considered a security issue.

I have open PRs from IIRC over a year ago that have gone untouched. I was added to OWNERS about a year ago, but have never been given push access, so I can't fix any of this myself.

If there's no interest in providing push access or regularly triaging these issues, then this code should be removed please, it's leading people down some dangerous paths if they're using it.",closed,False,2018-04-06 04:44:54,2018-12-14 22:47:57
contrib,mahuihuang,https://github.com/kubernetes/contrib/pull/2881,https://api.github.com/repos/kubernetes/contrib/issues/2881,Fix grammar mistake,Fix grammar mistake!,closed,True,2018-04-06 09:08:24,2018-04-06 15:57:10
contrib,erloop,https://github.com/kubernetes/contrib/issues/2882,https://api.github.com/repos/kubernetes/contrib/issues/2882,[keepalived vip ] real ip and nginx,"I have this setup

client(real_ip) -> keepalived -> ingress nginx

Can I see real_ip in ingress nginx?",closed,False,2018-04-06 12:14:47,2018-12-07 00:11:31
contrib,krzysied,https://github.com/kubernetes/contrib/pull/2883,https://api.github.com/repos/kubernetes/contrib/issues/2883,Update godep dependencies,"Adding dependency to k8s.io/kubernetes/test/e2e/framework/metrics.
Using MetricsCollection instead of custom structure to parse json file.
Godep: removing _workspace and adding vendor instead.",closed,True,2018-04-09 13:54:00,2018-04-09 14:19:25
contrib,krzysied,https://github.com/kubernetes/contrib/pull/2884,https://api.github.com/repos/kubernetes/contrib/issues/2884,Update doc + owners,"Updating project owners.
Adding README file.",closed,True,2018-04-09 15:32:37,2018-04-10 11:28:11
contrib,krzysied,https://github.com/kubernetes/contrib/pull/2885,https://api.github.com/repos/kubernetes/contrib/issues/2885,Moving perfdash to kubernetes/perf-tests,,closed,True,2018-04-11 09:28:27,2018-04-11 09:44:19
contrib,abhaynayak24,https://github.com/kubernetes/contrib/issues/2886,https://api.github.com/repos/kubernetes/contrib/issues/2886,Kubernetes features - deciding which pod runs on which worker,"Hi,

I'm exploring kubernetes and I see that when the pod is created, the master automatically assigns it to some worker (I'm guessing randomly) in the cluster.

However, if the worker1 (low CPU specs) gets overloaded it extends the pod to worker2 (high-end CPU specs) eventually. I know that every time the pod will be overloaded and shifted to this high-end worker but still the master always initializes the pod on worker1.

Is there any feature which can allow me to configure the master such that it deploys the pods directly on the high-end worker rather than deploying it on a weak system and then shifting on to a better one? ",closed,False,2018-04-13 06:59:03,2018-09-10 10:14:45
contrib,AdamDang,https://github.com/kubernetes/contrib/pull/2887,https://api.github.com/repos/kubernetes/contrib/issues/2887,small typo fix,small typo fix,closed,True,2018-04-17 05:48:19,2018-05-17 21:39:48
contrib,abhaynayak24,https://github.com/kubernetes/contrib/issues/2888,https://api.github.com/repos/kubernetes/contrib/issues/2888,Kubernetes: Using influxDB in heapster,"Hi,

Im working with heapster in kubernetes and want to access the pod usage and node usage data from influxdb. Is there any way to access those from inside influxdb?
I'm in influxdb using the following 

>influx -host 10.107.225.110 -port 8086
>Visit https://enterprise.influxdata.com to register for updates, InfluxDB server management, and monitoring.
>Connected to http://10.107.225.110:8086 version unknown
>InfluxDB shell 0.10.0
> show databases
>name: databases
>
>name
>_internal
>k8s
>
> use k8s
>Using database k8s
> 
                              
However, Im not able to access the data that we acquire when we enter kubectl top pods and kubectl top nodes",closed,False,2018-04-18 06:33:09,2018-04-24 11:12:50
contrib,Nayana-ibm,https://github.com/kubernetes/contrib/issues/2889,https://api.github.com/repos/kubernetes/contrib/issues/2889,exechealthz image with Tag 1.2 ,"I could see the exechealthz image uploaded [here ](https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/exechealthz-s390x?gcrImageListsize=50) and its for tag 1.2

I tried to build image from source as mentioned in [Readme](https://github.com/kubernetes/contrib/tree/cluster-autoscaler-0.5.1/exec-healthz)
Using branch: cluster-autoscaler-0.5.1
make TAG=1.2 ARCH=s390x

However, the image created shows the tag as 3b70a71b-dirty and not as 1.2
`gcr.io/google_containers/exechealthz-s390x    3b70a71b-dirty`

Changes needed in makefile or any other build file?


",closed,False,2018-04-23 09:29:36,2019-03-18 06:50:30
contrib,AdamDang,https://github.com/kubernetes/contrib/pull/2890,https://api.github.com/repos/kubernetes/contrib/issues/2890,Typo fix comand->command,Line 26: comand->command,closed,True,2018-04-26 13:05:51,2018-05-01 02:07:50
contrib,LauraMoraB,https://github.com/kubernetes/contrib/issues/2891,https://api.github.com/repos/kubernetes/contrib/issues/2891,Kafka Statefulset external access issue,"Hello,
I have deployed Kafka and Zookeeper in a Kubernetes cluster using Statefulsets with three replicas in a Softlayer machine, using this [configuration](https://github.com/kubernetes/contrib/tree/master/statefulsets) (deployment and Image), but changing the kafka version to 1.0. 
Now, I am having problems to produce/consume messages from outside the Kubernetes cluster, but everything works fine when I execute producers/consumers within the cluster. I have seen some issues regarding this problem, but neither resolved it for me.

I have this set up: 
```bash
NAME                    READY     STATUS    RESTARTS  
po/kafka-0                   1/1       Running   0      
po/kafka-1                   1/1       Running   0         
po/kafka-2                   1/1       Running   0          
po/zk-0                        1/1       Running   0         
po/zk-1                        1/1       Running   0        
po/zk-2                        1/1       Running   0          
NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      
svc/kafka-hs      ClusterIP        None             <none>        9092/TCP                    
svc/kafka-svc    NodePort     10.97.218.58     <none>        9093:30182/TCP              
svc/zk-hs          ClusterIP         None             <none>        2888/TCP,3888/TCP            
svc/zk-svc         ClusterIP   10.111.37.187    <none>        2181/TCP                     
```
I have two services:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-hs
  namespace: cdc1
  labels:
    app: kafka
spec:
  ports:
  - port: 9092
    name: server
  clusterIP: None
  selector:
    app: kafka        
```
```yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-svc
  namespace: cdc1
  labels:
    app: kafka
spec:
  type: NodePort
  ports:
  - port: 9093
    name: external
    nodePort: 30182
  selector:
    app: kafka
```
I have set this env variables for Kafka:
```yaml
 - name: KAFKA_ADVERTISED_LISTENERS
   value: ""INTERNAL_PLAINTEXT://kafka-hs:9092,EXTERNAL_PLAINTEXT://MACHINE-IP:30182""
 - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
   value: ""INTERNAL_PLAINTEXT:PLAINTEXT,EXTERNAL_PLAINTEXT:PLAINTEXT""
 - name: KAFKA_LISTENERS
   value: ""INTERNAL_PLAINTEXT://0.0.0.0:9092,EXTERNAL_PLAINTEXT://0.0.0.0:30182""
 - name: KAFKA_INTER_BROKER_LISTENER_NAME
   value: ""INTERNAL_PLAINTEXT""
```

And when I try to produce message from outside the cluster I get the following error:
`
2018-04-27 13:27:01 WARN  NetworkClient:241 - [Producer clientId=producer-2] Connection to node -1 could not be established. Broker may not be available.`

I set the bootstrap-server configuration in the producer to `machine-IP:30182`

Any help will be really appreciated! Thanks!


",closed,False,2018-04-27 13:47:19,2018-12-21 07:09:48
contrib,AdamDang,https://github.com/kubernetes/contrib/pull/2892,https://api.github.com/repos/kubernetes/contrib/issues/2892,Typo fix in README.md: Kuberenetes->Kubernetes,Kuberenetes->Kubernetes,closed,True,2018-04-29 06:11:03,2018-04-29 06:47:14
contrib,AdamDang,https://github.com/kubernetes/contrib/pull/2893,https://api.github.com/repos/kubernetes/contrib/issues/2893,Typo fix: Kuberenetes->Kubernetes,Line 119: Kuberenetes->Kubernetes,closed,True,2018-04-29 06:48:27,2018-05-01 02:03:59
contrib,AdamDang,https://github.com/kubernetes/contrib/pull/2894,https://api.github.com/repos/kubernetes/contrib/issues/2894,Typo fix: reseting->resetting/seperate->separate,"reseting->resetting
seperate->separate",closed,True,2018-05-02 11:26:10,2018-08-24 05:17:13
contrib,Random-Liu,https://github.com/kubernetes/contrib/pull/2895,https://api.github.com/repos/kubernetes/contrib/issues/2895,Remove docker benchmark from node-perf-dash.,"`ci-kubernetes-node-docker-benchmark` does not exist anymore.

Signed-off-by: Lantao Liu <lantaol@google.com>",closed,True,2018-05-02 17:53:17,2018-05-14 22:19:45
contrib,audriusb,https://github.com/kubernetes/contrib/issues/2896,https://api.github.com/repos/kubernetes/contrib/issues/2896,Typo in zookeper's zkGenConfig.sh,"https://github.com/kubernetes/contrib/blob/23789f77c010fa129f4aa6a0bc8fed4aba065332/statefulsets/zookeeper/zkGenConfig.sh#L102
This should be  autopurge.purgeInterval",closed,False,2018-05-03 09:20:46,2019-01-28 21:23:43
contrib,abhaynayak24,https://github.com/kubernetes/contrib/issues/2897,https://api.github.com/repos/kubernetes/contrib/issues/2897,Kubernetes on red hat linux,"I've worked on kubernetes on ubuntu and am looking for same or similar features on red hat linux. I came across OpenShift and Ansible but am still unclear about its installation and working, is there any step by step setup documentation available? Also if anyone could suggest which one should I go ahead with",closed,False,2018-05-10 06:23:49,2018-10-07 08:25:06
contrib,damlub,https://github.com/kubernetes/contrib/pull/2898,https://api.github.com/repos/kubernetes/contrib/issues/2898,Update README.md,remove alpha status from RBAC apiVersion,closed,True,2018-05-14 09:12:28,2018-12-06 19:06:32
contrib,evgenius,https://github.com/kubernetes/contrib/pull/2899,https://api.github.com/repos/kubernetes/contrib/issues/2899,Fix a typo in README,,closed,True,2018-05-14 14:41:16,2018-10-11 16:07:08
contrib,venezia,https://github.com/kubernetes/contrib/pull/2900,https://api.github.com/repos/kubernetes/contrib/issues/2900,Fix for ingress-nginx auth example,[ingress-nginx v0.9.0](https://github.com/kubernetes/ingress-nginx/releases/tag/nginx-0.9.0) changed default annotation prefixes.  This PR updates the documentation so it is correct for default installations of ingress-nginx versions release after 12/4/2017,closed,True,2018-05-24 13:16:16,2018-05-24 23:41:33
contrib,jessfraz,https://github.com/kubernetes/contrib/issues/2901,https://api.github.com/repos/kubernetes/contrib/issues/2901,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",closed,False,2018-05-24 14:33:55,2018-12-21 09:09:48
contrib,jangrewe,https://github.com/kubernetes/contrib/pull/2902,https://api.github.com/repos/kubernetes/contrib/issues/2902,Fix typo: oper(t)ation,"Sorry, OCD kicked in.",closed,True,2018-05-25 17:50:13,2018-07-06 17:17:18
contrib,jeromefroe,https://github.com/kubernetes/contrib/issues/2903,https://api.github.com/repos/kubernetes/contrib/issues/2903,scratch-debugger: debugger fails to start if the /docker-cli directory does not exist,"The `scratch-debugger/debug.sh` script fails to work if the `/docker-cli` directory does not exist because `wget` is unable to download docker into it. For example:

```
$ ./debug.sh httpbin-7cb89bfd94-tk4j2
Debug Target Container:
  Pod:          httpbin-7cb89bfd94-tk4j2
  Namespace:    default
  Node:         ip-172-20-62-198.ec2.internal
  Container:    httpbin
  Container ID: 64e9e36eee378541839bbfd66726b622b7234abe2a99af7d9d8d6d034529b859
  Runtime:      docker

  ""Installing busybox to /tmp/debug-tools ...""

...

+ wget -qO/docker-cli/docker.tgz https://download.docker.com/linux/static/stable/x86_64/docker-17.12.0-ce.tgz
wget: can't open '/docker-cli/docker.tgz': No such file or directory
pod ""debugger-8gq6n"" deleted
```

By adding a command to ensure `docker-cli` exists prior to the `wget` command I was able to get the container to start.",closed,False,2018-05-25 19:39:14,2018-12-31 16:51:28
contrib,jeromefroe,https://github.com/kubernetes/contrib/pull/2904,https://api.github.com/repos/kubernetes/contrib/issues/2904,scratch-debugger: Ensure /docker-cli directory exists,"This diff addresses #2903 by ensuring that the `/docker-cli` exists. Presently, if the directory doesn't exists, the `wget` command will fail because it can't download docker into the directory.",closed,True,2018-05-25 19:41:51,2018-12-31 17:52:26
contrib,ravisantoshgudimetla,https://github.com/kubernetes/contrib/pull/2905,https://api.github.com/repos/kubernetes/contrib/issues/2905, Add a readme and deprecation note for rescheduler,"Since we are moving to priorty and preemption, we don't need rescheduler to move pods around to make room for new pods coming in.",closed,True,2018-05-30 17:07:23,2018-06-01 18:16:35
contrib,ms4720,https://github.com/kubernetes/contrib/issues/2906,https://api.github.com/repos/kubernetes/contrib/issues/2906,[keepalived-vip] keepalived-vip/vip-configmap.yaml needs namespace tag in yaml,"Without an explisit namespace, see below, it defaults to the '' namespace not default:

apiVersion: v1
kind: ConfigMap
metadata:
  name: vip-configmap
  namespace: default
data:
  10.4.0.50: default/echoheaders",closed,False,2018-05-31 06:14:34,2018-10-28 07:39:12
contrib,ms4720,https://github.com/kubernetes/contrib/issues/2907,https://api.github.com/repos/kubernetes/contrib/issues/2907,[keepalive-vip] Are there any plans to update the project?,"Hi,

Are there any plans to update the project?  It does not work with k8s v1.10.  If there are no plans to update could you mark it as abandoned to make things clearer.

Thanks",closed,False,2018-05-31 07:07:14,2018-11-29 09:14:13
contrib,AdamDang,https://github.com/kubernetes/contrib/pull/2908,https://api.github.com/repos/kubernetes/contrib/issues/2908,Typo fix in returned message: formating->formatting,Line 101: formating->formatting,closed,True,2018-05-31 15:09:35,2018-07-06 17:18:19
contrib,xiaohui-zhangxh,https://github.com/kubernetes/contrib/issues/2909,https://api.github.com/repos/kubernetes/contrib/issues/2909,Is there a latest Docker image supports vrrp_version=2 ?,"On Sep 22, 2017, there is a update
https://github.com/kubernetes/contrib/commit/37690195004301bfdb76f7f2d83bad2bb349c69c , it supports setting vrrp version to 2. I need this feature since when running 3 keepaliveds, the third node got this error message:

```
bogus VRRP packet received on eth0 !!!
VRRP_Instance(vips) ignoring received advertisment...
(vips): Invalid VRRPv3 checksum
```
Seems vrrp version 2 resolves this issue https://github.com/acassen/keepalived/issues/642

Whereas the docker image described on master branch is `k8s.gcr.io/kube-keepalived-vip:0.11` , vrrp is 3 and not configureable.",closed,False,2018-06-02 14:28:39,2018-10-30 16:35:21
contrib,ravisantoshgudimetla,https://github.com/kubernetes/contrib/pull/2910,https://api.github.com/repos/kubernetes/contrib/issues/2910,Rescheduler for dspods only,"This PR:
- Ensures rescheduler is run for pods which are created from DS controller.
- Rebases kube to 1.10 and bumps Godeps.

/cc @bsalamat ",closed,True,2018-06-02 23:13:15,2018-06-04 19:07:53
contrib,AdamDang,https://github.com/kubernetes/contrib/pull/2911,https://api.github.com/repos/kubernetes/contrib/issues/2911,Typo fix: kubeclt->kubectl,"Line 232: fix wrong command: 
kubeclt->kubectl",closed,True,2018-06-03 02:37:16,2018-06-20 00:27:34
contrib,cornelius-keller,https://github.com/kubernetes/contrib/pull/2912,https://api.github.com/repos/kubernetes/contrib/issues/2912,add option to use notfy scripts for keepalived,"Hi,
i modified keepalived-vip to add an option to use notify scripts. 
My change should be 100% backwards compatible to the previous behavior. 
It is enabled by mounting your notification script to the pod via a configmap, and then setting the env var `KEEPALIVED_NOTIFY` to the location where it is mounted. This will include  a line like:
`notify /opt/your-notify-script.sh` to the keepalived config.

I use this feature on cloud providers were you need a special api call to get an failover / floating IP routed to your machine.

I would like to have this merged so that I don't have to maintain a fork of keepalived-vip. ",closed,True,2018-06-03 19:33:24,2019-02-12 15:31:20
contrib,VelizarVESSELINOV,https://github.com/kubernetes/contrib/issues/2913,https://api.github.com/repos/kubernetes/contrib/issues/2913,Broken URL,"https://github.com/kubernetes/contrib/tree/master/ingress/controllers/gce/examples/https
firewall rule link is not working: 404",closed,False,2018-06-03 20:49:04,2018-11-21 20:16:13
contrib,Dm3Ch,https://github.com/kubernetes/contrib/issues/2914,https://api.github.com/repos/kubernetes/contrib/issues/2914,Rebuild gcr.io/google-containers/startup-script image needed,"There is new commit one year ago for startup-script. That commit makes possible to run multiple different scripts. 
https://github.com/kubernetes/contrib/tree/master/startup-script

But in gcr.io registry latest image was uploaded at 2016 before this commit.
Please push new version to gcr.io",open,False,2018-06-05 13:23:51,2019-03-13 18:35:45
contrib,nbrinks,https://github.com/kubernetes/contrib/issues/2915,https://api.github.com/repos/kubernetes/contrib/issues/2915,[election] - Response content is JSON but `Content-Type` header is `text/plain`,"Using the leader election sidecar.
```yaml
# Relevant content
      - image: k8s.gcr.io/leader-elector:0.5
        imagePullPolicy: IfNotPresent
        name: elector
        args:
          - --election=my-election
          - --http=localhost:4040
        ports:
        - containerPort: 4040
          protocol: TCP
        resources:
          requests:
            cpu: 100m
```
Here is an example response from the sidecar.
```
some-user@some-pod-569f9cf4c5-pjh4b:/$ curl -v localhost:4040/
*   Trying ::1...
* TCP_NODELAY set
* connect to ::1 port 4040 failed: Connection refused
*   Trying 127.0.0.1...
* TCP_NODELAY set
* Connected to localhost (127.0.0.1) port 4040 (#0)
> GET / HTTP/1.1
> Host: localhost:4040
> User-Agent: curl/7.52.1
> Accept: */*
>
< HTTP/1.1 200 OK
< Date: Tue, 05 Jun 2018 16:55:12 GMT
< Content-Length: 44
< Content-Type: text/plain; charset=utf-8
<
* Curl_http_done: called premature == 0
* Connection #0 to host localhost left intact
{""name"":""some-pod-569f9cf4c5-pjh4b""}
```
I am pretty sure the fix is to place
```go
res.Header().Set(""Content-Type"", ""application/json"")
```
before [this line](https://github.com/kubernetes/contrib/blob/master/election/example/main.go#L79) but I am having trouble setting up a development environment to test this.",closed,False,2018-06-05 19:10:20,2018-11-02 20:49:38
contrib,jstangroome,https://github.com/kubernetes/contrib/pull/2916,https://api.github.com/repos/kubernetes/contrib/issues/2916,Fix `No such file or directory` error from wget,,closed,True,2018-06-06 22:24:00,2018-11-04 00:16:40
contrib,chestack,https://github.com/kubernetes/contrib/issues/2917,https://api.github.com/repos/kubernetes/contrib/issues/2917,"Nginx ingress controller(configured with ""custom-http-errors"") should return original error code instead of 200 ",,closed,False,2018-06-11 06:36:07,2018-06-11 06:59:58
contrib,vineeth695,https://github.com/kubernetes/contrib/pull/2918,https://api.github.com/repos/kubernetes/contrib/issues/2918,Name change,,closed,True,2018-06-11 12:57:50,2018-11-11 09:10:42
contrib,mirake,https://github.com/kubernetes/contrib/pull/2919,https://api.github.com/repos/kubernetes/contrib/issues/2919,Typo fix: dependancies -> dependencies,,closed,True,2018-06-11 15:14:41,2018-09-08 14:56:02
contrib,AdamDang,https://github.com/kubernetes/contrib/pull/2920,https://api.github.com/repos/kubernetes/contrib/issues/2920,Typo fix in path: kubernets->kubernetes,Line 17: kubernets->kubernetes,closed,True,2018-06-21 05:24:31,2018-06-21 10:43:08
contrib,phemmmie,https://github.com/kubernetes/contrib/issues/2921,https://api.github.com/repos/kubernetes/contrib/issues/2921,etcd cluster deployment with ansible,"
Hello All;

I desperately need assistance in deploying an etcd cluster with this specification below

Provision an etcd cluster to serve as a service registry using ansible. Create a simple  service that will register itself on etcd so that it's discoverable by other services.  

*   Automate the creation of the etcd cluster using ansible;

*  The etcd cluster should contain at least 2 nodes;  
",closed,False,2018-06-22 10:47:31,2018-11-19 12:22:32
contrib,bsalamat,https://github.com/kubernetes/contrib/pull/2922,https://api.github.com/repos/kubernetes/contrib/issues/2922,Rescheduler considers pods critical only when they are in kube-system namespace,"Rescheduler has always considered pods only in kube-system namespace as critical pods. Recently, we made a change to consider pods with highest priority levels as critical as well. This PR ensures that such pods are considered as critical when they have such high priorities AND they are in kube-system namespace. This could help reduce possibility of malfunction in clusters under resource pressure where regular users have created pods with critical priorities in non-system namespaces.",closed,True,2018-06-23 18:05:31,2018-06-26 17:57:43
contrib,ironslob,https://github.com/kubernetes/contrib/issues/2923,https://api.github.com/repos/kubernetes/contrib/issues/2923,ingress controller reloading backend regularly (between 30s-3m),"I've run the ingress with --v=2 to see what's happening and get the following, which always seems to be the same:

```
104.156.229.24 - [104.156.229.24] - - [25/Jun/2018:15:26:49 +0000] ""GET /pro/ HTTP/1.1"" 200 4180 ""https://www.twigdoo.com/"" ""Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/98 Safari/537.4 (StatusCake)"" 522 1.683 [apps-twigdoo-web] 172.20.84.227:5000 16631 1.684 200 f5981fc9e19566a250f13d8fa7cbce60
104.238.159.87 - [104.238.159.87] - - [25/Jun/2018:15:26:54 +0000] ""HEAD / HTTP/1.1"" 307 0 ""-"" ""updown.io daemon 2.2"" 216 0.003 [apps-twigdoo-web] 172.20.127.53:5000 0 0.004 307 00d6edaaba8425444128740683ab5e51
104.238.159.87 - [104.238.159.87] - - [25/Jun/2018:15:26:54 +0000] ""HEAD /pro/ HTTP/1.1"" 200 0 ""-"" ""updown.io daemon 2.2"" 220 0.024 [apps-twigdoo-web] 172.20.84.227:5000 0 0.024 200 fe2bfb80727df5e8c8133d9b1842df81
138.68.24.60 - [138.68.24.60] - - [25/Jun/2018:15:26:56 +0000] ""GET /services/weddings/ HTTP/1.1"" 200 3919 ""-"" ""Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/98 Safari/537.4 (StatusCake)"" 499 0.020 [apps-twigdoo-web] 172.20.127.53:5000 19261 0.020 200 b8a8335581c3f4938587857c06f9cc01
I0625 15:27:01.584519       6 controller.go:169] Configuration changes detected, backend reload required.
I0625 15:27:01.584543       6 util.go:68] rlimit.max=1048576
I0625 15:27:01.584568       6 nginx.go:522] Maximum number of open file descriptors: 523264
I0625 15:27:01.656091       6 nginx.go:629] NGINX configuration diff:
--- /etc/nginx/nginx.conf       2018-06-25 15:26:12.608700061 +0000
+++ /tmp/new-nginx-cfg220593655 2018-06-25 15:27:01.652822169 +0000
@@ -213,6 +213,7 @@
 
                server 172.20.84.227:5000 max_fails=0 fail_timeout=0;
                server 172.20.127.53:5000 max_fails=0 fail_timeout=0;
+               server 172.20.116.105:5000 max_fails=0 fail_timeout=0;
 
        }
 
I0625 15:27:01.699733       6 controller.go:179] Backend successfully reloaded.
84.201.133.36 - [84.201.133.36] - - [25/Jun/2018:15:27:04 +0000] ""GET /sitemap/england/south-west/cornwall/brunnion/ HTTP/1.1"" 200 2936 ""-"" ""Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)"" 345 0.103 [apps-twigdoo-web] 172.20.84.227:5000 9041 0.104 200 8156bb867d872abc6ac37897a3ce3b85
I0625 15:27:04.917910       6 controller.go:169] Configuration changes detected, backend reload required.
I0625 15:27:04.917931       6 util.go:68] rlimit.max=1048576
I0625 15:27:04.917937       6 nginx.go:522] Maximum number of open file descriptors: 523264
I0625 15:27:04.965493       6 nginx.go:629] NGINX configuration diff:
--- /etc/nginx/nginx.conf       2018-06-25 15:27:01.652822169 +0000
+++ /tmp/new-nginx-cfg538364737 2018-06-25 15:27:04.960830406 +0000
@@ -211,9 +211,8 @@
 
                keepalive 32;
 
-               server 172.20.84.227:5000 max_fails=0 fail_timeout=0;
-               server 172.20.127.53:5000 max_fails=0 fail_timeout=0;
                server 172.20.116.105:5000 max_fails=0 fail_timeout=0;
+               server 172.20.84.227:5000 max_fails=0 fail_timeout=0;
 
        }
 
I0625 15:27:05.007378       6 controller.go:179] Backend successfully reloaded.
```

Any help on resolving this would be great, as I'm seeing regular 502 responses.",closed,False,2018-06-25 15:30:22,2018-11-22 17:37:12
contrib,bsalamat,https://github.com/kubernetes/contrib/pull/2924,https://api.github.com/repos/kubernetes/contrib/issues/2924,Update Rescheduler's version tag,,closed,True,2018-06-25 23:07:51,2018-06-26 00:02:52
contrib,zhangkesheng,https://github.com/kubernetes/contrib/issues/2925,https://api.github.com/repos/kubernetes/contrib/issues/2925,Zookeeper statefulset restart failed when all node restart at the same time,"Hello All,  
I have deployed Kafka and Zookeeper in a Kubernetes cluster with with three replicas, using the [zookeeper.yaml](https://github.com/kubernetes/contrib/blob/master/statefulsets/zookeeper/zookeeper.yaml).
When three node are simultaneously powered off and restarted, zookeeper restart failed. The distribution is as follows: 
zk-0 is assigned to the node with myid=2;  
zk-1 is assigned to the node with myid=3;  
zk-2 is assigned to the node with myid=1;  
Then, zk restar failed.
Hope to get a solution, Thanks!
",closed,False,2018-06-26 08:31:16,2018-11-23 09:53:10
contrib,yguo0905,https://github.com/kubernetes/contrib/pull/2926,https://api.github.com/repos/kubernetes/contrib/issues/2926,Fix Rescheduler Makefile,"1. Allow overriding the `REGISTRY` from command line.
2. Create a image without the `ARCH` in its name for backward compatibility.

/kind bug
/assign @bsalamat ",closed,True,2018-06-26 17:56:24,2018-06-27 21:55:21
contrib,DStorck,https://github.com/kubernetes/contrib/pull/2927,https://api.github.com/repos/kubernetes/contrib/issues/2927,fixed typos and added two links,,closed,True,2018-07-02 17:03:14,2018-12-30 18:29:32
contrib,caiobrentano,https://github.com/kubernetes/contrib/pull/2928,https://api.github.com/repos/kubernetes/contrib/issues/2928,Typo Fix on statefulsets/kafka readme," ""...a Pod **Anti-Affinity** and a Pod **Anti-Affinity** rule."" is redundant.
It should be ""...a Pod Affinity and a Pod Anti-Affinity rule.""",closed,True,2018-07-05 14:19:08,2018-12-02 18:33:04
contrib,AdamDang,https://github.com/kubernetes/contrib/pull/2929,https://api.github.com/repos/kubernetes/contrib/issues/2929,Typo fix: architecures->architectures,Line 7: architecures->architectures,closed,True,2018-07-09 05:21:35,2018-10-11 10:05:24
contrib,bojanv55,https://github.com/kubernetes/contrib/issues/2930,https://api.github.com/repos/kubernetes/contrib/issues/2930,How is this possible?,"I installed curl in the elector image. Running 3 replicas.

```
# curl -sSk -H ""Authorization: Bearer $TOKEN"" \
      https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT/api/v1/namespaces/default/endpoints/example>
{
  ""kind"": ""Endpoints"",
  ""apiVersion"": ""v1"",
  ""metadata"": {
    ""name"": ""example"",
    ""namespace"": ""default"",
    ""selfLink"": ""/api/v1/namespaces/default/endpoints/example"",
    ""uid"": ""7dfccbfa-85b6-11e8-b2ab-08002702cbca"",
    ""resourceVersion"": ""3202186"",
    ""creationTimestamp"": ""2018-07-12T09:32:27Z"",
    ""annotations"": {
      ""control-plane.alpha.kubernetes.io/leader"": ""{\""holderIdentity\"":\""aspectcaffleader-84d5769dbd-wq9cf\"",\""leaseDurationSeconds\"":10,\""acquireTime\"":\""2018-07-12T09:34:59Z\"",\""renewTime\"":\""2018-07-12T10:13:39Z\"",\""leaderTransitions\"":0}""
    }
  }
}# curl localhost:4040
{""name"":""aspectcaffleader-59df7bb4fc-8slv8""}#
```

Master that is in the endpoint is not the same as the one reported by elector locally?

Update is done OK when master is deleted and new MASTER has OK name, but name of the master returned by STANDBY nodes is not correct.",open,False,2018-07-12 10:15:43,2019-03-20 08:06:53
contrib,guoshimin,https://github.com/kubernetes/contrib/pull/2931,https://api.github.com/repos/kubernetes/contrib/issues/2931,consider all addresses on an interface,An interface can have multiple addresses. We should consider all addresses on an interface when we are looking for an interface with a given address.,open,True,2018-07-19 23:34:08,2019-02-18 19:38:29
contrib,raggi,https://github.com/kubernetes/contrib/pull/2932,https://api.github.com/repos/kubernetes/contrib/issues/2932,[go2docker] remove unmaintained insecure contrib,"See #2880 for one example of the sad history.

I sent pull requests to update/fix things in the past that also went stale and were closed.

As no one with +2 rights is willing to perform even reviews, this code should be removed as it contains security issues.

@shtatfeld ",closed,True,2018-08-04 08:07:50,2018-08-05 01:16:44
contrib,fredrik-jansson-se,https://github.com/kubernetes/contrib/issues/2933,https://api.github.com/repos/kubernetes/contrib/issues/2933,Leader Elector HTTP server mismatch,"I have three pods that I distribute on three nodes using anti affinity rules. 

To test the leader election:
- I bring the nodes up, 
- cordons the leader pod's node (to buy some time to force the other pods to select a new leader)
- kills the leader pod

According to the logs, the two remaining pods agrees upon a new leader. 

Problem: One of the pod's leader-elector http server still returns the old leader.

I can consistently reproduce this on my cluster.

Kubernetes is v1.11.1
leader-elector is v0.5 (I also tested with v0.4 with the same results).

Given the widespread use of the leader-elector, I assume I do something wrong... but I really cannot figure out what.

```
frjansso@kube-1:/mnt/kube-ha/nso-ha-test$ kubectl get pod
NAME      READY     STATUS    RESTARTS   AGE
nso-0     2/2       Running   0          37m
nso-1     2/2       Running   0          37m
nso-2     2/2       Running   0          37m
frjansso@kube-1:/mnt/kube-ha/nso-ha-test$ kubectl exec -it nso-0 bash
Defaulting container name to nso-master.
Use 'kubectl describe pod/nso-0 -n default' to see all of the containers in this pod.
root@nso-0:/# curl http://localhost:4040
{""name"":""nso-0""}
root@nso-0:/# exit
exit
frjansso@kube-1:/mnt/kube-ha/nso-ha-test$ kubectl exec -it nso-2 bash
Defaulting container name to nso-master.
Use 'kubectl describe pod/nso-2 -n default' to see all of the containers in this pod.
root@nso-2:/# curl http://localhost:4040
{""name"":""nso-0""}
root@nso-2:/#
root@nso-2:/# exit
command terminated with exit code 127
frjansso@kube-1:/mnt/kube-ha/nso-ha-test$ kubectl get pod -o=wide
NAME      READY     STATUS    RESTARTS   AGE       IP                NODE
nso-0     2/2       Running   0          40m       192.168.89.225    kube-3
nso-1     2/2       Running   0          40m       192.168.79.237    kube-2
nso-2     2/2       Running   0          40m       192.168.126.121   kube-1
frjansso@kube-1:/mnt/kube-ha/nso-ha-test$ kubectl cordon kube-3
node/kube-3 cordoned
frjansso@kube-1:/mnt/kube-ha/nso-ha-test$ kubectl delete pod nso-0
pod ""nso-0"" deleted

frjansso@kube-1:/mnt/kube-ha/nso-ha-test$ kubectl logs nso-1 elector
....
I0806 16:45:05.408266       8 leaderelection.go:296] lock is held by nso-0 and has not yet expired
I0806 16:45:09.796249       8 leaderelection.go:296] lock is held by nso-0 and has not yet expired
**nso-1 is the leader**
I0806 16:45:14.161264       8 leaderelection.go:215] sucessfully acquired lease default/nso-svc
frjansso@kube-1:/mnt/kube-ha/nso-ha-test$ kubectl logs nso-2 elector
....
I0806 16:45:12.120216       7 leaderelection.go:296] lock is held by nso-0 and has not yet expired
I0806 16:45:16.479642       7 leaderelection.go:296] lock is held by nso-1 and has not yet expired

frjansso@kube-1:/mnt/kube-ha/nso-ha-test$ kubectl exec -it nso-1 bash
\Defaulting container name to nso-master.
Use 'kubectl describe pod/nso-1 -n default' to see all of the containers in this pod.
\root@nso-1:/# curl http://localhost:4040
{""name"":""nso-1""}
root@nso-1:/# exit
exit
frjansso@kube-1:/mnt/kube-ha/nso-ha-test$ kubectl exec -it nso-2 bash
Defaulting container name to nso-master.
Use 'kubectl describe pod/nso-2 -n default' to see all of the containers in this pod.
root@nso-2:/# curl http://localhost:4040
{""name"":""nso-0""}
root@nso-2:/#
```",closed,False,2018-08-06 17:04:25,2019-01-04 02:11:30
contrib,dims,https://github.com/kubernetes/contrib/pull/2934,https://api.github.com/repos/kubernetes/contrib/issues/2934,Remove apparmor/loader,"Moved this code to test/images/apparmor-loader/ in main k/k repository
as we need this in e2e tests.

Please see https://github.com/kubernetes/kubernetes/pull/67030

Change-Id: Ib805e676b9f2ddfc8648a080cef7a2ce9cf243d1",closed,True,2018-08-08 02:03:53,2018-08-10 12:12:05
contrib,xinau,https://github.com/kubernetes/contrib/pull/2935,https://api.github.com/repos/kubernetes/contrib/issues/2935,[keepalived-vip] add keepalived-cloud-provider as related project,"As the keepalived-cloud-provider is referenced in the [official documentation][1] and it uses the `kube-keepalived-vip`. I would like to add it as a related project.

[1]: https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#examples",closed,True,2018-08-08 08:32:45,2018-11-07 18:19:03
contrib,spiffxp,https://github.com/kubernetes/contrib/pull/2936,https://api.github.com/repos/kubernetes/contrib/issues/2936,Add spiffxp as root OWNER,I want to help remove things from this repo,closed,True,2018-08-09 13:38:15,2018-08-10 23:08:33
contrib,fqsghostcloud,https://github.com/kubernetes/contrib/pull/2937,https://api.github.com/repos/kubernetes/contrib/issues/2937,fix typo,fix typo,closed,True,2018-08-14 01:34:09,2019-02-09 17:02:20
contrib,johnaoss,https://github.com/kubernetes/contrib/pull/2938,https://api.github.com/repos/kubernetes/contrib/issues/2938,Fixes 404s in keepalived-vip README,,closed,True,2018-08-14 13:47:25,2018-12-12 17:29:35
contrib,wangxy518,https://github.com/kubernetes/contrib/pull/2939,https://api.github.com/repos/kubernetes/contrib/issues/2939,Update README.md,the url has been changed,closed,True,2018-08-16 07:10:06,2018-08-28 20:52:00
contrib,forkbomber,https://github.com/kubernetes/contrib/pull/2940,https://api.github.com/repos/kubernetes/contrib/issues/2940,Zookeeper: fix misspelled purgeInterval variable,,closed,True,2018-08-27 07:51:15,2018-09-05 18:26:17
contrib,alkaliphile,https://github.com/kubernetes/contrib/pull/2941,https://api.github.com/repos/kubernetes/contrib/issues/2941,Added nginx prefix to annotation metadata,,closed,True,2018-08-30 00:41:56,2018-08-30 17:51:14
contrib,arunpjohny,https://github.com/kubernetes/contrib/issues/2942,https://api.github.com/repos/kubernetes/contrib/issues/2942,Zookeeper: Auto purge task is not starting,"Even after setting the `ZK_PURGE_INTERVAL` environment variable, the auto purge task to remove older snapshots and logs is not getting started leading to out of disk errors.

Zookeeper startup logs

> + zkGenConfig.sh
> Validating environment
> ...
> ZK_PURGE_INTERVAL=1
> ENSEMBLE
> ...
> Environment validation successful
> Creating ZooKeeper configuration
> Wrote ZooKeeper configuration file to /opt/zookeeper/conf/zoo.cfg
> Creating ZooKeeper log4j configuration
> Wrote log4j configuration to /opt/zookeeper/conf/log4j.properties
> Creating ZooKeeper data directories and setting permissions
> Created ZooKeeper data directories and set permissions in /var/lib/zookeeper/data
> Creating JVM configuration file
> Wrote JVM configuration to /opt/zookeeper/conf/java.env
> + exec zkServer.sh start-foreground
> ZooKeeper JMX enabled by default
> ZooKeeper remote JMX Port set to 1099
> ZooKeeper remote JMX authenticate set to false
> ZooKeeper remote JMX ssl set to false
> ZooKeeper remote JMX log4j set to true
> Using config: /usr/bin/../etc/zookeeper/zoo.cfg
> 2018-08-30 16:13:36,321 [myid:] - INFO  [main:QuorumPeerConfig@134] - Reading configuration from: /usr/bin/../etc/zookeeper/zoo.cfg
> 2018-08-30 16:13:41,352 [myid:] - INFO  [main:QuorumPeer$QuorumServer@167] - Resolved hostname: gt-zk-zookeeper-2.gt-zk-zookeeper-headless.common.svc.cluster.local to address: gt-zk-zookeeper-2.gt-zk-zookeeper-headless.common.svc.cluster.local/100.120.0.11
> 2018-08-30 16:13:41,354 [myid:] - INFO  [main:QuorumPeer$QuorumServer@167] - Resolved hostname: gt-zk-zookeeper-1.gt-zk-zookeeper-headless.common.svc.cluster.local to address: gt-zk-zookeeper-1.gt-zk-zookeeper-headless.common.svc.cluster.local/100.115.0.8
> 2018-08-30 16:13:41,362 [myid:] - INFO  [main:QuorumPeer$QuorumServer@167] - Resolved hostname: gt-zk-zookeeper-0.gt-zk-zookeeper-headless.common.svc.cluster.local to address: gt-zk-zookeeper-0.gt-zk-zookeeper-headless.common.svc.cluster.local/100.123.128.7
> 2018-08-30 16:13:41,363 [myid:] - INFO  [main:QuorumPeerConfig@396] - Defaulting to majority quorums
> 2018-08-30 16:13:41,371 [myid:1] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
> 2018-08-30 16:13:41,372 [myid:1] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0
> 2018-08-30 16:13:41,372 [myid:1] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.",closed,False,2018-08-31 09:04:24,2018-09-11 21:46:58
contrib,arunpjohny,https://github.com/kubernetes/contrib/pull/2943,https://api.github.com/repos/kubernetes/contrib/issues/2943,Zookeeper: Typo fix in purge interval property,"Fix typo in the `autopurge.purgeInterval` property

Auto purge task is not getting scheduled because of the typo in this property

fixes #2942",closed,True,2018-08-31 09:18:34,2018-09-11 21:46:58
contrib,andrenarchy,https://github.com/kubernetes/contrib/issues/2944,https://api.github.com/repos/kubernetes/contrib/issues/2944,[peer-finder] let on-start/on-change write to stdout/stderr directly,"Currently peer-finder waits until the script finishes before writing the output to stdout. This makes it hard/impossible to debug when the script hangs. 

Instead it should write to both stdout and stderr directly. There was a PR https://github.com/kubernetes/contrib/pull/2866 to address this but it got closed because the approach apparently had some drawbacks.",closed,False,2018-09-01 08:05:41,2019-02-02 14:43:51
contrib,azrle,https://github.com/kubernetes/contrib/pull/2945,https://api.github.com/repos/kubernetes/contrib/issues/2945,add timeout to exec-healthz,"Hi,

It is a PR to show the idea that it maybe good to introduce `timeout` option.

The command itself sometimes hangs up for some reasons (e.g. network IO) and may run forever. It should be good to implement timeout in commands; however sometimes it is difficult to make changes to commands and cover every cases. Since timeout could be a general case, I would like to see it is implemented in `exec-healthz`.

Note: this is a just workable code sample to show what I want to do. It could break tests, dependency and minimum required version of golang and packages. (Sorry that I am not familiar with policies and rules of this repo and cannot make a perfect PR; also saw this project is moving)",closed,True,2018-09-03 11:08:48,2019-01-31 12:54:49
contrib,wangxy518,https://github.com/kubernetes/contrib/pull/2946,https://api.github.com/repos/kubernetes/contrib/issues/2946,Update console_windows.go,These urls have been changed.,closed,True,2018-09-04 01:58:25,2019-02-01 04:09:47
contrib,oliverisaac,https://github.com/kubernetes/contrib/pull/2947,https://api.github.com/repos/kubernetes/contrib/issues/2947,Use -domain and -ns flags when performing lookup,"In previous version of peer-finder if you specify -domain or -ns it doesn't actually look in those domains and namespaces. This PR switches the lookup from using just `$svc` to using `$svc.$ns.svc.$domain`. 

Note that due to [go 1.11 not supporting compressed hostnames in SRV records](https://groups.google.com/forum/#!topic/golang-nuts/vAbjprJNPV0) building this on go 1.11 will cause it to no work when using kube-dns.
I have successfully tested it with go 1.10.4 connecting to a kube-dns instance.",closed,True,2018-09-06 16:05:13,2019-03-04 21:07:10
contrib,mirake,https://github.com/kubernetes/contrib/pull/2948,https://api.github.com/repos/kubernetes/contrib/issues/2948,Fix typo: permissable -> permissible,Signed-off-by: ruicao <ruicao@alauda.io>,closed,True,2018-09-08 14:59:06,2018-09-12 11:59:40
contrib,mirake,https://github.com/kubernetes/contrib/pull/2949,https://api.github.com/repos/kubernetes/contrib/issues/2949,Fix typo: recommanded -> recommended,,closed,True,2018-09-12 12:03:09,2018-09-23 13:20:05
contrib,liuzhi1986,https://github.com/kubernetes/contrib/issues/2950,https://api.github.com/repos/kubernetes/contrib/issues/2950,create kafka cluster error,"my yaml file like this:
```
apiVersion: v1
kind: Service
metadata:
  name: kafka-hs
  labels:
    app: kafka
spec:
  ports:
  - port: 9093
    name: server
  clusterIP: None
  selector:
    app: kafka
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: kafka-pdb
spec:
  selector:
    matchLabels:
      app: kafka
  minAvailable: 1
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: kafka
spec:
  serviceName: kafka-hs
  replicas: 3
  template:
    metadata:
      labels:
        app: kafka
    spec:
      imagePullSecrets:
      - name: hub.nexus.key
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: ""app""
                    operator: In
                    values:
                    - kafka
              topologyKey: ""kubernetes.io/hostname""
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
             - weight: 1
               podAffinityTerm:
                 labelSelector:
                    matchExpressions:
                      - key: ""app""
                        operator: In
                        values:
                        - zk
                 topologyKey: ""kubernetes.io/hostname""
      terminationGracePeriodSeconds: 300
      containers:
      - name: kubernetes-kafka
        imagePullPolicy: Always
        image: nexus.sss.com:18443/kafka:2.11-2.0.0
        resources:
          requests:
            memory: ""1Gi""
            cpu: 500m
        ports:
        - containerPort: 9093
          name: server
        command:
        - sh
        - -c
        - ""exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} \
          --override listeners=PLAINTEXT://:9093 \
          --override port=9093 \
          --override zookeeper.connect=zk-0.zk-hs.default.svc.cluster.local:2181,zk-1.zk-hs.default.svc.cluster.local:2181,zk-2.zk-hs.default.svc.cluster.local:2181 \
          --override log.dir=/var/lib/kafka/log \
          --override log.dirs=/var/lib/kafka/log ""
        env:
        - name: KAFKA_HEAP_OPTS
          value : ""-Xmx512M -Xms512M""
        - name: KAFKA_OPTS
          value: ""-Dlogging.level=INFO""
        volumeMounts:
        - name: datadir
          mountPath: /var/lib/kafka
        readinessProbe:
          exec:
           command:
            - sh
            - -c
            - ""/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093""
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 10Gi

```
I try to start the cluster, the kafka-0 throw the exception like this:
```
[2018-09-13 07:18:29,166] WARN [Controller id=0, targetBrokerId=1] Error connecting to node kafka-1.kafka-hs.default.svc.cluster.local:9093 (id: 1 rack: null) (org.apache.kafka.clients.NetworkClient)
java.io.IOException: Can't resolve address: kafka-1.kafka-hs.default.svc.cluster.local:9093
        at org.apache.kafka.common.network.Selector.doConnect(Selector.java:235)
        at org.apache.kafka.common.network.Selector.connect(Selector.java:214)
        at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:864)
        at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:265)
        at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:64)
        at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:279)
        at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:233)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
Caused by: java.nio.channels.UnresolvedAddressException
        at sun.nio.ch.Net.checkAddress(Net.java:101)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:622)
        at org.apache.kafka.common.network.Selector.doConnect(Selector.java:233)
        ... 7 more
```
Has anyone ever been in a similar situation?
",closed,False,2018-09-13 07:19:41,2019-02-10 09:18:21
contrib,aedeph,https://github.com/kubernetes/contrib/pull/2951,https://api.github.com/repos/kubernetes/contrib/issues/2951,Fixes kafka stateful set configuration for proper directory entry in …,"…overrides section in run server

Motivation: according to [trunc code](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/KafkaConfig.scala#L1117) (which describes in quite a vague fashion in [official documentation](https://kafka.apache.org/documentation/#brokerconfigs)) log.dir section of configuration is used as a topic log directory if and only if log.dirs is not present. However, default /opt/kafka_2.11-0.10.2.0/config/server.properties from current gcr.io/google-samples/k8skafka contains entry of log.dirs=/tmp/kafka-logs, so override does nothing useful and in fact is very misleading, producing probable data loss, when mounting external volume to /var/lib/kafka.",closed,True,2018-09-17 12:49:10,2019-03-25 12:10:24
contrib,thenaeem,https://github.com/kubernetes/contrib/issues/2952,https://api.github.com/repos/kubernetes/contrib/issues/2952,"Leader Elector: endpoints ""example"" is forbidden.","I tried to run leader elector but failing with `crashLoopbackOff`.
Logs show the output of 
```
kubectl logs -f leader-elector-765897d8db-86cxw
F0918 09:07:53.969034       6 main.go:108] failed to create election: endpoints ""example"" is forbidden: User ""system:serviceaccount:default:default"" cannot get endpoints in the namespace ""default""
```
Is this something related to RBAC.

kubernetes version: v1.10.0",closed,False,2018-09-18 09:17:08,2018-10-18 11:01:29
contrib,NothingButTheHoff,https://github.com/kubernetes/contrib/pull/2953,https://api.github.com/repos/kubernetes/contrib/issues/2953,Corrected url to leader pod,,closed,True,2018-09-19 09:48:50,2019-03-27 07:15:44
contrib,dwdraju,https://github.com/kubernetes/contrib/pull/2954,https://api.github.com/repos/kubernetes/contrib/issues/2954,fix link for ingress controller gce,,closed,True,2018-09-19 17:48:27,2019-01-18 04:58:31
contrib,kamesh2229,https://github.com/kubernetes/contrib/issues/2955,https://api.github.com/repos/kubernetes/contrib/issues/2955,How to change cluster IP in a replication controller run time,"Hi,

I am using Kubernetes 1.0.3 where a master and 5 minion nodes deployed.
I have an Elasricsearch application that is deployed on 3 nodes using a replication controller and service is defined.
Now i have added a new minion node to the cluster and wanted to run the container elasticsearch on the new node.
I am scaling my replication controller to 4 so that based on the node label the elasticsearch container is deployed on new node.Below is my issue and please let me k ow if there is any solution ?

1) The cluster IP defined in the RC is wrong as it is not the same in service.yaml file. 
2) Now when I scale the RC new node is installed with the ES container pointing to the wrong Cluster IP due to which the new node is not joining the ES cluster.
3) Is there any way that I can modify the cluster IP of deployed RC so that when I scale the RC the image is deployed on new node with the correct cluster IP ? 

Since I am using old version I don't see kubectl edit command and I tried changing using kubectl patch command but the IP didn't change.

The problem is that I need to do this on a production cluster so I can't delete the existing pods but only option is to change the cluster IP of deployed RC and then scale so that it will take the new IP and image is started accordingly.

Please let me know if any way I can do this ? ",closed,False,2018-09-22 19:06:00,2019-02-25 09:11:37
contrib,mirake,https://github.com/kubernetes/contrib/pull/2956,https://api.github.com/repos/kubernetes/contrib/issues/2956,Fix some typos: Editting -> Editing/do -> to,"fix some typos:
Editting -> Editing
do -> to

Signed-off-by: Rui Cao <ruicao@alauda.io>",closed,True,2018-09-23 13:21:50,2018-10-02 14:13:42
contrib,cox88,https://github.com/kubernetes/contrib/issues/2957,https://api.github.com/repos/kubernetes/contrib/issues/2957,Change host interface of VIP,"On a k8s cluster with traefik ingress controller, I need keepalived in front of traefik to keep a VIP.
Each node has 2 net iface : ens3 & ens4.

The issue is the keepalived containers select only the first interface to assign VIP ( ens3 ). 
How to switch to second interface ? (ens4) 

",open,False,2018-09-27 12:10:36,2019-03-29 09:30:42
contrib,jonpulsifer,https://github.com/kubernetes/contrib/pull/2958,https://api.github.com/repos/kubernetes/contrib/issues/2958,[404-server] update image location,"Updates the location of the 404-server to point to the correct
repository. Requires https://github.com/kubernetes/ingress-gce/pull/503

Signed-off-by: Jonathan Pulsifer <jonathan.pulsifer@shopify.com>",open,True,2018-09-28 21:25:32,2019-03-19 03:25:52
contrib,mirake,https://github.com/kubernetes/contrib/pull/2959,https://api.github.com/repos/kubernetes/contrib/issues/2959,Fix typo: statisitcs -> statistics,Signed-off-by: Rui Cao <ruicao@alauda.io>,closed,True,2018-10-06 00:41:50,2019-01-28 03:31:05
contrib,fqsghostcloud,https://github.com/kubernetes/contrib/pull/2960,https://api.github.com/repos/kubernetes/contrib/issues/2960,fix typo,fix typo,closed,True,2018-10-09 02:53:45,2018-10-09 02:59:10
contrib,igostv,https://github.com/kubernetes/contrib/issues/2961,https://api.github.com/repos/kubernetes/contrib/issues/2961,Add to keepalived-vip Readme info about opportunity of VIP only with no backend service,"For now, a feature describtion can be found only in comments in controller.go

// if target is empty string we will not forward to any service but
// instead just configure the IP on the machine and let it up to
// another Pod or daemon to bind to the IP address",closed,False,2018-10-09 07:25:35,2019-03-08 08:29:23
contrib,sunlintong,https://github.com/kubernetes/contrib/pull/2962,https://api.github.com/repos/kubernetes/contrib/issues/2962,fix typo,,closed,True,2018-10-11 06:32:25,2018-10-11 06:34:29
contrib,xichengliudui,https://github.com/kubernetes/contrib/pull/2963,https://api.github.com/repos/kubernetes/contrib/issues/2963,Delete duplicate 'the',Delete duplicate 'the',closed,True,2018-10-17 06:11:38,2018-10-31 01:35:44
contrib,sagikazarmark,https://github.com/kubernetes/contrib/pull/2964,https://api.github.com/repos/kubernetes/contrib/issues/2964,Fix example command,,open,True,2018-10-19 15:09:56,2019-01-27 01:31:33
contrib,cablespaghetti,https://github.com/kubernetes/contrib/pull/2965,https://api.github.com/repos/kubernetes/contrib/issues/2965,Update Zookeeper Docker Container to 3.4.13 and Ubuntu 18.04,"I noticed that the Zookeeper container in this repository was a few versions out of date and while I was there I upgraded it to Ubuntu 18.04 from 16.04. I am running this in my environment and it works properly.

Built image here for convenient testing: https://hub.docker.com/r/cablespaghetti/k8szk",open,True,2018-10-24 11:10:44,2019-03-17 23:13:46
contrib,cablespaghetti,https://github.com/kubernetes/contrib/pull/2966,https://api.github.com/repos/kubernetes/contrib/issues/2966,Change golint path to fix travis failures,I noticed that the builds on my  PR (#2965) were failing for no good reason. This seems to be due to a URL change (see https://github.com/golang/lint/issues/415).,closed,True,2018-10-24 14:20:58,2019-03-23 15:26:29
contrib,mattymo,https://github.com/kubernetes/contrib/pull/2967,https://api.github.com/repos/kubernetes/contrib/issues/2967,Add new vars to zookeeper,These are extra vars that should be specified when deploying ZK for clickhouse,closed,True,2018-10-24 15:37:51,2019-03-23 17:28:27
contrib,HaraldNordgren,https://github.com/kubernetes/contrib/pull/2968,https://api.github.com/repos/kubernetes/contrib/issues/2968,Bump Go versions and use '.x' to always get latest patch versions,,closed,True,2018-10-28 19:27:59,2018-10-28 19:50:08
contrib,cablespaghetti,https://github.com/kubernetes/contrib/pull/2969,https://api.github.com/repos/kubernetes/contrib/issues/2969,Fix Broken Link #2913,,closed,True,2018-10-29 21:27:46,2019-03-28 23:34:24
contrib,zuoinlv,https://github.com/kubernetes/contrib/issues/2970,https://api.github.com/repos/kubernetes/contrib/issues/2970,"kafka pod connection failed in kubernetes v1.10.3, but works in kubernetes v1.11.1","I would like to expose the kafka service for external access using NodePort, but service could not be accessed from the pod, logs from one of kafka pods:

> [2018-10-29 17:41:54,070] WARN [Controller-0-to-broker-1-send-thread], Controller 0's connection to broker kafka-1.kafka.default.svc.cluster.local:9093 (id: 1 rack: null) was unsuccessful (kafka.controller.RequestSendThread)
> java.io.IOException: Connection to kafka-1.kafka.default.svc.cluster.local:9092 (id: 1 rack: null) failed


 Service object in kafka.yaml :
---
apiVersion: v1
kind: Service
metadata:
  name: kafka
  labels:
    app: kafka
spec:
  type: NodePort
  ports:
  - port: 9093
    name: server
  selector:
    app: kafka
---
",closed,False,2018-10-30 02:42:33,2019-03-29 06:41:22
contrib,yeya24,https://github.com/kubernetes/contrib/pull/2972,https://api.github.com/repos/kubernetes/contrib/issues/2972,fix a typo kubenetes -> kubernetes,fix a typo in line 16: kubenetes -> kubernetes,closed,True,2018-11-03 01:41:55,2018-11-05 08:19:43
contrib,mooncak,https://github.com/kubernetes/contrib/pull/2973,https://api.github.com/repos/kubernetes/contrib/issues/2973,"Fix typos: statisitcs->statistics, resonse->response","Fix typos: statisitcs->statistics, resonse->response",open,True,2018-11-10 11:23:45,2019-03-10 12:19:15
contrib,charlie-charlie,https://github.com/kubernetes/contrib/issues/2974,https://api.github.com/repos/kubernetes/contrib/issues/2974,my kafka broker failed to start.,"zooker pods started:
kubectl -n=vle-dev get pods
NAME         READY     STATUS    RESTARTS   AGE
kafka-zk-0   1/1       Running   0          45m
kafka-zk-1   1/1       Running   0          45m
kafka-zk-2   1/1       Running   0          44m
kubectl -n=vle-dev get svc
NAME      TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE
zk-svc    ClusterIP   None         <none>        2888/TCP,3888/TCP   48m
but when apply kafka.yaml,  it failed to start(following are exception of describe pod output):
Controlled By:  StatefulSet/kafka
Containers:
  k8skafka:
    Container ID:  docker://826d9d27b810886beeb8fdd53fbd4a5ff7c59aa49f8d900e1a4556485575a901
    Image:         gcr.io/google_samples/k8skafka:v1
    Image ID:      docker-pullable://gcr.io/google_samples/k8skafka@sha256:1be8f40245992b94196c998d42a27da3840104c41eb78b8a389276a2c5d3b96f
    Port:          9093/TCP
    Host Port:     0/TCP
    Command:
      sh
      -c
      exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} --override listeners=PLAINTEXT://:9093 --override zookeeper.connect=kafka-zk-0.zk-svc.vle-dev.svc.cluster.local:2181,kafka-zk-1.zk-svc.vle-dev.svc.cluster.local:2181,kafka-zk-2.zk-svc.vle-dev.svc.cluster.local:2181 --override log.dir=/var/lib/kafka --override auto.create.topics.enable=true --override auto.leader.rebalance.enable=true --override background.threads=10 --override compression.type=producer --override delete.topic.enable=false --override leader.imbalance.check.interval.seconds=300 --override leader.imbalance.per.broker.percentage=10 --override log.flush.interval.messages=9223372036854775807 --override log.flush.offset.checkpoint.interval.ms=60000 --override log.flush.scheduler.interval.ms=9223372036854775807 --override log.retention.bytes=-1 --override log.retention.hours=168 --override log.roll.hours=168 --override log.roll.jitter.hours=0 --override log.segment.bytes=1073741824 --override log.segment.delete.delay.ms=60000 --override message.max.bytes=1000012 --override min.insync.replicas=1 --override num.io.threads=8 --override num.network.threads=3 --override num.recovery.threads.per.data.dir=1 --override num.replica.fetchers=1 --override offset.metadata.max.bytes=4096 --override offsets.commit.required.acks=-1 --override offsets.commit.timeout.ms=5000 --override offsets.load.buffer.size=5242880 --override offsets.retention.check.interval.ms=600000 --override offsets.retention.minutes=1440 --override offsets.topic.compression.codec=0 --override offsets.topic.num.partitions=50 --override offsets.topic.replication.factor=3 --override offsets.topic.segment.bytes=104857600 --override queued.max.requests=500 --override quota.consumer.default=9223372036854775807 --override quota.producer.default=9223372036854775807 --override replica.fetch.min.bytes=1 --override replica.fetch.wait.max.ms=500 --override replica.high.watermark.checkpoint.interval.ms=5000 --override replica.lag.time.max.ms=10000 --override replica.socket.receive.buffer.bytes=65536 --override replica.socket.timeout.ms=30000 --override request.timeout.ms=30000 --override socket.receive.buffer.bytes=102400 --override socket.request.max.bytes=104857600 --override socket.send.buffer.bytes=102400 --override unclean.leader.election.enable=true --override zookeeper.session.timeout.ms=6000 --override zookeeper.set.acl=false --override broker.id.generation.enable=true --override connections.max.idle.ms=600000 --override controlled.shutdown.enable=true --override controlled.shutdown.max.retries=3 --override controlled.shutdown.retry.backoff.ms=5000 --override controller.socket.timeout.ms=30000 --override default.replication.factor=1 --override fetch.purgatory.purge.interval.requests=1000 --override group.max.session.timeout.ms=300000 --override group.min.session.timeout.ms=6000 --override inter.broker.protocol.version=0.10.2-IV0 --override log.cleaner.backoff.ms=15000 --override log.cleaner.dedupe.buffer.size=134217728 --override log.cleaner.delete.retention.ms=86400000 --override log.cleaner.enable=true --override log.cleaner.io.buffer.load.factor=0.9 --override log.cleaner.io.buffer.size=524288 --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308 --override log.cleaner.min.cleanable.ratio=0.5 --override log.cleaner.min.compaction.lag.ms=0 --override log.cleaner.threads=1 --override log.cleanup.policy=delete --override log.index.interval.bytes=4096 --override log.index.size.max.bytes=10485760 --override log.message.timestamp.difference.max.ms=9223372036854775807 --override log.message.timestamp.type=CreateTime --override log.preallocate=false --override log.retention.check.interval.ms=300000 --override max.connections.per.ip=2147483647 --override num.partitions=1 --override producer.purgatory.purge.interval.requests=1000 --override replica.fetch.backoff.ms=1000 --override replica.fetch.max.bytes=1048576 --override replica.fetch.response.max.bytes=10485760 --override reserved.broker.max.id=1000
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 11 Nov 2018 22:17:41 -0500
      Finished:     Sun, 11 Nov 2018 22:17:51 -0500
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 11 Nov 2018 22:17:30 -0500
      Finished:     Sun, 11 Nov 2018 22:17:40 -0500
    Ready:          False
    Restart Count:  1
    Limits:
      cpu:     300m
      memory:  678Mi
    Requests:
      cpu:      100m
      memory:   256Mi
    Readiness:  exec [sh -c /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      KAFKA_HEAP_OPTS:  -Xmx128M -Xms128M
      KAFKA_OPTS:       -Dlogging.level=INFO

It seems to me that start server failed. but not sure why it failed.
could someone tell me if anyone followed this repo successfully created kafka cluster? 
Thanks in advance.
",open,False,2018-11-12 03:27:46,2019-03-12 04:59:14
contrib,rfrink,https://github.com/kubernetes/contrib/issues/2975,https://api.github.com/repos/kubernetes/contrib/issues/2975,Kafka not aware of node relocation,"When Kubernetes relocates Kafka brokers onto new nodes, often the Kafka cluster becomes inaccessible, and the logs indicate it is still trying to use the OLD Node IPs.  Hence, a POD restart is required.  

Likewise, with consumers, often they dont get correct META_DATA updates with the new IPs of the Kafka brokers.

Have tried to update the JAVA Network TTL, but, inconsistent results.

Anyway, around this?",open,False,2018-11-29 22:32:21,2019-03-29 23:58:16
contrib,matthewsamuel95,https://github.com/kubernetes/contrib/pull/2977,https://api.github.com/repos/kubernetes/contrib/issues/2977,namespace f26 results in manifest unknown,updating namespace to f28 has allowed script to pull image ,closed,True,2018-12-04 18:18:44,2018-12-05 15:27:37
contrib,ilanni2460,https://github.com/kubernetes/contrib/issues/2978,https://api.github.com/repos/kubernetes/contrib/issues/2978,zookeeper.yaml file Format indentation error,"zookeeper.yaml file Format indentation error
this is a error Format ：
        - name : ZK_HEAP_SIZE
          valueFrom:
            configMapKeyRef:
XXXXXXXXname: zk-cm
XXXXXXXXkey: jvm.heap
        - name : ZK_TICK_TIME
          valueFrom:
            configMapKeyRef:
XXXXXXXXname: zk-cm
XXXXXXXXkey: tick
        - name : ZK_INIT_LIMIT
          valueFrom:
            configMapKeyRef:
XXXXXXXXname: zk-cm
XXXXXXXXkey: init
        - name : ZK_SYNC_LIMIT
          valueFrom:
            configMapKeyRef:
XXXXXXXXname: zk-cm
XXXXXXXXkey: tick
        - name : ZK_MAX_CLIENT_CNXNS
          valueFrom:
            configMapKeyRef:
XXXXXXXXname: zk-cm
XXXXXXXXkey: client.cnxns
        - name: ZK_SNAP_RETAIN_COUNT
          valueFrom:
            configMapKeyRef:
XXXXXXXXname: zk-cm
XXXXXXXXkey: snap.retain
        - name: ZK_PURGE_INTERVAL
          valueFrom:
            configMapKeyRef:
XXXXXXXXname: zk-cm
XXXXXXXXkey: purge.interval

theX have 8",open,False,2018-12-05 07:56:26,2019-04-04 13:08:16
contrib,fabriziofortino,https://github.com/kubernetes/contrib/pull/2979,https://api.github.com/repos/kubernetes/contrib/issues/2979,[zookeeper] 3.4.10 -> 3.4.13,Signed-off-by: Fabrizio Fortino <fabrizio.fortino@gmail.com>,closed,True,2018-12-07 12:28:08,2018-12-07 17:07:57
contrib,AlexProfi,https://github.com/kubernetes/contrib/issues/2980,https://api.github.com/repos/kubernetes/contrib/issues/2980,SSL Passthrough without termination,"Hello Is there an examp;e of all config infrastructure for ingress controller set up as daemon without external lb and deployments with pods that will be terminate ssl.
Ingress contoller must don't terminate ssl pnly pass it to pods on other worker nodes",open,False,2018-12-10 13:13:07,2019-03-10 13:20:08
contrib,arnoSCC,https://github.com/kubernetes/contrib/pull/2981,https://api.github.com/repos/kubernetes/contrib/issues/2981,Keepalived 2.0.10 + vlan handling mechanism,"Following a rework of kube-keepalived-vip for a customer, we are contributing back the modifications.

1) Upgrade to Keepalived 2.0.10 ==> According to keepalived.org, versions 2.x are the go-to versions where bugfixes and improvements happens (no backport).

2) Custom interface handling ==> Ability to use an other interface rather than the internalIP/externalIP interface.

3) Vlan handling. ==> Ability to create vlan tagged interfaces complete with subnet mask and gateway route.

This is my first contribution back to an open-source projet, i hope i wrote here everything needed, in any case, 2 additional sections exist in the README (Custom interface and Advanced configuration) explaining what happens.",closed,True,2018-12-11 10:19:23,2018-12-11 10:36:24
contrib,arnoSCC,https://github.com/kubernetes/contrib/pull/2982,https://api.github.com/repos/kubernetes/contrib/issues/2982,Keepalived 2.0.10 + vlan handling mechanism,"Following a rework of kube-keepalived-vip for a customer, we are contributing back the modifications.

    Upgrade to Keepalived 2.0.10 ==> According to keepalived.org, versions 2.x are the go-to versions where bugfixes and improvements happens (no backport).

    Custom interface handling ==> Ability to use an other interface rather than the internalIP/externalIP interface.

    Vlan handling. ==> Ability to create vlan tagged interfaces complete with subnet mask and gateway route.

This is my first contribution back to an open-source projet, i hope i wrote here everything needed, in any case, 2 additional sections exist in the README (Custom interface and Advanced configuration) explaining what happens.",closed,True,2018-12-11 10:43:44,2018-12-11 11:27:17
contrib,arnoSCC,https://github.com/kubernetes/contrib/pull/2983,https://api.github.com/repos/kubernetes/contrib/issues/2983,Keepalived 2.0.10 + vlan handling mechanism,"Following a rework of kube-keepalived-vip for a customer, we are contributing back the modifications.

    Upgrade to Keepalived 2.0.10 ==> According to keepalived.org, versions 2.x are the go-to versions where bugfixes and improvements happens (no backport).

    Custom interface handling ==> Ability to use an other interface rather than the internalIP/externalIP interface.

    Vlan handling. ==> Ability to create vlan tagged interfaces complete with subnet mask and gateway route.

This is my first contribution back to an open-source projet, i hope i wrote here everything needed, in any case, 2 additional sections exist in the README (Custom interface and Advanced configuration) explaining what happens.",closed,True,2018-12-11 11:52:55,2019-02-07 06:39:16
contrib,armanriazi,https://github.com/kubernetes/contrib/issues/2984,https://api.github.com/repos/kubernetes/contrib/issues/2984,Pending message for exposed externalApi,"I don't know why kubernets show pendding result.

sudo kubectl get svc -n ingress-nginx -v=4

When I run this command I get this result:

> no kind is registered for the type v1beta1.Table in scheme ""k8s.io/kubernetes/pkg/api/legacyscheme/scheme.go:29""



> Name:ingress-nginx
> Type:LoadBalancer
> InternalIP:10.108.240.88 ExternalIP:**pending**
> 
> PORT(s):80:30191/TCP,443:30616/TCP 21h
> 

Yaml file:
```
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
spec:
  externalTrafficPolicy: Local
  type: LoadBalancer
  loadBalancerIP: 172.18.3.11
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  - port: 443
    targetPort: 443
    protocol: TCP
    name: https

  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
```

I use docker 18.06,kubernete 1.13 to propose test on private organization with exp ip range 172.18.3.9-20

Flannel Log:
kubectl logs --namespace kube-system kube-flannel-ds-amd64-ms94w -c kube-flannel

Result:

> Failed to list *v1.Node: Get https://10.96.0.1:443/api/v1/nodes?resourceVersion=0: dial tcp 10.96.0.1:443: getsockopt: connection refused
> E1211 11:48:43.238318 1 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to list *v1.Node: Get https://10.96.0.1:443/api/v1/nodes?resourceVersion=0: net/http: TLS handshake timeout


Used kubeadm init:

> kubeadm init --pod-network-cidr 10.255.0.0/16 --service-cidr 10.244.0.0/16 --service-dns-domain ""k8s"" --apiserver-advertise-address 172.18.3.9

Dashboard kubernete shows every thing(pods,ingress,replicateSets,private docker registery container) Ok except this service for exposing cafe.example.com/cafe and externalIP!",closed,False,2018-12-11 13:01:26,2019-03-13 08:48:16
contrib,dalmarcogd,https://github.com/kubernetes/contrib/pull/2985,https://api.github.com/repos/kubernetes/contrib/issues/2985,Change the version of kafka,,open,True,2018-12-15 12:07:18,2019-03-15 12:15:49
contrib,xiuqiaoli,https://github.com/kubernetes/contrib/pull/2986,https://api.github.com/repos/kubernetes/contrib/issues/2986,Missing --kubeconfig argument name,,open,True,2018-12-17 14:02:10,2019-03-17 14:04:51
contrib,fejta,https://github.com/kubernetes/contrib/issues/2987,https://api.github.com/repos/kubernetes/contrib/issues/2987,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",open,False,2018-12-21 09:09:48,2019-03-30 05:09:33
contrib,huanwei,https://github.com/kubernetes/contrib/pull/2988,https://api.github.com/repos/kubernetes/contrib/issues/2988, fix the format of Membership Configuration in README.md,,open,True,2018-12-27 06:23:55,2019-03-28 16:27:11
contrib,zuberahmed1987,https://github.com/kubernetes/contrib/issues/2989,https://api.github.com/repos/kubernetes/contrib/issues/2989,Add IPv6 Support for keepalived-vip,"Hi Team

Please add IPv6 Support for keepalived-vip.

Thanks",open,False,2019-01-04 04:47:06,2019-04-04 05:00:12
contrib,llamahunter,https://github.com/kubernetes/contrib/pull/2990,https://api.github.com/repos/kubernetes/contrib/issues/2990,restart policy should be 'always',"The 'on-failure' restart policy means that if the kublet exits 'normally', it will not be restarted.  Problem is that systemd considers processes that are killed due to unhandled SIGPIPE as 'clean exits'.  See the 'Restart=' section of https://www.freedesktop.org/software/systemd/man/systemd.service.html, in particular:
""clean exit means an exit code of 0, or one of the signals SIGHUP, SIGINT, SIGTERM or SIGPIPE""
""If set to on-failure, the service will be restarted when the process .. is terminated by a signal (including on core dump, but *excluding the aforementioned four signals*)""
I saw this happen on my own cluster.  Kubelet got a SIGPIPE somehow, and exited, but because the systemd restart policy was 'on-failure', it didn't restart.",open,True,2019-01-08 02:19:23,2019-01-17 23:08:22
contrib,everflux,https://github.com/kubernetes/contrib/issues/2991,https://api.github.com/repos/kubernetes/contrib/issues/2991,Support ARM and ARM64 images for echoserver (multi arch),"Currently there is no ARM or ARM64 image for the echoserver on the new k8s.gcr.io registry.
It seems like the build is already prepared for docker manifests (multi arch) images, but not executed.

```
$ DOCKER_CLI_EXPERIMENTAL=enabled docker manifest inspect --verbose k8s.gcr.io/echoserver:1.10 | jq "".Descriptor.platform.architecture"" 
""amd64""
```

From the source in the repo it seems like the arm image would be located at echoserver-arm.

```
$ docker pull k8s.gcr.io/echoserver-arm                                                                                                
Using default tag: latest
Error response from daemon: manifest for k8s.gcr.io/echoserver-arm:latest not found
```

There is a single tag for an arm image on dockerhub: https://hub.docker.com/r/googlecontainer/echoserver-arm/tags but it does not seem to support ARM64.

Please adjust the build in order to submit the multi arch images for the echoserver, thanks a lot.",open,False,2019-01-11 19:58:29,2019-01-11 19:59:11
contrib,danielqsj,https://github.com/kubernetes/contrib/pull/2992,https://api.github.com/repos/kubernetes/contrib/issues/2992,Update nginx-slim link,"The link to nginx-slim image is changed.
This PR aims to update this link.",open,True,2019-01-14 09:09:06,2019-02-18 05:52:54
contrib,justinburke,https://github.com/kubernetes/contrib/pull/2993,https://api.github.com/repos/kubernetes/contrib/issues/2993,Create download directory,,open,True,2019-01-18 23:22:01,2019-02-19 22:48:08
contrib,charlie-charlie,https://github.com/kubernetes/contrib/issues/2994,https://api.github.com/repos/kubernetes/contrib/issues/2994,can I expose kafka service over ELB?,"Hi there,
I just followed the yaml file created 3 ZEROs and 3 brokers. I have 2 applications running. all of them are running in k8s. for producer/consumer, how to specify kafka endpoint?  can I expose kafka as a service? Thanks
",open,False,2019-02-05 20:09:27,2019-02-05 20:09:27
contrib,krzysied,https://github.com/kubernetes/contrib/pull/2995,https://api.github.com/repos/kubernetes/contrib/issues/2995,Fixing non-success response handling,Returning error instead of `nil` when there is non-success response.,closed,True,2019-02-07 09:55:56,2019-02-14 10:33:02
contrib,eskuai,https://github.com/kubernetes/contrib/issues/2996,https://api.github.com/repos/kubernetes/contrib/issues/2996,kafka stateful java.net.NoRouteToHostException: No route to host,"Kafka pods cannot connecto to zookeeper. They always show

java.net.NoRouteToHostException: No route to host

I 've tried with various connection url config:

        - ""exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} \
        --override zookeeper.connect=zk-0.zk-hs.default.svc.cluster.local:2181,zk-1.zk-hs.default.svc.cluster.local:2181,zk-2.zk-hs.default.svc.cluster.local:2181 \		            
		  --override zookeeper.connection.timeout.ms=60000 \

Neither use
--override zookeeper.connect=zk-c.default.svc.cluster.local:2181,
nor 
--override zookeeper.connect=zk-hs.default.svc.cluster.local:2181,

Kubenetes 1.13.3
 k8s_coredns_coredn

Attached stateful


apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: kafka
spec:
  serviceName: kafka-hs
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: kafka
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: ""app""
                    operator: In
                    values:
                    - kafka
              topologyKey: ""kubernetes.io/hostname""
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
             - weight: 1
               podAffinityTerm:
                 labelSelector:
                    matchExpressions:
                      - key: ""app""
                        operator: In
                        values:
                        - zk
                 topologyKey: ""kubernetes.io/hostname""
      terminationGracePeriodSeconds: 600
      containers:
      - name: k8skafka
        imagePullPolicy: Always
        image: gcr.io/google_containers/kubernetes-kafka:1.0-10.2.1
        resources:
          requests:
            memory: ""1Gi""
            cpu: ""0.5""
        ports:
        - containerPort: 9093
          name: server
        command:
        - sh
        - -c
        - ""exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} \
          --override listeners=PLAINTEXT://:9093 \          
          --override zookeeper.connect=zk-0.zk-hs.default.svc.cluster.local:2181,zk-1.zk-hs.default.svc.cluster.local:2181,zk-2.zk-hs.default.svc.cluster.local:2181 \		            
		  --override zookeeper.connection.timeout.ms=60000 \
          --override log.dir=/var/lib/kafka \
          --override auto.create.topics.enable=true \
          --override auto.leader.rebalance.enable=true \
          --override background.threads=10 \
          --override compression.type=producer \
          --override delete.topic.enable=false \
          --override leader.imbalance.check.interval.seconds=300 \
          --override leader.imbalance.per.broker.percentage=10 \
          --override log.flush.interval.messages=9223372036854775807 \
          --override log.flush.offset.checkpoint.interval.ms=60000 \
          --override log.flush.scheduler.interval.ms=9223372036854775807 \
          --override log.retention.bytes=-1 \
          --override log.retention.hours=168 \
          --override log.roll.hours=168 \
          --override log.roll.jitter.hours=0 \
          --override log.segment.bytes=1073741824 \
          --override log.segment.delete.delay.ms=60000 \
          --override message.max.bytes=1000012 \
          --override min.insync.replicas=1 \
          --override num.io.threads=8 \
          --override num.network.threads=3 \
          --override num.recovery.threads.per.data.dir=1 \
          --override num.replica.fetchers=1 \
          --override offset.metadata.max.bytes=4096 \
          --override offsets.commit.required.acks=-1 \
          --override offsets.commit.timeout.ms=5000 \
          --override offsets.load.buffer.size=5242880 \
          --override offsets.retention.check.interval.ms=600000 \
          --override offsets.retention.minutes=1440 \
          --override offsets.topic.compression.codec=0 \
          --override offsets.topic.num.partitions=50 \
          --override offsets.topic.replication.factor=3 \
          --override offsets.topic.segment.bytes=104857600 \
          --override queued.max.requests=500 \
          --override quota.consumer.default=9223372036854775807 \
          --override quota.producer.default=9223372036854775807 \
          --override replica.fetch.min.bytes=1 \
          --override replica.fetch.wait.max.ms=500 \
          --override replica.high.watermark.checkpoint.interval.ms=5000 \
          --override replica.lag.time.max.ms=10000 \
          --override replica.socket.receive.buffer.bytes=65536 \
          --override replica.socket.timeout.ms=30000 \
          --override request.timeout.ms=30000 \
          --override socket.receive.buffer.bytes=102400 \
          --override socket.request.max.bytes=104857600 \
          --override socket.send.buffer.bytes=102400 \
          --override unclean.leader.election.enable=true \
          --override zookeeper.session.timeout.ms=60000 \
          --override zookeeper.set.acl=false \
          --override broker.id.generation.enable=true \
          --override connections.max.idle.ms=600000 \
          --override controlled.shutdown.enable=true \
          --override controlled.shutdown.max.retries=3 \
          --override controlled.shutdown.retry.backoff.ms=5000 \
          --override controller.socket.timeout.ms=30000 \
          --override default.replication.factor=1 \
          --override fetch.purgatory.purge.interval.requests=1000 \
          --override group.max.session.timeout.ms=300000 \
          --override group.min.session.timeout.ms=60000 \
          --override inter.broker.protocol.version=0.10.2-IV0 \
          --override log.cleaner.backoff.ms=15000 \
          --override log.cleaner.dedupe.buffer.size=134217728 \
          --override log.cleaner.delete.retention.ms=86400000 \
          --override log.cleaner.enable=true \
          --override log.cleaner.io.buffer.load.factor=0.9 \
          --override log.cleaner.io.buffer.size=524288 \
          --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308 \
          --override log.cleaner.min.cleanable.ratio=0.5 \
          --override log.cleaner.min.compaction.lag.ms=0 \
          --override log.cleaner.threads=1 \
          --override log.cleanup.policy=delete \
          --override log.index.interval.bytes=4096 \
          --override log.index.size.max.bytes=10485760 \
          --override log.message.timestamp.difference.max.ms=9223372036854775807 \
          --override log.message.timestamp.type=CreateTime \
          --override log.preallocate=false \
          --override log.retention.check.interval.ms=300000 \
          --override max.connections.per.ip=2147483647 \
          --override num.partitions=1 \
          --override producer.purgatory.purge.interval.requests=1000 \
          --override replica.fetch.backoff.ms=1000 \
          --override replica.fetch.max.bytes=1048576 \
          --override replica.fetch.response.max.bytes=10485760 \
          --override reserved.broker.max.id=1000 ""
        env:
        - name: KAFKA_HEAP_OPTS
          value : ""-Xmx512M -Xms512M""
        - name: KAFKA_OPTS
          value: ""-Dlogging.level=DEBUG""
        volumeMounts:
        - name: kafka-pv-volume
          mountPath: /var/lib/kafka
        readinessProbe:
          exec:
           command:
            - sh
            - -c
            - ""/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093""
  volumeClaimTemplates:
  - metadata:
      name: kafka-pv-volume
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 10Gi
		  

Help!

",open,False,2019-02-25 12:12:58,2019-02-25 15:10:20
contrib,guanbear,https://github.com/kubernetes/contrib/issues/2997,https://api.github.com/repos/kubernetes/contrib/issues/2997,service-loadbalancer  3rd party replacements page  is 404,https://github.com/kubernetes/ingress/blob/master/docs/catalog.md is 404 now,open,False,2019-03-01 01:56:09,2019-03-01 01:57:02
contrib,elfiii,https://github.com/kubernetes/contrib/issues/2998,https://api.github.com/repos/kubernetes/contrib/issues/2998,ansible - vagrant - kubernetes error during vagrant up,"Hi,

I have just tried to deploy Kubernetes using Vagrant and Ansible. I have followed the instruction found at https://github.com/kubernetes/contrib/tree/master/ansible/vagrant and I got the following error:

    kube-node-3: Running ansible-playbook...
[DEPRECATION WARNING]: 'include' for playbook includes. You should use
'import_playbook' instead. This feature will be removed in version 2.8.
Deprecation warnings can be disabled by setting deprecation_warnings=False in
ansible.cfg.
ERROR! no action detected in task. This often indicates a misspelled module name, or incorrect module path.

The error appears to have been in '/Users/elenitrouva/Dev/contrib/ansible/roles/etcd/tasks/system_container.yml': line 6, column 3, but may
be elsewhere in the file depending on the exact syntax problem.

The offending line appears to be:
- name: Install or Update Etcd system container package
  ^ here

Ansible failed to complete successfully. Any error output should be
visible above. Please fix these errors and try again.",open,False,2019-03-05 15:09:23,2019-03-05 15:32:23
contrib,workflow,https://github.com/kubernetes/contrib/pull/2999,https://api.github.com/repos/kubernetes/contrib/issues/2999,Election README: Fix header formatting,,open,True,2019-03-07 03:46:22,2019-03-07 03:46:45
contrib,davinchia,https://github.com/kubernetes/contrib/issues/3000,https://api.github.com/repos/kubernetes/contrib/issues/3000,Release latest version of Kubernetes-Zookeeper.,"Hi, 

We are running into a bug that has already been fixed with https://github.com/kubernetes/contrib/issues/2942. We are compiling our own image in the mean time, but it'll be nice to have a latest release, since the last release was over a year ago.

Thanks!",open,False,2019-03-08 00:53:51,2019-03-08 00:53:51
contrib,micw,https://github.com/kubernetes/contrib/issues/3001,https://api.github.com/repos/kubernetes/contrib/issues/3001,Leader election lacks documentation about required permissions,"The leader election requires certain permissions to work (especially access to ""endpoints"" of the namespace where the election runs in). This should be documented.

There's already an issue where a user described the required RBAC role: https://github.com/kubernetes/contrib/issues/2952",open,False,2019-03-11 07:52:27,2019-03-11 07:52:27
contrib,micw,https://github.com/kubernetes/contrib/issues/3002,https://api.github.com/repos/kubernetes/contrib/issues/3002,Discussion: Simplify leader election code,"Hello,
before I switched to K8S, I was using Rancher 1.x as my private coud platform. They had a very simple leader detection algorith. I'd like to discuss if that would also work for K8S, so that we could maybe simplify the election code a lot (and removing the requirement to have write access to the cluster API).

The algorithm is just:
* get a list of running instances
* the instance with the lowest serial number is the leader

This works because these serial numbers are always ascending, so if the leader instance terminates, the next instance becomes the leader.

On K8S we could use the resourceVersion which also seems to be strictly ascending. That means after querying a service endpoint, we could directly decide who the leader is.

What do you think about this?",open,False,2019-03-11 07:57:22,2019-03-11 07:57:22
contrib,saqib-ahmed,https://github.com/kubernetes/contrib/issues/3003,https://api.github.com/repos/kubernetes/contrib/issues/3003,Zombie processes in the exec-healthz command,"I'm using exec-healthz container to poll google.com using `timeout -t1 wget -O/dev/null google.com` command. Everything works perfectly except the exit of the timeout leaves behind a zombie process. For details, you can watch this screencast:

https://asciinema.org/a/234430

Can anyone explain what's happening here?",open,False,2019-03-18 11:01:17,2019-03-18 11:01:52
contrib,mkumatag,https://github.com/kubernetes/contrib/pull/3004,https://api.github.com/repos/kubernetes/contrib/issues/3004,Change base image to gcr.io/distroless/static,Refer https://github.com/kubernetes/kubernetes/issues/70249 for more information.,open,True,2019-03-22 13:04:14,2019-04-05 06:57:48
contrib,krzysied,https://github.com/kubernetes/contrib/pull/3005,https://api.github.com/repos/kubernetes/contrib/issues/3005,Adding build numbers listing,"Adding listing of available build numbersfor  the given job from the Google project's GCS bucket.

ref https://github.com/kubernetes/perf-tests/issues/469",closed,True,2019-03-27 17:17:13,2019-03-28 19:49:58
contrib,xu282934741,https://github.com/kubernetes/contrib/pull/3006,https://api.github.com/repos/kubernetes/contrib/issues/3006,Update Dockerfile,"zookeeper-3.4.10.tar.gz could not be downloaded on that website。
so I changed the website and changed the version of ZK and remove the gpg commands。
Finally build docker images successfully ~",open,True,2019-03-30 06:47:23,2019-03-30 06:47:53
contrib,fejta,https://github.com/kubernetes/contrib/issues/3007,https://api.github.com/repos/kubernetes/contrib/issues/3007,Intent to archive kubernetes/contrib,"https://github.com/kubernetes/contrib/issues/2987 has gone unresolved for more than a quarter. 

During that time there has been only a single commit or so and no issues were fixed.

I intend to archive this repo and make it read only. If this is not the right course of action please create the required SECURITY_CONTACTS file (see above issue).

FYI @krzysied ",open,False,2019-04-03 23:18:43,2019-04-03 23:18:43
contrib,duncaan,https://github.com/kubernetes/contrib/pull/3008,https://api.github.com/repos/kubernetes/contrib/issues/3008,Upgrade zookeeper to 3.4.14,Also upgrade java to version 11,open,True,2019-04-05 02:41:06,2019-04-05 02:41:37
