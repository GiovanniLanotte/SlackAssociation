name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/1,https://api.github.com/repos/kubernetes/node-problem-detector/issues/1,Add initial version of node problem detector,"Here is a simple implementation of NodeProblemDetector for https://github.com/kubernetes/kubernetes/issues/23028.

The node problem detector is mainly composed of two parts:
- **ProblemDetector**. It is responsible for:
  - Collecting and consolidating events and conditions from different problem daemons;
  - Translating and aggregating them into `Event`s and `NodeCondition`s;
  - Syncing with apiserver.
- **KernelMonitor** is a simple kernel log monitor for demonstrating the Node Problem API. It is one (the only one for now) of the problem daemons. It monitors the kernel log and reports problems to ProblemDetector.
  - KernelMonitor is in charge of the ""kernel condition"" node condition.
  - If there is a permanent kernel error detected, it will report ""kernel condition"" (only `KernelDeadlock` for now) with corresponding reason and message to the problem detector.
  - If there is a temporary error detected, it will report an event to the problem detector.
  - KernelMonitor uses https://github.com/hpcloud/tail to monitor the kernel log, and uses regular expression to match problems in the kernel log. The patterns of problems are configured in the config file `config/kernel-monitor.json`. Now some known kernel bugs (https://github.com/kubernetes/kubernetes/issues/20096, https://github.com/kubernetes/kubernetes/issues/19986) have been added, and it would be easy to support other issue only if the pattern is stable and can be expressed in regular expression.
  - A lot of TODOs:
    - Support more flexible pattern match such as only matching start&end line etc.
    - Support journald.
    - ...

The node problem detector is a DaemonSet. It needs a ConfigMap `node-problem-detector-config` generated from `config/` directory. The bootstrapping is dumb now. To start the node problem detector, you need to:
1. Make sure the config mount, kernel log mount and apiserver endpoint in node-problem-detector.yaml are properly configured.
2. Make sure the log path and issue patterns `config/kernel-monitor.json` are properly configured.
3. Create ConfigMap `node-problem-detector-config` from directory `config/`.
4. Create DaemonSet `node-problem-detector.yaml`.

Now ProblemDetector and KernelMonitor are running in the same container and of course the same DaemonSet. In the future, we may want to:
- Option 1: Move the logic of ProblemDetector into kubelet, and leave problem daemons (such as KernelMonitor) as separate DaemonSets reporting directly to kubelet.
- Option 2: Move ProblemDetector into a DaemonSet, all problem daemons run as separate DaemonSets reporting to ProblemDetector.
- Option 3: Run ProblemDetector and problem daemons in separate containers, but wrap them in one DaemonSet.

This has not been decided yet, and may need more discussion. :)

@dchen1107 
/cc @kubernetes/sig-node 
",closed,True,2016-05-17 23:01:23,2016-05-19 00:31:00
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/2,https://api.github.com/repos/kubernetes/node-problem-detector/issues/2,Measure the resource overhead of NodeProblemDetector,"To roll out the NodeProblemDetector and run it as an addon pods of Kubernetes, we need to understand its overhead, at least including cpu and memory overhead.

/cc @kubernetes/sig-node 
",closed,False,2016-05-17 23:04:11,2016-05-24 00:13:55
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/3,https://api.github.com/repos/kubernetes/node-problem-detector/issues/3,KernelMonitor: Reconsider whether to use hpcloud/tail,"For move fast as soon as possible, we used https://github.com/hpcloud/tail for the first version.
However, in fact it has been proved to have some bugs, such as https://github.com/hpcloud/tail/issues/21.

We should reconsider whether we should try to fix the bugs or implement a simple tail lib ourselves based on https://github.com/google/cadvisor/pull/1264.
",closed,False,2016-05-17 23:28:34,2017-02-01 20:41:51
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/4,https://api.github.com/repos/kubernetes/node-problem-detector/issues/4,Write Readme.md for NodeProblemDetector,"Write Readme.md for NodeProblemDetector to demonstrate:
- Motivation and scope of NodeProblemDetector
- Usage of current NodeProblemDetector
- Future plan of NodeProblemDetector.

/cc @kubernetes/node-problem-detector-maintainers 
",closed,False,2016-05-17 23:53:19,2016-06-09 15:46:45
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/5,https://api.github.com/repos/kubernetes/node-problem-detector/issues/5,Write Readme.md for KernelMonitor,"Write Readme.md for KernelMonitor to demonstrate:
- Motivation of KernelMonitor
- Usage of current KernelMonitor
- Future plan of KernelMonitor.

/cc @kubernetes/node-problem-detector-maintainers 
",closed,False,2016-05-18 00:30:48,2016-06-25 00:08:11
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/6,https://api.github.com/repos/kubernetes/node-problem-detector/issues/6,Change gcr project to official google_containers,"This update the gcr project to the official one.
I've already upload an image `node-problem-detector:0.1` to google_containers, will send a PR to kubernetes to add it as an add-on pod soon.

@dchen1107 
",closed,True,2016-05-19 22:25:59,2016-05-19 22:26:57
node-problem-detector,derekwaynecarr,https://github.com/kubernetes/node-problem-detector/issues/7,https://api.github.com/repos/kubernetes/node-problem-detector/issues/7,"Unable to build code, missing godep","I cloned the repo and ran:

``` shell
$ make node-problem-detector
CGO_ENABLED=0 GOOS=linux godep go build -a -installsuffix cgo -ldflags '-w' -o node-problem-detector
vendor/gopkg.in/fsnotify.v1/inotify.go:19:2: cannot find package ""golang.org/x/sys/unix"" in any of:
    /home/decarr/go/src/k8s.io/node-problem-detector/vendor/golang.org/x/sys/unix (vendor tree)
    /usr/local/go/src/golang.org/x/sys/unix (from $GOROOT)
    /home/decarr/go/src/k8s.io/node-problem-detector/Godeps/_workspace/src/golang.org/x/sys/unix (from $GOPATH)
    /home/decarr/go/src/golang.org/x/sys/unix
godep: go exit status 1
Makefile:9: recipe for target 'node-problem-detector' failed
make: *** [node-problem-detector] Error 1
```

Looks like this is missing a godep in master:
https://github.com/kubernetes/node-problem-detector/tree/master/vendor
",closed,False,2016-05-23 17:38:24,2016-05-27 01:54:38
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/8,https://api.github.com/repos/kubernetes/node-problem-detector/issues/8,Add godep dependency golang.org/x/sys/unix,"Fix #7 

I've tried this in a brand new GOPATH.

Both ""godep restore + godep build"" and ""go build"" work.

@derekwaynecarr @eparis We don't have enough reviewer for this repo yet, are you ok with this PR? If so, I'll manually merge it. :)

/cc @dchen1107 
",closed,True,2016-05-23 18:29:17,2016-05-23 19:32:25
node-problem-detector,saad-ali,https://github.com/kubernetes/node-problem-detector/issues/9,https://api.github.com/repos/kubernetes/node-problem-detector/issues/9,Node-Problem-Detector should Patch NodeStatus not Update ,"_Problem:_
`kubelet`, `node-controller`, and now the `node-problem-detector` all update the `Node.Status` field.

Normally this is not a problem because if changes happen rapidly, resource version mismatch fails and everything is ok.

When a new field is added to Node Status, however:
- Since `kubelet` and `node-controller` are in the same repository they are recompiled with the new field and Status updates continue to operate normally.
- However, since `node-problem-detector` is in a separate repository and has not been recompiled with the new version, its `Update` calls end up squashing the new (unknown) field, resetting it to `nil`.

_Suggested Solution:_
`node-problem-detector` should do a patch instead of an update to prevent wiping out fields it is not aware of. Incidentally, `kubelet` and `node-controller` should do this as well, but that is not as critical since they are in the same repository and new types are normally added their first.

CC @kubernetes/sig-node @kubernetes/sig-api-machinery @Random-Liu @bgrant0607
",closed,False,2016-05-26 01:45:22,2016-06-02 20:59:28
node-problem-detector,freeformz,https://github.com/kubernetes/node-problem-detector/pull/10,https://api.github.com/repos/kubernetes/node-problem-detector/issues/10,"Vendor the ""right"" fsnotify","This is the version that hpcloud/tail vendors
",closed,True,2016-05-26 22:10:29,2016-05-26 23:57:11
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/11,https://api.github.com/repos/kubernetes/node-problem-detector/issues/11,Use patch instead of update status,"Fix https://github.com/kubernetes/node-problem-detector/issues/9.

Change node-problem-detector to use `Patch` instead of `UpdateStatus`.

@caesarxuchao There is no good fake for [`unversioned.Client`](https://github.com/kubernetes/kubernetes/blob/master/pkg/client/unversioned/client.go#L118), so I can't write proper test for `unversioned.Client` related operations. [`testclient.Fake`](https://github.com/kubernetes/kubernetes/blob/master/pkg/client/unversioned/testclient/testclient.go#L56) is a fake for [`unversioned.Interface`](https://github.com/kubernetes/kubernetes/blob/master/pkg/client/unversioned/client.go#L30), but there is no `Patch` in `unversioned.Interface`, so I have to use `unversioned.Client`. Is there any way to test `unversioned.Client`?

@dchen1107 @saad-ali 
",closed,True,2016-05-31 01:30:26,2016-06-01 23:26:07
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/12,https://api.github.com/repos/kubernetes/node-problem-detector/issues/12,Make condition and source configurable,"Based on #11, only the last commit is new.

This PR:
1. makes node condition and source configurable.
2. adds multiple events and conditions support in problem interface.

We make this change mainly for extensibility, testability and performance.
- Testability: The PR makes it possible to write e2e test which generates test only conditions and events without affecting real node problem detector.
- Extensibility: The PR makes it possible to extend condition types (we only support `KernelDeadlock` before) by just updating configuration.
- Performance: The PR makes it possible to batch reporting events and condition updates.

@dchen1107 
/cc @kubernetes/sig-node 
",closed,True,2016-06-01 09:25:45,2016-06-03 00:42:13
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/13,https://api.github.com/repos/kubernetes/node-problem-detector/issues/13,Hack for unsupported OS distros.,"This is a hack for unsupported OS distros.
KernelMonitor doesn't support some OS distros for now e.g. GCI. Ideally, we should only run KernelMonitor on nodes on supported OS distro. However, NodeProblemDetector is running as DaemonSet, it has to be deployed on each node:
- There is no node affinity support for DaemonSet now https://github.com/kubernetes/kubernetes/issues/22205
- There is no node label for OS Distro, so we can't use node selector, either

If some nodes have unsupported OS distro e.g. the OS distro of master node in gke/gce is GCI, KernelMonitor will throw out error, and NodeProblemDetector will be restarted again and again because it's a DaemonSet.

To avoid this, we decide to add this temporarily hack. When KernelMonitor can't find the kernel log file, it will print a log and then return nil channel and no error. Since nil channel will always be blocked, the NodeProblemDetector will block forever.

I've tried to schedule the NodeProblemDetector on GCI image, the cpu usage is `0.01% core` and memory usage is `< 5MB`.

TODOs:
1. Add journald support to support GCI. Consider reusing [the code in cadvisor](https://github.com/google/cadvisor/blob/master/utils/oomparser/oomparser.go#L168), and maybe replace hpcloud/tail with the simpler implementation in cadvisor https://github.com/kubernetes/node-problem-detector/issues/3
2. Schedule KernelMonitor only on nodes with supported OS distros. (with node label+selector or affinity)

@dchen1107 
/cc @kubernetes/sig-node 
",closed,True,2016-06-03 18:17:44,2016-06-03 18:32:34
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/14,https://api.github.com/repos/kubernetes/node-problem-detector/issues/14,KernelMonitor: Add journald support,"Add journald support.

Consider reusing [the code in cadvisor](https://github.com/google/cadvisor/blob/master/utils/oomparser/oomparser.go#L168), and maybe replace hpcloud/tail with the simpler implementation in cadvisor https://github.com/kubernetes/node-problem-detector/issues/3
",closed,False,2016-06-03 18:43:17,2017-02-01 20:41:52
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/15,https://api.github.com/repos/kubernetes/node-problem-detector/issues/15,Add entrypoint and copy kernel monitor config into docker image,"Add entrypoint and config into docker image, so as to simplify node problem detector deployment in Kubernetes.

Addon manager doesn't support ConfigMap now. We could change addon manager to support ConfigMap and also add ConfigMap in addon manifests, but that is not necessary and we should not make that big change before cutting release.
Even though we added entrypoint and config into the docker image, we can always overwrite them in the yaml file in the future.
",closed,True,2016-06-03 20:45:21,2016-06-04 00:17:33
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/16,https://api.github.com/repos/kubernetes/node-problem-detector/issues/16,Add initial README.md for node-problem-detector.,"Fix #4.

This PR updated README.md of node-problem-detector.

@dchen1107 
/cc @kubernetes/sig-node 
",closed,True,2016-06-06 00:01:15,2016-06-09 15:46:45
node-problem-detector,girishkalele,https://github.com/kubernetes/node-problem-detector/pull/17,https://api.github.com/repos/kubernetes/node-problem-detector/issues/17,Containerize the nethealth bandwidth measurement utility,"```
$ docker run -it --rm gcr.io/google_containers/kube-nethealth-amd64:1.0
2016/06/08 01:04:47 HTTP HEAD reports content length: 67108864 - running GET
2016/06/08 01:04:53 DOWNLOAD: 67108864 bytes 5956 ms Bandwidth ~ 11003 KiB/sec
2016/06/08 01:04:53 Hash Matches expected value
```
",closed,True,2016-06-08 00:59:08,2016-06-08 21:54:30
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/18,https://api.github.com/repos/kubernetes/node-problem-detector/issues/18,Remove unnecessary ENV configuration.,"Thanks for @mikedanese's suggestion.

This PR removed the manually configuration of apiserver ip and port.

Double checked with @freehan. The pod in host network should be able to use the cluster ip in the ENV value to access apiserver directly.

I've no idea what was wrong when I tried that before :(. It didn't work for me at that time...

@dchen1107 
",closed,True,2016-06-09 22:53:26,2016-06-10 00:59:50
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/19,https://api.github.com/repos/kubernetes/node-problem-detector/issues/19,Node Problem Detector should retry after failing to connect apiserver.,"Failure happens.

Node Problem Detector should not just panic when fails to connect apiserver.
",closed,False,2016-06-09 23:22:55,2017-01-24 08:39:51
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/20,https://api.github.com/repos/kubernetes/node-problem-detector/issues/20,Add README.md for kernel monitor,"Fix #5.

This is part of https://github.com/kubernetes/kubernetes.github.io/pull/702.

This PR added a README.md for kernel monitor.

@dchen1107 
",closed,True,2016-06-20 23:35:24,2016-06-25 00:08:11
node-problem-detector,sols1,https://github.com/kubernetes/node-problem-detector/issues/21,https://api.github.com/repos/kubernetes/node-problem-detector/issues/21,does this tool work outside Google?,"Panic crash:

```
kubectl get pods -o wide --all-namespaces
NAMESPACE     NAME                                READY     STATUS             RESTARTS   AGE       NODE
default       node-problem-detector-0kgkw         0/1       CrashLoopBackOff   3          1m        192.168.78.15
default       node-problem-detector-ar3tk         0/1       CrashLoopBackOff   3          1m        192.168.78.16

kubectl logs node-problem-detector-0kgkw
I0623 01:02:18.560287       1 kernel_monitor.go:86] Finish parsing log file: {WatcherConfig:{KernelLogPath:/log/kern.log} BufferSize:10 Source:kernel-monitor DefaultConditions:[{Type:KernelDeadlock Status:false Transition:0001-01-01 00:00:00 +0000 UTC Reason:KernelHasNoDeadlock Message:kernel has no deadlock}] Rules:[{Type:temporary Condition: Reason:OOMKilling Pattern:Kill process \d+ (.+) score \d+ or sacrifice child\nKilled process \d+ (.+) total-vm:\d+kB, anon-rss:\d+kB, file-rss:\d+kB} {Type:temporary Condition: Reason:TaskHung Pattern:task \S+:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:AUFSUmountHung Pattern:task umount\.aufs:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:DockerHung Pattern:task docker:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:UnregisterNetDeviceIssue Pattern:unregister_netdevice: waiting for \w+ to become free. Usage count = \d+}]}
I0623 01:02:18.560413       1 kernel_monitor.go:93] Got system boot time: 2016-06-17 17:51:02.560408109 +0000 UTC
panic: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory

goroutine 1 [running]:
panic(0x15fc280, 0xc8204fc000)
    /usr/local/go/src/runtime/panic.go:464 +0x3e6
k8s.io/node-problem-detector/pkg/problemclient.NewClientOrDie(0x0, 0x0)
    /usr/local/google/home/lantaol/workspace/src/k8s.io/node-problem-detector/pkg/problemclient/problem_client.go:56 +0x132
k8s.io/node-problem-detector/pkg/problemdetector.NewProblemDetector(0x7faa4f155140, 0xc8202a6900, 0x0, 0x0)
    /usr/local/google/home/lantaol/workspace/src/k8s.io/node-problem-detector/pkg/problemdetector/problem_detector.go:45 +0x36
main.main()
    /usr/local/google/home/lantaol/workspace/src/k8s.io/node-problem-detector/node_problem_detector.go:33 +0x56
```
",closed,False,2016-06-23 01:22:17,2017-01-10 19:53:49
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/22,https://api.github.com/repos/kubernetes/node-problem-detector/issues/22,Kernel Monitor: Add look back support and kernel panic handling,"This helps https://github.com/kubernetes/kubernetes/issues/27885.

This PR:
1) Add lookback support in kernel monitor. After started, Kernel monitor will check some old logs to detect old problems which happened before last node reboot.

2) Add `lookback` and `startPattern` in kernel monitor configuration.
- `lookback` specifies how long time kernel monitor should look back.
- `startPattern` specifies which log indicates the node is started. kernel monitor will clear all current node conditions once it finds a node start log. This makes sure that old problems won't change the node condition.

3) Add support for kernel panic monitoring, the null pointer and divide 0 kernel panic will be surfaced as event. Usually kernel monitor will report these events during looking back phase.

I've cut a branch v0.1 before this PR, and bump up the image version to v0.2 in this PR.
@dchen1107 
/cc @kubernetes/sig-node 
",closed,True,2016-06-24 06:33:11,2016-08-24 00:13:58
node-problem-detector,wangyumi,https://github.com/kubernetes/node-problem-detector/issues/23,https://api.github.com/repos/kubernetes/node-problem-detector/issues/23,node-problem-detector does not work if I change kubelet hostname to node ip.,"I am running k8s 1.3.0 and with docker image node-problem-detector:v0.1

I changed kubelet  --hostname-override with node ip on ehch minion:

```
root@SZX1000116607:~# cat /etc/default/kubelet
KUBELET_OPTS='--hostname-override=10.22.109.119 --api-servers=http://10.22.109.119:8080,http://10.22.69.237:8080,http://10.22.117.82:8080 --pod-infra-container-image=xxxxx/kubernetes/pause:latest --cluster-dns=192.168.1.1  --cluster-domain=test1 --low-diskspace-threshold-mb=2048 --cert-dir=/var/run/kubelet --allow-privileged=true'
```

when I start node-problem-detector as a daemon set, I got a lot of error messages:

```
2016-07-11T07:49:17.310422203Z I0711 07:49:17.309761       1 kernel_monitor.go:86] Finish parsing log file: {WatcherConfig:{KernelLogPath:/log/kern.log} BufferSize:10 Source:kernel-monitor DefaultConditions:[{Type:KernelDeadlock Status:false Transition:0001-01-01 00:00:00 +0000 UTC Reason:KernelHasNoDeadlock Message:kernel has no deadlock}] Rules:[{Type:temporary Condition: Reason:OOMKilling Pattern:Kill process \d+ (.+) score \d+ or sacrifice child\nKilled process \d+ (.+) total-vm:\d+kB, anon-rss:\d+kB, file-rss:\d+kB} {Type:temporary Condition: Reason:TaskHung Pattern:task \S+:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:AUFSUmountHung Pattern:task umount\.aufs:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:DockerHung Pattern:task docker:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:UnregisterNetDeviceIssue Pattern:unregister_netdevice: waiting for \w+ to become free. Usage count = \d+}]}
2016-07-11T07:49:17.310510490Z I0711 07:49:17.310026       1 kernel_monitor.go:93] Got system boot time: 2016-07-08 01:40:54.310019188 +0000 UTC
2016-07-11T07:49:17.311878174Z I0711 07:49:17.311436       1 kernel_monitor.go:102] Start kernel monitor
2016-07-11T07:49:17.311907663Z I0711 07:49:17.311654       1 kernel_log_watcher.go:173] unable to parse line: """", can't find timestamp prefix ""kernel: ["" in line """"
2016-07-11T07:49:17.311926515Z I0711 07:49:17.311696       1 kernel_log_watcher.go:110] Start watching kernel log
2016-07-11T07:49:17.311942118Z I0711 07:49:17.311720       1 problem_detector.go:60] Problem detector started
2016-07-11T07:49:17.314020808Z 2016/07/11 07:49:17 Seeked /log/kern.log - &{Offset:0 Whence:0}
2016-07-11T07:49:18.355974460Z E0711 07:49:18.355712       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:19.314395255Z E0711 07:49:19.314110       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:20.314302138Z E0711 07:49:20.313982       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:21.315151897Z E0711 07:49:21.314849       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:22.313977681Z E0711 07:49:22.313834       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:23.313906286Z E0711 07:49:23.313619       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:24.314428608Z E0711 07:49:24.314141       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:25.316626549Z E0711 07:49:25.316326       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:26.314346471Z E0711 07:49:26.314142       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:27.313901894Z E0711 07:49:27.313759       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:28.314356686Z E0711 07:49:28.314198       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:29.314747334Z E0711 07:49:29.314450       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
2016-07-11T07:49:30.314562756Z E0711 07:49:30.314235       1 manager.go:130] failed to update node conditions: nodes ""SZX1000116591"" not found
```

It seems node-problem-detector still use the  original os hostname to access node instead of overrided hostname that is registered in etcd. 

/wangyumi
",closed,False,2016-07-11 07:59:29,2017-11-02 10:26:54
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/24,https://api.github.com/repos/kubernetes/node-problem-detector/issues/24,Add --insecure-connection and --hostname-override flag in node problem detector,"Fix https://github.com/kubernetes/node-problem-detector/issues/23.
Fix https://github.com/kubernetes/node-problem-detector/issues/21.

This PR added 2 node problem detector flags:
- `--insecure-connection`: This flag will let node problem detector skip TLS verification when talking with apiserver.
- `--hostname-override`: The user could override the host name with this flag. Notice that if you want to override the hostname, you may have to run node problem detector as static pod on each node, and render the flag accordingly with your deployment tool (or manually) to make sure that every node problem detector is properly configured.

@sols1 @ApsOps can any of you help me verify the `--insecure-connection`? I don't have a cluster with no admission control in hand. The image version is v0.2.

@sols1 @ApsOps @wangyumi
/cc @dchen1107 
",closed,True,2016-07-11 21:21:56,2016-08-05 17:42:48
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/25,https://api.github.com/repos/kubernetes/node-problem-detector/issues/25,Get node name from downward api,"Now to get the node name, we have to let node problem detector run in host network.

After https://github.com/kubernetes/kubernetes/pull/27880 landing, we can change the node problem detector to get node name from the downward api. And that will also fix https://github.com/kubernetes/node-problem-detector/issues/23.
",closed,False,2016-07-13 23:50:18,2016-09-08 18:15:12
node-problem-detector,pwittrock,https://github.com/kubernetes/node-problem-detector/issues/26,https://api.github.com/repos/kubernetes/node-problem-detector/issues/26,Detect clock skew vs the master or some other central source,"The master depends on itself having a similar view of time as the Nodes, and the controller manager will not properly roll out deployments if the node and master clocks are skewed.  We should figure out a way of detecting this condition.
",closed,False,2016-07-19 21:24:52,2018-02-14 17:17:10
node-problem-detector,yuvipanda,https://github.com/kubernetes/node-problem-detector/issues/27,https://api.github.com/repos/kubernetes/node-problem-detector/issues/27,Allow reading a kubeconfig file for master / auth info,"This would allow it to be used with clusters that don't have service accounts enabled (they are disabled in our cluster because they do not play well with namespaced accounts yet)
",closed,False,2016-07-23 00:47:34,2017-01-10 07:39:40
node-problem-detector,zmerlynn,https://github.com/kubernetes/node-problem-detector/issues/28,https://api.github.com/repos/kubernetes/node-problem-detector/issues/28,NodeProblemDetector tests failing on AWS,"https://github.com/kubernetes/node-problem-detector/blob/9ab546a2e23613c51e51af7a109952a6079e0651/pkg/problemclient/problem_client.go#L61 assumes that the hostname is the same name as the cloud provider instance name. This is false on AWS. NPD needs to be going through `CurrentNodeName` to get the cloud provider agnostic hostname.

I'm disabling this test on AWS for now, but we're marching towards full support for AWS. Please treat this as if it had broken a build when it was checked in originally.
",closed,False,2016-08-09 21:33:23,2016-11-10 19:57:08
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/29,https://api.github.com/repos/kubernetes/node-problem-detector/issues/29,NPD: Get node name from pod,"Fixes https://github.com/kubernetes/node-problem-detector/issues/28.
Suggested in https://github.com/kubernetes/kubernetes/pull/27880#issuecomment-238776172.

This PR makes node problem detector to get node name from the pod where it is on, so that the node name should always be consistent with kubelet.
",closed,True,2016-08-11 19:11:07,2016-08-11 21:23:46
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/30,https://api.github.com/repos/kubernetes/node-problem-detector/issues/30,NPD: Get node name from the downward api.,"Fixes https://github.com/kubernetes/node-problem-detector/issues/25.
Fixes https://github.com/kubernetes/node-problem-detector/issues/23.

https://github.com/kubernetes/kubernetes/pull/27880 has been merged. This PR changes node problem detector to get node name from the downward api.

@dchen1107 
",closed,True,2016-08-21 02:05:30,2016-12-13 10:55:40
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/31,https://api.github.com/repos/kubernetes/node-problem-detector/issues/31,NPD: Enable travis test,"For lack of continuous integration test, there are some errors introduced in the code base.
This PR:
- fixed a failing unit test
- fix a missing godep
- add travis ci
",closed,True,2016-09-10 03:10:47,2016-09-12 21:03:30
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/32,https://api.github.com/repos/kubernetes/node-problem-detector/issues/32,NPD: Remove get start point,"Based on https://github.com/kubernetes/node-problem-detector/pull/31 to enable the presubmit test.

This PR addressed https://github.com/kubernetes/node-problem-detector/issues/3#issuecomment-246431485 to remove the unnecessary `getStartPoint` function.

With this change @AdoHe should be able to use simple tail library directly.
",closed,True,2016-09-12 18:57:21,2016-09-12 21:08:41
node-problem-detector,adohe,https://github.com/kubernetes/node-problem-detector/pull/33,https://api.github.com/repos/kubernetes/node-problem-detector/issues/33,add journal support,"needs to fix dependency and add test.
",closed,True,2016-09-16 07:48:25,2016-11-03 05:27:43
node-problem-detector,johscheuer,https://github.com/kubernetes/node-problem-detector/pull/34,https://api.github.com/repos/kubernetes/node-problem-detector/issues/34,Fix typo in Readme,"Fix a small typo in Readme.
",closed,True,2016-09-29 15:33:13,2016-09-29 19:04:41
node-problem-detector,jfilak,https://github.com/kubernetes/node-problem-detector/issues/35,https://api.github.com/repos/kubernetes/node-problem-detector/issues/35,How to hook up third-party daemons?,"I'm an [ABRT](http://abrt.readthedocs.io/en/latest/) developer and I would love to create a problem daemon reporting problems detected by ABRT to node-problem-detector.

ABRT's architecture is similar to node-problem-detector's - there are agents reporting detected problems to abrtd. An ABRT agent is either a tiny daemon watching logs (or systemd-journal) or a language error handler (Python sys.excepthook, Ruby at_exit callback, /proc/sys/kernel/core_pattern, Node.js uncaughtException event handler, Java JNI agent).

I've created a docker image that is capable to detect Kernel oopses, vmcores and core files on a host:
https://github.com/jfilak/docker-abrt/tree/atomic_minimal

(It should be possible to detect uncaught [Python, Ruby, Java] exceptions in the future)

ABRT provides several ways of reporting the detected problems to users - e-mail, FTP|SCP upload, D-Bus signal, Bugzilla bug, [micro-Report](https://github.com/abrt/faf/wiki/uReport), systemd-journal catalog message - and it is trivial to add another report destination.

The Design Doc defines ""Problem Report Interface"" but I've failed to find out how to register a new problem deamon to node-problem-detector or how to use the ""Problem Report Interface"" from a third party daemon.
",closed,False,2016-10-24 10:58:03,2018-03-10 22:46:34
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/36,https://api.github.com/repos/kubernetes/node-problem-detector/issues/36,Share time with the host,"Kernel monitor discard old kernel log based on the timestamp.

However, usually kernel log timestamp doesn't contain year and timezone information, e.g. `Oct 23 16:59:23`. There is no way to apply the right timezone, if the timezone inside the pod is different from the host.

This PR mount the `/etc/localtime` inside the container to make sure the container in the same zone with the host.

Sometimes `/etc/localtime` may be a symlink file. I've tested that both docker and kubernetes mount will follow the symlink and mount the real content.

@dchen1107

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/36)
<!-- Reviewable:end -->
",closed,True,2016-10-27 23:22:25,2017-02-01 20:45:50
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/37,https://api.github.com/repos/kubernetes/node-problem-detector/issues/37,Node problem detector should use apiserver cache.,"Currently, NPD sent a PATCH every 30 seconds, which could be a significant overhead for apiserver in large cluster.

Even in periodical resync, we should `GET` first and only send `PATCH` when the conditions are actually different.

Further more, we could use the same trick with kubelet to leverage the apiserver cache to reduce the load even more. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet_node_status.go#L313-L322",closed,False,2016-11-05 01:07:34,2018-02-16 14:00:01
node-problem-detector,roboll,https://github.com/kubernetes/node-problem-detector/pull/38,https://api.github.com/repos/kubernetes/node-problem-detector/issues/38,modify checks to enable journald support,"No longer defaults kernel log file when it is empty.

Currently, the journald support will never trigger because an empty path is always defaulted [here](https://github.com/kubernetes/node-problem-detector/blob/master/pkg/kernelmonitor/kernel_log_watcher.go#L84), but journald support checks for empty path [here](https://github.com/kubernetes/node-problem-detector/blob/master/pkg/kernelmonitor/kernel_log_watcher.go#L171).

The default config ships with a path in it, so this change shouldn't disrupt existing deployments. When the path is empty, will try journal. When the path is non-empty, existing behavior is intact.

",closed,True,2016-11-10 15:11:52,2016-11-10 20:00:39
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/39,https://api.github.com/repos/kubernetes/node-problem-detector/issues/39,Journald support,"Fixes #3.
Fixes #14.
This PR depends on https://github.com/google/cadvisor/pull/1531. I'll remove the 3rd commit and update Godeps after the cadvisor patch is merged.

This PR finished the journald support.

Building `sdjournal` needs `libsystemd-dev` or `libsystemd-journal-dev` which may not be easy to get on some os distros. So I added a knob to disable it:
```
make # wil build journald support by default
ENABLE_JOURNALD=0 make # will not build journald support, thus no libsystemd needed
```

@AdoHe @dchen1107 
/cc @kubernetes/sig-node

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/39)
<!-- Reviewable:end -->
",closed,True,2016-11-10 19:41:02,2017-02-01 21:00:39
node-problem-detector,apatil,https://github.com/kubernetes/node-problem-detector/pull/40,https://api.github.com/repos/kubernetes/node-problem-detector/issues/40,Support getting node name from k8s API,"Hi @Random-Liu, I noticed that you were grabbing the node name from the downward API, which isn't supported in k8s 1.3.7 and below. This patch adds a fallback to using the k8s API at runtime. What do you think?

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/40)
<!-- Reviewable:end -->
",closed,True,2016-11-10 20:31:48,2018-02-17 09:19:00
node-problem-detector,euank,https://github.com/kubernetes/node-problem-detector/pull/41,https://api.github.com/repos/kubernetes/node-problem-detector/issues/41,logwatchers: add new kmsg-based kernel log watcher,"All the other loggers (afaik), like rsyslog and journald and what have you, simply read `/dev/kmsg`.

It's probably easier for everyone if we just read that directly too!

I put together a bit of code doing so [over here](https://github.com/euank/go-kmsg-parser/blob/v1.0.0/kmsgparser/kmsgparser.go), and this commit leverages that code for the node-problem-detector.

I plan to replace cadvisor's similar hacky log parsing with the same code as well.

So, what are the implications?
Well, one thing this can't do as well is read far into the past. This can't read further back than the ring buffer extends, so on startup it's quite possible it won't get as many old messages. That means really large lookback values will behave differently now because they'll read less.

That could have already happened depending on logrotation policy, but now it's a lot harder for the user to control (kernel dmesg size is much harder to change :).

However, I think the significant simplicity and cross-distro gains by doing this are worth it.

~~I also preemptively removed the old integrations as I think this makes them fully redundant, as well as the translators thingy since why would you want to translate when you're reading the one and only true source?~~

Edit: I added this as an additional plugin, but didn't change any defaults or such.

Fixes #14 #39 

cc @Random-Liu @AdoHe @derekwaynecarr

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/41)
<!-- Reviewable:end -->
",closed,True,2016-11-14 06:55:50,2017-05-30 23:56:29
node-problem-detector,euank,https://github.com/kubernetes/node-problem-detector/pull/42,https://api.github.com/repos/kubernetes/node-problem-detector/issues/42,makefile: Make misc improvements,"* Add test target and re-use in travis
* Add timeout test target
* Build binary into ./bin folder
* Phony everything

I thought I had made a PR with this, but nope, it was just sitting around in my branch for the last week.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/42)
<!-- Reviewable:end -->
",closed,True,2016-11-21 23:46:21,2016-12-22 09:06:08
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/43,https://api.github.com/repos/kubernetes/node-problem-detector/issues/43,Add CI e2e test for NPD.,"We should add presubmit/postsubmit e2e test for NPD.

For now, we can just download newest kubernetes version and run the [node problem detector e2e test](https://github.com/kubernetes/kubernetes/blob/master/test/e2e/node_problem_detector.go).",closed,False,2016-11-22 00:22:36,2018-02-17 00:10:04
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/44,https://api.github.com/repos/kubernetes/node-problem-detector/issues/44,Generalize kernel monitor.,"Discussed with @apatil.

Kernel monitor was initially introduced to monitor kernel log and detect kernel issues.

However, in fact it could be extended to monitor other logs such as docker log, systemd log etc. by adding new [`translator`](https://github.com/kubernetes/node-problem-detector/blob/master/pkg/kernelmonitor/translator/translator.go). Currently it is already doable, but not very intuitive because:
1. All files, types and functions are named as `kernel xxx`.
2. Translator is not configurable.

We should refactor the code to make it easier and more intuitive to extend kernel monitor:
* Change `kernelmonitor` to `logmonitor`. We'll only use log monitor to monitor kernel log for K8s, but it should be easy for other users to reconfigure and extend it to monitor other logs.
* Extend the configuration to make translator and log source configurable after https://github.com/kubernetes/node-problem-detector/pull/41 landed, including:
  * Make the journald log filter configurable.
  * Make the translate function configurable.

/cc @kubernetes/sig-node ",closed,False,2016-11-22 00:43:35,2017-02-27 22:49:46
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/45,https://api.github.com/repos/kubernetes/node-problem-detector/issues/45,We need a ChangeLog for NPD.,We should add a changelog to make the release more clear and official.,closed,False,2016-11-30 00:14:44,2017-01-12 07:38:28
node-problem-detector,sandflee,https://github.com/kubernetes/node-problem-detector/issues/46,https://api.github.com/repos/kubernetes/node-problem-detector/issues/46,failed to build node-problem-detector,"- go version go1.7 darwin/amd64
- godep v74 (darwin/amd64/go1.7)

make failed with output:
```
CGO_ENABLED=0 GOOS=linux godep go build -a -installsuffix cgo -ldflags '-w' -o node-problem-detector
vendor/github.com/coreos/go-systemd/sdjournal/functions.go:19:2: no buildable Go source files in /Users/sandflee/goproject/src/k8s.io/node-problem-detector/vendor/github.com/coreos/pkg/dlopen
godep: go exit status 1
```",closed,False,2016-11-30 14:55:52,2016-12-01 18:48:58
node-problem-detector,euank,https://github.com/kubernetes/node-problem-detector/issues/47,https://api.github.com/repos/kubernetes/node-problem-detector/issues/47,"""unregister_netdevice"" isn't necessarily a KernelDeadlock","I have a node running CoreOS 1221.0.0 with kernel version 4.8.6-coreos.

The node-problem-detector marked it with ""KernelDeadlock 	True 	Sun, 04 Dec 2016 18:56:20 -0800 	Wed, 16 Nov 2016 00:03:33 -0800 	UnregisterNetDeviceIssue 	unregister_netdevice: waiting for lo to become free. Usage count = 1"".

If I check my kernel log, I see the following:

```
$ dmesg -T | grep -i unregister_netdevice -C 3
[Wed Nov 16 08:02:19 2016] docker0: port 5(vethfd2807b) entered blocking state
[Wed Nov 16 08:02:19 2016] docker0: port 5(vethfd2807b) entered forwarding state
[Wed Nov 16 08:02:19 2016] IPv6: eth0: IPv6 duplicate address fe80::42:aff:fe02:1206 detected!
[Wed Nov 16 08:03:33 2016] unregister_netdevice: waiting for lo to become free. Usage count = 1
[Wed Nov 16 08:14:35 2016] vethafecb94: renamed from eth0
[Wed Nov 16 08:14:35 2016] docker0: port 2(veth807b9e2) entered disabled state
[Wed Nov 16 08:14:35 2016] docker0: port 2(veth807b9e2) entered disabled state
```

Clearly, the node managed to continue to perform operations after printing that message. In addition, pods continue to function just fine and there aren't any long-term issues for me on this node.

I know that the config of what counts as a deadlock is configurable, but perhaps the default configuration shouldn't include this, or the check should be more advanced for it, since as-is it could be quite confusing.",closed,False,2016-12-05 03:01:52,2017-02-10 19:17:17
node-problem-detector,euank,https://github.com/kubernetes/node-problem-detector/issues/48,https://api.github.com/repos/kubernetes/node-problem-detector/issues/48,"""startPattern"" is fragile and wrong on newer kernels","Broken out from [here](https://github.com/kubernetes/node-problem-detector/pull/39#issuecomment-265877841)

Currently, the config has a default of `""startPattern"": ""Initializing cgroup subsys cpuset"",`

This pattern is meant to detect a node's boot process. Prior to the 4.5 kernel, this message was typically printed during boot of a node. After 4.5 however, due to [this change](https://github.com/torvalds/linux/commit/a5ae989957cbc3f3b7bc40677bd9e459c0917528), it is quite unlikely for that message to appear.


Furthermore, there's rarely a reason to detect whether a message is for the current boot in such a fragile way.

With the `kern.log` reader, every message is for the current boot because kern.log is usually handled where each `kern.log` file corresponds to one boot (e.g. `kern.log` is this boot, `kern.log.1` is the boot before, `kern.log.2.gz` the one before, etc). (EDIT: I'm wrong about this for gci at least)

With journald, the boot id is annotated in messages, and so it can accurately be correlated with the current boot id (see the ""_BOOT_ID"" record in journald messages).

With a kmsg reader, all messages will only be the current boot because kmsg is not persistent.

In none of those cases is `startPattern` useful. Each kernel log parsing plugin should be responsible for doing the right thing itself I think.",closed,False,2016-12-08 23:17:08,2017-02-10 19:17:17
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/49,https://api.github.com/repos/kubernetes/node-problem-detector/issues/49,add support for running standalone,"Add support for running `node-problem-detector` standalone. 

This PR adds two command line options
* `in-cluster-config` configures whether we deploy `NPD` in cluster as a daemon set or on a bare metal standalone. Default to be `true`, i.e., running in cluster as a daemon set.
* `apiserver-addr` is used to specify the apiserver address when `in-cluster-config` is set to `false`.

Finally, we can run `NPD` in standalone like this
```
node-problem-detector -in-cluster-config=false -apiserver-addr=APISERVER_ADDRESS -kernel-monitor=KERNEL_MONITOR_CONFIG_FILE
```


<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/49)
<!-- Reviewable:end -->
",closed,True,2016-12-14 14:51:44,2017-01-11 01:20:09
node-problem-detector,shyamjvs,https://github.com/kubernetes/node-problem-detector/issues/50,https://api.github.com/repos/kubernetes/node-problem-detector/issues/50,"Feature request for a ""hollow""-node-problem-detector having an empty list of conditions and rules inside kernel monitor config","As part of the effort to make testing using kubemark mimic real clusters as closely as possible, we are planning to add container for a ""hollow-node-problem-detector"" inside hollow-node alongside the existing containers ""hollow-kubelet"" and ""hollow-kubeproxy"". For this, it is required to have a node-problem-detector image which essentially has conditions and rules inside the kernel monitoring config set to an empty list. Also, this image should eventually (once it is tested to work fine in Kubemark) be pushed to gcr.io/google-containers.

@kubernetes/sig-scalability @wojtek-t @gmarek @Random-Liu ",closed,False,2016-12-15 14:37:43,2017-01-19 13:01:05
node-problem-detector,shyamjvs,https://github.com/kubernetes/node-problem-detector/pull/51,https://api.github.com/repos/kubernetes/node-problem-detector/issues/51,"Added functionality for ""hollow""-node-problem-detector","This PR fixes issue #50 
Made the following changes in order to incorporate creation and pushing of a hollow-node-problem-detector image to gcr:
1. Created a hollow KernelMonitor config with empty rule and condition lists
2. Converted Dockerfile to a template which can take path to kernelMonitor config as input
3. Added new targets to Makefile for hollow-node-problem-detector

cc @kubernetes/sig-scalability @wojtek-t @gmarek @Random-Liu

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/51)
<!-- Reviewable:end -->
",closed,True,2016-12-15 18:27:19,2016-12-16 13:02:02
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/52,https://api.github.com/repos/kubernetes/node-problem-detector/issues/52,bump kubernetes version to v1.4.0-beta.3,"bump kubernetes version to v1.4.0-beta.3. This is partially a former PR for #49 .

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/52)
<!-- Reviewable:end -->
",closed,True,2016-12-18 10:11:36,2016-12-20 19:14:58
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/53,https://api.github.com/repos/kubernetes/node-problem-detector/issues/53,add fmt and vet make target,"Add `fmt` and `vet` make target. And, add `fmt` and `vet` dependencies for `node-problem-detector` target.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/53)
<!-- Reviewable:end -->
",closed,True,2016-12-21 05:57:22,2016-12-22 09:05:48
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/54,https://api.github.com/repos/kubernetes/node-problem-detector/issues/54,change travis config script,"Currently, `make node-problem-detector` error with the following message: 

```
CGO_ENABLED=0 GOOS=linux godep go build -a -installsuffix cgo -ldflags '-w' -o node-problem-detector
vendor/github.com/coreos/go-systemd/sdjournal/functions.go:19:2: no buildable Go source files in /home/xiening/go/src/k8s.io/node-problem-detector/vendor/github.com/coreos/pkg/dlopen
godep: go exit status 1
Makefile:15: recipe for target 'bin/node-problem-detector' failed
make: *** [bin/node-problem-detector] Error 1
```

`dlopen` uses cgo to compile. So, we should turn on `CGO_ENABLED`. 

In order to make node-problem-detector statically linked. We can use `-extldflags ""-static"" `

* remove `./bin/node-problem-detector` target. IMO, it will not work as expected. It will not generate node-problem-detector binary under `bin` directory.
* change `go build -race` script to `make node-problem-detector`
* add `dep` target, it seems travis ci does not add godep binary automatically. So, we will check it and if it does not exist, we will manually download it.
* Enable cgo, because we dlopen uses it to compile. 
* In order to compile a statically linked binary. we may should use `-extldflags ""-static""`.


<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/54)
<!-- Reviewable:end -->
",closed,True,2016-12-21 06:02:28,2016-12-22 09:05:38
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/55,https://api.github.com/repos/kubernetes/node-problem-detector/issues/55,Remove dead code in kernel monitor,"Check for `linux` os with `runtime.GOOS` instead of `syscall.Sysinfo_t` which is not available on MacOS.

This will is possible to develop and `make node-problem-detector` on MacOS when #39 is merged with supporting disable `systemd` with an environment variable.

All available `runtime.GOOS` values can be found in [this page](https://golang.org/doc/install/source#environment).

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/55)
<!-- Reviewable:end -->
",closed,True,2016-12-21 11:54:16,2016-12-22 09:05:21
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/56,https://api.github.com/repos/kubernetes/node-problem-detector/issues/56,Fix the dockerfile to use the right binary.,"@andyxning Could you help me review this?

I found we missed the Dockerfile in https://github.com/kubernetes/node-problem-detector/pull/54.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/56)
<!-- Reviewable:end -->
",closed,True,2016-12-21 19:59:14,2016-12-22 03:01:51
node-problem-detector,kewubenduben,https://github.com/kubernetes/node-problem-detector/pull/57,https://api.github.com/repos/kubernetes/node-problem-detector/issues/57,Update README.md,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/57)
<!-- Reviewable:end -->
",closed,True,2016-12-27 09:57:36,2017-01-05 19:22:23
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/58,https://api.github.com/repos/kubernetes/node-problem-detector/issues/58,NPD Kubernetes 1.6 Planning,"NPD (node problem detector) is introduced in Kubernetes 1.3 as a default add-on in GCE cluster.

At that time, it is mainly targeted on default GCE Kubernetes setup. However, as time goes by, some limitations were found such as [Journald support](#14), [Authentication Issue](#21), [Scalability Issue](#37)  which affected the adoption of NPD in many other environment.

In Kubernetes 1.6, we plan to invest some time to improve NPD, make it production ready and rollout it in GKE.

Here are the working items and priorities:
- [x] **[P0] Journald support.** Many important OS distros are using systemd now, such as GCI, CoreOS, CentOS etc. This is essential for NPD adoption. (Issue:  https://github.com/kubernetes/node-problem-detector/issues/14, PR: https://github.com/kubernetes/node-problem-detector/pull/39, https://github.com/kubernetes/node-problem-detector/pull/33, @AdoHe) 
- [x] **[P0] Apiserver client option override.** By default, NPD is running as DaemonSet and [use `InClusterConfig` to access apiserver](http://kubernetes.io/docs/user-guide/accessing-the-cluster/#accessing-the-api-from-a-pod). However, this does not work when service account is not available. (Issue: https://github.com/kubernetes/node-problem-detector/issues/27, https://github.com/kubernetes/node-problem-detector/issues/21). We should make the apiserver client option configurable, so that user can customize it based on their cluster setup. **This is prerequisite of Standalone mode** (PR: https://github.com/kubernetes/node-problem-detector/pull/49, @andyxning)
- [x] **[P0] Standalone mode.** Make it possible to run NPD standalone, possibly as a systemd service. DaemonSet is easy to deploy and manage. However, docker still stops all containers when it's dead ([`live-restore`](https://docs.docker.com/engine/admin/live-restore/) is still [in validation](https://github.com/kubernetes/kubernetes/issues/38303)). Because of this, NPD may not be able to detect problems when docker is unresponsive. (Issue: #76)
- [ ] **[P1] Integrate NPD with K8s e2e framework.** NPD is already running in e2e cluster, but the information it collects is not well-surfaced from the test framework. We should make it visible by failing the test or collecting via a dashboard (Issue: https://github.com/kubernetes/kubernetes/issues/30811).
- [x] **[P1] Scalability and performance.** https://github.com/kubernetes/node-problem-detector/issues/85
  - [x] Some known performance issue needs to be fixed in NPD, such as reduce apiserver access (https://github.com/kubernetes/node-problem-detector/issues/37), and improve log parsing efficiency. (https://github.com/kubernetes/node-problem-detector/pull/79, #84)
  - [x] More benchmark to verify the performance of NPD. Both benchmark for NPD resource usage and apiserver load introduced by NPD (https://github.com/kubernetes/node-problem-detector/issues/50, @shyamjvs, https://github.com/kubernetes/node-problem-detector/issues/85).
- [ ] **[P2] Formalize the Project.** Formalize the process of the project, including:
  - [x] https://github.com/kubernetes/node-problem-detector/pull/66 Add change log. (#45) [P2]
  * Define release process. (#67) [P2]
  * Add pre/post submit e2e test. (#43) [P3]
- [x] **[P2] Docker problem detection.** Although kernel monitor could be extended to monitor other logs, it still needs some code change to achieve that. We should cleanup the code to make it easier to monitor other logs and add clear documentation for it. (https://github.com/kubernetes/node-problem-detector/issues/44) (#88) (PR: https://github.com/kubernetes/node-problem-detector/pull/88, https://github.com/kubernetes/node-problem-detector/pull/92, https://github.com/kubernetes/node-problem-detector/pull/94)
- [ ] **[P3] 3rd party problem daemon integration.** Kernel monitor is designed to detect known kernel problems with minimum overhead, it is not expected to be a comprehensive solution. NPD should be extensible to integrate with more small problem daemons or more mature solution. (https://github.com/kubernetes/node-problem-detector/issues/35)

Note that only P0s are release blocker.

@dchen1107 @fabioy @ajitak 
/cc @kubernetes/sig-node-misc ",closed,False,2017-01-07 00:56:33,2017-08-03 23:16:25
node-problem-detector,yunjing,https://github.com/kubernetes/node-problem-detector/issues/59,https://api.github.com/repos/kubernetes/node-problem-detector/issues/59,Support SSL certificates,Our k8s API server is running with a self-signed certificate to enable SSL. I am wondering what's the best way to specify a root CA when running node-problem-detector.,closed,False,2017-01-09 23:56:11,2018-02-18 16:50:01
node-problem-detector,shyamjvs,https://github.com/kubernetes/node-problem-detector/issues/60,https://api.github.com/repos/kubernetes/node-problem-detector/issues/60,"Bump the version to v0.3 in makefile, manifest and readme","After adding the standalone mode in #49 , I created and pushed a new docker image gcr.io/google_containers/node-problem-detector:v0.3 which includes these changes for use by [kubemark](https://github.com/kubernetes/kubernetes/tree/master/test/kubemark). However this version bump has to be reflected in:
1. version tag inside the Makefile
2. image inside the node-problem-detector.yaml
3. readme, to indicate that v0.3 is the latest version and/or suggested for use?

@Random-Liu Hope this push doesn't disturb any plans you might be having for npd release schedule. If yes, it's not difficult to revert this change, as I just did it for use in kubemark.
",closed,False,2017-01-10 16:59:05,2018-02-23 22:54:02
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/61,https://api.github.com/repos/kubernetes/node-problem-detector/issues/61,Add RBAC in the example yaml file and document,"Default Kubernetes setup removed ABAC support, and is only using RBAC now.
https://github.com/kubernetes/kubernetes/pull/39092

This is fine for the NPD in kube-system, which has enough permission. However, the NPD setup in the example and Readme.md may not be enough.

We should add documentation and example to demonstrate how to deploy NPD in a RBAC K8s cluster.",closed,False,2017-01-10 19:38:58,2018-02-19 06:02:59
node-problem-detector,shyamjvs,https://github.com/kubernetes/node-problem-detector/issues/62,https://api.github.com/repos/kubernetes/node-problem-detector/issues/62,Kernel monitor file is wrongly named in node-problem-detector.go,"The default path to the kernel monitor config in [node-problem-detector.go](https://github.com/kubernetes/node-problem-detector/blob/master/node_problem_detector.go#L30) is '/config/kernel_monitor.json'. However, the file in the config/ directory of the repo is named 'kernel-monitor.json' (hyphen at one place and underscore at the other). They need to be named the same.

cc @Random-Liu ",closed,False,2017-01-10 19:56:32,2017-01-10 23:18:34
node-problem-detector,shyamjvs,https://github.com/kubernetes/node-problem-detector/pull/63,https://api.github.com/repos/kubernetes/node-problem-detector/issues/63,Fixing minor bug in kernel monitor filepath,"Fixes #62 (kernel monitor filepath change)
Ref #60 (bumping version to v0.3)

@Random-Liu Can you PTAL?

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/63)
<!-- Reviewable:end -->
",closed,True,2017-01-10 20:08:03,2017-01-11 21:25:31
node-problem-detector,ApsOps,https://github.com/kubernetes/node-problem-detector/pull/64,https://api.github.com/repos/kubernetes/node-problem-detector/issues/64,"Fix ""Flags usage"" in README","cc: @Random-Liu

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/64)
<!-- Reviewable:end -->
",closed,True,2017-01-10 20:08:24,2017-01-11 22:35:56
node-problem-detector,ApsOps,https://github.com/kubernetes/node-problem-detector/issues/65,https://api.github.com/repos/kubernetes/node-problem-detector/issues/65,"Failed to update node conditions, but error <nil>","I'm seeing logs like these:
```
E0110 22:51:21.706489 1 manager.go:130] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-0-12-121.ec2.internal"": <nil>
```",closed,False,2017-01-11 04:58:31,2018-02-18 09:43:02
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/66,https://api.github.com/repos/kubernetes/node-problem-detector/issues/66,Add CHANGELOG.md for NPD repo.,"Fixes https://github.com/kubernetes/node-problem-detector/issues/45.
For https://github.com/kubernetes/node-problem-detector/issues/58.

This PR adds a CHANGELOG.md for NPD repo.

The format follows http://keepachangelog.com/.
@jfilak Thanks for your awesome suggestion! :)

@jfilak @andyxning Can you help me review this PR? Thanks a lot!

/cc @dchen1107 

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/66)
<!-- Reviewable:end -->
",closed,True,2017-01-11 09:29:01,2017-01-12 00:33:07
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/67,https://api.github.com/repos/kubernetes/node-problem-detector/issues/67,Release instruction is needed.,"Forked from https://github.com/kubernetes/node-problem-detector/pull/63#issuecomment-271804778.
We need a release instruction to standardize the release process. The release instruction should cover:
* How to build release packages
  * Build docker image.
  * Build tar ball for standalone.
  * ...
* How to cut release
  * Version a release
  * Update CHANGELOG.md
  * Create branch/tag
  * Create github release.
  * ...
* How to update kubernetes
  * Update e2e test
  * Update addon pods in cluster
  * ...

This has relatively lower priority than the features, will file the doc before cutting release.",closed,False,2017-01-11 21:31:20,2018-02-18 08:42:26
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/68,https://api.github.com/repos/kubernetes/node-problem-detector/issues/68,Add --hostname-override flag.,"Currently NPD gets hostname override from `NODE_NAME` env. https://github.com/kubernetes/node-problem-detector/blob/master/pkg/problemclient/problem_client.go#L73

This is not friendly for standalone mode.

We should add a `--hostname-override` flag, and set the flag with `NODE_NAME` from downward api in the yaml file. https://github.com/kubernetes/node-problem-detector/blob/master/node-problem-detector.yaml#L21",closed,False,2017-01-11 21:35:38,2017-01-18 19:03:59
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/69,https://api.github.com/repos/kubernetes/node-problem-detector/issues/69,"Create ""latest"" NPD image.",We should create a latest NPD image when each release.,closed,False,2017-01-11 23:59:35,2018-02-18 05:39:00
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/70,https://api.github.com/repos/kubernetes/node-problem-detector/issues/70,Add CHANGELOG.md for NPD repo.,"Fixes https://github.com/kubernetes/node-problem-detector/issues/45.
For https://github.com/kubernetes/node-problem-detector/issues/58.

This PR adds a CHANGELOG.md for NPD repo.

The format follows http://keepachangelog.com/.
@jfilak Thanks for your awesome suggestion! :)

@jfilak @andyxning Can you help me review this PR? Thanks a lot!

/cc @dchen1107

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/70)
<!-- Reviewable:end -->
",closed,True,2017-01-12 00:34:38,2017-01-12 07:38:28
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/71,https://api.github.com/repos/kubernetes/node-problem-detector/issues/71,Add --version flag.,"Add `--version` flag, which is useful for standalone NPD.

The version format is using `git describe --tags --dirty`.

Current version is like this:
```
$ make version
v0.2.0-39-gaedb371

$ ./node-problem-detector --version
v0.2.0-39-gaedb371
```

/cc @dchen1107 
<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/71)
<!-- Reviewable:end -->
",closed,True,2017-01-12 10:08:04,2017-01-13 18:52:06
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/72,https://api.github.com/repos/kubernetes/node-problem-detector/issues/72,detail how node-problem-detector get node name in README,"Ref: #68

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/72)
<!-- Reviewable:end -->
",closed,True,2017-01-12 14:02:23,2017-01-18 19:02:57
node-problem-detector,shyamjvs,https://github.com/kubernetes/node-problem-detector/issues/73,https://api.github.com/repos/kubernetes/node-problem-detector/issues/73,apiserver-override flag in standalone mode not working in kubemark,"So I create npd containers inside hollow-nodes (just for clarity: these are actually pods) of kubemark by passing the address of the kubemark master and the kubeconfig file through the override flag as follows:
```
""name"": ""hollow-node-problem-detector"",
""image"": ""gcr.io/google_containers/node-problem-detector:v0.3"",
""env"": [
						{
							""name"": ""NODE_NAME"",
							""valueFrom"": {
								""fieldRef"": {
									""fieldPath"": ""metadata.name""
								}
							}
						}
],
""command"": [
						""/node-problem-detector"",
						""--kernel-monitor=/config/kernel-monitor.json"",
						""--apiserver-override=https://104.198.41.48:443?inClusterConfig=false&auth=/kubeconfig/npd_kubeconfig"",
						""--alsologtostderr"",
						""1>>/var/logs/npd_$(NODE_NAME).log 2>&1""
],
""volumeMounts"": ....
```

I even checked inside the container using `kubectl exec` that the right kubeconfig file is present on the container and NODE_NAME env var is defined. Here's the kubeconfig:
```
apiVersion: v1
kind: Config
users:
- name: node-problem-detector
  user:
    token: 8qt8RLdwIKeZQ0QeVrSWg4BrqK3Cs8H8
clusters:
- name: kubemark
  cluster:
    insecure-skip-tls-verify: true
    server: https://104.198.41.48
contexts:
- context:
    cluster: kubemark
    user: node-problem-detector
  name: kubemark-npd-context
current-context: kubemark-npd-context
```

Despite this, the npd seems to communicate with the wrong master (the master of the real cluster underneath the kubemark cluster). It might probably be still using the inClusterConfig from the real node underneath the hollow-node, as that seems to be the only way by which it can get hold of the wrong master's IP address.

cc @Random-Liu @andyxning ",closed,False,2017-01-12 17:33:49,2018-02-23 04:36:03
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/74,https://api.github.com/repos/kubernetes/node-problem-detector/issues/74,Add release tarball,"Based on #71.

Add release tarball and more command in the Makefile. The name of the tarball is `node-problem-detector-$(VERSION).tar.gz`.

For my local dirty directory, it's `node-problem-detector-v0.2.0-40-gb3b72c5-dirty.tar.gz`. For a newly tagged release, it should be `node-problem-detector-v0.3.0.tar.gz`.

```
make build-container # Build the container
make build-tar # Build the tarball
make build # Build both the container and the tarball.
make push-container # Push the container to image registry
make push-tar # Push the tarball to cloud storage
make push # Push both the container and the tarball
```

The tarball will be used to deploy standalone NPD in kubernetes cluster.

@andyxning Could you help me review this? Since you are quite familiar with the Makefile. :)
/cc @dchen1107

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/74)
<!-- Reviewable:end -->
",closed,True,2017-01-13 01:40:01,2017-01-14 00:36:42
node-problem-detector,shyamjvs,https://github.com/kubernetes/node-problem-detector/pull/75,https://api.github.com/repos/kubernetes/node-problem-detector/issues/75,Made authFile override logic not fallback to in-cluster config,"Fixes #73 

Changed the function `GetKubeClientConfig`  to use `NewNonInteractiveClientConfig` instead of `NewNonInteractiveDeferredLoadingClientConfig` as we do not want to fallback to in-cluster-config if authFile is specified.
(Inspired from this example https://github.com/kubernetes/kubernetes/blob/master/cmd/kubelet/app/bootstrap.go#L149)
We would also want to have such a similar change in heapster.

cc @Random-Liu @andyxning

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/75)
<!-- Reviewable:end -->
",closed,True,2017-01-13 17:09:16,2017-01-13 19:19:50
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/76,https://api.github.com/repos/kubernetes/node-problem-detector/issues/76,Standalone NPD Support,"For better reliability, in Kubernetes 1.6, we plan to add standalone NPD support to run NPD as a system daemon on each node.

To achieve this, we need to:
- [x] Add `apiserver-override` option to make it possible to use customized client config instead of `InClusterConfig`. https://github.com/kubernetes/node-problem-detector/pull/49
- [x] Make NPD wait for: https://github.com/kubernetes/node-problem-detector/pull/79
  * Apiserver to come up. https://github.com/kubernetes/node-problem-detector/issues/19
  * Kubelet to register node.
This is necessary, because in standalone mode, NPD may be deployed before apiserver and kubelet are functioning. It should wait for them to come up.
- [x] NPD should host a simple http server for readiness check and health monitor. #83
  * Requred: Add /health for health monitoring.
  * Optional: Add /pprof for performance debugging.
  * Optional: Add debug endpoint to list internal state.
  * Optional: Add grpc problem report endpoint.
- [x] Resource Limit
  - [x] Get benchmark data about the NPD resource usage https://github.com/kubernetes/node-problem-detector/issues/85
  - [x] ~~~Adjust `NodeAllocatable` according to the resource usage accordingly.~~~
- [x] Add standalone NPD into K8s GCI cloud init script. https://github.com/kubernetes/kubernetes/pull/40206
  - [x] Package NPD binary as tar ball and upload it to google storage. https://github.com/kubernetes/node-problem-detector/pull/71, https://github.com/kubernetes/node-problem-detector/pull/74
  - [x] Download and setup NPD binary in K8s GCI cloud init script.
- [x] NPD should have a specific user account which has necessary permission in K8s cluster. Similar with [`system:kube-proxy`](https://github.com/kubernetes/kubernetes/blob/master/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/cluster-role-bindings.yaml#L90).
",closed,False,2017-01-13 22:17:33,2017-02-21 23:49:14
node-problem-detector,fate-grand-order,https://github.com/kubernetes/node-problem-detector/pull/77,https://api.github.com/repos/kubernetes/node-problem-detector/issues/77,"fix misspell ""initialize"" in problem_client.go","

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/77)
<!-- Reviewable:end -->
",closed,True,2017-01-17 07:14:24,2017-01-17 16:00:43
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/78,https://api.github.com/repos/kubernetes/node-problem-detector/issues/78,change apiserver-override argument from long option format to short o…,"change apiserver-override from long option format to short option format. I.e.:
`--apiserver-override` -> `-apiserver-override`

This makes README being coherent with the output of `node-problem-detector -h`.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/78)
<!-- Reviewable:end -->
",closed,True,2017-01-17 16:14:52,2017-01-17 16:29:28
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/79,https://api.github.com/repos/kubernetes/node-problem-detector/issues/79,Update NPD to only do forcibly sync every 1 minutes.,"This PR:
1) Change the condition manager to do resync every 10s. Every 10s condition manager will check whether there is a previously failed update, and update again if there is. This also fixes a bug that once apiserver is down more than `resyncPeriod` (30s), NPD will keep retrying every 1s and print a log of error logs.
2) Add 1m heartbeat period. Every 1m condition manager will sync with apiserver no matter what, we need this to:
  * Avoid other components changing the condition unexpectedly.
  * Update `LastHeartbeatTime` to indicate that NPD is still alive and current conditions are valid.

For https://github.com/kubernetes/node-problem-detector/issues/76.
Fixes https://github.com/kubernetes/node-problem-detector/issues/19.

/cc @andyxning Would you like to take a look at this PR? Thanks!
/cc @wojtek-t This PR changes NPD to `PATCH` every 1 minute.
/cc @dchen1107 

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/79)
<!-- Reviewable:end -->
",closed,True,2017-01-18 02:17:18,2017-01-24 08:39:51
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/80,https://api.github.com/repos/kubernetes/node-problem-detector/issues/80,Add doc for `-kernel-monitor` and `-version` flags.,"This PR added documentation about the flags `-version` and `-kernel-monitor` in README.md.

@andyxning Could you help me review this 3 line change? Thanks!

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/80)
<!-- Reviewable:end -->
",closed,True,2017-01-19 23:33:38,2017-01-20 07:38:35
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/81,https://api.github.com/repos/kubernetes/node-problem-detector/issues/81,Fix kernel monitor issues,"Based on https://github.com/kubernetes/node-problem-detector/pull/39. Will rebase after https://github.com/kubernetes/node-problem-detector/pull/39 gets merged.

Only the last commit is new.

This PR:
* Change `unregister_netdevice` to be an event to fix #47. /cc @euank 
* Change `KernelPanic` to `KernelOops` because we can't really handle kernel panic by simply parsing kernel log.
  * We can consider supporting [`ramoops`](http://lxr.free-electrons.com/source/Documentation/ramoops.txt), but GCE doesn't support it now.
  * Other options are to use `serial console` or `PVPANIC`, but it could not be achieved inside the node scope. We may introduce a cluster level node health monitoring component in the future.
* Use system boot time instead of [`StartPattern`](https://github.com/kubernetes/node-problem-detector/blob/master/config/kernel-monitor.json#L4) to fix #48 /cc @euank . Since `KernelPanic` is not in the scope any more, we can only care about current boot now.

/cc @dchen1107 @jfilak

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/81)
<!-- Reviewable:end -->
",closed,True,2017-01-21 02:24:56,2017-02-10 19:17:17
node-problem-detector,fate-grand-order,https://github.com/kubernetes/node-problem-detector/pull/82,https://api.github.com/repos/kubernetes/node-problem-detector/issues/82,correct spelling error in kernel_monitor.go,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/82)
<!-- Reviewable:end -->
",closed,True,2017-01-22 14:21:49,2017-01-23 18:39:01
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/83,https://api.github.com/repos/kubernetes/node-problem-detector/issues/83,"Add NPD endpoints: /debug/pprof, /healthz, /conditions.","For https://github.com/kubernetes/node-problem-detector/issues/76.

This PR:
* Added `/healthz` for health checking. Currently it always returns 200.
* Added `/conditions` to list all internal conditions inside the NPD. This is mainly used for debugging.
* Added `/debug/pprof` pprof endpoint. This will be used for following performance benchmark and optimization.

Note that to get more debug information, we removed the `-w` ldflag. The binary size only increased from 40MB+ to 60MB+, which should be fine.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/83)
<!-- Reviewable:end -->
",closed,True,2017-01-25 00:18:51,2017-02-04 04:41:12
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/84,https://api.github.com/repos/kubernetes/node-problem-detector/issues/84,Only change transition timestamp when condition is changed.,"Previously, we update transition timestamp whenever a new problem is matched from the log.
However, the kernel problem sometimes shows up for multiple even infinity times, for example the hung task problem. This will cause a lot of unnecessary condition updates.

This PR changes kernel monitor to only update the transition timestamp and message when problem status or reason is changed. This makes sure that we only update condition when it actually changes.

This change is also for scalability (Ref https://github.com/kubernetes/node-problem-detector/issues/58)
<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/84)
<!-- Reviewable:end -->
",closed,True,2017-01-27 22:50:31,2017-02-08 18:53:44
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/85,https://api.github.com/repos/kubernetes/node-problem-detector/issues/85,Performance benchmark and optimization,"We've got some data from https://github.com/kubernetes/node-problem-detector/issues/2.

However, after that we've made a lot of changes. We should do benchmark to measure the cpu and memory usage. And optimize the log parsing code of kernel monitor if the resource usage is too high.

An accurate and acceptable number is important for NPD rollout.

/cc @dchen1107 ",closed,False,2017-02-02 00:37:25,2018-02-19 12:08:59
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/86,https://api.github.com/repos/kubernetes/node-problem-detector/issues/86,We should add real case e2e test.,"We already have a NPD e2e test in kubernetes repo: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/node_problem_detector.go.

However, the e2e test only doesn't actually test the NPD deployed in the cluster. It:
* Deploy a NPD pod with test config.
* Generate test log in the test log file.
* Check whether test node condition is set.

Although this test is still necessary, we should add a new test to really inject log into kernel log or even trigger kernel problem and test the real NPD deployed in the cluster.

The test is disruptive, we may want to add `[Feature]` tag and create a dedicated jenkins job for it.

/cc @dchen1107 ",closed,False,2017-02-02 00:50:25,2018-02-18 22:55:59
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/87,https://api.github.com/repos/kubernetes/node-problem-detector/issues/87,change long option prefix to short option prefix,"This PR change long command line option prefix to short command line option prefix. I.e., `--` -> `-`.

@Random-Liu PTAL.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/87)
<!-- Reviewable:end -->
",closed,True,2017-02-03 10:41:05,2017-02-04 09:53:14
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/88,https://api.github.com/repos/kubernetes/node-problem-detector/issues/88,Add arbitray system log support,"For https://github.com/kubernetes/node-problem-detector/issues/44.

Based on https://github.com/kubernetes/node-problem-detector/pull/81.

This PR:
* Add `PluginConfig` for plugin specific configuration.
* Add `source` in journald plugin configuration to make sdjournal `SYSTEM_IDENTIFIER` configurable.
* Add `timestamp` regular expression, `message` regular expression and `timestampFormat` configuration in syslog plugin configuration. (We plan to change this to be `filelog`, because it can handle not only syslog)

TODOs for this work:
* Add multiple configuration files support. NPD should start a system log monitor for each passed in configurtion, so that it can monitor different services at the same time.
* Rename `syslog` to `filelog`.
* Rename `kernel monitor` to `system log monitor`, change all ""kernel"" reference in the code.
* Optimize the efficiency to make the system log monitor more light weight.
* Add document for the new behavior.
* Update kubernetes e2e test to test the new behavior.

/cc @dchen1107 @kubernetes/node-problem-detector-reviewers

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/88)
<!-- Reviewable:end -->
",closed,True,2017-02-03 11:22:08,2017-02-16 00:01:27
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/89,https://api.github.com/repos/kubernetes/node-problem-detector/issues/89,fix npd binary generation,"This PR fix [#39(discussion1)](https://github.com/kubernetes/node-problem-detector/pull/39#discussion_r89067352), [#39(discussion2)](https://github.com/kubernetes/node-problem-detector/pull/39#discussion_r99326431).

Way to repreduce(adjusted from [#39(discussion1)](https://github.com/kubernetes/node-problem-detector/pull/39#discussion_r89067352))

```
make
# The ./bin/node-problem-detector file didn't exist and make created it.
# systemd-journal is enabled.
make ENABLE_JOURNALD=0
# The ./bin/node-problem-detector file already exists and make skipped the ./bin/node-problem-detector target. 
# However the user believes that node-problem-detector is compiled with systemd-journal disabled.
```

/cc @dchen1107 @jfilak @kubernetes/node-problem-detector-reviewers

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/89)
<!-- Reviewable:end -->
",closed,True,2017-02-03 12:29:32,2018-03-09 11:11:36
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/90,https://api.github.com/repos/kubernetes/node-problem-detector/issues/90,change flag to pflag,"This PR is followed by #87 .

This PR will replace `flag` package with `spf13/pflag`. The output of `./bin/node-problem-detector -h` will like below:
```
Usage of ./bin/node-problem-detector:
      --address=""127.0.0.1"": The address to bind the node problem detector server.
      --alsologtostderr[=false]: log to standard error as well as files
      --apiserver-override="""": Custom URI used to connect to Kubernetes ApiServer
      --hostname-override="""": Custom node name used to override hostname
      --kernel-monitor=""/config/kernel-monitor.json"": The path to the kernel monitor config file
      --log_backtrace_at=:0: when logging hits line file:N, emit a stack trace
      --log_dir="""": If non-empty, write log files in this directory
      --logtostderr[=false]: log to standard error instead of files
      --port=10256: The port to bind the node problem detector server. Use 0 to disable.
      --stderrthreshold=2: logs at or above this threshold go to stderr
      --v=0: log level for V logs
      --version[=false]: Print version information and quit
      --vmodule=: comma-separated list of pattern=N settings for file-filtered logging
```

At the same time, this PR also changes command line argument usage info in docs and configuration. :)

/cc @dchen1107 @kubernetes/node-problem-detector-reviewers

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/90)
<!-- Reviewable:end -->
",closed,True,2017-02-05 15:33:47,2017-02-07 02:44:17
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/91,https://api.github.com/repos/kubernetes/node-problem-detector/issues/91,add options pkg,"This PR refactor npd with adding `options` package and moving all command line arguments to it.

This makes node problem detector more modular and clear. :)

/cc @dchen1107 @kubernetes/node-problem-detector-reviewers

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/91)
<!-- Reviewable:end -->
",closed,True,2017-02-06 12:45:18,2017-02-07 18:38:50
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/92,https://api.github.com/repos/kubernetes/node-problem-detector/issues/92,Generalize the kernel monitor code.,"For https://github.com/kubernetes/node-problem-detector/issues/44.
Based on https://github.com/kubernetes/node-problem-detector/pull/81 and https://github.com/kubernetes/node-problem-detector/pull/88.

his PR:
* Change kernel monitor to system log monitor, because we want to generalize kernel monitor to detect problems of different system daemons.
* Change `syslog` to `filelog`, because actually it does not only support syslog now, it should be able to support most file based logs.
* Update the README.md.

/cc @dchen1107 @kubernetes/node-problem-detector-reviewers

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/92)
<!-- Reviewable:end -->
",closed,True,2017-02-07 01:21:59,2017-02-15 21:14:43
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/93,https://api.github.com/repos/kubernetes/node-problem-detector/issues/93,Create a goroutine for the http server.,"The original goroutine is removed by mistake when addressing comments in https://github.com/kubernetes/node-problem-detector/pull/83.

An e2e test is really needed now. :(

@andyxning Could you help me review this?

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/93)
<!-- Reviewable:end -->
",closed,True,2017-02-08 03:14:28,2017-02-08 08:04:39
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/94,https://api.github.com/repos/kubernetes/node-problem-detector/issues/94,Add multiple system log monitor support,"For #44.
Based on #81, #88 and #92.

Only the last commit is new.

This PR change the `--log-monitor` flag to `--log-monitors`. Multiple log monitor config separated by comma can be passed through `--log-monitors`, NPD will start a separate log monitor for each of them.

This makes it possible to use NPD to monitor different system log at the same time.

/cc @dchen1107 @kubernetes/node-problem-detector-reviewers

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/94)
<!-- Reviewable:end -->
",closed,True,2017-02-08 23:24:08,2017-02-17 02:38:19
node-problem-detector,fate-grand-order,https://github.com/kubernetes/node-problem-detector/pull/95,https://api.github.com/repos/kubernetes/node-problem-detector/issues/95,"fix misspell ""timestamp""","

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/95)
<!-- Reviewable:end -->
",closed,True,2017-02-21 15:01:38,2017-02-21 21:33:56
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/96,https://api.github.com/repos/kubernetes/node-problem-detector/issues/96,refactor options pkg,"Refactor `options` package to `cmd` directory.


/cc @Random-Liu @dchen1107

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/96)
<!-- Reviewable:end -->
",closed,True,2017-02-22 13:44:21,2017-02-23 02:19:33
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/97,https://api.github.com/repos/kubernetes/node-problem-detector/issues/97,fix heapster reference to master branch,"Fix heapster document reference from commit to master branch. 


/cc @random-liu

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/97)
<!-- Reviewable:end -->
",closed,True,2017-02-22 13:51:18,2017-02-23 00:10:18
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/98,https://api.github.com/repos/kubernetes/node-problem-detector/issues/98,Fix journald plugin to only look at the current boot.,"Previously I thought `go-system/sdjournal` will only look at current boot by default.

However, it turns out that is not the case when I verified npd with some reboot test.

Ideally we should use [`sd_journal_seek_monotonic_usec`](https://www.freedesktop.org/software/systemd/man/sd_journal_seek_head.html), or [`sd_journal_seek_cursor`](https://www.freedesktop.org/software/systemd/man/sd_journal_seek_head.html), but sdjournal doesn't support it very well. So in this PR, we use the system uptime to make sure we npd only looks at current boot.

@dchen1107 @kubernetes/node-problem-detector-reviewers

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/98)
<!-- Reviewable:end -->
",closed,True,2017-03-02 22:06:54,2017-03-14 03:40:22
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/99,https://api.github.com/repos/kubernetes/node-problem-detector/issues/99,Add test kernel log generator.,"Add a ""kernel log generator"" container for test purpose.

I'm going to use this test container in npd cluster e2e functionality test and performance test.

Note that currently kernel log generator only works with journald.

@dchen1107 @kubernetes/node-problem-detector-reviewers

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/99)
<!-- Reviewable:end -->
",closed,True,2017-03-02 22:59:04,2017-03-15 20:43:33
node-problem-detector,euank,https://github.com/kubernetes/node-problem-detector/pull/100,https://api.github.com/repos/kubernetes/node-problem-detector/issues/100,gitignore: ignore the release tarball,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/100)
<!-- Reviewable:end -->
",closed,True,2017-03-10 04:43:44,2017-03-10 22:25:21
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/101,https://api.github.com/repos/kubernetes/node-problem-detector/issues/101,Fix journald plugin to only look at the current boot.,"Cherrypick #98 into v0.3 branch.

After this is merged. We need to cut a new release, update Kubernetes main repo, and possibly merge https://github.com/kubernetes/kubernetes/pull/42454.

@dchen1107

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/101)
<!-- Reviewable:end -->
",closed,True,2017-03-15 18:28:17,2017-03-15 21:23:11
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/102,https://api.github.com/repos/kubernetes/node-problem-detector/issues/102,DaemonSet NPD creates /var/log/journal on non-journald node.,"Ideally, we should not use journald config on non-systemd node. However, if we use, it is expected to error out.

However, now because of some unknown reason, NPD will create a `/var/log/journal` directory and keep going. Related code: https://github.com/kubernetes/node-problem-detector/blob/master/pkg/systemlogmonitor/logwatchers/journald/log_watcher.go#L141

This should not matter much, because no one will actually write any log, so NPD will just hang and not consume more resource. But we should still fix this.

@kubernetes/node-problem-detector-maintainers ",closed,False,2017-03-15 23:51:20,2018-02-20 07:27:59
node-problem-detector,zhangxiaoyu-zidif,https://github.com/kubernetes/node-problem-detector/pull/103,https://api.github.com/repos/kubernetes/node-problem-detector/issues/103,update README.md,"add ""which"" into the document.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/103)
<!-- Reviewable:end -->
",closed,True,2017-03-19 10:46:41,2017-03-27 17:55:44
node-problem-detector,juliusmilan,https://github.com/kubernetes/node-problem-detector/pull/104,https://api.github.com/repos/kubernetes/node-problem-detector/issues/104,Add detailed build information to README.md,"Your build information provided in README.md is not comprehensive.
For disinterested person without experience with go it may take too long to build it.
This patch adds the missing information.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/104)
<!-- Reviewable:end -->
",closed,True,2017-03-20 09:55:52,2018-02-20 15:36:00
node-problem-detector,juliusmilan,https://github.com/kubernetes/node-problem-detector/pull/105,https://api.github.com/repos/kubernetes/node-problem-detector/issues/105,Add ABRT adaptor config,"This config allows NPD to catch problems detected by ABRT, ABRT can find various problems (see the link) and log them to journalctl in format described here:
https://github.com/abrt/abrt/wiki/systemd-journal-catalog-messages

Work sequence:
1. ABRT processes problem and logs to journal
2. NDP catches ABRT logs and report them to upper layer

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/105)
<!-- Reviewable:end -->
",closed,True,2017-03-23 15:45:06,2017-03-29 17:45:06
node-problem-detector,strugglingyouth,https://github.com/kubernetes/node-problem-detector/issues/106,https://api.github.com/repos/kubernetes/node-problem-detector/issues/106,"Now i am running daemonset in all node, but how do i verify it is useful？","What happens to the node wrong after creating daemonset? And set node to unscheduler？   

Now i am running daemonset in all node, but how do i verify it is useful。

It is a bit difficult to simulate these problems。

Hardware issues: Bad cpu, memory or disk;
Kernel issues: Kernel deadlock, corrupted file system;
Container runtime issues: Unresponsive runtime daemon;
...",closed,False,2017-03-30 06:56:08,2018-04-09 09:04:40
node-problem-detector,PANLEI361,https://github.com/kubernetes/node-problem-detector/issues/107,https://api.github.com/repos/kubernetes/node-problem-detector/issues/107,"now,i canot run it in kub1.5.3","`Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath				Type		Reason			Message
  ---------	--------	-----	----			-------------				--------	------			-------
  28m		28m		1	{kubelet 10.8.65.157}						Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded for volume ""kubernetes.io/host-path/5203c2d8-1b73-11e7-8754-46c88707ea0b-log"" (spec.Name: ""log"") pod ""5203c2d8-1b73-11e7-8754-46c88707ea0b"" (UID: ""5203c2d8-1b73-11e7-8754-46c88707ea0b"").
  28m		28m		1	{kubelet 10.8.65.157}						Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded for volume ""kubernetes.io/host-path/5203c2d8-1b73-11e7-8754-46c88707ea0b-localtime"" (spec.Name: ""localtime"") pod ""5203c2d8-1b73-11e7-8754-46c88707ea0b"" (UID: ""5203c2d8-1b73-11e7-8754-46c88707ea0b"").
  28m		28m		1	{kubelet 10.8.65.157}	spec.containers{node-problem-detector}	Normal		Created			Created container with docker id e65759efc680; Security:[seccomp=unconfined]
  28m		28m		1	{kubelet 10.8.65.157}	spec.containers{node-problem-detector}	Normal		Started			Started container with docker id e65759efc680
  28m		28m		1	{kubelet 10.8.65.157}	spec.containers{node-problem-detector}	Normal		Created			Created container with docker id a7e78dfbe63a; Security:[seccomp=unconfined]
  28m		28m		1	{kubelet 10.8.65.157}	spec.containers{node-problem-detector}	Normal		Started			Started container with docker id a7e78dfbe63a
  27m		27m		2	{kubelet 10.8.65.157}						Warning		FailedSync		Error syncing pod, skipping: failed to ""StartContainer"" for ""node-problem-detector"" with CrashLoopBackOff: ""Back-off 10s restarting failed container=node-problem-detector pod=node-problem-detector-gfx8q_default(5203c2d8-1b73-11e7-8754-46c88707ea0b)""

  27m	27m	1	{kubelet 10.8.65.157}	spec.containers{node-problem-detector}	Normal	Created		Created container with docker id 6d3ac4da1b6e; Security:[seccomp=unconfined]
  27m	27m	1	{kubelet 10.8.65.157}	spec.containers{node-problem-detector}	Normal	Started		Started container with docker id 6d3ac4da1b6e
  27m	27m	2	{kubelet 10.8.65.157}						Warning	FailedSync	Error syncing pod, skipping: failed to ""StartContainer"" for ""node-problem-detector"" with CrashLoopBackOff: ""Back-off 20s restarting failed container=node-problem-detector pod=node-problem-detector-gfx8q_default(5203c2d8-1b73-11e7-8754-46c88707ea0b)""

`",closed,False,2017-04-07 09:47:51,2018-02-21 05:50:01
node-problem-detector,WIZARD-CXY,https://github.com/kubernetes/node-problem-detector/issues/108,https://api.github.com/repos/kubernetes/node-problem-detector/issues/108,can't update node condition,"Hi, I deployed npd yesterday in my k8s 1.6, Now I find it can't update node condition.Below is its log

```
E0418 09:48:32.826374       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""cs55"": there is a meaningful conflict (firstResourceVersion: ""881081"", currentResourceVersion: ""881106""):
 diff1={""metadata"":{""resourceVersion"":""881106""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-04-18T01:48:32Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2017-04-18T01:47:32Z"",""type"":""KernelDeadlock""},{""lastHeartbeatTime"":""2017-04-18T01:48:32Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2017-04-18T01:48:32Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2017-04-18T01:48:32Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-04-18T01:48:32Z"",""lastTransitionTime"":""2017-04-17T09:10:43Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
```
I wonder why this happen? @Random-Liu thx",closed,False,2017-04-18 02:02:27,2017-04-18 09:39:03
node-problem-detector,jason-riddle,https://github.com/kubernetes/node-problem-detector/issues/109,https://api.github.com/repos/kubernetes/node-problem-detector/issues/109,[Question] Should this tool be monitoring CPU steal?,I was reading through http://blog.scoutapp.com/articles/2013/07/25/understanding-cpu-steal-time-when-should-you-be-worried and I'm curious if the node-problem-detector should be watching for CPU steal?,closed,False,2017-04-27 15:14:24,2018-02-21 20:04:02
node-problem-detector,jason-riddle,https://github.com/kubernetes/node-problem-detector/issues/110,https://api.github.com/repos/kubernetes/node-problem-detector/issues/110,[Feature] Check for SELinux / AppArmor Denies,"If the kubelet is installed and then for some reason a security policy is changed later on a critical kubernetes component, it might be wise to watch out for that.",closed,False,2017-04-27 15:31:34,2018-02-21 20:04:02
node-problem-detector,juliusmilan,https://github.com/kubernetes/node-problem-detector/pull/111,https://api.github.com/repos/kubernetes/node-problem-detector/issues/111,Abrt readme and ccpp fix,"This PR contains 2 changes.

* First is README update, that contains documented use of ABRT by NPD, my leader asked me to do.
* Second is abrt-adaptor ccpp pattern fix, I found it didn't catch all possible cpp problem messages produced by ABRT.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/111)
<!-- Reviewable:end -->
",closed,True,2017-05-10 06:45:28,2017-05-15 15:58:22
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/112,https://api.github.com/repos/kubernetes/node-problem-detector/issues/112,Cleanup kmsg log watcher,"Address comments in https://github.com/kubernetes/node-problem-detector/pull/41.

@dchen1107

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/112)
<!-- Reviewable:end -->
",closed,True,2017-05-30 22:59:37,2017-05-30 23:46:15
node-problem-detector,bowei,https://github.com/kubernetes/node-problem-detector/issues/113,https://api.github.com/repos/kubernetes/node-problem-detector/issues/113,Documentation: example of how to add a new detector,"It would be good to put some documentation as to how to write a new detector. Maybe give an example of a trivial detector in the code base that contributors can take and extend.
",closed,False,2017-06-02 00:17:01,2018-02-24 00:55:50
node-problem-detector,jdfless,https://github.com/kubernetes/node-problem-detector/issues/114,https://api.github.com/repos/kubernetes/node-problem-detector/issues/114,status doesn't change when injecting log messages,"I cannot get the node problem detector to change a node status by injecting messages.

I am using kubernetes 1.5.2, Ubuntu 16.04, kernel 4.4.0-51-generic.

I run the npd as a daemonset. I have attempted to get this to work with the npd as version 0.3.0 and 0.4.0. I start the npd with the default command, using /config/kernel-monitor.json because my nodes use journald.

I have /dev/kmsg mounted into the pod, and I echo expressions matching the regexs in the kernel-monitor.json to /dev/kmsg on the node. I can view the fake logs I've echoed to /dev/kmsg in the pod.

Steps to reproduce:
```# have node problem detector version 0.3.0 or 0.4.0 running as a pod on a kubernetes node with journald
# as root on the node where your pod is running
echo ""task umount.aufs:123 blocked for more than 120 seconds."" >> /dev/kmsg
# I have verified that these logs show up in journalctl -k

# this should match the following permanent condition in /config/kernel-monitor.json
#	{
#		""type"": ""permanent"",
#		""condition"": ""KernelDeadlock"",
#		""reason"": ""AUFSUmountHung"",
#		""pattern"": ""task umount\\.aufs:\\w+ blocked for more than \\w+ seconds\\.""
#	},

# check the node status of the node where you ran this on
kubectl get node <node>
# status will still be Ready

# for further detail examine the json
kubectl get node <node> -o json | jq .status.conditions
# you will see that the KernelDeadlock condition is still ""False""

# I would expect the general status to change to ""KernelDeadlock""
```

If I am not testing this properly, could you please give a detailed breakdown of how to test the node problem detector is working properly for kernel logs AND docker logs?

I have also reproduced this behavior using a custom docker_monitor.json and having the systemd docker service write to the journald docker logs. I have still been unsuccessful in getting the node status to change.",closed,False,2017-06-08 17:57:42,2017-06-14 22:31:08
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/115,https://api.github.com/repos/kubernetes/node-problem-detector/issues/115,Use chroot instead of relying on a systemd base image.,"We want NPD to support journald log even when it's running as daemonset inside a container.

However, it's hard to consume host journal log inside container. Previously, we use `fedora` base image, and rely on the journald library inside the base image to understand and read the host journald log. However, there are several limitations:
1) The fedora base image is pretty big, and there are a lot of things we don't actually need.
2) The journald version inside the container may be mismatch with the host, and sometimes causes problems. E.g. the npd image doesn't work with new GCI image now, and also the user bug report https://github.com/kubernetes/node-problem-detector/issues/114.

A more generic solution may be mount the host `/` inside the container, and `chroot`. In this way, we could use the journald on the host directly, which eliminates the problems above.
```
$ docker run -it --privileged -v /:/rootfs --entrypoint=/bin/bash gcr.io/google_containers/node-problem-detector:v0.4.0
[root@07fb0f31a048 /]# chroot /rootfs
sh-4.3# journalctl -k
-- Logs begin at Sun 2017-06-11 12:12:02 UTC, end at Mon 2017-06-12 18:59:06 UTC. --
Jun 12 18:19:54 e2e-test-lantaol-master kernel: device veth43d8449 entered promiscuous mode
Jun 12 18:19:54 e2e-test-lantaol-master kernel: IPv6: ADDRCONF(NETDEV_UP): veth43d8449: link is not ready
Jun 12 18:19:54 e2e-test-lantaol-master kernel: eth0: renamed from veth75b3ac0
...
```
I haven't thought through the potential security problem yet, but since NPD is a privileged daemonset, it's reasonable to grant host fs access to it.",closed,False,2017-06-12 18:59:16,2018-02-25 13:31:49
node-problem-detector,jdfless,https://github.com/kubernetes/node-problem-detector/pull/116,https://api.github.com/repos/kubernetes/node-problem-detector/issues/116,Update documentation to recommended practices in v0.4.0,"Per the issues I ran into in #114, I have created this pull request to bring the docs up to date with the recommended practices for version 0.4.0.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/116)
<!-- Reviewable:end -->
",closed,True,2017-06-14 21:52:21,2017-06-14 22:13:14
node-problem-detector,ajitak,https://github.com/kubernetes/node-problem-detector/pull/117,https://api.github.com/repos/kubernetes/node-problem-detector/issues/117,Add rule for docker image pull error,"Add a new rule which catches docker image corruption. It will address https://github.com/kubernetes/kubernetes/issues/47219

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/117)
<!-- Reviewable:end -->
",closed,True,2017-06-14 23:58:26,2017-06-21 23:33:46
node-problem-detector,trinitronx,https://github.com/kubernetes/node-problem-detector/issues/118,https://api.github.com/repos/kubernetes/node-problem-detector/issues/118,node-problem-detector:v0.4.0 - failed to update node conditions: Timeout,"Hello,

I am testing out the [node-problem-detector cluster addon][npd-0.4.0] and am seeing this error on only 1 node: 

 - `failed to update node conditions: Timeout: request did not complete within allowed duration`

This cluster has had issues in the past due to some `etcd3` nodes becoming unhealthy, and `flanneld` being unable to connect to `etcd3` at one point causing `kube-apiserver` to fail to respond, and hence `kubelet`, `kube-controller-manager`, `kube-proxy`, and `kube-scheduler` to log various api timeout errors.

After restarting everything, the rest of the nodes are healthy & both `etcd3` + `kube-apiserver` appear to be contactable by the `kube-system` services.  I then started `npd-0.4.0` using the above manifest and am seeing only 1 node with this issue.  The rest are sometimes logging what appears to be the race condition for updating node status exactly as in #108.

I've checked the 5 node etcd cluster health and only 1 of 5 is in `unhealthy` state.  Requests from `kubectl` return most of the time, however I am seeing some timeouts from some `kubectl describe node XXX` commands.


Relevant log sections below:

**`/var/log/node-problem-detector.log`**:

```
[... BEGIN LOG ...]

I0617 05:08:14.386945       7 log_monitor.go:72] Finish parsing log monitor config file: {WatcherConfig:{Plugin:journald PluginConfig:map[source:kernel] LogPath:/var/log/journal
Lookback:5m} BufferSize:10 Source:kernel-monitor DefaultConditions:[{Type:KernelDeadlock Status:false Transition:0001-01-01 00:00:00 +0000 UTC Reason:KernelHasNoDeadlock Message:
kernel has no deadlock}] Rules:[{Type:temporary Condition: Reason:OOMKilling Pattern:Kill process \d+ (.+) score \d+ or sacrifice child\nKilled process \d+ (.+) total-vm:\d+kB, a
non-rss:\d+kB, file-rss:\d+kB} {Type:temporary Condition: Reason:TaskHung Pattern:task \S+:\w+ blocked for more than \w+ seconds\.} {Type:temporary Condition: Reason:UnregisterNe
tDevice Pattern:unregister_netdevice: waiting for \w+ to become free. Usage count = \d+} {Type:temporary Condition: Reason:KernelOops Pattern:BUG: unable to handle kernel NULL po
inter dereference at .*} {Type:temporary Condition: Reason:KernelOops Pattern:divide error: 0000 \[#\d+\] SMP} {Type:permanent Condition:KernelDeadlock Reason:AUFSUmountHung Patt
ern:task umount\.aufs:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:DockerHung Pattern:task docker:\w+ blocked for more than \w+ second
s\.}]}
I0617 05:08:14.387062       7 log_watchers.go:40] Use log watcher of plugin ""journald""
I0617 05:08:14.387253       7 log_monitor.go:72] Finish parsing log monitor config file: {WatcherConfig:{Plugin:filelog PluginConfig:map[timestamp:^.{15} message:kernel: \[.*\] (
.*) timestampFormat:Jan _2 15:04:05] LogPath:/var/log/kern.log Lookback:5m} BufferSize:10 Source:kernel-monitor DefaultConditions:[{Type:KernelDeadlock Status:false Transition:00
01-01-01 00:00:00 +0000 UTC Reason:KernelHasNoDeadlock Message:kernel has no deadlock}] Rules:[{Type:temporary Condition: Reason:OOMKilling Pattern:Kill process \d+ (.+) score \d
+ or sacrifice child\nKilled process \d+ (.+) total-vm:\d+kB, anon-rss:\d+kB, file-rss:\d+kB} {Type:temporary Condition: Reason:TaskHung Pattern:task \S+:\w+ blocked for more tha
n \w+ seconds\.} {Type:temporary Condition: Reason:UnregisterNetDevice Pattern:unregister_netdevice: waiting for \w+ to become free. Usage count = \d+} {Type:temporary Condition:
 Reason:KernelOops Pattern:BUG: unable to handle kernel NULL pointer dereference at .*} {Type:temporary Condition: Reason:KernelOops Pattern:divide error: 0000 \[#\d+\] SMP} {Typ
e:permanent Condition:KernelDeadlock Reason:AUFSUmountHung Pattern:task umount\.aufs:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:Dock
erHung Pattern:task docker:\w+ blocked for more than \w+ seconds\.}]}
I0617 05:08:14.387286       7 log_watchers.go:40] Use log watcher of plugin ""filelog""
I0617 05:08:14.387923       7 log_monitor.go:81] Start log monitor
E0617 05:08:14.387946       7 problem_detector.go:65] Failed to start log monitor ""/config/kernel-monitor-filelog.json"": failed to stat the file ""/var/log/kern.log"": stat /var/lo
g/kern.log: no such file or directory
I0617 05:08:14.387952       7 log_monitor.go:81] Start log monitor
I0617 05:08:14.389906       7 log_watcher.go:69] Start watching journald
I0617 05:08:14.389947       7 problem_detector.go:74] Problem detector started
I0617 05:08:14.390046       7 log_monitor.go:173] Initialize condition generated: [{Type:KernelDeadlock Status:false Transition:2017-06-17 05:08:14.390031295 +0000 UTC Reason:Ker
nelHasNoDeadlock Message:kernel has no deadlock}]
E0617 06:41:07.988201       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful confli
ct (firstResourceVersion: ""2163685"", currentResourceVersion: ""2163729""):
 diff1={""metadata"":{""resourceVersion"":""2163729""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T06:41:01Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2017-06-17T06:41
:01Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2017-06-17T06:41:01Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2017-06-17T06:41:01Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T06:41:02Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoD
eadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 08:10:02.768550       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful confli
ct (firstResourceVersion: ""2181151"", currentResourceVersion: ""2181185""):
 diff1={""metadata"":{""resourceVersion"":""2181185""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T08:10:02Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2017-06-17T08:10
:02Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2017-06-17T08:10:02Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2017-06-17T08:10:02Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T08:10:02Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoD
eadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 08:40:02.392305       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 08:40:32.394968       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 08:40:52.688694       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 10:19:47.886026       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful confli
ct (firstResourceVersion: ""2206191"", currentResourceVersion: ""2206223""):
 diff1={""metadata"":{""resourceVersion"":""2206223""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T10:19:44Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2017-06-17T10:19:44Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2017-06-17T10:19:44Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2017-06-17T10:19:44Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T10:19:47Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 10:20:03.868248       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful conflict (firstResourceVersion: ""2206223"", currentResourceVersion: ""2206276""):
 diff1={""metadata"":{""resourceVersion"":""2206276""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T10:19:57Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2017-06-17T10:19:57Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2017-06-17T10:19:57Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2017-06-17T10:19:57Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T10:19:58Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 10:27:14.278874       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful conflict (firstResourceVersion: ""2207596"", currentResourceVersion: ""2207627""):
 diff1={""metadata"":{""resourceVersion"":""2207627""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T10:27:09Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2017-06-17T10:27:09Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2017-06-17T10:27:09Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2017-06-17T10:27:09Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T10:27:11Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 10:49:49.999965       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 11:04:58.654339       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful conflict (firstResourceVersion: ""2214802"", currentResourceVersion: ""2214843""):
 diff1={""metadata"":{""resourceVersion"":""2214843""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:04:56Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2017-06-17T11:04:56Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2017-06-17T11:04:56Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2017-06-17T11:04:56Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:04:58Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 11:13:25.349869       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 11:13:27.448321       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful conflict (firstResourceVersion: ""2216376"", currentResourceVersion: ""2216452""):
 diff1={""metadata"":{""resourceVersion"":""2216452""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:13:26Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2017-06-17T11:13:26Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2017-06-17T11:13:26Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2017-06-17T11:13:26Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:13:25Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 11:27:54.707593       7 manager.go:160] failed to update node conditions: etcdserver: request timed out
E0617 11:28:03.676434       7 manager.go:160] failed to update node conditions: etcdserver: request timed out
E0617 11:28:11.687756       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful conflict (firstResourceVersion: ""2219181"", currentResourceVersion: ""2219220""):
 diff1={""metadata"":{""resourceVersion"":""2219220""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:27:56Z"",""type"":""KernelDeadlock""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:28:07Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 11:28:11.687756       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful conflict (firstResourceVersion: ""2219181"", currentResourceVersion: ""2219220""):
 diff1={""metadata"":{""resourceVersion"":""2219220""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:27:56Z"",""type"":""KernelDeadlock""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:28:07Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 11:35:22.216998       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful conflict (firstResourceVersion: ""2220568"", currentResourceVersion: ""2220603""):
 diff1={""metadata"":{""resourceVersion"":""2220603""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:35:12Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2017-06-17T11:35:12Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2017-06-17T11:35:12Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2017-06-17T11:35:12Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:35:21Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 11:35:39.515617       7 manager.go:160] failed to update node conditions: etcdserver: request timed out
E0617 11:36:44.370694       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful conflict (firstResourceVersion: ""2220826"", currentResourceVersion: ""2220882""):
 diff1={""metadata"":{""resourceVersion"":""2220882""},""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:36:40Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2017-06-17T11:36:40Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2017-06-17T11:36:40Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2017-06-17T11:36:40Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T11:36:43Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 11:43:04.549531       7 manager.go:160] failed to update node conditions: etcdserver: request timed out
E0617 11:52:29.011584       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 11:55:55.741212       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost

[...SNIP...]

E0617 20:48:43.758632       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 20:50:08.649485       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 20:50:36.742814       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 20:51:06.745291       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:51:18.310623       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 20:51:25.507013       7 manager.go:160] failed to update node conditions: etcdserver: request timed out
E0617 20:51:35.682252       7 manager.go:160] failed to update node conditions: etcdserver: request timed out
E0617 20:51:40.731491       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful confli
ct (firstResourceVersion: ""2329653"", currentResourceVersion: ""2329819""):
 diff1={""metadata"":{""resourceVersion"":""2329819""},""status"":{""conditions"":[{""lastTransitionTime"":""2017-06-17T20:49:33Z"",""message"":""Kubelet stopped posting node status."",""reason"":""N
odeStatusUnknown"",""status"":""Unknown"",""type"":""DiskPressure""},{""lastTransitionTime"":""2017-06-17T20:49:33Z"",""message"":""Kubelet stopped posting node status."",""reason"":""NodeStatusUnkn
own"",""status"":""Unknown"",""type"":""MemoryPressure""},{""lastTransitionTime"":""2017-06-17T20:49:33Z"",""message"":""Kubelet stopped posting node status."",""reason"":""NodeStatusUnknown"",""statu
s"":""Unknown"",""type"":""OutOfDisk""},{""lastTransitionTime"":""2017-06-17T20:49:33Z"",""message"":""Kubelet stopped posting node status."",""reason"":""NodeStatusUnknown"",""status"":""Unknown"",""ty
pe"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T20:51:39Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoD
eadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 20:52:05.903239       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 20:52:35.941987       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:53:05.990698       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:53:35.993274       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:54:05.995654       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:54:35.998026       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:55:06.009227       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:55:36.011974       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:56:01.937423       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 20:56:12.384973       7 manager.go:160] failed to update node conditions: etcdserver: request timed out
E0617 20:56:42.390952       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:57:12.393356       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:57:42.395943       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:58:12.405949       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 20:58:22.411682       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 20:58:33.305720       7 manager.go:160] failed to update node conditions: etcdserver: request timed out, possibly due to connection lost
E0617 20:58:34.805954       7 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-123-1-104.ec2.internal"": there is a meaningful confli
ct (firstResourceVersion: ""2330223"", currentResourceVersion: ""2330380""):
 diff1={""metadata"":{""resourceVersion"":""2330380""},""status"":{""conditions"":[{""lastTransitionTime"":""2017-06-17T20:52:28Z"",""message"":""Kubelet stopped posting node status."",""reason"":""N
odeStatusUnknown"",""status"":""Unknown"",""type"":""DiskPressure""},{""lastTransitionTime"":""2017-06-17T20:52:28Z"",""message"":""Kubelet stopped posting node status."",""reason"":""NodeStatusUnkn
own"",""status"":""Unknown"",""type"":""MemoryPressure""},{""lastTransitionTime"":""2017-06-17T20:52:28Z"",""message"":""Kubelet stopped posting node status."",""reason"":""NodeStatusUnknown"",""status"":""Unknown"",""type"":""OutOfDisk""},{""lastTransitionTime"":""2017-06-17T20:52:28Z"",""message"":""Kubelet stopped posting node status."",""reason"":""NodeStatusUnknown"",""status"":""Unknown"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2017-06-17T20:58:33Z"",""lastTransitionTime"":""2017-06-17T05:08:14Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
E0617 20:59:13.391379       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration

[...SNIP...]
[... MESSAGE REPEATS APPROX EVERY ~1/2 SECOND ...]
[...SNIP...]

E0617 21:22:43.790298       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:23:13.792892       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:23:43.795469       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:24:13.798194       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:24:43.800739       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:25:13.803335       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:25:43.805904       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:26:13.808357       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:26:43.810985       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:27:13.813560       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:27:43.816150       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:28:13.886384       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:28:43.889035       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:29:13.891666       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:29:43.894478       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:30:13.897056       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:30:43.922019       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:31:13.947162       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:31:43.949919       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:32:13.952527       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:32:43.955028       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:33:13.957574       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:33:43.960186       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:34:13.962807       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:34:43.965335       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:35:13.967982       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration
E0617 21:35:43.971220       7 manager.go:160] failed to update node conditions: Timeout: request did not complete within allowed duration

[...SNIP...]
[... MESSAGE REPEATS APPROX EVERY ~1/2 SECOND ...]
```

[npd-0.4.0]: https://github.com/kubernetes/kubernetes/blob/1d3979190cca558b39fdb40e9409b95891be6b3b/cluster/addons/node-problem-detector/npd.yaml",closed,False,2017-06-17 21:50:30,2018-06-15 02:00:25
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/119,https://api.github.com/repos/kubernetes/node-problem-detector/issues/119,Clarify the limitation of log matching pattern.,"@ajitak

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/119)
<!-- Reviewable:end -->
",closed,True,2017-06-21 01:11:50,2017-06-27 16:15:27
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/issues/120,https://api.github.com/repos/kubernetes/node-problem-detector/issues/120,A systemd service for test.,"After introducing a new pattern, we usually need to verify it.

However, it's hard to trigger a real problem for a given service, so we usually have to inject log to verify it.

It's hard to inject log into a given systemd service. We should have a test systemd service which does nothing but generating specified log. We could let NPD parse the log of the test service to verify the newly introduced pattern.",closed,False,2017-06-21 01:14:24,2018-02-27 09:14:52
node-problem-detector,ReSearchITEng,https://github.com/kubernetes/node-problem-detector/issues/121,https://api.github.com/repos/kubernetes/node-problem-detector/issues/121,does not create automatically /var/log/journal,"Hi,

1. great initiative. Here is my experience on RHEL/CentOS 7

2. Apparently node-problem-detector is not able to work with the /run/log/journal/ (unfortunately) - correct? If so, it seems like a big limitation, no? 

3. The issue is that even if cannot do that, it also fails to start, as it depends on the existence of ""/var/log/journal"".  Knowing that it's not able to use /run/log/journal, is it reasonable to expect that it will create ""/var/log/journal"" on each node?

4. ""/var/log/kern.log"" is missing on my OS. I guess it's not in use any longer or it should be configured?

Here are the logs:

```
I0621 14:32:46.173199       7 log_watchers.go:40] Use log watcher of plugin ""journald""
I0621 14:32:46.186164       7 log_monitor.go:72] Finish parsing log monitor config file: {WatcherConfig:{Plugin:filelog PluginConfig:map[timestamp:^.{15} message:kernel: \[.*\] (.*) timestampFormat:Jan _2 15:04:05] LogPath:/var/log/kern.log Lookback:5m} BufferSize:10 Source:kernel-monitor DefaultConditions:[{Type:KernelDeadlock Status:false Transition:0001-01-01 00:00:00 +0000 UTC Reason:KernelHasNoDeadlock Message:kernel has no deadlock}] Rules:[{Type:temporary Condition: Reason:OOMKilling Pattern:Kill process \d+ (.+) score \d+ or sacrifice child\nKilled process \d+ (.+) total-vm:\d+kB, anon-rss:\d+kB, file-rss:\d+kB} {Type:temporary Condition: Reason:TaskHung Pattern:task \S+:\w+ blocked for more than \w+ seconds\.} {Type:temporary Condition: Reason:UnregisterNetDevice Pattern:unregister_netdevice: waiting for \w+ to become free. Usage count = \d+} {Type:temporary Condition: Reason:KernelOops Pattern:BUG: unable to handle kernel NULL pointer dereference at .*} {Type:temporary Condition: Reason:KernelOops Pattern:divide error: 0000 \[#\d+\] SMP} {Type:permanent Condition:KernelDeadlock Reason:AUFSUmountHung Pattern:task umount\.aufs:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:DockerHung Pattern:task docker:\w+ blocked for more than \w+ seconds\.}]}
I0621 14:32:46.186204       7 log_watchers.go:40] Use log watcher of plugin ""filelog""
I0621 14:32:46.186926       7 log_monitor.go:81] Start log monitor
E0621 14:32:46.186945       7 problem_detector.go:65] Failed to start log monitor ""/config/kernel-monitor.json"": failed to stat the log path ""/var/log/journal"": stat /var/log/journal: no such file or directory
I0621 14:32:46.186951       7 log_monitor.go:81] Start log monitor
E0621 14:32:46.186958       7 problem_detector.go:65] Failed to start log monitor ""/config/kernel-monitor-filelog.json"": failed to stat the file ""/var/log/kern.log"": stat /var/log/kern.log: no such file or directory
F0621 14:32:46.186964       7 node_problem_detector.go:88] Problem detector failed with error: no log montior is successfully setup
```
",closed,False,2017-06-21 11:58:27,2018-08-25 22:26:16
node-problem-detector,ajitak,https://github.com/kubernetes/node-problem-detector/pull/122,https://api.github.com/repos/kubernetes/node-problem-detector/issues/122,Add a new rule which catches docker image corruption (cherrypick #117 to v0.4),"It will address https://github.com/kubernetes/kubernetes/issues/47219

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/122)
<!-- Reviewable:end -->
",closed,True,2017-06-21 21:37:32,2017-06-21 23:33:51
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/123,https://api.github.com/repos/kubernetes/node-problem-detector/issues/123,Now gcloud enforce `--` before docker args.,"Fixes
```
gcloud docker push gcr.io/google_containers/node-problem-detector:v0.4.1
ERROR: gcloud crashed (ArgumentError): argument DOCKER_ARGS: unrecognized args: push gcr.io/google_containers/node-problem-detector:v0.4.1
The '--' argument must be specified between gcloud specific args on the left and DOCKER_ARGS on the right.
```
Signed-off-by: Random-Liu <lantaol@google.com>

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/123)
<!-- Reviewable:end -->
",closed,True,2017-06-21 23:15:30,2017-06-27 16:15:21
node-problem-detector,shyamjvs,https://github.com/kubernetes/node-problem-detector/issues/124,https://api.github.com/repos/kubernetes/node-problem-detector/issues/124,NPD sending too many node-status updates in scale tests,"We ran 4k-node scalability test and observed that fluentd gets oom-killed frequently.
However, npd seems to send too many patch node-status requests (~3k qps out of ~11k total qps) due to it.
Ref: https://github.com/kubernetes/kubernetes/issues/47344#issuecomment-311018976 https://github.com/kubernetes/kubernetes/issues/47865#issuecomment-310861405

Why is it reporting oom events when kubelet seems to do that?
Can we make npd report ooms for just system processes and let kubelet alone take care of k8s containers (by modifying npd's 'OOMKilling' rule)?

cc @Random-Liu @gmarek @kubernetes/sig-node-bugs ",closed,False,2017-06-26 12:27:44,2018-06-29 13:39:44
node-problem-detector,therc,https://github.com/kubernetes/node-problem-detector/issues/125,https://api.github.com/repos/kubernetes/node-problem-detector/issues/125,OOMKilling not triggering with recent kernels,"With Linux 4.9, and perhaps earlier versions, there's a trailing

`, shmem-rss:\\d+kB`

Should it be optional in the built-in regex, are admins expect to edit their configuration, or something else?",closed,False,2017-06-28 16:23:46,2018-02-28 23:52:51
node-problem-detector,WIZARD-CXY,https://github.com/kubernetes/node-problem-detector/issues/126,https://api.github.com/repos/kubernetes/node-problem-detector/issues/126,what about the  remedy system?,"Now I have deployed the node-problem-detector in my cluster and defines some new node condition type, but only to find that I can do nothing about the new condition like KernelHasDeadlock, I read the doc talking about remedy system, I think it is a controller like nodeController kind of thing, I wonder is there any plan about this remedy system and If I want to define my own, where can I start? Thanks in advance.",closed,False,2017-07-06 01:36:27,2017-08-23 10:45:03
node-problem-detector,CaoShuFeng,https://github.com/kubernetes/node-problem-detector/issues/127,https://api.github.com/repos/kubernetes/node-problem-detector/issues/127,coredump detector for pods,"I commit this issue to ask whether ""coredump detector"" is in the scope of this add-on.
I wrote a proposal about coredump in kubernetes cluster:
https://github.com/kubernetes/community/pull/658

But smarterclayton said we should not add a new api for coredump feature, we only need a privileged pod running on the node to capture this and report it via a different channel.

So, I am planning to add a coredump detector to each node, and upload coredump files to a separate storage backend. So that users can download coredump files from the storage backend.
This is the architecture:
![1](https://user-images.githubusercontent.com/20855209/28048966-d25dbfc2-6626-11e7-92eb-559e8ab22c59.png)

Is this the right place for the ""coredump detector""?",closed,False,2017-07-11 02:52:08,2017-07-11 09:54:14
node-problem-detector,ajitak,https://github.com/kubernetes/node-problem-detector/pull/128,https://api.github.com/repos/kubernetes/node-problem-detector/issues/128,change NPD port as there is a port collision with kube-proxy,"See https://github.com/kubernetes/kubernetes/issues/49263 for more context.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/128)
<!-- Reviewable:end -->
",closed,True,2017-07-20 17:39:40,2017-07-27 14:46:29
node-problem-detector,asifdxtreme,https://github.com/kubernetes/node-problem-detector/pull/129,https://api.github.com/repos/kubernetes/node-problem-detector/issues/129,Add Build Status and GoReportCard badge,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/129)
<!-- Reviewable:end -->
",closed,True,2017-08-04 10:33:02,2017-08-04 17:10:50
node-problem-detector,fisherxu,https://github.com/kubernetes/node-problem-detector/pull/130,https://api.github.com/repos/kubernetes/node-problem-detector/issues/130,fix typo,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/130)
<!-- Reviewable:end -->
",closed,True,2017-08-04 17:07:16,2017-08-04 17:18:27
node-problem-detector,mbssaiakhil,https://github.com/kubernetes/node-problem-detector/pull/131,https://api.github.com/repos/kubernetes/node-problem-detector/issues/131,Fixed Typo in README of node-problem-detecter,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/131)
<!-- Reviewable:end -->
",closed,True,2017-08-06 03:32:56,2017-08-07 17:05:53
node-problem-detector,asifdxtreme,https://github.com/kubernetes/node-problem-detector/pull/132,https://api.github.com/repos/kubernetes/node-problem-detector/issues/132,Split Test and Build in Different Stages,"Split UT and Build Stages in different jobs of travis CI so that it's easy for user to traces what went wrong in CI

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/132)
<!-- Reviewable:end -->
",closed,True,2017-08-07 08:44:31,2017-08-08 01:04:03
node-problem-detector,mhristof,https://github.com/kubernetes/node-problem-detector/pull/133,https://api.github.com/repos/kubernetes/node-problem-detector/issues/133,given the daemonset a name in the docs,"otherwise, `kubectl create -f` was complaining with

```
The DaemonSet ""node-problem-detector"" is invalid: spec.template.metadata.labels: Invalid value: map[string]string(nil): `selector` does not match template `labels`
```

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/133)
<!-- Reviewable:end -->
",closed,True,2017-08-10 16:10:22,2018-03-09 22:22:34
node-problem-detector,CaoShuFeng,https://github.com/kubernetes/node-problem-detector/pull/134,https://api.github.com/repos/kubernetes/node-problem-detector/issues/134,return an error when error happens in SetConditions(),"@Random-Liu

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/134)
<!-- Reviewable:end -->
",closed,True,2017-08-23 09:57:50,2017-08-23 18:01:25
node-problem-detector,nlamirault,https://github.com/kubernetes/node-problem-detector/issues/135,https://api.github.com/repos/kubernetes/node-problem-detector/issues/135,ARM version,Is a multi-architecture version (ARM) possible?,closed,False,2017-08-25 08:14:03,2018-03-10 13:37:34
node-problem-detector,CaoShuFeng,https://github.com/kubernetes/node-problem-detector/pull/136,https://api.github.com/repos/kubernetes/node-problem-detector/issues/136,introduce boilerplate test,"This change is introduced because I found that some copyritht headers are different from each other.

This change add boilerplate test copyed from:
https://github.com/kubernetes/kubernetes/tree/master/hack/boilerplate

I found that most file in node-problem-detector has a different
copyright format from kubernetes/kubernetes:

node-problem-detector:
```
Copyright 2017 The Kubernetes Authors All rights reserved.
```

kubernetes:
```
Copyright 2017 The Kubernetes Authors.
```

@Random-Liu should we use the same copyright with kubernetes?

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/136)
<!-- Reviewable:end -->
",closed,True,2017-08-29 03:25:49,2018-03-11 23:10:33
node-problem-detector,cstuntz,https://github.com/kubernetes/node-problem-detector/pull/137,https://api.github.com/repos/kubernetes/node-problem-detector/issues/137,Adding in prometheus exporter for events and conditions,"**Reason for Pull**

Prometheus is becoming more and more popular to track metrics in Kubernetes clusters, and it is a very lightweight method of tracking node problems across a cluster. This PR adds in basic prometheus exporter for all events and conditions tracked by node problem detector, which makes tracking problems much easier.

**Three Main Changes**

- Add new file that is used to contain the map of counters.
-- This lets us dynamically add Prometheus counters from events and conditions as they appear, instead of needing to hard-code all of them.
- Using the Fetch method from the containers file, I add new counters or increment existing ones as the events are processed in problem_detector.go.
- Update the reason names so there aren't any duplicates.
-- Prometheus can't have any duplicate metric names registered.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/137)
<!-- Reviewable:end -->
",closed,True,2017-08-29 21:09:59,2018-04-10 20:46:25
node-problem-detector,jhorwit2,https://github.com/kubernetes/node-problem-detector/issues/138,https://api.github.com/repos/kubernetes/node-problem-detector/issues/138,port 10256 conflicts with kube-proxy,"it seems that a healthcheck/debug port was added [here](https://github.com/kubernetes/node-problem-detector/pull/83); however, it conflicts with the [kube-proxy healthz port](https://github.com/kubernetes/kubernetes/blob/01e961b380eea90d111156b8e99cc9621c5cdb5f/pkg/apis/componentconfig/v1alpha1/types.go#L110). They should be different. ",closed,False,2017-08-31 21:36:29,2017-09-04 13:59:57
node-problem-detector,CaoShuFeng,https://github.com/kubernetes/node-problem-detector/pull/139,https://api.github.com/repos/kubernetes/node-problem-detector/issues/139,update README,"update description about `Build Image`. When running `make`, it will
not push image to registry.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/139)
<!-- Reviewable:end -->
",closed,True,2017-09-19 07:06:38,2017-09-22 17:38:56
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/140,https://api.github.com/repos/kubernetes/node-problem-detector/issues/140,update golang to 1.8 and 1.9,"This PR will update travisCI golang version from `1.6` and `1.7` to `1.8` and `1.9`. Since golang `1.9` has been released for a while.

/cc @Random-Liu

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/140)
<!-- Reviewable:end -->
",closed,True,2017-10-11 05:26:45,2017-10-11 06:26:00
node-problem-detector,Cplo,https://github.com/kubernetes/node-problem-detector/issues/141,https://api.github.com/repos/kubernetes/node-problem-detector/issues/141,"No create function found for plugin ""journald""","hi, I used the master branch to build a new node-problem-detector（go build -o node-problem-detector）, then 
I try to run，but some error occored
```
node-problem-detector --apiserver-override=http://172.252.8.91:8080?inClusterConfig=false --system-log-monitors=/etc/docker-monitor.json --port=10257 --hostname-override=172.252.8.90 --logtostderr  --v 10
I1013 19:32:56.656118   26418 log_monitor.go:72] Finish parsing log monitor config file: {WatcherConfig:{Plugin:journald PluginConfig:map[source:docker] LogPath:/var/log/journal Lookback:5m} BufferSize:10 Source:licence-monitor DefaultConditions:[{Type:LicenceExecceed Status:false Transition:0001-01-01 00:00:00 +0000 UTC Reason:LicenceRequestExceedsLimit Message:licence expired}] Rules:[{Type:temporary Condition:LicenceExecceed Reason:LicenceExecceed Pattern:licence expired}]}
F1013 19:32:56.663221   26418 log_watchers.go:38] No create function found for plugin ""journald""
```
I want to know why, thank you☺",closed,False,2017-10-13 11:39:05,2018-01-15 13:35:26
node-problem-detector,Spellchaser,https://github.com/kubernetes/node-problem-detector/issues/142,https://api.github.com/repos/kubernetes/node-problem-detector/issues/142,Vague|Nonexistent Build Documentation,"As a fresh new user, I tried to get and build this project. 

My first issue is that documentation on where/how one is supposed to download this projects is nonexistent. 

These DON'T work
```
go get github.com/kubernetes/node-problem-detector
go get k8s.io/node-problem-detector
```

Also git cloning this project anywhere except $GOPATH/src/k8sio/|$GOROOT/src/k8sio/ fails to build 
```
[user@box node-problem-detector]$ make
CGO_ENABLED=1 GOOS=linux go build -o bin/node-problem-detector \
     -ldflags '-X k8s.io/node-problem-detector/pkg/version.version=v0.4.0-22-gffc7909' \
     -tags journald cmd/node_problem_detector.go
cmd/node_problem_detector.go:29:2: cannot find package ""k8s.io/node-problem-detector/cmd/options"" in any of:
        /home/user/go/src/node-problem-detector/vendor/k8s.io/node-problem-detector/cmd/options (vendor tree)
        /usr/lib/golang/src/k8s.io/node-problem-detector/cmd/options (from $GOROOT)
        /home/user/go/src/k8s.io/node-problem-detector/cmd/options (from $GOPATH)
cmd/node_problem_detector.go:30:2: cannot find package ""k8s.io/node-problem-detector/pkg/problemclient"" in any of:
        /home/user/go/src/node-problem-detector/vendor/k8s.io/node-problem-detector/pkg/problemclient (vendor tree)
        /usr/lib/golang/src/k8s.io/node-problem-detector/pkg/problemclient (from $GOROOT)
        /home/user/go/src/k8s.io/node-problem-detector/pkg/problemclient (from $GOPATH)
cmd/node_problem_detector.go:31:2: cannot find package ""k8s.io/node-problem-detector/pkg/problemdetector"" in any of:
        /home/user/go/src/node-problem-detector/vendor/k8s.io/node-problem-detector/pkg/problemdetector (vendor tree)
        /usr/lib/golang/src/k8s.io/node-problem-detector/pkg/problemdetector (from $GOROOT)
        /home/user/go/src/k8s.io/node-problem-detector/pkg/problemdetector (from $GOPATH)
cmd/node_problem_detector.go:32:2: cannot find package ""k8s.io/node-problem-detector/pkg/systemlogmonitor"" in any of:
        /home/user/go/src/node-problem-detector/vendor/k8s.io/node-problem-detector/pkg/systemlogmonitor (vendor tree)
        /usr/lib/golang/src/k8s.io/node-problem-detector/pkg/systemlogmonitor (from $GOROOT)
        /home/user/go/src/k8s.io/node-problem-detector/pkg/systemlogmonitor (from $GOPATH)
cmd/node_problem_detector.go:33:2: cannot find package ""k8s.io/node-problem-detector/pkg/version"" in any of:
        /home/user/go/src/node-problem-detector/vendor/k8s.io/node-problem-detector/pkg/version (vendor tree)
        /usr/lib/golang/src/k8s.io/node-problem-detector/pkg/version (from $GOROOT)
        /home/user/go/src/k8s.io/node-problem-detector/pkg/version (from $GOPATH)
make: *** [bin/node-problem-detector] Error 1

```

After locating where it expects to be cloned, making fails anyway. I guess this expects ubuntu, but this also ins't documented anywhere.
```
[user@box node-problem-detector]$ make
CGO_ENABLED=1 GOOS=linux go build -o bin/node-problem-detector \
     -ldflags '-X k8s.io/node-problem-detector/pkg/version.version=v0.4.0-22-gffc7909' \
     -tags journald cmd/node_problem_detector.go
# k8s.io/node-problem-detector/vendor/github.com/coreos/go-systemd/sdjournal
vendor/github.com/coreos/go-systemd/sdjournal/journal.go:27:33: fatal error: systemd/sd-journal.h: No such file or directory
 // #include <systemd/sd-journal.h>
                                 ^
compilation terminated.
make: *** [bin/node-problem-detector] Error 2
```",closed,False,2017-10-13 14:49:22,2018-06-04 02:14:56
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/issues/143,https://api.github.com/repos/kubernetes/node-problem-detector/issues/143,Bump to kubernetes 1.8 tracking issue,"This is a tracking issue for bump node-problem-detector kubernetes to 1.8.

- [ ] bump heapster kubernetes to 1.8 [heapster #1844](https://github.com/kubernetes/heapster/pull/1844). 
- [ ] bump node-problem-detector kubernetes to 1.8.

After bump kubernetes to 1.8, i will prepare the PR for adding support for customized problem detectors. :) 

Sorry for the delay.

/cc @Random-Liu ",closed,False,2017-10-15 16:07:36,2018-08-20 17:57:16
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/144,https://api.github.com/repos/kubernetes/node-problem-detector/issues/144,add setup readme,"partially fix #142 

This PR adds description about how to get the node-problem-detector repo and systemd build dependency.

Note: This PR should be merged only after [k8s.io #85](https://github.com/kubernetes/k8s.io/pull/85) is merged as description in this PR has `go get k8s.io/node-problem-detector` which is only valid after [k8s.io #85](https://github.com/kubernetes/k8s.io/pull/85) has been merged.
@Spellchaser

/cc @Random-Liu

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/144)
<!-- Reviewable:end -->
",closed,True,2017-10-16 03:27:32,2018-06-04 02:21:03
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/145,https://api.github.com/repos/kubernetes/node-problem-detector/issues/145,add custom problem detector plugin,"This PR will add custom plugin problem detector interface to node-problem-detector.

Proposal: [https://docs.google.com/document/d/1jK_5YloSYtboj-DtfjmYKxfNnUxCAvohLnsH5aGCAYQ/edit#](https://docs.google.com/document/d/1jK_5YloSYtboj-DtfjmYKxfNnUxCAvohLnsH5aGCAYQ/edit#)

Major changes:
* move `ApplyDefaultConfiguration` for `MonitorConfig` from function to method. [Ref]()
* move `ValidateRules ` for `MonitorConfig` from function to method for. [Ref](https://github.com/kubernetes/node-problem-detector/pull/145/files#diff-ae34550839d4dd21f93fdb2193f0798f)
* move `k8s.io/node-problem-detector/pkg/systemlogmonitor/util` to `k8s.io/node-problem-detector/pkg/util/tomb`. [Ref](https://github.com/kubernetes/node-problem-detector/pull/145/files#diff-fbf4ff688541f0b6d6243de3c50245cb)
* move `Rule` type for system log config to pkg `k8s.io/node-problem-detector/pkg/types`. [Ref](https://github.com/kubernetes/node-problem-detector/pull/145/files#diff-34dfcb9f9c18e85cfeb1525b94b82572)
* move `LogMonitor` interface to `k8s.io/node-problem-detector/pkg/types` and rename it to `Monitor`. [Ref](https://github.com/kubernetes/node-problem-detector/pull/145/files#diff-6d10bcbde757a47bc5d87ac0510e0215)
* Code clean. [Ref](https://github.com/kubernetes/node-problem-detector/pull/145/files#diff-6d10bcbde757a47bc5d87ac0510e0215L147)


<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/145)
<!-- Reviewable:end -->
",closed,True,2017-10-18 08:45:22,2017-11-22 18:53:06
node-problem-detector,mkumatag,https://github.com/kubernetes/node-problem-detector/pull/146,https://api.github.com/repos/kubernetes/node-problem-detector/issues/146,Add multiarch support,"This will enable to build `node-problem-detector` project for multiarch like amd64, arm, arm64 and ppc64le.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/146)
<!-- Reviewable:end -->
",closed,True,2017-10-20 10:10:20,2019-01-22 17:36:28
node-problem-detector,ananas-hebo,https://github.com/kubernetes/node-problem-detector/issues/147,https://api.github.com/repos/kubernetes/node-problem-detector/issues/147, flag provided but not defined: -apiserver-override?,"My k8s without rbac.
So  i use like : [docs usage](https://github.com/kubernetes/node-problem-detector#usage)

But i got a ""flag provided but not defined: -apiserver-override"" error.

yaml file:
```
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: node-problem-detector
spec:
  template:
    metadata:
      labels:
        app: node-problem-detector
    spec:
      containers:
      - name: node-problem-detector
        command:
        - /node-problem-detector
        - --apiserver-override=http://centos-master2.ans:8080?inClusterConfig=false
        - --logtostderr
        - --kernel-monitor=/config/kernel-monitor.json
        image: registry-ans.chaoxing.com/google_containers/node-problem-detector:v0.2
        imagePullPolicy: Always
        securityContext:
          privileged: true
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        volumeMounts:
        - name: log
          mountPath: /var/log
          readOnly: true
        - name: kmsg
          mountPath: /dev/kmsg
          readOnly: true
        # Make sure node problem detector is in the same timezone
        # with the host.
        - name: localtime
          mountPath: /etc/localtime
          readOnly: true
        #- name: config
        #  mountPath: /config
        #  readOnly: true
      volumes:
      - name: log
        # Config `log` to your system log directory
        hostPath:
          path: /var/log/
      - name: kmsg
        hostPath:
          path: /dev/kmsg
      - name: localtime
        hostPath:
          path: /etc/localtime
      #- name: config
      #  configMap:
      #    name: node-problem-detector-config
```",closed,False,2017-10-30 10:44:53,2018-03-29 13:04:53
node-problem-detector,Starefossen,https://github.com/kubernetes/node-problem-detector/pull/148,https://api.github.com/repos/kubernetes/node-problem-detector/issues/148,Update Docker Image tag reference to v0.4.1,"Upgrade references to the node-problem-detector Docker Image to latest available version on gcr.io v0.4.1.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/148)
<!-- Reviewable:end -->
",closed,True,2017-11-07 11:47:24,2018-02-14 11:46:56
node-problem-detector,kevtaylor,https://github.com/kubernetes/node-problem-detector/issues/149,https://api.github.com/repos/kubernetes/node-problem-detector/issues/149,Failed to update node conditions,"I have deployed the node-problem-detector daemonset with an RBAC enabled cluster. I have a serviceaccount and clusterrolebinding to system:node-problem-detector

```
failed to update node conditions: the server does not allow access to the requested resource (patch nodes ip-10-29-26-187.us-west-2.compute.internal)
```
k8 version is v1.7.7+coreos.0
npd is 0.4.1 

I am not sure if this is RBAC related or not, but if I use kubectl to patch the node using the serviceaccount token, it is not allowed through that role, but even if I create a rolebinding to cluster-admin, I can then perform the kubectl patch but the error message in the node log doesn't go away
",closed,False,2017-11-16 13:24:02,2018-04-21 01:24:51
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/issues/150,https://api.github.com/repos/kubernetes/node-problem-detector/issues/150,Tracking issue for adding custom plugins,"This is a tracking issue for what we still need to do after adding custom plugins in #145 .

- [x] Complete `pkg/custompluginmonitor/README.md`
- [x] Support per-rule interval
- [x] Emitting events when Condition changes.  
* Added in https://github.com/kubernetes/node-problem-detector/pull/151
- [x] support unknown condition status. 
* Added in https://github.com/kubernetes/node-problem-detector/pull/151",closed,False,2017-11-19 13:18:24,2018-07-09 06:18:19
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/151,https://api.github.com/repos/kubernetes/node-problem-detector/issues/151,Improve cpm,"Based on https://github.com/kubernetes/node-problem-detector/pull/145.

Addressed part of https://github.com/kubernetes/node-problem-detector/issues/150.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/151)
<!-- Reviewable:end -->
",closed,True,2017-11-21 23:29:56,2018-06-22 08:10:06
node-problem-detector,rramkumar1,https://github.com/kubernetes/node-problem-detector/pull/152,https://api.github.com/repos/kubernetes/node-problem-detector/issues/152,Network problem script,"Added a script that checks for important networking events. This script will be run by the custom plugin monitor. 

Currently the script only checks if the conntrack table is full and sends a temporary event if it is. More checks will be added in the future.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/152)
<!-- Reviewable:end -->
",closed,True,2017-11-27 19:37:16,2018-06-14 21:01:37
node-problem-detector,spiffxp,https://github.com/kubernetes/node-problem-detector/pull/153,https://api.github.com/repos/kubernetes/node-problem-detector/issues/153,Add code-of-conduct.md,"Refer to kubernetes/community as authoritative source for code of conduct

ref: kubernetes/community#1527

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/153)
<!-- Reviewable:end -->
",closed,True,2017-12-20 18:39:42,2018-01-03 02:18:22
node-problem-detector,thockin,https://github.com/kubernetes/node-problem-detector/pull/154,https://api.github.com/repos/kubernetes/node-problem-detector/issues/154,Convert registry to k8s.gcr.io,"This PR was auto-generated.  Please apply human expertise to review for correctness.

Followup to https://github.com/kubernetes/kubernetes/pull/54174

xref https://github.com/kubernetes/release/issues/281

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/154)
<!-- Reviewable:end -->
",closed,True,2017-12-22 18:01:11,2018-01-03 01:16:55
node-problem-detector,r4j4h,https://github.com/kubernetes/node-problem-detector/pull/155,https://api.github.com/repos/kubernetes/node-problem-detector/issues/155,Changed unknown status echo to exit in network_problem custom plugin,"My comment was eaten by github in #152 and wanted to raise attention incase this was meant to be an exit instead of an echo, otherwise feel free to close!

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/155)
<!-- Reviewable:end -->
",closed,True,2018-01-05 18:26:25,2018-04-20 00:09:00
node-problem-detector,thockin,https://github.com/kubernetes/node-problem-detector/pull/156,https://api.github.com/repos/kubernetes/node-problem-detector/issues/156,Pushes go to staging-k8s.gcr.io,"Context: https://github.com/kubernetes/kubernetes/pull/57824

xref kubernetes/release#281

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/156)
<!-- Reviewable:end -->
",closed,True,2018-01-17 22:23:45,2018-02-08 22:38:47
node-problem-detector,hangyan,https://github.com/kubernetes/node-problem-detector/pull/157,https://api.github.com/repos/kubernetes/node-problem-detector/issues/157,Fix broken ConfigMap usage links,"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/157)
<!-- Reviewable:end -->
",closed,True,2018-01-22 14:35:16,2018-02-05 02:27:26
node-problem-detector,cimomo,https://github.com/kubernetes/node-problem-detector/pull/158,https://api.github.com/repos/kubernetes/node-problem-detector/issues/158,Use camelCase instead of snake_case per Golang convention,"Use camelCase instead of snake_case per Golang convention.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/158)
<!-- Reviewable:end -->
",closed,True,2018-01-22 15:44:39,2018-02-23 06:18:18
node-problem-detector,Rcluoyi,https://github.com/kubernetes/node-problem-detector/pull/159,https://api.github.com/repos/kubernetes/node-problem-detector/issues/159,Fix doc links for ConfigMap in README,"This link seems to have been updated.
Signed-off-by: renchao <renchao@chinacloud.com.cn>

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/159)
<!-- Reviewable:end -->
",closed,True,2018-01-25 07:17:53,2018-02-23 06:24:41
node-problem-detector,filbranden,https://github.com/kubernetes/node-problem-detector/pull/160,https://api.github.com/repos/kubernetes/node-problem-detector/issues/160,Use debian-base image from kubernetes repository as base for NPD.,"This image is based on Debian Stretch (9) which has a recent version of systemd libraries that includes all necessary compression algorithms.

I propose using that image unconditionally (instead of Alpine when journald is disabled) since the image size with debian-base is not that much of a concern anymore.

FTR, I didn't really test this...

This is analogous to a change we're pushing to fluentd-gcp container in GoogleCloudPlatform/k8s-stackdriver#101

@Random-Liu 

Cheers,
Filipe

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/160)
<!-- Reviewable:end -->
",closed,True,2018-02-14 21:06:09,2018-02-26 22:06:04
node-problem-detector,gkGaneshR,https://github.com/kubernetes/node-problem-detector/issues/161,https://api.github.com/repos/kubernetes/node-problem-detector/issues/161,Unit testing for SetNodeNameOrDie in package cmd/options,"Is this a BUG REPORT or FEATURE REQUEST?:

Uncomment only one, leave it on its own line:
/kind feature

What happened:
Unit testing for node-problem-detector/cmd/options is yet to be done.

What you expected to happen:
Under package cmd/options, the testing for SetNodeNameOrDie need to decide Nodename based on environment variable ""NODE_NAME"" or hostname or hostnameoverride variable. 

How to reproduce it (as minimally and precisely as possible):
Run the go test

Anything else we need to know?:
The test needs to run with admin privileges since it mocks by changing hostname

Environment:

Kubernetes version (use kubectl version):
Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release): Ubuntu:16.04
Kernel (e.g. uname -a): x86_64 GNU/Linux
Install tools: go
Others:

@kubernetes/sig-testing

",closed,False,2018-03-02 15:33:12,2018-03-09 10:15:50
node-problem-detector,gkGaneshR,https://github.com/kubernetes/node-problem-detector/pull/162,https://api.github.com/repos/kubernetes/node-problem-detector/issues/162,Unit testing for SetNodeNameOrDie in package cmd/options,"1. Why is this change necessary ?
fixes: kubernetes/node-problem-detector#161

2. How does this change address the issue ?
Under package cmd/options, the testing for SetNodeNameOrDie need
to decide Nodename based on environment variable ""NODE_NAME"" or
hostname or hostnameoverride variable.

3. How to verify this change ?
Run ""go test"" with admin privilege i.e., ""make test""

Signed-off-by: gkGaneshR <gkganesh126@gmail.com>

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/node-problem-detector/162)
<!-- Reviewable:end -->
",closed,True,2018-03-02 15:39:40,2018-03-09 10:15:50
node-problem-detector,gkGaneshR,https://github.com/kubernetes/node-problem-detector/issues/163,https://api.github.com/repos/kubernetes/node-problem-detector/issues/163,Include cmd/options in makefile test to run go tests,"Is this a BUG REPORT or FEATURE REQUEST?:

Uncomment only one, leave it on its own line:
/kind feature

What happened:
Makefile's test must include cmd/options package for unit testing.

What you expected to happen:
Unit testing for node-problem-detector/cmd/options is yet to be done and the corresponding package path needs to be included in makefile test.

How to reproduce it (as minimally and precisely as possible):
Run the go test : ""make test""

Anything else we need to know?:
The test needs to run with admin privileges since it mocks by changing hostname

Environment:

Kubernetes version (use kubectl version):
Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release): Ubuntu:16.04
Kernel (e.g. uname -a): x86_64 GNU/Linux
Install tools: go
Others:

@kubernetes/sig-testing",closed,False,2018-03-02 16:23:11,2018-04-12 11:40:50
node-problem-detector,jsenon,https://github.com/kubernetes/node-problem-detector/pull/164,https://api.github.com/repos/kubernetes/node-problem-detector/issues/164,Update Readme,Update yaml file with new daemonset definition introduce by kubernetes 1.9,closed,True,2018-03-22 15:05:52,2018-05-02 11:54:42
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/issues/165,https://api.github.com/repos/kubernetes/node-problem-detector/issues/165,Enable k8s-ci-robot for node-problem-detector,"Enable k8s-ci-robot for node-problem-detector.
* add OWNERS file to the root of node-problem-detector
* enable node-problem-detector in test-infra just like what we do for kube-state-metrics in https://github.com/kubernetes/test-infra/pull/7027",closed,False,2018-03-25 11:11:44,2018-03-26 20:04:02
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/166,https://api.github.com/repos/kubernetes/node-problem-detector/issues/166,add OWNERS,"Fix #165 

This PR adds OWNERS to node-problem-detector.

/cc @Random-Liu @dchen1107 ",closed,True,2018-03-25 14:02:35,2018-04-16 06:01:04
node-problem-detector,weinliu,https://github.com/kubernetes/node-problem-detector/issues/167,https://api.github.com/repos/kubernetes/node-problem-detector/issues/167,LOG fails got parsed,"Hi @joelsmith ,

I used Log Generator to get logs for kernel-monitor-filelog.json
https://github.com/kubernetes/node-problem-detector/tree/master/test/kernel_log_generator

```
export PROBLEM=problems/unregister_netdevice
export LOG_PATH=/var/log/kern.log
./generator.sh 
```

But it fails to get parsed on pods (logs)
```
...
W0403 06:50:45.624741       1 log_watcher.go:127] Unable to parse line: ""Jan _2 15:04:05 kernel: unregister_netdevice: waiting for lo to become free. Usage count = 1\n"", failed to parse timestamp ""Jan _2 15:04:05"": parsing time ""Jan _2 15:04:05"" as ""Jan _2 15:04:05"": cannot parse ""_2 15:04:05"" as ""_2""
W0403 06:50:45.624752       1 log_watcher.go:127] Unable to parse line: ""Jan _2 15:04:05 kernel: unregister_netdevice: waiting for lo to become free. Usage count = 1\n"", failed to parse timestamp ""Jan _2 15:04:05"": parsing time ""Jan _2 15:04:05"" as ""Jan _2 15:04:05"": cannot parse ""_2 15:04:05"" as ""_2""
...
```

Any suggestions for generating the log?

The same issue for docker logs
docker-monitor-filelog.json

```
cat /var/log/docker.log
...
W0404 03:35:35.173973       1 log_watcher.go:127] Unable to parse line: ""time=\""(2008-01-02T15:04:05.999999999-07:00)\"" failed to register layer: rename /var/lib/docker/image/overlay/layerdb/tmp/layer-558969056 /var/lib/docker/image/overlay/layerdb/sha256/3d979a98dc302b351aec55cf58f1b2bd86880c57df9553fb4fe0f94087f04e0b: directory not empty\n"", failed to parse timestamp ""(2008-01-02T15:04:05.999999999-07:00)"": parsing time ""(2008-01-02T15:04:05.999999999-07:00)"" as ""2006-01-02T15:04:05.999999999-07:00"": cannot parse ""(2008-01-02T15:04:05.999999999-07:00)"" as ""2006""
...
```",closed,False,2018-04-04 13:18:52,2018-04-12 05:31:38
node-problem-detector,thockin,https://github.com/kubernetes/node-problem-detector/pull/168,https://api.github.com/repos/kubernetes/node-problem-detector/issues/168,Pass 2: k8s GCR vanity URL,,closed,True,2018-04-06 15:19:21,2018-04-16 05:49:58
node-problem-detector,zhangxiaoyu-zidif,https://github.com/kubernetes/node-problem-detector/pull/169,https://api.github.com/repos/kubernetes/node-problem-detector/issues/169,update example,the original ds example could not be created.,closed,True,2018-04-08 08:03:33,2018-04-13 00:54:41
node-problem-detector,weinliu,https://github.com/kubernetes/node-problem-detector/issues/170,https://api.github.com/repos/kubernetes/node-problem-detector/issues/170,How can I verify if NPD is working?,"Hi @Random-Liu ,and @joelsmith , I've got the same question with @xiuli2 , how can I verify the NPD is working?
I got docker-monitor.json and kernel-monitor enabled: `--system-log-monitors=/etc/npd/kernel-monitor.json,/etc/npd/docker-monitor.json`

Then I generated a fake journald log generated:

```
# echo ""BUG: unable to handle kernel NULL pointer dereference at 12345"" |systemd-cat
# echo ""Error trying v2 registry: failed to register layer: rename /var/lib/docker/image/aufs/layerdb/tmp/layer-632022140 /var/lib/docker/image/aufs/layerdb/sha256/011b303988d241a4ae28a6b82b0d8262751ef02910f0ae2265cb637504b72e36: directory not empty"" | systemd-cat
```

But  I can not see any output for NPD from `kubectl get events | grep Node` or `describe node NodeName`

So is kubectl logs

```
# kubectl logs node-problem-detector-72c4w
I0409 01:24:20.074715       1 log_monitor.go:63] Finish parsing log monitor config file: {WatcherConfig:{Plugin:journald PluginConfig:map[source:kernel] LogPath:/host/log/journal Lookback:5m} BufferSize:10 Source:kernel-monitor DefaultConditions:[{Type:KernelDeadlock Status:false Transition:0001-01-01 00:00:00 +0000 UTC Reason:KernelHasNoDeadlock Message:kernel has no deadlock}] Rules:[{Type:temporary Condition: Reason:OOMKilling Pattern:Kill process \d+ (.+) score \d+ or sacrifice child\nKilled process \d+ (.+) total-vm:\d+kB, anon-rss:\d+kB, file-rss:\d+kB} {Type:temporary Condition: Reason:TaskHung Pattern:task \S+:\w+ blocked for more than \w+ seconds\.} {Type:temporary Condition: Reason:UnregisterNetDevice Pattern:unregister_netdevice: waiting for \w+ to become free. Usage count = \d+} {Type:temporary Condition: Reason:KernelOops Pattern:BUG: unable to handle kernel NULL pointer dereference at .*} {Type:temporary Condition: Reason:KernelOops Pattern:divide error: 0000 \[#\d+\] SMP} {Type:permanent Condition:KernelDeadlock Reason:AUFSUmountHung Pattern:task umount\.aufs:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:DockerHung Pattern:task docker:\w+ blocked for more than \w+ seconds\.}]}
I0409 01:24:20.074881       1 log_watchers.go:40] Use log watcher of plugin ""journald""
I0409 01:24:20.074999       1 log_monitor.go:63] Finish parsing log monitor config file: {WatcherConfig:{Plugin:journald PluginConfig:map[source:docker] LogPath:/host/log/journal Lookback:5m} BufferSize:10 Source:docker-monitor DefaultConditions:[] Rules:[{Type:temporary Condition: Reason:CorruptDockerImage Pattern:Error trying v2 registry: failed to register layer: rename /var/lib/docker/image/(.+) /var/lib/docker/image/(.+): directory not empty.*}]}
I0409 01:24:20.075013       1 log_watchers.go:40] Use log watcher of plugin ""journald""
I0409 01:24:20.075872       1 log_monitor.go:72] Start log monitor
I0409 01:24:20.097735       1 log_watcher.go:69] Start watching journald
I0409 01:24:20.097789       1 log_monitor.go:72] Start log monitor
I0409 01:24:20.099870       1 log_monitor.go:163] Initialize condition generated: [{Type:KernelDeadlock Status:false Transition:2018-04-09 01:24:20.099830399 -0400 EDT m=+0.043253068 Reason:KernelHasNoDeadlock Message:kernel has no deadlock}]
I0409 01:24:20.114934       1 log_watcher.go:69] Start watching journald
I0409 01:24:20.115008       1 problem_detector.go:73] Problem detector started
I0409 01:24:20.117235       1 log_monitor.go:163] Initialize condition generated: []
```


Thanks a lot.",closed,False,2018-04-09 09:20:48,2018-04-12 02:40:33
node-problem-detector,erstaples,https://github.com/kubernetes/node-problem-detector/issues/171,https://api.github.com/repos/kubernetes/node-problem-detector/issues/171,v0.4.1 image does not support `--custom-plugin-monitors` flag,"I'm using the `gcr.io/google_containers/node-problem-detector:v0.4.1` image.

When I deploy the node problem detector with a custom plugin monitor, I get the following error:

```
unknown flag: --custom-plugin-monitors
Usage of /node-problem-detector:
      --address=""127.0.0.1"": The address to bind the node problem detector server.
      --alsologtostderr[=false]: log to standard error as well as files
      --apiserver-override="""": Custom URI used to connect to Kubernetes ApiServer
      --hostname-override="""": Custom node name used to override hostname
      --log_backtrace_at=:0: when logging hits line file:N, emit a stack trace
      --log_dir="""": If non-empty, write log files in this directory
      --logtostderr[=false]: log to standard error instead of files
      --port=10256: The port to bind the node problem detector server. Use 0 to disable.
      --stderrthreshold=2: logs at or above this threshold go to stderr
      --system-log-monitors=[]: List of paths to system log monitor config files, comma separated.
      --v=0: log level for V logs
      --version[=false]: Print version information and quit
      --vmodule=: comma-separated list of pattern=N settings for file-filtered logging
```

This is an available option according to the README.md: 

```
--custom-plugin-monitors: List of paths to custom plugin monitor config files, comma separated, e.g. config/custom-plugin-monitor.json. Node problem detector will start a separate custom plugin monitor for each configuration. You can use different custom plugin monitors to monitor different node problems.
```",closed,False,2018-04-18 21:03:11,2018-05-04 01:52:31
node-problem-detector,calvinyv,https://github.com/kubernetes/node-problem-detector/issues/172,https://api.github.com/repos/kubernetes/node-problem-detector/issues/172,Wrong parm --kernel-monitor in NPD 0.4.1 sample yaml,"I update the image to NPD 0.4.1 in sample yaml file deployment/node-problem-detector.yaml, I'm using  k8s 1.10.2, when apply this file, I got below error:  

The DaemonSet ""node-problem-detector"" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{""app"":""node-problem-detector""}: `selector` does not match template `labels`  

Then I edit that yaml, and make below change:  
    metadata:  
      labels:  
        name: node-problem-detector  

Then I apply this file successfully and container fail to create because of below error:  

  Warning  FailedMount            37s               kubelet, i-362pv0q9  Unable to mount volumes for pod ""node-problem-detector-fgn8l_default(b4ee45bd-53f6-11e8-8cf6-5254ad805bab)"": timeout expired waiting for volumes to attach or mount for pod ""default""/""node-problem-detector-fgn8l"". list of unmounted volumes=[config]. list of unattached volumes=[log kmsg localtime config default-token-vxprc]
  Warning  FailedMount            29s (x9 over 2m)  kubelet, i-362pv0q9  MountVolume.SetUp failed for volume ""config"" : configmaps ""node-problem-detector-config"" not found

so what should the content of node-problem-detector-config be?  ",closed,False,2018-05-10 02:16:45,2018-10-25 10:31:14
node-problem-detector,danielzhanghl,https://github.com/kubernetes/node-problem-detector/issues/173,https://api.github.com/repos/kubernetes/node-problem-detector/issues/173,new release,"in the release tab, that is released this on Jun 22, 2017, but seems the code changes after that.
 could you load a latest one? thanks.",closed,False,2018-05-11 03:23:42,2018-05-11 05:56:19
node-problem-detector,MoShitrit,https://github.com/kubernetes/node-problem-detector/issues/174,https://api.github.com/repos/kubernetes/node-problem-detector/issues/174,Is there an option to read `journald` without a file? (basically tail `journalctl` output),"We would like to start using `npd` in our environment. However, the way our logging architecture is designed- we're not storing any logs locally. We have a `syslog-ng` client that tails `journalctl` output and forwards it to our `syslog` server without storing any local copies. Hence it seems like `npd` can't really read all these events. I tried to go through the documentation but didn't find anything that could be relevant to our scenario. Did I miss something? Is there a way to accomplish what I'm looking to get?
Thanks!",closed,False,2018-05-15 15:03:16,2018-05-30 13:27:10
node-problem-detector,jessfraz,https://github.com/kubernetes/node-problem-detector/issues/175,https://api.github.com/repos/kubernetes/node-problem-detector/issues/175,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",closed,False,2018-05-24 14:37:18,2018-05-29 02:32:32
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/176,https://api.github.com/repos/kubernetes/node-problem-detector/issues/176,add SECURITY_CONTACTS,"Fixes #175

This PR will adds a SECURITY_CONTACTS file under the root directory.
",closed,True,2018-05-28 07:49:59,2018-05-29 05:07:44
node-problem-detector,rlguarino,https://github.com/kubernetes/node-problem-detector/pull/177,https://api.github.com/repos/kubernetes/node-problem-detector/issues/177,fix typo in daemonset labelSelector,"The label on the pod is `app: node-problem-detector` while the
label in the `labelSelector` was `name: node-problem-detector`.
This CL updates the `labelSelector` to use `app` rather than `name`.",closed,True,2018-05-30 00:22:35,2018-05-30 03:52:54
node-problem-detector,ahakanbaba,https://github.com/kubernetes/node-problem-detector/issues/178,https://api.github.com/repos/kubernetes/node-problem-detector/issues/178,Can an owner generate a release please ,"We would like to start using node-problem-detector. 
If you could cut a release, using this would be much simpler. ",closed,False,2018-06-12 19:00:43,2018-07-07 01:09:47
node-problem-detector,ahakanbaba,https://github.com/kubernetes/node-problem-detector/issues/179,https://api.github.com/repos/kubernetes/node-problem-detector/issues/179,"make ./bin/node-problem-detector expects a git repository, hinders builds from releases","The `make ./bin/node-problem-detector` [command](https://github.com/kubernetes/node-problem-detector/blob/2f915ecf29dcb3370611f51bbd8297c9180109bc/Makefile#L80)  uses the VERSION [variable](https://github.com/kubernetes/node-problem-detector/blob/2f915ecf29dcb3370611f51bbd8297c9180109bc/Makefile#L22)

Version variable executes a git command that expects a repo. 

If you try to build after extracting the tarfile from a release, the code will not have a .git directory and  build fails with 

```+ make ./bin/node-problem-detector
fatal: Not a git repository (or any of the parent directories): .git
```

One could use TAG in the make ./bin/node-problem-detector rule or one could make VERSION optional too with 

`VERSION?=$(shell git describe --tags --dirty)`",closed,False,2018-06-12 20:55:42,2018-08-22 01:25:23
node-problem-detector,dashpole,https://github.com/kubernetes/node-problem-detector/pull/180,https://api.github.com/repos/kubernetes/node-problem-detector/issues/180,Add log-counter plugin written in go,"This PR adds a new binary to the node-problem-detector repository: log-counter.
The new binary uses the kmsg log watcher to get kmsg log events, and checks the number of events that occurred.
The binary accepts command-line flags for the pattern, count, and period of time to look back.
It sets the condition `NodeRecreationRequired` when it sees the unregister_netdevice error 3 times in 20 minutes, and runs every 10 minutes.

/assign @Random-Liu ",closed,True,2018-06-18 19:37:06,2018-06-21 16:46:35
node-problem-detector,CaoShuFeng,https://github.com/kubernetes/node-problem-detector/issues/181,https://api.github.com/repos/kubernetes/node-problem-detector/issues/181,invalid user agent set when accessing kube-apiserver,"When start node-problem-detector with --v=10.
We can see such logs:
```
curl -k -v -XPATCH  -H ""Accept: application/json, */*"" -H ""Content-Type: application/strategic-merge-patch+json"" -H ""User-Agent: node-problem-detector/v1.4.0 (linux/amd64) kubernetes/$Format"" http://172.16.29.130:8080/api/v1/nodes/node2/status
```

The user agent is set to node-problem-detector/v1.4.0 (linux/amd64) kubernetes/$Format

The $Format seems should be replaced with the `git rev-parse HEAD`",closed,False,2018-06-20 09:35:19,2018-06-26 01:15:52
node-problem-detector,AdamDang,https://github.com/kubernetes/node-problem-detector/pull/182,https://api.github.com/repos/kubernetes/node-problem-detector/issues/182,Typo fix: encounts->encounters,Line 34: encounts->encounters,closed,True,2018-06-22 06:05:07,2018-06-22 06:51:09
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/183,https://api.github.com/repos/kubernetes/node-problem-detector/issues/183,adjust client-go User-Agent,"Fix #181 

This PR will customize client-go, node-problem-detector uses, User-Agent to `node-problem-detector/VERSION`.",closed,True,2018-06-24 02:41:43,2018-06-26 02:12:28
node-problem-detector,jesseshieh,https://github.com/kubernetes/node-problem-detector/issues/184,https://api.github.com/repos/kubernetes/node-problem-detector/issues/184,No events detected on vanilla EKS cluster (Amazon Linux),"Hi, first of all thanks for this great project! I use npd on a k8s cluster on gce with no problems, but I recently decided to give aws eks a try and can't seem to get npd to report any events.  I'm sure I've just got it installed wrong somehow, but any help would be greatly appreciated! I also hope this helps anyone else trying to run npd on eks.

Here is what I did
1. follow the  [getting started guide](https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html) to create a new eks cluster. note that the worker nodes are running Amazon Linux. [[1]](#snip-1)
2. install npd with `kubectl apply -f npd.yaml` [[2]](#snip-2)
3. exec into a pod and use a bunch of memory to generate an OOMKilled event. note that the journal shows the event correctly [[3]](#snip-3)
4. note that there is nothing in the node-problem-detector.log except for periodic status checks [[4]](#snip-4)

Things I checked
1. the journal files are indeed in `/var/log/journal` and the `Kill process` messages are indeed in there.
2. i scanned the rule regex by hand and it seems to match except for the leading `Memory cgroup out of memory: ` but I assume that's ok
3. `/var/log` is mounted into the npd pod.

I've run out of ideas short of forking npd and doing some debug logging, which I'm hoping to avoid.

<a name=""snip-1"">[1]</a>
```
[ec2-user@ip-192-168-227-171 etc]$ cat /etc/os-release 
NAME=""Amazon Linux""
VERSION=""2 (2017.12)""
ID=""amzn""
ID_LIKE=""centos rhel fedora""
VERSION_ID=""2""
PRETTY_NAME=""Amazon Linux 2 (2017.12) LTS Release Candidate""
ANSI_COLOR=""0;33""
CPE_NAME=""cpe:2.3:o:amazon:amazon_linux:2""
HOME_URL=""https://amazonlinux.com/""

```

<a name=""snip-2"">[2]</a>
```
$ cat npd.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: node-problem-detector
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: npd-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node-problem-detector
subjects:
- kind: ServiceAccount
  name: node-problem-detector
  namespace: kube-system
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: npd-v0.4.1
  namespace: kube-system
  labels:
    k8s-app: node-problem-detector
    version: v0.4.1
spec:
  template:
    metadata:
      labels:
        k8s-app: node-problem-detector
        version: v0.4.1
    spec:
      containers:
      - name: node-problem-detector
        image: k8s.gcr.io/node-problem-detector:v0.4.1
        command:
        - ""/bin/sh""
        - ""-c""
        # Pass both config to support both journald and syslog.
        - ""exec /node-problem-detector --v=99 --logtostderr --system-log-monitors=/config/kernel-monitor.json,/config/docker-monitor.json >>/var/log/node-problem-detector.log 2>&1""
        securityContext:
          privileged: true
        resources:
          limits:
            cpu: ""200m""
            memory: ""100Mi""
          requests:
            cpu: ""20m""
            memory: ""20Mi""
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        volumeMounts:
        - name: log
          mountPath: /var/log
        - name: localtime
          mountPath: /etc/localtime
          readOnly: true
      volumes:
      - name: log
        hostPath:
          path: /var/log/
      - name: localtime
        hostPath:
          path: /etc/localtime
          type: ""FileOrCreate""
      serviceAccountName: node-problem-detector
      tolerations:
      - operator: ""Exists""
        effect: ""NoExecute""
      - key: ""CriticalAddonsOnly""
 ```

<a name=""snip-3"">[3]</a>
```
[ec2-user@ip-192-168-227-171 ~]$ journalctl -f
Jun 25 03:34:05 ip-192-168-227-171.ec2.internal kernel: Memory cgroup out of memory: Kill process 14200 (beam.smp) score 1847 or sacrifice child
Jun 25 03:34:05 ip-192-168-227-171.ec2.internal kernel: Killed process 14200 (beam.smp) total-vm:4711536kB, anon-rss:447836kB, file-rss:3460kB, shmem-rss:0kB
Jun 25 03:34:05 ip-192-168-227-171.ec2.internal kernel: oom_reaper: reaped process 14200 (beam.smp), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB
```

<a name=""snip-4"">[4]</a>
```
[ec2-user@ip-192-168-227-171 ~]$ tail -f /var/log/node-problem-detector.log
...
I0625 03:32:36.102456       1 round_trippers.go:318] PATCH https://10.100.0.1:443/api/v1/nodes/ip-192-168-227-171.ec2.internal/status 200 OK in 15 milliseconds
...
```",closed,False,2018-06-25 05:05:35,2019-04-01 17:01:46
node-problem-detector,chadswen,https://github.com/kubernetes/node-problem-detector/issues/185,https://api.github.com/repos/kubernetes/node-problem-detector/issues/185,gcr Image Tag v0.5.0 is missing bash and other binaries,"With the latest release it looks like the base image may have changed and some binaries are no longer packaged. Most custom plugin scripts depend on bash, and as a result those are no longer working for me.

[Image tag v0.5.0 details on gcr](https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/node-problem-detector@sha256:ba9daf428e462bfb0e7078819193ec7d30e2d1fb38aa0cf8ba1187f9bba217dd/details?tab=info)

Refs #160 ",closed,False,2018-06-29 22:21:49,2018-07-16 03:03:56
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/186,https://api.github.com/repos/kubernetes/node-problem-detector/issues/186,make VERSION value more flexible,"Fix #179 

This PR will make the `VERSION` value in Makefile more flexible and the build output less error info log.
It will first check whether this is a git repo by checking whether a `.git` directory exists or not. If it is a git repo then `VERSION` value will be `git describe --tags --dirty`, otherwise it will use a default value `UNKNOWN`",closed,True,2018-07-08 04:15:52,2018-08-22 02:09:44
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/187,https://api.github.com/repos/kubernetes/node-problem-detector/issues/187,bump kubernetes to 1.9,"Fix #143 

This PR will bump Kubernetes version to 1.9.

/cc @Random-Liu ",closed,True,2018-07-09 06:06:46,2018-08-21 02:34:15
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/188,https://api.github.com/repos/kubernetes/node-problem-detector/issues/188,support go 1.10 in travis ci,"Since Golang 1.10 has been released for a long time, it should be tested in travis ci. :)

/cc @Random-Liu ",closed,True,2018-07-09 08:16:21,2018-08-21 02:40:12
node-problem-detector,chadswen,https://github.com/kubernetes/node-problem-detector/pull/189,https://api.github.com/repos/kubernetes/node-problem-detector/issues/189,Add bash to Dockerfile for custom-plugin-monitors,"Utilizes the clean-install script from the debian-base image

Fixes #185",closed,True,2018-07-13 23:05:35,2018-07-17 20:59:36
node-problem-detector,aojea,https://github.com/kubernetes/node-problem-detector/issues/190,https://api.github.com/repos/kubernetes/node-problem-detector/issues/190,Missing CONTRIBUTING.md file,"All K8s subrepositories should have a CONTRIBUTING.md file, which at the minimum should point to https://github.com/kubernetes/community/blob/master/contributors/guide/README.md. Care should be taken that all information is in sync with the contributor guide.

Subrepositories may also have contributing guidelines specific to that repository. They should be explicitly documented and explained in the CONTRIBUTING.md

Ref:  https://github.com/kubernetes/community/issues/1832",closed,False,2018-07-16 09:23:36,2018-08-20 17:56:03
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/191,https://api.github.com/repos/kubernetes/node-problem-detector/issues/191,add CONTRIBUTING.md,"Fix #190 

This PR adds CONTRIBUTING.md to node-problem-detector.
",closed,True,2018-07-17 10:35:21,2018-08-21 02:34:04
node-problem-detector,poothia,https://github.com/kubernetes/node-problem-detector/pull/192,https://api.github.com/repos/kubernetes/node-problem-detector/issues/192,Adding Missing CONTRIBUTING.md,"It adds Contribution Guideline document for node-problem-detector
fixes #190 ",closed,True,2018-07-17 10:36:52,2018-07-17 10:41:45
node-problem-detector,poothia,https://github.com/kubernetes/node-problem-detector/pull/193,https://api.github.com/repos/kubernetes/node-problem-detector/issues/193,Adding missing CONTRIBUTING.md,"It adds contribution guidelines for node-problem-detector
fixes #190 ",closed,True,2018-07-17 10:44:31,2018-07-19 06:15:30
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/194,https://api.github.com/repos/kubernetes/node-problem-detector/issues/194,Add required package for Debian in README and ignore entire /bin/ dir,Add required package for Debian in README and ignore entire /bin/ dir,closed,True,2018-07-19 21:18:35,2018-07-23 22:24:31
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/195,https://api.github.com/repos/kubernetes/node-problem-detector/issues/195,Include some concrete usage examples in the README,"Add a ""Try It Out"" section to include some concrete usage examples in the README file.",closed,True,2018-07-24 00:08:53,2018-07-31 16:40:40
node-problem-detector,jheidbrink,https://github.com/kubernetes/node-problem-detector/pull/196,https://api.github.com/repos/kubernetes/node-problem-detector/issues/196,Adapt OOMKilling pattern to current kernels,"[commit eca56ff9](https://github.com/torvalds/linux/commit/eca56ff906bdd0239485e8b47154a6e73dd9a2f3#diff-c14acc69885a24f354bc607686800e23) in the Linux Kernel repository added a column for shared memory to the `Killed process` message. As the patterns in the `kernel-monitor(-filelog).json` have to match full lines, they don't match the OOMKilling messages of current Kernels anymore.

This PR fixes this by simply adding a '.*' to the pattern.",closed,True,2018-07-30 12:13:33,2018-08-01 01:51:39
node-problem-detector,max-rocket-internet,https://github.com/kubernetes/node-problem-detector/issues/197,https://api.github.com/repos/kubernetes/node-problem-detector/issues/197,failed to update node conditions: Operation cannot be fulfilled on nodes xxxx: there is a meaningful conflict,"What does this last log message mean?

```
I0820 16:04:30.139107       1 log_monitor.go:72] Finish parsing log monitor config file: {WatcherConfig:{Plugin:journald PluginConfig:map[source:kernel] LogPath:/var/log/journal Lookback:5m} BufferSize:10 Source:kernel-monitor DefaultConditions:[{Type:KernelDeadlock Status:false Transition:0001-01-01 00:00:00 +0000 UTC Reason:KernelHasNoDeadlock Message:kernel has no deadlock}] Rules:[{Type:temporary Condition: Reason:OOMKilling Pattern:Kill process \d+ (.+) score \d+ or sacrifice child\nKilled process \d+ (.+) total-vm:\d+kB, anon-rss:\d+kB, file-rss:\d+kB} {Type:temporary Condition: Reason:TaskHung Pattern:task \S+:\w+ blocked for more than \w+ seconds\.} {Type:temporary Condition: Reason:UnregisterNetDevice Pattern:unregister_netdevice: waiting for \w+ to become free. Usage count = \d+} {Type:temporary Condition: Reason:KernelOops Pattern:BUG: unable to handle kernel NULL pointer dereference at .*} {Type:temporary Condition: Reason:KernelOops Pattern:divide error: 0000 \[#\d+\] SMP} {Type:permanent Condition:KernelDeadlock Reason:AUFSUmountHung Pattern:task umount\.aufs:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:DockerHung Pattern:task docker:\w+ blocked for more than \w+ seconds\.}]}
I0820 16:04:30.139198       1 log_watchers.go:40] Use log watcher of plugin ""journald""
I0820 16:04:30.139356       1 log_monitor.go:72] Finish parsing log monitor config file: {WatcherConfig:{Plugin:journald PluginConfig:map[source:docker] LogPath:/var/log/journal Lookback:5m} BufferSize:10 Source:docker-monitor DefaultConditions:[] Rules:[{Type:temporary Condition: Reason:CorruptDockerImage Pattern:Error trying v2 registry: failed to register layer: rename /var/lib/docker/image/(.+) /var/lib/docker/image/(.+): directory not empty.*}]}
I0820 16:04:30.139381       1 log_watchers.go:40] Use log watcher of plugin ""journald""
I0820 16:04:30.140094       1 log_monitor.go:81] Start log monitor
I0820 16:04:30.141668       1 log_watcher.go:69] Start watching journald
I0820 16:04:30.141785       1 log_monitor.go:81] Start log monitor
I0820 16:04:30.142137       1 log_watcher.go:69] Start watching journald
I0820 16:04:30.142157       1 problem_detector.go:74] Problem detector started
I0820 16:04:30.142193       1 log_monitor.go:173] Initialize condition generated: [{Type:KernelDeadlock Status:false Transition:2018-08-20 16:04:30.142166997 +0000 UTC Reason:KernelHasNoDeadlock Message:kernel has no deadlock}]
I0820 16:04:30.153874       1 log_monitor.go:173] Initialize condition generated: []
E0821 01:47:31.191057       1 manager.go:160] failed to update node conditions: Operation cannot be fulfilled on nodes ""ip-10-0-23-130.ec2.internal"": there is a meaningful conflict (firstResourceVersion: ""2229935"", currentResourceVersion: ""2229956""):
 diff1={""metadata"":{""resourceVersion"":""2229956""},""status"":{""$setElementOrder/conditions"":[{""type"":""KernelDeadlock""},{""type"":""OutOfDisk""},{""type"":""MemoryPressure""},{""type"":""DiskPressure""},{""type"":""PIDPressure""},{""type"":""Ready""}],""conditions"":[{""lastHeartbeatTime"":""2018-08-21T01:47:31Z"",""type"":""OutOfDisk""},{""lastHeartbeatTime"":""2018-08-21T01:47:31Z"",""type"":""MemoryPressure""},{""lastHeartbeatTime"":""2018-08-21T01:47:31Z"",""type"":""DiskPressure""},{""lastHeartbeatTime"":""2018-08-21T01:47:31Z"",""type"":""PIDPressure""},{""lastHeartbeatTime"":""2018-08-21T01:47:31Z"",""type"":""Ready""}]}}
, diff2={""status"":{""conditions"":[{""lastHeartbeatTime"":""2018-08-21T01:47:31Z"",""lastTransitionTime"":""2018-08-20T16:04:30Z"",""message"":""kernel has no deadlock"",""reason"":""KernelHasNoDeadlock"",""status"":""False"",""type"":""KernelDeadlock""}]}}
```",closed,False,2018-08-21 09:04:06,2018-08-21 09:05:56
node-problem-detector,negz,https://github.com/kubernetes/node-problem-detector/issues/198,https://api.github.com/repos/kubernetes/node-problem-detector/issues/198,Built in custom problem detectors require bash,"Both built in custom problem detector scripts use `/bin/bash`, which is not installed as of version 0.5.0. This results in issues like:

```
Events:
  Type     Reason         Age                From                                                                            Message
  ----     ------         ----               ----                                                                            -------
  Warning  ConntrackFull  1s (x31 over 15m)  network-custom-plugin-monitor, tfk-negz-ing-xcb9.c.REDACTED.internal  Error in running plugin. Please check the error log
```

```
 kubectl --kubeconfig=/Users/negz/tfk-negz.kcfg -n kube-system logs node-problem-detector-vl8tw|head -n1
E0822 23:37:13.175202       1 plugin.go:115] Error in running plugin ""./config/plugin/network_problem.sh"": error - fork/exec ./config/plugin/network_problem.sh: no such file or directory. output - """"
```",closed,False,2018-08-22 23:53:00,2018-08-28 01:56:44
node-problem-detector,negz,https://github.com/kubernetes/node-problem-detector/issues/199,https://api.github.com/repos/kubernetes/node-problem-detector/issues/199,FYI - Simple remedy system designed for use with NPD,"Hello,

I wanted to bring [Draino](https://github.com/negz/draino) to your attention, in case it's useful to others. Draino is a very simple 'remedy' system for permanent problems detected by the Node Problem Detector - it simply cordons and drains nodes exhibiting configurable Node Conditions.

At [Planet](http://planet.com/) we run a small handful of Kubernetes clusters on GCE (not GKE). We have a particular analytics workload that is really good at killing GCE persistent volumes. Without going into too much detail, we see persistent volume related processes (`mkfs.ext4`, `mount`, etc) hanging forever in uninterruptible sleep, preventing the pods wanting to consume said volumes from running. We're working with GCP to resolve this issue, but in the meantime we got tired of manually cordoning and draining affected nodes, so we wrote Draino.

Our remedy system looks like:
1. Detect permanent node problems and set Node Conditions using the Node Problem Detector.
2. Configure Draino to cordon and drain nodes when they exhibit the NPD's `KernelDeadlock` condition, or a variant of `KernelDeadlock` we call `VolumeTaskHung`.
3. Let the Cluster Autoscaler scale down underutilised nodes, including the nodes Draino has drained.

It's worth noting that once the [Descheduler](https://github.com/kubernetes-incubator/descheduler) supports descheduling pods based on taints Draino could be replaced by the Descheduler running in combination with the scheduler's `TaintNodesByCondition` functionality.",closed,False,2018-08-29 00:50:19,2018-09-05 09:22:34
node-problem-detector,aga20,https://github.com/kubernetes/node-problem-detector/issues/200,https://api.github.com/repos/kubernetes/node-problem-detector/issues/200,Condition message doesn't update when the status is same,"Hello,

we want to use the node problem detector  with custom plugins in our env where we have an internally developed alarm handling solution. We will implement a layer between kubernetes an our alarm handling solution which can raise alarm based on conditions and events. 
First we should check the free spaces on the nodes' partitions but we have a lot of node types with another volumes and partions (storage, etc...) so we can't able cover all node with one config (the npd run as a daemon set so the per node config is a little bit hard). We try to figure out a general solution and our final idea is next: we use only one general config and run one script which check the partitions/volume on the nodes. If one partition is full it exit with code 1 and print out the useful information for example /mnt/part 1 full on the node. This is ok when condition's status is change between True/False. But we want to update the condition message if an another partition for example part2 is full but the contdition status didn't change back to false yet (so the part1 is still full). As I see until the status is same you don't update the message [here](https://github.com/kubernetes/node-problem-detector/blob/master/pkg/custompluginmonitor/custom_plugin_monitor.go#L129).
I want to know this is the expected behaviour and what is the reason or we can change this behaviour? For us it can help a lot. 

Thanks!",closed,False,2018-09-03 11:32:55,2018-12-14 01:44:24
node-problem-detector,negz,https://github.com/kubernetes/node-problem-detector/pull/201,https://api.github.com/repos/kubernetes/node-problem-detector/issues/201,Document Draino remedy system,CC @andyxning. Fixes #199.,closed,True,2018-09-05 08:55:16,2018-09-05 09:22:34
node-problem-detector,aga20,https://github.com/kubernetes/node-problem-detector/issues/202,https://api.github.com/repos/kubernetes/node-problem-detector/issues/202,Reason change once and it stay in wrong state with custom plugins,"Hello,

I found something again I think but maybe I follwed wrong way. 
I'm using a custom config based on the ntp example: 

```
{
    ""plugin"": ""custom"",
    ""pluginConfig"": {
        ""invoke_interval"": ""30s"",
        ""timeout"": ""5s"",
        ""max_output_length"": 80,
        ""concurrency"": 3
    },
    ""source"": ""ntp-custom-plugin-monitor"",
    ""conditions"": [
        {
            ""type"": ""CustomProblem"",
            ""reason"": ""CustomIsUp"",
            ""message"": ""Status of the custom service""
        }
    ],
    ""rules"": [
        {
            ""type"": ""permanent"",
            ""condition"": ""CustomProblem"",
            ""reason"": ""CustomIsDown"",
            ""path"": ""/usr/bin/custom.sh"",
            ""timeout"": ""3s""
        }
    ]
}

```
The /usr/bin/custom.sh script is very simple: exit with 0 or 1. 

So when the node problem detector start it set the condition:

```
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  CustomProblem    False   Wed, 05 Sep 2018 14:40:26 +0200   Wed, 05 Sep 2018 14:40:25 +0200   CustomIsUp                   Status of the custom service
  OutOfDisk        False   Wed, 05 Sep 2018 14:40:23 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Wed, 05 Sep 2018 14:40:23 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 05 Sep 2018 14:40:23 +0200   Wed, 05 Sep 2018 13:17:06 +0200   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 05 Sep 2018 14:40:23 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 05 Sep 2018 14:40:23 +0200   Thu, 30 Aug 2018 17:35:04 +0200   KubeletReady                 kubelet is posting ready status

```
After it run the script (what returned with 0 in this case) the Status stay false but the Reason field changed to what I set in the rule section:

```
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  CustomProblem    False   Wed, 05 Sep 2018 14:41:56 +0200   Wed, 05 Sep 2018 14:40:55 +0200   CustomIsDown                 Status of the custom service
  OutOfDisk        False   Wed, 05 Sep 2018 14:42:23 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Wed, 05 Sep 2018 14:42:23 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 05 Sep 2018 14:42:23 +0200   Wed, 05 Sep 2018 13:17:06 +0200   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 05 Sep 2018 14:42:23 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 05 Sep 2018 14:42:23 +0200   Thu, 30 Aug 2018 17:35:04 +0200   KubeletReady                 kubelet is posting ready status
```

So ok, in the next run the script exited with 1. The Status is True, and the Reason still same (this is what I set under the rule):

```
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  CustomProblem    True    Wed, 05 Sep 2018 14:43:56 +0200   Wed, 05 Sep 2018 14:43:55 +0200   CustomIsDown                 Status of the custom service
  OutOfDisk        False   Wed, 05 Sep 2018 14:43:53 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Wed, 05 Sep 2018 14:43:53 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 05 Sep 2018 14:43:53 +0200   Wed, 05 Sep 2018 13:17:06 +0200   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 05 Sep 2018 14:43:53 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 05 Sep 2018 14:43:53 +0200   Thu, 30 Aug 2018 17:35:04 +0200   KubeletReady                 kubelet is posting ready status
```

In the next round the script returned with 0 again and Status changed back to false but the Reason didn't change: 

```
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  CustomProblem    False   Wed, 05 Sep 2018 14:44:26 +0200   Wed, 05 Sep 2018 14:44:25 +0200   CustomIsDown                 Status of the custom service
  OutOfDisk        False   Wed, 05 Sep 2018 14:44:23 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Wed, 05 Sep 2018 14:44:23 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 05 Sep 2018 14:44:23 +0200   Wed, 05 Sep 2018 13:17:06 +0200   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 05 Sep 2018 14:44:23 +0200   Thu, 30 Aug 2018 17:16:47 +0200   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 05 Sep 2018 14:44:23 +0200   Thu, 30 Aug 2018 17:35:04 +0200   KubeletReady                 kubelet is posting ready status
```

As I see you overwrite the condition's rule and maybe the original condition lost and the node problem detector never can't set it again. [https://github.com/kubernetes/node-problem-detector/blob/master/pkg/custompluginmonitor/custom_plugin_monitor.go#L140](https://github.com/kubernetes/node-problem-detector/blob/master/pkg/custompluginmonitor/custom_plugin_monitor.go#L140)
But maybe I missed something.

Thank you!",closed,False,2018-09-05 12:59:46,2019-04-04 15:12:28
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/203,https://api.github.com/repos/kubernetes/node-problem-detector/issues/203,fix custom plugin monitor condition change,"Fix #202 

/cc @aga20",closed,True,2018-09-10 06:17:11,2018-12-10 02:57:53
node-problem-detector,hercynium,https://github.com/kubernetes/node-problem-detector/pull/204,https://api.github.com/repos/kubernetes/node-problem-detector/issues/204,Enable building the NPD in a docker container,"This has several benefits. For example, now I can develop and build linux binaries of the NPD on my mac. Also other devs don't need to make sure they have the systemd headers installed.",closed,True,2018-09-11 20:48:37,2018-09-13 14:14:15
node-problem-detector,zadoryzsolt,https://github.com/kubernetes/node-problem-detector/pull/205,https://api.github.com/repos/kubernetes/node-problem-detector/issues/205,Always update the message field of custom conditions,,closed,True,2018-09-12 08:46:00,2018-09-12 08:47:11
node-problem-detector,max-rocket-internet,https://github.com/kubernetes/node-problem-detector/issues/206,https://api.github.com/repos/kubernetes/node-problem-detector/issues/206,Submitting a helm chart,"I just made a PR to helm charts repo:
https://github.com/helm/charts/pull/7795

Any feedback? I've largely just copied the config from the addons repo.

I also noticed the files in [here](https://github.com/kubernetes/node-problem-detector/tree/master/deployment) are out of date. Should I make a PR to update those too?",closed,False,2018-09-18 14:52:04,2018-11-02 08:49:22
node-problem-detector,du2016,https://github.com/kubernetes/node-problem-detector/issues/207,https://api.github.com/repos/kubernetes/node-problem-detector/issues/207,Unable to parse line xxx with regular expression kernel: \[.*\] (.*),"i have create node-problem-detector daemonset with this file: https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/node-problem-detector/npd.yaml

but the log in /var/log/node-problem-detector.log have this : 

```
W0919 13:28:10.176557       1 log_watcher.go:127] Unable to parse line: ""Sep 19 13:28:09 172 kernel: overlayfs: upper fs needs to support d_type.\n"", no message found in line ""Sep 19 13:28:09 172 kernel: overlayfs: upper fs needs to support d_type."" with regular expression kernel: \[.*\] (.*)
```


my os is centos 7 ,i have config rsyslog with this : 
```
kern.*                                                 /var/log/kern.log
```


the log in /var/log/kern.log like this:
```
Sep 19 10:29:11 172 kernel: overlayfs: upper fs needs to support d_type.
Sep 19 10:29:11 172 kernel: overlayfs: upper fs needs to support d_type.
Sep 19 11:40:24 172 kernel: overlayfs: upper fs needs to support d_type.
Sep 19 11:40:24 172 kernel: overlayfs: upper fs needs to support d_type.
Sep 19 11:40:24 172 kernel: overlayfs: upper fs needs to support d_type.
Sep 19 11:40:25 172 kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Sep 19 11:40:25 172 kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Sep 19 11:41:28 172 kernel: overlayfs: upper fs needs to support d_type.
Sep 19 11:41:28 172 kernel: overlayfs: upper fs needs to support d_type.
Sep 19 11:41:28 172 kernel: overlayfs: upper fs needs to support d_type.
```


is there something wrong",closed,False,2018-09-19 05:32:49,2018-11-13 08:14:03
node-problem-detector,AdamDang,https://github.com/kubernetes/node-problem-detector/pull/208,https://api.github.com/repos/kubernetes/node-problem-detector/issues/208,Typo fix in systemlogmonitor/README.md,Line 78: configurtion->configuration,closed,True,2018-09-25 02:14:14,2018-10-08 02:42:14
node-problem-detector,DazWilkin,https://github.com/kubernetes/node-problem-detector/issues/209,https://api.github.com/repos/kubernetes/node-problem-detector/issues/209,[Please] Fix README.md so that noobs can get to a working deployment quickly,"@max-rocket-internet has been diligent in pointing out some issue with Kubernetes deployment (#206). 

These issues remain unresolved and will trap (as they did me) noob users.

It's unclear whether my hacky fixes are correct but, to get everything working:

### 1. Registry
For the majority of users, `staging-k8s.gcr.io` won't be appropriate, this could perhaps more conveniently be mapped to an environment variable `=${YOUR_REGISTRY}`
https://github.com/kubernetes/node-problem-detector/blob/f85b416b28639357c95a2c4a32b0f89b569ccb81/Makefile#L29

### 2. RBAC
I took @max-rocket-internet Helm sample and reused the `ServiceAccount` and `ClusterRoleBinding` in [`node-problem-detector.yaml`](https://github.com/kubernetes/node-problem-detector/blob/master/deployment/node-problem-detector.yaml):
```
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: node-problem-detector
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: ""true""
    addonmanager.kubernetes.io/mode: Reconcile
...
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: node-problem-detector-binding
  labels:
    kubernetes.io/cluster-service: ""true""
    addonmanager.kubernetes.io/mode: Reconcile
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node-problem-detector
subjects:
- kind: ServiceAccount
  name: node-problem-detector
  namespace: kube-system
...
```
I then also changed the `DaemonSet` to run the containers under the Service Account using:
```
serviceAccountName: node-problem-detector
```

### 3. Command
The current command is incorrect:
```
/node-problem-detector \
--logtostderr \
--kernel-monitor=/config/kernel-monitor.json
```
Again, the Helm chart was helpful:
```
/node-problem-detector \
--logtostderr \
--system-log-monitors=...
```

### 4. Config
The documentation is clear but I found it confusing. `make` bundles the `config` directory into the image and so the Deployment's inclusion of a volume mount is redundant. Rather than be provided by default, this should perhaps be provided as ""If you wish to override the images `/config` directory, you may use a `ConfigMap`...""

### 5. Kubernetes Engine
I'm using Kubernetes Engine and -- by default -- it uses [Container-Optimized OS (COS)](https://cloud.google.com/container-optimized-os). NPD incorrectly reports that COS is not supported. IIUC
* `kernel-monitor-filelog` doesn't (!) work with COS because COS provides no `/var/log/kern.log` so this configuration should be removed to avoid its errors.
* `docker-monitor-filelog` doesn't (!) work with COS because COS provides no `/var/log/docker.log` so this configuration should be removed too.
* There may be other configurations that need tweaking|removing.

Otherwise, NPD works under COS

#### 5. `apply` not `create` (Minor)
In my experience, it's generally better to `kubectl apply` than `kubectl create`.

Rather than:
```
kubectl create -f node-problem-detector.yaml
```
Use:
```
kubectl apply --filename=./node-problem-detector.yaml
```
I use long flags for clarity.

### 6. Makefile
`gcloud docker -- push ...` is deprecated and should be replaced with `docker push ...` once [authenticated](https://cloud.google.com/container-registry/docs/advanced-authentication).
https://github.com/kubernetes/node-problem-detector/blob/f85b416b28639357c95a2c4a32b0f89b569ccb81/Makefile#L115",closed,False,2018-09-28 17:02:30,2018-12-25 06:23:52
node-problem-detector,duffqiu,https://github.com/kubernetes/node-problem-detector/pull/210,https://api.github.com/repos/kubernetes/node-problem-detector/issues/210,Fix error path for plugin,"remove the "".""",closed,True,2018-10-09 05:58:30,2018-10-10 01:44:58
node-problem-detector,Lynzabo,https://github.com/kubernetes/node-problem-detector/issues/211,https://api.github.com/repos/kubernetes/node-problem-detector/issues/211,Restore node condition,"Now I use NPD to add a few conditions for each node.

eg:

```shell
Conditions:
  Type             Status    LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------    -----------------                 ------------------                ------                       -------
  NTPProblem       Unknown   Fri, 12 Oct 2018 14:48:55 +0800   Fri, 12 Oct 2018 14:34:48 +0800   NTPIsDown                    Error in running plugin. Please check the error log
  KernelDeadlock   False     Fri, 12 Oct 2018 14:53:36 +0800   Fri, 12 Oct 2018 14:49:32 +0800   KernelHasNoDeadlock          kernel has no deadlock
  OutOfDisk        False     Fri, 12 Oct 2018 14:54:21 +0800   Mon, 27 Aug 2018 10:03:23 +0800   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False     Fri, 12 Oct 2018 14:54:21 +0800   Mon, 27 Aug 2018 10:03:23 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False     Fri, 12 Oct 2018 14:54:21 +0800   Mon, 27 Aug 2018 10:03:23 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False     Fri, 12 Oct 2018 14:54:21 +0800   Mon, 20 Aug 2018 19:05:38 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True      Fri, 12 Oct 2018 14:54:21 +0800   Mon, 27 Aug 2018 10:03:33 +0800   KubeletReady                 kubelet is posting ready status. AppArmor enabled
```
But how do I undo these conditions for all Nodes?
",closed,False,2018-10-12 07:07:38,2018-11-30 22:44:38
node-problem-detector,hchenxa,https://github.com/kubernetes/node-problem-detector/pull/212,https://api.github.com/repos/kubernetes/node-problem-detector/issues/212,update the deployment file and use configmap,Edit the deployment file and add the configmap sample,closed,True,2018-10-22 10:06:37,2018-10-22 10:19:31
node-problem-detector,hchenxa,https://github.com/kubernetes/node-problem-detector/pull/213,https://api.github.com/repos/kubernetes/node-problem-detector/issues/213,update the deployment file and use configmap,update the deployment file and add the sample configmap,closed,True,2018-10-22 10:20:34,2018-11-12 06:52:07
node-problem-detector,eatwithforks,https://github.com/kubernetes/node-problem-detector/issues/214,https://api.github.com/repos/kubernetes/node-problem-detector/issues/214,custom-plugin-monitor.json is reporting Systemd is not supported when node has systemctl.,"I'm running node-problem-detector as a daemonset built off of master branch. I'm receiving an error where the logs of node-problem-detector pods reports systemd is not supported for the check_ntp check. But I have systemctl in the nodes. Details below:

**kubectl logs ""node-problem-detector pod""**
```
I1023 23:57:56.323795       1 plugin.go:92] Finish running custom plugins
I1023 23:57:56.323802       1 custom_plugin_monitor.go:98] New status generated: &{Source:ntp-custom-plugin-monitor Events:[{Severity:warn Timestamp:2018-10-23 23:57:56.323798923 +0000 UTC m=+3480.060276471 Reason:NTPIsDown Message:Systemd is not supported}] Conditions:[{Type:NTPProblem Status:Unknown Transition:2018-10-23 23:00:26.323663714 +0000 UTC m=+30.060141456 Reason:NTPIsDown Message:Systemd is not supported}]}
```

**In the node:** 
`which systemctl` returns `/bin/systemctl`

**Expected:**

Node conditions status for ntp to show running.

**Actual:**

Node conditions status for ntp shows unknown with
```
  Type			Status		LastHeartbeatTime			LastTransitionTime			Reason				Message
  ----			------		-----------------			------------------			------				-------
  NTPProblem 		Unknown 	Wed, 24 Oct 2018 00:00:04 +0000 	Tue, 23 Oct 2018 23:00:26 +0000 	NTPIsDown 			Systemd is not supported
```",closed,False,2018-10-24 00:05:43,2018-10-25 16:43:00
node-problem-detector,byxorna,https://github.com/kubernetes/node-problem-detector/issues/215,https://api.github.com/repos/kubernetes/node-problem-detector/issues/215,Journald plugin not resetting condition after condition clears,"It is unclear to me (by docs, and spelunking through the code) how to use a non-custom plugin (i.e. `journald`) to set permanent Conditions on a node. For example, with the following configuration, once a pattern triggers the condition to go to `reason=KubenetIPAllocationFailure`, there is no way to ""timeout"" or reset to healthy after N seconds.

It seems like this plugin has no notion of a ""clear state"", where the condition can be no longer true. Is this by design?

```
{
  ""plugin"": ""journald"",
  ""pluginConfig"": {
    ""source"": ""kubelet""
  },
  ""logPath"": ""/run/log/journal"",
  ""lookback"": ""5m"",
  ""bufferSize"": 10,
  ""source"": ""kubelet-monitor"",
  ""conditions"": [
    {
      ""type"": ""KubeletNetworking"",
      ""reason"": ""Healthy"",
      ""message"": ""kubelet is functioning properly""
    }
  ],
  ""rules"": [
    {
      ""type"": ""permanent"",
      ""condition"": ""KubeletNetworking"",
      ""reason"": ""KubenetIPAllocationFailure"",
      ""pattern"": "".*NetworkPlugin kubenet failed to set up pod .*no IP addresses available in range.*""
    }
  ]
}
```

## Expected Behavior

Condition goes back to Healthy after a user-specified period of no triggering events.

## Actual Behavior

Condition stays in KubenetIPAllocationFailure forever. 
",open,False,2018-10-25 16:38:50,2019-03-20 14:10:08
node-problem-detector,mml,https://github.com/kubernetes/node-problem-detector/issues/216,https://api.github.com/repos/kubernetes/node-problem-detector/issues/216,NPD should detect when static pods are rejected by admission controller,"This is related to kubernetes#70499.

This is a class of bootstrapping problem that NPD should be able to report.",closed,False,2018-10-31 18:46:26,2018-11-12 10:33:28
node-problem-detector,max-rocket-internet,https://github.com/kubernetes/node-problem-detector/pull/217,https://api.github.com/repos/kubernetes/node-problem-detector/issues/217,Adding helm chart info to readme and fixing some formatting,"Now the a helm chart is available in the stable repo, I think we should include it here in the installation instructions. Especially since the YAML files in here are quite out of date.

https://github.com/helm/charts/tree/master/stable/node-problem-detector",closed,True,2018-11-02 10:08:25,2018-11-14 08:37:53
node-problem-detector,gjcarneiro,https://github.com/kubernetes/node-problem-detector/issues/218,https://api.github.com/repos/kubernetes/node-problem-detector/issues/218,The draino link to github is broken,https://github.com/negz/draino -> 404,closed,False,2018-11-08 16:00:35,2018-11-21 08:47:05
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/219,https://api.github.com/repos/kubernetes/node-problem-detector/issues/219,fix broken link for draino,"Fix #218

/cc @Random-Liu ",closed,True,2018-11-12 10:22:32,2018-11-21 09:37:57
node-problem-detector,ganeshvasudevan,https://github.com/kubernetes/node-problem-detector/issues/220,https://api.github.com/repos/kubernetes/node-problem-detector/issues/220,Getting log counter to work,"Hi,

I am trying to use the log counter to work with default config file. With the default configuration files I don't see the log counter working as well as any logs about the initialisation or any extra logs that I added.

Is there some extra steps that need to be done to get the log counter to work? 
Could some one share working config as well as the plugin used?

_Ganesh",closed,False,2018-11-15 15:25:10,2018-11-30 22:46:16
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/221,https://api.github.com/repos/kubernetes/node-problem-detector/issues/221,Detect readonly filesystem,"The PR adds one more rule to kernel monitor to detect read-only filesystem. It also changes the plugin from journald to kmsg, because when filessystem becomes readonly, no new logs can be saved into `/var/log`. Then journald does not have new data for problem detection.",closed,True,2018-11-20 00:27:34,2018-11-20 21:26:15
node-problem-detector,jstangroome,https://github.com/kubernetes/node-problem-detector/pull/222,https://api.github.com/repos/kubernetes/node-problem-detector/issues/222,Fix the spelling of monitor in the error message,,closed,True,2018-11-20 03:01:10,2018-11-20 04:25:42
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/223,https://api.github.com/repos/kubernetes/node-problem-detector/issues/223,Detect kubelet and container runtime frequent restarts,"This PR adds customer plugin that watch systemd logs for frequent kubelet and container runtime restarts. It reuses log counter for frequent restart detection. Notice that kubelet and container runtime restart logs are actually in systemd logs, not in kubelet or container runtime logs.

Noticeable changes:
- Add delay option to log watchers, so that they can delay watching until the node is stable.
- Change log counter to watch journald instead of kmsg.
- Update docker monitor to use `dockerd` as the source name.
- Fix vet errors
- Update README on injecting systemd logs",closed,True,2018-11-20 21:53:31,2018-11-27 08:35:24
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/224,https://api.github.com/repos/kubernetes/node-problem-detector/issues/224,Detect corrupt docker overlay2,This PR adds one more custom plugin that can detect corrupt docker overlay2 issue.,closed,True,2018-11-21 00:18:57,2018-11-27 08:49:29
node-problem-detector,SataQiu,https://github.com/kubernetes/node-problem-detector/pull/225,https://api.github.com/repos/kubernetes/node-problem-detector/issues/225,"Fix typos: NDDE -> NODE, permenantly -> permanently","Fix typos: NDDE -> NODE, permenantly -> permanently",closed,True,2018-11-21 09:37:03,2018-11-21 09:47:27
node-problem-detector,chen-joe1015,https://github.com/kubernetes/node-problem-detector/issues/226,https://api.github.com/repos/kubernetes/node-problem-detector/issues/226,"plugin “filelog” :unable to parse line: ""time=\xxxxxxxx\""","ERROR:
![image](https://user-images.githubusercontent.com/31652157/48963646-02909300-efd2-11e8-81b2-cf2da40d31c2.png)
CONFIG:
![image](https://user-images.githubusercontent.com/31652157/48963650-1fc56180-efd2-11e8-8a35-d5efe615f36e.png)
 thanks
",closed,False,2018-11-24 02:17:44,2018-12-05 02:42:04
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/issues/227,https://api.github.com/repos/kubernetes/node-problem-detector/issues/227,NPD HEAD not working on GKE,"How to reproduce:

1. Create GKE cluster with version v1.10.9-gke.5 (The NPD shipped with this GKE version is using NPD v0.5.0, with log counter support. See https://github.com/kubernetes/kubernetes/pull/65342).
2. Log onto a node and verify that NPD is working fine.
3. Compile NPD at HEAD and get tarball.
4. Copy newly built NPD tarball to the node.
4. Untar NPD.
5. Run the following commands:
```
sudo systemctl stop node-problem-detector.service
sudo cp -f bin/node-problem-detector /home/kubernetes/bin/node-problem-detector
sudo systemctl start node-problem-detector.service
```
6. Verify that NPD is not working. Saw logs as follows:
```
$ sudo journalctl -u node-problem-detector.service
...
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 systemd[1]: Started Kubernetes node problem detector.
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]: I1126 20:02:44.918435   14438 log_monitor.go:64] Finish parsing log monitor config file: {WatcherConfig:{Plugin:kmsg PluginConfig:map[] LogPath:/dev/kmsg Lookback:5m} BufferSize:10 Source:kernel-monitor DefaultConditions:[{Type:KernelDeadlock Stat
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]: I1126 20:02:44.919043   14438 log_watchers.go:40] Use log watcher of plugin ""kmsg""
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]: I1126 20:02:44.919373   14438 log_monitor.go:64] Finish parsing log monitor config file: {WatcherConfig:{Plugin:journald PluginConfig:map[source:docker] LogPath:/var/log/journal Lookback:5m} BufferSize:10 Source:docker-monitor DefaultConditions:[]
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]: I1126 20:02:44.919558   14438 log_watchers.go:40] Use log watcher of plugin ""journald""
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]: I1126 20:02:44.919812   14438 custom_plugin_monitor.go:67] Finish parsing custom plugin monitor config file: {Plugin:custom PluginGlobalConfig:{InvokeIntervalString:0xc00030b4c0 TimeoutString:0xc00030b4d0 InvokeInterval:5m0s Timeout:1m0s MaxOutput
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]: panic: invalid configuration: no server found for cluster ""local""
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]: goroutine 1 [running]:
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]: k8s.io/node-problem-detector/pkg/problemclient.NewClientOrDie(0xc0002f0f80, 0xc0002e69c0, 0xc0000405a0)
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]:         /usr/local/google/home/zhenw/go/src/k8s.io/node-problem-detector/pkg/problemclient/problem_client.go:69 +0x3f7
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]: main.main()
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 node-problem-detector[14438]:         /usr/local/google/home/zhenw/go/src/k8s.io/node-problem-detector/cmd/node_problem_detector.go:90 +0x302
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 systemd[1]: node-problem-detector.service: Main process exited, code=exited, status=2/INVALIDARGUMENT
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 systemd[1]: node-problem-detector.service: Unit entered failed state.
Nov 26 20:02:44 gke-npd-default-pool-17da7bcb-kb52 systemd[1]: node-problem-detector.service: Failed with result 'exit-code'.
```

I manually tried commits in the history and found that the culprit PR is https://github.com/kubernetes/node-problem-detector/pull/187.

/cc @Random-Liu 
/cc @andyxning 
/cc @jiayingz ",closed,False,2018-11-26 20:06:32,2018-12-06 01:26:23
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/228,https://api.github.com/repos/kubernetes/node-problem-detector/issues/228,More fix to custom plugin monitor condition change,"When `condition.Status` does not change, we should check `condition.Reason` instead of `condition.Message` because Message could be spammy and change a lot. 

This is part of #202.",closed,True,2018-11-27 18:57:14,2018-12-10 03:15:04
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/229,https://api.github.com/repos/kubernetes/node-problem-detector/issues/229,Add wangzhen127 as a maintainer.,"Add @wangzhen127 as a new maintainer of NPD. 
/cc @andyxning @wangzhen127 
Signed-off-by: Lantao Liu <lantaol@google.com>",closed,True,2018-11-28 02:29:43,2018-11-28 18:50:55
node-problem-detector,Hashfyre,https://github.com/kubernetes/node-problem-detector/issues/230,https://api.github.com/repos/kubernetes/node-problem-detector/issues/230,v0.6.0 not available on k8s.gcr.io,"The newest release has not yet been published to k8s.gcr.io, is there an ETA to the process?
Was hoping to try out `docker-monitor-counter.json` for overlay2 corruption detection.

```
> docker pull k8s.gcr.io/node-problem-detector:v0.6.0
Error response from daemon: manifest for k8s.gcr.io/node-problem-detector:v0.6.0 not found
```

May I know was this missed or it's part of the release timeline that I have to wait for.",closed,False,2018-11-28 10:34:24,2018-12-20 11:47:12
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/231,https://api.github.com/repos/kubernetes/node-problem-detector/issues/231,Update base image for CVE fix.,"There are several CVEs in `k8s.gcr.io/debian-base-amd64:0.3`.
![image](https://user-images.githubusercontent.com/5821883/49189677-30543f80-f324-11e8-9078-5469a6378b60.png)

Update the base image to fix those CVEs.
![image](https://user-images.githubusercontent.com/5821883/49189702-42ce7900-f324-11e8-976a-43b36733cb0d.png)
",closed,True,2018-11-28 23:42:51,2018-11-29 00:07:15
node-problem-detector,Atoms,https://github.com/kubernetes/node-problem-detector/issues/232,https://api.github.com/repos/kubernetes/node-problem-detector/issues/232,Journald plugin not working on 0.6.1,"Tried to deploy 0.6.1 version, and kernel-monitor with journald plugin fails with message:
```
E1129 09:42:51.150454       1 problem_detector.go:64] Failed to start log monitor ""/config/kernel-monitor.json"": failed to create journal client from path ""/var/log/journal/"": unable to open a handle to the library
```
downgraded to 0.5.0 and it works, cannot verify on 0.6.0 as image is not available. ",closed,False,2018-11-29 09:47:01,2018-11-29 22:23:23
node-problem-detector,Random-Liu,https://github.com/kubernetes/node-problem-detector/pull/233,https://api.github.com/repos/kubernetes/node-problem-detector/issues/233,Explicitly include libsystemd0 in the image.,"Fixes https://github.com/kubernetes/node-problem-detector/issues/232.

`libsystemd0` was excluded from the base image https://github.com/kubernetes/kubernetes/pull/69995

Explicitly include libsystemd0 because we need it for npd.",closed,True,2018-11-29 22:16:43,2019-01-07 19:35:45
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/issues/236,https://api.github.com/repos/kubernetes/node-problem-detector/issues/236,NPD is lack of k8s cluster level e2e test coverage,"We should add k8s cluster level e2e test coverage for NPD, so that situations like #227 could be prevented. There were past effort #43 and #86, but those never got completed. It is time to revisit.",open,False,2018-11-30 21:49:45,2019-03-20 18:54:06
node-problem-detector,eatwithforks,https://github.com/kubernetes/node-problem-detector/issues/237,https://api.github.com/repos/kubernetes/node-problem-detector/issues/237,Please add documentation for the possible fields in plugins,i'm getting confused by the multiple timeouts and what exactly concurrency means. ,closed,False,2018-12-06 19:48:04,2018-12-10 21:12:32
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/238,https://api.github.com/repos/kubernetes/node-problem-detector/issues/238,enable codnition updaet when message change for custom plugin,"Fix #200 

This PR adds a custom global config to control whether NPD should update condition status when message changes.",closed,True,2018-12-10 04:12:32,2018-12-12 02:24:26
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/239,https://api.github.com/repos/kubernetes/node-problem-detector/issues/239,add docs about custom plugin,Fix #237,closed,True,2018-12-10 06:02:39,2018-12-11 01:37:00
node-problem-detector,george-miller,https://github.com/kubernetes/node-problem-detector/issues/240,https://api.github.com/repos/kubernetes/node-problem-detector/issues/240,Add Resource Limits to the node-problem-detector-pod,Please add resource limits to this project to ensure stability!,closed,False,2018-12-14 21:56:42,2018-12-27 03:55:49
node-problem-detector,dazdaz,https://github.com/kubernetes/node-problem-detector/issues/241,https://api.github.com/repos/kubernetes/node-problem-detector/issues/241,Has this been tested on Azure Kubernetes Service,"Hi,

Has this been tested on Microsoft's Azure Kubernetes Service, Kubernetes v1.11.5  and did all of the features work as expected ?",open,False,2018-12-17 10:57:10,2019-03-25 07:05:17
node-problem-detector,oscarwest,https://github.com/kubernetes/node-problem-detector/issues/242,https://api.github.com/repos/kubernetes/node-problem-detector/issues/242,helm install stable/node-problem-detector Error: ,"Azure AKS

kubectl version
`
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.1"", GitCommit:""eec55b9ba98609a46fee712359c7b5b365bdd920"", GitTreeState:""clean"", BuildDate:""2018-12-13T19:44:19Z"", GoVersion:""go1.11.2"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.5"", GitCommit:""753b2dbc622f5cc417845f0ff8a77f539a4213ea"", GitTreeState:""clean"", BuildDate:""2018-11-26T14:31:35Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
`

helm repo list
`
NAME         	URL
stable       	https://kubernetes-charts.storage.googleapis.com
local        	http://127.0.0.1:8879/charts
azure-samples	https://azure-samples.github.io/helm-charts/
`


helm version
`
Client: &version.Version{SemVer:""v2.12.0"", GitCommit:""d325d2a9c179b33af1a024cdb5a4472b6288016a"", GitTreeState:""clean""}
Server: &version.Version{SemVer:""v2.12.0"", GitCommit:""d325d2a9c179b33af1a024cdb5a4472b6288016a"", GitTreeState:""clean""}
`

helm install..
`
west@Oscars-MBP:~/scripts/azure|⇒  helm install stable/node-problem-detector
Error: release washing-bronco failed: DaemonSet.apps ""washing-bronco-node-problem-detector"" is invalid: spec.template.spec.containers[0].imagePullPolicy: Unsupported value: ""<nil>"": supported values: ""Always"", ""IfNotPresent"", ""Never""
`",closed,False,2018-12-17 14:37:16,2018-12-25 06:20:03
node-problem-detector,Hashfyre,https://github.com/kubernetes/node-problem-detector/issues/243,https://api.github.com/repos/kubernetes/node-problem-detector/issues/243,Crashloopbackoff on v.0.6.1 with *-counter.json,"Env:
 - AWS + Kops
 - Ubuntu-16.04 hosts
 - Deployed with
`helm upgrade --install node-problem-detector -f node-problem-detector/helm/values/dev/values.yaml --version '1.0' --namespace=kube-system stable/node-problem-detector`
 - values:
```
---
settings:
  log_monitors:
    - /config/abrt-adaptor.json
    - /config/docker-monitor.json
    - /config/docker-monitor-filelog.json
    - /config/docker-monitor-counter.json
    - /config/kernel-monitor.json
    - /config/kernel-monitor-filelog.json
    - /config/kernel-monitor-counter.json
    - /config/network-problem-monitor.json
    - /config/systemd-monitor-counter.json

image:
  repository: k8s.gcr.io/node-problem-detector
  tag: v0.6.1
  pullPolicy: IfNotPresent

nameOverride: ""node-problem-detector""
fullnameOverride: ""node-problem-detector""

rbac:
  create: true

resources: {}

annotations:
  sidecar.istio.io/inject: ""false""

tolerations:
  - effect: NoSchedule
    operator: Exists
  - key: CriticalAddonsOnly
    operator: Exists

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""node-problem-detector""

affinity: {}

```
Logs:
```
I1221 12:13:10.771074       1 log_monitor.go:64] Finish parsing log monitor config file: {WatcherConfig:{Plugin:journald PluginConfig:map[source:abrt-notification] LogPath:/var/log/journal Lookback:5m Delay:} BufferSize:10 Source:abrt-adaptor DefaultConditions:[] Rules:[{Type:temporary Condition: Reason:CCPPCrash Pattern:Process \d+ \(\S+\) crashed in .*} {Type:temporary Condition: Reason:UncaughtException Pattern:Process \d+ \(\S+\) of user \d+ encountered an uncaught \S+ exception} {Type:temporary Condition: Reason:XorgCrash Pattern:Display server \S+ crash in \S+} {Type:temporary Condition: Reason:VMcore Pattern:System encountered a fatal error in \S+} {Type:temporary Condition: Reason:Kerneloops Pattern:System encountered a non-fatal error in \S+}]}
I1221 12:13:10.771124       1 log_watchers.go:40] Use log watcher of plugin ""journald""
I1221 12:13:10.771202       1 log_monitor.go:64] Finish parsing log monitor config file: {WatcherConfig:{Plugin:journald PluginConfig:map[source:dockerd] LogPath:/var/log/journal Lookback:5m Delay:} BufferSize:10 Source:docker-monitor DefaultConditions:[] Rules:[{Type:temporary Condition: Reason:CorruptDockerImage Pattern:Error trying v2 registry: failed to register layer: rename /var/lib/docker/image/(.+) /var/lib/docker/image/(.+): directory not empty.*}]}
I1221 12:13:10.771215       1 log_watchers.go:40] Use log watcher of plugin ""journald""
I1221 12:13:10.771281       1 log_monitor.go:64] Finish parsing log monitor config file: {WatcherConfig:{Plugin:filelog PluginConfig:map[message:msg=""([^
]*)"" timestampFormat:2006-01-02T15:04:05.999999999-07:00 timestamp:^time=""(\S*)""] LogPath:/var/log/docker.log Lookback:5m Delay:} BufferSize:10 Source:docker-monitor DefaultConditions:[] Rules:[{Type:temporary Condition: Reason:CorruptDockerImage Pattern:Error trying v2 registry: failed to register layer: rename /var/lib/docker/image/(.+) /var/lib/docker/image/(.+): directory not empty.*}]}
I1221 12:13:10.771294       1 log_watchers.go:40] Use log watcher of plugin ""filelog""
F1221 12:13:10.771364       1 log_monitor.go:56] Failed to unmarshal configuration file ""/config/docker-monitor-counter.json"": json: cannot unmarshal number into Go struct field MonitorConfig.pluginConfig of type string
```",open,False,2018-12-21 12:16:40,2019-03-20 15:38:11
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/244,https://api.github.com/repos/kubernetes/node-problem-detector/issues/244,bump NPD image version to 0.6.1,/cc @Random-Liu @wangzhen127 ,closed,True,2018-12-25 07:34:58,2018-12-26 03:44:16
node-problem-detector,andyxning,https://github.com/kubernetes/node-problem-detector/pull/245,https://api.github.com/repos/kubernetes/node-problem-detector/issues/245,add resource,"Fix #240 

The requests and limits are only references. They have no requirements. End users can adjust this themselves. 

/cc @Random-Liu @wangzhen127 ",closed,True,2018-12-25 07:39:07,2018-12-27 07:08:13
node-problem-detector,chotiwat,https://github.com/kubernetes/node-problem-detector/pull/246,https://api.github.com/repos/kubernetes/node-problem-detector/issues/246,Add log-counter to the Dockerfile,"Hi, I'm getting the `rule path ""/home/kubernetes/bin/log-counter"" does not exist` when trying to use the `systemd-monitor-counter.json` as a custom plugin monitor. Below is the full log line.

```
F1227 00:23:30.513039       1 custom_plugin_monitor.go:64] Failed to validate custom plugin config {Plugin:custom PluginGlobalConfig:{InvokeIntervalString:0xc0002d7fb0 TimeoutString:0xc0002d7fc0 InvokeInterval:5m0s Timeout:1m0s MaxOutputLength:0xc000133b18 Concurrency:0xc000133b48} Source:systemd-monitor DefaultConditions:[{Type:FrequentKubeletRestart Status: Transition:0001-01-01 00:00:00 +0000 UTC Reason:NoFrequentKubeletRestart Message:kubelet is functioning properly} {Type:FrequentDockerRestart Status: Transition:0001-01-01 00:00:00 +0000 UTC Reason:NoFrequentDockerRestart Message:docker is functioning properly} {Type:FrequentContainerdRestart Status: Transition:0001-01-01 00:00:00 +0000 UTC Reason:NoFrequentContainerdRestart Message:containerd is functioning properly}] Rules:[0xc000118c40 0xc000118d90 0xc000118e00]}: rule path ""/home/kubernetes/bin/log-counter"" does not exist. Rule: &amp;{Type:permanent Condition:FrequentKubeletRestart Reason:FrequentKubeletRestart Path:/home/kubernetes/bin/log-counter Args:[--journald-source=systemd --log-path=/var/log/journal --lookback=20m --delay=5m --count=5 --pattern=Started Kubernetes kubelet.] TimeoutString:0xc000300000 Timeout:1m0s}
```

This pr adds the missing binary, as required by all the `*-counter.json`, to the container.


",closed,True,2018-12-27 01:11:24,2018-12-27 03:17:29
node-problem-detector,pigletfly,https://github.com/kubernetes/node-problem-detector/issues/247,https://api.github.com/repos/kubernetes/node-problem-detector/issues/247,CrashLoopBackOff node-problem-detector:v0.2 ,"env

```
kubernetes: 1.7.8
node-problem-detector: v2.0
```
and deployed with DaemonSet

logs from node-problem-detector

```
I1227 09:17:44.203531       1 kernel_monitor.go:83] Finish parsing log file: {WatcherConfig:{KernelLogPath:/dev/kmsg StartPattern: Lookback:5m} BufferSize:10 Source:kernel-monitor DefaultConditions:[{Type:KernelDeadlock Status:false Transition:0001-01-01 00:00:00 +0000 UTC Reason:KernelHasNoDeadlock Message:kernel has no deadlock} {Type:ReadonlyFilesystem Status:false Transition:0001-01-01 00:00:00 +0000 UTC Reason:FilesystemIsReadOnly Message:Filesystem is read-only}] Rules:[{Type:temporary Condition: Reason:OOMKilling Pattern:Kill process \d+ (.+) score \d+ or sacrifice child\nKilled process \d+ (.+) total-vm:\d+kB, anon-rss:\d+kB, file-rss:\d+kB.*} {Type:temporary Condition: Reason:TaskHung Pattern:task \S+:\w+ blocked for more than \w+ seconds\.} {Type:temporary Condition: Reason:UnregisterNetDevice Pattern:unregister_netdevice: waiting for \w+ to become free. Usage count = \d+} {Type:temporary Condition: Reason:KernelOops Pattern:BUG: unable to handle kernel NULL pointer dereference at .*} {Type:temporary Condition: Reason:KernelOops Pattern:divide error: 0000 \[#\d+\] SMP} {Type:permanent Condition:KernelDeadlock Reason:AUFSUmountHung Pattern:task umount\.aufs:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:KernelDeadlock Reason:DockerHung Pattern:task docker:\w+ blocked for more than \w+ seconds\.} {Type:permanent Condition:ReadonlyFilesystem Reason:FilesystemIsReadOnly Pattern:Remounting filesystem read-only}]}
I1227 09:17:44.204189       1 kernel_monitor.go:97] Start kernel monitor
I1227 09:17:44.204208       1 kernel_log_watcher.go:110] Start watching kernel log
I1227 09:17:44.204216       1 problem_detector.go:60] Problem detector started
I1227 09:17:44.204241       1 kernel_monitor.go:190] Initalize condition generated: [{Type:KernelDeadlock Status:false Transition:2018-12-27 09:17:44.204219855 +0800 CST Reason:KernelHasNoDeadlock Message:kernel has no deadlock} {Type:ReadonlyFilesystem Status:false Transition:2018-12-27 09:17:44.204220368 +0800 CST Reason:FilesystemIsReadOnly Message:Filesystem is read-only}]
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x45fc9e]

goroutine 14 [running]:
panic(0x1180c20, 0xc420016020)
	/usr/local/go/src/runtime/panic.go:500 +0x1a1
k8s.io/node-problem-detector/pkg/kernelmonitor.(*kernelLogWatcher).watchLoop(0xc42042acb0)
	/usr/local/google/home/lantaol/workspace/src/k8s.io/node-problem-detector/pkg/kernelmonitor/kernel_log_watcher.go:133 +0x16e
created by k8s.io/node-problem-detector/pkg/kernelmonitor.(*kernelLogWatcher).Watch
	/usr/local/google/home/lantaol/workspace/src/k8s.io/node-problem-detector/pkg/kernelmonitor/kernel_log_watcher.go:111 +0x2a1
```",open,False,2018-12-27 01:19:31,2019-03-30 13:39:30
node-problem-detector,nak3,https://github.com/kubernetes/node-problem-detector/pull/248,https://api.github.com/repos/kubernetes/node-problem-detector/issues/248,Add validation for the required flag,"If `--system-log-monitors` or `--custom-plugin-monitors` are not
specified, npd gave us unclear message.

This patch adds the validation and clear error message.

BEFORE
```
$ ./bin/node-problem-detector --apiserver-override=http://$APISERVER_IP:$APISERVER_INSECURE_PORT?inClusterConfig=false
F0117 13:15:02.298876   12161 node_problem_detector.go:99] Problem detector failed with error: no log monitor is successfully setup
goroutine 1 [running]:
k8s.io/node-problem-detector/vendor/github.com/golang/glog.stacks(0xc000412900, 0xc0002b6000, 0x84, 0xc8)
	/home/knakayam/.go/src/k8s.io/node-problem-detector/vendor/github.com/golang/glog/glog.go:766 +0xd4
k8s.io/node-problem-detector/vendor/github.com/golang/glog.(*loggingT).output(0x1d320a0, 0xc000000003, 0xc00011a630, 0x1a9f55c, 0x18, 0x63, 0x0)
	/home/knakayam/.go/src/k8s.io/node-problem-detector/vendor/github.com/golang/glog/glog.go:717 +0x306
k8s.io/node-problem-detector/vendor/github.com/golang/glog.(*loggingT).printf(0x1d320a0, 0x3, 0x10e4868, 0x26, 0xc00051ff38, 0x1, 0x1)
	/home/knakayam/.go/src/k8s.io/node-problem-detector/vendor/github.com/golang/glog/glog.go:655 +0x14b
k8s.io/node-problem-detector/vendor/github.com/golang/glog.Fatalf(0x10e4868, 0x26, 0xc00051ff38, 0x1, 0x1)
	/home/knakayam/.go/src/k8s.io/node-problem-detector/vendor/github.com/golang/glog/glog.go:1145 +0x67
main.main()
	/home/knakayam/.go/src/k8s.io/node-problem-detector/cmd/node_problem_detector.go:99 +0x4a2
```

AFTER:
```
$ ./bin/node-problem-detector --apiserver-override=http://$APISERVER_IP:$APISERVER_INSECURE_PORT?inClusterConfig=false
panic: Either --system-log-monitors or --custom-plugin-monitors is required

goroutine 1 [running]:
k8s.io/node-problem-detector/cmd/options.(*NodeProblemDetectorOptions).ValidOrDie(0xc00052bc80)
	/home/knakayam/.go/src/k8s.io/node-problem-detector/cmd/options/options.go:84 +0xff
main.main()
	/home/knakayam/.go/src/k8s.io/node-problem-detector/cmd/node_problem_detector.go:65 +0x82
```",closed,True,2019-01-17 04:39:04,2019-01-17 05:42:29
node-problem-detector,nak3,https://github.com/kubernetes/node-problem-detector/pull/249,https://api.github.com/repos/kubernetes/node-problem-detector/issues/249,Run travis against go v1.10 and v1.11,"Currently travis is failing on `TestGoroutineLeak` test but it is only
on Go v1.9.

Go v1.9 is not so new version, hence this patch updates to run travis
against v1.10 and v1.11.

ref:
https://travis-ci.org/kubernetes/node-problem-detector/builds/472040385
https://travis-ci.org/kubernetes/node-problem-detector/builds/465834849
https://travis-ci.org/kubernetes/node-problem-detector/builds/465809562

```
--- FAIL: TestGoroutineLeak (0.00s)
	Error Trace:	log_monitor_test.go:154
	Error:		Not equal: 4 (expected)
			        != 3 (actual)
```",closed,True,2019-01-18 01:42:18,2019-03-05 17:22:49
node-problem-detector,mkumatag,https://github.com/kubernetes/node-problem-detector/pull/250,https://api.github.com/repos/kubernetes/node-problem-detector/issues/250,Add multiarch support and push fat manifest,"This will enable to build node-problem-detector project for multiarch like amd64, arm, arm64 and ppc64le and push the fat manifest.

Fixes: https://github.com/kubernetes/kubernetes/issues/73364",open,True,2019-01-22 17:34:44,2019-03-22 20:43:19
node-problem-detector,rrs45,https://github.com/kubernetes/node-problem-detector/issues/251,https://api.github.com/repos/kubernetes/node-problem-detector/issues/251,Enable JSON support for monitoring log files in json format ,"Currently, the system-log-monitor supports 'filelog' which supports only regex based translation. For log file which are in JSON, it would be much easier to enable JSON parsing support. This would make users life easier by not having to come up complex/ugly regex's

",open,False,2019-01-22 21:48:59,2019-01-22 21:48:59
node-problem-detector,gtorre,https://github.com/kubernetes/node-problem-detector/issues/252,https://api.github.com/repos/kubernetes/node-problem-detector/issues/252,Should Ready condition change if other condition states are False?,"```
Conditions:
  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message
  ----                          ------  -----------------                 ------------------                ------                          -------
  **IsChainsed                  True    Tue, 27 Mar 2018 14:09:31 -0400   Tue, 27 Mar 2018 14:09:30 -0400   Chainsed                        Node is chainsed
  KernelDeadlock                False   Thu, 24 Jan 2019 15:32:05 -0500   Tue, 04 Dec 2018 15:44:07 -0500   KernelHasNoDeadlock             kernel has no deadlock
  OutOfDisk                     False   Thu, 24 Jan 2019 17:25:54 -0500   Mon, 05 Mar 2018 18:07:06 -0500   KubeletHasSufficientDisk        kubelet has sufficient disk space available
  MemoryPressure                False   Thu, 24 Jan 2019 17:25:54 -0500   Fri, 06 Jul 2018 02:08:40 -0400   KubeletHasSufficientMemory      kubelet has sufficient memory available
  DiskPressure                  False   Thu, 24 Jan 2019 17:25:54 -0500   Fri, 06 Jul 2018 02:08:40 -0400   KubeletHasNoDiskPressure        kubelet has no disk pressure
  Ready                         True    Thu, 24 Jan 2019 17:25:54 -0500   Thu, 24 Jan 2019 17:23:11 -0500   KubeletReady                    kubelet is posting ready status
  FrequentUnregisterNetDevice   False   Thu, 24 Jan 2019 17:25:14 -0500   Thu, 24 Jan 2019 17:22:26 -0500   NoFrequentUnregisterNetDevice   node is functioning properly
  PIDPressure                   False   Thu, 24 Jan 2019 17:25:54 -0500   Tue, 16 Oct 2018 13:56:40 -0400   KubeletHasSufficientPID         kubelet has sufficient PID available
```

We wrote a custom plugin under the impression that if the plugin returned a 1 exit code, it would make the node unavailable for scheduling. However, since the Ready condition never updates to False, pods continue to be scheduled on the kubelet.

Is the expectation that any `permanent` problems would trigger the Ready state to become false? Or this assumption wrong?",open,False,2019-01-24 22:56:21,2019-02-09 00:46:45
node-problem-detector,stribb,https://github.com/kubernetes/node-problem-detector/pull/253,https://api.github.com/repos/kubernetes/node-problem-detector/issues/253,Empty LogPath will use journald's default path.,"At present, the empty LogPath will use the _code's_ default journal path, without regard for the _host's_ default journal path.

This code changes the default to use the host's path.",open,True,2019-01-30 11:14:28,2019-03-21 18:01:56
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/254,https://api.github.com/repos/kubernetes/node-problem-detector/issues/254,Add script for presubmit and CI jobs,"This PR adds a script for presubmit and CI jobs. It is needed by the presubmit and CI setup on test-infra side. This is part of https://github.com/kubernetes/node-problem-detector/issues/236.

This PR needs to work with the test-infra side PR https://github.com/kubernetes/test-infra/pull/11208.",closed,True,2019-02-09 00:28:28,2019-03-13 18:09:37
node-problem-detector,rrs45,https://github.com/kubernetes/node-problem-detector/issues/255,https://api.github.com/repos/kubernetes/node-problem-detector/issues/255,Allow multiple rules to be combined into single condition,"Often times we have multiple checks which can be grouped together into one condition. And it would be ideal if that condition would reflect only those checks which failed.

Currently, if a 'permanent' rule type is defined in the config then it appears in the 'conditions' field when using 'kubectl describe node', forever. It shows up even when we no longer need that check and the conditions keep growing as we add/remove such rule types. This is very inflexible.

For example, we want to monitor sensu check failures and report them as a node condition. We only want to see which of those checks failed as a single condition.

For this, we need:
1.  A way to aggregate multiple rules into a group and create a single condition which becomes true when any of the rules( within the group ) gets triggered. And becomes false when none of them gets triggered
2. Dynamically update 'Message' field of the condition by combining results from only those rules   which get triggered.",open,False,2019-02-09 01:09:28,2019-02-11 02:02:51
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/256,https://api.github.com/repos/kubernetes/node-problem-detector/issues/256,dummy,For testing. Do not submit,closed,True,2019-03-01 22:26:23,2019-03-02 00:14:05
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/257,https://api.github.com/repos/kubernetes/node-problem-detector/issues/257,Fix travis ci file,"This PR fixes the travis CI file. Previous problems:
1. There is always a default build step before jobs, which does not include the journald build tag.
2. The jobs only inherit the first golang version, which is 1.9. Version 1.10 is never tested.

Due to constraint on travis, now we combine `make` and `make test` and override the default script. We also add more golang versions here.

Part of https://github.com/kubernetes/node-problem-detector/issues/236.",closed,True,2019-03-02 00:17:16,2019-03-14 05:00:18
node-problem-detector,mitar,https://github.com/kubernetes/node-problem-detector/issues/258,https://api.github.com/repos/kubernetes/node-problem-detector/issues/258,Does not run without a config map,"README looks like config map is optional, but just using [`node-problem-detector.yaml`](https://github.com/kubernetes/node-problem-detector/blob/master/deployment/node-problem-detector.yaml) as-is does not work and it fails because config map is missing.",open,False,2019-03-07 22:04:21,2019-03-07 22:04:21
node-problem-detector,jpds,https://github.com/kubernetes/node-problem-detector/issues/259,https://api.github.com/repos/kubernetes/node-problem-detector/issues/259,Prometheus metrics endpoint,Could a `/metrics` endpoint be added to n-p-d so that tools like Prometheus can gather these and then create alerts based on those?,open,False,2019-03-08 09:22:14,2019-03-08 09:22:14
node-problem-detector,joelsmith,https://github.com/kubernetes/node-problem-detector/pull/260,https://api.github.com/repos/kubernetes/node-problem-detector/issues/260,Update embargo doc link in SECURITY_CONTACTS and change PST to PSC,See https://github.com/kubernetes/security/issues/8 for more information,closed,True,2019-03-08 17:49:40,2019-03-11 06:23:28
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/261,https://api.github.com/repos/kubernetes/node-problem-detector/issues/261,Install system lib in build.sh,"Install journald system lib in build.sh

Part of https://github.com/kubernetes/node-problem-detector/issues/236.",closed,True,2019-03-13 23:37:57,2019-03-19 02:00:27
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/262,https://api.github.com/repos/kubernetes/node-problem-detector/issues/262,Fix CI job race condition,"Current CI job has race condition. The related CI jobs are:
- [ci-npd-build](https://k8s-testgrid.appspot.com/sig-node-node-problem-detector#ci-npd-build): Build and push NPD to staging, push `ci.env` file to staging.
- [ci-npd-e2e-kubernetes-gce-gci](https://k8s-testgrid.appspot.com/sig-node-node-problem-detector#ci-npd-e2e-kubernetes-gce-gci): Download `ci.env` and export the env; then start to run E2E test; when creating the cluster, use the env exported in `ci.env` to download the NPD builds.
- [ci-npd-e2e-kubernetes-gce-ubuntu](https://k8s-testgrid.appspot.com/sig-node-node-problem-detector#ci-npd-e2e-kubernetes-gce-ubuntu): Similar to the gci one, just running against ubuntu.

The CI jobs are configured to run every 2 hours. The race condition happens as follows:
1. Time `t`:  All 3 CI jobs run. `ci-npd-build` push the new builds to staging (with new sha1 value).
2. Time `t+2hour`: All 3 CI jobs run again. `ci-npd-build` starts to run, but hasn't finished the build. `ci-npd-e2e-kubernetes-gce-gci` downloads the `ci.env` pushed in step 1 at time `t`.
3. Time `t+2hour+2min`: `ci-npd-build` finishes building and pushes a new build to staging.
4. Time `t+2hour+5min`: `ci-npd-e2e-kubernetes-gce-gci` created the cluster and try to download NPD build. It used the old `ci.env` as downloaded in step 2, but the NPD build on staging has been updated to the new build in step 3. Then the sha1 value does not match. NPD installation fails with following error:
```
Mar 19 14:51:37.716008 e2e-69-7abb6-minion-group-23dx configure.sh[787]: Downloading node-problem-detector-v0.6.2-10-g796c8d0.tar.gz.
Mar 19 14:51:37.738122 e2e-69-7abb6-minion-group-23dx configure.sh[787]:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
Mar 19 14:51:37.738404 e2e-69-7abb6-minion-group-23dx configure.sh[787]:                                  Dload  Upload   Total   Spent    Left  Speed
Mar 19 14:51:38.062481 e2e-69-7abb6-minion-group-23dx configure.sh[787]: [237B blob data]
Mar 19 14:51:38.124829 e2e-69-7abb6-minion-group-23dx configure.sh[787]: == node-problem-detector-v0.6.2-10-g796c8d0.tar.gz corrupted, sha1 f815d9b7fdec53a0363bb77c19b575c8296dc1fe doesn't match expected 2b7e70b122f338bf75c4beb909f6ded0a569c042 ==
Mar 19 14:51:38.125255 e2e-69-7abb6-minion-group-23dx configure.sh[787]: == Hash validation of https://storage.googleapis.com/node-problem-detector-staging/ci/node-problem-detector/node-problem-detector-v0.6.2-10-g796c8d0.tar.gz failed. Retrying. ==
```

This PR adds current time to version and tag, so that the NPD builds will not be overridden. The GCS bucket is configured to only keep the builds for 7 days, so old files will be cleaned up.

Part of https://github.com/kubernetes/node-problem-detector/issues/236.",closed,True,2019-03-19 16:31:31,2019-03-19 22:34:36
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/263,https://api.github.com/repos/kubernetes/node-problem-detector/issues/263,Register gcloud as a Docker credential in Makefile,"`gcloud docker` is deprecated. This PR updates the Makefile to register gcloud as docker credential.

Part of https://github.com/kubernetes/node-problem-detector/issues/236.",closed,True,2019-03-22 18:50:54,2019-03-22 19:58:54
node-problem-detector,yuwenma,https://github.com/kubernetes/node-problem-detector/pull/264,https://api.github.com/repos/kubernetes/node-problem-detector/issues/264,Support building NPD images in different arch,Test: `make` generates a .tar.gz file and the exec file node-problem-detector can be upstarted. ,closed,True,2019-03-30 02:09:23,2019-04-01 21:44:36
node-problem-detector,wangzhen127,https://github.com/kubernetes/node-problem-detector/pull/265,https://api.github.com/repos/kubernetes/node-problem-detector/issues/265,Disable glog writing to files for log-counter,"On GKE, we encountered issues where NPD creates high amount of log files on node for log-counter with NPD v0.6.0, where we added more custom plugins that are using log counter. 

Log counter is a custom plugin, it is configured to run once per few minutes (e.g. 5min). Every time it runs, it will generate a tmp file. Eventually it can fill the tmp directory. We should just redirect the logs to stderr. Custom plugins are just using stdout as the interface to talk to NPD, so it should be fine.

This problem is similar to https://github.com/kubernetes-sigs/cri-tools/pull/291.",closed,True,2019-04-03 20:16:58,2019-04-03 22:01:00
