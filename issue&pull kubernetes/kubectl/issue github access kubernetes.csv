name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/1,https://api.github.com/repos/kubernetes/kubectl/issues/1,Initial repo boilerplate,,closed,True,2017-03-30 20:27:45,2017-03-30 20:53:30
kubectl,jeremyjjbrown,https://github.com/kubernetes/kubectl/issues/2,https://api.github.com/repos/kubernetes/kubectl/issues/2,Kubectl errors when it needs to be updated instead of notifying.,"It would be nice if kubectl would provide a hint when it just needs an update.

Using a slightly older version of kubectl against Api server 1.6 and I got

```bash
kubectl get pods
error: group map[autoscaling:0xc8203c7110 batch:0xc8203c72d0 policy:0xc8203c6070 rbac.authorization.k8s.io:0xc8203c60e0 storage.k8s.io:0xc8203c6150 componentconfig:0xc8203c73b0 extensions:0xc8203c6000 federation:0xc8203c69a0 :0xc8203c6e70 apps:0xc8203c6ee0 authentication.k8s.io:0xc8203c6f50 authorization.k8s.io:0xc8203c70a0 certificates.k8s.io:0xc8203c7340] is already registered
```

Needless to say the user likely is going to waste a lot of time troubleshooting this before replacing kubectrl that was working just fine minutes before.",closed,False,2017-04-12 15:29:30,2017-05-31 02:37:28
kubectl,lihan,https://github.com/kubernetes/kubectl/issues/3,https://api.github.com/repos/kubernetes/kubectl/issues/3,kubectl get po columns aren't aligned when using watch mode,"`kubectl version`

`Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.0"", GitCommit:""fff5156092b56e6bd60fff75aad4dc9de6b6ef37"", GitTreeState:""clean"", BuildDate:""2017-03-28T16:36:33Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""darwin/amd64""}`

`Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.1"", GitCommit:""b0b7a323cc5a4a2019b2e9520c21c7830b7f708e"", GitTreeState:""clean"", BuildDate:""2017-04-03T20:33:27Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}`

See the below screenshot, when executing `kubectl get po -o wide -w`, the columns aren't aligned

![untitled-1](https://cloud.githubusercontent.com/assets/655051/25209031/a819c39c-25bb-11e7-89fc-611a796295d9.jpg)
",closed,False,2017-04-20 01:23:25,2019-01-02 05:53:57
kubectl,owenmorgan,https://github.com/kubernetes/kubectl/issues/4,https://api.github.com/repos/kubernetes/kubectl/issues/4,Cant set context,"When using kubectl 

`kubectl version                                                                                                                                                            (jd-uat/default)
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T22:51:55Z"", GoVersion:""go1.8.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.2"", GitCommit:""08e099554f3c31f6e6f07b448ab3ed78d0520507"", GitTreeState:""clean"", BuildDate:""2017-01-12T04:52:34Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}`

i have 2 different contexts set in config trying to set context from one to another doesn't switch.

`▶ kubectl config current-context                                                                                           
od-uat

~
▶ kubectl config set-context oi-uat
Context ""oi-uat"" set.

~
▶ kubectl config current-context
od-uat`

And the config file still shows as 

`current-context: od-uat`
",closed,False,2017-04-20 09:21:17,2017-04-21 06:38:14
kubectl,dictvm,https://github.com/kubernetes/kubectl/issues/5,https://api.github.com/repos/kubernetes/kubectl/issues/5,kubectl cp doesn't feature tab completion,"```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.0"", GitCommit:""fff5156092b56e6bd60fff75aad4dc9de6b6ef37"", GitTreeState:""clean"", BuildDate:""2017-03-28T19:15:41Z"", GoVersion:""go1.8"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.2"", GitCommit:""08e099554f3c31f6e6f07b448ab3ed78d0520507"", GitTreeState:""clean"", BuildDate:""2017-01-12T04:52:34Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}
```

It would be helpful if `kubectl cp` completed pod-names, like most other `kubectl` parameters do. This way, you wouldn't have to issue `kubectl get pod` and copy/paste the pod name before copying a file from or to a pod.",closed,False,2017-04-21 12:36:54,2018-05-08 22:24:08
kubectl,liggitt,https://github.com/kubernetes/kubectl/issues/6,https://api.github.com/repos/kubernetes/kubectl/issues/6,kubectl proxy --reject-methods flag is non-functional,"The flag is registered, but the specified value is never read or used

This means all http methods are allowed by default (despite the help saying PUT/POST/PATCH being disallowed), and users have no way to control which methods are accepted",closed,False,2017-04-25 20:41:54,2017-05-16 21:47:49
kubectl,Vrtak-CZ,https://github.com/kubernetes/kubectl/issues/7,https://api.github.com/repos/kubernetes/kubectl/issues/7,kubectl delete all -l foo=bar - does not delete ingress,"```
kubectl delete all -l foo=bar
```

does not delete ingress with label `foo=bar`.",open,False,2017-04-26 13:27:43,2019-03-26 09:04:01
kubectl,kendallarm,https://github.com/kubernetes/kubectl/issues/8,https://api.github.com/repos/kubernetes/kubectl/issues/8,l --help,,closed,False,2017-04-27 12:29:52,2017-04-27 12:30:18
kubectl,gyliu513,https://github.com/kubernetes/kubectl/issues/9,https://api.github.com/repos/kubernetes/kubectl/issues/9,"Adding ""PodPreset"" to ""kubectl get help""","@gyliu513 commented on [Wed May 10 2017](https://github.com/kubernetes/kubernetes/issues/45597)

<!-- Thanks for filing an issue! Before hitting the button, please answer these questions.-->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):


**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):


**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:
The PodPreset was available in Kubernetes 1.6, so we should expose this in the kubectl CLI.

**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:



",closed,False,2017-05-10 15:52:34,2018-09-25 18:58:46
kubectl,gyliu513,https://github.com/kubernetes/kubectl/issues/10,https://api.github.com/repos/kubernetes/kubectl/issues/10,"Adding ""PodPreset"" to ""kubectl get help""","@gyliu513 commented on [Wed May 10 2017](https://github.com/kubernetes/kubernetes/issues/45597)

<!-- Thanks for filing an issue! Before hitting the button, please answer these questions.-->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):


**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):


**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:
The PodPreset was available in Kubernetes 1.6, so we should expose this in the kubectl CLI.

**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:



---

@gyliu513 commented on [Wed May 10 2017](https://github.com/kubernetes/kubernetes/issues/45597#issuecomment-300527307)

Issue moved to [kubernetes/kubectl #9](https://github.com/kubernetes/kubectl/issues/9) via [**ZenHub**](https://www.zenhub.com/)


",closed,False,2017-05-10 15:52:38,2017-05-10 15:54:50
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/11,https://api.github.com/repos/kubernetes/kubectl/issues/11,Refactor kubectl run + expose and kubectl create xxx to use shared code,"The `kubectl run` and `kubectl expose` commands can be used to create various workloads and services.  `kubectl create xxx` commands can also create the same resources, but use different code and have support for different flags.

Refactor the commands that create the same objects to use common code and bring feature parity",open,False,2017-05-17 23:17:20,2019-01-23 21:09:12
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/12,https://api.github.com/repos/kubernetes/kubectl/issues/12,Add kubectl create for missing workloads,"Add new `kubectl create` commands for missing workloads.  Flags for fields shared between all workloads - e.g. `image` (and everything else in Pod or PodTemplate) should be factored into a single library.  Flags for field shared between many workloads should be factored into another library - e.g. `replicas`.

- `create statefulset`
- `create daemonset`
- `create replicaset`
- `create pod`",open,False,2017-05-17 23:40:54,2019-02-03 10:08:21
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/13,https://api.github.com/repos/kubernetes/kubectl/issues/13,Add issue template for kubectl repo,,closed,True,2017-05-18 17:07:49,2017-05-18 17:39:38
kubectl,sanderploegsma,https://github.com/kubernetes/kubectl/issues/14,https://api.github.com/repos/kubernetes/kubectl/issues/14,`kubectl scale` completion does not include statefulset,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): A bit of both, I guess.

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`): 1.6.2


**Environment**:
- **Cloud provider or hardware configuration**: Macbook Pro 2016
- **OS** (e.g. from /etc/os-release): macOS Sierra 10.12.4
- **Kernel** (e.g. `uname -a`): `Darwin Sanders-MacBook-Pro-2.local 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64`
- **Install tools**: `gcloud components install kubectl`?
- **Others**: Running zsh

**What happened**: When trying to scale my statefulset using `kubectl`, autocomplete only offers `--replicas             deployment             job                    replicaset             replicationcontroller`. ",closed,False,2017-05-19 08:50:57,2017-06-24 14:06:05
kubectl,veris-neerajdhiman,https://github.com/kubernetes/kubectl/issues/15,https://api.github.com/repos/kubernetes/kubectl/issues/15,Issues in Pull local image irrespective of ` imagePullPolicy: Never` tag,"- I am trying to create a pod from local docker image which I have created . 

![selection_006](https://cloud.githubusercontent.com/assets/15321333/26352052/b2c74612-3fd7-11e7-8c9d-ffdcbf62fd97.png)

- Below is my yaml file 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: dockertest-web-1
      image: nd/djclone
      ports:
      - containerPort: 8000
      imagePullPolicy: Never
```
- Command : 
![selection_007](https://cloud.githubusercontent.com/assets/15321333/26352167/38202aea-3fd8-11e7-8651-939997926878.png)

- Dashboard view : 
![kubernetes dashboard](https://cloud.githubusercontent.com/assets/15321333/26352210/5fa8182a-3fd8-11e7-87ce-3490208b79ba.png)




",closed,False,2017-05-23 11:47:58,2017-05-31 02:32:39
kubectl,veris-neerajdhiman,https://github.com/kubernetes/kubectl/issues/16,https://api.github.com/repos/kubernetes/kubectl/issues/16,CrashLoopBackOff while creating Pod,"- I am getting below error while creating a pod : 
![kubernetes dashboard 1](https://cloud.githubusercontent.com/assets/15321333/26358534/7cf46bdc-3ff0-11e7-8bac-f6df9453052d.png)

- yaml file : 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: docker-io-image
spec:
  containers:
    - name: dockertest-web-1
      image: inforian/dockertest_web
      ports:
      - containerPort: 8000
```

- kubectl config : 

```
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T20:33:11Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.0"", GitCommit:""fff5156092b56e6bd60fff75aad4dc9de6b6ef37"", GitTreeState:""clean"", BuildDate:""2017-05-09T23:19:49Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""linux/amd64""}
```",closed,False,2017-05-23 14:17:38,2017-05-31 02:31:31
kubectl,danwinship,https://github.com/kubernetes/kubectl/issues/17,https://api.github.com/repos/kubernetes/kubectl/issues/17,NetworkPolicy describer isn't useful,"""kubectl describe"" isn't useful for NetworkPolicies. @thockin says so!
",closed,False,2017-05-26 12:03:32,2017-10-30 22:38:36
kubectl,fabianofranz,https://github.com/kubernetes/kubectl/issues/18,https://api.github.com/repos/kubernetes/kubectl/issues/18,Cached discovery issue on windows,"From the master branch at least (need to check previous versions), `kubectl get` and potentially a number of other commands are failing miserably on Windows:

```
$ kubectl get pod -v 6
REDACTED
I0518 16:51:47.561501    4196 cached_discovery.go:134] failed to write cache to C:\Users\star\.kube\$master_cluster\servergroups.json due to chmod C:\Users\star\.kube\$master_cluster\servergroups.json.689571775: not supported by windows
```
",closed,False,2017-05-29 21:00:28,2018-08-28 02:25:13
kubectl,ddcprg,https://github.com/kubernetes/kubectl/issues/19,https://api.github.com/repos/kubernetes/kubectl/issues/19,kubectl run --env option restricts the format of the variable values,"**Kubernetes version**:

```
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.3"", GitCommit:""0480917b552be33e2dba47386e51decb1a211df6"", GitTreeState:""clean"", BuildDate:""2017-05-10T23:29:08Z"", GoVersion:""go1.8.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.0"", GitCommit:""fff5156092b56e6bd60fff75aad4dc9de6b6ef37"", GitTreeState:""clean"", BuildDate:""2017-05-09T23:22:45Z"", GoVersion:""go1.7.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **OS**: Mac OS Sierra v10.12.4
- **Kernel**: Darwin mylaptop.localdomain 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64 i386 MacBookPro11,2 Darwin
- **Others**:
    - minikube version: v0.19.0
    - Docker version 17.03.1-ce, build c6d412e

**What happened**:

I'm invoking `kubectl run` with the `--env` option. For my deployment I must pass a JSON string as the value of an environment variable. It seems like `kubectl` doesn't accept image environment variable values that are not C identifiers although If my JSON string has only one attribute then `kubectl` accepts the command. The 2 commands can be found in the *How to reproduce it* section below.

The relevant code seems:

https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/run.go#L863

https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/util/validation/validation.go#L178

**What you expected to happen**:

The environment variable should have been accepted and the image started.

**How to reproduce it**:

This works:

```
$> kubectl run consul --image=consul:0.8.3 --image-pull-policy=IfNotPresent --port=8500 --env=""CONSUL_LOCAL_CONFIG={'acl_datacenter':'dc1'}""
deployment ""consul"" created
```

This does not:

```
$> kubectl run consul --image=consul:0.8.3 --image-pull-policy=IfNotPresent --port=8500 --env=""CONSUL_LOCAL_CONFIG={'acl_datacenter':'dc1','acl_default_policy':'allow','acl_down_policy':'extend-cache','acl_master_token':'the_one_ring','bootstrap_expect':1,'datacenter':'dc1','data_dir':'/usr/local/bin/consul.d/data','server':true}""
error: invalid env: 'acl_default_policy':'allow'
```

With the latter, even adding a single attribute seems to prevent `kubectl` from processing the command - perhaps related to the comma in the string.

**Anything else we need to know**:

All this is running in a local development environment on my laptop. The versions of the software I'm using are specified above.
",closed,False,2017-05-30 15:46:49,2017-07-12 16:42:32
kubectl,xiangpengzhao,https://github.com/kubernetes/kubectl/issues/20,https://api.github.com/repos/kubernetes/kubectl/issues/20,Mark deprecated commands in 'kubectl help',"We have several deprecated commands. When running `kubectl help`, we get below (I don't list other commands here).

```
Basic Commands (Beginner):
  run            Run a particular image on the cluster
  run-container  Run a particular image on the cluster

Deploy Commands:
  rolling-update Perform a rolling update of the given ReplicationController
  rollingupdate  Perform a rolling update of the given ReplicationController
  scale          Set a new size for a Deployment, ReplicaSet, Replication Controller, or Job
  resize         Set a new size for a Deployment, ReplicaSet, Replication Controller, or Job

Cluster Management Commands:
  cluster-info   Display cluster info
  clusterinfo    Display cluster info

Advanced Commands:
  replace        Replace a resource by filename or stdin
  update         Replace a resource by filename or stdin
```

For new end-users, they are confused by the different commands with the same usage. They might be not sure which one to use until they get `Command ""foo"" is deprecated, use ""bar"" instead`. We need to mark the deprecated commands in the help info.

I will create a PR to mark them in help info. However, I wonder if we still need to keep these deprecated commands. Most of them have been deprecated for a long time. For example, `rollingupdate` was marked in https://github.com/kubernetes/kubernetes/pull/6118 two years ago. We might want to delete them.

@bgrant0607 Do we have deprecation policy for such case?


",closed,False,2017-05-31 09:20:24,2017-06-23 10:27:08
kubectl,jjfiguep,https://github.com/kubernetes/kubectl/issues/21,https://api.github.com/repos/kubernetes/kubectl/issues/21,Error pulling images from external registry when the user changes,"**What keywords did you search in Kubernetes issues before filing this one?**
docker login private registry 

**Is this a BUG REPORT or FEATURE REQUEST?** BUG REPORT

**Kubernetes version**:
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.0"", GitCommit:""87d9d8d7bc5aa35041a8ddfe3d4b367381112f89"", GitTreeState:""clean"", BuildDate:""2016-12-12T21:10:52Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.0"", GitCommit:""87d9d8d7bc5aa35041a8ddfe3d4b367381112f89"", GitTreeState:""clean"", BuildDate:""2016-12-12T21:10:52Z"", GoVersion:""go1.6.2"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**: Private virtual servers.
- **OS** (e.g. from /etc/os-release):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.3 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VERSION_ID=""7.3""
PRETTY_NAME=""Red Hat Enterprise Linux Server 7.3 (Maipo)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.3:GA:server""
HOME_URL=""https://www.redhat.com/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""
REDHAT_BUGZILLA_PRODUCT_VERSION=7.3
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""
REDHAT_SUPPORT_PRODUCT_VERSION=""7.3""

- **Kernel** (e.g. `uname -a`):
Linux <hostname> 3.10.0-514.2.2.el7.x86_64 #1 SMP Wed Nov 16 13:15:13 EST 2016 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**:
- **Others**:


**What happened**:
1.- Having a Kubernetes cluster using a private V2 docker registry, basic http authentication and following this steps (https://kubernetes.io/docs/concepts/containers/images/#configuring-nodes-to-authenticate-to-a-private-repository) to configure the authentication, everything was working as spected.
2.- The registry has two users granted to download images, let´s say user1 and user2 (both with the same privileges) but only user1 is configured along all kube-nodes, as said before everything ok.
3.- For some reason has been un-granted user1 from docker registry, as expected, images pull starts to fail.
4.- The problem comes here, changing all kube-nodes configuration to authenticate with user2, it doesn´t work. It is observed in pod status: ImagePullBackOff and in pod events:
Error syncing pod, skipping: failed to ""StartContainer"" for ""imagexxx"" with ErrImagePull: ""manifest unknown: The named manifest is not known to the registry.""
5.- Manual pull from kube-nodes (docker pull ...) works.
6.- Restoring user1 in docker registry (keeping user2 in all nodes) it start to work.

Seems that the first configuration keeps cached somewhere in kubertentes.


**What you expected to happen**:
Kubernetes should get and use the new configuration related with registry authentication.

**How to reproduce it** (as minimally and precisely as possible):
1.- Grant two users (user1 and user2) with the same privileges to download images in docker private registry.
2.- Configure all kube-nodes authentication (docker login <registry_url>) with user1
3.- Deploy something pulling some image from the registry.
4.- Ungrant user1 from docker registry configuration and configure all kube-nodes with user2.
5.- Deploy something forcing to download a new image.


**Anything else we need to know**:
I have restarted all kube services in master and slave, even I have restarted the server but it didn´t help.

",closed,False,2017-06-01 10:56:25,2017-06-01 12:00:53
kubectl,droot,https://github.com/kubernetes/kubectl/issues/22,https://api.github.com/repos/kubernetes/kubectl/issues/22,kubectl Get command should use open-api extension to display a resource,"kubectl Get command does not provide a rich experience for resources retrieved through federated apiservers and types not compiled into the kubectl binary.

Open-api schema for resources retrieved through federated api servers will have additional metadata as  x-kubernetes-print-columns extension. Kubectl should make use of the extension information to display richer output for such types. So behavior is: if user has not specified any custom output format and x-kubernetes-print-column extension is defined for that type, use extension information to format the output for that resource.

This is part of the detailed proposal described [here](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/sig-cli/get-describe-apiserver-extensions.md) 

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FEATURE REQUEST



",closed,False,2017-06-01 22:07:30,2017-06-07 14:37:58
kubectl,alkar,https://github.com/kubernetes/kubectl/issues/23,https://api.github.com/repos/kubernetes/kubectl/issues/23,serviceAccountName default value,"**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4"", GitCommit:""d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae"", GitTreeState:""clean"", BuildDate:""2017-05-19T20:41:07Z"", GoVersion:""go1.8.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2+coreos.0"", GitCommit:""79fee581ce4a35b7791fdd92e0fc97e02ef1d5c0"", GitTreeState:""clean"", BuildDate:""2017-04-19T23:13:34Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release):
```
NAME=""Container Linux by CoreOS""
ID=coreos
VERSION=1353.8.0
VERSION_ID=1353.8.0
BUILD_ID=2017-05-30-2322
PRETTY_NAME=""Container Linux by CoreOS 1353.8.0 (Ladybug)""
ANSI_COLOR=""38;5;75""
HOME_URL=""https://coreos.com/""
BUG_REPORT_URL=""https://issues.coreos.com""
```

- **Kernel** (e.g. `uname -a`):
```
Linux ip-10-66-21-135.eu-west-1.compute.internal 4.9.24-coreos #1 SMP Tue May 30 23:12:01 UTC 2017 x86_64 Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz GenuineIntel GNU/Linux
```

- **Install tools**: `terraform`
- **Others**: N/A


**What happened**:
1. Given the following manifest:
```
---
kind: Namespace
apiVersion: v1
metadata:
  name: test
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-sa
  namespace: test
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: test-deployment
  namespace: test
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: test-app
    spec:
      containers:
        - name: test-container
          image: gcr.io/google_containers/pause:1.0
```

```
$ kubectl apply -f test.yaml
namespace ""test"" configured
serviceaccount ""test-sa"" configured
deployment ""test-deployment"" created
```

2. Modify the manifest to specify a `serviceAccountName`:

```
      serviceAccountName: test-sa
```

Applying again with kubectl will update the deployment and cause the running pod to be replaced, as expected.

3. Modify the manifest again and remove the `serviceAccountName`, apply again. The deployment is not updated:

```
$ kubectl -ntest describe deployment test-deployment | grep 'Service Account'
  Service Account:	test-sa
```

**What you expected to happen**:
I expected the deployment to be updated to use the default namespace Service Account again.

**How to reproduce it** (as minimally and precisely as possible):
See the steps above.


",closed,False,2017-06-06 12:54:38,2017-06-14 18:38:08
kubectl,minherz,https://github.com/kubernetes/kubectl/issues/24,https://api.github.com/repos/kubernetes/kubectl/issues/24,kubectl does not set container name according to deployment manifest,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T20:22:08Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T20:22:08Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}


**Environment**:
- **Cloud provider or hardware configuration**: Azure
- **OS** (e.g. from /etc/os-release):
```
NAME=""Ubuntu""
VERSION=""16.04.2 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.2 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
```
- **Kernel** (e.g. `uname -a`): `Linux k8s-master-42728370-0 4.4.0-77-generic #98-Ubuntu SMP Wed Apr 26 08:34:02 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux``
- **Install tools**:
- **Others**: The host is kubernetes master host


**What happened**:
When deploying a manifest that defines `name` for container, the actual name that is assigned is in a form `k8s_dicom-viewer.bd9132cf_dicom-viewer-deployment-665650083-5tl7f_prototype3_765a98cf-4aef-11e7-891b-000d3a304426_35acc9a7` where `dicom-viewer-deployment` is a name of the deployment.

**What you expected to happen**:
I would expect that a container will have a name that was defined in manifest file under spec/template/spec/containers/name

**How to reproduce it** (as minimally and precisely as possible):
1. create kubernetes cluster using acs-engine
2. deploy a container with a name

**Anything else we need to know**:
The cluster has Windows agent nodes.
Additionally, it is impossible to pull logs (via kubernetes dashboard). It shows `Get https://42728acs9001:10250/containerLogs/prototype3/minio-deployment-3747382929-07jk9/minio?timestamps=true: dial tcp: lookup 42728acs9001 on 10.7.224.100:53: no such host` error message.",closed,False,2017-06-06 20:31:09,2017-06-16 23:04:02
kubectl,tonglil,https://github.com/kubernetes/kubectl/issues/25,https://api.github.com/repos/kubernetes/kubectl/issues/25,Unable to kubectl get -o jsonpath annotation value,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): `kubectl get annotation json`

This feature request seems like it would help in this scenario: https://github.com/kubernetes/kubernetes/issues/19817

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG

**Kubernetes version** (use `kubectl version`):
<details><summary>Output:</summary>

```
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4"", GitCommit:""d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae"", GitTreeState:""clean"", BuildDate:""2017-05-19T18:44:27Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4"", GitCommit:""d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae"", GitTreeState:""clean"", BuildDate:""2017-05-19T18:33:17Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
```
</details>

**Environment**:
- **Cloud provider or hardware configuration**: GKE
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:
Unable to get annotation value when there is a `/` in the annotation key.

**What you expected to happen**:
Get the annotation value (see below).

**How to reproduce it** (as minimally and precisely as possible):
<details><summary>Data:</summary>

```
{
    ""apiVersion"": ""extensions/v1beta1"",
    ""kind"": ""Ingress"",
    ""metadata"": {
        ""annotations"": {
            ""description"": ""my frontend"",
            ""ingress.gcp.kubernetes.io/pre-shared-cert"": ""tony-cert-1"",
            ""ingress.kubernetes.io/backends"": ""{\""k8s-be-30237--d785be79bbf6d463\"":\""HEALTHY\""}"",
            ""ingress.kubernetes.io/https-forwarding-rule"": ""k8s-fws-default-echo-app-tls-2--d785be79bbf6d463"",
            ""ingress.kubernetes.io/https-target-proxy"": ""k8s-tps-default-echo-app-tls-2--d785be79bbf6d463"",
            ""ingress.kubernetes.io/ssl-cert"": ""tony-cert-1"",
            ""ingress.kubernetes.io/url-map"": ""k8s-um-default-echo-app-tls-2--d785be79bbf6d463"",
            ""kubectl.kubernetes.io/last-applied-configuration"": ""{\""apiVersion\"":\""extensions/v1beta1\"",\""kind\"":\""Ingress\"",\""metadata\"":{\""annotations\"":{\""ingress.gcp.kubernetes.io/pre-shared-cert\"":\""tony-cert-1\"",\""kubernetes.io/ingress.allow-http\"":\""false\"",\""kubernetes.io/ingress.global-static-ip-name\"":\""make-static\""},\""name\"":\""echo-app-tls-2\"",\""namespace\"":\""default\""},\""spec\"":{\""backend\"":{\""serviceName\"":\""echo-app\"",\""servicePort\"":88}}}\n"",
            ""kubernetes.io/ingress.allow-http"": ""false"",
            ""kubernetes.io/ingress.global-static-ip-name"": ""make-static""
        },
        ""creationTimestamp"": ""2017-05-24T19:57:16Z"",
        ""generation"": 1,
        ""name"": ""echo-app-tls-2"",
        ""namespace"": ""default"",
        ""resourceVersion"": ""17247450"",
        ""selfLink"": ""/apis/extensions/v1beta1/namespaces/default/ingresses/echo-app-tls-2"",
        ""uid"": ""2fe93467-40bb-11e7-a242-42010a8000f8""
    },
    ""spec"": {
        ""backend"": {
            ""serviceName"": ""echo-app"",
            ""servicePort"": 88
        }
    },
    ""status"": {
        ""loadBalancer"": {
            ""ingress"": [
                {
                    ""ip"": ""..............""
                }
            ]
        }
    }
}
```
</details>

<details><summary>Works:</summary>

```
$ kubectl get ing/echo-app-tls-2 -o jsonpath='{.metadata.annotations.description}'
my frontend
```
</details>

<details><summary>Does not work:</summary>

```
$ kubectl get ing/echo-app-tls-2 -o jsonpath='{.metadata.annotations.""ingress.kubernetes.io/url-map""}'
<blank>

$ kubectl get ing/echo-app-tls-2 -o jsonpath='{.metadata.annotations.ingress.kubernetes.io/url-map}'
<blank>

$ kubectl get ing/echo-app-tls-2 -o jsonpath=""{.metadata.annotations.'ingress.kubernetes.io/url-map'}""
<blank>

$ kubectl get ing/echo-app-tls-2 -o jsonpath=""{.metadata.annotations[ingress.kubernetes.io/url-map]}""
error: error parsing jsonpath {.metadata.annotations[ingress.kubernetes.io/url-map]}, invalid array index ingress.kubernetes.io/url-map

$ kubectl get ing/echo-app-tls-2 -o jsonpath=""{.metadata.annotations['ingress.kubernetes.io/url-map']}""
<blank>

$ kubectl get ing/echo-app-tls-2 -o jsonpath='{.metadata.annotations[""ingress.kubernetes.io/url-map""]}'
error: error parsing jsonpath {.metadata.annotations[""ingress.kubernetes.io/url-map""]}, invalid array index ""ingress.kubernetes.io/url-map""
```
</details>

**Anything else we need to know**:
Tested the above data with https://jsonpath.curiousconcept.com/ and these selectors were able to get the value:
- `.metadata.annotations['ingress.kubernetes.io/url-map']`
- `.metadata.annotations.['ingress.kubernetes.io/url-map']`",closed,False,2017-06-07 21:16:47,2018-07-17 09:14:33
kubectl,pmorie,https://github.com/kubernetes/kubectl/issues/26,https://api.github.com/repos/kubernetes/kubectl/issues/26,kubectl get should support PodPreset,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

podpreset

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

bug; 

**Kubernetes version** (use `kubectl version`):

1.7


**What happened**:

`kubectl get podpresets -n test-ns` yields raw output:

```
$ k get podpreset my-pod-preset -n test-ns
error: unknown type &settings.PodPreset{TypeMeta:v1.TypeMeta{Kind:"""", APIVersion:""""}, ObjectMeta:v1.ObjectMeta{Name:""my-pod-preset"", GenerateName:"""", Namespace:""test-ns"", SelfLink:""/apis/settings.k8s.io/v1alpha1
/namespaces/test-ns/podpresets/my-pod-preset"", UID:""0d2170ab-4ba9-11e7-b784-68f728db1985"", ResourceVersion:""1441"", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{sec:63632454309, nsec:0, loc:(*time.Locat
ion)(0x2bd6c00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finali
zers:[]string(nil), ClusterName:""""}, Spec:settings.PodPresetSpec{Selector:v1.LabelSelector{MatchLabels:map[string]string{""app"":""my-app""}, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}, Env:[]api.EnvVar(ni
l), EnvFrom:[]api.EnvFromSource{api.EnvFromSource{Prefix:"""", ConfigMapRef:(*api.ConfigMapEnvSource)(nil), SecretRef:(*api.SecretEnvSource)(0xc420ab8d00)}}, Volumes:[]api.Volume(nil), VolumeMounts:[]api.VolumeMou
nt(nil)}}
```

**What you expected to happen**:

Expected normal formatted columns

**How to reproduce it** (as minimally and precisely as possible):

Make a podpreset, `kubectl get` it
",closed,False,2017-06-08 20:23:32,2017-06-13 21:58:27
kubectl,tomfotherby,https://github.com/kubernetes/kubectl/issues/27,https://api.github.com/repos/kubernetes/kubectl/issues/27,kubectl patch initContainers image doesn't work,"**Is this a request for help?** : Nope

**What keywords did you search in Kubernetes issues before filing this one?** : ""patch""

---

**Is this a BUG REPORT or FEATURE REQUEST?** : BUG REPORT? 

**Kubernetes version** (use `kubectl version`): v1.6.2

```
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T20:33:11Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4+coreos.0"", GitCommit:""8996efde382d88f0baef1f015ae801488fcad8c4"", GitTreeState:""clean"", BuildDate:""2017-05-19T21:11:20Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: aws
- **OS** (e.g. from /etc/os-release): CoreOS
- **Install tools**: [Tack](https://github.com/kz8s/tack)


**What happened**:

I have a pod that contains nginx and php-fpm containers that need to reference the same sourcecode. To do this I use a [`initContainers`](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) to copy some sourcecode to a `emptyDir` volume. It works nicely.

To deploy a new version of the app, the only thing that needs changing is the image of the container in the initContainers. I would like to be able to patch it like this:

    $ kubectl patch deployment thedeployment -p'{""spec"":{""template"":{""spec"":{""initContainers"":[{""name"":""init-sourcecode"",""image"":""vendor/app_data:master.53""}]}}}}'

It returns: 

> deployment ""thedeployment"" patched
 
but checking with `kubectl get deployment thedeployment -o json | less` shows it's not the case.

**What you expected to happen**:
The deployment should be updated with a the new image. (and a new deployment triggered)

**How to reproduce it** :

```
$ cat nginx-test.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-test-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      initContainers:
      - name: init-webpage
        image: busybox
        command: [""sleep"", ""3""]
        volumeMounts:
        - mountPath: /work-dir
          name: workdir

      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
      dnsPolicy: Default
      volumes:
      - name: workdir
        emptyDir: {}
```

    kubectl create -f nginx-test.yaml

Try to patch the image to change from `busybox:latest` to `busybox:1.25`:

    kubectl patch deployment nginx-test-deployment -p'{""spec"":{""template"":{""spec"":{""initContainers"":[{""name"":""init-webpage"",""image"":""busybox:1.25""}]}}}}'

> deployment ""nginx-test-deployment"" patched

Even though it says it's patched, check busybox image in the deployment to see it's not changed with:

    kubectl get deployment nginx-test-deployment -o json | grep busybox",closed,False,2017-06-09 15:05:01,2017-06-09 23:29:48
kubectl,derekwaynecarr,https://github.com/kubernetes/kubectl/issues/28,https://api.github.com/repos/kubernetes/kubectl/issues/28,kubectl drain errors if a pod has already been deleted,"**Kubernetes version** (use `kubectl version`):
kubernetes/master

**What happened**:
kubectl drain of a node threw an error when a pod was already deleted.

**What you expected to happen**:
kubectl drain should not error if the pod it attempts to delete is already deleted

**How to reproduce it** (as minimally and precisely as possible):
create a pod, run kubectl drain on the node, delete the pod prior to drain completing on that node.

**Anything else we need to know**:
nope.  i have a pr prepared with a fix.",closed,False,2017-06-09 21:00:20,2017-06-12 07:44:07
kubectl,brendandburns,https://github.com/kubernetes/kubectl/issues/29,https://api.github.com/repos/kubernetes/kubectl/issues/29,Optionally use azure active directory for kubectl auth.,Kubectl authentication is pluggable. Azure Active Directory is a popular authentication service for both cloud and on-premise identity management. Kubectl should support azure active directory authentication to enable those users to continue to use their existing identity management solutions.,closed,False,2017-06-13 04:18:36,2017-06-14 17:23:18
kubectl,timothysc,https://github.com/kubernetes/kubectl/issues/30,https://api.github.com/repos/kubernetes/kubectl/issues/30,"BUG: Info log on every kubectl command ""duplicate proto type registered""","**This a BUG REPORT** (choose one):
```
$ kubectl version
2017-06-13 09:32:31.930035 I | proto: duplicate proto type registered: google.protobuf.Any
2017-06-13 09:32:31.930096 I | proto: duplicate proto type registered: google.protobuf.Duration
2017-06-13 09:32:31.930111 I | proto: duplicate proto type registered: google.protobuf.Timestamp
```

**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.0"", GitCommit:""dfaba882698e26a47fb769be45fe5c048a9fe4ad"", GitTreeState:""clean"", BuildDate:""2017-06-13T01:15:53Z"", GoVersion:""go1.8.1"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7+"", GitVersion:""v1.7.0-beta.1"", GitCommit:""dfaba882698e26a47fb769be45fe5c048a9fe4ad"", GitTreeState:""clean"", BuildDate:""2017-06-08T01:43:08Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:
Every kubectl command outputs a proto info message about dbl registration. 
```
2017-06-13 09:32:31.930035 I | proto: duplicate proto type registered: google.protobuf.Any
2017-06-13 09:32:31.930096 I | proto: duplicate proto type registered: google.protobuf.Duration
2017-06-13 09:32:31.930111 I | proto: duplicate proto type registered: google.protobuf.Timestamp
```

**What you expected to happen**:
No output

**How to reproduce it** (as minimally and precisely as possible):
Run any kubectl command. 
",closed,False,2017-06-13 14:37:46,2018-11-28 14:23:26
kubectl,pmorie,https://github.com/kubernetes/kubectl/issues/31,https://api.github.com/repos/kubernetes/kubectl/issues/31,Kubectl should warn when resources aren't found and discovery failed,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

Aggregator

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

```
$ k version
Client Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.0-alpha.0.527+bb877f1ee6b30c"", GitCommit:""bb877f1ee6b30c0f70068aa5ffc4324e6443c89c"", GitTreeState:""clean"", BuildDate:""2017-06-13T12:26:12Z"", GoVersion:""go1.8"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.0-alpha.0.527+bb877f1ee6b30c"", GitCommit:""bb877f1ee6b30c0f70068aa5ffc4324e6443c89c"", GitTreeState:""clean"", BuildDate:""2017-06-13T12:21:56Z"", GoVersion:""go1.8"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:

I set up an API service with the aggregator that had an SSL certificate which was signed with the wrong DNS name.  The API service resource had a good status, but I would get 'resource not found' errors from kubectl trying to use it.  Eventually with @deads2k's help we found that there were discovery errors against the aggregated API group.  See a gist with relevant details here: https://gist.github.com/pmorie/0e8d5ba6c43d0c7000cd05a39a8ae190

**What you expected to happen**:

I would expect kubectl to warn that discovery failed instead of just saying that the resource didn't exist.

**How to reproduce it** (as minimally and precisely as possible):

Set up an API service resource with an incorrectly signed cert and try to use one of the aggregated resources.
",open,False,2017-06-13 14:39:44,2019-03-24 21:56:18
kubectl,alexandercampbell,https://github.com/kubernetes/kubectl/issues/32,https://api.github.com/repos/kubernetes/kubectl/issues/32,Too many functions are coupled to cobra.Command,"Many of the functions in `kubectl/cmd` require instances of `cobra.Command`, a complex and occasionally counterintuitive struct. In many cases the business logic of our application is very tightly coupled to `cobra.Command`.

This is bad for a few reasons:
1. Switching to an alternative configuration system for any reason is impossible.
2. Testing even simple methods requires ""stubbing"" `&cobra.Command{}`.
3. We can't refactor similar commands to use a common codepath (as in #11) without introducing unpredictability due to the differences in the Command instances.

`cobra.Command` would be better used as a value retrieval system.",open,False,2017-06-14 19:42:01,2018-09-25 20:20:39
kubectl,k82cn,https://github.com/kubernetes/kubectl/issues/33,https://api.github.com/repos/kubernetes/kubectl/issues/33,kubectl did not print readable message of PVC's capacity.,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
Bug

**Kubernetes version** (use `kubectl version`):
master branch

**What happened**:

Here's the output when we print `Quantity` as string.

```
Volume Claims:
  Name:		www
  StorageClass:	anything
  Labels:	<none>
  Annotations:	volume.beta.kubernetes.io/storage-class=anything
  Capacity:	{{%!s(int64=1073741824) %!s(resource.Scale=0)} {%!s(*inf.Dec=<nil>)} 1Gi BinarySI}
  Access Modes:	[ReadWriteOnce]
```

**What you expected to happen**:
print `1Gi`

**How to reproduce it** (as minimally and precisely as possible):
`kubectl describe statefulsets`

/assign


xref https://github.com/kubernetes/kubernetes/issues/47571",closed,False,2017-06-15 05:18:04,2017-06-21 09:18:57
kubectl,hasanatkazmi,https://github.com/kubernetes/kubectl/issues/34,https://api.github.com/repos/kubernetes/kubectl/issues/34,`kubectl top nodes/pods` doesn't show Storage usage,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

Mostly feature

On running,
>> kubectl top -h
Display Resource (CPU/Memory/Storage) usage. 

Though it says but there is no option to get storage information. 
Storage information should also be displayed by `kubectl top nodes/pods` 

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

>> kubectl top -h
Display Resource (CPU/Memory/Storage) usage. 

Though it says but there is no option to get storage information. 

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

Storage information should also be displayed by `kubectl top nodes/pods` 

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

1.6

**Environment**:
- **Cloud provider or hardware configuration**:
GKE
- **OS** (e.g. from /etc/os-release):
mac
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:


**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:


",closed,False,2017-06-17 00:40:51,2017-08-09 21:44:35
kubectl,sandoracs,https://github.com/kubernetes/kubectl/issues/35,https://api.github.com/repos/kubernetes/kubectl/issues/35,cmd.Flags().Set() accepts invalid configurations in create_service_test.go,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
Bug report

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.0-alpha.0.644+2ddde09a6ca650-dirty"", GitCommit:""2ddde09a6ca6504f200572b44282dc9983641618"", GitTreeState:""dirty"", BuildDate:""2017-06-16T20:43:23Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4"", GitCommit:""d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae"", GitTreeState:""clean"", BuildDate:""2017-05-30T22:03:41Z"", GoVersion:""go1.7.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**:  MacBook Pro mid 2015
- **OS** (e.g. from /etc/os-release): OS X 10.11.6 (15G1510)
- **Kernel** (e.g. `uname -a`): 15.6.0 Darwin Kernel Version 15.6.0

**What happened**:
cmd.Flags().Set() accepts invalid configurations in pkg/kubectl/cmd/create_service_test.go, e.g. cmd.Flags().Set(""tcp"", ""8080:X"") 

**What you expected to happen**:
Invalid configs should fail in tests

**How to reproduce it** (as minimally and precisely as possible):
Change cmd.Flags().Set(""tcp"", ""8080:8080"") to cmd.Flags().Set(""tcp"", ""8080:X"") in pkg/kubectl/cmd/create_service_test.go and run go test
",closed,False,2017-06-17 10:59:17,2018-01-29 17:17:48
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/36,https://api.github.com/repos/kubernetes/kubectl/issues/36,Kubectl upgrade tests failing 1.6 -> 1.7,"https://k8s-testgrid.appspot.com/1.6-1.7-upgrade#gke-cvm-upgrade-cluster&sort-by-failures=

> [k8s.io] Kubectl client [k8s.io] Kubectl describe should check if kubectl describe prints relevant information for rc and pods [Conformance]",closed,False,2017-06-19 21:25:30,2017-06-19 23:54:10
kubectl,ryanwalls,https://github.com/kubernetes/kubectl/issues/37,https://api.github.com/repos/kubernetes/kubectl/issues/37,Current bash-completion instructions do not work on Mac OS 10.11.6 and bash 3.2,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):  ""bash-completion""

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): Bug Report

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.4"", GitCommit:""7243c69eb523aa4377bce883e7c0dd76b84709a1"", GitTreeState:""clean"", BuildDate:""2017-03-07T23:53:09Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""darwin/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Local env
- **OS** (e.g. from /etc/os-release):  Mac OS 10.11.6 
- **Kernel** (e.g. `uname -a`): 
- **Install tools**:  
- **Others**:  bash 3.2


**What happened**:
Following instructions here https://kubernetes.io/docs/tasks/tools/install-kubectl/#on-macos-using-bash, bash autocompletion did not work for me.  

**What you expected to happen**:
Expected bash-completion to work.

**How to reproduce it** (as minimally and precisely as possible):
* Manually install kubectl
* Install bash-completion using homebrew
* `source <(kubectl completion bash)`

To get my bash completion to work, I had to follow a final step that is hinted at here: https://kubernetes.io/docs/user-guide/kubectl/v1.6/#completion.  I had to write the bash completion output to a file and then source that file.  

```
kubectl completion bash > ~/.kube/completion.bash.inc
printf ""\n# Kubectl shell completion\nsource '$HOME/.kube/completion.bash.inc'\n"" >> $HOME/.bash_profile
source $HOME/.bash_profile
```

**Anything else we need to know**:
Seems at least one other person has experienced this, but I couldn't find an open issue: https://github.com/kubernetes/kubernetes/issues/27876#issuecomment-272137153
",closed,False,2017-06-20 13:40:13,2019-01-30 08:35:14
kubectl,aguilbau,https://github.com/kubernetes/kubectl/issues/38,https://api.github.com/repos/kubernetes/kubectl/issues/38,inconsistencies when using flag '-o json',"**kubernetes version**
```
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4"", GitCommit:""d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae"", GitTreeState:""clean"", BuildDate:""2017-05-19T20:41:24Z"", GoVersion:""go1.8.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4+coreos.0"", GitCommit:""8996efde382d88f0baef1f015ae801488fcad8c4"", GitTreeState:""clean"", BuildDate:""2017-05-19T21:11:20Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- aws
- coreos
- `Linux ip-10-0-65-22 4.11.2-coreos #1 SMP Tue May 23 22:04:34 UTC 2017 x86_64 Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz GenuineIntel GNU/Linux`
- tectonic installer:


**What happened**:
I have a daemonset, running on 5 nodes. When I kill a pod of this daemonset and get it's status
with kubectl, I will notice inconsistencies with the json output ( `-o json` )

For example, when I kill the pod and run
`kubectl get pod ""$POD_NAME""`,
it's status will be `Terminating`

But, killing it and running
`kubectl get pod ""$POD_NAME"" -o json | jq '.status.phase'`,
I will get `Running`

Also, as I said, this pod is from a daemonset.
When I do `kubectl get daemonset ""$DAEMONSET_NAME"" -o json`,
the output of `status` will sometime be inconsistent too.
for example, after killing a pod and waiting for it to be up again ( status Running ),
the status of the daemonset will look like that
```
    ""status"": {
        ""currentNumberScheduled"": 5,
        ""desiredNumberScheduled"": 5,
        ""numberAvailable"": 4,
        ""numberMisscheduled"": 0,
        ""numberReady"": 4,
        ""numberUnavailable"": 1,
        ""observedGeneration"": 3,
        ""updatedNumberScheduled"": 5
    }
```

**What you expected to happen**:

Both command should give me the same result, ie `Terminating`

**How to reproduce it** (as minimally and precisely as possible):

- kill a pod
- get it's status using `kubectl get pod ""$POD_NAME""`
- kill another pod
- get it's status using `kubectl get pod ""$POD_NAME"" -o json | jq '.status.phase'`

",closed,False,2017-06-21 07:58:41,2017-06-22 01:10:05
kubectl,cameronbrunner,https://github.com/kubernetes/kubectl/issues/39,https://api.github.com/repos/kubernetes/kubectl/issues/39,"Scaling a deployment always sets 'schedulerName' to 'default-scheduler' (kubectl 1.5.7, master 1.6.4)","<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
'schedulerName', 'scale'

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```
$ ./kubectl-1.5.7 version
Client Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.7"", GitCommit:""8eb75a5810cba92ccad845ca360cf924f2385881"", GitTreeState:""clean"", BuildDate:""2017-04-27T10:00:30Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4"", GitCommit:""d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae"", GitTreeState:""clean"", BuildDate:""2017-05-19T18:33:17Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: GKE
- **OS** (e.g. from /etc/os-release): client: darwin and linux(alpine 3.5.2).  Server Debian 7.
- **Kernel** (e.g. `uname -a`): client darwin 15.6.0 and Debian 3.16.39-1.  Server Debian 3.16.39-1
- **Install tools**: GKE for master.  Curl'ed version of kubectl from GCS
- **Others**:


**What happened**:
When scaling a replication controller or deployment a using version 1.5.7 of kubectl against a 1.6.4 master`kubectl scale` sets the schedulerName attribute of podspec within the controller to 'default-scheduler' regardless of the original value.  All new pods created by the controller have their schedulerName attribute then set to 'default-scheduler'.

**What you expected to happen**:
The value of 'schedulerName' should not be set to default-scheduler.

**How to reproduce it** (as minimally and precisely as possible):
Complete example showing the problem:
```
$ cat deployment.yaml 
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2 
  template:
    metadata:
      labels:
        app: nginx
    spec:
      schedulerName: third-party
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

$ ./kubectl-1.5.7 create -f deployment.yaml 
deployment ""nginx-deployment"" created
$ ./kubectl-1.5.7 get deployment nginx-deployment -o yaml | grep schedulerName
      schedulerName: third-party
$ ./kubectl-1.5.7 scale deployment nginx-deployment --replicas=3
deployment ""nginx-deployment"" scaled
$ ./kubectl-1.5.7 get deployment nginx-deployment -o yaml | grep schedulerName
      schedulerName: default-scheduler
```

**Anything else we need to know**:
The system works as expected if you have a 1.6.4 version of kubectl.  Additionally I would expect other scalable controllers to have this issue; I just tested with deployments and replication controllers.  As seen in my example the system allows you to create a deployment with a schedulerName set to a value of 'third-party' just the patching of the object during the scale operation seems to cause the problem.
",closed,False,2017-06-21 21:43:59,2017-08-09 21:43:51
kubectl,mnussbaum,https://github.com/kubernetes/kubectl/issues/40,https://api.github.com/repos/kubernetes/kubectl/issues/40,Cannot fetch multi-container pod logs by selector,"**kubectl version**: 

```bash
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.1"", GitCommit:""b0b7a323cc5a4a2019b2e9520c21c7830b7f708e"", GitTreeState:""clean"", BuildDate:""2017-04-03T20:44:38Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.1"", GitCommit:""b0b7a323cc5a4a2019b2e9520c21c7830b7f708e"", GitTreeState:""clean"", BuildDate:""2017-04-03T20:33:27Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:

When attempting to fetch the logs of a pod using a selector kubectl seems to demand a container name, but then does not allow a container name to be passed.

```bash
$ kubectl logs --selector app=kube-dns
Error from server (BadRequest): a container name must be specified for pod kube-dns-2325730542-99bh1, choose one of: [kubedns dnsmasq dnsmasq-metrics healthz]
$ kubectl logs --selector app=kube-dns --container  kubedns
error: a container cannot be specified when using a selector (-l)
```

**What you expected to happen**:

1. With `kubectl logs --selector app=kube-dns` I expected to see the logs for all containers in all my `kube-dns` labeled pods.
2. With `kubectl logs --selector app=kube-dns --container  kubedns` I expected to see the logs for all the `kubedns` containers in all my pod `kube-dns` labeled pods.

Also acceptable would be if one of those two commands failed with one of the above errors, but then the other command worked.

**How to reproduce it**:

Attempt to get logs for a multi-pod container using a selector.

",closed,False,2017-06-22 21:03:08,2018-03-23 14:10:49
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/41,https://api.github.com/repos/kubernetes/kubectl/issues/41,Kubectl caches partially populated api discovery,Kubectl caches the discovery service results which may not be fully populated by the apiserver.  Kubectl should use etags for caching the discovery service results.,closed,False,2017-06-23 23:24:36,2017-08-09 21:41:04
kubectl,AXington,https://github.com/kubernetes/kubectl/issues/42,https://api.github.com/repos/kubernetes/kubectl/issues/42,Namespace flag causes completion error.,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.
-->

**Kubernetes version** (use `kubectl version`):
(Output is current, but this is after I upgraded from 1.6.4 where I first saw the problem)
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.5"", GitCommit:""490c6f13df1cb6612e0993c4c14f2ff90f8cdbf3"", GitTreeState:""clean"", BuildDate:""2017-06-14T20:15:53Z"", GoVersion:""go1.7.6"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.5"", GitCommit:""894ff23729bbc0055907dd3a496afb725396adda"", GitTreeState:""clean"", BuildDate:""2017-03-22T00:17:51Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}


**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): VERSION=""16.04.2 LTS (Xenial Xerus)""
- **Kernel** (e.g. `uname -a`): 4.8.0-56-generic
- **Install tools**:
- **Others**: using zsh


**What happened**:
typed: ```kubectl --namespace=$NAMESPACE g``` then hit tab
kubectl threw an error ```kubectl --namespace=foo g__handle_flag:25: bad math expression: operand expected at end of string```

**What you expected to happen**:
g would complete to 'get'

**How to reproduce it** (as minimally and precisely as possible):
enter ``` kubectl --namespace=foo g``` and hit tab

**Anything else we need to know**:
Does not happen without the --namespace flag
",closed,False,2017-06-28 14:45:15,2017-08-07 00:32:43
kubectl,fiksn,https://github.com/kubernetes/kubectl/issues/43,https://api.github.com/repos/kubernetes/kubectl/issues/43,kubectl --sort-by not working in 1.7.0,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
bug report
<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.0"", GitCommit:""d3ada0119e776222f11ec7945e6d860061339aad"", GitTreeState:""clean"", BuildDate:""2017-06-29T23:15:59Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.0"", GitCommit:""d3ada0119e776222f11ec7945e6d860061339aad"", GitTreeState:""clean"", BuildDate:""2017-06-29T22:55:19Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
Kubernetes cluster is running on CoreOS

VERSION=1409.5.0
VERSION_ID=1409.5.0
BUILD_ID=2017-06-22-2222
PRETTY_NAME=""Container Linux by CoreOS 1409.5.0 (Ladybug)""

Kernel 4.11.6-coreos-r1, but that should be pretty irrelevant as it seems the problem is in kubectl.
Tried on MacOS 10.12.5 with 1.7.0 kubectl and on various GNU/Linux machines. It doesn't work also against a 1.6.6 Kubernetes cluster so I am quite confident this is a client issue.

**What happened**:
$ kubectl get pods --sort-by='{.metadata.name}'
error: unknown type *api.Pod, expected unstructured in map[reflect.Type]*printers.handlerEntry{}

**What you expected to happen**:
A list of pods sorted by name should have appeared as was the case with 1.6.6.",closed,False,2017-07-02 13:55:49,2017-08-09 21:04:24
kubectl,alexandercampbell,https://github.com/kubernetes/kubectl/issues/44,https://api.github.com/repos/kubernetes/kubectl/issues/44,Consolidate the Deployment Generators,"Issue type: refactor

### Description

[`pkg/kubectl/deployment.go`](https://github.com/kubernetes/kubernetes/blob/cb712e41d435dbb42519bded680fef4043dd23b3/pkg/kubectl/deployment.go) is the same 100 lines copy/pasted. `DeploymentBasicAppsGeneratorV1` and `DeploymentBasicGeneratorV1` are identical except they return `extensionsv1beta1.Deployment` and `appsv1beta1.Deployment` respectively.

If I consolidate these structs to use the same codepath, I don't have to make [the same changes twice](https://github.com/alexandercampbell/kubernetes/pull/1) to add new parameters (like `--label`, for instance).

### Acceptance criteria

- [ ] `--label` can be added to the ""kubectl create deployment"" Generators without making the same changes in two places.
- [ ] Generator behavior is exactly the same as it is in master.",closed,False,2017-07-14 16:24:34,2017-07-19 16:48:23
kubectl,abstrctn,https://github.com/kubernetes/kubectl/issues/45,https://api.github.com/repos/kubernetes/kubectl/issues/45,kubectl run --dry-run raises error,"**What keywords did you search in Kubernetes issues before filing this one?**: There were several issues in github.com/kubernetes/kubernetes, chiefly this one that suggested creating an issue in this repo instead, which I'm doing now since nothing seems to exist in this repo: https://github.com/kubernetes/kubernetes/issues/47180

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): Bug report

**Kubernetes version** (use `kubectl version`): kubectl 1.7.0, server 1.7.1-gke.0

**Environment**:
- **Cloud provider or hardware configuration**: GKE 1.7.1

**What happened**:
Running `kubectl run nginx --image=nginx --dry-run`, as suggested in the [docs](https://kubernetes.io/docs/user-guide/kubectl/v1.7/#run), raises an error:

```
error: unknown type *v1beta1.Deployment, expected unstructured in map[reflect.Type]*printers.handlerEntry{(*reflect.rtype)(0x2c435c0):(*printers.handlerEntry)(0xc4203f9b80), (*reflect.rtype)(0x2c442e0):(*printers.handlerEntry)(0xc4206341e0), (*reflect.rtype)(0x2b3ce20):(*printers.handlerEntry)(0xc420635400), (*reflect.rtype)(0x2c40140):(*printers.handlerEntry)(0xc4203f8280), (*reflect.rtype)(0x2c3df20):(*printers.handlerEntry)(0xc4203f8960), (*reflect.rtype)(0x2c403e0):(*printers.handlerEntry)(0xc4203f90e0), (*reflect.rtype)(0x2c3e9a0):(*printers.handlerEntry)(0xc4203f93b0), (*reflect.rtype)(0x2c43da0):(*printers.handlerEntry)(0xc4203f9680), (*reflect.rtype)(0x2b3dc20):(*printers.handlerEntry)(0xc420635680), (*reflect.rtype)(0x2b3d360):(*printers.handlerEntry)(0xc4204fbbd0), (*reflect.rtype)(0x2b3cf00):(*printers.handlerEntry)(0xc4204fbd10), (*reflect.rtype)(0x2b3c480):(*printers.handlerEntry)(0xc4203f8320), (*reflect.rtype)(0x2b3caa0):(*printers.handlerEntry)(0xc4203f99a0), (*reflect.rtype)(0x2b3cfe0):(*printers.handlerEntry)(0xc4203f9860), (*reflect.rtype)(0x2b3bbc0):(*printers.handlerEntry)(0xc4203f8e60), (*reflect.rtype)(0x2b3d440):(*printers.handlerEntry)(0xc4203f9d60), (*reflect.rtype)(0x2b3d600):(*printers.handlerEntry)(0xc4206343c0), (*reflect.rtype)(0x2c45540):(*printers.handlerEntry)(0xc4206355e0), (*reflect.rtype)(0x2c3f960):(*printers.handlerEntry)(0xc4204fb950), (*reflect.rtype)(0x2b3bca0):(*printers.handlerEntry)(0xc4203f88c0), (*reflect.rtype)(0x2b3b680):(*printers.handlerEntry)(0xc4203f95e0), (*reflect.rtype)(0x2b3b840):(*printers.handlerEntry)(0xc4203f86e0), (*reflect.rtype)(0x2c3fea0):(*printers.handlerEntry)(0xc4203f8fa0), (*reflect.rtype)(0x2b3be60):(*printers.handlerEntry)(0xc4203f94a0), (*reflect.rtype)(0x2c457e0):(*printers.handlerEntry)(0xc4206354a0), (*reflect.rtype)(0x2c3d9e0):(*printers.handlerEntry)(0xc4203f9a40), (*reflect.rtype)(0x2c44040):(*printers.handlerEntry)(0xc4203f9cc0), (*reflect.rtype)(0x2b3d980):(*printers.handlerEntry)(0xc420634dc0), (*reflect.rtype)(0x2b3cc60):(*printers.handlerEntry)(0xc4203f81e0), (*reflect.rtype)(0x2b3d0c0):(*printers.handlerEntry)(0xc4203f8460), (*reflect.rtype)(0x2b3c8e0):(*printers.handlerEntry)(0xc4203f85a0), (*reflect.rtype)(0x2c3e460):(*printers.handlerEntry)(0xc4203f8dc0), (*reflect.rtype)(0x2c3ec40):(*printers.handlerEntry)(0xc4203f9270), (*reflect.rtype)(0x2c40bc0):(*printers.handlerEntry)(0xc4206357c0), (*reflect.rtype)(0x2b3cd40):(*printers.handlerEntry)(0xc4204fbe50), (*reflect.rtype)(0x2c3dc80):(*printers.handlerEntry)(0xc4203f8640), (*reflect.rtype)(0x2b3d8a0):(*printers.handlerEntry)(0xc420635130), (*reflect.rtype)(0x2bedce0):(*printers.handlerEntry)(0xc420635720), (*reflect.rtype)(0x2c44ac0):(*printers.handlerEntry)(0xc420634b40), (*reflect.rtype)(0x2b3bf40):(*printers.handlerEntry)(0xc4204fb450), (*reflect.rtype)(0x2c3e700):(*printers.handlerEntry)(0xc4203f8780), (*reflect.rtype)(0x2b3b920):(*printers.handlerEntry)(0xc4203f8a00), (*reflect.rtype)(0x2b3c1e0):(*printers.handlerEntry)(0xc4203f8d20), (*reflect.rtype)(0x2c3d740):(*printers.handlerEntry)(0xc4203f9540), (*reflect.rtype)(0x2c420c0):(*printers.handlerEntry)(0xc4204fbf90), (*reflect.rtype)(0x2b3d520):(*printers.handlerEntry)(0xc4203f9720), (*reflect.rtype)(0x2b3b760):(*printers.handlerEntry)(0xc4203f9ae0), (*reflect.rtype)(0x2c3d200):(*printers.handlerEntry)(0xc4203f9e00), (*reflect.rtype)(0x2c452a0):(*printers.handlerEntry)(0xc4206348c0), (*reflect.rtype)(0x2b3d7c0):(*printers.handlerEntry)(0xc4204fb8b0), (*reflect.rtype)(0x2c43860):(*printers.handlerEntry)(0xc4204fba90), (*reflect.rtype)(0x2c42360):(*printers.handlerEntry)(0xc4204fbdb0), (*reflect.rtype)(0x2b3c2c0):(*printers.handlerEntry)(0xc4203f9040), (*reflect.rtype)(0x2c42de0):(*printers.handlerEntry)(0xc4203f97c0), (*reflect.rtype)(0x2b3dd00):(*printers.handlerEntry)(0xc420635540), (*reflect.rtype)(0x2c41b80):(*printers.handlerEntry)(0xc4203f9900), (*reflect.rtype)(0x2b3d1a0):(*printers.handlerEntry)(0xc4206340a0), (*reflect.rtype)(0x2c45000):(*printers.handlerEntry)(0xc420634500), (*reflect.rtype)(0x2c3f420):(*printers.handlerEntry)(0xc4204fb5e0), (*reflect.rtype)(0x2b3c020):(*printers.handlerEntry)(0xc4204fb6d0), (*reflect.rtype)(0x2c40e60):(*printers.handlerEntry)(0xc4203f8500), (*reflect.rtype)(0x2c3e1c0):(*printers.handlerEntry)(0xc4203f8aa0), (*reflect.rtype)(0x2c3fc00):(*printers.handlerEntry)(0xc4203f8c80), (*reflect.rtype)(0x2b3da60):(*printers.handlerEntry)(0xc420634a00), (*reflect.rtype)(0x2c44820):(*printers.handlerEntry)(0xc4204fb810), (*reflect.rtype)(0x2c42b40):(*printers.handlerEntry)(0xc4204fbc70), (*reflect.rtype)(0x2c43080):(*printers.handlerEntry)(0xc4203f83c0), (*reflect.rtype)(0x2b3c100):(*printers.handlerEntry)(0xc4204fb9f0), (*reflect.rtype)(0x2b3bd80):(*printers.handlerEntry)(0xc4203f9310), (*reflect.rtype)(0x2b3b220):(*printers.handlerEntry)(0xc4203f9ea0), (*reflect.rtype)(0x2c43320):(*printers.handlerEntry)(0xc4203f9f40), (*reflect.rtype)(0x2c44d60):(*printers.handlerEntry)(0xc420634fa0), (*reflect.rtype)(0x2b3db40):(*printers.handlerEntry)(0xc4206346e0), (*reflect.rtype)(0x2c428a0):(*printers.handlerEntry)(0xc420635360), (*reflect.rtype)(0x2c3eee0):(*printers.handlerEntry)(0xc4204fb4a0), (*reflect.rtype)(0x2b3ba00):(*printers.handlerEntry)(0xc4203f8b40), (*reflect.rtype)(0x2b3c3a0):(*printers.handlerEntry)(0xc4203f9180), (*reflect.rtype)(0x2b3d280):(*printers.handlerEntry)(0xc4203f9c20), (*reflect.rtype)(0x2b3c800):(*printers.handlerEntry)(0xc420635860)}
```

**What you expected to happen**:
It should print out the object that would be sent to the server.

**How to reproduce it** (as minimally and precisely as possible):
1. Create a new GKE cluster running version 1.7.1
2. Configure kubectl to use the new cluster as it's default context
3. Run: `kubectl run nginx --image=nginx --dry-run`",closed,False,2017-07-24 19:42:27,2017-10-19 16:00:52
kubectl,foldingbeauty,https://github.com/kubernetes/kubectl/issues/46,https://api.github.com/repos/kubernetes/kubectl/issues/46,A password with special characters is not escaped right when create a docker-registry secret.,"**BUG REPORT**

**Kubernetes version** 

```
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.1"", GitCommit:""1dc5c66f5dd61da08412a74221ecc79208c2165b"", GitTreeState:""clean"", BuildDate:""2017-07-14T02:00:46Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.1"", GitCommit:""1dc5c66f5dd61da08412a74221ecc79208c2165b"", GitTreeState:""clean"", BuildDate:""2017-07-14T01:48:01Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: all cloudproviders
- **OS** : 16.04.3 LTS (Xenial Xerus) 
- **Kernel** : Linux master 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

**What happened**:

When you create a docker-registry secret with a special character you can't pull images. Looks like the special character is not escaped.

For example:

`kubectl create secret docker-registry index.docker.io-v1 --docker-server=https://index.docker.io/v1/ --docker-username=hello --docker-password=password$withspecialcharacter --namespace=test --docker-email=text@example.com`

You can't pull the image because the password is not correctly used. 

When you run this command it works:

`kubectl create secret docker-registry index.docker.io-v1 --docker-server=https://index.docker.io/v1/ --docker-username=hello --docker-password=password\$withspecialcharacter --namespace=test --docker-email=text@example.com`

`\$` instead of `$`

**What you expected to happen**:

I was expected that the special character can be used as a password without adding my own escape character.

**How to reproduce it** :

- Create an account for your private image repo (for example docker hub) 
- Use a special character in the password (for example: `mypassword$help`)
- Create the secret
- Try to deploy a pod with an image from docker hub
- Go get an error can't login to repo because of wrong username/password

**Anything else we need to know**:

Ask if you have any question ;-) Keep up the good work! You are all awesome.
",closed,False,2017-08-03 09:34:32,2019-02-12 10:44:26
kubectl,DarqueWarrior,https://github.com/kubernetes/kubectl/issues/47,https://api.github.com/repos/kubernetes/kubectl/issues/47,Rejecting valid Environment Variable Names,"**Kubernetes version:**

```
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.0"", GitCommit:""d3ada0119e776222f11ec7945e6d860061339aad"", GitTreeState:""clean"", BuildDate:""2017-06-29T23 15:59Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""windows/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.6"", GitCommit:""7fa1c1756d8bc963f1a389f4a6937dc71f08ada2"", GitTreeState:""clean"", BuildDate:""2017-06-16T18 21:54Z"", GoVersion:""go1.7.6"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Azure Container Service
- **OS**: Windows 10
- **Install tools**: 
- **Others**:

**What happened**:
I was invoking the following command:
`kubectl run myname --image=myimage:86 --port=80 --env=""ConnectionStrings:DefaultConnection=Data Source=tcp:mySqlServer,1433;Initial Catalog=myDB;User Id=myUser;Password=mypassword;""`
I got errors about invalid env:
error: invalid env: ConnectionStrings:DefaultConnection=Data Source=tcp:mySqlServer

I tried this with a YAML file as well and got a better error message stating I was using invalid characters for my name. However, : is valid the only values not allowed in names is ""="".  To work around this I removed the : then I got an error because it split my value on "","" between my server and port.

**What you expected to happen**:
I expected a new environment variable to be created with the following information:
- name: ConnectionStrings:DefaultConnection
- value: Data Source=tcp:mySqlServer,1433;Initial Catalog=myDB;User Id=myUser;Password=mypassword;

**How to reproduce it** (as minimally and precisely as possible):
Just try and set the environment variable name to something that includes a "":"" and have a value that contains a "","".  The splitting should not split on "","" in the value of the environment variable. 

**Anything else we need to know**:
I am using .NET Core and it allows you to create nested configurations that are built using a : in the environment variable name.  I was able to use this just fine with docker run commands against my own host because docker and Linux does support the string as I originally submitted it. This only breaks because of the validation of values and parsing logic of kubectl. 
",open,False,2017-08-05 21:39:43,2019-01-23 18:33:56
kubectl,jaraco,https://github.com/kubernetes/kubectl/issues/48,https://api.github.com/repos/kubernetes/kubectl/issues/48,Kubectl fails to resolve names except through DNS,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): Not exactly

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): kubectl ""unable to connect to the server"" ""read udp""

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T15:13:53Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: unknown
- **OS** (e.g. from /etc/os-release): macOS 10.12.6
- **Kernel** (e.g. `uname -a`): n/a
- **Install tools**: Homebrew
- **Others**:


**What happened**:

```
$ time kubectl version
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T15:13:53Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Unable to connect to the server: dial tcp: lookup kub1.mycorp.local on 192.168.14.1:53: read udp 192.168.14.131:49181->192.168.14.1:53: i/o timeout
kubectl version  0.11s user 0.02s system 0% cpu 16.153 total
$ ping kub1
PING kub1.mycorp.local (10.10.9.161): 56 data bytes
64 bytes from 10.10.9.161: icmp_seq=0 ttl=61 time=387.593 ms
$ python -c ""import socket; print(socket.gethostbyname('kub1'))""
10.10.9.161
```

It takes 15+ seconds to fail to connect to the cluster master and fails because it can't figure out the name.

192.168.14.1 is the IP address of my local wifi router. It doesn't (and shouldn't) know anything about kub1.  As you can see `ping` and `gethostbyname` both resolve the name through the Cisco VPN client installed and connected on the host.

**What you expected to happen**:

kubectl should connect to `kub1` and `kub1.mycorp.local` like any other application on my system. It shouldn't be making UDP calls to the nameserver directly but should use the IP stack on the host.

~~Additionally, the command probably shouldn't be attempting to connect for a `version` command. Preferable would be for the command to return the version immediately... and for this issue only to appear if a cluster-relevant command were issued.~~

**How to reproduce it** (as minimally and precisely as possible): See above.


**Anything else we need to know**:

",closed,False,2017-08-09 18:40:56,2017-11-23 18:39:28
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/49,https://api.github.com/repos/kubernetes/kubectl/issues/49,Allow client-side validation against OpenAPI rather than Swagger,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): validation, openapi

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): Feature request

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
1.7

**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:


**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",closed,False,2017-08-11 19:54:00,2017-09-03 01:18:48
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/50,https://api.github.com/repos/kubernetes/kubectl/issues/50,Remove kubectl dependencies on unversioned kubernetes API packages,Change libraries that rely on unversioned API structs from kubernetes/kubernetes to use externally available versioned structs from kubernetes/api,closed,False,2017-08-13 19:06:26,2017-10-19 16:02:37
kubectl,xilabao,https://github.com/kubernetes/kubectl/issues/51,https://api.github.com/repos/kubernetes/kubectl/issues/51,kubectl set resource/selector/subject -o yaml doesn't return the expected format,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.0-alpha.2.1821+7c87bdd628715c-dirty"", GitCommit:""7c87bdd628715c3a12a90267c689bdacaf99a069"", GitTreeState:""dirty"", BuildDate:""2017-08-14T02:21:19Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.0-alpha.2.1821+7c87bdd628715c-dirty"", GitCommit:""7c87bdd628715c3a12a90267c689bdacaf99a069"", GitTreeState:""dirty"", BuildDate:""2017-08-14T02:21:19Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}


**What happened**:
```
# kubectl set subject rolebinding/admin --user=user1 --group=group1 -o yaml
rolebinding ""admin"" subjects updated
```


**What you expected to happen**:
```
# kubectl set subject rolebinding/admin --user=user1 --group=group1 -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: 2017-08-14T02:26:11Z
  name: admin
  namespace: default
  resourceVersion: ""381""
  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/default/rolebindings/admin
  uid: f0238ea3-8097-11e7-ab4c-7427ea6f0fe3
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: foo1
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: admin
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: user1
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: group1
```

**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",closed,False,2017-08-14 02:48:24,2017-09-28 18:05:41
kubectl,gtirloni,https://github.com/kubernetes/kubectl/issues/52,https://api.github.com/repos/kubernetes/kubectl/issues/52,`kubectl run --expose` continues after error (missing parameter),"**Versions**:

```
$ kubectl version 
Client Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.0-alpha.1.3005+04a648105961ce"", GitCommit:""04a648105961cef6126cb6cbcc751b3548c572f1"", GitTreeState:""clean"", BuildDate:""2017-08-16T13:29:04Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-11T16:58:53Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

$ kubectl get nodes
NAME       STATUS    AGE       VERSION
minikube   Ready     26m       v1.7.3
```

**What happened**:

When using `--expose`, it's necessary to specify `--port` and kubectl correctly terminates with an error, but the deployment is still created.

```
$ kubectl run nginx --image=nginx --replicas=2 --expose
error: --port must be set when exposing a service

$ echo $?
1

$ kubectl get deploy
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     2         2         2            2           3s

$ kubectl get pods
NAME                     READY     STATUS    RESTARTS   AGE
nginx-4217019353-4btgl   1/1       Running   0          6s
nginx-4217019353-ndk1d   1/1       Running   0          6s

$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.0.0.1     <none>        443/TCP   29m
```


**What you expected to happen**:
I expected the kubectl wouldn't proceed with the action and nothing would be created.

**How to reproduce it** (as minimally and precisely as possible):
Run `kubectl run nginx --image=nginx --replicas=2 --expose` without specifying a port.

**Anything else we need to know**:
Tested on kubectl compiled from source (2017-08-16) and minikube 0.21.0 with Kubernetes 1.7.3
",closed,False,2017-08-16 14:01:26,2017-08-18 15:42:17
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/53,https://api.github.com/repos/kubernetes/kubectl/issues/53,Test issue in milestone,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):


**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:


**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",closed,False,2017-08-24 23:24:44,2018-03-10 13:37:33
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/54,https://api.github.com/repos/kubernetes/kubectl/issues/54,Test issue move out of milestone,This should be moved out because it is missing labels,closed,False,2017-08-24 23:30:11,2018-01-03 22:05:58
kubectl,mengqiy,https://github.com/kubernetes/kubectl/issues/55,https://api.github.com/repos/kubernetes/kubectl/issues/55,kubectl apply should use openapi ,"kubectl apply should use openapi to calculate diff.
This will mitigate version skew issue.
",closed,False,2017-08-25 07:13:33,2017-11-22 19:59:49
kubectl,cbluth,https://github.com/kubernetes/kubectl/issues/56,https://api.github.com/repos/kubernetes/kubectl/issues/56,kubectl cycle pods,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): cycle, refresh

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FEATURE

This is a *VERY* common occurrence:
1. update configmap values (using envFrom)
2. desire this new environment variable to be ""pushed out"" to all the currently running pods.
3. at this point i manually delete pods in order for it to take effect, or i have also discovered that adding an extra/non-needed environment variable to the deployment will have a similar effect.
I would prefer a feature that allows me to cycle or refresh all the pods of a given selector, so that they grab the new config.
4. Am I doing this wrong, is there a ""better"" way to do it?

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.1"", GitCommit:""b0b7a323cc5a4a2019b2e9520c21c7830b7f708e"", GitTreeState:""clean"", BuildDate:""2017-04-03T20:44:38Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T20:22:08Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release):

```core@ip-10-20-25-116 ~ $ cat /etc/os-release 
NAME=""Container Linux by CoreOS""
ID=coreos
VERSION=1465.6.0
VERSION_ID=1465.6.0
BUILD_ID=2017-08-16-0012
PRETTY_NAME=""Container Linux by CoreOS 1465.6.0 (Ladybug)""
ANSI_COLOR=""38;5;75""
HOME_URL=""https://coreos.com/""
BUG_REPORT_URL=""https://issues.coreos.com""
COREOS_BOARD=""amd64-usr""
core@ip-10-20-25-116 ~ $```

- **Kernel** (e.g. `uname -a`):
```core@ip-10-20-25-116 ~ $ uname -a
Linux ip-10-20-25-116.ec2.internal 4.12.7-coreos #1 SMP Tue Aug 15 23:54:56 UTC 2017 x86_64 Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz GenuineIntel GNU/Linux
core@ip-10-20-25-116 ~ $```
- **Install tools**: kops
- **Others**:


**What happened**:


**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",closed,False,2017-08-28 18:21:36,2017-10-03 03:49:43
kubectl,tnozicka,https://github.com/kubernetes/kubectl/issues/57,https://api.github.com/repos/kubernetes/kubectl/issues/57,kubectl delete returns incorrect exit status,"**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.0-alpha.2.2074+858d9d485741f6"", GitCommit:""858d9d485741f671f45d39a801c99213f68084ad"", GitTreeState:""clean"", BuildDate:""2017-08-17T15:47:31Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.0+695f48a16f"", GitCommit:""d2e5420"", GitTreeState:""clean"", BuildDate:""2017-08-29T12:41:31Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:
`kubectl delete` returns bad exit status. It was unable to delete specified resources and exited with code 0.


**What you expected to happen**:
Exit with non zero exit code.


**How to reproduce it** (as minimally and precisely as possible):
```
$ kubectl delete po --ignore-not-found=false -l foo=bar --grace-period 0 && echo $?
No resources found
0
```
",closed,False,2017-08-29 13:04:15,2017-10-16 23:50:27
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/58,https://api.github.com/repos/kubernetes/kubectl/issues/58,Verify that kubectl can parse openapi,"Since https://github.com/kubernetes/kube-openapi/pull/10 is moving parsing code for `openapi` from the main repo to https://github.com/kubernetes/kube-openapi/, we need to verify that the full swagger openapi file can still be parsed.

This is just a reminder that we need to do that.
In other word, we need to add a test in the main repo that verifies that the parsing code can parse the openapi swagger.",closed,False,2017-09-01 00:03:26,2018-09-25 20:32:40
kubectl,JorritSalverda,https://github.com/kubernetes/kubectl/issues/59,https://api.github.com/repos/kubernetes/kubectl/issues/59,Return list of operation ids from 'kubectl apply' and add 'kubectl await' to wait for their results,"**FEATURE REQUEST**

I use `kubectl apply` to perform rolling updates for _Deployments_, rolling updates for _StatefulSets_, updates for _Deamonsets_, etc. Usually with multiple manifests in one file (namespace, secret, service, deployment).

To verify whether those updates finish successfully I currently need to figure out which one - or combination - of the above actions I just applied. Nicer would be if `kubectl apply` returns a list of operation IDs for every manifest in the applied file, whether it's a namespace, a service account, a secret, a deployment or any other type of object in Kubernetes.

Armed with this list of operation IDs another kubectl function could either monitor them individually or all at once to see progress.

For this a new command like `kubectl await` - or another possibly better command name - could monitor the operations until they either succeed or fail. It could be an alias for commands like `kubectl rollout status deploy/nginx-deployment` that blocks until it's either completed successfully or failed and returns an exit code of 0 or 1 depending on this success. The `kubectl await` would work for any type of object creation, update, etc action that's the result of `kubectl apply`. And multiple of those actions running at the same time.

In pseudo code

**apply the manifest**
```
operationIDs []int := kubectl apply -f <filename>
```

**await the operations to finish**
```
kubectl await <operationIDs>
```

I'm not sure if every operation in Kubernetes already gets and ID and if there are any functions available that could already help with the above. Looking forward to hear more about it.",closed,False,2017-09-01 09:54:57,2018-09-25 20:52:55
kubectl,aleksandra-malinowska,https://github.com/kubernetes/kubectl/issues/60,https://api.github.com/repos/kubernetes/kubectl/issues/60,kubectl annotate fails for annotations with prefix,"`kubectl annotate` fails to set annotation containing prefix:

```
$ kubectl annotate node aleksandram-minion-group-9559 cluster-autoscaler.kubernetes.io/scale-down-disabled=true

The Node ""aleksandram-minion-group-9559"" is invalid: metadata.annotations: Invalid value: ""cluster-autoscaler.kubernetes.io~1scale-down-disabled"": name part must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]')
```

kubectl version:

```
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.4"", GitCommit:""793658f2d7ca7f064d2bdf606519f9fe1229c381"", GitTreeState:""clean"", BuildDate:""2017-08-17T08:48:23Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.0-alpha.3.502+5030391c0750ea-dirty"", GitCommit:""5030391c0750eaa3fa7ccdef609ae7b22f29e477"", GitTreeState:""dirty"", BuildDate:""2017-08-30T08:48:19Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

This has been replicated on head (v1.9.0-alpha.0.408+ea017719e52138-dirty) and with different kubectl versions (1.7.0, 1.7.5). Same command succeeds in GKE running 1.7.4.",closed,False,2017-09-07 15:53:34,2017-09-12 12:16:32
kubectl,Winterflower,https://github.com/kubernetes/kubectl/issues/61,https://api.github.com/repos/kubernetes/kubectl/issues/61,"kubectl edit <resource> fails with Edit cancelled, no changes made even when file has modifications","<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->


**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

* edit ingress
* edit cancelled

also Google search on error message and kubectls editing

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

BUG REPORT 

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.4"", GitCommit:""793658f2d7ca7f064d2bdf606519f9fe1229c381"", GitTreeState:""clean"", BuildDate:""2017-08-17T08:48:23Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T06:43:48Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}


**Environment**:
- **Cloud provider or hardware configuration**:
GCE
- **OS** (e.g. from /etc/os-release): 

NAME=""Ubuntu""
VERSION=""16.04 LTS (Xenial Xerus)""

- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:
1. Open ingress file for editing
``kubectl --namespace=<myspace> edit ingress <myingressname>``
This opened up vim as expected. I made changes and saved them. 
2. Close vim.
3. See message 
""Edit cancelled, no changes made""
Tried this multiple times and made sure I always did a colon-w to write and same result everytime.
When I reopen the file, the changes have not been saved.

**What you expected to happen**:
Changes to ingress saved correctly.

**How to reproduce it** (as minimally and precisely as possible):
Seems like an intermittent problem (steps are detailed in the What happened section""), because I have been previous able to edit ingress entries correctly.

**Anything else we need to know**:

",closed,False,2017-09-11 09:46:27,2019-03-20 22:53:10
kubectl,aauren,https://github.com/kubernetes/kubectl/issues/62,https://api.github.com/repos/kubernetes/kubectl/issues/62,Create Key Agent Integration for kubectl,"**Feature Description**
For those that don't want to have an unencrypted set of x509 keys on their system for kubectl to use, it would be helpful if kubectl were able to retrieve the x509 keys from a secure agent such as gpg-agent. That way, instead of having the certificates either specified by path or within the kubectl context itself in an insecure manner, kubectl could reference and retrieve them by name from the agent for use.

A fallback request from this would be to add a parameter to kubectl, something liked: `--client-key-passphrase` which would tell kubectl to prompt the user for a decryption passphrase, decrypt the x509 certificate, and then use it for authentication to the kube-apiserver. While I think that this would likely be simpler to implement, it would put a higher burden on the user because they would have to supply a passphrase for the certificate for each command.

However, I believe that both of these ideas would be an improvement to the current security model.

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.2+922a86cfcd659"", GitCommit:""922a86cfcd65915a9b2f69f3f193b8907d741d9c"", GitTreeState:""not a git tree"", BuildDate:""2017-08-16T21:21:32Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.4+793658f2d7ca7"", GitCommit:""793658f2d7ca7f064d2bdf606519f9fe1229c381"", GitTreeState:""not a git tree"", BuildDate:""2017-08-28T23:44:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**: N/A
- **OS** (e.g. from /etc/os-release): Gentoo
- **Kernel** (e.g. `uname -a`): Linux 4.9.40-vanilla-base-2
- **Install tools**: N/A
- **Others**: N/A

",closed,False,2017-09-11 19:42:38,2018-03-20 18:12:35
kubectl,smarterclayton,https://github.com/kubernetes/kubectl/issues/63,https://api.github.com/repos/kubernetes/kubectl/issues/63,ANSI escape sequences aren't terminated by kubectl logs,Partial ANSI sequences escape `kubectl logs` (at least on my OS X terminal).  I *think* we may need to look at what other tools like head and tail do.,closed,False,2017-09-21 22:39:00,2018-01-09 17:30:43
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/64,https://api.github.com/repos/kubernetes/kubectl/issues/64,add types for descriptor and manifest,"We finally agreed to land on kubectl repo.
Moved from main repo PR: https://github.com/kubernetes/kubernetes/pull/52570.
A lot of discussion there.
",closed,True,2017-09-28 14:26:14,2017-09-28 22:22:25
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/65,https://api.github.com/repos/kubernetes/kubectl/issues/65,start kexpand,"A place to hang code.

Starting with `kexpand` since `expand` is already taken on linux systems, and since it's
unique enough to change with `find ./ | xargs sed -i 's/kexpand/foo/'`
",closed,True,2017-09-28 19:16:20,2018-04-06 21:27:13
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/66,https://api.github.com/repos/kubernetes/kubectl/issues/66,"Deprecate ""Discovery"" and replace with openapi","This is a feature request, or an invitation to discuss the above idea.

The cache mechanism for discovery is far from perfect, and there is quite some code that we could remove from kubectl by using openapi rather than the current discovery mechanisms.

I'll try to investigate what this would mean, but I think that no so many things would be impacted.

cc @pwittrock @mbohlool 

*Please cc whoever might find this relevant.*

**Kubernetes version** (use `kubectl version`): kubectl 1.8",closed,False,2017-09-29 20:44:38,2017-10-16 23:47:22
kubectl,iamcaleberic,https://github.com/kubernetes/kubectl/issues/67,https://api.github.com/repos/kubernetes/kubectl/issues/67,Error when trying to run kubectl create command on yaml files,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): 

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

`Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5"", GitCommit:""17d7182a7ccbb167074be7a87f0a68bd00d58d97"", GitTreeState:""clean"", BuildDate:""2017-09-18T20:30:29Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
`
**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): 
NAME=""Arch Linux""
PRETTY_NAME=""Arch Linux""
ID=arch
ID_LIKE=archlinux
ANSI_COLOR=""0;36""
HOME_URL=""https://www.archlinux.org/""
SUPPORT_URL=""https://bbs.archlinux.org/""
BUG_REPORT_URL=""https://bugs.archlinux.org/""

- **Kernel** (e.g. `uname -a`): 4.13.3-1-ARCH
- **Install tools**: 
- **Others**:


**What happened**:
Created a manual secrets file and while trying to create this error is thrown
`$ kubectl create -f ./file.yaml`
 
`error: error validating ""./file.yaml"": error validating data: unknown object type schema.GroupVersionKind{Group:"""", Version:""v1"", Kind:""Secret""}; if you choose to ignore these errors, turn validation off with --validate=false`

Retried with the the secrets file in the documentation just to confirm it was not a syntax mistake but same outcome.

`kubectl create -f redis.yaml`

Also happens with the rest of the service files other than secrets  


**What you expected to happen**:
the expectation was a confirmation that the secret ""file"" created

or 

service ""redis"" created
replicationcontroller ""redis"" created


**How to reproduce it** (as minimally and precisely as possible):
`$ kubectl create -f ./file.yaml`

**Anything else we need to know**:

if i turn the validation off everything seems to be fine with the flag validate=false",closed,False,2017-10-02 12:59:21,2017-10-04 15:47:17
kubectl,AnthonyWC,https://github.com/kubernetes/kubectl/issues/68,https://api.github.com/repos/kubernetes/kubectl/issues/68,kubectl config get-contexts show no cluster?,"Just set up a new AWS cluster with private DNS + private topology on AWS:

```kops create cluster --zones=$AWS_ZONE --name=${CLUSTER_NAME} --vpc=${VPC_ID} --topology private --bastion=true --networking weave --associate-public-ip=false --cloud=aws```

kops recognize the cluster:
```
kops get cluster
NAME                            CLOUD   ZONES
beta.example.com        aws     us-east-1a

kops get beta.example.com
Cluster
NAME                            CLOUD   ZONES
beta.example.com        aws     us-east-1a

Instance Groups
NAME                    ROLE    MACHINETYPE     MIN     MAX     SUBNETS
master-us-east-1a       Master  c4.large        1       1       us-east-1a
nodes                   Node    t2.medium       2       2       us-east-1a
```

But output of ```kubectl config get-contexts``` does not list anything under cluster
```
CURRENT   NAME                                               CLUSTER                                            AUTHINFO                                           NAMESPACE
          minikube                                           minikube                                           minikube                                           
*         example-poc   
```
 

I tried to manually set it using with:

```
kubectl config set-cluster beta1.example.com --server=https://api.beta.example.com

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5"", GitCommit:""17d7182a7ccbb167074be7a87f0a68bd00d58d97"", GitTreeState:""clean"", BuildDate:""2017-08-31T09:14:02Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}


**Environment**:
- **Cloud provider or hardware configuration**: AWS

- **OS** (e.g. from /etc/os-release): 
NAME=""Ubuntu""
VERSION=""16.04.3 LTS (Xenial Xerus)""

- **Kernel** (e.g. `uname -a`): 
Linux noctil 4.10.0-35-generic #39~16.04.1-Ubuntu SMP Wed Sep 13 09:02:42 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
```

**What happened**:
After creating cluster, kops has the cluster registered but not kubectl",closed,False,2017-10-03 15:11:03,2017-10-03 18:55:59
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/69,https://api.github.com/repos/kubernetes/kubectl/issues/69,Refactor kubectl apply code to be maintainable,,closed,False,2017-10-03 20:47:13,2017-10-03 21:41:15
kubectl,marun,https://github.com/kubernetes/kubectl/issues/70,https://api.github.com/repos/kubernetes/kubectl/issues/70,IGNORE: Test Issue,Foo,closed,False,2017-10-04 16:40:10,2017-10-05 22:25:21
kubectl,marun,https://github.com/kubernetes/kubectl/issues/71,https://api.github.com/repos/kubernetes/kubectl/issues/71,IGNORE: Another test issue,,closed,False,2017-10-05 02:37:45,2017-10-05 22:25:13
kubectl,marun,https://github.com/kubernetes/kubectl/issues/72,https://api.github.com/repos/kubernetes/kubectl/issues/72,IGNORE: Yet another test issue,,closed,False,2017-10-05 02:39:12,2017-10-05 22:25:08
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/73,https://api.github.com/repos/kubernetes/kubectl/issues/73,"Replace type ""switch"" statements with KindVisitor","Time commitment: 5-10 hours (assuming knowledge of Go)

These sorts of type switches are brittle and break when new groups are added.

https://github.com/kubernetes/kubernetes/blob/28857a2f02ef1490bb6b0109dc1fd7fa5ecae11b/pkg/kubectl/history.go#L56

Instead we should use a common library that is updated, so we don't have to update the code in a dozen places:

https://github.com/kubernetes/kubernetes/blob/e8e617f31dfbfe2ad8ad4743cfcc17ebd6f283a1/pkg/kubectl/apps/kind_visitor.go#L27",closed,False,2017-10-06 20:50:12,2017-11-16 20:51:08
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/74,https://api.github.com/repos/kubernetes/kubectl/issues/74,Transition ownership of tests that tests resource specific logic to SIG apps,"Per-discussion with @erictune, SIG apps should own kubectl tests that primarily test generators and logic for specific resources.",open,False,2017-10-09 15:53:50,2018-09-25 20:39:57
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/75,https://api.github.com/repos/kubernetes/kubectl/issues/75,Setup tools for an oncall rotation for SIG cli,We need to be able field setup a rotation for fielding support and keeping tests healthy,closed,False,2017-10-09 15:54:50,2018-09-25 19:01:18
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/76,https://api.github.com/repos/kubernetes/kubectl/issues/76,Move kubectl tests in common test jobs into their own jobs,"The test results on the [test grid page](https://k8s-testgrid.appspot.com/sig-cli) show the results of test jobs running both non-cli tests and cli tests.  Isolate the signal for cli tests from non-cli tests, we should move the cli tests into their own test jobs.

",open,False,2017-10-09 16:57:55,2019-03-24 21:56:19
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/77,https://api.github.com/repos/kubernetes/kubectl/issues/77,Update skew tests on test grid to be 1.8-1.9 instead of 1.7-1.8,"The test results on the [test grid page](https://k8s-testgrid.appspot.com/sig-cli) show the results of test jobs running both non-cli tests and cli tests.  Isolate the signal for cli tests from non-cli tests, we should move the cli tests into their own test jobs.

",closed,False,2017-10-09 16:59:29,2018-03-12 17:28:45
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/78,https://api.github.com/repos/kubernetes/kubectl/issues/78,New apply path crashes when there is no group,,closed,False,2017-10-16 17:05:27,2017-10-16 17:58:30
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/79,https://api.github.com/repos/kubernetes/kubectl/issues/79,Publish SIG-CLI 1.9 goals,Write a quick document outlining the 1.10 goals for kubectl and check it into the repo.  Solicit what the community is planning.,closed,False,2017-10-16 20:34:41,2018-03-15 21:42:45
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/80,https://api.github.com/repos/kubernetes/kubectl/issues/80,Umbrella Issue: Remove kubectl dependencies on kubernetes/kubernetes,"Remove from:
- [x] pkg/kubectl/resources #81 

To Remove:
- [ ] pkg/apis #83",open,False,2017-10-17 20:36:07,2019-03-28 22:44:43
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/81,https://api.github.com/repos/kubernetes/kubectl/issues/81,Remove pkg/kubectl/resource deps on kubernetes/kubernetes,Part of #80 ,closed,False,2017-10-17 20:36:53,2017-10-21 04:31:11
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/82,https://api.github.com/repos/kubernetes/kubectl/issues/82,Write guide for maintainers sustaining engineering tasks.,,closed,True,2017-10-18 16:56:26,2017-10-18 19:22:13
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/83,https://api.github.com/repos/kubernetes/kubectl/issues/83,Remove Kubectl dependencies on kubernetes/pkg/api and kubernetes/pkg/apis,"Background:
Kubernetes resource structs have 2 versions - an internal version and an external version.  Kubectl should only depend on the external versions so that it can be moved into its own repo and be maintained independently from the server.  To do this, we must replace instances where an internal type and client is used with the external type and client.

An example PR of how to do this can be found here: https://github.com/kubernetes/kubernetes/pull/54157

**NOTE**: That PR is unmerged and someone should clone it and then complete it by fixing any broken tests and sending it for review.

List of api pkgs that must be removed:

- [ ] k8s.io/kubernetes/pkg/api
- [ ] k8s.io/kubernetes/pkg/api/resource
- [ ] k8s.io/kubernetes/pkg/api/v1
- [ ] k8s.io/kubernetes/pkg/api/v1/pod
- [ ] k8s.io/kubernetes/pkg/api/validation
- [ ] k8s.io/kubernetes/pkg/apis/apps
- [ ] k8s.io/kubernetes/pkg/apis/batch
- [ ] k8s.io/kubernetes/pkg/apis/extensions #92
- [ ] k8s.io/kubernetes/pkg/apis/policy #90
- [ ] k8s.io/kubernetes/pkg/apis/rbac #91 @shiywang 

To get started:

- If something has an issue already, assign yourself to it if no one is on it
- If something doesn't have an issue, file an issue for it and add it as a comment to this issue.

Strategy:

- Look for import statements under packages under pkg/kubctl that reference the above packages
- Look for the simplest ones to remove where the external type can be used instead with an external client
- Create a new issue for that package and update this issue with a comment containing the issue number
- Fix them and send a PR



",closed,False,2017-10-18 17:10:40,2018-10-16 19:23:03
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/84,https://api.github.com/repos/kubernetes/kubectl/issues/84,SIG CLI Leads Tasks,"- [ ] Routinely check [maintainers tasks](https://github.com/kubernetes/kubectl/blob/master/docs/maintainers/MAINTAINERS.md)
- [ ] Identify new contributor for SIG CLI and provide them opportunities and guidance
- [x] Setup on call rotation for maintainers tasks - @shiywang is starting to look into this
- [ ] Clean up reviewers / approvers, remove old entries
  - Who has left the project in the contributor capacity?
  - Who is still part of the project, but no longer active in SIG CLI
    - Need to define ""active""
  - Who is active in SIG CLI but no longer wants to be a reviewer 
- [x]  Identify and document expectations for SIG CLI members
  - Who are the ""members"" today?
  - Helping with maintenance tasks?
  - Periodically having an all-hands SIG CLI meeting to resolve issues and get consensus on things
  - Should we have members who are not reviewers / approvers?
- [x]  Identify process for accepting change proposals
  - How do we ensure everyone in the SIG knows a proposal is being considered
  - What is the appropriate time to discuss and respond to comments? - we should time box this so things don't drag on and fizzle out
  - Use [kep](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/0000-kep-template.md)
  - Figure out where the decision is made, and who is part of the lazy consensus quorum (members of SIG cli?)
- [ ] Report to the community meeting every quarter about SIG cli ongoings",closed,False,2017-10-18 19:23:15,2018-09-25 19:10:01
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/85,https://api.github.com/repos/kubernetes/kubectl/issues/85,Add items to Maintainers tasks,"- new contributor tasks
- reporting at community meeting
- setting up f2f",closed,True,2017-10-18 19:46:17,2017-10-18 19:46:27
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/86,https://api.github.com/repos/kubernetes/kubectl/issues/86,Support generating CA and self signed certs,"Time commitment: 30-60 hours (assuming knowledge of Go and crypto libraries)

We need to be able to generate CA's and self signed certificates and provide them to the Kubernetes apiserver for creating certain resources.

Currently this is done by users who need to run shell commands to generate the certificates.  Kubectl should support generating canonical certs for users.

We should write a go library that generates the CA and certs for users

1. create a library that takes the subject and days as arguments (in a go struct) and returns the 6 results in a datastructure (as []bytes)
2. add functions to the result struct to get the fields as base64 encoded []byte or string
3. write a library that can take the result and write out files
4. expose the library as a cobra command under the `kubectl alpha create certificates`

Required research:
- go crypto cert creation libraries
- cobra libraries

The library should duplicate the functionality of the following commands

```sh
openssl req -x509 -newkey rsa:2048 -keyout certificates/apiserver_ca.key -out certificates/apiserver_ca.crt -days 365 -nodes -subj /C=/ST=/L=/O=/OU=/CN=test-certificate-authority

openssl req -out certificates/apiserver.csr -new -newkey rsa:2048 -nodes -keyout certificates/apiserver.key -subj /C=/ST=/L=/O=/OU=/CN=test.default.svc

openssl x509 -req -days 365 -in certificates/apiserver.csr -CA certificates/apiserver_ca.crt -CAkey certificates/apiserver_ca.key -CAcreateserial -out certificates/apiserver.crt
```

Library should probably be in kubernetes/common

",open,False,2017-10-19 16:17:26,2019-01-23 21:08:16
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/87,https://api.github.com/repos/kubernetes/kubectl/issues/87,Support integration test framework that brings up a new control plane,"Time commitment: 30-60 hours (assuming knowledge of Go and the Ginkgo test framework)

We need a way to write integration tests for kubectl and point it at a control plane (apiserver, controller-manager, etcd, scheduler) running locally.

As a kubectl developer, when I write an integration test for kubectl, there should be a ginkgo test framework that launches a new control plane, and provides a kubeconfig to the test both as a tmp file and as a datastructure.

Framework should take the control plane binaries as flags instead of compiling them from source.

See example of a test framework that brings up the control plane [here](https://github.com/kubernetes-incubator/apiserver-builder/blob/master/pkg/test/server.go).

[Link](https://k8s-testgrid.appspot.com/sig-cli-master#gke) to existing e2e tests
",closed,False,2017-10-19 21:36:20,2018-07-10 05:56:29
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/88,https://api.github.com/repos/kubernetes/kubectl/issues/88,Create test library that emulates a node,"As a kubectl contributor, I want to be able to run integration tests against a control plane without real nodes.

We want to be able to support the following in kubectl tests using a real apiserver + etcd + controll-ermanager + schedule, but without any real node:
- testing`describe nodes`
  - needs to register itself as a node with the apiserver
  - needs to support multiple nodes
- testing `logs` for Pods
- testing `exec` & `attach` for Pods
- node going down

Step 1: Figure out if we can reuse what [kubemark](https://github.com/eBay/Kubernetes/blob/master/docs/devel/kubemark-guide.md) does with [hollow kubelet](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubemark/hollow_kubelet.go) which is already close to what we want, but for a very different purpose.  If we can just use theirs, lets do that.
Step 2: If we can't use kubemark, figure out how to register a fake ""Node"" with an apiserver using client-go and get the scheduler to schedule to it.  Make sure it periodically updates anything needed for liveness checks done by the apiserver.
Step 3: Given a clientconfig pointing at an apiserver build an interface to start a new ""fake"" Node with set CPU & Memory and ""start it"".


",closed,False,2017-10-20 00:01:19,2018-05-15 02:51:51
kubectl,mindprince,https://github.com/kubernetes/kubectl/issues/89,https://api.github.com/repos/kubernetes/kubectl/issues/89,`kubectl logs -f` is not working correctly on 1.8,"/kind bug

`kubectl logs -f ` is not working on Kubernetes 1.8. It gets stuck and only returns when the container exits.

I tested it in GKE 1.6.11 and 1.7.8 and it works there. But doesn't work on GKE 1.8.0.

Here's a simple reproduction:
1. Create a 1.8.0 or 1.8.1 cluster.
2. Run a simple busybox container and `echo ""hello""`.
```
$ kubectl run -i -t busybox --image=busybox --restart=Never
If you don't see a command prompt, try pressing enter.
/ # echo ""hello""
hello
```
3. In another terminal, try `kubectl logs` without `-f`. It works.
```
$ kubectl logs po/busybox
/ # echo ""hello""
hello
```

4. Try `kubect logs -f` now. It gets stuck.
```
$ kubectl logs po/busybox -f
```

5. Enter more commands in the busybox terminal but there are still no logs in the `-f` terminal.
```
...
/ # echo ""hello""
hello
/ # echo ""hi""
hi
```

6. In another terminal try `kubectl logs` without `-f` again. That still works.
```
$ kubectl logs po/busybox
/ # echo ""hello""
hello
/ # echo ""hi""
hi
```
7.  Exit the busybox container:
```
...
/ # echo ""hi""
hi
/ # exit
```

8. Logs would show up on the `-f` terminal now.

/sig cli

cc - @pwittrock 
https://github.com/kubernetes/kubernetes/issues/54205",closed,False,2017-10-20 18:36:55,2017-12-02 23:23:21
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/90,https://api.github.com/repos/kubernetes/kubectl/issues/90,Remove dependency on k8s.io/kubernetes/pkg/apis/policy from kubectl,Remove dependencies from packages under `k8s.io/kubernetes/pkg/kubectl/...` on `k8s.io/kubernetes/pkg/apis/policy` by converting them to use the `k8s.io/api/policy/v1beta1` package,closed,False,2017-10-20 18:40:44,2017-10-25 22:16:09
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/91,https://api.github.com/repos/kubernetes/kubectl/issues/91,Remove dependency on k8s.io/kubernetes/pkg/apis/rbac from kubectl,Remove dependencies from packages under `k8s.io/kubernetes/pkg/kubectl/...` on `k8s.io/kubernetes/pkg/apis/rbac` by converting them to use the `k8s.io/api/rbac/v1` package,closed,False,2017-10-20 18:43:36,2018-08-08 20:19:52
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/92,https://api.github.com/repos/kubernetes/kubectl/issues/92,Remove dependency on k8s.io/kubernetes/pkg/apis/extensions from kubectl,Remove dependencies from packages under k8s.io/kubernetes/pkg/kubectl/... on `k8s.io/kubernetes/pkg/apis/extensions/v1beta1` by converting them to use the `k8s.io/api/policy/v1beta1` package,closed,False,2017-10-20 18:55:10,2018-05-15 02:51:50
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/93,https://api.github.com/repos/kubernetes/kubectl/issues/93,Meta/Umbrella: Improve `pkg/kubectl/resource`unit / integration test coverage and structure,"**Note:** The items can be completed incrementally, but are not easy to parallelize.  Taking this issue does not commit you to completing all of the items, just the next ones on the list. 

The [pkg/kubectl/resource](https://github.com/kubernetes/kubernetes/tree/master/pkg/kubectl/resource) is critically important, but we have very little unit or integration test coverage for it and even less documentation.

- [ ] Read the libraries and document the functions and code
- [ ]  Audit the which functions are covered by unit tests, integration tests and aren't covered at all
  - Run go test coverage tools to figure this out, attach results to the issue
- [ ] Write unit tests for functions that aren't tested at all
  - Send multiple small PRs instead of 1 big one
- [ ] Write unit tests for functions that are only tested with integration tests
  - Send multiple small PRs instead of 1 big one
- [ ] Write integration tests for anything with weak coverage using tmp files",closed,False,2017-10-21 00:05:40,2018-05-28 19:27:28
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/94,https://api.github.com/repos/kubernetes/kubectl/issues/94,Create library for parsing commandline flags -> protocol buffer,"Expected hours: 20-50 (assuming some knowledge of Go, Cobra and ProtocolBuffers)

Topics:
- Go
- Go spf13/Cobra
- Go ProtocolBuffers
  - Field options / extensions
  - SelfDescribing messages

Problem: Business logic in cli libraries read flags directly and depend upon cobra.  This makes it hard to test and reuse the business logic in other clis, since the libraries depend directly upon the flags.

Solution: Introduce a layer of indirection.  Business logic should take datastructures as arguments that have the flags already passed into them, not read the flags directly. 

Project: Develop a go library that can dynamically parse command line flags into Protocol Buffers.

Write a library that takes a ProtocolBuffer `Message` and returns an interface that will populate its fields with the matching values for commandline flags

- Given a ProtocolBuffer `Message`:
  - Get the [DescriptorProto](https://github.com/golang/protobuf/blob/master/descriptor/descriptor.go#L81)
  - Use the [spf13/cobra](https://github.com/spf13/cobra) libraries to register flags for each field of string, []string, int, []int, bool, or []bool.  
    - Use Protocol buffer *field options* add extensions to get the flag name and description
  - Provide a function to get the populated message, or populate the message that is passed in
- Write tests for the library
- Write documentation for the library in the form of a `doc.go` in the package you create
",closed,False,2017-10-21 16:34:53,2018-11-02 02:31:41
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/95,https://api.github.com/repos/kubernetes/kubectl/issues/95,Document k8s.io/kubernetes/pkg/kubectl/resource,"The [kubectl/resource](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/resource/doc.go) package has almost no documentation

- Write the doc.go with overview of what the package does and how to use it. Include examples.
  - Look at how it is already used to help figure this out
- Document the code for types and functions
- Write a CONTRIBUTING.md for this package with anything you discover
  - Include TODOs for any clean up items
  - Include tips, pitfalls and anything you learn that you wish was written down",closed,False,2017-10-21 16:52:39,2018-05-15 02:51:50
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/96,https://api.github.com/repos/kubernetes/kubectl/issues/96,Document the kubectl ring factories,"Time commitment: 10-20 hours

kubectl uses ""ringfactory"" objects to hold state and initialize dependencies kubectl commands.

The design is somewhat complex, and additional documentation would be helpful.

We should write a `doc.go` file that goes into more detail about the factory patterns, providing examples of why the pattern is necessary, how to override sub factories, how to initialize a new factory, and how to use common functions.

https://github.com/kubernetes/kubernetes/tree/4ff6ef4a378748d4c1667f352fb5d300170e4429/pkg/kubectl/cmd/util",closed,False,2017-10-21 17:27:34,2018-06-23 18:24:30
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/97,https://api.github.com/repos/kubernetes/kubectl/issues/97,Add conflict detection to pkg/kubectl/apply/strategy,"Time commitment: 5-15 hours

First read the apply documentation:
https://kubernetes.io/docs/tutorials/object-management-kubectl/declarative-object-management-configuration/

Apply is being rewritten under `pkg/kubectl/apply/strategy`.  The new merge and replace code should check for conflicts between the last seen value and the current value, and optionally return an error if they do not match with the field and details.

strategy.Options already has a field to fail on conflicts.
A conflict is if the same field is specified in BOTH the recorded and the remote values of an object, but does not match.

1. Update the [merge visitor](https://github.com/kubernetes/kubernetes/blob/4ff6ef4a378748d4c1667f352fb5d300170e4429/pkg/kubectl/apply/strategy/merge_visitor.go) functions to check for conflict if the options are enabled, and return an error if a conflict is detected
2. Update the tests to verify the new behavior
3. update the [replace visitor](https://github.com/kubernetes/kubernetes/blob/41568a060234c62742cd751c046de622fedb14ce/pkg/kubectl/apply/strategy/replace_visitor.go)  functions to check for conflict if the options are enabled, and return an error if a conflict is detected
4. Update the tests to verify the new behavior",closed,False,2017-10-21 17:48:01,2018-09-25 19:18:06
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/98,https://api.github.com/repos/kubernetes/kubectl/issues/98,Document pkg/kubectl/cmd/util/env,"Document [pkg/kubectl/cmd/util/env](https://github.com/kubernetes/kubernetes/tree/master/pkg/kubectl/cmd/util/env).
- Add a `doc.go` describing what the package is for and providing examples on how to use it
- Improve type and function code-comment documentation",closed,False,2017-10-21 23:06:09,2018-05-15 00:49:51
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/99,https://api.github.com/repos/kubernetes/kubectl/issues/99,Document pkg/kubectl/cmd/util/editor package,"Document the [pkg/kubectl/cmd/util/editor](https://github.com/kubernetes/kubernetes/tree/master/pkg/kubectl/cmd/util/editor) package

- Add a `doc.go` with overview of the package and examples for usage
- Audit and improve type and function code comments.  Improve documentation.",open,False,2017-10-21 23:08:22,2019-03-30 12:19:22
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/100,https://api.github.com/repos/kubernetes/kubectl/issues/100,Document pkg/kubectl/cmd/util/openapi,"Document code under [pkg/kubectl/cmd/util/openapi](https://github.com/kubernetes/kubernetes/tree/master/pkg/kubectl/cmd/util/openapi)

- Write a `doc.go` file with an overview and examples
- Audit functions.  Write code comments for any under documented functions",closed,False,2017-10-21 23:14:47,2019-01-07 18:50:47
kubectl,frodenas,https://github.com/kubernetes/kubectl/issues/101,https://api.github.com/repos/kubernetes/kubectl/issues/101,Several kubectl Pod Disruption Budget Generators issues,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): Pod Disruption Budget 

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`): `Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.1"", GitCommit:""f38e43b221d08850172a9a4ea785a86a3ffa3b3a"", GitTreeState:""clean"", BuildDate:""2017-10-12T00:44:37Z"", GoVersion:""go1.9.1"", Compiler:""gc"", Platform:""darwin/amd64""}`


**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): `Ubuntu 16.04.3 LTS (Xenial Xerus)`
- **Kernel** (e.g. `uname -a`): `Linux ubuntu 4.4.0-97-generic #120-Ubuntu SMP Tue Sep 19 17:28:18 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux`
- **Install tools**:
- **Others**:


**What happened**:

* Pod Disruption Budget Generators don't have any tests
* Pod Disruption Budget V2 Generator  has a typo when checking the `max-unavailable`parameter, it is using [max-available](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/pdb.go#L146) when it should be [max-unavailable](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/pdb.go#L125)
* Pod Disruption Budget V2 Generator [selector](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/pdb.go#L126) paramater is not marked as required, when it should be
* Pod Disruption Budget V2 Generator is still setting a `min-available` default, this has been [deprecated](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/pdb.go#L175) and should have been removed in 1.8
* When [checking for parameter types](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/pdb.go#L136) (ie string), the error messages always print a zero value instead of the parameter provided (if the parameter holds an incorrect type, the assertion will return the zero value of parameter type)

**What you expected to happen**:

* Pod Disruption Budget Generators must have tests to validate their correct behavior

**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",closed,False,2017-10-23 12:55:24,2017-11-17 11:40:37
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/102,https://api.github.com/repos/kubernetes/kubectl/issues/102,add kexpand util,"Move some util code from https://github.com/kubernetes/kubernetes/pull/52682 to kubectl repo.

There will a followup PR to move https://github.com/kubernetes/kubernetes/pull/52682/files#diff-0c3f4528c27638ae0b2b2fd6e2ecff6b code to kubectl repo. But it is blocked by some dependency issue of kubectl factory code.
",closed,True,2017-10-23 19:48:20,2017-10-25 02:05:19
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/103,https://api.github.com/repos/kubernetes/kubectl/issues/103,Add travis CICD coverage.,"Also, rename ""kexpand"" to ""kinflate"" to ease pronunciation.",closed,True,2017-10-24 23:18:25,2018-04-06 21:27:14
kubectl,php-coder,https://github.com/kubernetes/kubectl/issues/104,https://api.github.com/repos/kubernetes/kubectl/issues/104,kubectl exec: mark -p option as deprecated in the help output,"**Is this a request for help?**:
No

**What keywords did you search in Kubernetes issues before filing this one?**:
kubectl deprecate

---

**Is this a BUG REPORT or FEATURE REQUEST?**:
bug

**Kubernetes version** (use `kubectl version`):
```console
$ ./cluster/kubectl.sh version
Client Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.0-alpha.1.1693+94acd5a076590e-dirty"", GitCommit:""94acd5a076590e083df693f08b3b4f17b133165b"", GitTreeState:""dirty"", BuildDate:""2017-10-25T13:24:02Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.0-alpha.1.1693+94acd5a076590e-dirty"", GitCommit:""94acd5a076590e083df693f08b3b4f17b133165b"", GitTreeState:""dirty"", BuildDate:""2017-10-25T13:24:02Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:
`-p` option of `kubectl exec` is deprecated:
```console
./cluster/kubectl.sh exec -p nginx -i -t id
W1025 17:16:42.763587    7619 cmd.go:398] -p POD_NAME is DEPRECATED and will be removed in a future version. Use exec POD_NAME instead.
uid=0(root) gid=0(root) groups=0(root)
```
But it's not marked as deprecated in the help output:
```console
$ ./cluster/kubectl.sh exec -h | grep -e '-p'
  -p, --pod='': Pod name
```

**What you expected to happen**:
Option description should mention deprecation or this option should be absent in the help output.",closed,False,2017-10-25 15:26:48,2018-07-28 01:05:06
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/105,https://api.github.com/repos/kubernetes/kubectl/issues/105,tweak precommit,untabify and make the intent more obvious,closed,True,2017-10-25 17:32:46,2018-04-06 21:27:15
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/106,https://api.github.com/repos/kubernetes/kubectl/issues/106,Create issue_backlog.md,Doc for describing how issues should be picked for new contributors.,closed,True,2017-10-25 23:27:47,2017-10-26 22:50:33
kubectl,hartfordfive,https://github.com/kubernetes/kubectl/issues/107,https://api.github.com/repos/kubernetes/kubectl/issues/107,Panic on deleting statefulset on kubernetes 1.8.1-gke.0,"**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?**:  BUG REPORT

**Kubernetes version**:   1.8.1-gke.0

**Environment**:
- **Cloud provider or hardware configuration**: GKE
- **OS** (e.g. from /etc/os-release):  Container-Optimized OS
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:
   - Google Cloud SDK 177.0.0
- **kubectl version output**:
```
Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.0"", GitCommit:""fff5156092b56e6bd60fff75aad4dc9de6b6ef37"", GitTreeState:""clean"", BuildDate:""2017-03-28T16:36:33Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.1-gke.0"", GitCommit:""a8e22959b5a4c0ca54ef8860231074428c962466"", GitTreeState:""clean"", BuildDate:""2017-10-12T19:32:15Z"", GoVersion:""go1.8.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:

When setting a zookeeper cluster, as described in your official example [here](https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/), I ran into a panic when deleting the statefulset.  Consequently, the statefulset doesn't get deleted, therefore the only way to get rid of it is by removing the related namespace.

**What you expected to happen**:

It should have deleted the stateful set and returned successfully

**How to reproduce it** (as minimally and precisely as possible):

```
kubectl delete -f k8s/13-stateful-set.json
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x20 pc=0x226024]

goroutine 1 [running]:
panic(0x17ceb20, 0xc42000e0e0)
	/usr/local/go/src/runtime/panic.go:500 +0x1a1
k8s.io/kubernetes/pkg/kubectl.ReaperFor(0xc420319800, 0x4, 0xc420319860, 0xb, 0x0, 0x0, 0x1805c20, 0xc42025a730, 0xc4209f5320, 0x1)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/stop.go:92 +0x1244
k8s.io/kubernetes/pkg/kubectl/cmd/util.(*ring1Factory).Reaper(0xc42025a080, 0xc4202a7730, 0x109e5, 0xc4202ab320, 0xc4209f5338, 0x1d6603)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/util/factory_object_mapping.go:290 +0x151
k8s.io/kubernetes/pkg/kubectl/cmd/util.(*factory).Reaper(0xc42072e240, 0xc4202a7730, 0x19a46e0, 0xc42002e788, 0xc4209f5490, 0x17a332)
	<autogenerated>:92 +0x54
k8s.io/kubernetes/pkg/kubectl/cmd.ReapResult.func1(0xc4203cee80, 0x0, 0x0, 0x9, 0x24e701)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/delete.go:252 +0xdb
k8s.io/kubernetes/pkg/kubectl/resource.ContinueOnErrorVisitor.Visit.func1(0xc4203cee80, 0x0, 0x0, 0x0, 0x0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:348 +0x153
k8s.io/kubernetes/pkg/kubectl/resource.DecoratedVisitor.Visit.func1(0xc4203cee80, 0x0, 0x0, 0x0, 0x0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:324 +0xe5
k8s.io/kubernetes/pkg/kubectl/resource.FlattenListVisitor.Visit.func1(0xc4203cee80, 0x0, 0x0, 0xc4209f5640, 0x436d5)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:389 +0x57c
k8s.io/kubernetes/pkg/kubectl/resource.EagerVisitorList.Visit.func1(0xc4203cee80, 0x0, 0x0, 0x1000, 0xc420045dd0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:207 +0x153
k8s.io/kubernetes/pkg/kubectl/resource.(*StreamVisitor).Visit(0xc4200d5d00, 0xc420855fa0, 0x2625c20, 0xc42085c000)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:552 +0x308
k8s.io/kubernetes/pkg/kubectl/resource.(*FileVisitor).Visit(0xc420855e40, 0xc420855fa0, 0x0, 0x0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:501 +0x184
k8s.io/kubernetes/pkg/kubectl/resource.EagerVisitorList.Visit(0xc4203196e0, 0x1, 0x1, 0xc420713740, 0x1, 0xc420713740)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:211 +0xf4
k8s.io/kubernetes/pkg/kubectl/resource.(*EagerVisitorList).Visit(0xc420855ee0, 0xc420713740, 0x0, 0x1adfae6)
	<autogenerated>:122 +0x6e
k8s.io/kubernetes/pkg/kubectl/resource.FlattenListVisitor.Visit(0x2621ce0, 0xc420855ee0, 0xc4200d5cc0, 0xc4200d5d40, 0xc420855f01, 0xc4200d5d40)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:417 +0x94
k8s.io/kubernetes/pkg/kubectl/resource.(*FlattenListVisitor).Visit(0xc420855f00, 0xc4200d5d40, 0x18, 0x18)
	<autogenerated>:138 +0x6e
k8s.io/kubernetes/pkg/kubectl/resource.DecoratedVisitor.Visit(0x2621d60, 0xc420855f00, 0xc4203196f0, 0x2, 0x2, 0xc420855f60, 0xc420319701, 0xc420855f60)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:325 +0xd6
k8s.io/kubernetes/pkg/kubectl/resource.(*DecoratedVisitor).Visit(0xc420713710, 0xc420855f60, 0x184f400, 0xc41ffe40a4)
	<autogenerated>:158 +0x78
k8s.io/kubernetes/pkg/kubectl/resource.ContinueOnErrorVisitor.Visit(0x2621c60, 0xc420713710, 0xc42037eb60, 0x2c44000, 0x0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:352 +0xf1
k8s.io/kubernetes/pkg/kubectl/resource.(*ContinueOnErrorVisitor).Visit(0xc420319700, 0xc42037eb60, 0xfec8, 0x70)
	<autogenerated>:153 +0x65
k8s.io/kubernetes/pkg/kubectl/resource.(*Result).Visit(0xc42037eaf0, 0xc42037eb60, 0x26e1d10, 0x2621c60)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/result.go:96 +0x4e
k8s.io/kubernetes/pkg/kubectl/cmd.ReapResult(0xc42037eaf0, 0x2644260, 0xc42072e240, 0x2620c60, 0xc42002e010, 0x2620001, 0x0, 0xffffffffffffffff, 0x2630000, 0x263bfa0, ...)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/delete.go:277 +0x16f
k8s.io/kubernetes/pkg/kubectl/cmd.(*DeleteOptions).RunDelete(0xc420210f20, 0x2644260, 0xc42072e240)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/delete.go:237 +0xc5
k8s.io/kubernetes/pkg/kubectl/cmd.NewCmdDelete.func1(0xc4203d78c0, 0xc4203039c0, 0x0, 0x2)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/delete.go:143 +0x17c
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute(0xc4203d78c0, 0xc420303860, 0x2, 0x2, 0xc4203d78c0, 0xc420303860)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:603 +0x439
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0xc4202fe000, 0xc4204252f0, 0x1, 0x1)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:689 +0x367
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute(0xc4202fe000, 0xc42072e240, 0x2620c20)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:648 +0x2b
k8s.io/kubernetes/cmd/kubectl/app.Run(0x0, 0x0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubectl/app/kubectl.go:39 +0xd5
main.main()
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubectl/kubectl.go:26 +0x22
```

**Anything else we need to know**:
",closed,False,2017-10-26 10:26:09,2017-11-02 20:57:13
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/108,https://api.github.com/repos/kubernetes/kubectl/issues/108,Skip CICD build on markdown or doc changes.,"Noticed that merging of
  https://github.com/kubernetes/kubectl/blob/master/docs/maintainers/issue_backlog.md 
launched all the go gets, go tests, drama for no reason.
",closed,True,2017-10-26 23:25:59,2017-10-28 14:39:17
kubectl,php-coder,https://github.com/kubernetes/kubectl/issues/109,https://api.github.com/repos/kubernetes/kubectl/issues/109,kubectl create clusterrolebinding -o yaml shouldn't create a binding,"**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

clusterrolebinding

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

Bug

**Kubernetes version** (use `kubectl version`):
```console
$ ./cluster/kubectl.sh version
Client Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.0-alpha.1.1890+b00c15f1a40162-dirty"", GitCommit:""b00c15f1a40162d46fc4b96f4e6714f20aef9e6c"", GitTreeState:""dirty"", BuildDate:""2017-10-27T17:43:51Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.0-alpha.1.1890+b00c15f1a40162-dirty"", GitCommit:""b00c15f1a40162d46fc4b96f4e6714f20aef9e6c"", GitTreeState:""dirty"", BuildDate:""2017-10-27T17:43:51Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:
`kubectl create clusterrolebinding -o yaml` is creating cluster role binding in addition to output it to the console:

```console
$ kubectl create clusterrolebinding test-binding --clusterrole=restricted-psp-user --user=joe -o yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: 2017-10-27T18:12:06Z
  name: test-binding
  resourceVersion: ""1815""
  selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindings/test-binding
  uid: 573abbeb-bb42-11e7-8b10-507b9d2b16b9
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: restricted-psp-user
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: joe
$ kubectl create clusterrolebinding test-binding --clusterrole=restricted-psp-user --user=joe -o yaml
Error from server (AlreadyExists): clusterrolebindings.rbac.authorization.k8s.io ""test-binding"" already exists
```

**What you expected to happen**:
It shouldn't create a binding and only show the YAML document.

**How to reproduce it** (as minimally and precisely as possible):
Execute the `kubectl create clusterrolebinding test-binding --clusterrole=restricted-psp-user --user=joe -o yaml` command twice.

",closed,False,2017-10-27 18:22:47,2017-11-07 20:21:30
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/110,https://api.github.com/repos/kubernetes/kubectl/issues/110,test travis filter,,closed,True,2017-10-28 14:43:19,2018-04-06 21:27:16
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/111,https://api.github.com/repos/kubernetes/kubectl/issues/111,Tweak travis early exit,,closed,True,2017-10-28 20:48:35,2018-04-06 21:27:17
kubectl,spiffxp,https://github.com/kubernetes/kubectl/pull/112,https://api.github.com/repos/kubernetes/kubectl/issues/112,Rename OWNERS assignees: to approvers:,"They are effectively the same, assignees is deprecated

ref: kubernetes/test-infra#3851",closed,True,2017-10-31 23:35:44,2017-11-06 22:22:29
kubectl,mihnjong,https://github.com/kubernetes/kubectl/issues/113,https://api.github.com/repos/kubernetes/kubectl/issues/113,Need a command to list all available/existing types (or resources) in a cluster ,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): no. `kubectl get all --all-namespaces` does not show all resources such as secrets.

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): ""secrets"", ""all resources"", ""all types"".

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FEATURE REQUEST

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.6"", GitCommit:""4bc5e7f9a6c25dc4c03d4d656f2cefd21540e28c"", GitTreeState:""clean"", B\
uildDate:""2017-09-14T06:55:55Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}                                                          
Server Version: version.Info{Major:""1"", Minor:""7+"", GitVersion:""v1.7.6-gke.1"", GitCommit:""407dbfe965f3de06b332cc22d2eb1ca07fb4d3fb"", GitTreeState:""cl\
ean"", BuildDate:""2017-09-27T21:21:34Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}     


**Environment**: Google Container Engine

-   Details: 
    -   Node version: 1.7.6
    -   Node image: Container-Optimized OS (cos)
    -   Machine type: n1-standard-1 (1 vCPU, 3.75 GB memory)
    -   Total cores	: 3 vCPUs
    -   Total memory: 11.25 GB

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:

`kubectl get all --all-namespaces` does not show all existing resources such as Secrets.

**What you expected to happen**:

`kubectl get all --all-namespaces` shows all existing resources such as Secrets as well as all resource types from other API servers.

**How to reproduce it** (as minimally and precisely as possible):

`kubectl get all --all-namespaces`

**Anything else we need to know**:

""all"" should mean all the resources, not some of important resources.

",closed,False,2017-11-01 01:22:28,2018-07-27 14:28:43
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/114,https://api.github.com/repos/kubernetes/kubectl/issues/114,[UMBRELLA] Kubectl independence,"The goal of this issue is to track research, implementations and improvements that can be done in order to have a more independent development of Kubectl features.

This effort is based on @pwittrock documents: https://goo.gl/2RYvqG and https://goo.gl/j2hnrG

Kubectl independence would lead to less friction with other Kubernetes development, increased velocity, simplification of the testing infrastructure, of code components, and maybe its own release cadence.

Orthogonal to that issue would be a split of kubectl outside of the core repository.

Here is the list of things that needs to be figured out to improve the independence of Kubectl development:
- #115 Testing
- #116 Releasing
- #117 code refactoring
- #118 documentation
",closed,False,2017-11-01 21:27:21,2018-09-26 08:41:16
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/115,https://api.github.com/repos/kubernetes/kubectl/issues/115,[UMBRELLA] Kubectl independence: testing,"Part of #114.

Improve Kubectl independence by improving testing.

Currently, kubectl is tested against kubernetes and kubernetes is tested using kubectl. This tight coupling prevents kubectl from being tested quickly and adequately.

The goal of this issue is to track the steps and improvements that can be done to test kubectl independently from Kubernetes (or at least improve that state).",open,False,2017-11-01 21:52:41,2019-03-25 11:09:19
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/116,https://api.github.com/repos/kubernetes/kubectl/issues/116,[UMBRELLA] Kubectl independence: releasing,"Part of #114.

Improve Kubectl independence by releasing independently.

Kubectl is currently released as part of core kubernetes, on a 3-month schedule. 

The goal of this issue is to track the feasibility of releasing independently, and the challenges and improvements that needs to be done.",closed,False,2017-11-01 21:54:30,2018-08-13 00:05:21
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/117,https://api.github.com/repos/kubernetes/kubectl/issues/117,[UMBRELLA] Kubectl independence: code refactoring,"Part of #114.

Improve Kubectl independence by refactoring the code into well-documented, well-tested, re-usable pieces with mockable/fakeable interfaces. These pieces of code should have a limited number of dependencies (to avoid spaghetti).

The goal of this issue is to track the refactoring efforts.

- [ ] #132 Walking/Reading/Writing to Unstructured types with OpenAPI
- [ ] #133 Move packages from `k8s.io/kubernetes/pkg/kubectl` to `k8s.io/kubectl/pkg`",open,False,2017-11-01 21:56:33,2019-01-23 23:28:51
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/118,https://api.github.com/repos/kubernetes/kubectl/issues/118,[UMBRELLA] Kubectl independence: documentation,"Part of #114.

Improve Kubectl independence by improving its documentation.

This is mostly about:
- Documenting the code and the interfaces
- Maintaining kubectl documentation
- Document the release-process",closed,False,2017-11-01 21:58:35,2018-07-30 04:41:51
kubectl,shiywang,https://github.com/kubernetes/kubectl/issues/119,https://api.github.com/repos/kubernetes/kubectl/issues/119,kubectl convert should return error when out-version is an invalid value,"**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):


**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.0-alpha.1.2069+b518a80cef0302-dirty"", GitCommit:""b518a80cef03022132e701b73da43dc93f956eaa"", GitTreeState:""dirty"", BuildDate:""2017-11-02T06:08:38Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}


**What happened**:

```
➜  amd64 git:(convert_fix) ✗ kubectl convert -f test.yaml --local --output-version=no/exist 
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: php-apache
spec:
  maxReplicas: 10
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1beta1
    kind: Deployment
    name: php-apache
  targetCPUUtilizationPercentage: 50
status:
  currentReplicas: 0
  desiredReplicas: 0
```

**What you expected to happen**:
```
➜  amd64 git:(convert_fix) ✗ ./kubectl convert -f test.yaml --local --output-version=no/exist
error: 'no/exist' is not a registered version.
See 'kubectl convert -h' for help and examples.
```

**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",closed,False,2017-11-02 06:15:27,2017-11-03 19:07:53
kubectl,calind,https://github.com/kubernetes/kubectl/issues/120,https://api.github.com/repos/kubernetes/kubectl/issues/120,Cannot autocomplete kubectl bash aliases,"The default `kubectl` complete function `__start_kubectl` does not work with bash aliases:

```
# ~/.aliases
alias k=""kubectl""
complete -o default -F __start_kubectl k
```

__Actual output:__
```
~$ k [TAB]
k kubectl

~$ k k [TAB]
k kubectl
```

__Expected output:__
```
~$ k [TAB]
annotate        convert         expose          rolling-update
api-versions    cordon          get             rollout
apply           cp              label           run
attach          create          logs            scale
auth            delete          options         set
autoscale       describe        patch           taint
certificate     drain           plugin          top
cluster-info    edit            port-forward    uncordon
completion      exec            proxy           version
config          explain         replace
```",closed,False,2017-11-02 10:36:02,2019-03-20 09:09:59
kubectl,bkrannich,https://github.com/kubernetes/kubectl/issues/121,https://api.github.com/repos/kubernetes/kubectl/issues/121,Autocomplete issues with --from-file and --from-literal,"**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.2"", GitCommit:""bdaeafa71f6c7c04636251031f93464384d54963"", GitTreeState:""clean"", BuildDate:""2017-10-24T21:07:53Z"", GoVersion:""go1.9.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4"", GitTreeState:""dirty"", BuildDate:""2017-10-17T15:09:55Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: minikube
- **OS** (e.g. from /etc/os-release): macOS Sierra, 10.12.6
- **Kernel** (e.g. `uname -a`): `Darwin WDFM31654402A 16.7.0 Darwin Kernel Version 16.7.0: Wed Oct  4 00:17:00 PDT 2017; root:xnu-3789.71.6~1/RELEASE_X86_64 x86_64`
- **Install tools**: n/a
- **Others**: `GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin16)`


**What happened**:
I was following along the Kubernetes Up & Running book and found that autocomplete for some of the configmaps and secrets examples doesn't work as expected.

**What you expected to happen**:
I was expecting the autocomplete to work for the `--from-file` and `--from-literal` parameters, however, I got some bash errors.

**How to reproduce it** (as minimally and precisely as possible):
* Type:
```
kubectl create configmap my-config --from-file=<tab>
```
Result:
```
kubectl create configmap my-config --from-file=--from-file= (instead of a list of files in the local directory)
```
* Type:
```
kubectl create configmap my-config --from-file=my-config.txt --from-li<tab>
```
Result (my-config.txt should of course exist in the local directory):
```
kubectl create configmap my-config --from-file=my-config.txt --from-li-bash: --from-file=: syntax error: operand expected (error token is ""="")
```

Similar behavior for `kubectl create secret`.",closed,False,2017-11-06 10:02:46,2018-02-04 16:42:17
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/122,https://api.github.com/repos/kubernetes/kubectl/issues/122,Setup submit queue and prow testing for kubectl PRs,"Complete kubectl repo setup with issue / pr bots, submit queue, and PR tests

- We should make sure all the pull tests work against feature-branches as well as master.",closed,False,2017-11-06 19:44:42,2017-11-07 21:25:43
kubectl,smarterclayton,https://github.com/kubernetes/kubectl/issues/123,https://api.github.com/repos/kubernetes/kubectl/issues/123,kubectl CLI should more consistently and concisely convey discovery,"Moved from kubernetes/kubernetes#53387

----------

Now that discovery is complete as an API, and CRD and aggregation are in heavy use, we need to make some minor tweaks to kubectl to improve discoverability of apis from clients.

Common issues:

1. I want to see all resources that I might be able to create
2. I want to be able to disambiguate conflicting resources
3. I want to distinguish between cluster scoped and non-cluster scoped resources
4. I want an equivalent to the `all` shortcut that is actually all resource types that are namespace scoped

We want to orient brand new users to kubectl from a config perspective into:

1. what can i `get`
2. if i want to create a config resource, how do i specify it (`explain`, which could be better)
3. i want to know what a resource is *for*

Some thoughts:

1. `kubectl api-versions` is too simple, and needs to be better or removed
2. There should be a command that makes it easy to see the list of:
    1. all namespace scoped resources
    2. all cluster scoped resources
    3. the shortcuts of those resources
    4. what the resource does (potentially as a `-v` option)
    5. a brief discussion of how to deal with version conflicting names (pod v1beta.special.bar and pod v1)
    6. potentially what versions exist for those resources
3. `explain` and `get` should suggest that command when you specify something it doesn't recognize
4. We could piggyback this into get except discovery resources aren't quite a format API, so it would be somewhat hacky.  However, get and describe already work so a few special cases aren't terrible (we did reserve APIGroupList v1 and APIGroup v1)

Hypothetical

```
$ kubectl get apigroups
NAME      VERSIONS  RESOURCES
autoscaling v1         horizontalpodautoscaler

$ kubectl describe apis
API Groups
  autoscaling:
    v1: 
       Resources: horizontalpodautoscaler
    v2:
       ...

$ kubectl get apis
NAME      VERSIONS  RESOURCES
autoscaling v1         horizontalpodautoscaler

$ kubectl apis
GROUP      VERSIONS  RESOURCES
autoscaling v1         horizontalpodautoscaler

$ kubectl apis --group=autoscaling --version=v1
GROUP      VERSIONS  RESOURCES
autoscaling v1         horizontalpodautoscaler

$ kubectl explain
error: You must select a resource type to explain:

   Nicely formatted two column table - one for namespace scoped and one for cluster scoped
```

It should be easy to select all namespaced resources from a command - we should add:

```
$ kubectl get all-namespaced --all
$ kubectl describe all-namespaced
$ kubectl delete all-resources --all
```

We can't change `all` because of backwards compatibility (anyone with `kubectl delete all --all` would be broken if we change what all covers), but also because it's deliberately short to remain easy to type for end users.

----

This is really annoying now that I'm dealing with more and more groups.

Explain should tell me what group something is
I need to be able to see info on a given resource type in the exact form that I would specify it to get (the so called ""resource-arg"" form resource.version.group). I.e. ""I got an error that told me this resource doesn't do what I think - how do i find out more about it"".
Error messages need to do a better job of returning a discoverable type (for 2) - right now we're doing either the struct print for many of them or the GroupVersionResource.String() method which is not an accepted form.
I think this is the #1 issue dealing with extensions - no place to go to find them, no way to know how to use them in kubectl, no way to check that the ones I care about are registered.

Use cases:

When I as a user get errors about mismatches between what I typed and what is available on the server, I need to discover what is on the server
As a user, I expect to get canonical representations of resources so as to avoid having to explain how to transform the representations myself


---------

We already have `kubectl api-versions` and there was a PR to add `kubectl api-resources`.  But when I think about the operations that these commands perform, and that I may want to describe the objects they return in more detail, I think it makes more sense to expose these as resources under kubectl get and describe.  That also makes it easier to suggest options:

```
$ kubectl get
usage: This command expects a resource type and an optional name of a resource to locate.  Use `kubectl get apiresources` to see all valid resource types.
```

",open,False,2017-11-08 02:52:40,2018-09-25 21:17:16
kubectl,shiywang,https://github.com/kubernetes/kubectl/issues/124,https://api.github.com/repos/kubernetes/kubectl/issues/124,[E2E_TEST] Move run_initializer_tests into runTests(),"In `hack/make-rules/test-cmd-util.sh` e2e function [run_initializer_test](https://github.com/kubernetes/kubernetes/blob/5590c1fb94e09f9a2b7a09c9776e8d6535b18304/hack/make-rules/test-cmd-util.sh#L4968) was not being record by function `record_command`, it would be better if we can records its output/error messages in junit format",open,False,2017-11-08 03:13:24,2019-01-23 23:29:26
kubectl,AlexandreRoba,https://github.com/kubernetes/kubectl/issues/125,https://api.github.com/repos/kubernetes/kubectl/issues/125,Kubectl completion zsh error ,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): Kubectl completion zsh

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.2"", GitCommit:""bdaeafa71f6c7c04636251031f93464384d54963"", GitTreeState:""clean"", BuildDate:""2017-10-24T19:48:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7+"", GitVersion:""v1.7.8-gke.0"", GitCommit:""a7061d4b09b53ab4099e3b5ca3e80fb172e1b018"", GitTreeState:""clean"", BuildDate:""2017-10-10T18:48:45Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**: Cloud
- **OS** (e.g. from /etc/os-release): Mac OS High Sierra 10.13.2 Beta
- **Kernel** (e.g. `uname -a`):
- **Install tools**: 
Iterm2
shell zsh 5.2
- **Others**:


**What happened**:
source <(Kubectl completion zsh) -> returns a SIG 127 when .zshrc completes
The autocomplete seems to work but I get an error every time I start an Iterm2 zsh terminal

**What you expected to happen**:
No error

**How to reproduce it** (as minimally and precisely as possible):
source <(Kubectl completion zsh)

**Anything else we need to know**:

",open,False,2017-11-08 12:51:02,2019-03-13 22:51:51
kubectl,mahdix,https://github.com/kubernetes/kubectl/issues/126,https://api.github.com/repos/kubernetes/kubectl/issues/126,"Update type switches in ""kind_visitor""","(Quoting from @janetkuo)
```
The type switches in GroupKindElement.Accept() needs to be updated:
https://github.com/kubernetes/kubernetes/blob/33f873dbbee58caffcff9e7b44f174a32ec1df92/pkg/kubectl/apps/kind_visitor.go#L51-L52

ReplicaSet exists in ""apps"" group as well.
CronJob is missing.
```
This is a follow up from https://github.com/kubernetes/kubectl/issues/73.

",closed,False,2017-11-09 14:17:41,2017-11-17 13:08:08
kubectl,mahdix,https://github.com/kubernetes/kubectl/issues/127,https://api.github.com/repos/kubernetes/kubectl/issues/127,"Replace type ""switch"" statements with KindVisitor in ""rollback""","This is a follow-up ticket from https://github.com/kubernetes/kubectl/issues/73

Quoting @janetkuo:

> Btw, based on the description, this issue is more general than just HistoryViewer? For example, rollback.go uses type switches too: https://github.com/kubernetes/kubernetes/blob/33f873dbbee58caffcff9e7b44f174a32ec1df92/pkg/kubectl/rollback.go#L59-L69
",closed,False,2017-11-09 14:20:44,2017-12-14 21:54:38
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/128,https://api.github.com/repos/kubernetes/kubectl/issues/128,Clarify requirements in README.md,,closed,True,2017-11-13 21:55:07,2017-11-21 00:43:48
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/129,https://api.github.com/repos/kubernetes/kubectl/issues/129,Use deps,Use github.com/golang/deps to manage dependencies. It's cleaner.,closed,True,2017-11-13 22:19:19,2017-11-28 22:51:24
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/130,https://api.github.com/repos/kubernetes/kubectl/issues/130,"Simplify travis script, work against `vendor/`","The current travis script currently fails if we have a `vendor/`
directory, and is also quite longer than necessary.",closed,True,2017-11-13 22:44:41,2017-11-28 22:34:22
kubectl,wgordon17,https://github.com/kubernetes/kubectl/issues/131,https://api.github.com/repos/kubernetes/kubectl/issues/131,Please add fish completion,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): ""fish""

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FEATURE REQUEST

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.
-->

I would like to be able to run `source (kubectl completion fish)` from my `fish` based shell to enable `kubectl` completion

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.3"", GitCommit:""f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd"", GitTreeState:""clean"", BuildDate:""2017-11-09T07:26:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""darwin/amd64""}
",closed,False,2017-11-14 14:47:35,2018-11-09 07:04:58
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/132,https://api.github.com/repos/kubernetes/kubectl/issues/132,Kubectl independence: Walking/Reading/Writing to Unstructured types with OpenAPI,"It'd be nice to be able to use Unstructured types as easily as actual Go types.

We could give some thoughts to use OpenAPI to provide a nice framework to find fields, read values, and changes some fields.

This issue will track these efforts.

This is also orthogonal to fixing the union types in the kubernetes API (issue #?).",open,False,2017-11-15 18:17:22,2018-09-25 21:24:12
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/133,https://api.github.com/repos/kubernetes/kubectl/issues/133,Kubectl Independence: Move packages from `k8s.io/kubernetes/pkg/kubectl` to `k8s.io/kubectl/pkg`,"Rather than moving everything from Kubectl directly to a new repo, let's move packages that could be use either by Kubectl or by any other kubernetes client application, to `k8s.io/kubectl`.

I'll try to keep track of the packages moved/to be moved in that issue (help wanted):

- [ ] #135 Move `k8s.io/kubernetes/pkg/kubectl/apply` to `k8s.io/kubectl/pkg/apply`
- [ ] #136 Move `k8s.io/kubernetes/pkg/kubectl/explain` to `k8s.io/kubectl/pkg/explain`
- [ ] #137 Move `k8s.io/kubernetes/pkg/kubectl/cmd/util/openapi` to `k8s.io/kubectl/pkg/openapi` 

The goal is not to copy-paste the code and call it the day, but to take that opportunity to:
- Add missing tests,
- Refactor the code to have clean interfaces, prune most dependencies,
- Add more tests,
- Document the interface properly, make sure they look nice on `godoc.org`,
- Add test-examples for the documentation

~~Once the code is copied in that repo, it needs to be vendored in `k8s.io/kubernetes` and replaced there.~~

@mengqiy wrote a proposal: https://docs.google.com/document/d/1yba5jRJSVf1IIXYBO11N8nHbwTbzkDFDssADIC4nbig/edit?usp=sharing",closed,False,2017-11-15 18:25:28,2018-05-12 02:40:53
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/134,https://api.github.com/repos/kubernetes/kubectl/issues/134,Write `kubectl alpha apply`,"@pwittrock has written a great new piece of code to do apply using openapi. It's much easier to read, modify and use. It's not used at all for now though. It would be nice to build a new `kubectl alpha apply` to start using it. Maybe also fix the bugs.

We could also, on top of that, add new layers to fix the broken behavior in `kubectl apply` that we can't fix with the existing code.

@pwittrock Can you please write a list of these broken behaviors?",closed,False,2017-11-15 21:55:01,2018-09-25 20:08:13
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/135,https://api.github.com/repos/kubernetes/kubectl/issues/135,Move `k8s.io/kubernetes/pkg/kubectl/apply` to `k8s.io/kubectl/pkg/apply`,"And make sure it's properly tested, documented, with nice examples, few dependencies.",closed,False,2017-11-15 22:45:55,2018-05-27 18:49:04
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/136,https://api.github.com/repos/kubernetes/kubectl/issues/136,Move `k8s.io/kubernetes/pkg/kubectl/explain` to `k8s.io/kubectl/pkg/explain`,"And make sure it's properly tested, documented, with nice examples, few dependencies.",closed,False,2017-11-15 22:46:57,2018-05-15 00:49:50
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/137,https://api.github.com/repos/kubernetes/kubectl/issues/137,Move `k8s.io/kubernetes/pkg/kubectl/cmd/util/openapi` to `k8s.io/kubectl/pkg/openapi` ,"Or maybe it should go somewhere else, to be thought through.",closed,False,2017-11-15 22:48:45,2018-05-15 00:49:50
kubectl,mengqiy,https://github.com/kubernetes/kubectl/issues/138,https://api.github.com/repos/kubernetes/kubectl/issues/138,Use structured generator for kubectl autoscale,"`kubectl autoscale` is using parameter injection style generator at the moment.

It should implement the [structured generator interface](https://github.com/kubernetes/kubernetes/blob/c3ed0f26636d0fc5c3905a0c09bf4886da8512c9/pkg/kubectl/generate.go#L49-L52) like deployment generator: https://github.com/kubernetes/kubernetes/blob/c3ed0f26636d0fc5c3905a0c09bf4886da8512c9/pkg/kubectl/deployment.go#L119-L147.
After implementing the interface, we should
- refactor `kubectl autoscale` to use  the new generator
- create [`kubectl create hpa`](https://github.com/kubernetes/kubernetes/issues/39833) (optional)

When implementing the interface, make sure you generate a external type object. ",closed,False,2017-11-16 05:53:10,2018-02-26 01:40:58
kubectl,zzq392342084,https://github.com/kubernetes/kubectl/issues/139,https://api.github.com/repos/kubernetes/kubectl/issues/139,"kubectl logs Error from server (Forbidden): Forbidden (user=system:anonymous, verb=get, resource=nodes, subresource=proxy)","kubectl create command works correctly,but kubectl logs and kubectl exec not : 
Error from server (Forbidden): Forbidden (user=system:anonymous, verb=get, resource=nodes, subresource=proxy)",closed,False,2017-11-16 11:34:56,2017-12-06 19:23:09
kubectl,naisanza,https://github.com/kubernetes/kubectl/issues/140,https://api.github.com/repos/kubernetes/kubectl/issues/140,"kubectl -n kube-system and kubectl --all-namespaces, differ in behavior","
**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): kube-system all-namespaces 

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): Bug?

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.3"", GitCommit:""f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd"", GitTreeState:""clean"", BuildDate:""2017-11-08T18:39:33Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.3"", GitCommit:""f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd"", GitTreeState:""clean"", BuildDate:""2017-11-08T18:27:48Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: 
- **OS** (e.g. from /etc/os-release): 16.04.3 LTS (Xenial Xerus)
- **Kernel** (e.g. `uname -a`): 4.4.0-98-generic
- **Install tools**: `zfs` `bidge`
- **Others**:


**What happened**:
kubectl -n kube-system get sa:
```
root@bigma:~# kubectl -n kube-system get sa
NAME                         SECRETS   AGE
attachdetach-controller      1         18h
bootstrap-signer             1         18h
certificate-controller       1         18h
cronjob-controller           1         18h
daemon-set-controller        1         18h
default                      1         18h
deployment-controller        1         18h
disruption-controller        1         18h
endpoint-controller          1         18h
flannel                      1         18h
generic-garbage-collector    1         18h
horizontal-pod-autoscaler    1         18h
job-controller               1         18h
kube-dns                     1         18h
kube-proxy                   1         18h
kubernetes-dashboard         1         18h
namespace-controller         1         18h
node-controller              1         18h
persistent-volume-binder     1         18h
pod-garbage-collector        1         18h
replicaset-controller        1         18h
replication-controller       1         18h
resourcequota-controller     1         18h
service-account-controller   1         18h
service-controller           1         18h
statefulset-controller       1         18h
token-cleaner                1         18h
ttl-controller               1         18h
```

kubectl --all-namespaces get sa:
```
root@bigma:~# kubectl --all-namespaces get sa
Error: unknown command ""sa"" for ""kubectl""

Did you mean this?
        set
        cp

Run 'kubectl --help' for usage.
```

**What you expected to happen**:
Both to exhibit the same behavior, whether `-n` is used or `--all-namespaces` is used

**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",closed,False,2017-11-16 21:24:00,2018-04-14 14:41:36
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/141,https://api.github.com/repos/kubernetes/kubectl/issues/141,OpenAPI: Use OpenAPI semantic rather than x-kubernetes custom directives,"We currently implement union through the `x-kubernetes-patch-strategy: retainKeys` sub-strategy. We could replace that with OpenAPI ""oneof"" semantic, available in [OpenAPI 3.0](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md).

We might be able to remove other custom directives by having a better definition of our objects.",closed,False,2017-11-16 23:36:12,2018-05-30 18:00:44
kubectl,wrossmann,https://github.com/kubernetes/kubectl/issues/142,https://api.github.com/repos/kubernetes/kubectl/issues/142,"`kubectl proxy --accept-hosts` appears to filter destination, not source.","**Is this a BUG REPORT or FEATURE REQUEST?**: BUG REPORT

**Kubernetes version**:
- Client: v1.8.3
- Server: v1.7.8-gke.0

**Environment**:
- **Cloud provider**: Google [GKE]
- **OS**:
    - Client: CentOS 6/7 [client]
    - Server: cos-stable-61-9765-66-0
- **Install tools**: gcloud

**What happened**:
In my development VM [10.0.1.100] I started kubectl proxy to provide full access to the Dashboard UI to my workstation [10.0.2.200], but not everyone else in the office. The command used was:

    kubectl proxy --port=8080 --address=10.0.1.100 --accept-hosts='^10\.0\.2\.200$'

as well as many variants of the regular expression with/without `\.`, `^`, and `$`

Which yielded the following for all endpoints:

    <h3>Unauthorized</h3>

It was suggested in the `#kubernetes-users` Slack channel that it may be running the regex against a hostname, but I've double-checked and we do not have reverse lookups enabled for our network, and I don't think that `kubectl` would be looking up NetBIOS info.

It can be made to work with `--accept-hosts='^.*$'` as you would expect, however I also decided to try `--accept-hosts='^10\.0\.1\.100$'` on a hunch, which ended up working the same as the wildcard, allowing *anyone on the network* access to the proxy.

**What you expected to happen**:

1. `--accept-hosts='^10\.0\.2\.200$'` should allow *only* the stated IP access to the proxy.
    - actual: universal inaccessibility
2. `--accept-hosts='^10\.0\.1\.100$'` should allow only local requests, similar to the `127.0.0.1` defaults.
    - actual: open access to all

**How to reproduce it**:

With two hosts attached to the same network:
- Host A: `kubectl proxy --port=8080 --address=HOST_A_IP --accept-hosts='^HOST_B_IP$'`
- Host B: `curl http://HOST_A_IP:8080/`

**Anything else we need to know**:

This has wider implications than me simply being paranoid about my local network. In searching for solutions to this problem I found pages upon pages of people stating `--accept-hosts='^.*$'` and `--disable-filter=true` as general solutions, even in the face of machines with public-facing interfaces. The supposed workarounds for this issue are causing people to unwittingly expose very sensitive information and APIs to potentially the entire world unless they take additional action to prevent it, eg: firewall rules.

eg: See the discussion in https://github.com/kubernetes/dashboard/issues/692",open,False,2017-11-16 23:46:42,2019-03-26 11:14:27
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/143,https://api.github.com/repos/kubernetes/kubectl/issues/143,Enable use-openapi-print-columns in kubectl,"If the openapi scheme lists the columns to print for an object, we should print them.",closed,False,2017-11-17 01:55:36,2017-11-18 19:36:19
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/144,https://api.github.com/repos/kubernetes/kubectl/issues/144,Check in kinflate,"check in
- kinflate code
- add an example
- add script to copy code to `/vendor`
- update `/vendor`
",closed,True,2017-11-17 23:52:26,2017-12-05 22:48:31
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/145,https://api.github.com/repos/kubernetes/kubectl/issues/145,Tweak pre-commit to ignore vendor directory.,,closed,True,2017-11-18 01:30:13,2018-04-06 21:27:43
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/146,https://api.github.com/repos/kubernetes/kubectl/issues/146,"--show-labels shows up on ""kubectl create [...]""","- (ideally) `--show-labels` should not show up at `kubectl create service` command('s help)
- and it should error out when it's silently ignored like it's done today

For example:

```sh
➜  ~ kubectl create service  loadbalancer --show-labels lb-1 --tcp=80:80
service ""lb-1"" created
```

this command should fail and say `invalid flag ""--show-labels"" for ""create""` or something like that.",closed,False,2017-11-19 23:49:55,2018-05-22 09:42:16
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/147,https://api.github.com/repos/kubernetes/kubectl/issues/147,"""kubectl describe"" events query is too specific","Right now if I run a command like `kubectl describe service foo -v=10` I see that the query to retrieve `Events` is like:

`I1120 14:07:00.770878   34700 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.8.2 (darwin/amd64) kubernetes/bdaeafa"" https://35.192.225.48/api/v1/namespaces/doctor/events?fieldSelector=involvedObject.kind%3DService%2CinvolvedObject.uid%3D8c1f404b-ce32-11e7-b2c9-42010a8000f2%2CinvolvedObject.name%3Dlb-16%2CinvolvedObject.namespace%3Ddoctor`

To parse that `fieldSelector` out:
- involvedObject.kind=Service
- involvedObject.uid=8c1f404b-ce32-11e7-b2c9-42010a8000f2
- involvedObject.name=lb-16
- involvedObject.namespace=doctor

Are all these field selectors really necessary? I feel like `involvedObject.uid=8c1f404b-ce32-11e7-b2c9-42010a8000f2` would be sufficient?

I'm not sure how fieldSelectors works internally, or whether this whole thing needs to be sent over the wire for some reason (maybe altogether it forms the ObjectReference or something). I'm not sure if this has a performance burden on the API/etcd either. Just wanted to point out if it's an oversight and can be made more concise.",open,False,2017-11-20 22:10:14,2019-01-23 23:28:08
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/148,https://api.github.com/repos/kubernetes/kubectl/issues/148,kubectl create service: ignores extra positional arguments,"See:

    $ kubectl create service loadbalancer --tcp=80:80 lb-1 lb-2 lb-3
    service ""lb-1"" created

it will silently ignore `lb-2` and `lb-3` arguments. If command takes exactly 1 positional argument, it should fail when executed command is not exactly 1.",closed,False,2017-11-20 23:10:38,2018-01-02 22:09:46
kubectl,totherme,https://github.com/kubernetes/kubectl/pull/149,https://api.github.com/repos/kubernetes/kubectl/issues/149,Beginnings of a test framework,"As discussed in https://github.com/kubernetes/kubectl/issues/87 and in slack and hangouts.

Known issues:
- If etcd starts slowly, the apiserver might not start in a sane state. This will be fixed soon.
- If anything is currently listening on any of the ports we use, there will be clashes.
",closed,True,2017-11-21 16:51:31,2017-11-30 20:55:10
kubectl,mengqiy,https://github.com/kubernetes/kubectl/issues/150,https://api.github.com/repos/kubernetes/kubectl/issues/150,Remove Registry usage in kubectl,"Echo what @caesarxuchao said in https://github.com/kubernetes/kubernetes/pull/55388#issuecomment-346227092:
```
In short, I think we should try to refactor kubectl to not depending on the install packages, rather than exporting them.
Users (including kubectl) should only need to populate scheme by calling AddToScheme.

Compared to AddToScheme, the additional things done by the install package are populating the GroupFactoryRegistry and the Registry, but neither seem to be useful.

I searched the code it seems no one is using the GroupFactoryRegistry.

As for the Registry, it has two functionalities,

- Building RESTMapper, but we are moving to build the mapper dynamically, so no one should need that.
- Recording what group/versions are registered and enabled. I think kubectl should allow any group/versions that are recognized by its scheme,
  so registry is not useful. I don't see how other users will need the registry either.
  - I think registry is useful in tests, but each test should just create their own registry.
```

IMO we can (should) get rid of `Registry` and `GroupFactoryRegistry` in [`legacyscheme`](https://github.com/kubernetes/kubernetes/blob/4e2f5e2212b05a305435ef96f4b49dc0932e1264/pkg/api/legacyscheme/scheme.go#L28-L33) and [`kubectl's scheme`](https://github.com/kubernetes/kubernetes/blob/4e2f5e2212b05a305435ef96f4b49dc0932e1264/pkg/kubectl/scheme/scheme.go#L30-L34).
It seems kubectl is the only consumer of `Registry` now.

> Building RESTMapper, but we are moving to build the mapper dynamically, so no one should need that.

IIUC what RESTMapper does is just map `<Group, Version, Kind>` to a URL. So if kubectl need to run in standalone mode (e.g. --local),  it doesn't need a RESTMapper. IOW, it doesn't need to construct the RESTMapper from `Registry`. But we need to refactor kubectl's `builder` pkg to make it possible.

Did I miss something important?

@lavalamp @smarterclayton @liggitt @sttts Thoughts or concerns?

cc: @pwittrock @fabianofranz @monopole 
",open,False,2017-11-22 19:16:33,2019-03-25 12:10:24
kubectl,tback,https://github.com/kubernetes/kubectl/issues/151,https://api.github.com/repos/kubernetes/kubectl/issues/151,kubectl get all does not list all resources in a namespace,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
get all, get all resources 

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.2"", GitCommit:""bdaeafa71f6c7c04636251031f93464384d54963"", GitTreeState:""clean"", BuildDate:""2017-10-24T19:48:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.2-gke.0"", GitCommit:""52ea03646e64b35a5b092ab32bb529400c296aa6"", GitTreeState:""clean"", BuildDate:""2017-10-24T23:31:18Z"", GoVersion:""go1.8.3b4"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**:
gke
- **OS** (e.g. from /etc/os-release):
macos 10.12.6

**What happened**:
`kubectl get all` does not list all resources in a namespace.

**What you expected to happen**:
As a user performing `kubectl get all` I expect to see all objects in the current context, including limits.

**How to reproduce it** (as minimally and precisely as possible):
```
kubectl create namespace tmp
kubectl create -f https://k8s.io/docs/tasks/administer-cluster/memory-defaults.yaml --namespace=tmp
kubectl get all --namespace tmp
namespace ""tmp"" created
limitrange ""mem-limit-range"" created
No resources found.
```
",open,False,2017-11-28 11:21:38,2019-03-05 18:09:59
kubectl,totherme,https://github.com/kubernetes/kubectl/issues/152,https://api.github.com/repos/kubernetes/kubectl/issues/152,The Test Framework should not require special treatment in pre-commit.sh,"There is an MVP of the test framework discussed in #87 , but it relies on having special treatment in pre-commit.sh to pass the travis tests. In particular:

- The test framework directory is currently exempt from having to run `go test ./...`
- Instead, a custom script `pkg/framework/test/scripts/run-tests.sh` is called, which runs the tests using the ginkgo CLI

The tests of the test framework should be changed so as not to require this special treatment. At first glance, this will involve:

- Changing the framework to bring up etcd and the apiserver on random unused ports, instead of the default ones
- Having sensible defaults for all environment variables that the framework relies on.

This issue began as a conversation with @apelisse in #149 ",closed,False,2017-11-30 11:18:15,2017-12-07 19:03:08
kubectl,totherme,https://github.com/kubernetes/kubectl/issues/153,https://api.github.com/repos/kubernetes/kubectl/issues/153,The Test Framework's test fakes shouldn't be so tightly coupled with the implementations they test,"In the test framework MVP (discussed in #87 ), there is currently a tight coupling between the test fakes `pkg/frameworks/test/assets/fakeetcd/` and `etcd.go`. In particular, both of these currently know the exact flags that `etcd` ought to be called with. There is a similar issue with the apiserver fake.

We should find a way to adequately test this wiring which doesn't leave us with tightly coupled test-doubles like this. This might involve leaning more heavily on the integration tests for the happy path, and removing the unit-level happy path tests.

This issue began as a conversation with @apelisse in #149",closed,False,2017-11-30 11:26:47,2017-12-05 22:26:13
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/154,https://api.github.com/repos/kubernetes/kubectl/issues/154,Proper fake for etcd,"@apelisse , this is the first part of us removing this fake binaries and thus the tight coupling as described in https://github.com/kubernetes/kubectl/issues/153 .",closed,True,2017-12-04 12:31:02,2017-12-06 15:12:55
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/155,https://api.github.com/repos/kubernetes/kubectl/issues/155,Proper fake for apiserver,"@apelisse , this is the second part of us removing this fake binaries and thus the tight coupling as described in #153 .

Related to #154 .",closed,True,2017-12-04 13:54:58,2017-12-06 15:12:47
kubectl,mengqiy,https://github.com/kubernetes/kubectl/issues/156,https://api.github.com/repos/kubernetes/kubectl/issues/156,Apply should not assume that publishing openapi spec means support strategic merge patch,"It was discussed in https://github.com/kubernetes/kubernetes/pull/51321#pullrequestreview-80212182.

According to https://github.com/kubernetes/kubernetes/pull/51321#discussion_r154271945, what we should do for this release is:
send strategic merge patch if the type is registered, JSON merge patch otherwise.

Creating this issue for tracking purpose.
",closed,False,2017-12-04 17:40:24,2017-12-04 18:55:17
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/157,https://api.github.com/repos/kubernetes/kubectl/issues/157,No special treatment for test framework,"Based on top of
- #154 
- #155 

As discussed with @apelisse .

[closes #152]",closed,True,2017-12-05 13:58:24,2017-12-22 14:49:24
kubectl,AlexKhotian,https://github.com/kubernetes/kubectl/pull/158,https://api.github.com/repos/kubernetes/kubectl/issues/158,Move `openapi` package from `k8s.io/kubernetes` to `k8s.io/kubectl`,"- Moved packages openapi and validation;
- Improved  a bit the documentation of openapi package;
- Added examples for validation and openapi_getter;
- Cherry-picked a swagger.json for tests(validation_test and openapi_test)

Fixes #137",closed,True,2017-12-05 17:33:36,2017-12-13 20:23:11
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/159,https://api.github.com/repos/kubernetes/kubectl/issues/159,WIP: Check in initial kinflate,"This supersede https://github.com/kubernetes/kubectl/pull/144.

Please ignore the following 2 commits for now.
`hacky update vendor`: this commit will be replaced by another PR that is purely generated by dep, after we fix the publish bot.
`move pkgs`: this commit will be replaced by another PR that move some packages from `k8s.io/kubernetes` to `k8s.io/kubectl`.

`add example`: this commit contains an example. You can try it by running `kinflate -f pkg/kinflate/example/instances/exampleinstance/`
`implement initial kinflate`: this commit is kinflate code.

We should not merge this PR until the PRs for 1st and 2nd are merged.
",closed,True,2017-12-05 22:47:46,2017-12-15 19:27:01
kubectl,wackxu,https://github.com/kubernetes/kubectl/pull/160,https://api.github.com/repos/kubernetes/kubectl/issues/160,[WIP] Move `k8s.io/kubernetes/pkg/kubectl/explain` to `k8s.io/kubectl/pkg/explain,fix https://github.com/kubernetes/kubectl/issues/136,closed,True,2017-12-06 08:01:34,2018-05-07 18:59:07
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/issues/161,https://api.github.com/repos/kubernetes/kubectl/issues/161,"In the testing framework, the Etcd struct should have sane 0-value behaviour","In #157 @apelisse commented:

```
// I want default behaviors
APIServer{}.Start()
Etcd{}.Start()
// Yay!
```

That doesn't currently work. Let's make the Etcd bit of it it work. (the apiserver bit is in #162, and 
making it work for `Fixtures{}` is #163)

More specifically:

`&Etcd{}.Start()` should start an etcd server listening on some random port. We ought to be able to query the struct for that port.",closed,False,2017-12-07 11:18:58,2017-12-08 18:28:14
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/issues/162,https://api.github.com/repos/kubernetes/kubectl/issues/162,"In the testing framework, the APIServer struct should have sane 0-value behaviour","In #157 @apelisse commented:

```
// I want default behaviors
APIServer{}.Start()
Etcd{}.Start()
// Yay!
```

That doesn't currently work. Let's make the APIServer bit of it it work. (the etcd bit is in #161, and 
making it work for `Fixtures{}` is #163)

More specifically:

`&APIServer{}.Start()` should start an etcd server listening on some random port, then start an api server that talks to that etcd. The api server should be listening on a different random port. We ought to be able to query the APIServer struct for the api server port.

## Things to think about:

### Runtime dependancy on etcd

We think that for a 0-value APIServer to start successfully, one of the following three cases needs to hold:
  1. The api server binary can run without talking to etcd. This is not the case.
  2. The APIServer struct default behaviour is to connect its api server binary to an etcd running on the default port. Testing this with `go test ./...` is tricky, because those tests all run concurrently, and ports can clash. See the work in #157 to remove our dependency on a default etcd port.
  3. The APIServer struct could be responsible for starting etcd as well as the api server. This way, we can get the port that etcd actually starts on, and connect to that.

Of these three options, only option (3) seems tenable. If we go down that route, this means that  something like the Fixtures struct can no-longer be responsible for starting the api server and etcd in parallel. Whatever the APIServer does to start etcd is how it has to be.

### Possible future dependancies

If we find that we want to add other services that the APIServer needs to talk to (such as a controller manager, or a fake scheduler, or whatever), then we might hit something like this issue again:
  1. The api server binary can run without talking to the controller manager. This *is* the case.
  2. The APIServer struct default behaviour is to connect its api server binary to a controller manager running on the default port. Testing this with `go test ./...` is tricky, because those tests all run concurrently, and ports can clash.
  3. The APIServer struct could be responsible for starting the controller manager as well as the api server. This way, we can get the port that the controller manager actually starts on, and connect to that.

If we choose option (3) this time, then the APIServer becomes something like the current implementation of the Fixtures, coordinating processes, but in addition to that also starting the api server process. It seems that for all non-etcd future dependencies, we should choose option 1.",closed,False,2017-12-07 11:19:12,2017-12-15 00:03:30
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/issues/163,https://api.github.com/repos/kubernetes/kubectl/issues/163,"In the testing framework, the Fixtures struct should have sane 0-value behaviour","In #157 @apelisse commented:

```
// I want default behaviors
APIServer{}.Start()
Etcd{}.Start()
// Yay!
```

That doesn't currently work for either of those structs, or even for the `Fixtures{}` struct. Let's make it work for `Fixtures`. (the etcd bit is in #161, and the apiserver bit is in #162).

More specifically:

`&Fixtures{}.Start()` should behave like `NewFixtures(""some/sane/etcd/binary/path"", ""some/sane/apiserver/path"").Start()`",closed,False,2017-12-07 11:50:49,2017-12-14 23:21:29
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/164,https://api.github.com/repos/kubernetes/kubectl/issues/164,Understand and potentially remove `k8s.io/kubernetes/pkg/kubectl/testing`,"I don't understand what this is for. The `doc.go` file is plain wrong as it's just been copy-pasted from somewhere else (likely `k8s.io/kubernetes/pkg/kubectl`) and left unmodified.

Also the code is barely used anywhere, I'm wondering if we could remove that.
",closed,False,2017-12-07 21:28:17,2018-01-03 08:30:31
kubectl,wackxu,https://github.com/kubernetes/kubectl/pull/165,https://api.github.com/repos/kubernetes/kubectl/issues/165,add gitignore file,/assign @apelisse ,closed,True,2017-12-08 07:27:40,2018-06-24 07:37:31
kubectl,totherme,https://github.com/kubernetes/kubectl/pull/166,https://api.github.com/repos/kubernetes/kubectl/issues/166,Add a sane default constructor for the Etcd and EtcdConfig structs,"[closes #161]

We need to do some calculation to find binary paths and free ports, so we can't make a 0-value Etcd struct useful. But we can provide a constructor that takes no arguments, and gives you good defaults.

Cc @hoegaarden @apelisse ",closed,True,2017-12-08 14:42:48,2017-12-08 18:28:14
kubectl,mindprince,https://github.com/kubernetes/kubectl/issues/167,https://api.github.com/repos/kubernetes/kubectl/issues/167,"`kubectl get all --all-namespaces` has duplicate output for `ds`, `deploy` and `rs`.","**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
/kind bug

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.4"", GitCommit:""9befc2b8928a9426501d3bf62f72849d5cbcd5a3"", GitTreeState:""clean"", BuildDate:""2017-11-20T19:12:24Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.4-gke.0"", GitCommit:""04502ae78d522a3d410de3710e1550cfb16dad4a"", GitTreeState:""clean"", BuildDate:""2017-11-27T19:19:56Z"", GoVersion:""go1.8.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

`kubectl get all --all-namespaces` has duplicate output for `ds`, `deploy` and `rs`.
```
$ kubectl get all --all-namespaces
NAMESPACE     NAME                    DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                              AGE
kube-system   ds/fluentd-gcp-v2.0.9   3         3         3         3            3           beta.kubernetes.io/fluentd-ds-ready=true   6m

NAMESPACE     NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-system   deploy/event-exporter-v0.1.7   1         1         1            1           6m
kube-system   deploy/heapster-v1.4.3         1         1         1            1           6m
kube-system   deploy/kube-dns                2         2         2            2           6m
kube-system   deploy/kube-dns-autoscaler     1         1         1            1           6m
kube-system   deploy/kubernetes-dashboard    1         1         1            1           6m
kube-system   deploy/l7-default-backend      1         1         1            1           6m

NAMESPACE     NAME                                  DESIRED   CURRENT   READY     AGE
kube-system   rs/event-exporter-v0.1.7-7cb7c5d4bf   1         1         1         6m
kube-system   rs/heapster-v1.4.3-57b54d54dc         0         0         0         6m
kube-system   rs/heapster-v1.4.3-6c6c9549df         1         1         1         5m
kube-system   rs/heapster-v1.4.3-f5c9c667d          0         0         0         5m
kube-system   rs/kube-dns-778977457c                2         2         2         6m
kube-system   rs/kube-dns-autoscaler-7db47cb9b7     1         1         1         6m
kube-system   rs/kubernetes-dashboard-76c679977c    1         1         1         6m
kube-system   rs/l7-default-backend-6497bcdb4d      1         1         1         6m

NAMESPACE     NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-system   deploy/event-exporter-v0.1.7   1         1         1            1           6m
kube-system   deploy/heapster-v1.4.3         1         1         1            1           6m
kube-system   deploy/kube-dns                2         2         2            2           6m
kube-system   deploy/kube-dns-autoscaler     1         1         1            1           6m
kube-system   deploy/kubernetes-dashboard    1         1         1            1           6m
kube-system   deploy/l7-default-backend      1         1         1            1           6m

NAMESPACE     NAME                    DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                              AGE
kube-system   ds/fluentd-gcp-v2.0.9   3         3         3         3            3           beta.kubernetes.io/fluentd-ds-ready=true   6m

NAMESPACE     NAME                                  DESIRED   CURRENT   READY     AGE
kube-system   rs/event-exporter-v0.1.7-7cb7c5d4bf   1         1         1         6m
kube-system   rs/heapster-v1.4.3-57b54d54dc         0         0         0         6m
kube-system   rs/heapster-v1.4.3-6c6c9549df         1         1         1         5m
kube-system   rs/heapster-v1.4.3-f5c9c667d          0         0         0         5m
kube-system   rs/kube-dns-778977457c                2         2         2         6m
kube-system   rs/kube-dns-autoscaler-7db47cb9b7     1         1         1         6m
kube-system   rs/kubernetes-dashboard-76c679977c    1         1         1         6m
kube-system   rs/l7-default-backend-6497bcdb4d      1         1         1         6m

NAMESPACE     NAME                                                     READY     STATUS    RESTARTS   AGE
kube-system   po/event-exporter-v0.1.7-7cb7c5d4bf-lhh5b                2/2       Running   0          6m
kube-system   po/fluentd-gcp-v2.0.9-5sfrz                              2/2       Running   0          6m
kube-system   po/fluentd-gcp-v2.0.9-6nb78                              2/2       Running   0          6m
kube-system   po/fluentd-gcp-v2.0.9-xpnjg                              2/2       Running   0          6m
kube-system   po/heapster-v1.4.3-6c6c9549df-wl45c                      3/3       Running   0          5m
kube-system   po/kube-dns-778977457c-d524d                             3/3       Running   0          6m
kube-system   po/kube-dns-778977457c-gmqqq                             3/3       Running   0          5m
kube-system   po/kube-dns-autoscaler-7db47cb9b7-cjtgl                  1/1       Running   0          6m
kube-system   po/kube-proxy-gke-blog-post-default-pool-9bc7a7c2-bnw6   1/1       Running   0          6m
kube-system   po/kube-proxy-gke-blog-post-default-pool-9bc7a7c2-h57f   1/1       Running   0          6m
kube-system   po/kube-proxy-gke-blog-post-default-pool-9bc7a7c2-vtnc   1/1       Running   0          6m
kube-system   po/kubernetes-dashboard-76c679977c-bdf2j                 1/1       Running   0          6m
kube-system   po/l7-default-backend-6497bcdb4d-knsfc                   1/1       Running   0          6m

NAMESPACE     NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
default       svc/kubernetes             ClusterIP   10.43.240.1     <none>        443/TCP         6m
kube-system   svc/default-http-backend   NodePort    10.43.240.250   <none>        80:31072/TCP    6m
kube-system   svc/heapster               ClusterIP   10.43.240.148   <none>        80/TCP          6m
kube-system   svc/kube-dns               ClusterIP   10.43.240.10    <none>        53/UDP,53/TCP   6m
kube-system   svc/kubernetes-dashboard   ClusterIP   10.43.245.216   <none>        80/TCP          6m
```

/assign @mengqiy ",closed,False,2017-12-10 17:20:16,2018-01-15 21:51:14
kubectl,totherme,https://github.com/kubernetes/kubectl/pull/168,https://api.github.com/repos/kubernetes/kubectl/issues/168,Finalize the interface of the Fixtures struct,"[closes #163]

In the process of making the APIServer constructor take 0 arguments (#162), we naturally find ourselves making the Fixtures constructor take 0 arguments (#163).

We'll be cleaning things up even more as we continue to work on #162, but this seems like a reasonable mid-point where there's already some user-value. In particular, the main interface to the test framework (The Fixtures constructor and methods) ought to be stable now:

```go
myFixtures, err := test.NewFixtures()
err = myFixtures.Start()
myServerURL = myFixtures.APIServerURL()
// Run tests using myServerURL
err = myFixtures.Stop()
```

Cc @hoegaarden @apelisse ",closed,True,2017-12-11 14:02:31,2017-12-14 23:21:29
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/169,https://api.github.com/repos/kubernetes/kubectl/issues/169,Move several packages,"k8s.io/kubernetes/pkg/kubectl/categories -> k8s.io/kubectl/pkg/categories
k8s.io/kubernetes/pkg/kubectl/scheme -> k8s.io/kubectl/pkg/scheme
k8s.io/kubernetes/pkg/kubectl/validation -> k8s.io/kubectl/pkg/validation
",closed,True,2017-12-11 23:41:38,2018-04-13 03:45:17
kubectl,lenalebt,https://github.com/kubernetes/kubectl/issues/170,https://api.github.com/repos/kubernetes/kubectl/issues/170,Cannot ssh into (hanging) init container,"**What keywords did you search in Kubernetes issues before filing this one?** bash, init, exec

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
/kind FeatureRequest

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.11"", GitCommit:""b13f2fd682d56eab7a6a2b5a1cab1a3d2c8bdd55"", GitTreeState:""clean"", BuildDate:""2017-11-25T17:51:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

(but did not work in minikube 0.24.1 with an 1.8 cluster as well)

**Environment**:
- **Cloud provider or hardware configuration**: n/a
- **OS** (e.g. from /etc/os-release): n/a
- **Kernel** (e.g. `uname -a`): n/a
- **Install tools**: n/a
- **Others**: n/a


**What happened**:
Have a hanging init container that I'd like to debug. It writes some log files, but does not write everything to stdout. Sources are not under my control.

    [brueder:~/projects … e/caas/kubernetes/helm/caas] feature/CAAS-365-mongo-cluster(+4/-4)+* ± 
    kubectl exec -it caas-mongo-0 -- /bin/bash
    error: unable to upgrade connection: container not found (""caas-mongo"")

    [brueder:~/projects … e/caas/kubernetes/helm/caas] feature/CAAS-365-mongo-cluster(+4/-4)+* ± 
    kubectl get po
    NAME                                 READY     STATUS     RESTARTS   AGE
    caas-mongo-0                         0/1       Init:1/3   0          52m


**What you expected to happen**:
I'd expect ""kubectl exec"" to be able to ssh into that init container. If not naturally, then with ""-c $init_container_name"", as in ""kubectl logs"", for example

    kubectl exec -c $init_container_name -it caas-mongo-0 -- /bin/bash


**How to reproduce it** (as minimally and precisely as possible):
n/a

**Anything else we need to know**:
I can ssh into the host node and then ""docker exec"" into the init container. This only works because I have access to the nodes directly, but it should not be necessary.


  ",closed,False,2017-12-12 08:19:12,2019-01-18 19:42:06
kubectl,zjj2wry,https://github.com/kubernetes/kubectl/issues/171,https://api.github.com/repos/kubernetes/kubectl/issues/171,Use discovery and dynamic client for cluster-info dump,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):


**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:


**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",closed,False,2017-12-12 09:13:57,2018-05-11 05:01:35
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/172,https://api.github.com/repos/kubernetes/kubectl/issues/172,"""kubectl drain"" must print warnings about eviction errors","I am using [Pod disruption policies](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) so my `kubectl drain` cannot move forward with the eviction.

However it's not printing anything and **just hangs here**:
```sh
$ kubectl drain --ignore-daemonsets --force gke-test-default-pool-acbbabc2-zvmh
node ""gke-test-default-pool-acbbabc2-zvmh"" already cordoned
WARNING: Ignoring DaemonSet-managed pods: calico-node-vhshm, fluentd-gcp-v2.0.9-spwqn, ip-masq-agent-shrqv; Deleting pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: kube-proxy-gke-test-default-pool-acbbabc2-zvmh
(HANGS HERE FOREVER)
```

Only when I added `-v=10` I realized it's continuously failing and retrying:

```
I1212 12:29:29.996421   80942 round_trippers.go:436] POST https://35.192.225.48/api/v1/namespaces/doctor/pods/web-7d8f767544-pk4ch/eviction 429 Too Many Requests in 43 milliseconds
I1212 12:29:29.996441   80942 round_trippers.go:442] Response Headers:
I1212 12:29:29.996449   80942 round_trippers.go:445]     Content-Type: application/json
I1212 12:29:29.996455   80942 round_trippers.go:445]     Content-Length: 321
I1212 12:29:29.996461   80942 round_trippers.go:445]     Date: Tue, 12 Dec 2017 20:29:30 GMT
I1212 12:29:29.996496   80942 request.go:836] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":
""Cannot evict pod as it would violate the pod's disruption budget."",""reason"":""TooManyRequests"",""details"":{""causes"":[{""reason"":
""DisruptionBudget"",""message"":""The disruption budget web-pdb needs 7 healthy pods and has 6 currently""}]},""code"":429}
```

Maybe `kubectl drain` should print this error to the output.",closed,False,2017-12-12 20:30:58,2018-09-25 21:40:45
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/173,https://api.github.com/repos/kubernetes/kubectl/issues/173,Move openapi,"Copy the openapi package from kubernetes `k8s.io/kubernetes/pkg/kubectl/cmd/util/openapi` to `k8s.io/kubectl/pkg/openapi/`.

@AlexKhotian has done pretty much everything. I've just simplified some of the history.",closed,True,2017-12-12 23:42:04,2018-05-23 02:59:59
kubectl,Lion-Wei,https://github.com/kubernetes/kubectl/issues/174,https://api.github.com/repos/kubernetes/kubectl/issues/174,deprecate the print flag `--template`,"**What keywords did you search in Kubernetes issues before filing this one?** :
'template'  'printer'  'kubectl'
---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

FEATURE REQUEST

**What happened**:

Recently I was learning code about printer. Little confused about the flag `--template`.
The flag `--template` have a description: ""Template string or path to template file to use when -o=go-template, -o=go-template-file.""
But base the code:
```go
	if flag := flags.Lookup(""template""); flag != nil {
		if flag.Value.Type() == ""string"" {
			templateFile = GetFlagString(cmd, ""template"")
		}
	}
	if len(outputFormat) == 0 && len(templateFile) != 0 {
		outputFormat = ""template""
	}
```
This flag be useful only if don't have `-o` flag.

And also, this flag have the same function with `-o go-template=****`. I'm not sure why we have this flag, is there any other reasons I don't know? If so, I think we should at least change the description of this flag.

Besides, since `-o template\-o templatefile` is same with `-o go-template\-o go-template-file`, can we chose one? There don't have any hint in `-o` flag about `-o template\-o templatefile` .

**What you expected to happen**:
1. Maybe deprecate the flag `--template` if we can make sure this flag is useless.
2. Change the description of flag `--template` if it still useful.
3. Maybe chose one in `-o template`\`-o templatefile` and  `-o go-template`\`-o go-template-file`.

**Anything else we need to know**:


  ",closed,False,2017-12-13 08:08:14,2018-06-04 03:43:16
kubectl,Lion-Wei,https://github.com/kubernetes/kubectl/issues/175,https://api.github.com/repos/kubernetes/kubectl/issues/175,kubectl get multi-resources can't show kind when using with `--sort-by` flag,"
**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
/kind bug

**Kubernetes version** (use `kubectl version`):
master

**What happened**:
When using `kubectl get all --sort-by ***`, the result we got don't have resource kind information. For example: 
```shell
# k get all --sort-by=kind
NAME              READY     STATUS    RESTARTS   AGE
test-curl-9rczp   1/1       Running   243        16d
test-curl-qjfvn   1/1       Running   101        7d
whoami-7rvmh      1/1       Running   0          16d
whoami-tc6f5      1/1       Running   0          7d

NAME        DESIRED   CURRENT   READY     AGE
test-curl   2         2         2         16d
test-curl   2         2         2         16d

NAME      DESIRED   CURRENT   READY     AGE
whoami    2         2         2         16d

NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP          16d
whoami       NodePort    10.110.153.125   <none>        3029:31531/TCP   16d
```

**What you expected to happen**:

```shell
# ./kubectl get all --sort-by=kind
NAME                 READY     STATUS    RESTARTS   AGE
po/test-curl-9rczp   1/1       Running   243        16d
po/test-curl-qjfvn   1/1       Running   101        7d
po/whoami-7rvmh      1/1       Running   0          16d
po/whoami-tc6f5      1/1       Running   0          7d

NAME           DESIRED   CURRENT   READY     AGE
rs/test-curl   2         2         2         16d
rs/test-curl   2         2         2         16d

NAME        DESIRED   CURRENT   READY     AGE
rc/whoami   2         2         2         16d

NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
svc/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP          16d
svc/whoami       NodePort    10.110.153.125   <none>        3029:31531/TCP   16d
```

**How to reproduce it** (as minimally and precisely as possible):
`kubectl get all --sort-by=kind`

",closed,False,2017-12-14 09:00:50,2018-05-04 17:22:29
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/176,https://api.github.com/repos/kubernetes/kubectl/issues/176,162 apiserver sane default constructor,"Based on #168:

The APIServer has now sane defaults for the constructor.
- The Port/Address allocation/management is handled by a (new thing called) AddressManager
- The path to APIServer's binary is now managed by a PathFinder

[closes #162]

@totherme ",closed,True,2017-12-14 09:44:10,2017-12-15 00:03:30
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/177,https://api.github.com/repos/kubernetes/kubectl/issues/177,Zero conf structs,"Based on: #176 

This is some more cleanup and work on the test framework. The most important changes of this PR are:
- Rename `Fixtures` to `ControlPlane` (and `FixtureProcess` to `ControlPlaneProcess`, and others like that)
- Have `Etcd` & `APIServer` work as 0-conf structs, just use `etcd := &Etcd{}`
- Remove `PortFinder` in favour of `AddressManager`
- Remove both `EtcdConfig` and `APIServerConfig`
- Some additional testing

@totherme ",closed,True,2017-12-14 09:57:12,2017-12-22 14:48:27
kubectl,totherme,https://github.com/kubernetes/kubectl/pull/178,https://api.github.com/repos/kubernetes/kubectl/issues/178,Control pane process timeouts,"This PR is really on top of #177 , so it's not as huge as it looks.

This PR removes an edge case in which using the framework from a standard go test (not using the ginkgo framework) could have caused a panic rather than a clean test failure. You can see this edge case in the old code as a call to `.Wait()` on a `gexec.Session` struct. That method implicitly uses an `Eventually` assertion from the ginkgo/gomega framework. The new code manages the waiting in our framework, and returns an error in the case of a timeout.

Cc: @hoegaarden 

/assign @apelisse ",closed,True,2017-12-14 15:29:10,2017-12-15 04:40:30
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/179,https://api.github.com/repos/kubernetes/kubectl/issues/179,ktl proposal,,closed,True,2017-12-14 19:34:10,2018-05-07 02:49:04
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/180,https://api.github.com/repos/kubernetes/kubectl/issues/180,Examples,"- Refactor AddressManager
- Add documentation for APIServer (and Etcd) and add some examples

Cc @totherme 

/assign @apelisse ",closed,True,2017-12-15 16:22:25,2017-12-18 17:08:35
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/181,https://api.github.com/repos/kubernetes/kubectl/issues/181,check in kinflate,"This supersede #159 

Use the example:
```
kinflate -f pkg/kinflate/example/instances/exampleinstance/
```
",closed,True,2017-12-15 19:25:10,2018-08-16 02:21:30
kubectl,jberkhahn,https://github.com/kubernetes/kubectl/pull/182,https://api.github.com/repos/kubernetes/kubectl/issues/182,Add framework to init config in a plugin,"Hello. This is a PR of the extracted framework from the service-catalog plugins I've been writing. I discussed it with @pwittrock at Kubecon, and he expressed interest in me contributing it here for general use.

It's use is to create a config based on the system kubectl config and the environment variables passed in from the calling plugin framework. I've got most of them worked out, however the overrides for the config cluster, context, and user still need to be done. I thought I would get this out here and get some feedback on what I have so far.

Thanks,
@jberkhahn",closed,True,2017-12-15 20:12:58,2018-01-24 21:38:39
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/183,https://api.github.com/repos/kubernetes/kubectl/issues/183,update vendor,"Pin the following repo to master branch:
- api
- apimachinery
- client-go
",closed,True,2017-12-15 21:54:19,2017-12-15 23:27:52
kubectl,totherme,https://github.com/kubernetes/kubectl/pull/184,https://api.github.com/repos/kubernetes/kubectl/issues/184,Integration Framework: Simplify paths,"This PR follows up on some of the comments from #180. It simplifies the business of finding paths to the etcd and apiserver binaries.

/assign @apelisse ",closed,True,2017-12-18 15:17:04,2017-12-18 17:13:35
kubectl,EmpireJones,https://github.com/kubernetes/kubectl/issues/185,https://api.github.com/repos/kubernetes/kubectl/issues/185,Watch flag should indicate when a resource is deleted,"Feature suggestion/improvement:
`kubectl get pods --watch` should indicate when a pod is actually deleted.

Currently it just appends another line or two indicating that it has a terminating status, around when it gets deleted.

This might apply to other resource types too.

**Kubernetes version** (use `kubectl version`):
Client: 1.8.4
Server: 1.8.0 (via minikube v0.24.1)

**Environment**:

Minikube v0.24.1

Host Env:
Ubuntu 17.10
Linux Laser 4.13.0-16-generic #19-Ubuntu SMP Wed Oct 11 18:35:14 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

**What happened**:

    $ kubectl get pods --watch
    NAME                               READY     STATUS        RESTARTS   AGE
    my-pod-5c7dd74bcb-2xj54   0/1       Terminating   6          11m
    my-pod-7bcddb9489-j4qzz   1/1       Running       0          51s
    my-pod-5c7dd74bcb-2xj54   0/1       Terminating   6         11m
    my-pod-5c7dd74bcb-2xj54   0/1       Terminating   6         11m


**What you expected to happen**:
There should be some sort of 'Terminated' or 'Deleted' status.

**How to reproduce it** (as minimally and precisely as possible):

- run `kubectl get pods --watch` while deleting pods. 

**Anything else we need to know**:
- ran via bash shell
- I created/deleted pods via helm, but that shouldn't matter
",open,False,2017-12-19 03:15:57,2019-03-25 16:46:44
kubectl,mortenlj,https://github.com/kubernetes/kubectl/issues/186,https://api.github.com/repos/kubernetes/kubectl/issues/186,Default output from kubectl when getting a CRD named Status is wrong,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): NO

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): Status

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
╼ kubectl version
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.3"", GitCommit:""f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd"", GitTreeState:""clean"", BuildDate:""2017-11-08T18:39:33Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4"", GitTreeState:""clean"", BuildDate:""2017-11-29T22:43:34Z"", GoVersion:""go1.9.1"", Compiler:""gc"", Platform:""linux/amd64""}


**Environment**: minikube
╼ minikube version
minikube version: v0.24.1

**What happened**:
When listing objects of a CRD called Status, the output is blank, except for headers that are not what I was expecting.

**What you expected to happen**:
I expected something along the lines of what you get when you list other objects, such as name and possibly other relevant fields. 

**How to reproduce it** (as minimally and precisely as possible):
```
fimojoha@fimojoha-w530:/tmp$ minikube start --kubernetes-version v1.8.0
Starting local Kubernetes v1.8.0 cluster...
Starting VM...
Getting VM IP address...
Moving files into cluster...
Setting up certs...
Connecting to cluster...
Setting up kubeconfig...
Starting cluster components...
Kubectl is now configured to use the cluster.
Loading cached images from config file.
fimojoha@fimojoha-w530:/tmp$ kubectl create -f - <<EOF
> apiVersion: apiextensions.k8s.io/v1beta1
> kind: CustomResourceDefinition
> metadata:
>   name: statuses.fiaas.schibsted.io
> spec:
>   group: fiaas.schibsted.io
>   names:
>     kind: Status
>     listKind: StatusList
>     plural: statuses
>     shortNames:
>     - status
>     - fs
>     singular: status
>   scope: Namespaced
>   version: v1
> EOF
customresourcedefinition ""statuses.fiaas.schibsted.io"" created
fimojoha@fimojoha-w530:/tmp$ kubectl create -f - <<EOF
> apiVersion: fiaas.schibsted.io/v1
> kind: Status
> metadata:
>   labels:
>     app: nginx-example
>   name: nginx-example
>   namespace: default
> result: RUNNING
> EOF
Error from server: error when creating ""STDIN"": 
fimojoha@fimojoha-w530:/tmp$ kubectl get statuses.fiaas.schibsted.io
STATUS    REASON    MESSAGE
                    
fimojoha@fimojoha-w530:/tmp$ kubectl get statuses.fiaas.schibsted.io -ocustom-columns=NAME:.metadata.name,KIND:.kind
NAME            KIND
nginx-example   Status
fimojoha@fimojoha-w530:/tmp$ kubectl get statuses.fiaas.schibsted.io -oyaml
apiVersion: v1
items:
- apiVersion: fiaas.schibsted.io/v1
  kind: Status
  metadata:
    clusterName: """"
    creationTimestamp: 2017-12-19T12:48:55Z
    deletionGracePeriodSeconds: null
    deletionTimestamp: null
    initializers: null
    labels:
      app: nginx-example
    name: nginx-example
    namespace: default
    resourceVersion: ""253""
    selfLink: /apis/fiaas.schibsted.io/v1/namespaces/default/statuses/nginx-example
    uid: f984002d-e4ba-11e7-8296-0800274322ad
  result: RUNNING
kind: List
metadata:
  resourceVersion: """"
  selfLink: """"
fimojoha@fimojoha-w530:/tmp$ kubectl get statuses.fiaas.schibsted.io nginx-example -oyaml
Error from server: 
fimojoha@fimojoha-w530:/tmp$ 
```


**Anything else we need to know**:

",open,False,2017-12-19 12:49:57,2019-04-02 20:29:13
kubectl,spiffxp,https://github.com/kubernetes/kubectl/pull/187,https://api.github.com/repos/kubernetes/kubectl/issues/187,Update code-of-conduct.md,"Refer to kubernetes/community as authoritative source for code of conduct

ref: kubernetes/community#1527",closed,True,2017-12-20 18:33:35,2018-01-04 16:42:15
kubectl,mpashka,https://github.com/kubernetes/kubectl/issues/188,https://api.github.com/repos/kubernetes/kubectl/issues/188,Kubectl doesn't respect --user parameter when execution create clusterrolebinding,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
username

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->
**What happened**:
If <default_user> is configured in kubectl. When applying kubectl command with another user:
kubectl --user='admin' create clusterrolebinding <role_name> --clusterrole=view --group=<group_name>
Kubectl ignores --user parameter. And I get error
Error from server (Forbidden): User ""<default_user>"" cannot create clusterrolebindings...

**What you expected to happen**:
This should result in creating appropriate clusterrolebinding on behalf of admin user.
admin user is to be used for api server authentication, not viewer.

**How to reproduce it (as minimally and precisely as possible)**:
Setup kubernetes cluster.

Configure kubectl. Create cluster configuration, context, 2 users, set default context. To reproduce this issue create two users in kubectl config - one user with permissions to create clusterrolebinding (admin) and another without (viewer). Set user without permissions (viewer) as default for context.

Apply kubectl create clusterrolebinding with different user specified. E.g.
kubectl --user='admin' create clusterrolebinding <role_name> --clusterrole=view --group=<group_name>

Instead of successfully created clusterrolebinding I get error message:
Error from server (Forbidden): User ""viewer"" cannot create clusterrolebindings.rbac.authorization.k8s.io at the cluster scope. (post clusterrolebindings.rbac.authorization.k8s.io)

**Anything else we need to know?**:
There are at least 2 workarounds:
Specify admin user as default in kubectl config
Create file with clusterrolebinding and apply kubectl create -f <file_name>

**Kubernetes version** (use `kubectl version`):
I checked this with kubectl 1.7.11 and 1.9.0
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.4"", GitCommit:""793658f2d7ca7f064d2bdf606519f9fe1229c381"", GitTreeState:""clean"", BuildDate:""2017-08-17T08:48:23Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.11"", GitCommit:""b13f2fd682d56eab7a6a2b5a1cab1a3d2c8bdd55"", GitTreeState:""clean"", BuildDate:""2017-11-25T17:51:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

",closed,False,2017-12-22 22:29:18,2018-01-19 17:54:23
kubectl,stewart-yu,https://github.com/kubernetes/kubectl/issues/189,https://api.github.com/repos/kubernetes/kubectl/issues/189,Does we need print apigroup for  'ds'、'deployments'?,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

FEATURE REQUEST

Now that `kubectl get all --all-namespaces` has duplicate output for `ds`, `deploy` and `rs`,  obvious, thoes duplicate in each apisource in different apiGroup, should we  better printer relative apiGroup?
If need, i will help

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):


**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:


**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",closed,False,2017-12-27 03:08:42,2018-01-15 21:51:14
kubectl,dyson,https://github.com/kubernetes/kubectl/issues/190,https://api.github.com/repos/kubernetes/kubectl/issues/190,kubctl explain resourcequota.spec.hard incomplete ,"/kind bug

**What happened**:
Ran:
```
kubectl explain resourcequota.spec.hard
```

**What you expected to happen**:
To see list of available hard resource quota limits.
Instead there is only a link to a page that 404's.

**How to reproduce it (as minimally and precisely as possible)**:
Run:
```
kubectl explain resourcequota.spec.hard
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
:; kubectl version
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"",GitTreeState:""clean"", BuildDate:""2017-12-16T03:15:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4"",GitTreeState:""clean"", BuildDate:""2017-11-29T22:43:34Z"", GoVersion:""go1.9.1"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: minikube
- OS (e.g. from /etc/os-release):
```
$ cat /etc/os-release
NAME=Buildroot
VERSION=2017.02
ID=buildroot
VERSION_ID=2017.02
PRETTY_NAME=""Buildroot 2017.02""
```
",closed,False,2017-12-27 11:17:05,2017-12-27 16:51:34
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/191,https://api.github.com/repos/kubernetes/kubectl/issues/191,update manifest types,"- Update type definition for manifest to make align with https://github.com/kubernetes/kubernetes/blob/d8d680be6e2bd1455bb6861f9bd6ed7d4f7dce43/pkg/kubectl/configmap.go#L39-L44 and https://github.com/kubernetes/kubernetes/blob/d8d680be6e2bd1455bb6861f9bd6ed7d4f7dce43/pkg/kubectl/secret.go#L39-L44.
- Change `TLS` to be a pointer.
  ",closed,True,2018-01-03 19:38:14,2018-01-05 19:34:51
kubectl,kumarganesh2814,https://github.com/kubernetes/kubectl/issues/192,https://api.github.com/repos/kubernetes/kubectl/issues/192,"kubectl exec connection error: desc = ""transport: dial unix docker-containerd.sock: connect: connection refused"": unknown","<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
`
 kubectl version
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.4"", GitCommit:""9befc2b8928a9426501d3bf62f72849d5cbcd5a3"", GitTreeState:""clean"", BuildDate:""2017-11-20T05:28:34Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.5"", GitCommit:""cce11c6a185279d037023e02ac5249e14daa22bf"", GitTreeState:""clean"", BuildDate:""2017-12-07T16:05:18Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
`

**Environment**:
- **Cloud provider or hardware configuration**:Baremetal
- **OS** (e.g. from /etc/os-release):CentOS Linux release 7.4.1708 (Core)
- **Kernel** (e.g. `uname -a`):
Linux kuber-poc-app6 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**:
- **Others**:


**What happened**:

When trying to run kubectl exec command on pods it erroring as below.

``

 kubectl exec cassandra-0 --v=9 -- ls /tmp
I0103 23:12:03.562000   28045 loader.go:357] Config loaded from file /root/.kube/config
I0103 23:12:03.564115   28045 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.8.5 (linux/amd64) kubernetes/cce11c6"" https://10.127.38.106:6443/api/v1/namespaces/default/pods/cassandra-0
I0103 23:12:03.582226   28045 round_trippers.go:436] GET https://10.127.38.106:6443/api/v1/namespaces/default/pods/cassandra-0 200 OK in 18 milliseconds
I0103 23:12:03.582244   28045 round_trippers.go:442] Response Headers:
I0103 23:12:03.582251   28045 round_trippers.go:445]     Content-Type: application/json
I0103 23:12:03.582257   28045 round_trippers.go:445]     Content-Length: 4094
I0103 23:12:03.582275   28045 round_trippers.go:445]     Date: Thu, 04 Jan 2018 07:12:03 GMT
I0103 23:12:03.582404   28045 request.go:836] Response Body: {""kind"":""Pod"",""apiVersion"":""v1"",""metadata"":{""name"":""cassandra-0"",""generateName"":""cassandra-"",""namespace"":""default"",""selfLink"":""/api/v1/namespaces/default/pods/cassandra-0"",""uid"":""981671a6-e48a-11e7-8636-00505601371c"",""resourceVersion"":""2009735"",""creationTimestamp"":""2017-12-19T07:02:36Z"",""labels"":{""app"":""cassandra"",""controller-revision-hash"":""cassandra-7666d9bbbd""},""annotations"":{""kubernetes.io/created-by"":""{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""StatefulSet\"",\""namespace\"":\""default\"",\""name\"":\""cassandra\"",\""uid\"":\""631b3fb9-e16c-11e7-8636-00505601371c\"",\""apiVersion\"":\""apps/v1beta1\"",\""resourceVersion\"":\""856338\""}}\n""},""ownerReferences"":[{""apiVersion"":""apps/v1beta1"",""kind"":""StatefulSet"",""name"":""cassandra"",""uid"":""631b3fb9-e16c-11e7-8636-00505601371c"",""controller"":true,""blockOwnerDeletion"":true}]},""spec"":{""volumes"":[{""name"":""cassandra-data"",""emptyDir"":{}},{""name"":""default-token-2l8l5"",""secret"":{""secretName"":""default-token-2l8l5"",""defaultMode"":420}}],""containers"":[{""name"":""cassandra"",""image"":""gcr.io/google-samples/cassandra:v12"",""ports"":[{""name"":""intra-node"",""containerPort"":7000,""protocol"":""TCP""},{""name"":""tls-intra-node"",""containerPort"":7001,""protocol"":""TCP""},{""name"":""jmx"",""containerPort"":7199,""protocol"":""TCP""},{""name"":""cql"",""containerPort"":9042,""protocol"":""TCP""}],""env"":[{""name"":""MAX_HEAP_SIZE"",""value"":""512M""},{""name"":""HEAP_NEWSIZE"",""value"":""100M""},{""name"":""CASSANDRA_SEEDS"",""value"":""cassandra-0.cassandra.default.svc.cluster.local""},{""name"":""CASSANDRA_CLUSTER_NAME"",""value"":""K8Demo""},{""name"":""CASSANDRA_DC"",""value"":""DC1-K8Demo""},{""name"":""CASSANDRA_RACK"",""value"":""Rack1-K8Demo""},{""name"":""CASSANDRA_AUTO_BOOTSTRAP"",""value"":""false""},{""name"":""POD_IP"",""valueFrom"":{""fieldRef"":{""apiVersion"":""v1"",""fieldPath"":""status.podIP""}}}],""resources"":{""limits"":{""cpu"":""500m"",""memory"":""1Gi""},""requests"":{""cpu"":""500m"",""memory"":""1Gi""}},""volumeMounts"":[{""name"":""cassandra-data"",""mountPath"":""/cassandra_data""},{""name"":""default-token-2l8l5"",""readOnly"":true,""mountPath"":""/var/run/secrets/kubernetes.io/serviceaccount""}],""readinessProbe"":{""exec"":{""command"":[""/bin/bash"",""-c"",""/ready-probe.sh""]},""initialDelaySeconds"":15,""timeoutSeconds"":5,""periodSeconds"":10,""successThreshold"":1,""failureThreshold"":3},""lifecycle"":{""preStop"":{""exec"":{""command"":[""/bin/sh"",""-c"",""PID=$(pidof java) \u0026\u0026 kill $PID \u0026\u0026 while ps -p $PID \u003e /dev/null; do sleep 1; done""]}}},""terminationMessagePath"":""/dev/termination-log"",""terminationMessagePolicy"":""File"",""imagePullPolicy"":""Always"",""securityContext"":{""capabilities"":{""add"":[""IPC_LOCK""]}}}],""restartPolicy"":""Always"",""terminationGracePeriodSeconds"":30,""dnsPolicy"":""ClusterFirst"",""nodeSelector"":{""app"":""cassandraee""},""serviceAccountName"":""default"",""serviceAccount"":""default"",""nodeName"":""kong-poc-k8node6"",""securityContext"":{},""hostname"":""cassandra-0"",""subdomain"":""cassandra"",""schedulerName"":""default-scheduler"",""tolerations"":[{""key"":""node.alpha.kubernetes.io/notReady"",""operator"":""Exists"",""effect"":""NoExecute"",""tolerationSeconds"":300},{""key"":""node.alpha.kubernetes.io/unreachable"",""operator"":""Exists"",""effect"":""NoExecute"",""tolerationSeconds"":300}]},""status"":{""phase"":""Running"",""conditions"":[{""type"":""Initialized"",""status"":""True"",""lastProbeTime"":null,""lastTransitionTime"":""2017-12-19T07:02:36Z""},{""type"":""Ready"",""status"":""False"",""lastProbeTime"":null,""lastTransitionTime"":""2017-12-27T03:39:02Z"",""reason"":""ContainersNotReady"",""message"":""containers with unready status: [cassandra]""},{""type"":""PodScheduled"",""status"":""True"",""lastProbeTime"":null,""lastTransitionTime"":""2017-12-19T07:02:36Z""}],""hostIP"":""10.127.38.114"",""podIP"":""10.244.6.6"",""startTime"":""2017-12-19T07:02:36Z"",""containerStatuses"":[{""name"":""cassandra"",""state"":{""running"":{""startedAt"":""2017-12-19T07:02:38Z""}},""lastState"":{},""ready"":false,""restartCount"":0,""image"":""gcr.io/google-samples/cassandra:v12"",""imageID"":""docker-pullable://gcr.io/google-samples/cassandra@sha256:aa17d03b156fee863a73e59eb4d7adf4528f4c039f7e6e29e82d9006b7d93e8d"",""containerID"":""docker://bf13fac0152e424f936f87e41b9e664dc9c3a6cdf79bf7d04284cc8c76219675""}],""qosClass"":""Guaranteed""}}
I0103 23:12:03.643666   28045 round_trippers.go:417] curl -k -v -XPOST  -H ""X-Stream-Protocol-Version: v4.channel.k8s.io"" -H ""X-Stream-Protocol-Version: v3.channel.k8s.io"" -H ""X-Stream-Protocol-Version: v2.channel.k8s.io"" -H ""X-Stream-Protocol-Version: channel.k8s.io"" https://10.127.38.106:6443/api/v1/namespaces/default/pods/cassandra-0/exec?command=ls&command=%2Ftmp&container=cassandra&container=cassandra&stderr=true&stdout=true
I0103 23:12:03.643721   28045 round_trippers.go:417] curl -k -v -XPOST  -H ""X-Stream-Protocol-Version: v4.channel.k8s.io"" -H ""X-Stream-Protocol-Version: v3.channel.k8s.io"" -H ""X-Stream-Protocol-Version: v2.channel.k8s.io"" -H ""X-Stream-Protocol-Version: channel.k8s.io"" -H ""User-Agent: kubectl/v1.8.5 (linux/amd64) kubernetes/cce11c6"" https://10.127.38.106:6443/api/v1/namespaces/default/pods/cassandra-0/exec?command=ls&command=%2Ftmp&container=cassandra&container=cassandra&stderr=true&stdout=true
I0103 23:12:03.979226   28045 round_trippers.go:436] POST https://10.127.38.106:6443/api/v1/namespaces/default/pods/cassandra-0/exec?command=ls&command=%2Ftmp&container=cassandra&container=cassandra&stderr=true&stdout=true 101 Switching Protocols in 335 milliseconds
I0103 23:12:03.979264   28045 round_trippers.go:442] Response Headers:
I0103 23:12:03.979274   28045 round_trippers.go:445]     Date: Thu, 04 Jan 2018 07:12:03 GMT
I0103 23:12:03.979281   28045 round_trippers.go:445]     Connection: Upgrade
I0103 23:12:03.979287   28045 round_trippers.go:445]     Upgrade: SPDY/3.1
I0103 23:12:03.979293   28045 round_trippers.go:445]     X-Stream-Protocol-Version: v4.channel.k8s.io
I0103 23:12:03.979300   28045 round_trippers.go:436] POST https://10.127.38.106:6443/api/v1/namespaces/default/pods/cassandra-0/exec?command=ls&command=%2Ftmp&container=cassandra&container=cassandra&stderr=true&stdout=true 101 Switching Protocols in 335 milliseconds
I0103 23:12:03.979309   28045 round_trippers.go:442] Response Headers:
I0103 23:12:03.979315   28045 round_trippers.go:445]     Connection: Upgrade
I0103 23:12:03.979321   28045 round_trippers.go:445]     Upgrade: SPDY/3.1
I0103 23:12:03.979326   28045 round_trippers.go:445]     X-Stream-Protocol-Version: v4.channel.k8s.io
I0103 23:12:03.979332   28045 round_trippers.go:445]     Date: Thu, 04 Jan 2018 07:12:03 GMT
connection error: desc = ""transport: dial unix /var/run/docker/containerd/docker-containerd.sock: connect: connection refused"": unknown
F0103 23:12:04.548658   28045 helpers.go:120] command terminated with exit code 126

``

Not Sure why only kubectl exec command doesnt work all other kubectl command works fine for this pod

``
kubectl describe po cassandra-0
Name:           cassandra-0
Namespace:      default
Node:           kong-poc-k8node6/10.127.38.114
Start Time:     Mon, 18 Dec 2017 23:02:36 -0800
Labels:         app=cassandra
                controller-revision-hash=cassandra-7666d9bbbd
Annotations:    kubernetes.io/created-by={""kind"":""SerializedReference"",""apiVersion"":""v1"",""reference"":{""kind"":""StatefulSet"",""namespace"":""default"",""name"":""cassandra"",""uid"":""631b3fb9-e16c-11e7-8636-00505601371c"",""apiVer...
Status:         Running
IP:             10.244.6.6
Created By:     StatefulSet/cassandra
Controlled By:  StatefulSet/cassandra
Containers:
  cassandra:
    Container ID:   docker://bf13fac0152e424f936f87e41b9e664dc9c3a6cdf79bf7d04284cc8c76219675
    Image:          gcr.io/google-samples/cassandra:v12
    Image ID:       docker-pullable://gcr.io/google-samples/cassandra@sha256:aa17d03b156fee863a73e59eb4d7adf4528f4c039f7e6e29e82d9006b7d93e8d
    Ports:          7000/TCP, 7001/TCP, 7199/TCP, 9042/TCP
    State:          Running
      Started:      Mon, 18 Dec 2017 23:02:38 -0800
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  1Gi
    Requests:
      cpu:      500m
      memory:   1Gi
    Readiness:  exec [/bin/bash -c /ready-probe.sh] delay=15s timeout=5s period=10s #success=1 #failure=3
    Environment:
      MAX_HEAP_SIZE:             512M
      HEAP_NEWSIZE:              100M
      CASSANDRA_SEEDS:           cassandra-0.cassandra.default.svc.cluster.local
      CASSANDRA_CLUSTER_NAME:    K8Demo
      CASSANDRA_DC:              DC1-K8Demo
      CASSANDRA_RACK:            Rack1-K8Demo
      CASSANDRA_AUTO_BOOTSTRAP:  false
      POD_IP:                     (v1:status.podIP)
    Mounts:
      /cassandra_data from cassandra-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2l8l5 (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          False
  PodScheduled   True
Volumes:
  cassandra-data:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
  default-token-2l8l5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-2l8l5
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  app=cassandraee
Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                  From                       Message
  ----     ------     ----                 ----                       -------
  Warning  Unhealthy  2m (x70795 over 8d)  kubelet, kong-poc-k8node6  Readiness probe failed: connection error: desc = ""transport: dial unix /var/run/docker/containerd/docker-containerd.sock: connect: connection refused"": unknown

``


**What you expected to happen**:

kubectl should run fine 

``
 kubectl get po
NAME                    READY     STATUS    RESTARTS   AGE
cassandra-0             0/1       Running   0          16d
cassandra-1             0/1       Running   0          16d

``
**How to reproduce it** (as minimally and precisely as possible):

create casandra pods and then try to run kubectl exec.
**Anything else we need to know**:

",closed,False,2018-01-04 08:22:00,2018-07-05 01:49:42
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/193,https://api.github.com/repos/kubernetes/kubectl/issues/193,Simplify directory handling,"We now use simple strings for the paths to the certificate directory (APIServer) and the data directory (Etcd) instead of the `TempDirManager`.

cc: @totherme @apelisse 
  ",closed,True,2018-01-05 16:41:14,2018-01-08 17:34:12
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/194,https://api.github.com/repos/kubernetes/kubectl/issues/194,Simplify address handling,"To configure the host/port the APIServer and Etcd are listening on, we now use a `url.URL` instead of the `AddressManager`; for defaulting we still use the `DefaultAddressManager`.

This PR is based on #193 

cc: @totherme @apelisse 

/assign @apelisse ",closed,True,2018-01-05 16:44:32,2018-01-08 17:35:12
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/195,https://api.github.com/repos/kubernetes/kubectl/issues/195,format comments for types.go,Per https://github.com/kubernetes/kubectl/pull/191#pullrequestreview-86986658,closed,True,2018-01-06 00:43:46,2018-01-08 23:29:12
kubectl,totherme,https://github.com/kubernetes/kubectl/pull/196,https://api.github.com/repos/kubernetes/kubectl/issues/196,Hide test framework internals,"Hopefully this'll make the godocs a lot simpler.

This is built on top of #194 , so it's probably not as big as it seems at first.

CC: @hoegaarden 

/assign @apelisse ",closed,True,2018-01-08 14:12:49,2018-01-08 17:36:11
kubectl,php-coder,https://github.com/kubernetes/kubectl/issues/197,https://api.github.com/repos/kubernetes/kubectl/issues/197,kubectl apply includes raw and useless data into the error message,"**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

""error when retrieving current configuration""

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
/kind bug

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:"""", Minor:"""", GitVersion:""v1.9.0-beta1"", GitCommit:""3258431"", GitTreeState:""clean"", BuildDate:""2018-01-04T16:20:36Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10+"", GitVersion:""v1.10.0-alpha.1.556+af78b9bac4d34e-dirty"", GitCommit:""af78b9bac4d34eeec4adcf48f90845d5b325e081"", GitTreeState:""dirty"", BuildDate:""2018-01-08T14:52:54Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:
`kubectl apply` includes Golang struct into the error message that makes it unreadable and unhelpful to user.

**What you expected to happen**:
1) the error message should be more clear about what's wrong and how I can fix it
2) the error message shouldn't show addresses in the memory that are useless

**How to reproduce it** (as minimally and precisely as possible):
```
$ kubectl apply -f nnp-test.yaml 
error: error when retrieving current configuration of:
&{0xc420094300 0xc420bc6310 default  nnp-test.yaml 0xc4220eb4a8  false}
from server for: ""nnp-test.yaml"": resource name may not be empty
$ cat nnp-test.yaml
kind: Pod
apiVersion: v1
metadata:
  generateName: testing-nnp-
spec:
  restartPolicy: Never
  initContainers:
  - name: init-container
	image: busybox
	command:
	- grep NoNewPriv /proc/self/status
  containers:
  - name: simple-container
	image: busybox
	command:
	- grep NoNewPriv /proc/self/status
```

**Anything else we need to know**:
```
$ kubectl apply --loglevel=8 -f nnp-test.yaml 
...
I0108 16:42:00.839592    5121 decoder.go:224] decoding stream as YAML
I0108 16:42:00.843520    5121 request.go:624] Error in request: resource name may not be empty
F0108 16:42:00.844222    5121 helpers.go:119] error: error when retrieving current configuration of:
&{0xc4212beb40 0xc420b68070 default  nnp-test.yaml 0xc422ea58c8  false}
from server for: ""nnp-test.yaml"": resource name may not be empty
```
",closed,False,2018-01-08 19:23:03,2018-03-27 11:08:15
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/198,https://api.github.com/repos/kubernetes/kubectl/issues/198,copy some configmap and secret related pkgs for kinflate,"This PR copied the pkgs and added one field:
https://github.com/kubernetes/kubectl/pull/198/files#diff-02b8e163d2953b0f138313481b32d784R42",closed,True,2018-01-08 20:39:41,2018-01-08 22:46:52
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/199,https://api.github.com/repos/kubernetes/kubectl/issues/199,pod template visitor,"Implement of pod template visitor for kinflate.

This PR depends on #198
",closed,True,2018-01-08 20:43:21,2018-01-08 23:46:40
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/200,https://api.github.com/repos/kubernetes/kubectl/issues/200,configmap and secret helper functions,"This depends on #198 and #201.
  ",closed,True,2018-01-08 21:53:16,2018-01-09 23:25:38
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/201,https://api.github.com/repos/kubernetes/kubectl/issues/201,update simple example,Some files will be used in the unit test for https://github.com/kubernetes/kubectl/pull/200 after this PR merged.,closed,True,2018-01-09 00:55:58,2018-01-09 22:53:21
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/202,https://api.github.com/repos/kubernetes/kubectl/issues/202,Initial install,,closed,True,2018-01-09 22:48:47,2018-07-18 03:57:52
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/203,https://api.github.com/repos/kubernetes/kubectl/issues/203,Add some update helpers for kinflate,,closed,True,2018-01-09 23:54:15,2018-01-10 00:54:29
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/204,https://api.github.com/repos/kubernetes/kubectl/issues/204,add one more update helper function,,closed,True,2018-01-10 17:17:47,2018-01-10 17:26:37
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/205,https://api.github.com/repos/kubernetes/kubectl/issues/205,more support for configmap and secret,There will be some test to replace https://github.com/kubernetes/kubectl/blob/c61d60e246663574dd531a0c61a99c325e81f809/cmd/kinflate/kinflate_test.go#L27-L32 soon.,closed,True,2018-01-10 17:39:44,2018-01-12 18:11:17
kubectl,bhack,https://github.com/kubernetes/kubectl/issues/206,https://api.github.com/repos/kubernetes/kubectl/issues/206,Kubectl xpra,"Can `kubectl exec` work with [xpra](https://xpra.org/trac/wiki/Usage)?
Many dev  wants to access to a GUI in the pod  in the development phase (i.e. for visual debug a job before launching at scale).
The standard way was to install sshd in the pod but if will be possible to connect directly with kubectl I think that could be useful to add a section in the documentation (i.e. access to a X session in the pod).
What do you think?",closed,False,2018-01-13 12:53:57,2018-01-23 17:22:35
kubectl,totherme,https://github.com/kubernetes/kubectl/pull/207,https://api.github.com/repos/kubernetes/kubectl/issues/207,Test framework: Simplify UX,"In order to simplify the UX of our library, we decided to temporarily abandon our precious unit tests, and explore driving out the library interface using only the godocs. These commits tell that story.

The end result is that most of the actual logic has been moved into an internal package, and the visible parts of the library are largely configuration structs.

CC: @hoegaarden 
/assign @apelisse ",closed,True,2018-01-15 14:27:55,2018-01-17 19:17:28
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/208,https://api.github.com/repos/kubernetes/kubectl/issues/208,Fix formatting issues as suggested by gofmt,"[Travis](https://travis-ci.org/kubernetes/kubectl/builds/329076589#L521) showed those issues today.

cc: @totherme 
/assign @apelisse ",closed,True,2018-01-15 17:17:48,2018-01-17 19:16:27
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/209,https://api.github.com/repos/kubernetes/kubectl/issues/209,Test framework: Configure the APIServer with the URL to Etcd,"The `APIServer` does not create, start & stop `Etcd` itself anymore, but gets configured with the URL to `Etcd`. Creating, starting & stopping of `Etcd` is now the responsibility of the `ControlPlane`.

This PR is based on #207, thus it looks way bigger then it actually is.

CC: @totherme 
/assign @apelisse ",closed,True,2018-01-16 15:28:10,2018-01-17 21:01:03
kubectl,ssubramanian123,https://github.com/kubernetes/kubectl/issues/210,https://api.github.com/repos/kubernetes/kubectl/issues/210,kubectl --serviceaccount option use ,"Hi,

I have 40+ Deployments which we created before RBAC enabled in our cluster. Now we want to enable RBAC and re-deploy these services. For new deployment, through automated process service account get added, but for the existing deployment either we have to use kubectl edit to modify it or modify existing deployment.yaml file. Since we have 40+ services is there any alternate way to directly pass the service account.

For example 

```kubectl deploy -f mydeployment.ymal --serviceaccount=myserviceaccount ```

or any other best way to add the service account to existing deployments.


Thanks,
Sakthi


",closed,False,2018-01-16 17:05:48,2018-04-14 14:43:53
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/211,https://api.github.com/repos/kubernetes/kubectl/issues/211,pre-commit: Check gofmt output for errors,"gofmt doesn't return an error exit code when it fails (because some
files are improperly formatted). Make sure that the output is empty
instead.",closed,True,2018-01-16 17:17:29,2018-01-17 19:29:33
kubectl,anitakumar,https://github.com/kubernetes/kubectl/issues/212,https://api.github.com/repos/kubernetes/kubectl/issues/212,give the loadbalancer a name ,"when creating a Service and using the type LoadBalancer is it possible to provide a name for the load balance and kubectl to create the loadbalancer with the name specified
",closed,False,2018-01-16 22:45:12,2018-01-23 17:22:08
kubectl,hoegaarden,https://github.com/kubernetes/kubectl/pull/213,https://api.github.com/repos/kubernetes/kubectl/issues/213,Test framework: Update docs,"This is based on #209, so agian appears to be much bigger than it actually is.

CC: @totherme 
/assign @apelisse ",closed,True,2018-01-17 11:16:24,2018-01-17 19:30:23
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/214,https://api.github.com/repos/kubernetes/kubectl/issues/214,update mutate func,"update mutate func:
- mutateField now takes a list of mutation func
- support create if not present

Fix a bug in the last commit.",closed,True,2018-01-17 21:28:30,2018-01-19 18:17:08
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/215,https://api.github.com/repos/kubernetes/kubectl/issues/215,Nameprefix util,"Add
- encode and decode util
- nameprefix util

We want each functionality in kinflate to implement interface
```
type Interface interface {
	Run(m map[GroupVersionKindName]*unstructured.Unstructured) error
	RunWrapper([]byte) ([]byte, error)
}
```
so that it
- can be chained together easily
- can be easily turned into a kubectl command

After 1) name prefix 2) apply labels 3) apply annotations 4) update name reference are all implemented in the new format, kinflate will migrate to use the new util.
The code base will be more modular and cleaner.
",closed,True,2018-01-19 18:57:54,2018-01-22 18:59:40
kubectl,sybeck2k,https://github.com/kubernetes/kubectl/issues/216,https://api.github.com/repos/kubernetes/kubectl/issues/216,Kubectl through a proxy,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
http proxy, https proxy
---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT

**Kubernetes version** (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.3"", GitCommit:""f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd"", GitTreeState:""clean"", BuildDate:""2017-11-08T18:39:33Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""windows/amd64""}`

**Environment**:
Windows 10, server is deployed on AWS running K8S 1.8.2

**What happened**:
I'm using kubectl through an authenticating proxy for OIDC. I cannot use the OIDC authenticator provided with kubectl because I cannot get auto refresh of tokens to work with my iDP (keycloak). To prevent users from relogging constantly, I'm proxying the K8S api with an OIDC authenticator - which in turn injects appropriate auth headers to the API.
I can confirm that the proxy is working correctly:
`curl -k --proxy http://192.168.99.100:3000 https://prod-k8s.dev.ci5.io/apis` returns a JSON object with correct headers.
On the other hand, using `kubectl` with `HTTPS_PROXY=http://192.168.99.100:3000 kubectl  --insecure-skip-tls-verify get namespaces -v=10` will not work:
```
I0122 11:17:18.382819   15568 round_trippers.go:436] GET https://prod-k8s.dev.ci5.io/api 200 OK in 373 milliseconds
I0122 11:17:18.382819   15568 round_trippers.go:442] Response Headers:
I0122 11:17:18.382819   15568 round_trippers.go:445]     Content-Type: application/json
I0122 11:17:18.382819   15568 round_trippers.go:445]     Date: Mon, 22 Jan 2018 10:17:17 GMT
I0122 11:17:18.383799   15568 request.go:836] Response Body: {""kind"":""APIVersions"",""versions"":[""v1""],""serverAddressByClientCIDRs"":[{""clientCIDR"":""0.0.0.0/0"",""serverAddress"":""172.20.1.90:443""}]}
I0122 11:17:18.385754   15568 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl.exe/v1.8.3 (windows/amd64) kubernetes/f0efb3c"" -H ""Authorization: Basic dXNlbGVzczp1c2VsZXNz"" https://prod-k8s.dev.ci5.io/apis
I0122 11:17:18.394554   15568 round_trippers.go:436] GET https://prod-k8s.dev.ci5.io/apis 200 OK in 8 milliseconds
I0122 11:17:18.394554   15568 round_trippers.go:442] Response Headers:
I0122 11:17:18.394554   15568 round_trippers.go:445]     Content-Type: application/json
I0122 11:17:18.394554   15568 round_trippers.go:445]     Date: Mon, 22 Jan 2018 10:17:17 GMT
I0122 11:17:18.396505   15568 request.go:836] Response Body:
I0122 11:17:18.397484   15568 cached_discovery.go:126] skipped caching discovery info due to 0-length response
F0122 11:17:18.397484   15568 helpers.go:120] error: 0-length respons
```
As you can see, the `/api` call goes through with a correct reply. The subsequent request to `/apis` instead returns an empty body. 
Investigating the logs of my OIDC proxy show that there is no request to `/apis` going through the proxy, there are instead 2 calls to `/api` only.
Notice that I use a fake user in kubectl config (because a user is required) - the Authentication header is removed automatically from the proxy that will replace it according to OIDC authentication.

**What you expected to happen**:
Kubectl should work through the local proxy.

**How to reproduce it** (as minimally and precisely as possible):
(Requires OIDC authenticator and an identity provider)
- Run the OIDC authenticator:
```
docker run -d --name kc-proxy -p 3000:3000 quay.io/gambol99/keycloak-proxy:latest --discovery-url=https://kcauth/auth/realms/myrealm \
--client-id=kubernetes \
--client-secret=xxxx \
--redirection-url=http://192.168.99.100:3000 \
--listen=0.0.0.0:3000 \
--enable-forwarding=true \
--forwarding-username=user \
--forwarding-password=""password"" \
--forwarding-domains=myk8s.domain
```
- let kubectl use the proxy to connect: `HTTPS_PROXY=http://192.168.99.100:3000 kubectl  --insecure-skip-tls-verify get namespaces -v=10`

**Anything else we need to know**:
Using `curl` to access the K8S API through the authenticating proxy works correctly. 
",open,False,2018-01-22 11:18:34,2019-03-25 12:10:22
kubectl,anitakumar,https://github.com/kubernetes/kubectl/issues/217,https://api.github.com/repos/kubernetes/kubectl/issues/217,"Error: error validating """": error validating data: kind not set","i have a deployment and the service yaml files as follows

service.yaml 
apiVersion: v1
kind: Service
...
deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
...
i am using helm but i believe this is kubectl issue

below  is the version of kubectl  i am using
lient Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.3"", GitCommit:""f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd"", GitTreeState:""clean"", BuildDate:""2017-11-08T18:39:33Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.11"", GitCommit:""b13f2fd682d56eab7a6a2b5a1cab1a3d2c8bdd55"", GitTreeState:""clean"", BuildDate:""2017-11-25T17:51:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}


is the problem to do with the kind or is further down the file. i am not sure what it is trying to validate as it is blank



",closed,False,2018-01-22 17:31:31,2018-01-25 10:13:22
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/218,https://api.github.com/repos/kubernetes/kubectl/issues/218,additional labels and annotations util,"An implementation of `Transformer` interface to apply additional labels annotations.

",closed,True,2018-01-22 19:03:32,2018-01-24 01:06:12
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/219,https://api.github.com/repos/kubernetes/kubectl/issues/219,util to update name reference,"This PR is based on top of https://github.com/kubernetes/kubectl/pull/218. The first 2 commits are from https://github.com/kubernetes/kubectl/pull/218.

Add util to update the name field that references another object's name.

",closed,True,2018-01-23 01:08:43,2018-01-24 23:18:22
kubectl,ant31,https://github.com/kubernetes/kubectl/pull/220,https://api.github.com/repos/kubernetes/kubectl/issues/220,kinflate: Separate 'resources' from 'packages',"I've been trying kinflate and have two feedbacks:

##### 1. Should not be forced to use 'overlays'
The initial proposal differentiated Overlays and BaseManifests but we later concluded that they could be merged into the resource Manifest, which can *optionally* include other Manifests.
 
In the current version, a user can't inflate a package that doesn't reference an another package
``` shell
$ kinflate -f pkg/kinflate/examples/simple/package/
error: open pkg/kinflate/examples/simple/package/deployment/deployment.yaml/Kube-manifest.yaml: not a directory
```
`simple/package/Kube-manifest.yaml`  is a valid manifest and the expected output is:
```yaml
---
apiVersion: v1
kind: Service
metadata:
  annotations: {}
  labels:
    app: mungebot
  name: mungebot-service
spec:
  ports:
  - port: 7002
  selector:
    app: mungebot
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations: {}
  labels:
    app: mungebot
  name: mungebot
spec:
  replicas: 1
  template:
    annotations: {}
    metadata:
      labels:
        app: mungebot
    spec:
      containers:
      - env:
        - name: foo
          value: bar
        image: nginx
        name: nginx
        ports:
        - containerPort: 80
```

##### 2. Can't include new resources in 'overlays'
The `resources` field is used for two distinct purposes and they can't be mixed
 - Reference other packages
 - Include resources

It should be possible to include additional resources to a package (e.g. an ingress) 
I propose to add a new field 'packages'  instead of mixing them under the `resources` field. 
```
packages:
  ../../package
resources:
 - files/ingress.yaml
```

cc @monopole / @mengqiy 
",closed,True,2018-01-23 15:41:29,2018-02-09 00:18:38
kubectl,MrHohn,https://github.com/kubernetes/kubectl/issues/221,https://api.github.com/repos/kubernetes/kubectl/issues/221,Unable to change service from type=NodePort to type=ClusterIP with kubectl,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
/kind bug

**Kubernetes version** (use `kubectl version`):
```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.6"", GitCommit:""6260bb08c46c31eea6cb538b34a9ceb3e406689c"", GitTreeState:""clean"", BuildDate:""2017-12-21T06:34:11Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10+"", GitVersion:""v1.10.0-alpha.1.930+0d1986ad8219bf-dirty"", GitCommit:""0d1986ad8219bf1a64fc657eac407cc84d3ad647"", GitTreeState:""dirty"", BuildDate:""2018-01-17T03:10:36Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:
- Create a service with type=ClusterIP. Let's say kube-dns:
```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: kube-dns
    kubernetes.io/cluster-service: ""true""
    kubernetes.io/name: KubeDNS
  name: kube-dns
  namespace: kube-system
spec:
  clusterIP: 10.0.0.10
  ports:
  - name: dns
    port: 53
    protocol: UDP
    targetPort: 53
  - name: dns-tcp
    port: 53
    protocol: TCP
    targetPort: 53
  selector:
    k8s-app: kube-dns
  type: ClusterIP
```
- Edit type from ClusterIP to NodePort, and now we have:
```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: kube-dns
    kubernetes.io/cluster-service: ""true""
    kubernetes.io/name: KubeDNS
  name: kube-dns
  namespace: kube-system
spec:
  clusterIP: 10.0.0.10
  ports:
  - name: dns
    nodePort: 31716
    port: 53
    protocol: UDP
    targetPort: 53
  - name: dns-tcp
    nodePort: 30492
    port: 53
    protocol: TCP
    targetPort: 53
  selector:
    k8s-app: kube-dns
  type: NodePort
```
- Edit type from NodePort to ClusterIP with **all the auto-allocated nodePorts removed**, and we hit this error:
```
# services ""kube-dns"" was not valid:
# * spec.ports[1].nodePort: Forbidden: may not be used when `type` is 'ClusterIP'
```
- Try to use `kubectl apply -f` with a similar kube-dns yaml (type=ClusterIP with all nodePorts removed) and hit the same error:
```
$ kubectl apply -f kube-dns.yaml 
The Service ""kube-dns"" is invalid: spec.ports[1].nodePort: Forbidden: may not be used when `type` is 'ClusterIP'
```

**What you expected to happen**:
Users should be able to modify service type from NodePort  to CluserIP.

**Anything else we need to know**:
Is this a regression? There was a similar bug before (https://github.com/kubernetes/kubernetes/issues/42282), which was fixed.

@mengqiy @kubernetes/sig-cli-bugs ",closed,False,2018-01-23 18:30:28,2019-03-12 15:07:40
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/222,https://api.github.com/repos/kubernetes/kubectl/issues/222,fix golink and refactor test,,closed,True,2018-01-24 02:10:08,2018-01-24 22:20:16
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/223,https://api.github.com/repos/kubernetes/kubectl/issues/223,Remove pkg/framework/test that has moved to https://github.com/kubern…,…etes-sig-testing/frameworks,closed,True,2018-01-24 17:21:43,2018-02-07 18:15:48
kubectl,anitakumar,https://github.com/kubernetes/kubectl/issues/224,https://api.github.com/repos/kubernetes/kubectl/issues/224,error trying to create a Role,"
i tried to create the Role with a cluster running in aws using the file below
```
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  namespace: dev1
  name: developer
rules:
- apiGroups: ["""", ""extensions"", ""apps""]
  resources: [""deployments"", ""replicasets"", ""pods""]
  verbs: [""get"", ""list"", ""watch"", ""create"", ""update"", ""patch"", ""delete""]
```
```
kubectl create -f <filename>
```
errror
```
Error from server (Forbidden): error when creating ""nodejs-role1.yaml"": roles.rbac.authorization.k8s.io ""developer"" is forbidden: attempt to grant
 extra privileges: [PolicyRule{Resources:[""deployments""], APIGroups:[""""], Verbs:[""get""]} PolicyRule{Resources:[""deployments""], APIGroups:[""""], Ver
bs:[""list""]} PolicyRule{Resources:[""deployments""], APIGroups:[""""], Verbs:[""watch""]} PolicyRule{Resources:[""deployments""], APIGroups:[""""], Verbs:[""
create""]} PolicyRule{Resources:[""depl
```
**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.3"", GitCommit:""f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd"", GitTreeState:""clean"", BuildDate:""2017-11-08T18:39:33Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.11"", GitCommit:""b13f2fd682d56eab7a6a2b5a1cab1a3d2c8bdd55"", GitTreeState:""clean"", BuildDate:""2017-11-25T17:51:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}


i have seen something on the internet about google cloud there is a workaround.
is there something similar for aws




",closed,False,2018-01-24 17:50:00,2018-04-14 14:44:55
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/225,https://api.github.com/repos/kubernetes/kubectl/issues/225,Rebase kinflate on new utils,"Rebase kinflate on new utils and remove some dead code.
kinflate no longer requires overlays. i.e. each manifest can be applied alone.
This PR addresses the 1st issue in https://github.com/kubernetes/kubectl/pull/220#issue-290884591
",closed,True,2018-01-24 22:23:50,2018-01-27 04:30:18
kubectl,rednatto,https://github.com/kubernetes/kubectl/issues/226,https://api.github.com/repos/kubernetes/kubectl/issues/226,kubectl panic on pod delete,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): panic, SIGSEGV

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT ?

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
 - Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.2"", GitCommit:""5fa2db2bd46ac79e5e00a4e6ed24191080aa463b"", GitTreeState:""clean"", BuildDate:""2018-01-18T21:12:46Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""darwin/amd64""}
 - Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T20:55:30Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}



**Environment**:
- **Cloud provider or hardware configuration**: Minikube (minikube version: v0.24.1 (also seems to be same at v0.25.0)) ( --kubernetes-version v1.9.0 --bootstrapper kubeadm --vm-driver virtualbox  )
- **OS** (e.g. from /etc/os-release): macOS High Sierra (Version 10.13.2)
- **Kernel** (e.g. `uname -a`): Darwin A966.local 17.3.0 Darwin Kernel Version 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64
- **Install tools**: brew?
- **Others**: Docker for mac running?


**What happened**:
```
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x58 pc=0x21b8f83]
```


**What you expected to happen**:
No panic.

**How to reproduce it** (as minimally and precisely as possible):
Get ngnix-deployment.yaml from https://raw.githubusercontent.com/kubernetes/website/master/docs/concepts/workloads/controllers/nginx-deployment.yaml .

Create the deployment:
`λ kubectl create -f nginx-deployment.yaml`                                     
```
deployment ""nginx-deployment"" created
```

Delete a pod from the deployment (either one or all gives the same result):
`λ kubectl delete pod nginx-deployment-6c54bd5869-2lshn`                        
```
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x58 pc=0x21b8f83]

goroutine 1 [running]:
k8s.io/kubernetes/pkg/kubectl.ReaperFor(0x0, 0x0, 0x2370237, 0x3, 0x0, 0x0, 0x0, 0xc420a094a0, 0x13fdaa2, 0xbe9284d8ac70ccf0)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/delete.go:82 +0x1373
k8s.io/kubernetes/pkg/kubectl/cmd/util.(*ring1Factory).Reaper(0xc420751200, 0xc4204f79d0, 0x3666b40, 0x1a0003825900, 0xb8, 0xb8)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/util/factory_object_mapping.go:295 +0x151
k8s.io/kubernetes/pkg/kubectl/cmd/util.(*factory).Reaper(0xc420751230, 0xc4204f79d0, 0x3825900, 0x3e2b6c8, 0x0, 0x2678340)
        <autogenerated>:1 +0x47
k8s.io/kubernetes/pkg/kubectl/cmd.ReapResult.func1(0xc4204f7b20, 0x0, 0x0, 0x3815c00, 0x2853435)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/delete.go:250 +0xe9
k8s.io/kubernetes/pkg/kubectl/resource.ContinueOnErrorVisitor.Visit.func1(0xc4204f7b20, 0x0, 0x0, 0x0, 0x0)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:396 +0x164
k8s.io/kubernetes/pkg/kubectl/resource.DecoratedVisitor.Visit.func1(0xc4204f7b20, 0x0, 0x0, 0x7, 0xa)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:372 +0xe7
k8s.io/kubernetes/pkg/kubectl/resource.FlattenListVisitor.Visit.func1(0xc4204f7b20, 0x0, 0x0, 0xc42096d140, 0x414e798)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:433 +0x4fe
k8s.io/kubernetes/pkg/kubectl/resource.(*Info).Visit(0xc4204f7b20, 0xc42096d140, 0x0, 0x10103e7)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:105 +0x42
k8s.io/kubernetes/pkg/kubectl/resource.VisitorList.Visit(0xc4201f2350, 0x1, 0x1, 0xc42096d140, 0x1, 0xc42096d140)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:234 +0x63
k8s.io/kubernetes/pkg/kubectl/resource.(*VisitorList).Visit(0xc420450900, 0xc42096d140, 0x3e2b6c8, 0x0)
        <autogenerated>:1 +0x58
k8s.io/kubernetes/pkg/kubectl/resource.FlattenListVisitor.Visit(0x365a640, 0xc420450900, 0xc4203516c0, 0xc4207864c0, 0x60000000001, 0xc4207864c0)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:428 +0x9e
k8s.io/kubernetes/pkg/kubectl/resource.(*FlattenListVisitor).Visit(0xc420450920, 0xc4207864c0, 0x28, 0x3825900)
        <autogenerated>:1 +0x58
k8s.io/kubernetes/pkg/kubectl/resource.DecoratedVisitor.Visit(0x365a5c0, 0xc420450920, 0xc4201f2360, 0x2, 0x2, 0xc420450980, 0x1, 0xc420450980)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:363 +0x9b
k8s.io/kubernetes/pkg/kubectl/resource.(*DecoratedVisitor).Visit(0xc42096d110, 0xc420450980, 0xc4201f2370, 0xc420a09b60)
        <autogenerated>:1 +0x62
k8s.io/kubernetes/pkg/kubectl/resource.ContinueOnErrorVisitor.Visit(0x365a4c0, 0xc42096d110, 0xc4204f7b90, 0x0, 0x2)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:391 +0xe4
k8s.io/kubernetes/pkg/kubectl/resource.(*ContinueOnErrorVisitor).Visit(0xc4201f2370, 0xc4204f7b90, 0x1010b48, 0x70)
        <autogenerated>:1 +0x4f
k8s.io/kubernetes/pkg/kubectl/resource.(*Result).Visit(0xc4202ae600, 0xc4204f7b90, 0xc42095bd10, 0x3679b80)```
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/resource/result.go:98 +0x62
k8s.io/kubernetes/pkg/kubectl/cmd.ReapResult(0xc4202ae600, 0x3686980, 0xc420751230, 0x3659480, 0xc42000c018, 0xc420130001, 0x0, 0xffffffffffffffff, 0xc4209a0000, 0x3679b80, ...)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/delete.go:245 +0x16c
k8s.io/kubernetes/pkg/kubectl/cmd.(*DeleteOptions).RunDelete(0xc4207104d0, 0xc420290480, 0x0)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/delete.go:235 +0xd2
k8s.io/kubernetes/pkg/kubectl/cmd.NewCmdDelete.func1(0xc420290480, 0xc42013d4c0, 0x2, 0x2)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/delete.go:142 +0x178
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute(0xc420290480, 0xc42013d300, 0x2, 0x2, 0xc420290480, 0xc42013d300)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:603 +0x234
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0xc420396900, 0x8000104, 0x0, 0xffffffffffffffff)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:689 +0x2fe
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute(0xc420396900, 0xc420751230, 0x3659440)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:648 +0x2b
k8s.io/kubernetes/cmd/kubectl/app.Run(0x0, 0x0)
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/cmd/kubectl/app/kubectl.go:41 +0xd5
main.main()
        /private/tmp/kubernetes-cli-20180118-36889-kx51hi/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/cmd/kubectl/kubectl.go:27 +0x26
```

**Anything else we need to know**:
`kubectl describe pod nginx-deployment-6c54bd5869-2lshn`
Gives correct result.

Any deployment gives the same result.
",closed,False,2018-01-25 09:08:31,2018-08-29 20:42:52
kubectl,dls314,https://github.com/kubernetes/kubectl/issues/227,https://api.github.com/repos/kubernetes/kubectl/issues/227,Consider preferring USERPROFILE as the home path more often on Windows,"**Is this a request for help?**: No

**What keywords did you search in Kubernetes issues before filing this one?** : homedrive

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): bug-feature report-request

**Kubernetes version** (use `kubectl version`): 1.8.2

**Environment**:
- **Cloud provider or hardware configuration**: Docker for Windows, Minikube
- **OS** (e.g. from /etc/os-release): Windows 10
- **Kernel** (e.g. `uname -a`): n/a
- **Install tools**: n/a
- **Others**: n/a

**What happened**:
**What you expected to happen**:
**How to reproduce it** (as minimally and precisely as possible):
**Anything else we need to know**:

The `.kube` directory and `.kube\config` are being located inconsistently among some set of tooling including Docker for Windows CE 18.02.0-ce-rc1-win50 (15489) and kubectl 1.8.2.

I noticed this because `kubectl config get-clusters` returned an empty list, but I expected to see `docker-for-desktop` and `minikube` based on the contents from my `%USERPROFILE%\.kube\config` file.

I'm in a corporate environment where HOMEDRIVE HOMEPATH and HOMESHARE are in use to redirect home directories to a network drive. I only mention this because it leads to these paths differing from USERPROFILE.

In minikube, I think that the preference/fallback order leads to picking USERPROFILE as in this code:
https://github.com/kubernetes/minikube/blob/master/vendor/github.com/docker/machine/libmachine/mcnutils/utils.go#L30-L33

In kubectl, I think that the preference/fallback order leads to picking HOMEDRIVE + HOMEPATH as in this code:
https://github.com/kubernetes/kubectl/blob/master/pkg/pluginutils/plugin_client.go#L41-L44

In docker ce, I think that the preference/fallback order leads to picking USERPROFILE as in this code:
https://github.com/docker/docker-ce/blob/master/components/engine/pkg/homedir/homedir_windows.go

Those are only my best guesses about relevant code, and there are lots of other places in the wider codebases where you can find `os.Getenv(""USERPROFILE"")` or `os.Getenv(""HOMEPATH"")`, but for this narrow combination, based on behavior, I think that kubectl is the odd one out.

I think a different fallback order that prefers USERPROFILE as the other tools seem to would be nice.








",open,False,2018-01-26 20:23:51,2018-09-25 21:53:16
kubectl,dls314,https://github.com/kubernetes/kubectl/pull/228,https://api.github.com/repos/kubernetes/kubectl/issues/228,Update plugin_client.go,"Prefer USERPROFILE as the home path if available.

Possible fix for
https://github.com/kubernetes/kubectl/issues/227",closed,True,2018-01-26 21:02:01,2018-01-29 19:20:08
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/229,https://api.github.com/repos/kubernetes/kubectl/issues/229,fix name ref issue,Fix an issue when failing to find the name of a configmap or a secret.,closed,True,2018-01-26 23:04:11,2018-01-27 00:33:49
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/230,https://api.github.com/repos/kubernetes/kubectl/issues/230,delete dead code,"Delete `pod_template_visitor` and the pkg it depends on, i.e. `pkg/kinflate/apps`.",closed,True,2018-01-29 20:58:24,2018-01-29 21:54:38
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/231,https://api.github.com/repos/kubernetes/kubectl/issues/231,Add init command.,,closed,True,2018-01-29 23:43:16,2018-04-06 21:28:12
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/232,https://api.github.com/repos/kubernetes/kubectl/issues/232,add mock fs and file for testing,,closed,True,2018-01-30 17:53:28,2018-01-30 18:23:15
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/233,https://api.github.com/repos/kubernetes/kubectl/issues/233,"Tweak the fakes, add a test of fake.",,closed,True,2018-01-30 19:16:37,2018-04-06 21:28:13
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/234,https://api.github.com/repos/kubernetes/kubectl/issues/234,real kinflate init,,closed,True,2018-01-30 21:09:33,2018-01-30 21:14:25
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/235,https://api.github.com/repos/kubernetes/kubectl/issues/235,kinflate: Create AddConfigMap structure,This is empty and does nothing.,closed,True,2018-01-30 22:14:39,2018-01-31 18:54:12
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/236,https://api.github.com/repos/kubernetes/kubectl/issues/236,start addresource command,,closed,True,2018-01-30 22:47:34,2018-04-06 21:28:14
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/237,https://api.github.com/repos/kubernetes/kubectl/issues/237,More addresource code,,closed,True,2018-01-31 00:09:30,2018-04-06 21:28:16
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/238,https://api.github.com/repos/kubernetes/kubectl/issues/238,Init writes only a manifest to the current dir.,,closed,True,2018-01-31 17:54:00,2018-04-06 21:28:17
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/239,https://api.github.com/repos/kubernetes/kubectl/issues/239,Apelisse droot seans3,,closed,True,2018-01-31 19:47:24,2018-01-31 22:40:06
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/240,https://api.github.com/repos/kubernetes/kubectl/issues/240,kinflate: Manifest type improvements,"This also updates the code, the tests and the example.",closed,True,2018-02-01 18:01:52,2018-02-05 21:10:54
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/241,https://api.github.com/repos/kubernetes/kubectl/issues/241,multitransformer,,closed,True,2018-02-01 22:03:50,2018-02-02 17:16:05
kubectl,droot,https://github.com/kubernetes/kubectl/pull/242,https://api.github.com/repos/kubernetes/kubectl/issues/242,WIP kinflate add configmap/secret commands,,closed,True,2018-02-02 00:09:33,2018-02-06 23:37:07
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/243,https://api.github.com/repos/kubernetes/kubectl/issues/243,restructure the code a bit,"This PR depends on https://github.com/kubernetes/kubectl/pull/241.
The 1st commit is from https://github.com/kubernetes/kubectl/pull/241.
No new code in the 2nd commit.
",closed,True,2018-02-02 01:12:58,2018-02-02 17:16:10
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/244,https://api.github.com/repos/kubernetes/kubectl/issues/244,add a constructor for multitransformer,,closed,True,2018-02-02 17:44:44,2018-02-02 18:11:36
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/245,https://api.github.com/repos/kubernetes/kubectl/issues/245,add missing command,,closed,True,2018-02-02 20:46:24,2018-04-06 21:28:18
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/246,https://api.github.com/repos/kubernetes/kubectl/issues/246,register addResource,,closed,True,2018-02-02 20:59:38,2018-02-02 21:00:44
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/247,https://api.github.com/repos/kubernetes/kubectl/issues/247,All Run functions should not depend on Cobra,,closed,True,2018-02-02 21:35:31,2018-02-03 00:28:48
kubectl,monopole,https://github.com/kubernetes/kubectl/issues/248,https://api.github.com/repos/kubernetes/kubectl/issues/248,inf loop if cycle in manifest search,"kinflate has inf loop if one has a structure like

dir/
  <no manifest>
  subdir/
    manifest that specifies a resource at ""..""
",closed,False,2018-02-03 00:09:09,2018-05-18 21:23:26
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/249,https://api.github.com/repos/kubernetes/kubectl/issues/249,fix a bug of nameprefix lost after applying patch,,closed,True,2018-02-03 00:46:53,2018-02-06 05:37:20
kubectl,peterfication,https://github.com/kubernetes/kubectl/issues/250,https://api.github.com/repos/kubernetes/kubectl/issues/250,kubectl proxy with protocol in stdout message,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): `proxy`

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FEATURE REQUEST

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

It would be nice to have the protocol added to the address in the output of the `kubectl proxy` command.

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T15:13:29Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.2"", GitCommit:""08e099554f3c31f6e6f07b448ab3ed78d0520507"", GitTreeState:""clean"", BuildDate:""2017-01-12T04:52:34Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:
I ran `kubectl proxy` and the output was:
```
Starting to serve on 127.0.0.1:8001
```

**What you expected to happen**:
I would appreciate the following output:
```
Starting to serve on http://127.0.0.1:8001
```
The advantage here is that I can click in the console on it and the browser opens the URL.

The naive thought about this is of course, yeah shouldn't be a problem looking at the code [here](https://github.com/kubernetes/kubernetes/blob/048757b8a51333f59d3112d2b228d2f0102a4afc/pkg/kubectl/cmd/proxy.go#L156). But maybe someone needs `https` instead of `http` and then it gets already more complicated. Maybe there are also other security implications I don't know. I'm happy to hear some thoughts on this.",open,False,2018-02-03 11:30:04,2019-01-23 23:27:48
kubectl,hangyan,https://github.com/kubernetes/kubectl/pull/251,https://api.github.com/repos/kubernetes/kubectl/issues/251,Fix typo in function name and comment,,closed,True,2018-02-05 09:32:55,2018-02-05 17:37:12
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/252,https://api.github.com/repos/kubernetes/kubectl/issues/252,WIP: Kinflate addConfigMap,,closed,True,2018-02-05 18:59:58,2018-02-08 18:29:52
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/253,https://api.github.com/repos/kubernetes/kubectl/issues/253,add setprefixname command,,closed,True,2018-02-05 21:27:13,2018-04-06 21:28:19
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/254,https://api.github.com/repos/kubernetes/kubectl/issues/254,modifyREADME,show simple usage,closed,True,2018-02-05 22:04:05,2018-04-06 21:28:23
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/255,https://api.github.com/repos/kubernetes/kubectl/issues/255,tweakREADME,,closed,True,2018-02-05 23:44:36,2018-04-06 21:27:55
kubectl,droot,https://github.com/kubernetes/kubectl/pull/256,https://api.github.com/repos/kubernetes/kubectl/issues/256,Add configmap,"This PR adds ""configmap"" command to kinflate.",closed,True,2018-02-06 00:24:52,2018-02-06 18:01:31
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/257,https://api.github.com/repos/kubernetes/kubectl/issues/257,Kinflate add generic secret command.,,closed,True,2018-02-06 18:34:10,2018-02-06 19:44:17
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/258,https://api.github.com/repos/kubernetes/kubectl/issues/258,Unstruct package framework for json/yaml manipulation,"The package ""unstruct"" provides utility functions to walk through and modify an unstructured json/yaml object (as decoded in an `interface{}`)
",closed,True,2018-02-06 19:50:24,2018-03-19 22:43:22
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/259,https://api.github.com/repos/kubernetes/kubectl/issues/259,Kinflate add secret tls command.,,closed,True,2018-02-06 19:57:02,2018-02-06 23:40:45
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/260,https://api.github.com/repos/kubernetes/kubectl/issues/260,Change command setprefixname to setnameprefix,,closed,True,2018-02-06 23:06:28,2018-02-09 20:11:17
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/261,https://api.github.com/repos/kubernetes/kubectl/issues/261,add short and longer demo,,closed,True,2018-02-06 23:34:21,2018-04-06 21:27:53
kubectl,droot,https://github.com/kubernetes/kubectl/pull/262,https://api.github.com/repos/kubernetes/kubectl/issues/262,kinflate: implemented 'add' subcommand,,closed,True,2018-02-07 00:49:43,2018-02-07 18:20:46
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/263,https://api.github.com/repos/kubernetes/kubectl/issues/263,fix wrong paths of name reference transformer config,,closed,True,2018-02-07 01:02:55,2018-02-07 01:16:51
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/264,https://api.github.com/repos/kubernetes/kubectl/issues/264,demotweak,,closed,True,2018-02-07 01:18:41,2018-02-07 01:18:54
kubectl,lmxia,https://github.com/kubernetes/kubectl/pull/265,https://api.github.com/repos/kubernetes/kubectl/issues/265,fix typos in pkg/kinflate/util_test.go and pkg/framework/test/etcd.go,,closed,True,2018-02-07 03:16:10,2018-02-14 17:41:41
kubectl,lmxia,https://github.com/kubernetes/kubectl/pull/266,https://api.github.com/repos/kubernetes/kubectl/issues/266,fix typos in cmd/kinflate/shortDemo.md,,closed,True,2018-02-07 03:21:33,2018-02-09 00:08:48
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/267,https://api.github.com/repos/kubernetes/kubectl/issues/267,tweakdemo,,closed,True,2018-02-07 16:52:00,2018-04-06 21:27:52
kubectl,droot,https://github.com/kubernetes/kubectl/pull/268,https://api.github.com/repos/kubernetes/kubectl/issues/268,kinflate: added set subcommand,,closed,True,2018-02-07 20:40:02,2018-02-07 20:54:44
kubectl,PCQ,https://github.com/kubernetes/kubectl/issues/269,https://api.github.com/repos/kubernetes/kubectl/issues/269,No major and minor serverVersion-Information using Minikube v0.25.0 and K8s v1.9.0,"Hello,
I am working on my Mac with the last minikube and kubernetes version
```
minikube version
> minikube version: v0.25.0
```
If have started minikube with
`minikube start --kubernetes-version v1.9.0`

But when I use kubectl to get the k8s version I only get empty Strings as minor and major serverVersion

```
kubectl version -o yaml
clientVersion:
  buildDate: 2018-01-04T11:52:23Z
  compiler: gc
  gitCommit: 3a1c9449a956b6026f075fa3134ff92f7d55f812
  gitTreeState: clean
  gitVersion: v1.9.1
  goVersion: go1.9.2
  major: ""1""
  minor: ""9""
  platform: darwin/amd64
serverVersion:
  buildDate: 2018-01-26T19:04:38Z
  compiler: gc
  gitCommit: 925c127ec6b946659ad0fd596fa959be43f0cc05
  gitTreeState: clean
  gitVersion: v1.9.0
  goVersion: go1.9.1
  major: """"
  minor: """"
  platform: linux/amd64`

```

I am using this informations in my Unit-Test to validate that the cluster is running in the correct version. ",closed,False,2018-02-08 12:26:03,2018-07-30 04:41:52
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/270,https://api.github.com/repos/kubernetes/kubectl/issues/270,manifest tree builder,"There will be another PR to update the transformers and switch kinflate inflate to use the tree builder.
",closed,True,2018-02-08 22:03:12,2018-02-09 18:37:18
kubectl,grodrigues3,https://github.com/kubernetes/kubectl/issues/271,https://api.github.com/repos/kubernetes/kubectl/issues/271,Add examples to existing Kapps demonstrating customization of key dimensions strategy (3),"Right now, the main customization that we want to give users as an option is the persistent storage.

List of apps to migrate: 
- [x] https://github.com/kinflate/mysql https://github.com/kinflate/mysql/pull/3
- [x] https://github.com/kinflate/wordpress https://github.com/kinflate/wordpress/pull/2
- [x] https://github.com/kinflate/wordpress-mysql https://github.com/kinflate/wordpress-mysql/pull/3
- [x] https://github.com/kinflate/redis https://github.com/kinflate/redis/pull/2
- [ ] https://github.com/kinflate/ldap
- [ ] https://github.com/kinflate/phpldapadmin
- [ ] https://github.com/kinflate/jenkins
- [x] https://github.com/kinflate/ldap-phpldapadmin",closed,False,2018-02-09 21:53:06,2018-02-23 04:15:59
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/272,https://api.github.com/repos/kubernetes/kubectl/issues/272,fix demo,,closed,True,2018-02-09 21:56:05,2018-04-06 21:27:52
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/273,https://api.github.com/repos/kubernetes/kubectl/issues/273,add demo testing,,closed,True,2018-02-10 00:41:43,2018-04-06 21:27:51
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/274,https://api.github.com/repos/kubernetes/kubectl/issues/274,Kinflate use manifest tree builder,,closed,True,2018-02-10 02:22:02,2018-02-12 23:54:40
kubectl,yank1,https://github.com/kubernetes/kubectl/pull/275,https://api.github.com/repos/kubernetes/kubectl/issues/275,fix a typo init.go,fix a typo init.go,closed,True,2018-02-11 06:28:06,2018-02-12 07:06:49
kubectl,CalvinHartwell,https://github.com/kubernetes/kubectl/issues/276,https://api.github.com/repos/kubernetes/kubectl/issues/276,Bug: kubectl create secret --from-file does not accept tilde ~ character in path,"**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.4"", GitCommit:""9befc2b8928a9426501d3bf62f72849d5cbcd5a3"", GitTreeState:""clean"", BuildDate:""2017-11-20T05:28:34Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.2"", GitCommit:""5fa2db2bd46ac79e5e00a4e6ed24191080aa463b"", GitTreeState:""clean"", BuildDate:""2018-01-18T09:42:01Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**: desktop/workstation and server/production/vm
- **Cloud provider or hardware configuration**:  k8s running on AWS but kubectl executed on x86 desktop machine. 
- **OS** (e.g. from /etc/os-release): Ubuntu 17.10
- **Kernel** (e.g. `uname -a`): Linux ubuntu-ws 4.13.0-17-generic #20-Ubuntu SMP Mon Nov 6 10:04:08 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**: snap
- **Others**:


**What happened**:
Attempt to create cluster secret from file using a relative path via tilde to refer to home directory. I.E: ~/somefile

But this does not work, it generates a file not found error. 

**What you expected to happen**:

I expect to be able to use tilde ~ in paths to files. 

**How to reproduce it** (as minimally and precisely as possible):

  # does not work, file not found. 
  kubectl create secret generic kubeconfig --from-file=~/somefile

  # works fine
  kubectl create secret generic kubeconfig --from-file=/home/<your-user>/somefile

**Anything else we need to know**:

",closed,False,2018-02-12 19:07:54,2018-08-15 18:41:37
kubectl,seans3,https://github.com/kubernetes/kubectl/issues/277,https://api.github.com/repos/kubernetes/kubectl/issues/277,Kinflate: Better Error Messages,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

FEATURE REQUEST:

kinflate checks for errors in ApplicationManifest and config files and prints meaningful error messages + exit non-0 (8)

- Check correctness of PATCH - that it applies correctly to an object that it finds
- Resources parse as objects
- Options are well formed - ApplicationManifest schema parses
- replacement for lint. lint is going to call this 
- hierarchy of errors 
- may depend on technical debt (file system) issue

e.g.

Filenot found errors

",closed,False,2018-02-12 23:11:05,2018-02-12 23:15:53
kubectl,seans3,https://github.com/kubernetes/kubectl/issues/278,https://api.github.com/repos/kubernetes/kubectl/issues/278,Use Internal Error Library throughout Application to Display More Descriptive Error Messages (8),"- [ ] Check correctness of PATCH - that it applies correctly to an object that it finds
- [ ] Resources parse as objects
- [ ] Options are well formed - ApplicationManifest schema parses
- [ ] replacement for lint. lint is going to call this 
- [x] hierarchy of errors 
- [x] may depend on technical debt (file system) issue

e.g.

Filenot found errors",closed,False,2018-02-12 23:13:55,2018-09-25 20:07:05
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/279,https://api.github.com/repos/kubernetes/kubectl/issues/279,error out when a patch has not matching object,"Address some comments from https://github.com/kubernetes/kubectl/pull/274:
- error out when a patch has not matching object
- it stop deleting the object from the overlay KObject.",closed,True,2018-02-13 00:08:43,2018-02-13 01:40:03
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/280,https://api.github.com/repos/kubernetes/kubectl/issues/280,More specific error messages for missing/bad manifest file,,closed,True,2018-02-13 00:58:21,2018-02-13 21:08:51
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/281,https://api.github.com/repos/kubernetes/kubectl/issues/281,Noop transformer,,closed,True,2018-02-13 01:00:10,2018-02-13 02:02:09
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/282,https://api.github.com/repos/kubernetes/kubectl/issues/282,WIP: fix filesystem tech debt,"Replaced the os.Stat by filesystem.Stat in builder.go
Improved the error message when loading resources
",closed,True,2018-02-13 18:27:29,2018-02-22 23:41:24
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/283,https://api.github.com/repos/kubernetes/kubectl/issues/283,Add links to existing demo and tutorial pages.,"Add links to existing demo and tutorial pages.
",closed,True,2018-02-13 18:48:11,2018-04-06 21:27:40
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/284,https://api.github.com/repos/kubernetes/kubectl/issues/284,Make test failures easier to see and understand.,"It's too difficult to read travis output and understand how many failures happened and where they occurred, because the output is one continuous stream and by design accumulates errors, only reporting a logical OR at the end.

This change increases the readability of the output, and makes it easy to add new tests that report themselves in a consistent fashion.


",closed,True,2018-02-13 21:27:22,2018-04-06 21:27:39
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/285,https://api.github.com/repos/kubernetes/kubectl/issues/285,kinflate: Generate secrets with commands,"Remove the DataSources for secret, and generate the content of secrets
with the output of commands. This allows more dynamic encrypted/secure
secrets that are not just saved plain in the repository.

Things that are not perfect with this code:
- I'm wondering if we should just merge back TLS secret with generic secrets now. I think that TLS secrets should also be generated by commands. It would re-simplify the code and the Kube-manifest file.
- I've removed many tests, the coverage for the new code is definitely not as good,
- I'm not sure this is going to work flawlessly when there are multiple Kube-manifest files in the process. Are the commands going to be executed from where they should? ... Do we have a good way to test that use-case?",closed,True,2018-02-14 17:40:43,2018-02-17 00:08:40
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/286,https://api.github.com/repos/kubernetes/kubectl/issues/286,As a user I have a way of managing Secrets outside of my source control. (5),"Create a document that answers:

- ""How do users use secrets today?""
- ""What secret storage systems do they use that we should support?""
- ""What commands / config do I need to integrate 'applying' these secrets?""",closed,False,2018-02-14 19:11:50,2018-02-21 19:37:38
kubectl,grodrigues3,https://github.com/kubernetes/kubectl/issues/287,https://api.github.com/repos/kubernetes/kubectl/issues/287,Figure out the story for examples - where do they live.  CLA bot for cncf?  etc,/assign @grodrigues3,closed,False,2018-02-14 19:17:03,2018-02-21 21:03:33
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/288,https://api.github.com/repos/kubernetes/kubectl/issues/288,"kinflate: Create abstraction for ""Application"" (above Kube-manifest)","Currently, most operations in Kinflate use the ""Manifest"" as the main abstraction for the application. But this is quite inaccurate, since an application can be built from multiple Manifest (multiple-layers, composed layers, ...). We need to create a new abstraction for the application that will handle these Manifests into a unified interface. That abstraction would be responsible for managing orders/priorities in which things need to be applied and overriden, etc.",closed,False,2018-02-14 22:20:13,2018-02-22 19:19:46
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/289,https://api.github.com/repos/kubernetes/kubectl/issues/289,Checkpoint toward better kinflate error messages.,"This is a partial refactor toward an organization which better supports kinlfate error messages. We recognize that both the Loader and ManifestLoader classes are incoherent, and we intend to address this issue in future PR's. We've included several TODO's.",closed,True,2018-02-14 22:55:39,2018-02-14 23:09:51
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/290,https://api.github.com/repos/kubernetes/kubectl/issues/290,Tutorial for kinflate commands (3),"init, add, set, inflate, patch overlay, 

How polished should this be:
- Tutorial on the K8s docs page on how to use this.  
- Aim for a 10-15 minute tutorial. 
- Should work on a mac or mobile devices
- Shouldn't have to install programs in order to follow tutorial",closed,False,2018-02-14 22:56:23,2018-02-20 17:58:32
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/291,https://api.github.com/repos/kubernetes/kubectl/issues/291,Technical Debt: Missing file system parameter (5),,closed,False,2018-02-14 22:58:02,2018-05-18 21:33:53
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/292,https://api.github.com/repos/kubernetes/kubectl/issues/292,add kinflate diff command,"- support noop option for kinflate inflate
- add a test for noop option
- port some diff util and update vendor to make it builds.
- add kinflate diff command (ONLY diff the current layer(Node), not the whole tree)

fixes: #295 ",closed,True,2018-02-15 00:18:19,2018-02-21 21:00:33
kubectl,ant31,https://github.com/kubernetes/kubectl/issues/293,https://api.github.com/repos/kubernetes/kubectl/issues/293,[kinflate] use namePrefix in every reference/direct link,"I have some following resources:
one ingress, one svc
```
---
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: webapp
  name: webapp
spec:
  ports:
  - name: http
    port: 5000
    protocol: TCP
    targetPort: 5000
  selector:
    k8s-app: webapp
  type: ClusterIP
```
and
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: webapp
spec:
  rules:
  - host: webapp-example.com
    http:
      paths:
      - backend:
          serviceName: webapp
          servicePort: 5000
```

In this case, using `namePrefix` isn't practical because I have to edit manually the svc:
```
- backend:
          serviceName: $namePrefix-webapp
          servicePort: 5000
```

I would expect a replacement in the serviceName too",closed,False,2018-02-15 15:30:25,2018-05-18 21:23:01
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/294,https://api.github.com/repos/kubernetes/kubectl/issues/294,New structure: ManifestError,"New structure (ManifestError) to encasulate info needed for descriptve errors while parsing, creating kinflate manifests.",closed,True,2018-02-15 21:04:16,2018-02-15 21:43:39
kubectl,mengqiy,https://github.com/kubernetes/kubectl/issues/295,https://api.github.com/repos/kubernetes/kubectl/issues/295,"As a user, I want to know exactly what kinflate is going to do and how it will change my resources (8).","Suggestion: `diff`
As a user I can see how an ApplicationManifest transforms a set of resources.

- Shows each resource kinflate parses
- Shows how patches are applied
- Shows newly created resources
- Shows label and name changes",closed,False,2018-02-15 21:53:04,2018-02-21 19:20:40
kubectl,droot,https://github.com/kubernetes/kubectl/pull/296,https://api.github.com/repos/kubernetes/kubectl/issues/296,kinflate: removed non-existing resources from manifest file,"`kinflate init` generates a manifest file which contains
non-existent resources in `resources` section. This change
keeps those resources in commented form so that 'kinflate inflate'
doesnt barf at the user.",closed,True,2018-02-15 23:20:05,2018-02-16 17:33:41
kubectl,droot,https://github.com/kubernetes/kubectl/pull/297,https://api.github.com/repos/kubernetes/kubectl/issues/297,kinflate: improved README and added getting-started,"Improves README.md
Added Getting Started guide",closed,True,2018-02-15 23:53:00,2018-02-17 00:15:38
kubectl,StegSchreck,https://github.com/kubernetes/kubectl/issues/298,https://api.github.com/repos/kubernetes/kubectl/issues/298,Make watch flag `-w` usable also for `kubectl get all`,"**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
* `watch`
* `-w`

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FEATURE REQUEST

I would like to use the watch flag `-w` not only for single resource types, but also for the summary of all resources. I would like to see that it is possible to call `kubectl get all -w`

**Kubernetes version** (use `kubectl version`):
Client 1.9.1
Server 1.9.1
",closed,False,2018-02-16 09:26:04,2018-07-24 03:18:53
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/299,https://api.github.com/repos/kubernetes/kubectl/issues/299,better integration test,,closed,True,2018-02-16 18:38:38,2018-02-16 19:58:21
kubectl,monopole,https://github.com/kubernetes/kubectl/issues/300,https://api.github.com/repos/kubernetes/kubectl/issues/300,kinflate lint,"jeff -
Sunil and i chatted about the difference between

  kinflate inflate -f foo >/dev/null

and
  
  kinflate lint -f foo

I.e. given that the inflate command is going to report errors like manifest pointing empty files and what not, is linting even a thing?  How is it different from running inflate and discarding the output?

One option is the Go approach.  A runnable Go package can fail “golint” because it  deviates from the best-practices / idioms that someone encoded into golint, and it can fail “govet” because it has unreachable code, useless assignments, locks passed by value, etc.   This is similar in some respects to “gcc -Wall -Wextra -pedantic”, i.e. a stricter mode of operation that fails if additional requirements are not met.

An example: 

Suppose an app has a resource file containing a Deployment with “Kind: apps/v1beta1”.

This command succeeds:

  kinflate inflate -f foo >/dev/null

And either of these equivalent commands fails:

  kinflate inflate --strict -f foo >/dev/null
  kinflate lint -f foo

So the task is add the command lint, and have it do everything inflate does PLUS more stuff and simply not output YAML, or add a --strict command to inflate that also imposes the same rules.

phil:
> How is it different from running inflate and discarding the output

Linting can break backwards compatibility by introducing new errors.  Kinflate cannot break backwards compatibility and has to continue to support anti-patterns that we discover in the future.",closed,False,2018-02-16 22:46:47,2018-05-18 21:28:50
kubectl,ellenkorbes,https://github.com/kubernetes/kubectl/pull/301,https://api.github.com/repos/kubernetes/kubectl/issues/301,Resource Library,"Package resource implements tools for the discovery and filtering of resources in an API server.

Tests to be implemented in a subsequent PR.",closed,True,2018-02-16 23:58:54,2018-03-06 17:09:53
kubectl,richardmarshall,https://github.com/kubernetes/kubectl/issues/302,https://api.github.com/repos/kubernetes/kubectl/issues/302,Support symlinks for kubectl plugins directory,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

- plugins
- symlink

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FEATURE REQUEST

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.2"", GitCommit:""5fa2db2bd46ac79e5e00a4e6ed24191080aa463b"", GitTreeState:""clean"", BuildDate:""2018-01-18T21:12:46Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""darwin/amd64""}
```

**What happened**:

If `~/.kube/plugins` is a symlink running `kubectl plugin` does not find plugins.

```
$ ln -s ~/src/github.com/kubernetes/kubernetes/pkg/kubectl/plugins/examples ~/.kube/plugins
$ kubectl plugin
error: no plugins installed.
```

**What you expected to happen**:

Running `kubectl plugin` lists the plugins at the target of the symlink.

```
$ ln -s ~/src/github.com/kubernetes/kubernetes/pkg/kubectl/plugins/examples ~/.kube/plugins
$ kubectl plugin
Runs a command-line plugin.

Plugins are subcommands that are not part of the major command-line distribution and can even be provided by
third-parties. Please refer to the documentation and examples for more information about how to install and write your
own plugins.

Available Commands:
  aging       Aging shows pods by age
  hello       I say hello!

Usage:
  kubectl plugin NAME [options]

Use ""kubectl <command> --help"" for more information about a given command.
Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

**Anything else we need to know**:

While this can also be solved by using the `KUBECTL_PLUGINS_PATH` environment variable having the ability to setup a symlink at `~/.kube/plugins` would be more convenient to manage in my opinion.

Looking at the [plugin loading code](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/plugins/loader.go#L71) it would seem the reason this doesn't work currently is due to the fact that `filepath.Walk` does not [expand symlinks](https://golang.org/pkg/path/filepath/#Walk). As a quick test I added a call to `filepath.EvalSymlinks` for the path that will get passed to `filepath.Walk` which results in the plugins being correctly enumerated.

```go
	base, err = filepath.EvalSymlinks(base)
	if err != nil {
		return nil, err
	}
```

If this idea seems reasonable I'm happy to implement the change and submit a PR.",closed,False,2018-02-18 21:37:57,2018-09-25 22:21:09
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/303,https://api.github.com/repos/kubernetes/kubectl/issues/303,Create different internal error types,,closed,True,2018-02-20 18:46:11,2018-02-20 23:33:17
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/304,https://api.github.com/repos/kubernetes/kubectl/issues/304,"Create internal error library different types.  For example, ManifestError, ResourceError (5)",,closed,False,2018-02-20 19:33:43,2018-02-21 19:37:29
kubectl,jethrogb,https://github.com/kubernetes/kubectl/issues/305,https://api.github.com/repos/kubernetes/kubectl/issues/305,"get --watch specifying a resource uses ""watch list"" instead of ""watch"" API","<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): watch

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.7"", GitCommit:""b30876a5539f09684ff9fde266fda10b37738c9c"", GitTreeState:""clean"", BuildDate:""2018-01-16T21:59:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.7"", GitCommit:""b30876a5539f09684ff9fde266fda10b37738c9c"", GitTreeState:""clean"", BuildDate:""2018-01-16T21:52:38Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Bare metal
- **OS** (e.g. from /etc/os-release): Ubuntu Xenial
- **Kernel** (e.g. `uname -a`): 4.4.0-104-generic #127-Ubuntu
- **Install tools**: kubeadm
- **Others**:


**What happened**:
When running `kubectl get RESOURCETYPE --watch OBJECTNAME`, it uses the ""watch list"" API with a fieldSelector on the resource name. If the current role doesn't allow watch list for this resource type, this results in a 403.

**What you expected to happen**:
I expected the ""watch"" API to be used.

**How to reproduce it** (as minimally and precisely as possible):
```
# Setup RBAC on your account to allow watch but not watch list
$ kubectl --v=6 get cm -n swdist swdist --watch|&grep GET
I0220 22:20:41.482970    2907 round_trippers.go:436] GET https://10.245.0.1:443/api/v1/namespaces/swdist/configmaps/swdist 200 OK in 9 milliseconds
I0220 22:20:41.492379    2907 round_trippers.go:436] GET https://10.245.0.1:443/api/v1/namespaces/swdist/configmaps?fieldSelector=metadata.name%3Dswdist&resourceVersion=0&watch=true 403 Forbidden in 1 milliseconds
```

**Anything else we need to know**:

",closed,False,2018-02-20 22:29:57,2018-12-28 15:41:22
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/306,https://api.github.com/repos/kubernetes/kubectl/issues/306,"""kubectl get"" downloads the entire swagger spec (GET /swagger-2.0.0.pb-v1) on every invocation","/kind bug

```
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-09T21:51:54Z"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.5-gke.0"", GitCommit:""2c2a807131fa8708abc92f3513fe167126c8cce5"", GitTreeState:""clean"", BuildDate:""2017-12-19T20:05:45Z"", GoVersion:""go1.8.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

When I run `kubectl get nodes` or `kubectl get storageclass` now, I see an extra `GET /swagger-2.0.0.pb-v1` request. This makes me think **it's downloading the entire swagger spec on every kubectl call**. Is this network traffic necessary?

It's also ruining the request/response debugging experience we used to get by adding `-v=10`, now it's printing an entire hexdump of the swagger response, it's incredibly long. I wish not to see this hexdump in the `-v=10` output if possible.

**115,000 lines omitted from the output below:** (this means 16*115000=1.8 MB, does this ever need to be pulled to the client at all?) 
```
I0220 14:42:24.143305   87296 loader.go:357] Config loaded from file /Users/ahmetb/.kube/config
I0220 14:42:24.146303   87296 cached_discovery.go:117] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/servergroups.json
I0220 14:42:24.147274   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apiregistration.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.147633   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/extensions/v1beta1/serverresources.json
I0220 14:42:24.147890   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apps/v1beta1/serverresources.json
I0220 14:42:24.148154   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apps/v1beta2/serverresources.json
I0220 14:42:24.148421   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authentication.k8s.io/v1/serverresources.json
I0220 14:42:24.148689   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authentication.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.148966   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authorization.k8s.io/v1/serverresources.json
I0220 14:42:24.149254   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authorization.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.149525   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/autoscaling/v1/serverresources.json
I0220 14:42:24.149826   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/autoscaling/v2beta1/serverresources.json
I0220 14:42:24.150143   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/batch/v1/serverresources.json
I0220 14:42:24.150389   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/batch/v1beta1/serverresources.json
I0220 14:42:24.150673   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/certificates.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.150925   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/networking.k8s.io/v1/serverresources.json
I0220 14:42:24.151182   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/policy/v1beta1/serverresources.json
I0220 14:42:24.151425   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/rbac.authorization.k8s.io/v1/serverresources.json
I0220 14:42:24.151661   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/rbac.authorization.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.151884   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/storage.k8s.io/v1/serverresources.json
I0220 14:42:24.152105   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/storage.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.152325   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apiextensions.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.152653   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/v1/serverresources.json
I0220 14:42:24.153159   87296 cached_discovery.go:117] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/servergroups.json
I0220 14:42:24.153405   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apiregistration.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.153704   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/extensions/v1beta1/serverresources.json
I0220 14:42:24.153973   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apps/v1beta1/serverresources.json
I0220 14:42:24.154241   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apps/v1beta2/serverresources.json
I0220 14:42:24.154459   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authentication.k8s.io/v1/serverresources.json
I0220 14:42:24.154647   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authentication.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.154841   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authorization.k8s.io/v1/serverresources.json
I0220 14:42:24.155030   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authorization.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.155216   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/autoscaling/v1/serverresources.json
I0220 14:42:24.155398   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/autoscaling/v2beta1/serverresources.json
I0220 14:42:24.155578   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/batch/v1/serverresources.json
I0220 14:42:24.155761   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/batch/v1beta1/serverresources.json
I0220 14:42:24.155949   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/certificates.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.156163   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/networking.k8s.io/v1/serverresources.json
I0220 14:42:24.156368   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/policy/v1beta1/serverresources.json
I0220 14:42:24.156574   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/rbac.authorization.k8s.io/v1/serverresources.json
I0220 14:42:24.156778   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/rbac.authorization.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.156966   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/storage.k8s.io/v1/serverresources.json
I0220 14:42:24.157260   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/storage.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.157576   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apiextensions.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.158072   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/v1/serverresources.json
I0220 14:42:24.158507   87296 cached_discovery.go:117] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/servergroups.json
I0220 14:42:24.158812   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apiregistration.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.159124   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/extensions/v1beta1/serverresources.json
I0220 14:42:24.159372   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apps/v1beta1/serverresources.json
I0220 14:42:24.159671   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apps/v1beta2/serverresources.json
I0220 14:42:24.159979   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authentication.k8s.io/v1/serverresources.json
I0220 14:42:24.160247   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authentication.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.160482   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authorization.k8s.io/v1/serverresources.json
I0220 14:42:24.160716   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authorization.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.160983   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/autoscaling/v1/serverresources.json
I0220 14:42:24.161300   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/autoscaling/v2beta1/serverresources.json
I0220 14:42:24.161523   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/batch/v1/serverresources.json
I0220 14:42:24.161742   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/batch/v1beta1/serverresources.json
I0220 14:42:24.161983   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/certificates.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.162201   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/networking.k8s.io/v1/serverresources.json
I0220 14:42:24.162479   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/policy/v1beta1/serverresources.json
I0220 14:42:24.162731   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/rbac.authorization.k8s.io/v1/serverresources.json
I0220 14:42:24.162970   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/rbac.authorization.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.163204   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/storage.k8s.io/v1/serverresources.json
I0220 14:42:24.163478   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/storage.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.163685   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apiextensions.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.164030   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/v1/serverresources.json
I0220 14:42:24.164320   87296 cached_discovery.go:117] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/servergroups.json
I0220 14:42:24.164531   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apiregistration.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.164775   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/extensions/v1beta1/serverresources.json
I0220 14:42:24.164993   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apps/v1beta1/serverresources.json
I0220 14:42:24.165234   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apps/v1beta2/serverresources.json
I0220 14:42:24.165453   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authentication.k8s.io/v1/serverresources.json
I0220 14:42:24.165645   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authentication.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.165847   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authorization.k8s.io/v1/serverresources.json
I0220 14:42:24.166041   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/authorization.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.166227   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/autoscaling/v1/serverresources.json
I0220 14:42:24.166421   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/autoscaling/v2beta1/serverresources.json
I0220 14:42:24.166622   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/batch/v1/serverresources.json
I0220 14:42:24.166812   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/batch/v1beta1/serverresources.json
I0220 14:42:24.167028   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/certificates.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.167212   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/networking.k8s.io/v1/serverresources.json
I0220 14:42:24.167389   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/policy/v1beta1/serverresources.json
I0220 14:42:24.167575   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/rbac.authorization.k8s.io/v1/serverresources.json
I0220 14:42:24.167775   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/rbac.authorization.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.167955   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/storage.k8s.io/v1/serverresources.json
I0220 14:42:24.168142   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/storage.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.168331   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/apiextensions.k8s.io/v1beta1/serverresources.json
I0220 14:42:24.168618   87296 cached_discovery.go:70] returning cached discovery info from /Users/ahmetb/.kube/cache/discovery/35.202.32.200/v1/serverresources.json
I0220 14:42:24.169522   87296 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json"" -H ""User-Agent: kubectl/v1.9.3 (darwin/amd64) kubernetes/d283541"" https://35.202.32.200/apis/storage.k8s.io/v1/storageclasses?limit=500
I0220 14:42:24.351039   87296 round_trippers.go:436] GET https://35.202.32.200/apis/storage.k8s.io/v1/storageclasses?limit=500 200 OK in 181 milliseconds
I0220 14:42:24.351061   87296 round_trippers.go:442] Response Headers:
I0220 14:42:24.351075   87296 round_trippers.go:445]     Content-Length: 641
I0220 14:42:24.351086   87296 round_trippers.go:445]     Date: Tue, 20 Feb 2018 22:42:24 GMT
I0220 14:42:24.351096   87296 round_trippers.go:445]     Audit-Id: 0403b1b7-20c0-4757-b504-27852fd1e6f6
I0220 14:42:24.351105   87296 round_trippers.go:445]     Content-Type: application/json
I0220 14:42:24.351172   87296 request.go:873] Response Body: {""kind"":""StorageClassList"",""apiVersion"":""storage.k8s.io/v1"",""metadata"":{""selfLink"":""/apis/storage.k8s.io/v1/storageclasses"",""resourceVersion"":""6357994""},""items"":[{""metadata"":{""name"":""standard"",""selfLink"":""/apis/storage.k8s.io/v1/storageclasses/standard"",""uid"":""99b1ea2e-f4a4-11e7-8ede-42010a8000b9"",""resourceVersion"":""215"",""creationTimestamp"":""2018-01-08T18:49:04Z"",""labels"":{""addonmanager.kubernetes.io/mode"":""EnsureExists"",""kubernetes.io/cluster-service"":""true""},""annotations"":{""storageclass.beta.kubernetes.io/is-default-class"":""true""}},""provisioner"":""kubernetes.io/gce-pd"",""parameters"":{""type"":""pd-standard""},""reclaimPolicy"":""Delete""}]}
I0220 14:42:24.357536   87296 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.9.3 (darwin/amd64) kubernetes/d283541"" -H ""If-None-Match: \""7FB2CB2FBF2711DD0224E796342116CE3876F6865D6375D31865EF28BD86BAC21B43E7C2C8E9884EC8CE52895A5611FE2CF7E6BB66CAE9DFF6A5B8ADFAD2D4A6\"""" -H ""If-Modified-Since: Wed, 24 Jan 2018 05:32:53 GMT"" https://35.202.32.200/swagger-2.0.0.pb-v1
I0220 14:42:24.400922   87296 round_trippers.go:436] GET https://35.202.32.200/swagger-2.0.0.pb-v1 304 Not Modified in 43 milliseconds
I0220 14:42:24.400962   87296 round_trippers.go:442] Response Headers:
I0220 14:42:24.400977   87296 round_trippers.go:445]     Etag: ""7FB2CB2FBF2711DD0224E796342116CE3876F6865D6375D31865EF28BD86BAC21B43E7C2C8E9884EC8CE52895A5611FE2CF7E6BB66CAE9DFF6A5B8ADFAD2D4A6""
I0220 14:42:24.400992   87296 round_trippers.go:445]     Vary: Accept-Encoding
I0220 14:42:24.401016   87296 round_trippers.go:445]     Date: Tue, 20 Feb 2018 22:42:24 GMT
I0220 14:42:24.480831   87296 request.go:871] Response Body:
00000000  0a 03 32 2e 30 12 14 0a  0a 4b 75 62 65 72 6e 65  |..2.0....Kuberne|
00000010  74 65 73 12 06 76 31 2e  38 2e 35 42 a5 88 52 12  |tes..v1.8.5B..R.|
00000020  ca 02 0a 05 2f 61 70 69  2f 12 c0 02 12 bd 02 0a  |..../api/.......|
00000030  04 63 6f 72 65 1a 1a 67  65 74 20 61 76 61 69 6c  |.core..get avail|
00000040  61 62 6c 65 20 41 50 49  20 76 65 72 73 69 6f 6e  |able API version|
00000050  73 2a 12 67 65 74 43 6f  72 65 41 50 49 56 65 72  |s*.getCoreAPIVer|
00000060  73 69 6f 6e 73 32 10 61  70 70 6c 69 63 61 74 69  |sions2.applicati|
00000070  6f 6e 2f 6a 73 6f 6e 32  10 61 70 70 6c 69 63 61  |on/json2.applica|
00000080  74 69 6f 6e 2f 79 61 6d  6c 32 23 61 70 70 6c 69  |tion/yaml2#appli|
00000090  63 61 74 69 6f 6e 2f 76  6e 64 2e 6b 75 62 65 72  |cation/vnd.kuber|
000000a0  6e 65 74 65 73 2e 70 72  6f 74 6f 62 75 66 3a 10  |netes.protobuf:.|
000000b0  61 70 70 6c 69 63 61 74  69 6f 6e 2f 6a 73 6f 6e  |application/json|
000000c0  3a 10 61 70 70 6c 69 63  61 74 69 6f 6e 2f 79 61  |:.application/ya|
000000d0  6d 6c 3a 23 61 70 70 6c  69 63 61 74 69 6f 6e 2f  |ml:#application/|
000000e0  76 6e 64 2e 6b 75 62 65  72 6e 65 74 65 73 2e 70  |vnd.kubernetes.p|
000000f0  72 6f 74 6f 62 75 66 4a  6c 0a 51 0a 03 32 30 30  |rotobufJl.Q..200|
00000100  12 4a 0a 48 0a 02 4f 4b  12 42 0a 40 0a 3e 23 2f  |.J.H..OK.B.@.>#/|
00000110  64 65 66 69 6e 69 74 69  6f 6e 73 2f 69 6f 2e 6b  |definitions/io.k|
00000120  38 73 2e 61 70 69 6d 61  63 68 69 6e 65 72 79 2e  |8s.apimachinery.|
00000130  70 6b 67 2e 61 70 69 73  2e 6d 65 74 61 2e 76 31  |pkg.apis.meta.v1|
00000140  2e 41 50 49 56 65 72 73  69 6f 6e 73 0a 17 0a 03  |.APIVersions....|
00000150  34 30 31 12 10 0a 0e 0a  0c 55 6e 61 75 74 68 6f  |401......Unautho|
00000160  72 69 7a 65 64 52 05 68  74 74 70 73 12 d4 02 0a  |rizedR.https....|
00000170  08 2f 61 70 69 2f 76 31  2f 12 c7 02 12 c4 02 0a  |./api/v1/.......|
00000180  07 63 6f 72 65 5f 76 31  1a 17 67 65 74 20 61 76  |.core_v1..get av|
00000190  61 69 6c 61 62 6c 65 20  72 65 73 6f 75 72 63 65  |ailable resource|
000001a0  73 2a 15 67 65 74 43 6f  72 65 56 31 41 50 49 52  |s*.getCoreV1APIR|
000001b0  65 73 6f 75 72 63 65 73  32 10 61 70 70 6c 69 63  |esources2.applic|
000001c0  61 74 69 6f 6e 2f 6a 73  6f 6e 32 10 61 70 70 6c  |ation/json2.appl|
000001d0  69 63 61 74 69 6f 6e 2f  79 61 6d 6c 32 23 61 70  |ication/yaml2#ap|
000001e0  70 6c 69 63 61 74 69 6f  6e 2f 76 6e 64 2e 6b 75  |plication/vnd.ku|
000001f0  62 65 72 6e 65 74 65 73  2e 70 72 6f 74 6f 62 75  |bernetes.protobu|
00000200  66 3a 10 61 70 70 6c 69  63 61 74 69 6f 6e 2f 6a  |f:.application/j|
00000210  73 6f 6e 3a 10 61 70 70  6c 69 63 61 74 69 6f 6e  |son:.application|
00000220  2f 79 61 6d 6c 3a 23 61  70 70 6c 69 63 61 74 69  |/yaml:#applicati|
00000230  6f 6e 2f 76 6e 64 2e 6b  75 62 65 72 6e 65 74 65  |on/vnd.kubernete|
00000240  73 2e 70 72 6f 74 6f 62  75 66 4a 70 0a 55 0a 03  |s.protobufJp.U..|
00000250  32 30 30 12 4e 0a 4c 0a  02 4f 4b 12 46 0a 44 0a  |200.N.L..OK.F.D.|
00000260  42 23 2f 64 65 66 69 6e  69 74 69 6f 6e 73 2f 69  |B#/definitions/i|
00000270  6f 2e 6b 38 73 2e 61 70  69 6d 61 63 68 69 6e 65  |o.k8s.apimachine|
00000280  72 79 2e 70 6b 67 2e 61  70 69 73 2e 6d 65 74 61  |ry.pkg.apis.meta|
00000290  2e 76 31 2e 41 50 49 52  65 73 6f 75 72 63 65 4c  |.v1.APIResourceL|
000002a0  69 73 74 0a 17 0a 03 34  30 31 12 10 0a 0e 0a 0c  |ist....401......|
000002b0  55 6e 61 75 74 68 6f 72  69 7a 65 64 52 05 68 74  |UnauthorizedR.ht|
000002c0  74 70 73 12 c3 1e 0a 19  2f 61 70 69 2f 76 31 2f  |tps...../api/v1/|
000002d0  63 6f 6d 70 6f 6e 65 6e  74 73 74 61 74 75 73 65  |componentstatuse|
000002e0  73 12 a5 1e 12 c7 03 0a  07 63 6f 72 65 5f 76 31  |s........core_v1|
000002f0  1a 24 6c 69 73 74 20 6f  62 6a 65 63 74 73 20 6f  |.$list objects o|
00000300  66 20 6b 69 6e 64 20 43  6f 6d 70 6f 6e 65 6e 74  |f kind Component|
00000310  53 74 61 74 75 73 2a 19  6c 69 73 74 43 6f 72 65  |Status*.listCore|
00000320  56 31 43 6f 6d 70 6f 6e  65 6e 74 53 74 61 74 75  |V1ComponentStatu|
00000330  73 32 10 61 70 70 6c 69  63 61 74 69 6f 6e 2f 6a  |s2.application/j|
00000340  73 6f 6e 32 10 61 70 70  6c 69 63 61 74 69 6f 6e  |son2.application|
00000350  2f 79 61 6d 6c 32 23 61  70 70 6c 69 63 61 74 69  |/yaml2#applicati|
00000360  6f 6e 2f 76 6e 64 2e 6b  75 62 65 72 6e 65 74 65  |on/vnd.kubernete|
00000370  73 2e 70 72 6f 74 6f 62  75 66 32 1d 61 70 70 6c  |s.protobuf2.appl|
00000380  69 63 61 74 69 6f 6e 2f  6a 73 6f 6e 3b 73 74 72  |ication/json;str|
00000390  65 61 6d 3d 77 61 74 63  68 32 30 61 70 70 6c 69  |eam=watch20appli|
000003a0  63 61 74 69 6f 6e 2f 76  6e 64 2e 6b 75 62 65 72  |cation/vnd.kuber|
000003b0  6e 65 74 65 73 2e 70 72  6f 74 6f 62 75 66 3b 73  |netes.protobuf;s|
000003c0  74 72 65 61 6d 3d 77 61  74 63 68 3a 03 2a 2f 2a  |tream=watch:.*/*|
000003d0  4a 62 0a 47 0a 03 32 30  30 12 40 0a 3e 0a 02 4f  |Jb.G..200.@.>..O|
000003e0  4b 12 38 0a 36 0a 34 23  2f 64 65 66 69 6e 69 74  |K.8.6.4#/definit|
000003f0  69 6f 6e 73 2f 69 6f 2e  6b 38 73 2e 61 70 69 2e  |ions/io.k8s.api.|
00000400  63 6f 72 65 2e 76 31 2e  43 6f 6d 70 6f 6e 65 6e  |core.v1.Componen|
00000410  74 53 74 61 74 75 73 4c  69 73 74 0a 17 0a 03 34  |tStatusList....4|
00000420  30 31 12 10 0a 0e 0a 0c  55 6e 61 75 74 68 6f 72  |01......Unauthor|
00000430  69 7a 65 64 52 05 68 74  74 70 73 6a 1e 0a 13 78  |izedR.httpsj...x|
00000440  2d 6b 75 62 65 72 6e 65  74 65 73 2d 61 63 74 69  |-kubernetes-acti|
00000450  6f 6e 12 07 12 05 6c 69  73 74 0a 6a 51 0a 1f 78  |on....list.jQ..x|
00000460  2d 6b 75 62 65 72 6e 65  74 65 73 2d 67 72 6f 75  |-kubernetes-grou|
00000470  70 2d 76 65 72 73 69 6f  6e 2d 6b 69 6e 64 12 2e  |p-version-kind..|
00000480  12 2c 67 72 6f 75 70 3a  20 22 22 0a 6b 69 6e 64  |.,group: """".kind|
00000490  3a 20 43 6f 6d 70 6f 6e  65 6e 74 53 74 61 74 75  |: ComponentStatu|
000004a0  73 0a 76 65 72 73 69 6f  6e 3a 20 76 31 0a 4a a6  |s.version: v1.J.|
000004b0  06 0a a3 06 12 a0 06 1a  9d 06 12 05 71 75 65 72  |............quer|
000004c0  79 1a fe 05 54 68 65 20  63 6f 6e 74 69 6e 75 65  |y...The continue|
000004d0  20 6f 70 74 69 6f 6e 20  73 68 6f 75 6c 64 20 62  | option should b|
000004e0  65 20 73 65 74 20 77 68  65 6e 20 72 65 74 72 69  |e set when retri|
000004f0  65 76 69 6e 67 20 6d 6f  72 65 20 72 65 73 75 6c  |eving more resul|
00000500  74 73 20 66 72 6f 6d 20  74 68 65 20 73 65 72 76  |ts from the serv|
00000510  65 72 2e 20 53 69 6e 63  65 20 74 68 69 73 20 76  |er. Since this v|
00000520  61 6c 75 65 20 69 73 20  73 65 72 76 65 72 20 64  |alue is server d|
00000530  65 66 69 6e 65 64 2c 20  63 6c 69 65 6e 74 73 20  |efined, clients |
00000540  6d 61 79 20 6f 6e 6c 79  20 75 73 65 20 74 68 65  |may only use the|
00000550  20 63 6f 6e 74 69 6e 75  65 20 76 61 6c 75 65 20  | continue value |
00000560  66 72 6f 6d 20 61 20 70  72 65 76 69 6f 75 73 20  |from a previous |
00000570  71 75 65 72 79 20 72 65  73 75 6c 74 20 77 69 74  |query result wit|
00000580  68 20 69 64 65 6e 74 69  63 61 6c 20 71 75 65 72  |h identical quer|
00000590  79 20 70 61 72 61 6d 65  74 65 72 73 20 28 65 78  |y parameters (ex|
000005a0  63 65 70 74 20 66 6f 72  20 74 68 65 20 76 61 6c  |cept for the val|
000005b0  75 65 20 6f 66 20 63 6f  6e 74 69 6e 75 65 29 20  |ue of continue) |
000005c0  61 6e 64 20 74 68 65 20  73 65 72 76 65 72 20 6d  |and the server m|
000005d0  61 79 20 72 65 6a 65 63  74 20 61 20 63 6f 6e 74  |ay reject a cont|
000005e0  69 6e 75 65 20 76 61 6c  75 65 20 69 74 20 64 6f  |inue value it do|
000005f0  65 73 20 6e 6f 74 20 72  65 63 6f 67 6e 69 7a 65  |es not recognize|
00000600  2e 20 49 66 20 74 68 65  20 73 70 65 63 69 66 69  |. If the specifi|
00000610  65 64 20 63 6f 6e 74 69  6e 75 65 20 76 61 6c 75  |ed continue valu|
00000620  65 20 69 73 20 6e 6f 20  6c 6f 6e 67 65 72 20 76  |e is no longer v|
00000630  61 6c 69 64 20 77 68 65  74 68 65 72 20 64 75 65  |alid whether due|
00000640  20 74 6f 20 65 78 70 69  72 61 74 69 6f 6e 20 28  | to expiration (|
00000650  67 65 6e 65 72 61 6c 6c  79 20 66 69 76 65 20 74  |generally five t|
00000660  6f 20 66 69 66 74 65 65  6e 20 6d 69 6e 75 74 65  |o fifteen minute|
00000670  73 29 20 6f 72 20 61 20  63 6f 6e 66 69 67 75 72  |s) or a configur|
00000680  61 74 69 6f 6e 20 63 68  61 6e 67 65 20 6f 6e 20  |ation change on |
00000690  74 68 65 20 73 65 72 76  65 72 20 74 68 65 20 73  |the server the s|
000006a0  65 72 76 65 72 20 77 69  6c 6c 20 72 65 73 70 6f  |erver will respo|
000006b0  6e 64 20 77 69 74 68 20  61 20 34 31 30 20 52 65  |nd with a 410 Re|
000006c0  73 6f 75 72 63 65 45 78  70 69 72 65 64 20 65 72  |sourceExpired er|
000006d0  72 6f 72 20 69 6e 64 69  63 61 74 69 6e 67 20 74  |ror indicating t|
000006e0  68 65 20 63 6c 69 65 6e  74 20 6d 75 73 74 20 72  |he client must r|
000006f0  65 73 74 61 72 74 20 74  68 65 69 72 20 6c 69 73  |estart their lis|
00000700  74 20 77 69 74 68 6f 75  74 20 74 68 65 20 63 6f  |t without the co|
00000710  6e 74 69 6e 75 65 20 66  69 65 6c 64 2e 20 54 68  |ntinue field. Th|
00000720  69 73 20 66 69 65 6c 64  20 69 73 20 6e 6f 74 20  |is field is not |
00000730  73 75 70 70 6f 72 74 65  64 20 77 68 65 6e 20 77  |supported when w|
00000740  61 74 63 68 20 69 73 20  74 72 75 65 2e 20 43 6c  |atch is true. Cl|
00000750  69 65 6e 74 73 20 6d 61  79 20 73 74 61 72 74 20  |ients may start |
00000760  61 20 77 61 74 63 68 20  66 72 6f 6d 20 74 68 65  |a watch from the|
00000770  20 6c 61 73 74 20 72 65  73 6f 75 72 63 65 56 65  | last resourceVe|
00000780  72 73 69 6f 6e 20 76 61  6c 75 65 20 72 65 74 75  |rsion value retu|
00000790  72 6e 65 64 20 62 79 20  74 68 65 20 73 65 72 76  |rned by the serv|
000007a0  65 72 20 61 6e 64 20 6e  6f 74 20 6d 69 73 73 20  |er and not miss |
000007b0  61 6e 79 20 6d 6f 64 69  66 69 63 61 74 69 6f 6e  |any modification|
000007c0  73 2e 22 08 63 6f 6e 74  69 6e 75 65 32 06 73 74  |s."".continue2.st|
000007d0  72 69 6e 67 a0 01 01 4a  87 01 0a 84 01 12 81 01  |ring...J........|
000007e0  1a 7f 12 05 71 75 65 72  79 1a 5c 41 20 73 65 6c  |....query.\A sel|
000007f0  65 63 74 6f 72 20 74 6f  20 72 65 73 74 72 69 63  |ector to restric|
00000800  74 20 74 68 65 20 6c 69  73 74 20 6f 66 20 72 65  |t the list of re|
00000810  74 75 72 6e 65 64 20 6f  62 6a 65 63 74 73 20 62  |turned objects b|
00000820  79 20 74 68 65 69 72 20  66 69 65 6c 64 73 2e 20  |y their fields. |
00000830  44 65 66 61 75 6c 74 73  20 74 6f 20 65 76 65 72  |Defaults to ever|
00000840  79 74 68 69 6e 67 2e 22  0d 66 69 65 6c 64 53 65  |ything."".fieldSe|
00000850  6c 65 63 74 6f 72 32 06  73 74 72 69 6e 67 a0 01  |lector2.string..|
00000860  01 4a 77 0a 75 12 73 1a  71 12 05 71 75 65 72 79  |.Jw.u.s.q..query|
00000870  1a 46 49 66 20 74 72 75  65 2c 20 70 61 72 74 69  |.FIf true, parti|
00000880  61 6c 6c 79 20 69 6e 69  74 69 61 6c 69 7a 65 64  |ally initialized|
00000890  20 72 65 73 6f 75 72 63  65 73 20 61 72 65 20 69  | resources are i|
000008a0  6e 63 6c 75 64 65 64 20  69 6e 20 74 68 65 20 72  |ncluded in the r|
000008b0  65 73 70 6f 6e 73 65 2e  22 14 69 6e 63 6c 75 64  |esponse."".includ|
000008c0  65 55 6e 69 6e 69 74 69  61 6c 69 7a 65 64 32 07  |eUninitialized2.|
000008d0  62 6f 6f 6c 65 61 6e a0  01 01 4a 87 01 0a 84 01  |boolean...J.....|
000008e0  12 81 01 1a 7f 12 05 71  75 65 72 79 1a 5c 41 20  |.......query.\A |
000008f0  73 65 6c 65 63 74 6f 72  20 74 6f 20 72 65 73 74  |selector to rest|
00000900  72 69 63 74 20 74 68 65  20 6c 69 73 74 20 6f 66  |rict the list of|
00000910  20 72 65 74 75 72 6e 65  64 20 6f 62 6a 65 63 74  | returned object|
00000920  73 20 62 79 20 74 68 65  69 72 20 6c 61 62 65 6c  |s by their label|
00000930  73 2e 20 44 65 66 61 75  6c 74 73 20 74 6f 20 65  |s. Defaults to e|
00000940  76 65 72 79 74 68 69 6e  67 2e 22 0d 6c 61 62 65  |verything."".labe|
...
...
~115000 lines omitted here
...
001c1550  2e 52 6f 6c 65 52 65 66  22 3f 44 65 70 72 65 63  |.RoleRef""?Deprec|
001c1560  61 74 65 64 2e 20 50 6c  65 61 73 65 20 75 73 65  |ated. Please use|
001c1570  20 69 6f 2e 6b 38 73 2e  61 70 69 2e 72 62 61 63  | io.k8s.api.rbac|
001c1580  2e 76 31 62 65 74 61 31  2e 52 6f 6c 65 52 65 66  |.v1beta1.RoleRef|
001c1590  20 69 6e 73 74 65 61 64  2e 0a a3 01 0a 2f 69 6f  | instead...../io|
001c15a0  2e 6b 38 73 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |.k8s.kubernetes.|
001c15b0  70 6b 67 2e 61 70 69 73  2e 72 62 61 63 2e 76 31  |pkg.apis.rbac.v1|
001c15c0  62 65 74 61 31 2e 53 75  62 6a 65 63 74 12 70 0a  |beta1.Subject.p.|
001c15d0  2d 23 2f 64 65 66 69 6e  69 74 69 6f 6e 73 2f 69  |-#/definitions/i|
001c15e0  6f 2e 6b 38 73 2e 61 70  69 2e 72 62 61 63 2e 76  |o.k8s.api.rbac.v|
001c15f0  31 62 65 74 61 31 2e 53  75 62 6a 65 63 74 22 3f  |1beta1.Subject""?|
001c1600  44 65 70 72 65 63 61 74  65 64 2e 20 50 6c 65 61  |Deprecated. Plea|
001c1610  73 65 20 75 73 65 20 69  6f 2e 6b 38 73 2e 61 70  |se use io.k8s.ap|
001c1620  69 2e 72 62 61 63 2e 76  31 62 65 74 61 31 2e 53  |i.rbac.v1beta1.S|
001c1630  75 62 6a 65 63 74 20 69  6e 73 74 65 61 64 2e 0a  |ubject instead..|
001c1640  ac 01 0a 32 69 6f 2e 6b  38 73 2e 6b 75 62 65 72  |...2io.k8s.kuber|
001c1650  6e 65 74 65 73 2e 70 6b  67 2e 61 70 69 73 2e 73  |netes.pkg.apis.s|
001c1660  74 6f 72 61 67 65 2e 76  31 2e 53 74 6f 72 61 67  |torage.v1.Storag|
001c1670  65 43 6c 61 73 73 12 76  0a 30 23 2f 64 65 66 69  |eClass.v.0#/defi|
001c1680  6e 69 74 69 6f 6e 73 2f  69 6f 2e 6b 38 73 2e 61  |nitions/io.k8s.a|
001c1690  70 69 2e 73 74 6f 72 61  67 65 2e 76 31 2e 53 74  |pi.storage.v1.St|
001c16a0  6f 72 61 67 65 43 6c 61  73 73 22 42 44 65 70 72  |orageClass""BDepr|
001c16b0  65 63 61 74 65 64 2e 20  50 6c 65 61 73 65 20 75  |ecated. Please u|
001c16c0  73 65 20 69 6f 2e 6b 38  73 2e 61 70 69 2e 73 74  |se io.k8s.api.st|
001c16d0  6f 72 61 67 65 2e 76 31  2e 53 74 6f 72 61 67 65  |orage.v1.Storage|
001c16e0  43 6c 61 73 73 20 69 6e  73 74 65 61 64 2e 0a b8  |Class instead...|
001c16f0  01 0a 36 69 6f 2e 6b 38  73 2e 6b 75 62 65 72 6e  |..6io.k8s.kubern|
001c1700  65 74 65 73 2e 70 6b 67  2e 61 70 69 73 2e 73 74  |etes.pkg.apis.st|
001c1710  6f 72 61 67 65 2e 76 31  2e 53 74 6f 72 61 67 65  |orage.v1.Storage|
001c1720  43 6c 61 73 73 4c 69 73  74 12 7e 0a 34 23 2f 64  |ClassList.~.4#/d|
001c1730  65 66 69 6e 69 74 69 6f  6e 73 2f 69 6f 2e 6b 38  |efinitions/io.k8|
001c1740  73 2e 61 70 69 2e 73 74  6f 72 61 67 65 2e 76 31  |s.api.storage.v1|
001c1750  2e 53 74 6f 72 61 67 65  43 6c 61 73 73 4c 69 73  |.StorageClassLis|
001c1760  74 22 46 44 65 70 72 65  63 61 74 65 64 2e 20 50  |t""FDeprecated. P|
001c1770  6c 65 61 73 65 20 75 73  65 20 69 6f 2e 6b 38 73  |lease use io.k8s|
001c1780  2e 61 70 69 2e 73 74 6f  72 61 67 65 2e 76 31 2e  |.api.storage.v1.|
001c1790  53 74 6f 72 61 67 65 43  6c 61 73 73 4c 69 73 74  |StorageClassList|
001c17a0  20 69 6e 73 74 65 61 64  2e 0a bc 01 0a 37 69 6f  | instead.....7io|
001c17b0  2e 6b 38 73 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |.k8s.kubernetes.|
001c17c0  70 6b 67 2e 61 70 69 73  2e 73 74 6f 72 61 67 65  |pkg.apis.storage|
001c17d0  2e 76 31 62 65 74 61 31  2e 53 74 6f 72 61 67 65  |.v1beta1.Storage|
001c17e0  43 6c 61 73 73 12 80 01  0a 35 23 2f 64 65 66 69  |Class....5#/defi|
001c17f0  6e 69 74 69 6f 6e 73 2f  69 6f 2e 6b 38 73 2e 61  |nitions/io.k8s.a|
001c1800  70 69 2e 73 74 6f 72 61  67 65 2e 76 31 62 65 74  |pi.storage.v1bet|
001c1810  61 31 2e 53 74 6f 72 61  67 65 43 6c 61 73 73 22  |a1.StorageClass""|
001c1820  47 44 65 70 72 65 63 61  74 65 64 2e 20 50 6c 65  |GDeprecated. Ple|
001c1830  61 73 65 20 75 73 65 20  69 6f 2e 6b 38 73 2e 61  |ase use io.k8s.a|
001c1840  70 69 2e 73 74 6f 72 61  67 65 2e 76 31 62 65 74  |pi.storage.v1bet|
001c1850  61 31 2e 53 74 6f 72 61  67 65 43 6c 61 73 73 20  |a1.StorageClass |
001c1860  69 6e 73 74 65 61 64 2e  0a c8 01 0a 3b 69 6f 2e  |instead.....;io.|
001c1870  6b 38 73 2e 6b 75 62 65  72 6e 65 74 65 73 2e 70  |k8s.kubernetes.p|
001c1880  6b 67 2e 61 70 69 73 2e  73 74 6f 72 61 67 65 2e  |kg.apis.storage.|
001c1890  76 31 62 65 74 61 31 2e  53 74 6f 72 61 67 65 43  |v1beta1.StorageC|
001c18a0  6c 61 73 73 4c 69 73 74  12 88 01 0a 39 23 2f 64  |lassList....9#/d|
001c18b0  65 66 69 6e 69 74 69 6f  6e 73 2f 69 6f 2e 6b 38  |efinitions/io.k8|
001c18c0  73 2e 61 70 69 2e 73 74  6f 72 61 67 65 2e 76 31  |s.api.storage.v1|
001c18d0  62 65 74 61 31 2e 53 74  6f 72 61 67 65 43 6c 61  |beta1.StorageCla|
001c18e0  73 73 4c 69 73 74 22 4b  44 65 70 72 65 63 61 74  |ssList""KDeprecat|
001c18f0  65 64 2e 20 50 6c 65 61  73 65 20 75 73 65 20 69  |ed. Please use i|
001c1900  6f 2e 6b 38 73 2e 61 70  69 2e 73 74 6f 72 61 67  |o.k8s.api.storag|
001c1910  65 2e 76 31 62 65 74 61  31 2e 53 74 6f 72 61 67  |e.v1beta1.Storag|
001c1920  65 43 6c 61 73 73 4c 69  73 74 20 69 6e 73 74 65  |eClassList inste|
001c1930  61 64 2e 62 11 0a 0f 0a  0b 42 65 61 72 65 72 54  |ad.b.....BearerT|
001c1940  6f 6b 65 6e 12 00 62 0f  0a 0d 0a 09 48 54 54 50  |oken..b.....HTTP|
001c1950  42 61 73 69 63 12 00 6a  82 01 0a 4d 0a 0b 42 65  |Basic..j...M..Be|
001c1960  61 72 65 72 54 6f 6b 65  6e 12 3e 12 3c 0a 06 61  |arerToken.>.<..a|
001c1970  70 69 4b 65 79 12 0d 61  75 74 68 6f 72 69 7a 61  |piKey..authoriza|
001c1980  74 69 6f 6e 1a 06 68 65  61 64 65 72 22 1b 42 65  |tion..header"".Be|
001c1990  61 72 65 72 20 54 6f 6b  65 6e 20 61 75 74 68 65  |arer Token authe|
001c19a0  6e 74 69 63 61 74 69 6f  6e 0a 31 0a 09 48 54 54  |ntication.1..HTT|
001c19b0  50 42 61 73 69 63 12 24  0a 22 0a 05 62 61 73 69  |PBasic.$.""..basi|
001c19c0  63 12 19 48 54 54 50 20  42 61 73 69 63 20 61 75  |c..HTTP Basic au|
001c19d0  74 68 65 6e 74 69 63 61  74 69 6f 6e              |thentication|
NAME                 PROVISIONER            AGE
standard (default)   kubernetes.io/gce-pd   43d

```",closed,False,2018-02-20 22:46:44,2018-04-18 13:18:10
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/307,https://api.github.com/repos/kubernetes/kubectl/issues/307,initial implementation for file loader,,closed,True,2018-02-21 02:09:26,2018-02-21 18:00:34
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/308,https://api.github.com/repos/kubernetes/kubectl/issues/308,Move ManifestLoader under tree package,,closed,True,2018-02-21 18:49:45,2018-02-21 19:36:40
kubectl,droot,https://github.com/kubernetes/kubectl/pull/309,https://api.github.com/repos/kubernetes/kubectl/issues/309,removed '-u' option from install command,"'go get' complains if package exists and has been checked out with
different protocol (https/git). This results in tutorial integration
test failures. Disabling the option untill we have a solid solution.",closed,True,2018-02-21 19:40:22,2018-02-21 19:41:40
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/310,https://api.github.com/repos/kubernetes/kubectl/issues/310,update yaml import pkg,make all the yaml pkgs we use consist.,closed,True,2018-02-21 22:38:26,2018-03-02 02:22:58
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/311,https://api.github.com/repos/kubernetes/kubectl/issues/311,Use ManifestLoader to read and write the kubemanifest file,Use ManifestLoader to clean the TODO item in set_name_prefix.go,closed,True,2018-02-21 22:49:38,2018-02-21 22:52:40
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/312,https://api.github.com/repos/kubernetes/kubectl/issues/312,First pass at Application abstraction with unimplemented methods,Includes the first pass at the Resource interface,closed,True,2018-02-21 23:05:09,2018-02-21 23:08:40
kubectl,droot,https://github.com/kubernetes/kubectl/pull/313,https://api.github.com/repos/kubernetes/kubectl/issues/313,added configmap resource implementation,This is a very early idea of implementing resource for configmap objects just to get a handle on the shape of the solution.,closed,True,2018-02-22 03:59:28,2018-02-23 20:40:05
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/314,https://api.github.com/repos/kubernetes/kubectl/issues/314,Add appresource implementation,,closed,True,2018-02-22 23:29:38,2018-03-07 17:22:39
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/315,https://api.github.com/repos/kubernetes/kubectl/issues/315,Refactor Loader interface,"* The Loader interface adds New(root) -> Loader method to clone new Loaders
* The factory becomes an internal implementation of loaderImpl
* Removes fakefileloader, since we can just create a loader with a fake filesystem.
TODO: Implement better filepath calculation",closed,True,2018-02-22 23:41:57,2018-02-23 21:08:48
kubectl,droot,https://github.com/kubernetes/kubectl/pull/316,https://api.github.com/repos/kubernetes/kubectl/issues/316,configmap resource implementation using loader,,closed,True,2018-02-23 23:56:00,2018-02-26 19:53:45
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/317,https://api.github.com/repos/kubernetes/kubectl/issues/317,Refactor Loader interface,"* Adds Root() method to Loader interface
* Exposes SchemeLoader interface
* Renames RootLoader() to Init()
* Dynamically adds SchemeLoaders to root loader in Init()
* Removes private fileLoader tests",closed,True,2018-02-24 00:50:29,2018-02-26 23:26:05
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/318,https://api.github.com/repos/kubernetes/kubectl/issues/318,kinflate/transformers: Keep private things private,,closed,True,2018-02-26 17:25:08,2018-02-26 18:30:46
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/319,https://api.github.com/repos/kubernetes/kubectl/issues/319,Add secret resource,"Create Resource from one `SecretGenerator` object.
It reads the command, executes the command and use the output as the secret data. 
When executing the command, it need a path `p`, which is the root directory containing Kube-manifest.YAML file. With #317, we can call this function by `NewFromSecretGenerator(s, loader.Root())`",closed,True,2018-02-26 17:43:44,2018-03-07 17:22:38
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/320,https://api.github.com/repos/kubernetes/kubectl/issues/320,Change function name from ResourcesFromPath to NewFromPath,change function name to be consistent with `NewFromConfigmap` and `NewFromSecretGenerator`,closed,True,2018-02-26 19:44:50,2018-03-07 17:22:36
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/321,https://api.github.com/repos/kubernetes/kubectl/issues/321,change function name from ResourceFromPath to NewFromPath,,closed,True,2018-02-26 21:30:40,2018-02-26 21:38:47
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/322,https://api.github.com/repos/kubernetes/kubectl/issues/322,implement Resources func for applicationImpl,,closed,True,2018-02-26 21:52:25,2018-03-02 20:42:41
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/323,https://api.github.com/repos/kubernetes/kubectl/issues/323,Move fake loader into its own package; Update loader test,,closed,True,2018-02-26 23:45:43,2018-02-27 00:45:46
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/324,https://api.github.com/repos/kubernetes/kubectl/issues/324,Modify Encode function,,closed,True,2018-02-27 17:37:41,2018-03-07 17:22:19
kubectl,droot,https://github.com/kubernetes/kubectl/issues/325,https://api.github.com/repos/kubernetes/kubectl/issues/325,Define `app` package and move all Application related code in one place (5),This is to help with the refactoring kinflate codebase.,closed,False,2018-02-27 18:22:14,2018-02-27 18:22:40
kubectl,droot,https://github.com/kubernetes/kubectl/issues/326,https://api.github.com/repos/kubernetes/kubectl/issues/326,Implement Application's methods which gives us a good idea of shape of the solution from top-down (5),Implement application.Resources() method which implements the core functionality of inflate command.,closed,False,2018-02-27 18:23:52,2018-03-03 00:08:40
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/327,https://api.github.com/repos/kubernetes/kubectl/issues/327,Update FakeLoader for simpler interface; FakeLoader uses real Loader,,closed,True,2018-02-27 20:43:57,2018-02-27 21:30:48
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/328,https://api.github.com/repos/kubernetes/kubectl/issues/328,Simplified Loader.New() to always take a directory,Allow the FakeLoader to be initialized with a directory; not just a full file path.,closed,True,2018-02-27 23:31:42,2018-02-28 22:58:56
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/329,https://api.github.com/repos/kubernetes/kubectl/issues/329,Add nameHashTransformer,,closed,True,2018-02-27 23:49:08,2018-03-07 17:22:17
kubectl,ant31,https://github.com/kubernetes/kubectl/issues/330,https://api.github.com/repos/kubernetes/kubectl/issues/330,[kinflate] Override resources if redefined on overlays ,"**What happened**:
1, I created  'base-package' with a default configmap:
cat package/Kube-manifest.yaml
```
configmaps:
- name: app-env
  env: configmap/app.env
```

2. Created an overlay and modified the file app.env
```
configmaps:
- name: app-env
  env: configmap/new-app.env
```

3. Kinflate returned an error:
```
$ kinflate inflate -f . 
error: there is already an entry: ""v1_ConfigMap_app-env.yaml""
```
**What you expected to happen**:
I expected that the configmap 'redefined' in the overlay to override the parent one.

Beside configmap, I think that all resources declared in an overlay should not conflict with the parent one and just replace them.

",closed,False,2018-02-28 15:32:43,2018-05-18 21:21:12
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/331,https://api.github.com/repos/kubernetes/kubectl/issues/331,update resource package,This PR will be useful for implementing Resources func for applicationImpl in https://github.com/kubernetes/kubectl/pull/322.,closed,True,2018-03-01 01:27:24,2018-03-01 22:18:15
kubectl,joaocc,https://github.com/kubernetes/kubectl/issues/332,https://api.github.com/repos/kubernetes/kubectl/issues/332,Error message swapping arguments,"---
BUG REPORT

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-07T12:22:21Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.7"", GitCommit:""b30876a5539f09684ff9fde266fda10b37738c9c"", GitTreeState:""clean"", BuildDate:""2018-01-16T21:52:38Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**What happened**:
When a config map specifies ```[]``` instead of ```{}``` the returned message is:
```
error: error validating ""./ingress-controller-tcp-udp-map"": error validating data: ValidationError(ConfigMap.data): invalid type for io.k8s.api.core.v1.ConfigMap.data: got ""map"", expected ""array""; if you choose to ignore these errors, turn validation off with --validate=false
```

**What you expected to happen**:
The message should say ```got ""array"", expected ""map"";```

**How to reproduce it** (as minimally and precisely as possible):
```
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-controller-config-udp
  namespace: kube-system
data: []
```

**Anything else we need to know**:

",closed,False,2018-03-01 16:01:52,2018-03-16 22:14:46
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/333,https://api.github.com/repos/kubernetes/kubectl/issues/333,Change inflate to use Application interface,"The change is covered by existing tests.

Note:
We will have a different PR to remove the mode flag and add tests for `kinflate diff`.
The type `KObject` will be removed in the future.",closed,True,2018-03-02 20:56:58,2018-03-07 17:22:15
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/334,https://api.github.com/repos/kubernetes/kubectl/issues/334,Update inflate/diff and corresponding tests,"Current code uses a mode flag to switch between noop mode and normal mode when running inflate and diff. As a result, diff.go refers to some function in inflate.go. This change includes
- Rewrite `RunDiff` to call Application interface
- Remove the `mode` flag from both diff.go and inflate.go
- Add tests for `kinflate diff` using the same test data for `kinflate inflate`.",closed,True,2018-03-02 22:31:41,2018-03-07 17:22:15
kubectl,ewdurbin,https://github.com/kubernetes/kubectl/issues/335,https://api.github.com/repos/kubernetes/kubectl/issues/335,Inaccurate error message when cluster EOFs,"**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

- `EOF`
- `multiple kinds`
- `falling back to hardcoded types`

**Kubernetes version** (use `kubectl version`):

tested two version:

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.2"", GitCommit:""5fa2db2bd46ac79e5e00a4e6ed24191080aa463b"", GitTreeState:""clean"", BuildDate:""2018-01-18T21:12:46Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""darwin/amd64""}
Unable to connect to the server: EOF
```

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-09T21:51:54Z"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Unable to connect to the server: EOF
```

**Environment**:

I'm connecting from a `Darwin` host with Kernel version `17.4.0` to an AWS based CoreOS Tectonic Cluster via an empty (no healthy hosts yet) ELB with a TCP Listener.

**What happened**:

I attempted to talk to the cluster before it was ready and received a bewildering error message:

```
$ kubectl get all
error: {batch  cronjobs} matches multiple kinds [batch/v1beta1, Kind=CronJob batch/v2alpha1, Kind=CronJob]
```

Perhaps `cluster-info` would have been a wiser choice.

**What you expected to happen**:

Prescriptive error message such as ""failed to connect to <hostname>""

**How to reproduce it** (as minimally and precisely as possible):

Ask `kubectl` to talk to an AWS TCP ELB Listener which does not have any healthy hosts.


**Anything else we need to know**:

```
$ kubectl --v=100 get all 
I0302 21:29:26.811946    7470 loader.go:357] Config loaded from file /Users/ewdurbin/ernestd/pypi-infra/kubernetes/pypi_2018-03-02_10-21-59/generated/auth/kubeconfig
I0302 21:29:26.813585    7470 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.9.2 (darwin/amd64) kubernetes/5fa2db2"" https://k8s-api.cmh1.psf.io:443/api
I0302 21:29:26.861777    7470 round_trippers.go:436] GET https://k8s-api.cmh1.psf.io:443/api  in 48 milliseconds
I0302 21:29:26.861795    7470 round_trippers.go:442] Response Headers:
I0302 21:29:26.861848    7470 cached_discovery.go:124] skipped caching discovery info due to Get https://k8s-api.cmh1.psf.io:443/api: EOF
I0302 21:29:26.861950    7470 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.9.2 (darwin/amd64) kubernetes/5fa2db2"" https://k8s-api.cmh1.psf.io:443/api
I0302 21:29:26.909375    7470 round_trippers.go:436] GET https://k8s-api.cmh1.psf.io:443/api  in 47 milliseconds
I0302 21:29:26.909402    7470 round_trippers.go:442] Response Headers:
I0302 21:29:26.909428    7470 cached_discovery.go:124] skipped caching discovery info due to Get https://k8s-api.cmh1.psf.io:443/api: EOF
I0302 21:29:26.909741    7470 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.9.2 (darwin/amd64) kubernetes/5fa2db2"" https://k8s-api.cmh1.psf.io:443/api
I0302 21:29:26.956856    7470 round_trippers.go:436] GET https://k8s-api.cmh1.psf.io:443/api  in 47 milliseconds
I0302 21:29:26.956883    7470 round_trippers.go:442] Response Headers:
I0302 21:29:26.956921    7470 cached_discovery.go:124] skipped caching discovery info due to Get https://k8s-api.cmh1.psf.io:443/api: EOF
I0302 21:29:26.956937    7470 factory_object_mapping.go:93] Unable to retrieve API resources, falling back to hardcoded types: Get https://k8s-api.cmh1.psf.io:443/api: EOF
F0302 21:29:26.957292    7470 helpers.go:119] error: {batch  cronjobs} matches multiple kinds [batch/v1beta1, Kind=CronJob batch/v2alpha1, Kind=CronJob]
```

```
$ kubectl --v=100 get all 
I0302 21:31:27.344220    9388 loader.go:357] Config loaded from file /Users/ewdurbin/ernestd/pypi-infra/kubernetes/pypi_2018-03-02_10-21-59/generated/auth/kubeconfig
I0302 21:31:27.345723    9388 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.9.3 (darwin/amd64) kubernetes/d283541"" https://k8s-api.cmh1.psf.io:443/api
I0302 21:31:27.393570    9388 round_trippers.go:436] GET https://k8s-api.cmh1.psf.io:443/api  in 47 milliseconds
I0302 21:31:27.393591    9388 round_trippers.go:442] Response Headers:
I0302 21:31:27.393645    9388 cached_discovery.go:124] skipped caching discovery info due to Get https://k8s-api.cmh1.psf.io:443/api: EOF
I0302 21:31:27.393752    9388 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.9.3 (darwin/amd64) kubernetes/d283541"" https://k8s-api.cmh1.psf.io:443/api
I0302 21:31:27.442691    9388 round_trippers.go:436] GET https://k8s-api.cmh1.psf.io:443/api  in 48 milliseconds
I0302 21:31:27.442717    9388 round_trippers.go:442] Response Headers:
I0302 21:31:27.442756    9388 cached_discovery.go:124] skipped caching discovery info due to Get https://k8s-api.cmh1.psf.io:443/api: EOF
I0302 21:31:27.443052    9388 round_trippers.go:417] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.9.3 (darwin/amd64) kubernetes/d283541"" https://k8s-api.cmh1.psf.io:443/api
I0302 21:31:27.490758    9388 round_trippers.go:436] GET https://k8s-api.cmh1.psf.io:443/api  in 47 milliseconds
I0302 21:31:27.490775    9388 round_trippers.go:442] Response Headers:
I0302 21:31:27.490835    9388 cached_discovery.go:124] skipped caching discovery info due to Get https://k8s-api.cmh1.psf.io:443/api: EOF
I0302 21:31:27.490850    9388 factory_object_mapping.go:93] Unable to retrieve API resources, falling back to hardcoded types: Get https://k8s-api.cmh1.psf.io:443/api: EOF
F0302 21:31:27.491188    9388 helpers.go:119] error: {batch  cronjobs} matches multiple kinds [batch/v1beta1, Kind=CronJob batch/v2alpha1, Kind=CronJob]
```",closed,False,2018-03-03 02:39:27,2019-02-23 00:15:38
kubectl,nicolls1,https://github.com/kubernetes/kubectl/issues/336,https://api.github.com/repos/kubernetes/kubectl/issues/336,kubectl create secret with docker-registry creates a secret that can't be used,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): Bug

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T21:07:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:"""", Minor:"""", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2018-01-26T19:04:38Z"", GoVersion:""go1.9.1"", Compiler:""gc"", Platform:""linux/amd64""}


**Environment**:
- **Cloud provider or hardware configuration**:Minikube
- **OS** (e.g. from /etc/os-release):Ubuntu 16.04
- **Kernel** (e.g. `uname -a`):Linux barley 4.4.0-62-generic #83-Ubuntu SMP Wed Jan 18 14:10:15 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**:
- **Others**:


**What happened**: I followed the guild lines [here](https://kubernetes.io/docs/concepts/containers/images/#configuring-nodes-to-authenticate-to-a-private-repository) for accessing a private docker image repo. The command `kubectl create secret docker-registry myregistrykey ...` created a secret that looked like the following when printing in yaml format:
```
apiVersion: v1
data:
  .dockercfg: *******
kind: Secret
metadata:
  creationTimestamp: 2018-03-03T19:48:59Z
  name: regcred
  namespace: default
  resourceVersion: ""6564""
  selfLink: /api/v1/namespaces/default/secrets/regcred2
  uid: ea913199-1f1b-11e8-8166-080027d635ca
type: kubernetes.io/dockercfg
```

When i added regcred to my deployment imagePullSecret I was still getting pull errors. After looking around and trying a bunch of different things I finally got the following to work.
```
apiVersion: v1
kind: Secret
metadata:
  name: regcred
data:
  .dockerconfigjson: *****
type: kubernetes.io/dockerconfigjson
```

I used the exact same base64 string in both and the difference seems to be just the `type: kubernetes.io/dockerconfigjson` field where that is the one that works while the command uses the `type: kubernetes.io/dockercfg` type.

**What you expected to happen**: I expected the secret to be generated in the correct format


**How to reproduce it** (as minimally and precisely as possible): Run `kubectl create secret docker-registry myregistrykey ...`and see that generated secret uses `type: kubernetes.io/dockercfg`
",closed,False,2018-03-03 20:11:59,2018-04-14 14:47:22
kubectl,tamalsaha,https://github.com/kubernetes/kubectl/issues/337,https://api.github.com/repos/kubernetes/kubectl/issues/337,[plugin] kubectl plugin env variable is not incorrect,"I am experimenting with writing plugins for kubectl. One issue I found is that `KUBECTL_PLUGINS_GLOBAL_FLAG_AS_GROUP` flag is set to `[]` when this flag is not set.
```
KUBECTL_PLUGINS_GLOBAL_FLAG_AS_GROUP=[]
```
When I try to set this env value back via `pflag.Set()`, it is considered as `[]string{""[]""}`. .This causes `DefaultClientConfig(flags)` to throw error. You can find my demo code here: https://github.com/tamalsaha/kubectl-plugin-demo/blob/master/root.go#L79

The real problem seems to be that [`String()`](https://github.com/spf13/pflag/blob/master/string_array.go#L30) and [`Set(val string)`](https://github.com/spf13/pflag/blob/master/string_array.go#L16) methods of `stringArrayValue` in pflag library does not properly handle ""empty"" value for flags.",closed,False,2018-03-05 02:54:39,2018-09-25 22:16:14
kubectl,mengqiy,https://github.com/kubernetes/kubectl/issues/338,https://api.github.com/repos/kubernetes/kubectl/issues/338,Work with Barni on airflow (3),"Collect requirements, see what we can do for him.",closed,False,2018-03-05 23:14:29,2018-05-18 21:32:11
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/339,https://api.github.com/repos/kubernetes/kubectl/issues/339,Create an example-example (3),"Fix examples - we clear need ""bases"" and (distinct) clear ""end-user customizations"" (with correct sibling to base layouts).

The (non-empty) emptyDir things and git submodules aren't best practice examples (for today's definition of best practice layout in go/kinflate-glossary)",closed,False,2018-03-05 23:22:01,2018-03-07 20:04:17
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/340,https://api.github.com/repos/kubernetes/kubectl/issues/340,fall back to JSON merge patch for types w/o schema (CRD),,closed,True,2018-03-06 01:56:59,2018-03-06 18:42:04
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/341,https://api.github.com/repos/kubernetes/kubectl/issues/341,Add util functions to read/write manifest file and update sub commands,"This change removes all the dependency on package tree from sub commands add resource, add configmap, set nameprefix",closed,True,2018-03-06 22:33:06,2018-03-07 00:49:53
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/342,https://api.github.com/repos/kubernetes/kubectl/issues/342,Remove package tree by refactoring kinflate sub commands by using Application Interface or FileSystem(3),,closed,False,2018-03-06 22:36:09,2018-03-07 21:55:42
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/343,https://api.github.com/repos/kubernetes/kubectl/issues/343,Remove package tree,,closed,True,2018-03-07 17:05:54,2018-03-07 21:54:27
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/344,https://api.github.com/repos/kubernetes/kubectl/issues/344,appending name hash only applies to current layer,"@Liujingfang1 observed a bug the hash can be appended in multiple layers, e.g. `configmap-<hash1>-<hash2>`

The name hash transformer should be only applied to the configmap and secrets generated from the current layer.
",closed,True,2018-03-08 01:57:26,2018-03-09 04:35:20
kubectl,tamalsaha,https://github.com/kubernetes/kubectl/issues/345,https://api.github.com/repos/kubernetes/kubectl/issues/345,[plugin] Value must be assigned for boolean flags,"With spf13/pflag library, you can just specify a boolean flags as `--flagname` to set the flag value to `true`. When a boolean flag is used in a kubectl plugin, this results in an error. Setting the flag to `--flagname=true` works as expected.

This is what the error message looks like:

```
$ kubectl plugin kubectl-plugin-demo env --analytics
Error: flag needs an argument: --analytics
```
Example plugin: https://github.com/tamalsaha/kubectl-plugin-demo

/kind bug
",closed,False,2018-03-08 03:53:39,2018-09-25 22:11:58
kubectl,tamalsaha,https://github.com/kubernetes/kubectl/issues/346,https://api.github.com/repos/kubernetes/kubectl/issues/346,[plugin] Plugins have not way to resolve relative paths,"We are experimenting with writing kubectl  plugins. Currently one of the issues is that the plugin current directory is set to the `~/.kube/plugins/<name>` directory. This means that plugin can't resolve relative paths like `-f ./d1/d2/file.yaml`.

I think the current directory for the plugin should be set to the same current directory for `kubectl` so that plugins can resolve relative paths properly.

I tracked down the source of the issue here: 
- https://github.com/kubernetes/kubernetes/blob/a0844c17bfb964e3d1709784203e8404f790116c/pkg/kubectl/plugins/loader.go#L95
- https://github.com/kubernetes/kubernetes/blob/a0844c17bfb964e3d1709784203e8404f790116c/pkg/kubectl/cmd/plugin.go#L112

/kind bug",closed,False,2018-03-08 04:02:57,2018-08-11 06:24:22
kubectl,mengqiy,https://github.com/kubernetes/kubectl/issues/347,https://api.github.com/repos/kubernetes/kubectl/issues/347,Make update name reference and patching works when there are multiple layers (3),,closed,False,2018-03-08 23:04:30,2018-03-13 00:08:31
kubectl,jberkhahn,https://github.com/kubernetes/kubectl/pull/348,https://api.github.com/repos/kubernetes/kubectl/issues/348,Use passed-in context in pluginutils config initializer,"This will grab the context out of the config file if one is specified by the KUBECTL_GLOBAL_FLAG_CONTEXT passed in from the plugin framework.

@carolynvs might want to take a look at this.",closed,True,2018-03-09 00:02:47,2018-03-22 21:45:03
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/349,https://api.github.com/repos/kubernetes/kubectl/issues/349,Rename KObject to ResourceCollection,There will be another PR to change the related function signature to return `ResourceCollection` instead of `[]*Resource`.,closed,True,2018-03-10 00:37:41,2018-03-10 00:51:46
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/350,https://api.github.com/repos/kubernetes/kubectl/issues/350,cleanup Resouce slice related code,"Change function signature to return ResourceCollection instead of []*Resource.
Cleanup tests and embed test data according the tott article `remove logic from the test`.
Moved ResourceCollection to resource pkg.

Most of this PR is mechanical change, nothing very interesting.",closed,True,2018-03-12 19:26:45,2018-03-12 22:12:08
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/351,https://api.github.com/repos/kubernetes/kubectl/issues/351,Add testing script to invoke example testing,,closed,True,2018-03-12 22:49:39,2018-03-13 17:19:11
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/352,https://api.github.com/repos/kubernetes/kubectl/issues/352,fix bug when update name ref in multi layer setup,https://github.com/kubernetes/kubectl/pull/349 and https://github.com/kubernetes/kubectl/pull/350 are preparation for this PR. So this PR can be write in a cleaner way.,closed,True,2018-03-12 22:53:55,2018-03-13 00:08:57
kubectl,feiskyer,https://github.com/kubernetes/kubectl/issues/353,https://api.github.com/repos/kubernetes/kubectl/issues/353,kubectl complete error: command not found: __debug,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

When using kubectl with autocomplete, e.g. `kubectl create -f nginx <TAB>`, then there is `command not found: __debug` error reported:

```sh
kubectl create -f ng__kubectl_filedir:3: command not found: __debug
__kubectl_filedir:20: command not found: __debug
inx
```

kubectl version: v1.10.0-alpha.3.1792+891b471064ddc1.

**What you expected to happen**:

kubectl should autocomplete with no errors.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",closed,False,2018-03-14 05:42:15,2018-03-15 01:48:06
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/354,https://api.github.com/repos/kubernetes/kubectl/issues/354,unstructpath: Create json path type package.,"This package let's you find specific path in your unstruct types, kind
of like jsonpath does. But this is statically compile.

This replaces #258 

It doesn't have the controversial types anymore.",closed,True,2018-03-14 17:12:45,2018-03-16 20:53:25
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/355,https://api.github.com/repos/kubernetes/kubectl/issues/355,Update main.sh to exit non-zero when there is a failure,,closed,True,2018-03-16 20:19:15,2018-03-16 20:30:57
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/356,https://api.github.com/repos/kubernetes/kubectl/issues/356,Update directories in test script,"Assume there is only one example directory we want to test. If we can find `tests/test.sh` inside that directory, we run the test against that dir.

Also remove lines `cd <dir>`",closed,True,2018-03-16 22:06:35,2018-03-16 22:40:27
kubectl,m4r10k,https://github.com/kubernetes/kubectl/issues/357,https://api.github.com/repos/kubernetes/kubectl/issues/357,role missing in kubectl get command line completion,"**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):  completion

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-07T12:22:21Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-07T11:55:20Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: on-prem
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04.4
- **Kernel** (e.g. `uname -a`):
- **Install tools**: kubeadm

**What happened**:
The command line completion of `kubectl get` does not show `role`. It shows only `rolebinding`.
```
 kubectl get 
certificatesigningrequest  endpoints                  persistentvolume           rolebinding
clusterrolebinding         event                      persistentvolumeclaim      secret
componentstatus            horizontalpodautoscaler    pod                        service
configmap                  ingress                    poddisruptionbudget        serviceaccount
controllerrevision         job                        podsecuritypolicy          statefulset
cronjob                    namespace                  podtemplate                status
daemonset                  networkpolicy              replicaset                 storageclass
deployment                 node                       replicationcontroller      

```

**What you expected to happen**:
`kubectl get` command line completion should also show `role` because `getting` a role is possible:
```
kubectl get role pod-reader-default-role
NAME                      AGE
pod-reader-default-role   13m
```

**How to reproduce it** (as minimally and precisely as possible):
`kubectl get [tab,tab]` 
",closed,False,2018-03-18 08:03:11,2018-05-04 17:08:53
kubectl,a8uhnf,https://github.com/kubernetes/kubectl/pull/358,https://api.github.com/repos/kubernetes/kubectl/issues/358,Fix label test in documentation appropriately,,closed,True,2018-03-19 04:55:44,2018-12-17 07:14:11
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/359,https://api.github.com/repos/kubernetes/kubectl/issues/359,unstructpath: Move predicates to their own package,"Move the predicates to a ""predicates"" package, to simplify the
structure. This is purely mechanical change. Better documentation for
the package will come next, as well as a few other changes.",closed,True,2018-03-19 16:21:08,2018-03-19 16:54:02
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/360,https://api.github.com/repos/kubernetes/kubectl/issues/360,predicates: Rename Value into Interface,"Since the original ""Value"" class is gone, it doesn't make sense anymore
to refer to that name. Now it's simply an ""interface{}"", so the name
Interface is more suited.",closed,True,2018-03-19 17:16:57,2018-03-19 21:45:01
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/361,https://api.github.com/repos/kubernetes/kubectl/issues/361,kubectl create clusterrolebinding: must error if --namespace is specified,"Users can easily confuse

```
kubectl create rolebinding serviceaccounts-view \
  --clusterrole=view \
  --group=system:serviceaccounts:my-namespace \
  --namespace=my-namespace
```

and

```
kubectl create clusterrolebinding serviceaccounts-view \
  --clusterrole=view \
  --group=system:serviceaccounts:my-namespace \
  --namespace=my-namespace
```

In latter command, the `--namespace` argument is SILENTLY IGNORED because clusterrolebinding is not a namespaced resource.

In such cases (silently ignoring something) we should be explicit and if `--namespace` is specified, `kubectl create clusterrolebinding` should error out!",closed,False,2018-03-19 20:15:09,2018-06-20 17:18:59
kubectl,apelisse,https://github.com/kubernetes/kubectl/pull/362,https://api.github.com/repos/kubernetes/kubectl/issues/362,"selectors: Remove ""Map"" and ""Slice""","Transforming from Interface to Map or Slice in order to get a sub-item
was cumbersome, so now you can only get children directly from the
interface.
    
Maps and Slices are at the same level as Number or string (you can only
filter or get). Also rename Map/Slice/Number/String to
AsMap/AsSlice/AsNumber/AsString to remove ambiguity.

Also, move predicates and unstructpath to path

And rename unstructpath to selectors, since the package only contains
selectors now. The name of types in the selectors package could be
improved now that the package name is more specific.

Also, remove the suffix S from selectors

Since the package is now called ""selectors"", the S at the end of each
type name is no longer needed.",closed,True,2018-03-20 22:17:48,2018-03-20 22:40:02
kubectl,apelisse,https://github.com/kubernetes/kubectl/issues/363,https://api.github.com/repos/kubernetes/kubectl/issues/363,Path framework,"The goal of this issue is to track some of the progress made to the [path framework](https://github.com/kubernetes/kubectl/tree/master/pkg/framework/path), and what we'd want to do. Feel free to update the following list:

- [ ] Improve the documentation for the ""predicates"" package. Explain what predicates are, and why they exist.
- [ ] Improve the documentation for ""selectors"" package. Explain what selectors are, how they work, how they also are predicates,
- [ ] Find a smart way to pass values to selector at the beginning rather than the end: e.g. `From(yaml).....Select()` instead of `....SelectFrom(yaml)`
- [ ] Add selectors for the OpenAPI (type specific)
- [ ] Associate type with value (openstruct?)
- [ ] Add selectors for openstruct

I'm probably missing many things.",closed,False,2018-03-20 23:29:49,2019-02-23 00:15:39
kubectl,grodrigues3,https://github.com/kubernetes/kubectl/issues/364,https://api.github.com/repos/kubernetes/kubectl/issues/364,Explore the solution generated service names.  Come up (list of approaches) that we can run through.  Brainstorm as a group (3),/assign @droot,closed,False,2018-03-21 22:36:52,2018-05-18 21:15:46
kubectl,grodrigues3,https://github.com/kubernetes/kubectl/issues/365,https://api.github.com/repos/kubernetes/kubectl/issues/365,PoC for best solution for generated service names (3).,/assign @droot,closed,False,2018-03-21 22:37:26,2018-05-18 21:13:09
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/366,https://api.github.com/repos/kubernetes/kubectl/issues/366,Integrate openapi,"Pin all the version in vendor to the same version as k/k



",closed,True,2018-03-22 00:22:45,2018-07-31 18:37:56
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/367,https://api.github.com/repos/kubernetes/kubectl/issues/367,change packages to bases in Manifest,,closed,True,2018-03-22 16:33:50,2018-03-26 23:14:02
kubectl,droot,https://github.com/kubernetes/kubectl/pull/368,https://api.github.com/repos/kubernetes/kubectl/issues/368,updated namereference config for missing references.,"highlights:
RBAC references
ServiceAccountName reference
StatefulSet headless service reference",closed,True,2018-03-22 18:30:28,2018-04-11 20:34:56
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/369,https://api.github.com/repos/kubernetes/kubectl/issues/369,"in manifest, [packages](https://github.com/kubernetes/kubectl/blob/master/pkg/apis/manifest/v1alpha1/types.go#L88) field becomes ""bases"" (3)","kinflate is not about packaging.  

this was a point of confusion in the k8s eng review.",closed,False,2018-03-22 20:34:25,2018-05-18 21:12:24
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/370,https://api.github.com/repos/kubernetes/kubectl/issues/370,support multiple patches for the same GVKN,"Before: multiple patches against the same GVKN is not supported, because loaded patches are stored in the ResourceCollection which is indexed by GVKN. So it will complain if there are multiple patches for one GVKN.

After: loaded patches are stored in a slice of Resource.
",closed,True,2018-03-22 21:10:33,2018-03-23 16:11:58
kubectl,Nowaker,https://github.com/kubernetes/kubectl/issues/371,https://api.github.com/repos/kubernetes/kubectl/issues/371,"Support multi-container pod for ""kubectl logs"" without ""-c""","**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FEATURE REQUEST

`kubectl logs -l` will print logs for pods with the same label, however it doesn't support printing logs from pods with multiple containers at once, without `-c` flag.

```
+ kubectl logs -l release=review-feature-su-ja9rfo --tail 200 --timestamps
Error from server (BadRequest): a container name must be specified for pod review-feature-su-ja9rfo-lde-appserver-app-5656d48fbf-rph9t, choose one of: [lde-appserver-api2-signup-8080 lde-appserver-dhwebpanel-8081]
```

There is a PR for this feature but the original author abandoned it: https://github.com/kubernetes/kubernetes/pull/45275.

That being said, this feature request is legit and deserves a dedicated issue.



**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.6"", GitCommit:""9f8ebd171479bec0ada837d7ee641dec2f8c6dd1"", GitTreeState:""clean"", BuildDate:""2018-03-21T15:21:50Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**: not relevant",closed,False,2018-03-23 14:09:41,2018-03-30 18:56:20
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/372,https://api.github.com/repos/kubernetes/kubectl/issues/372,update vendor apimachinery,Pick up some change in SMP.,closed,True,2018-03-23 17:20:54,2018-03-23 18:30:37
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/373,https://api.github.com/repos/kubernetes/kubectl/issues/373,Add different behavior for configmaps in overlay,"In our current kindflate code, we don't support redefining configmap or secrets in overlays. If we want to change a configmap in the base, we are only allowed to apply a patch file containing the new configmap content.

This change is to enable us to redefine configmap in overlays. For example, in `base/Kube-manifest.yaml`, we define
```configMaps:
- name: demo-configmap
  files:
    - application.properties
```
In `instance/staging/Kube-manifest.yaml`, if we define it as
```configMaps:
- name: demo-configmap
  behavior: replace
  files:
    - staging-application.properties
```
Then `staging-application.properties` is going to be used instead of `application.properties`.

In `instance/production/Kube-manifest.yaml`, if we define it as
```configMaps:
- name: demo-configmap
  behavior: merge
  files:
    - production-env.properties
```
Then both `application.properties` and `production-env.properties` are used ",closed,True,2018-03-23 21:17:40,2018-04-03 22:38:08
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/374,https://api.github.com/repos/kubernetes/kubectl/issues/374,Kinflate should allow configMap overlay.,"Considering the base and instances for spring-boot example: https://github.com/kinflate/spring-boot-helloworld
The base defines a configmap from file application.properties.
The staging instance wants to use the same configmap but with a different application.properties file.
Currently kinflate inflate gives the error as:
`there is already an entry: ""v1_ConfigMap_demo-configmap.yaml""`",closed,False,2018-03-23 21:21:07,2018-05-18 21:12:09
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/375,https://api.github.com/repos/kubernetes/kubectl/issues/375,Conflict detection in kinflate,"Support conflict detection for patches in the same Kube-manifest.yaml.

This depends on https://github.com/kubernetes/kubernetes/pull/61905.
",closed,True,2018-03-23 22:21:50,2018-04-09 22:43:05
kubectl,Lion-Wei,https://github.com/kubernetes/kubectl/issues/376,https://api.github.com/repos/kubernetes/kubectl/issues/376,Extend kubectl describe for networkPolicy,"
**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
/kind feature

**What happened**:
Since [Allow including both podSelector and namespaceSelector in a NetworkPolicyPeer](https://github.com/kubernetes/kubernetes/pull/60452) got merged, we need extend `kubectl describe` to describe policies where a NetworkPolicyPeer has both podSelector and namespaceSelector set.


",closed,False,2018-03-26 04:05:51,2018-04-19 02:36:11
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/377,https://api.github.com/repos/kubernetes/kubectl/issues/377,detect unknown fields in Kube-manifest.yaml,This PR depends https://github.com/kinflate/tuthello/pull/2.,closed,True,2018-03-27 00:54:10,2018-03-28 19:37:58
kubectl,Lion-Wei,https://github.com/kubernetes/kubectl/issues/378,https://api.github.com/repos/kubernetes/kubectl/issues/378,kubectl reject annotation information without any warning,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
/kind bug

**Kubernetes version** (use `kubectl version`):
master

**What happened**:
If we use `k create -f <file name> --save-config=true` or `kubectl apply <file name>`, and this file have annotations with type not `map[string]string`, then kubectl will quietly reject those annotations.

For example, you have a yaml described object wich annotation is: 
```yaml
metadata:
  name: test
  annotations:
    a: haha
    b: 1
```
We can see `b: 1` is not a correct annotation type. Then you use `kubectl apply` or `kubectl create --save-config` create this object, you will get an object with annotations like:
```yaml
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiVersion"":""v1"",""kind"":""ConfigMap"",""metadata"":{""annotations"":{},""name"":""test"",""namespace"":""default""}}
```

And if you already have this object with annotations, after the `kubectl apply` command, the annotation will be cleared.


**What you expected to happen**:
In this case, kubectl should error out, with an err `invalid annotation` or something else.

**How to reproduce it** (as minimally and precisely as possible):
1. Create a file with content like:
```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: test
  annotations:
    a: haha
    b: 1
```
2. Use `k create -f <file name> --save-config=true` or `kubectl apply <file name>` to create this object.
3. Check the new created object annotations.

**Anything else we need to know**:

",closed,False,2018-03-27 12:10:21,2018-08-25 08:57:38
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/379,https://api.github.com/repos/kubernetes/kubectl/issues/379,A new hello-world demo.,"This demo uses an existing server and existing base config, both of which have their own documentation -
making this demo cleaner and shorter.
",closed,True,2018-03-27 20:20:59,2018-04-06 21:27:38
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/380,https://api.github.com/repos/kubernetes/kubectl/issues/380,newshortdemo,"Tighten the home page and the mysql example.
",closed,True,2018-03-27 23:21:56,2018-04-06 21:27:37
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/381,https://api.github.com/repos/kubernetes/kubectl/issues/381,renameDemos,file names now give a better hint as to purpose,closed,True,2018-03-28 00:10:29,2018-04-06 21:27:36
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/382,https://api.github.com/repos/kubernetes/kubectl/issues/382,combine demos into a subdir,,closed,True,2018-03-28 20:11:29,2018-04-06 21:27:29
kubectl,jberkhahn,https://github.com/kubernetes/kubectl/pull/383,https://api.github.com/repos/kubernetes/kubectl/issues/383,Return rest config and regular config  from pluginutil Init method,"-add namespace overriding based on kubectl plugin env var

We added namespace overriding in svcat's standalone mode, so we need to be able to override it in plugin mode as well. I figured while I was in here, I might as well make the init method return the whole config, in case we need to pull anything off it in the future.

Thanks!
",closed,True,2018-03-28 21:13:57,2018-04-03 18:36:08
kubectl,carlosmkb,https://github.com/kubernetes/kubectl/issues/384,https://api.github.com/repos/kubernetes/kubectl/issues/384,"kubectl ""--dry-run"" with ""-o yaml"" does not show 'apiVersion' neither 'kind'","**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`): 1.10.0


**Environment**:
- **Cloud provider or hardware configuration**: baremetal
- **OS** (e.g. from /etc/os-release): centos 7
- **Kernel** (e.g. `uname -a`): 3.10.0-693.17.1.el7.x86_64
- **Install tools**: kubeadm
- **Others**:


**What happened**:
the output of `kubectl create namespace my-namespace -o yaml --dry-run` does not show 'apiVersion' neither 'kind' as previous versions.
`kubectl create namespace my-namespace -o yaml --dry-run`
Output:
**metadata:
  creationTimestamp: null
  name: my-namespace
spec: {}
status: {}**

**What you expected to happen**:
expected the output of previous versions, like:
`$ kubectl create namespace my-namespace -o yaml --dry-run`
Output:
**apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: my-namespace
spec: {}
status: {}**

I use the ""--dry-run"" and ""-o yaml"" to execute `kubectl create namespace my-namespace -o yaml --dry-run | kubectl apply -f -`

",closed,False,2018-03-29 14:56:23,2018-04-05 05:40:32
kubectl,ihoegen,https://github.com/kubernetes/kubectl/issues/385,https://api.github.com/repos/kubernetes/kubectl/issues/385,Kubectl 1.10.0 is unable to create deployments on clusters running 1.8.7 ,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

/kind bug

**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-27T00:13:02Z"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.7"", GitCommit:""b30876a5539f09684ff9fde266fda10b37738c9c"", GitTreeState:""clean"", BuildDate:""2018-01-16T21:52:38Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: AWS
- **OS**: macOS 10.13.3
- **Kernel**: Darwin 17.4.0
- **Install tools**: brew


**What happened**:

When creating a sample deployment using the command `kubectl create -f deployment.yaml`, the following output appears:

`Error from server (NotAcceptable): unknown`

**What you expected to happen**:

When running the same command again, `kubectl create -f deployment.yaml`, with kubectl version 1.9.3, the following output appears:

`deployment ""nginx-deployment"" created`

A better error message describing incompatibility might be useful.

**How to reproduce it** (as minimally and precisely as possible):

1. Use the following config, and save it as `deployment.yaml`:


``` yaml
apiVersion: apps/v1beta2 
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template: 
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```

2. Run `kubectl create -f deployment.yaml` using kubectl 1.10.0

",closed,False,2018-03-29 19:21:39,2018-07-04 13:48:12
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/386,https://api.github.com/repos/kubernetes/kubectl/issues/386,kubectl config view: should error out on positional argument,"/sig cli
/kind bug

`kubectl config view` does not complain when I give it a positional argument.

Apparently this command only uses the current-context set.

So I've been confused for 5 minutes straight why 

    kubectl config view A
    kubectl config view B

are returning the same thing.

Please make this command exit with ERROR message/code when someone passes an unused/silently-ignored positional argument.
",closed,False,2018-03-30 21:32:59,2018-12-25 11:12:01
kubectl,droot,https://github.com/kubernetes/kubectl/pull/387,https://api.github.com/repos/kubernetes/kubectl/issues/387,integrated glog in command structure,,closed,True,2018-03-30 23:06:46,2018-03-30 23:21:04
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/388,https://api.github.com/repos/kubernetes/kubectl/issues/388,Add glossary,"and pointers to it from the main README.

Need a place to define terminology.

This is a conversion and update of the content at go/kinflate-glossary

At the last minute, i pulled workflows out of the glossary, to make it a freestanding MD file, to which we can figures.  Perhaps this stuff goes into the main README, but trying to keep that short.

",closed,True,2018-04-02 19:00:34,2018-04-06 21:27:27
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/389,https://api.github.com/repos/kubernetes/kubectl/issues/389,add workflow figs,,closed,True,2018-04-02 23:16:41,2018-04-06 21:27:27
kubectl,droot,https://github.com/kubernetes/kubectl/pull/390,https://api.github.com/repos/kubernetes/kubectl/issues/390,kinflate: added version command,add version package. Will be using it in container builder to embed version in all the releases. It will help us during debugging any external issues.,closed,True,2018-04-03 00:10:27,2018-04-03 02:42:05
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/391,https://api.github.com/repos/kubernetes/kubectl/issues/391,"ASUI want understanding of the ""secret"" field in manifest.","Rename with sufffix Generator, i.e. configmapgenerator, secretgenerator.",closed,False,2018-04-03 19:16:42,2018-05-18 21:11:38
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/392,https://api.github.com/repos/kubernetes/kubectl/issues/392,AAUI expect secret and configmap generators to have same behavior options,"provide  add vs replace for secret.  

is there a reasonable ""merge"" option - as one has with configmap?",closed,False,2018-04-03 19:17:29,2018-05-18 21:11:19
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/393,https://api.github.com/repos/kubernetes/kubectl/issues/393,Add different behavior for secretGenerator in overlay,"Add `Behavior` to `SecretGenerator`
Add test in `kinflate inflate` and `kinflate diff`

Since the logic is already handled in #373 , when we add `Behavior` for SecretGenerator, it automatically works.",closed,True,2018-04-03 22:58:32,2018-04-03 23:32:07
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/394,https://api.github.com/repos/kubernetes/kubectl/issues/394,change ConfigMap to ConfigMapGenerator,,closed,True,2018-04-03 23:19:08,2018-04-04 22:21:10
kubectl,droot,https://github.com/kubernetes/kubectl/pull/395,https://api.github.com/repos/kubernetes/kubectl/issues/395,kinflate: add build automation,"Highlights:
- Adds a build script to generate the archive for Kinflate
- Adds configs for Google Container Builder to generate the build locally or in Google container builder

I have tested the build triggers with my fork.",closed,True,2018-04-03 23:51:47,2018-04-04 17:44:09
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/396,https://api.github.com/repos/kubernetes/kubectl/issues/396,kubectl config: no commands to manage users,"kubectl v1.10.0
/kind feature
/sig cli

`kubectl config` command lets users create/delete/update ""context""s. Contexts are composed of ""cluster""s and ""user""s. Example:

```yaml
contexts:
- name: my-context
  context:
    cluster: my-cluster-name
    user: my-user
```

`kubectl config` lets you create/update/delete ""cluster""s, but IT DOESN'T let you manage ""user""s:

```
Available Commands:
  current-context Displays the current-context
  delete-cluster  Delete the specified cluster from the kubeconfig
  delete-context  Delete the specified context from the kubeconfig
  get-clusters    Display clusters defined in the kubeconfig
  get-contexts    Describe one or many contexts
  rename-context  Renames a context from the kubeconfig file.
  set             Sets an individual value in a kubeconfig file
  set-cluster     Sets a cluster entry in kubeconfig
  set-context     Sets a context entry in kubeconfig
  set-credentials Sets a user entry in kubeconfig
  unset           Unsets an individual value in a kubeconfig file
  use-context     Sets the current-context in a kubeconfig file
  view            Display merged kubeconfig settings or a specified kubeconfig file
```

Why is that?

I think `*-cluster` commands listed above should also exist for `*-user`.",open,False,2018-04-04 00:47:15,2019-02-12 07:57:48
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/397,https://api.github.com/repos/kubernetes/kubectl/issues/397,kubectl config set: parameter name split with space in --help,"kubectl v1.10.0
/kind bug
/sig cli

See the space in `PROPERTY _NAME` and `PROPERTY _VALUE` occurrences below:

```
$ kubectl config set --help
Sets an individual value in a kubeconfig file

PROPERTY _NAME is a dot delimited name where each token represents either an attribute name or a map key.  Map keys may
not contain dots.

PROPERTY _VALUE is the new value you wish to set. Binary fields such as 'certificate-authority-data' expect a base64
encoded string unless the --set-raw-bytes flag is used.

Options:
      --set-raw-bytes=false: When writing a []byte PROPERTY_VALUE, write the given string directly without base64
decoding.

Usage:
  kubectl config set PROPERTY_NAME PROPERTY_VALUE [options]
```",closed,False,2018-04-04 00:50:37,2019-02-23 00:15:39
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/398,https://api.github.com/repos/kubernetes/kubectl/issues/398,kubectl config set: please add some examples,"kubectl v1.10
/kind documentation
/sig cli

This command's --help message is not useful at all. Can we please have some commonly used tasks listed in EXAMPLES section of the --help message?

```
$ kubectl config set --help
Sets an individual value in a kubeconfig file

PROPERTY _NAME is a dot delimited name where each token represents either an attribute name or a map key.  Map keys may
not contain dots.

PROPERTY _VALUE is the new value you wish to set. Binary fields such as 'certificate-authority-data' expect a base64
encoded string unless the --set-raw-bytes flag is used.

Options:
      --set-raw-bytes=false: When writing a []byte PROPERTY_VALUE, write the given string directly without base64
decoding.

Usage:
  kubectl config set PROPERTY_NAME PROPERTY_VALUE [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```",closed,False,2018-04-04 01:06:44,2019-01-25 14:41:32
kubectl,costinm,https://github.com/kubernetes/kubectl/issues/399,https://api.github.com/repos/kubernetes/kubectl/issues/399,"Kubectl 1.10 ""create secret"" missing kind and apiversion","
**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

""create secret""

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

Bug

**Kubernetes version** (use `kubectl version`):

1.10

**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:

``` 
kubectl1.10 create secret generic foo --from-file=a.pem=... --dry-run -o yaml
data:
  a.pem: RGViaWFuIEdOVS9MaW51eCBidXN0ZXIvc2lkCg==
metadata:
  creationTimestamp: null
  name: foo
```


**What you expected to happen**:

```
kubectl1.9 create secret generic foo --from-file=a.pem=... --dry-run -o yaml
apiVersion: v1
data:
  a.pem: RGViaWFuIEdOVS9MaW51eCBidXN0ZXIvc2lkCg==
kind: Secret
metadata:
  creationTimestamp: null
  name: foo
```

**How to reproduce it** (as minimally and precisely as possible):

Run the above commands with kubectl 1.9 and 1.10, notice the difference in result.


**Anything else we need to know**:

Istio 0.7 was using the command as part of one of install scripts. We will stop using
the script 0.8, recommend users stick with kubectl 1.9 until istio 0.8 is released. ",closed,False,2018-04-04 15:27:01,2018-04-14 14:33:46
kubectl,djschny,https://github.com/kubernetes/kubectl/issues/400,https://api.github.com/repos/kubernetes/kubectl/issues/400,SIGSEGV - panic when running busybox,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): sigsegv, panic

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:55:54Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.15"", GitCommit:""8c7c1c8b0ce0866d5ded2ce4fa402a716ce6bb6c"", GitTreeState:""clean"", BuildDate:""2018-03-19T14:15:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```


**Environment**:
- **Cloud provider or hardware configuration**: Google Cloud
- **OS** (e.g. from /etc/os-release):
```
NAME=""Ubuntu""
VERSION=""16.04.4 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.4 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
```
- **Kernel** (e.g. `uname -a`): `Linux lab-client 4.13.0-1011-gcp #15-Ubuntu SMP Mon Feb 12 16:29:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux`
- **Install tools**:
- **Others**:


**What happened**:
```
$ kubectl run busybox --rm -ti --image=busybox /bin/sh
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x140c46e]

goroutine 1 [running]:
k8s.io/kubernetes/pkg/kubectl/cmd.RunRun(0x2344c80, 0xc42069b4a0, 0x231a080, 0xc42000c010, 0x231a0c0, 0xc42000c018, 0x231a0c0, 0xc42000c020, 0xc4204b5180, 0xc4205e1590, ...)
	/workspace/anago-v1.10.0-rc.1.9+fc32d2f3698e36/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/run.go:328 +0x10ae
k8s.io/kubernetes/pkg/kubectl/cmd.NewCmdRun.func1(0xc4204b5180, 0xc4205e1590, 0x2, 0x5)
	/workspace/anago-v1.10.0-rc.1.9+fc32d2f3698e36/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/run.go:105 +0x144
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute(0xc4204b5180, 0xc4205e1540, 0x5, 0x5, 0xc4204b5180, 0xc4205e1540)
	/workspace/anago-v1.10.0-rc.1.9+fc32d2f3698e36/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:757 +0x2c1
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0xc420468f00, 0xc4208efed8, 0xc4208efee0, 0xc4204854a0)
	/workspace/anago-v1.10.0-rc.1.9+fc32d2f3698e36/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:843 +0x334
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute(0xc420468f00, 0x18c9378, 0x24e48e0)
	/workspace/anago-v1.10.0-rc.1.9+fc32d2f3698e36/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:791 +0x2b
main.main()
	/workspace/anago-v1.10.0-rc.1.9+fc32d2f3698e36/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubectl/kubectl.go:50 +0x196
```

**What you expected to happen**:
No panic

**How to reproduce it** (as minimally and precisely as possible):
Issue command

**Anything else we need to know**:
",closed,False,2018-04-04 16:15:50,2019-02-23 00:15:40
kubectl,dag24,https://github.com/kubernetes/kubectl/issues/401,https://api.github.com/repos/kubernetes/kubectl/issues/401,kubectl create secret docker-registry omits apiVersion and kind,"**Is this a BUG REPORT or FEATURE REQUEST?**: Bug

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:55:54Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:44:10Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: AWS EC2
- **OS** (e.g. from /etc/os-release): Amazon Linux 2017.12
- **Kernel** (e.g. `uname -a`): Linux 4.9.76-38.79.amzn2.x86_64


**What happened**:
```
$ kubectl create secret docker-registry regsecret --dry-run --docker-server=$REGISTRY --docker-username=AWS --docker-password=$DOCKER_PASSWORD --docker-email=""none@example.com"" -o yaml | kubectl apply -f -
error: error validating ""STDIN"": error validating data: [apiVersion not set, kind not set]; if you choose to ignore these errors, turn validation off with --validate=false
```

**What you expected to happen**: A successful secret creation. This can be achieved with the following workaround:
```
kubectl create secret docker-registry regsecret --dry-run --docker-server=$REGISTRY --docker-username=AWS --docker-password=$DOCKER_PASSWORD --docker-email=""none@example.com"" -o yaml > secret.yml
echo ""apiVersion: v1"" >> secret.yml
echo ""kind: Secret"" >> secret.yml
kubectl apply -f secret.yml
```
The expectation here is that `apiVersion` and `kind` are included in the output of `kubectl create secret docker-registry regsecret --dry-run`.

**How to reproduce it** (as minimally and precisely as possible):
```
kubectl create secret docker-registry test --dry-run --docker-server=foo --docker-username=AWS --docker-password=bar --docker-email=""none@tindecofs.com"" -o yaml | kubectl apply -f -
```

**Anything else we need to know**: I believe this is a regression, but I don't know the precise version number it changed in.",closed,False,2018-04-04 16:42:20,2018-08-24 17:40:30
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/402,https://api.github.com/repos/kubernetes/kubectl/issues/402,some wording tweaks,"Trying to make more sense, and provide more links into the glossary for easy access to definitions.",closed,True,2018-04-04 17:17:52,2018-04-06 21:27:25
kubectl,droot,https://github.com/kubernetes/kubectl/pull/403,https://api.github.com/repos/kubernetes/kubectl/issues/403,kinflate: remove prune field from the manifest,,closed,True,2018-04-04 17:42:57,2018-04-04 18:12:10
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/404,https://api.github.com/repos/kubernetes/kubectl/issues/404,Remove the -f option of inflate,"The manifest path is now just the first arg after the command.
If no arg specified, the current working directory is assumed.
",closed,True,2018-04-04 21:00:39,2018-04-06 21:27:07
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/405,https://api.github.com/repos/kubernetes/kubectl/issues/405,Add springboot example demo,first revision of spring boot example demo,closed,True,2018-04-05 22:28:52,2018-04-09 22:25:10
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/406,https://api.github.com/repos/kubernetes/kubectl/issues/406,Add logging for replacing and merging configmap and secrets,"Tested locally with ` kinflate inflate --alsologtostderr  --v 4 instances/staging`. 
Here is the output
``` 
I0405 16:07:23.576661  257572 decoder.go:224] decoding stream as YAML
I0405 16:07:23.577546  257572 decoder.go:224] decoding stream as YAML
I0405 16:07:23.578280  257572 util.go:116] Merge object map[kind:ConfigMap apiVersion:v1 metadata:map[name:my-demo-configmap creationTimestamp:<nil>] data:map[application.properties:app.name=Kinflate Demo for a Spring Boot application
]] with map[kind:ConfigMap apiVersion:v1 metadata:map[name:demo-configmap creationTimestamp:<nil>] data:map[application.properties:app.name=Staging Kinflate Demo
spring.jpa.hibernate.ddl-auto=update
spring.datasource.url=jdbc:mysql://35.184.58.94:3306/db_example
spring.datasource.username=roott
spring.datasource.password=admin
 foo:bar staging:]]
I0405 16:07:23.578347  257572 util.go:119] The merged object is map[kind:ConfigMap apiVersion:v1 metadata:map[creationTimestamp:<nil> labels:map[] annotations:map[] name:my-demo-configmap] data:map[application.properties:app.name=Staging Kinflate Demo
spring.jpa.hibernate.ddl-auto=update
spring.datasource.url=jdbc:mysql://35.184.58.94:3306/db_example
spring.datasource.username=roott
spring.datasource.password=admin
 foo:bar staging:]]
```",closed,True,2018-04-05 23:09:17,2018-04-06 22:00:09
kubectl,zerda,https://github.com/kubernetes/kubectl/issues/407,https://api.github.com/repos/kubernetes/kubectl/issues/407,kubectl zsh completion cause directory path completion has extra space,"**Is this a BUG REPORT or FEATURE REQUEST?** : BUG REPORT

**Kubernetes version**:
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:55:54Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:44:10Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Aliyun ECS
- **OS**: Ubuntu 16.04.4 LTS
- **Kernel**: 4.4.0-105-generic
- **Install tools**: zsh 5.1.1

**What happened**:

When hit <kbd>tab</kbd> to complete a sub directory path, zsh added an extra space after the directory path. 
It causes press <kbd>tab</kbd> repeatly to complete additional path impossible.

**What you expected to happen**:

Like bash completion, only file completion with extra space, there should be no space after directory.

**How to reproduce it**:

1. Says there is a file `foo/bar.yaml`;
1. Typing `kubectl apply -f f`, then press <kbd>tab</kbd>;
1. It complete with `foo/<space>`, instead of `foo/`;
1. I need to delete this extra space, then press <kbd>tab</kbd> again to complete the file `bar.yaml`。 

But there is another situation working correctly.
1. Same as `~/foo/bar.yaml`;
1. Typing `kubectl apply -f ~/f`, then press <kbd>tab</kbd>, it finished with `kubectl apply -f ~/foo/`. 

**Anything else we need to know**:

From debug log,  I got this for `f` completion.
```
_filedir @(json|yaml|yml) cur=f
RET=foo len=1
```

Got this for `~/f` completion.
```
_filedir @(json|yaml|yml) cur=~/f
RET= len=0
```

It seems there is something different in `RET=( $(compgen -f) )` of [completion.go](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/cmd/completion.go#L239).
",open,False,2018-04-06 03:35:09,2019-03-28 16:34:02
kubectl,thockin,https://github.com/kubernetes/kubectl/pull/408,https://api.github.com/repos/kubernetes/kubectl/issues/408,Use k8s GCR vanity URL,,closed,True,2018-04-06 15:52:14,2019-02-26 02:11:24
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/409,https://api.github.com/repos/kubernetes/kubectl/issues/409,update vendor apimachinery,Pick up the change in https://github.com/kubernetes/kubernetes/pull/61905 for https://github.com/kubernetes/kubectl/pull/375,closed,True,2018-04-06 17:05:00,2018-04-06 18:05:57
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/410,https://api.github.com/repos/kubernetes/kubectl/issues/410,newreadme,quick change per Ahmet's suggestions,closed,True,2018-04-06 17:34:04,2018-04-06 21:27:02
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/411,https://api.github.com/repos/kubernetes/kubectl/issues/411,onePageHelloWorld,"per Ahmet's suggestion, put hello world on one page.

This demo is tested as part of the presubmit.

",closed,True,2018-04-06 18:04:50,2018-04-06 21:27:00
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/412,https://api.github.com/repos/kubernetes/kubectl/issues/412,add tree output to demo,"This makes it easier to understand the file layout without having to actually execute the commands.
",closed,True,2018-04-06 19:59:12,2018-04-06 21:26:59
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/413,https://api.github.com/repos/kubernetes/kubectl/issues/413,README for demos,,closed,True,2018-04-06 22:03:02,2018-04-26 21:18:46
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/414,https://api.github.com/repos/kubernetes/kubectl/issues/414,Command organization - build and edit," * inflate becomes build
 * edit sits on top of add, set, etc.
",closed,True,2018-04-06 22:55:27,2018-04-26 21:18:45
kubectl,jheiss,https://github.com/kubernetes/kubectl/issues/415,https://api.github.com/repos/kubernetes/kubectl/issues/415,kubectl config set hangs on some invalid property names,"(Previously reported by someone else in kubernetes/kubernetes#29312)

I haven't quite figured out exactly what the pattern is, but property names that are almost but maybe not quite right seem to trigger it. For example, this hangs: `kubectl config set users.jheiss.exec.command test`
",open,False,2018-04-09 13:53:16,2019-03-25 12:10:21
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/416,https://api.github.com/repos/kubernetes/kubectl/issues/416,structure for manifest documentation,"We need some documentation for the manifest on the website.

Here's an initial structure, that uses the ""kinflate init"" command as a source of truth
to create an example that displays on the website.

To update the docs, create a branch, run ""make docs"", and merge the branch.

Down the road we could use the makefile to do more sophisticated inclusion of generated code with the website.

",closed,True,2018-04-09 18:54:36,2018-04-26 21:18:44
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/417,https://api.github.com/repos/kubernetes/kubectl/issues/417,AAUI don't want to think kinflate is a package management tool.,"So we change the config file name from
_Kube-manifest.yaml_ to _kustomize.yaml_

Also update all docs.

(There was a thread relating the word _manifest_ to package management)",closed,False,2018-04-10 16:15:40,2018-05-18 21:10:54
kubectl,karan,https://github.com/kubernetes/kubectl/issues/418,https://api.github.com/repos/kubernetes/kubectl/issues/418,Kinflate: dec.DisallowUnknownFields undefined,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
kinflate
kinflate json

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-27T00:13:02Z"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.3"", GitCommit:""f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd"", GitTreeState:""clean"", BuildDate:""2017-11-08T18:27:48Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**: Macbook Pro
- **OS** (e.g. from /etc/os-release): MacOS 10.13.3
- **Kernel** (e.g. `uname -a`): Darwin Kernel Version 17.4.0
- **Install tools**:
- **Others**: go 1.9.2


**What happened**:

Tried to install kinflate:

```
$ go get k8s.io/kubectl/cmd/kinflate
# k8s.io/kubectl/pkg/kinflate/app
../../../kubectl/pkg/kinflate/app/application.go:253:5: dec.DisallowUnknownFields undefined (type *json.Decoder has no field or method DisallowUnknownFields)

```

**What you expected to happen**:

Install succeeds.

**How to reproduce it** (as minimally and precisely as possible):

Try to install kinflate using go1.9.2 on MacOS.

**Anything else we need to know**:

",closed,False,2018-04-10 17:40:52,2018-05-18 21:10:24
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/419,https://api.github.com/repos/kubernetes/kubectl/issues/419,swtich to depend on the scheme in client-go,,closed,True,2018-04-10 18:18:44,2018-04-10 23:48:32
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/420,https://api.github.com/repos/kubernetes/kubectl/issues/420,change Kube-manifest.yaml to kustomize.yaml,Will update the example-ldap and example-springboot after this PR is merged,closed,True,2018-04-10 18:37:03,2018-04-10 19:07:11
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/421,https://api.github.com/repos/kubernetes/kubectl/issues/421,AAUI want to know the name of this thing,"After the move (different story), replace every occurance of kinflate with kustomize.  Do this in one shot, rather than piecemeal.",closed,False,2018-04-10 20:49:03,2018-05-18 21:05:12
kubectl,monopole,https://github.com/kubernetes/kubectl/issues/422,https://api.github.com/repos/kubernetes/kubectl/issues/422,Write a test for updates to Configmaps. (5),"@monopole has taken this.

**Rolling updates for ConfigMaps**

As a user, when I change a value in a ConfigMap, that change should be rolled out to all Workloads using that ConfigMap *in the same KubeManifest*

i.e. verify that the names of generated config maps is a hash of the contents and that any deployments in the output of inflate referene this new name/",closed,False,2018-04-10 21:26:35,2018-04-11 00:00:46
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/423,https://api.github.com/repos/kubernetes/kubectl/issues/423,Test config map names,"This demonstrates how configmap changes result in name changes
that force rolling updates for deployments.

Since the demo is tested, this 
Fixes #422 
",closed,True,2018-04-10 21:29:23,2018-04-26 21:18:42
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/424,https://api.github.com/repos/kubernetes/kubectl/issues/424,change kinflate to kustomize,,closed,True,2018-04-10 22:29:23,2018-04-11 19:50:50
kubectl,przemek-sl,https://github.com/kubernetes/kubectl/issues/425,https://api.github.com/repos/kubernetes/kubectl/issues/425,"""kubectl label all"" skips secrets","**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
kubectl label secrets

---

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

**Kubernetes version**:
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-27T00:13:02Z"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:"""", Minor:"""", GitVersion:""v1.9.4"", GitCommit:""bee2d1505c4fe820744d26d41ecd3fdd4a3d6546"", GitTreeState:""clean"", BuildDate:""2018-03-21T21:48:36Z"", GoVersion:""go1.9.1"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **OS**: Mac OS X (10.13.2, build 17C88)
- **Kernel**: 17.3.0 Darwin Kernel Version 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64


**What happened**:
`kubectl label all` skips labelling secrets. They have to be set explicitly in parameters.

```
$ kubectl label all --all key2=value2
pod ""lunging-spaniel-myproject-85c64d89df-7vsjx"" labeled
service ""kubernetes"" labeled
service ""lunging-spaniel-myproject” labeled
deployment.extensions ""lunging-spaniel-myproject"" labeled
replicaset.extensions ""lunging-spaniel-myproject85c64d89df"" labeled
deployment.apps ""lunging-spaniel-myproject"" not labeled
replicaset.apps ""lunging-spaniel-myproject-85c64d89df"" not labeled

$ kubectl label secrets --all key2=value2
secret ""default-token-sjjdm"" labeled
secret ""lunging-spaniel-myproject"" labeled
secret ""lunging-spaniel-myproject-jwt"" labeled
````

**What you expected to happen**:
`kubectl label all` should label all resources, without any exceptions.

",closed,False,2018-04-11 14:27:03,2018-04-14 14:35:17
kubectl,sacha-cs,https://github.com/kubernetes/kubectl/issues/426,https://api.github.com/repos/kubernetes/kubectl/issues/426,kubectl describe (statefulset|clusterrole|clusterrolebinding|cronjobs) fails in v1.10.0,"**What keywords did you search in Kubernetes issues before filing this one?** :
kubectl describe statefulset

---

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

**Kubernetes version**:
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-27T00:14:31Z"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.7"", GitCommit:""b30876a5539f09684ff9fde266fda10b37738c9c"", GitTreeState:""clean"", BuildDate:""2018-01-16T21:52:38Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: AWS
- **OS**: macOS Sierra 10.12.6
- **Kernel**: Darwin Kernel Version 16.7.0
- **Install tools**: brew


**What happened**:
```
$ kubectl describe statefulset <statefulset> -n <namespace>
Error from server (NotFound): the server could not find the requested resource
```

`kubectl delete statefulset ...` also fails with the same error


**What you expected to happen**:
The `kubectl describe|delete statefulset` command to succeed


**How to reproduce it**:
`kubectl describe statefulset <statefulset> -n <namespace>` on an existing statefulset in the cluster


**Anything else we need to know**:
- `kubectl get statefulsets` works fine
- I can run `kubectl describe` on other resources successfully such as pods.

It works fine in the previous kubectl version, since comparing the same command with both installations shows it:
```
$ /usr/local/Cellar/kubernetes-cli/1.9.2/bin/kubectl describe statefulset <statefultset> -n <namespace>
Name:                   statefulset
Namespace:         namespace
CreationTimestamp:  Mon, 09 Apr 2018 18:21:53 +0100
...
```

```
$ /usr/local/Cellar/kubernetes-cli/1.10.0/bin/kubectl describe statefulset <statefultset> -n <namespace>
Error from server (NotFound): the server could not find the requested resource
```
",closed,False,2018-04-11 14:51:08,2018-09-25 22:43:15
kubectl,jordansissel,https://github.com/kubernetes/kubectl/issues/427,https://api.github.com/repos/kubernetes/kubectl/issues/427,Cannot use Windows paths in create secret --from-file,"**What keywords did you search in Kubernetes issues before filing this one?** kubectl create secret generic from-file windows ""is not a valid key name for a Secret""

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): bug

**Kubernetes version** (use `kubectl version`):

```
> kubectl version
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:55:54Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""windows/amd64""}
```


**Environment**:
- **Cloud provider or hardware configuration**: N/A
- **OS** (e.g. from /etc/os-release): Windows 10
- **Kernel** (e.g. `uname -a`): N/A
- **Install tools**: Chocolatey
- **Others**:


**What happened**:

```
> kubectl create secret generic example --from-file .\example\thing.txt
error: "".\\example\\thing.txt"" is not a valid key name for a Secret: a valid config key must consist of alphanumeric characters, '-', '_' or '.' (e.g. 'key.name',  or 'KEY_NAME',  or 'key-name', regex used for validation is '[-._a-zA-Z0-9]+')
```

**What you expected to happen**:

The contents of the file `.\example\thing.txt` should be used to create the secret named `example`


**How to reproduce it** (as minimally and precisely as possible):

```
# On windows (PowerShell)
> mkdir example
> echo foo | out-file .\example\thing.txt
> kubectl create secret generic example --from-file .\example\thing.txt
```

**Anything else we need to know**:

* If the file does not exist, you get a different error:

```
> rm .\example\thing.txt
> kubectl create secret generic example --from-file .\example\thing.txt
error: error reading .\example\thing.txt: The system cannot find the file specified.
```

* You can work around this by using unix-style paths:

```
> kubectl create secret generic example --from-file .\example\thing.txt
<fails>

> kubectl create secret generic example --from-file ./example/thing.txt
secret ""example"" created
```",closed,False,2018-04-11 16:42:01,2019-01-18 14:37:33
kubectl,droot,https://github.com/kubernetes/kubectl/pull/428,https://api.github.com/repos/kubernetes/kubectl/issues/428,change kinflate to kustomize,Rebased and resolved conflict in `pkg/kustomize/transformers/overlay.go`,closed,True,2018-04-11 18:13:12,2018-04-11 18:25:56
kubectl,monopole,https://github.com/kubernetes/kubectl/issues/429,https://api.github.com/repos/kubernetes/kubectl/issues/429,"AAUI find the init command creates work, rather than removes it.","A user who seeks to do customization has no useful concept of a default customization, so there's no good way to populate an official initial `kustomize.yaml` file.

Whatever we my offer as an example namePrefix, label, or configmapgenerator only forces the user to remove them.  If they mistakenly leave these examples in, they get strange ""helloworld"" or whatever customizations in their actual resources.  This problem is appearing in all the examples too.

So delete the init command, and remove it from examples.",closed,False,2018-04-11 18:53:57,2018-04-13 18:00:01
kubectl,monopole,https://github.com/kubernetes/kubectl/issues/430,https://api.github.com/repos/kubernetes/kubectl/issues/430,AAUI need to see a completely documented kustomize.yaml example on website (3),,closed,False,2018-04-11 18:56:45,2018-04-17 16:31:59
kubectl,monopole,https://github.com/kubernetes/kubectl/issues/431,https://api.github.com/repos/kubernetes/kubectl/issues/431,AAUI don't want to think kustomize is a package manager (part 2).,"As a followup to the name change from 
_Kube-manifest.yaml_ to _kustomize.yaml_ (#417), remove the use of the word _manifest_ from visible documents, demos, examples.

E.g. in a sentence like ""Now edit the manifest file... "" replace _manifest_ with the literal file name _kustomize.yaml_, or with the phrase _kustomize config file_.",closed,False,2018-04-11 18:57:22,2018-04-13 03:43:59
kubectl,monopole,https://github.com/kubernetes/kubectl/issues/432,https://api.github.com/repos/kubernetes/kubectl/issues/432,Move everything to the new org,"Script to do the move:
https://github.com/monopole/scratch/blob/master/bulkRepoMoveRetainingHistory.sh

Possible destination orgs:
* https://github.com/googlecontainertools
* https://github.com/kubernetes-sigs

Decision thread:  https://groups.google.com/a/google.com/forum/#!topic/gke-kubernetes-app-lifecycle/-aNokW0TIiU

If this thread remains blocked on no decision, we can move into https://github.com/googlecloudplatform, then later use a github tool to move the repo to a new home, leaving forwarding rules (so links don't stop working).

Stuff to move

 1. Everything _kustomize_ under https://github.com/kubernetes/kubectl
 1.  The examples we still use from  https://github.com/kinflate
 1. travis settings and scripts (kubernets/kubectl/bin)
 1. the example test framework",closed,False,2018-04-11 20:24:11,2018-05-18 21:04:56
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/433,https://api.github.com/repos/kubernetes/kubectl/issues/433,Remove init and manifest from demos,This partially addresses both #429 (remove init from everything) and #432 (remove manifest from everything).,closed,True,2018-04-11 21:00:17,2018-04-26 21:18:40
kubectl,droot,https://github.com/kubernetes/kubectl/pull/434,https://api.github.com/repos/kubernetes/kubectl/issues/434,kustomize: automate github release publishing,"An example release looks like:

https://github.com/droot/kubectl/releases/tag/v0.1.0",closed,True,2018-04-11 22:20:03,2018-04-11 22:23:55
kubectl,mengqiy,https://github.com/kubernetes/kubectl/pull/435,https://api.github.com/repos/kubernetes/kubectl/issues/435,Update vendor and related hash func,"The 1st commit pins api, apimachinery, client-go to the latest version.
The 2nd commit updates the related hash func",closed,True,2018-04-11 22:21:55,2018-04-14 22:19:41
kubectl,monopole,https://github.com/kubernetes/kubectl/issues/436,https://api.github.com/repos/kubernetes/kubectl/issues/436,AAUI don't want to see unusable fields in kustomize.yaml (3),"remove the unused k8s api fields:
```
apiVersion: manifest.k8s.io/v1alpha1
kind: Manifest
metadata:
  name: foo
```

from all examples and documentation",closed,False,2018-04-12 00:23:03,2018-05-18 21:31:35
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/437,https://api.github.com/repos/kubernetes/kubectl/issues/437,Drop k8s metadata fields from kustomize.yaml struct,"### Move
  `pkg/apis/manifest/v1alpha1/types.go`
to
  `pkg/kustomize/types/manifest.go`

Will rename the `Manifest` type to `Kustomization` in a subsequent PR
(so this file name will change then).

### Remove user visible references to the word _manifest_
...in the files this PR touches.

### Replace
```
apiVersion: manifest.k8s.io/v1alpha1	
kind: Package	
metadata:	
  name: foo
```
with
```
kustomizationName: foo
```
This latter field makes it possible to add a name to the overall customization, although it's not used for any purpose.  It's basically a documentation field that survives unmarshall/marshall.

Partially addresses #436",closed,True,2018-04-12 00:23:50,2018-04-12 02:51:56
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/438,https://api.github.com/repos/kubernetes/kubectl/issues/438,manifest becomes kustomization,"Rename ""manifest file"" to ""kustomization file"" everywhere.

No more use of the word manifest.  Fixes #431 

_To kustomize is to use a kustomization file._",closed,True,2018-04-12 23:44:11,2018-04-26 21:18:39
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/439,https://api.github.com/repos/kubernetes/kubectl/issues/439,Drop the init command,"Dropping the `init` command because there's no such thing as good default values for a customization.

Anything we'd put in a kustomization file created by init has to be modified or deleted by the user, so it hinders them rather than helps them.  We've noticed this inconvenience when writing demos, etc.

Fixes #429
",closed,True,2018-04-13 17:08:48,2018-04-26 21:18:38
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/440,https://api.github.com/repos/kubernetes/kubectl/issues/440,Example kustomize.yaml,"Fixes #430
",closed,True,2018-04-13 18:22:02,2018-04-26 21:18:11
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/441,https://api.github.com/repos/kubernetes/kubectl/issues/441,Some kustomization field renaming.," * drop `kustomizationName` and `description`
 * change `objectLabels` to `labelsToAdd`
 * change `objectAnnotations` to `annotationsToAdd`",closed,True,2018-04-13 20:37:06,2018-04-26 21:18:30
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/442,https://api.github.com/repos/kubernetes/kubectl/issues/442,Update figs,"New file names, new field names, no more k8s fields.",closed,True,2018-04-16 20:44:32,2018-04-26 21:18:29
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/443,https://api.github.com/repos/kubernetes/kubectl/issues/443,updated figs,,closed,True,2018-04-16 21:01:54,2018-04-26 21:18:27
kubectl,rcorre,https://github.com/kubernetes/kubectl/issues/444,https://api.github.com/repos/kubernetes/kubectl/issues/444,kubectl exec should take a deploy as an argument,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

```
is:issue kubectl exec  in:title deploy in:title
```
---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

FEATURE REQUEST

**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T21:07:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.5"", GitCommit:""f01a2bf98249a4db383560443a59bed0c13575df"", GitTreeState:""clean"", BuildDate:""2018-03-19T15:50:45Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Feature**

`kubectl logs deploy/mydeploy` gets the logs for one pod of the deployment. It would be nice to have a parallel command for `exec`, where `kubectl exec deploy/mydeploy -it -- /bin/sh` gets me a shell onto one of the pods from `mydeploy`.

Right now I get:

```
error: invalid resource name ""deploy/mydeploy"": [may not contain '/']
```
",closed,False,2018-04-17 18:05:38,2019-03-21 18:45:04
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/445,https://api.github.com/repos/kubernetes/kubectl/issues/445,kustomize.yaml becomes kustomization.yaml,"Allows one to more succinctly refer to the kustomize configuration file as a _kustomization_.
Formerly, it was called a manifest, but that term no longer used as it suggests package management.
",closed,True,2018-04-17 18:20:29,2018-04-26 21:18:10
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/446,https://api.github.com/repos/kubernetes/kubectl/issues/446,new figs,,closed,True,2018-04-17 20:49:48,2018-04-26 21:18:09
kubectl,AdamDang,https://github.com/kubernetes/kubectl/pull/447,https://api.github.com/repos/kubernetes/kubectl/issues/447,Typo fix in glossary.md,"mySql->MySQL
refers to to->refers to",closed,True,2018-04-23 14:39:49,2018-05-01 02:13:03
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/448,https://api.github.com/repos/kubernetes/kubectl/issues/448,add a tested breakfast configuration demo,it's not just about k8s resources.,closed,True,2018-04-24 20:06:48,2018-04-26 21:18:08
kubectl,nikhilagrawal577,https://github.com/kubernetes/kubectl/issues/449,https://api.github.com/repos/kubernetes/kubectl/issues/449,Kubectl Top nodes and kubectl describe nodes are showing different values,"
/sig cli
/kind bug

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.5"", GitCommit:""f01a2bf98249a4db383560443a59bed0c13575df"", GitTreeState:""clean"", BuildDate:""2018-03-19T15:59:24Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.1"", GitCommit:""d4ab47518836c750f9949b9e0d387f20fb92260b"", GitTreeState:""clean"", BuildDate:""2018-04-12T14:14:26Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
NAME=""Ubuntu""
VERSION=""14.04.5 LTS, Trusty Tahr""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 14.04.5 LTS""
VERSION_ID=""14.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""

- **Kernel** (e.g. `uname -a`):
```
Linux mo-635ef62b9 4.2.0-27-generic #32~14.04.1-Ubuntu SMP Fri Jan 22 15:32:26 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux 
```
- **Install tools**:
- **Others**:


**What happened**:
Executed ""Kubectl top nodes "" to see CPU/Memory metrics of the node. Took a node which is consuming more CPU/Memory.
Checked details of the node by executing ""kubectl describe node node_name"" 
but CPU/memory Consumption are different from what it shows in ""Kubectl top nodes "" output.


**What you expected to happen**:
CPU/Memory values should be same for a node.

**How to reproduce it** (as minimally and precisely as possible):
1. Create a 10 node cluster.
2. Deploy few large application
3. Execute ""kubectl top nodes"", take one node and 
4. Execute ""kubectl describe node node_name"" on that node
5. Compare the CPU/Memory values of both output.

**Anything else we need to know**:

",open,False,2018-04-26 10:38:31,2019-03-25 12:10:20
kubectl,wknapik,https://github.com/kubernetes/kubectl/issues/450,https://api.github.com/repos/kubernetes/kubectl/issues/450,"Kubectl returns pods in various states when only those in Running state are selected via --field-selector, or -o jsonpath","**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
No.

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
field-selector, field selector, status phase

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.1"", GitCommit:""d4ab47518836c750f9949b9e0d387f20fb92260b"", GitTreeState:""clean"", BuildDate:""2018-04-12T14:26:04Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.7"", GitCommit:""b30876a5539f09684ff9fde266fda10b37738c9c"", GitTreeState:""clean"", BuildDate:""2018-01-16T21:52:38Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**:
AWS
- **OS** (e.g. from /etc/os-release):
Arch Linux
- **Kernel** (e.g. `uname -a`):
Linux something 4.16.3-1-ARCH #1 SMP PREEMPT Thu Apr 19 09:17:56 UTC 2018 x86_64 GNU/Linux
- **Install tools**:
?
- **Others**:
?

**What happened**:
```
kubectl get pods --field-selector=status.phase=Running --namespace some-ns
```
returns pods in other states than Running.

**What you expected to happen**:
I expected only pods in state Running to be returned.

**How to reproduce it** (as minimally and precisely as possible):
```
kubectl get pods --field-selector=status.phase=Running --namespace some-ns
```

**Anything else we need to know**:
That's way too deep... I don't think I can tackle that question here...
",closed,False,2018-04-26 10:44:29,2019-03-15 23:31:26
kubectl,richardmarshall,https://github.com/kubernetes/kubectl/pull/451,https://api.github.com/repos/kubernetes/kubectl/issues/451,Add subcommand for adding patches to the kustomization file,"Adds a new sub-command for adding patches to the kustomization file.

```
kustomize edit add patch my_neat_patch.yaml
```

Also updated the springboot demo to use the new command instead of sed.",closed,True,2018-04-27 14:28:54,2018-04-30 23:47:23
kubectl,AdamDang,https://github.com/kubernetes/kubectl/pull/452,https://api.github.com/repos/kubernetes/kubectl/issues/452,Typo fix: resourse->resource,Line 253: resourse->resource,closed,True,2018-04-27 15:29:54,2018-05-01 01:57:14
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/453,https://api.github.com/repos/kubernetes/kubectl/issues/453,configmapgenerator example,,closed,True,2018-04-27 22:28:24,2018-05-08 19:12:10
kubectl,xellsys,https://github.com/kubernetes/kubectl/issues/454,https://api.github.com/repos/kubernetes/kubectl/issues/454,kubectl cp to work on stopped/completed pods,"**Status Quo:**
The current implementation of `kubectl cp` requires the container to be running by encapsulating the exec command using the tar binary.

**Requirement:**
Allow copying files _from_ (possibly also _to_) a stopped container.

**Additional info:**
This relates to #58512.
It would also more closely align to the `docker cp` functionality.
No need for the tar binary in the container anymore.

-----
**Background:**
My current use case is running `helm test` with a more sophisticated test set (0 and 1 results do not suffice for analysis) and having a _simple_ way of persisting test results at the end (Jenkins or w/e).
I know there are other solutions but I want to keep my test pod simple (not actively pushing test results to some endpoint or keeping it alive) and would like to avoid extensive configuration of a persistent storage.",open,False,2018-04-29 08:18:40,2019-03-20 11:15:49
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/455,https://api.github.com/repos/kubernetes/kubectl/issues/455,pull helloWorld demo data into code repo,"As part of the kinflate to kustomize rename, the org https://github.com/kinflate will be going away.

Also, it won't be replaced, since who would own the new org is problematic.  Perhaps we'll be able to put examples into their own repos under kubernetes-sigs, as siblings to the kustomize repo.  Until then, will pull the demo data in one at a time.  This PR pulls in helloworld.
",closed,True,2018-04-30 23:00:25,2018-05-08 19:12:08
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/456,https://api.github.com/repos/kubernetes/kubectl/issues/456,fix glitch in hello world,"fix some wording now that this example no longer does ""git clone""",closed,True,2018-05-01 18:05:20,2018-05-08 19:12:07
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/457,https://api.github.com/repos/kubernetes/kubectl/issues/457,add configGeneration to README in demos dir,,closed,True,2018-05-01 18:50:09,2018-05-08 19:12:06
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/458,https://api.github.com/repos/kubernetes/kubectl/issues/458,capture springboot demo data,,closed,True,2018-05-01 20:57:11,2018-05-08 19:12:05
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/459,https://api.github.com/repos/kubernetes/kubectl/issues/459,Modify mysql and springboot demo to download their data from the code…,"… repo, not the kinflate repo",closed,True,2018-05-01 22:03:54,2018-05-08 19:11:55
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/460,https://api.github.com/repos/kubernetes/kubectl/issues/460,drop unused pngs,"Fix some nits (bad links, kustomize in normal font, etc.) and drop two unused png files.",closed,True,2018-05-01 22:44:36,2018-05-08 19:11:53
kubectl,gregkeys,https://github.com/kubernetes/kubectl/issues/461,https://api.github.com/repos/kubernetes/kubectl/issues/461,--all-namespaces should work with kubectl set image deployments,"/kind feature

**What happened**:
received the following error `unknown flag: --all-namespaces`

**What you expected to happen**:
I expected all deployments to be updated to the given image:tag across all name spaces 

**How to reproduce it (as minimally and precisely as possible)**:
deploy nginx into 2 or more namespaces with a label of `app: nginx` then run the following command
` kubectl set image deployments nginx=nginx:1.9.1 --selector='app=nginx' --all-namespaces`

I was able to achieve the desired result via the following bash script
```
for name in `kubectl get po --all-namespaces --selector app=nginx -o=jsonpath='{.items[*].metadata.namespace}'`; do kubectl set image deployments nginx=nginx:1.9.1 --all --namespace=${name}; done
```

it seems like this should be built in to commands where it make sense to perform actions across multiple namespaces. The script loop feels like an error prone solution. 

Our use case is as follows:
we have two clusters Production and Development. Our namespaces use the following pattern 
\<customer\>-\<environment\>

 e.g. on the production cluster they would be `acme-prod`, `acme-uat` on the development cluster they are `acme-dev`, `acme-qa`

This essentially gives us 4 environments per customer across our clusters, our app is built with micro services 90% of which are used in every single environment (e.g. activities-api, channels-api) this means that we may have 400 pods running an image that needs to be updated. 

Changes to our services are first deployed to all \<customer\>-dev namespaces once the tests pass they are deployed to \<customer\>-qa once the tests pass QA then our charts are updated with the approved versions which we would also like to be able to apply across all our \<customer\>-uat and \<customer\>-prod namespaces where the labels and selector match up appropriately.

/sig cli",closed,False,2018-05-02 03:50:00,2019-02-23 01:16:38
kubectl,AdamDang,https://github.com/kubernetes/kubectl/pull/462,https://api.github.com/repos/kubernetes/kubectl/issues/462,Typo fix in the returned info.,Line 58: creat->create,closed,True,2018-05-02 11:30:02,2018-05-02 23:08:51
kubectl,AdamDang,https://github.com/kubernetes/kubectl/pull/463,https://api.github.com/repos/kubernetes/kubectl/issues/463,Typo fix: defintion->definition,Line 148: defintion->definition,closed,True,2018-05-03 14:38:38,2018-05-03 18:11:52
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/464,https://api.github.com/repos/kubernetes/kubectl/issues/464,Add ldap example to demo data,"`base` folder is copied from `kinflate/example-ldap`
`overlays` folder is copied from `kinflate/example-ldap-instances`",closed,True,2018-05-03 20:27:41,2018-05-04 01:38:51
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/465,https://api.github.com/repos/kubernetes/kubectl/issues/465,Move example-ldap,"Move github.com/kinflate/example-ldap and sibling repos -ldap-customized and -ldap-instances to the new org, retaining the integration tests run by kubernetes/test-infra
https://k8s-testgrid.appspot.com/sig-cli-misc#kinflate-periodic-default-gke",closed,False,2018-05-03 20:28:20,2018-05-04 19:37:31
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/466,https://api.github.com/repos/kubernetes/kubectl/issues/466,change test script to use ldap from the new location,"This change is for the integration test of kustomize: kinflate-periodic-default-gke
It will pick up the ldap example from new location.",closed,True,2018-05-04 16:32:43,2018-05-04 18:08:52
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/467,https://api.github.com/repos/kubernetes/kubectl/issues/467,Add ldap demo,locally tested by `bin/pre-commit.sh`,closed,True,2018-05-04 17:58:42,2018-05-04 18:07:53
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/468,https://api.github.com/repos/kubernetes/kubectl/issues/468,Remove example-ldap repo from the integration test,"Since the data of example-ldap has been moved to `cmd/kustomize`. We don't need to checkout the `example-ldap` repo any more.
We can remove this repo from the checkout list.
As well as `example-ldap-customized`

https://k8s-testgrid.appspot.com/sig-cli-misc#kinflate-periodic-default-gke",closed,False,2018-05-07 16:57:38,2018-05-07 22:13:44
kubectl,AdamDang,https://github.com/kubernetes/kubectl/pull/469,https://api.github.com/repos/kubernetes/kubectl/issues/469,Typo fix: cluser->cluster,Line 241: cluser->cluster,closed,True,2018-05-08 16:00:28,2018-05-11 20:31:43
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/470,https://api.github.com/repos/kubernetes/kubectl/issues/470,add apache license file for kustomize,,closed,True,2018-05-08 17:35:07,2018-05-08 17:41:04
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/471,https://api.github.com/repos/kubernetes/kubectl/issues/471,Replace term 'instance' with 'variant'.,,closed,True,2018-05-08 18:42:04,2018-05-08 19:11:50
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/472,https://api.github.com/repos/kubernetes/kubectl/issues/472,reorderGloss,,closed,True,2018-05-08 19:10:27,2018-05-08 19:11:48
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/473,https://api.github.com/repos/kubernetes/kubectl/issues/473,updateFigs,,closed,True,2018-05-08 19:20:48,2018-05-08 19:20:55
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/474,https://api.github.com/repos/kubernetes/kubectl/issues/474,updateFig2,,closed,True,2018-05-08 19:26:14,2018-05-08 19:26:22
kubectl,nikhilagrawal577,https://github.com/kubernetes/kubectl/issues/475,https://api.github.com/repos/kubernetes/kubectl/issues/475,kubectl get pods --all-namespaces returns error ,"/kind bug
/sig cli

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
Bug Report

**Kubernetes version** (use `kubectl version`):
1.8.5

**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): 
NAME=""Ubuntu""
VERSION=""14.04.5 LTS, Trusty Tahr""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 14.04.5 LTS""
VERSION_ID=""14.04""

- **Kernel** (e.g. `uname -a`):
Linux mo-635ef62b9 4.2.0-27-generic #32~14.04.1-Ubuntu SMP Fri Jan 22 15:32:26 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

- **Install tools**:
- **Others**:


**What happened**:
With AWS, when i try to get all the pods in the cluster, it shows error. 
```
FVFV50CPJ1WV:kubernetes i076657$ kubectl get pods --all-namespaces
Stream error http2.StreamError{StreamID:0x33, Code:0x2, Cause:error(nil)} when reading response body, may be caused by closed connection. Please retry
```

**What you expected to happen**:
It should show all the pods available in all the namespaces. 

**How to reproduce it** (as minimally and precisely as possible):

kubectl get pods --all-namespaces 

**Anything else we need to know**:

",closed,False,2018-05-09 08:33:10,2018-09-25 22:58:55
kubectl,majewsky,https://github.com/kubernetes/kubectl/issues/476,https://api.github.com/repos/kubernetes/kubectl/issues/476,kubectl shows localized messages even though LC_MESSAGES=C,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): bug report

**Kubernetes version** (use `kubectl version`): 

```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.1"", GitCommit:""d4ab47518836c750f9949b9e0d387f20fb92260b"", GitTreeState:""clean"", BuildDate:""2018-04-12T14:26:04Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: N/A
- **OS** (e.g. from /etc/os-release): Arch Linux
- **Kernel** (e.g. `uname -a`): Linux hostname 4.16.7-1-ARCH #1 SMP PREEMPT Wed May 2 21:12:36 UTC 2018 x86_64 GNU/Linux
- **Install tools**: curl

`kubectl --help` prints some messages localized to German even though I explicitly forbade that in my locale settings.

```
$ env | grep $'LC\nLANG'
LANG=de_DE.UTF-8
LANGUAGE=
LC_MESSAGES=C
$ kubectl set --help
Configure application resources

These commands help you make changes to existing application resources.

Available Commands:
  env            Update environment variables on a pod template
  image          Aktualisiere das Image einer Pod-Template
  resources      Aktualisiere Resourcen requests/limits auf Objekten mit Pod-Templates
  selector       Setze den Selektor auf einer Resource
  serviceaccount Update ServiceAccount of a resource
  subject        Update User, Group or ServiceAccount in a RoleBinding/ClusterRoleBinding

Usage:
  kubectl set SUBCOMMAND [options]

Use ""kubectl <command> --help"" for more information about a given command.
Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```",closed,False,2018-05-09 12:20:50,2018-10-12 18:20:00
kubectl,soltysh,https://github.com/kubernetes/kubectl/pull/477,https://api.github.com/repos/kubernetes/kubectl/issues/477,Add myself to owners,,closed,True,2018-05-09 13:53:41,2018-05-10 14:00:45
kubectl,AdamDang,https://github.com/kubernetes/kubectl/pull/478,https://api.github.com/repos/kubernetes/kubectl/issues/478,Typo fix: implmentation->implementation,Line 32: implmentation->implementation,closed,True,2018-05-09 14:55:38,2018-05-14 16:50:46
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/479,https://api.github.com/repos/kubernetes/kubectl/issues/479,Kustomize: Help text for `kustomize edit add` shouldn't show secret.,"`kustomize edit add` shows an example of adding a secret, but this doesn't exist",closed,False,2018-05-09 19:43:43,2018-05-18 21:04:23
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/480,https://api.github.com/repos/kubernetes/kubectl/issues/480,Kustomize: Support `kustomize edit add labels` and `kustomize edit add annotations`,"kustomize supports adding labels and annotations through the declarative config, but it isn't obvious how to add them from the cli.  We should add commands to do so.",closed,False,2018-05-09 19:44:49,2018-05-18 21:03:25
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/481,https://api.github.com/repos/kubernetes/kubectl/issues/481,Kustomize: Support `kustomize edit add base`,Add `kustomize edit add base` command to modify the kustomization.yaml with a base.,closed,False,2018-05-09 20:18:58,2018-05-18 21:01:56
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/482,https://api.github.com/repos/kubernetes/kubectl/issues/482,Add owners section for subproject owners,Add an 'owners' section of the OWNERS file. Members of this section are specifically enumerated as the Subproject Owner role for the 'kubeclt' subproject in the SIG CLI charter.,closed,True,2018-05-10 00:27:15,2018-05-10 15:05:51
kubectl,zentron,https://github.com/kubernetes/kubectl/issues/483,https://api.github.com/repos/kubernetes/kubectl/issues/483,Provide the ability to clear the current kubectl context without explicitly setting a new one,"**Feature Request**

When attempting to run scripts we sometimes want to create and use a ""temporary"" context that can be easily cleaned up afterward. (as opposed to appending the `--server` etc arguments to every command since they may be externally loaded)

e.g
```
...
$originalContext = $(kubectl config current-context)
kubectl config set-context tempcontext --user=tempuser --cluster=tempcluster
kubectl config use-context tempcontext 
...

do some stuff
...
kubectl config use-context $originalContext
kubectl config delete-context tempcontext 
kubectl config delete-cluster tempcluster
...etc

```
However the `delete-context` result may end up with `warning: this removed your active context, use ""kubectl config use-context"" to select a different one` if there was no original context set (lets say on a fresh machine).  Although the context can start as """" in the config, there is no way to just set it to """" since `kubectl config use-context` with no context _name_ results in an error due to ""unexpected args""

It would be nice if we could either 
1. Use the `use-context` command and provide a null or empty context (or some `--clear` arg)
2. Or provide a `kubectl config clear-context`",open,False,2018-05-10 06:45:18,2019-03-18 21:42:10
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/484,https://api.github.com/repos/kubernetes/kubectl/issues/484,add code of conduct,,closed,True,2018-05-11 18:10:54,2018-05-11 18:11:04
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/485,https://api.github.com/repos/kubernetes/kubectl/issues/485,owner mods,,closed,True,2018-05-11 18:38:40,2018-05-11 18:38:53
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/486,https://api.github.com/repos/kubernetes/kubectl/issues/486,Remove kustomize,Moved to https://github.com/kubernetes-sigs/kustomize,closed,True,2018-05-11 22:21:14,2018-05-14 03:28:25
kubectl,mrbobbytables,https://github.com/kubernetes/kubectl/issues/487,https://api.github.com/repos/kubernetes/kubectl/issues/487,Error describing cronjobs with kubectl v1.10+,"/kind bug
/sig cli
**Is this a request for help?** No

**What keywords did you search in Kubernetes issues before filing this one?** 

kubectl describe cronjob
""the server could not find the requested resource""
**Note:** Possibly related to #426 

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT

**Kubernetes version** (use `kubectl version`):

Impacted Client Versions:
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-27T00:13:02Z"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.1"", GitCommit:""d4ab47518836c750f9949b9e0d387f20fb92260b"", GitTreeState:""clean"", BuildDate:""2018-04-13T22:27:55Z"", GoVersion:""go1.9.5"", Compiler:""gc"", Platform:""darwin/amd64""}
```

Server Versions Tested:
```
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.9-gke.1"", GitCommit:""c205eb8319d25aa46b1eb39b6c1d2bc5d1a49fae"", GitTreeState:""clean"", BuildDate:""2018-03-13T17:49:52Z"", GoVersion:""go1.8.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:44:10Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**: minikube v0.26.1 and GKE with v1.8.9-gke.1 (will list minikube)
- **Cloud provider or hardware configuration**:  minikbue
- **OS** (e.g. from /etc/os-release): Buildroot
- **Kernel** (e.g. `uname -a`): Linux minikube 4.9.64 #1 SMP Fri Mar 30 21:27:22 UTC 2018 x86_64 GNU/Linux
- **Install tools**: minikube

**What happened**: 
Attempting to describe a cronjob will result in the following error:
```
Error from server (NotFound): the server could not find the requested resource
```
Switching to an older client, pre v1.10.0 works as intended with no error thrown. This was tested with kubectl versions 1.93. and 1.9.6
```
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-09T21:51:54Z"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.6"", GitCommit:""9f8ebd171479bec0ada837d7ee641dec2f8c6dd1"", GitTreeState:""clean"", BuildDate:""2018-03-21T20:49:12Z"", GoVersion:""go1.9.4"", Compiler:""gc"", Platform:""darwin/amd64""}
```

**What you expected to happen**:
Return the description of a cronjob


**How to reproduce it** (as minimally and precisely as possible):
Create and then describe a cronjob, in testing I used the below example:
```
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: cronjob-error-example
spec:
  schedule: ""*/1 * * * *""
  jobTemplate:
    spec:
      completions: 1
      template:
        spec:
          containers:
          - name: hello
            image: alpine:latest
            command: [""/bin/sh"", ""-c""]
            args: [""echo hello from $HOSTNAME!""]
          restartPolicy: Never
```

Command to describe
```
$ kubectl describe cronjob cronjob-error-example
```

",closed,False,2018-05-13 19:00:47,2018-09-25 23:05:36
kubectl,monopole,https://github.com/kubernetes/kubectl/pull/488,https://api.github.com/repos/kubernetes/kubectl/issues/488,update vendoring post kustomize delete,"This is just a ""dep ensure"", to get rid of vendoring that's no longer needed.",closed,True,2018-05-16 23:43:21,2018-05-17 17:50:50
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/489,https://api.github.com/repos/kubernetes/kubectl/issues/489,kubectl config -o=jsonpath returns golang []byte represenation for string field,"
**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): **BUG REPORT**

**Kubernetes version** (use `kubectl version`): 1.10.2

**What happened**: I am trying to export my CA certificate through kubectl.

I ran this command:

    kubectl config view --minify --flatten -o=jsonpath='{.clusters[*].cluster.certificate-authority-data}'

and got this:

```
[45 45 45 45 45 66 69 71 73 78 32 67 69 82 84 73 70 73 67 65 84 69 45 45 45 45 45 10 77 73 73 68 68 68 67 67 65 102 83 103 65 119 73 66 65 103 73 82 65 76 110 85 43 48 122 82 79 120 84 70 71 104 54 87 105 50 73 83 82 65 52 119 68 81 89 74 75 111 90 73 104 118 99 78 65 81 69 76 66 81 65 119 10 76 122 69 116 77 67 115 71 65 49 85 69 65 120 77 107 79 87 69 122 77 87 69 51 90 109 69 116 78 106 78 107 77 105 48 48 78 122 103 52 76 87 69 120 77 50 73 116 89 84 108 109 89 50 81 119 90 109 82 105 79 84 69 119 10 77 66 52 88 68 84 69 52 77 68 85 120 77 84 69 53 78 68 107 120 78 49 111 88 68 84 73 122 77 68 85 120 77 68 73 119 78 68 107 120 78 49 111 119 76 122 69 116 77 67 115 71 65 49 85 69 65 120 77 107 79 87 69 122 10 77 87 69 51 90 109 69 116 78 106 78 107 77 105 48 48 78 122 103 52 76 87 69 120 77 50 73 116 89 84 108 109 89 50 81 119 90 109 82 105 79 84 69 119 77 73 73 66 73 106 65 78 66 103 107 113 104 107 105 71 57 119 48 66 10 65 81 69 70 65 65 79 67 65 81 56 65 77 73 73 66 67 103 75 67 65 81 69 65 110 66 121 110 112 112 102 108 118 97 113 105 43 87 78 119 67 87 78 117 116 122 115 70 65 47 120 78 85 79 50 65 117 81 81 113 113 90 70 118 10 90 108 90 52 53 43 112 113 78 115 57 53 89 80 87 56 121 55 73 98 101 55 81 101 79 107 81 55 100 71 120 114 86 47 80 72 85 70 76 82 122 85 108 73 67 117 117 108 77 118 54 74 117 67 100 83 103 43 80 111 110 68 54 49 10 65 97 50 87 56 72 50 111 72 43 106 82 112 55 84 51 55 99 103 121 43 65 102 51 122 47 53 47 54 81 108 89 83 68 103 75 104 77 76 86 122 122 74 104 65 54 102 57 72 54 51 87 86 88 113 55 48 50 101 98 120 70 111 72 10 117 53 118 101 117 80 80 122 89 55 78 80 54 66 54 74 114 56 84 103 109 85 110 52 81 89 103 57 81 121 111 53 112 56 86 53 82 104 117 57 66 109 55 111 103 69 50 100 79 109 119 48 118 65 89 43 49 69 68 50 115 75 81 47 10 51 101 97 66 85 81 80 89 119 86 82 104 68 72 89 57 105 120 116 79 78 72 120 101 98 80 87 106 70 112 102 69 118 80 74 51 53 121 73 121 120 80 117 112 110 103 51 98 70 111 115 106 47 115 48 83 120 120 65 122 104 109 90 74 10 84 102 116 101 79 77 52 79 48 113 122 89 49 120 103 66 122 109 68 70 77 98 52 49 103 84 83 99 101 100 79 49 55 57 78 57 110 56 66 54 82 48 116 53 79 119 73 68 65 81 65 66 111 121 77 119 73 84 65 79 66 103 78 86 10 72 81 56 66 65 102 56 69 66 65 77 67 65 103 81 119 68 119 89 68 86 82 48 84 65 81 72 47 66 65 85 119 65 119 69 66 47 122 65 78 66 103 107 113 104 107 105 71 57 119 48 66 65 81 115 70 65 65 79 67 65 81 69 65 10 72 54 120 80 72 49 86 99 113 56 117 105 114 81 107 57 85 97 85 55 89 122 89 51 99 87 76 57 49 51 71 43 86 106 118 107 112 79 120 107 110 49 90 85 66 43 49 113 116 120 55 119 67 53 72 66 56 88 79 82 112 43 104 68 10 113 115 65 108 118 112 113 88 72 106 112 122 82 70 43 117 66 99 51 81 56 105 55 81 111 88 103 80 75 106 65 100 121 55 113 70 84 89 51 56 78 119 113 50 70 87 66 79 86 108 80 49 112 113 57 83 82 79 53 104 111 88 82 57 10 101 72 120 118 57 82 54 84 119 54 81 116 81 52 120 48 86 68 110 121 74 117 116 104 68 67 78 112 77 109 89 79 108 112 67 111 52 106 85 49 55 117 104 103 68 52 89 98 75 112 55 83 70 80 79 74 47 111 48 117 100 71 56 109 10 52 106 76 119 104 69 100 111 43 122 47 53 50 84 121 100 122 48 67 99 67 102 76 68 57 51 106 71 121 67 90 110 113 55 82 115 109 108 84 100 56 82 110 49 71 89 54 65 122 77 73 100 86 51 51 81 86 97 65 70 100 85 109 47 10 86 82 100 77 50 87 89 57 118 117 101 88 102 121 119 67 119 50 110 47 103 85 108 106 65 51 101 79 87 76 53 101 110 118 114 82 86 76 90 110 72 104 43 114 67 56 78 102 97 71 120 86 72 76 73 70 51 65 116 74 90 88 48 101 10 43 65 83 110 51 73 52 68 52 77 101 68 48 51 122 100 43 68 89 73 57 65 61 61 10 45 45 45 45 45 69 78 68 32 67 69 82 84 73 70 73 67 65 84 69 45 45 45 45 45 10]
```

**What you expected to happen**:

I expected to get this:

```
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURERENDQWZTZ0F3SUJBZ0lSQUxuVSswelJPeFRGR2g2V2kySVNSQTR3RFFZSktvWklodmNOQVFFTEJRQXcKTHpFdE1Dc0dBMVVFQXhNa09XRXpNV0UzWm1FdE5qTmtNaTAwTnpnNExXRXhNMkl0WVRsbVkyUXdabVJpT1RFdwpNQjRYRFRFNE1EVXhNVEU1TkRreE4xb1hEVEl6TURVeE1ESXdORGt4TjFvd0x6RXRNQ3NHQTFVRUF4TWtPV0V6Ck1XRTNabUV0TmpOa01pMDBOemc0TFdFeE0ySXRZVGxtWTJRd1ptUmlPVEV3TUlJQklqQU5CZ2txaGtpRzl3MEIKQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBbkJ5bnBwZmx2YXFpK1dOd0NXTnV0enNGQS94TlVPMkF1UVFxcVpGdgpabFo0NStwcU5zOTVZUFc4eTdJYmU3UWVPa1E3ZEd4clYvUEhVRkxSelVsSUN1dWxNdjZKdUNkU2crUG9uRDYxCkFhMlc4SDJvSCtqUnA3VDM3Y2d5K0FmM3ovNS82UWxZU0RnS2hNTFZ6ekpoQTZmOUg2M1dWWHE3MDJlYnhGb0gKdTV2ZXVQUHpZN05QNkI2SnI4VGdtVW40UVlnOVF5bzVwOFY1Umh1OUJtN29nRTJkT213MHZBWSsxRUQyc0tRLwozZWFCVVFQWXdWUmhESFk5aXh0T05IeGViUFdqRnBmRXZQSjM1eUl5eFB1cG5nM2JGb3NqL3MwU3h4QXpobVpKClRmdGVPTTRPMHF6WTF4Z0J6bURGTWI0MWdUU2NlZE8xNzlOOW44QjZSMHQ1T3dJREFRQUJveU13SVRBT0JnTlYKSFE4QkFmOEVCQU1DQWdRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQQpINnhQSDFWY3E4dWlyUWs5VWFVN1l6WTNjV0w5MTNHK1ZqdmtwT3hrbjFaVUIrMXF0eDd3QzVIQjhYT1JwK2hECnFzQWx2cHFYSGpwelJGK3VCYzNROGk3UW9YZ1BLakFkeTdxRlRZMzhOd3EyRldCT1ZsUDFwcTlTUk81aG9YUjkKZUh4djlSNlR3NlF0UTR4MFZEbnlKdXRoRENOcE1tWU9scENvNGpVMTd1aGdENFliS3A3U0ZQT0ovbzB1ZEc4bQo0akx3aEVkbyt6LzUyVHlkejBDY0NmTEQ5M2pHeUNabnE3UnNtbFRkOFJuMUdZNkF6TUlkVjMzUVZhQUZkVW0vClZSZE0yV1k5dnVlWGZ5d0N3Mm4vZ1VsakEzZU9XTDVlbnZyUlZMWm5IaCtyQzhOZmFHeFZITElGM0F0SlpYMGUKK0FTbjNJNEQ0TWVEMDN6ZCtEWUk5QT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
```

since I assumed this field is a ""string"" because this is how it looks like in `kubectl config view --minify --flatten` output.

**How to reproduce it** (as minimally and precisely as possible): Have a current-context set in your kubeconfig and run:

    kubectl config view --minify --flatten -o=jsonpath='{.clusters[*].cluster.certificate-authority-data}'

If needed, I can also attach my kubeconfig file.



**Anything else we need to know**: Looks like a less-specific commands also causes this problem:

    kubectl config view --minify --flatten -o=jsonpath='{.clusters[*].cluster}'
    kubectl config view --minify --flatten -o=jsonpath='{.clusters[*]}'
    kubectl config view --minify --flatten -o=jsonpath='{}'

and shows the Go []byte's string representation of the CA cert field.
",closed,False,2018-05-21 20:56:03,2018-06-22 21:52:41
kubectl,jessfraz,https://github.com/kubernetes/kubectl/issues/490,https://api.github.com/repos/kubernetes/kubectl/issues/490,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",closed,False,2018-05-24 14:43:05,2018-10-01 18:01:37
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/491,https://api.github.com/repos/kubernetes/kubectl/issues/491,"Possible to bypass Service validation via ""apply"", loses declarative spec and locks down the object forever","**Kubernetes version** (use `kubectl version`): 1.10.2

**What happened**:

I have managed to have ""kubectl apply"":

1. **bypass the  corev1.Service validation** (prevents you from specifying both UDP/TCP ports if `spec.type:LoadBalancer`),
2. have items in my declarative specification **was silently ignored and not applied**,
3. break my Service object **broken and no further `apply` works anymore**.

**How to reproduce it** (as minimally and precisely as possible):

1.  ""kubectl apply"" this yaml file, notice it is denied with error `""spec.ports: Invalid value: []core.ServicePort{...}: cannot create an external load balancer with mix protocols""`

    ```yaml
    kind: Service
    apiVersion: v1
    metadata:
    name: foo-lb
    spec:
    type: LoadBalancer
    selector:
        app: foo
    ports:
    - name: foo1
        port: 1053
        targetPort: 1053
        protocol: TCP
    - name: foo2
        port: 1053
        targetPort: 1053
        protocol: UDP
    ```

2. Delete the UDP port `foo2` and retry ""kubectl apply"", it should work (as expected)

    ```yaml
    kind: Service
    apiVersion: v1
    metadata:
    name: foo-lb
    spec:
    type: LoadBalancer
    selector:
        app: foo
    ports:
    - name: foo1
        port: 1053
        targetPort: 1053
        protocol: TCP
    ```

1. Now, use the YAML back in (1) with both `foo1` and `foo2` ports, and run
   kubectl apply, it will silently succeed (**UNEXPECTED!**)

    ```
    $ kubectl apply -f foo-lb.yaml
    service ""foo-lb"" configured
    ```

1. Inspect the object, and note that:

    -  `ports` section contains only `foo1` (**UNEXPECTED!** the specified UDP
       port is ignored)
    -  `kubectl.kubernetes.io/last-applied-configuration` annotation contains
       the spec with `foo2` (UDP) port.

1. **Now here's the real problematic part:** Somehow your UDP&TCP config was
   accepted above, but let's say you're trying to update your Service spec.

   So take the YAML in Step (2) that has only `foo1` TCP port and apply it
   and you'll get this weird error that is not at all accurate:

   ```sh
   $ kubectl apply -f foo-lb.yaml
   The Service ""foo-lb"" is invalid: spec.ports: Required value

1. :warning: There is **no way to recover** from this situation (as far as I can tell) unless
   you delete & recreate this `Service`.",closed,False,2018-05-24 22:32:03,2018-12-26 15:35:46
kubectl,gianrubio,https://github.com/kubernetes/kubectl/issues/492,https://api.github.com/repos/kubernetes/kubectl/issues/492,Can we have an oficial kubectl image?,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FR

**What happened**:

Nowadays there's no official image from kubectl available, I'd expect this repo provide an image. 
I can do a PR to provide that but first I want to know if they maintainers are willing to merge it.",closed,False,2018-05-25 10:05:23,2018-10-22 18:38:18
kubectl,Netdoc41,https://github.com/kubernetes/kubectl/issues/493,https://api.github.com/repos/kubernetes/kubectl/issues/493,[Windows] Unable to open kube config file,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
issue/bug
<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.2"", GitCommit:""81753b10df112992bf51bbc2c2f85208aad78335"", GitTre
eState:""clean"", BuildDate:""2018-04-27T09:22:21Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""windows/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**: Azure ACS
- **OS** (e.g. from /etc/os-release): Mixed Windows and Linux
- **Kernel** (e.g. `uname -a`):
- **Install tools**: Tried Powershell and choco
- **Others**:


**What happened**:
kubectl : error: Error loading config file ""C:\Users\cg\.kube"": read C:\Users\cg\.kube: The handle is invalid.
At line:1 char:1
+ kubectl config view
+ ~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (error: Error lo...dle is invalid.:String) [], RemoteException
    + FullyQualifiedErrorId : NativeCommandError
 
I have installed and uninstalled and reinstalle kubectl. I have set different paths i have moved the files around. No dice.

**What you expected to happen**:
none of the action commands work even kubectl config view returns the above

**How to reproduce it** (as minimally and precisely as possible):
run kubectl config view. 

**Anything else we need to know**:
I would assume that it is some odd configuration i must have since this does not appear to be reported. But its a standard windows box. I connected up to the master in Azure and copied the config file down I also rebuilt it from scratch using kubectl commands
",open,False,2018-05-25 19:17:16,2019-03-07 13:11:06
kubectl,davidopp,https://github.com/kubernetes/kubectl/issues/494,https://api.github.com/repos/kubernetes/kubectl/issues/494,Have an option to print the URL and body of the request that will be sent to the server,"For educational purposes it would be cool if you could request to have kubectl dump to stdout or stderr the REST path and request body corresponding to the kubectl command you run. Perhaps if this were implemented it would make sense to also have a ""dry run"" mode so you would only get this output and not actually make the request to the server.
",closed,False,2018-05-27 03:22:58,2018-05-27 06:50:59
kubectl,vidhill,https://github.com/kubernetes/kubectl/issues/495,https://api.github.com/repos/kubernetes/kubectl/issues/495,Way to get list of pods running in a service,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** no

**What keywords did you search in Kubernetes issues before filing this one?** pods service
---

**Is this a BUG REPORT or FEATURE REQUEST?**: Feature request

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version**:
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.3"", GitCommit:""d2835416544f298c919e2ead3be3d0864b52323b"", GitTreeState:""clean"", BuildDate:""2018-02-07T12:22:21Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4+coreos.0"", GitCommit:""8996efde382d88f0baef1f015ae801488fcad8c4"", GitTreeState:""clean"", BuildDate:""2017-05-19T21:11:20Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

I think this is a feature request, but perhaps it does exist but isn't easy to find.
I searched quite a bit, but couldn't find an answer.

I want to get a list of pods that are selected/exposed by a particular service..

I found a workaround, using a bash script but it feels like something I would have expected to be in kubectl.

https://gist.github.com/vidhill/20d4c6ae231aa84e34078ac83e10c5f0



",closed,False,2018-06-01 21:38:15,2018-09-25 23:36:16
kubectl,DStorck,https://github.com/kubernetes/kubectl/issues/496,https://api.github.com/repos/kubernetes/kubectl/issues/496,kubectl set --help : please add examples ,"kubectl version 1.10.0
/kind documentation
/sig cli

**Is this a request for help?** : No, but it would be very helpful. The [Kubernetes references docs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#set) do not provide examples either for `kubectl set`

**What keywords did you search in Kubernetes issues before filing this one?** :` kubectl`,  `set`  - there is a [similar issue](https://github.com/kubernetes/kubectl/issues/398) filed for `kubectl config set` but I'd like to see example for `set` on its own as well

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): feature request

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

Many other kubectl commands give lots of examples with `--help` , but there are none, so it is tough to know quickly what kinds of things this can be used with. It just inconsistent, most other commands have examples. 

Current display: 
```$ kubectl set --help
Configure application resources

These commands help you make changes to existing application resources.

Available Commands:
  env            Update environment variables on a pod template
  image          Update image of a pod template
  resources      Update resource requests/limits on objects with pod templates
  selector       Set the selector on a resource
  serviceaccount Update ServiceAccount of a resource
  subject        Update User, Group or ServiceAccount in a RoleBinding/ClusterRoleBinding

Usage:
  kubectl set SUBCOMMAND [options]

Use ""kubectl <command> --help"" for more information about a given command.
Use ""kubectl options"" for a list of global command-line options (applies to all commands).```
",open,False,2018-06-01 22:22:10,2019-03-25 11:09:27
kubectl,chrissound,https://github.com/kubernetes/kubectl/issues/497,https://api.github.com/repos/kubernetes/kubectl/issues/497,"panic: interface conversion: interface {} is []interface {}, not map[string]interface {}","panic: interface conversion: interface {} is []interface {}, not map[string]interface {}

goroutine 1 [running]:
k8s.io/kubernetes/pkg/kubectl/cmd/util/openapi/validation.getObjectKind(0x14e0d20, 0xc4210343a0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xffffffffffffff01, 0xc4204058c0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/util/openapi/validation/validation.go:111 +0x539
k8s.io/kubernetes/pkg/kubectl/cmd/util/openapi/validation.(*SchemaValidation).ValidateBytes(0xc420bb14f0, 0xc4203e6700, 0x32f, 0x380, 0xc42075d450, 0x4ed384)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/util/openapi/validation/validation.go:49 +0x8f
k8s.io/kubernetes/pkg/kubectl/validation.ConjunctiveSchema.ValidateBytes(0xc420eacea0, 0x2, 0x2, 0xc4203e6700, 0x32f, 0x380, 0x4ed029, 0xc4203e6700)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/validation/schema.go:130 +0x9a
k8s.io/kubernetes/pkg/kubectl/validation.(*ConjunctiveSchema).ValidateBytes(0xc420ead100, 0xc4203e6700, 0x32f, 0x380, 0xc42075d528, 0x443693)
	<autogenerated>:3 +0x7d
k8s.io/kubernetes/pkg/kubectl/resource.ValidateSchema(0xc4203e6700, 0x32f, 0x380, 0x2190f80, 0xc420ead100, 0x20, 0xc42075d500)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:222 +0x68
k8s.io/kubernetes/pkg/kubectl/resource.(*StreamVisitor).Visit(0xc4202093c0, 0xc4202ec260, 0x2197000, 0xc4202ec300)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:543 +0x269
k8s.io/kubernetes/pkg/kubectl/resource.(*FileVisitor).Visit(0xc42041bf20, 0xc4202ec260, 0x0, 0x0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:502 +0x181
k8s.io/kubernetes/pkg/kubectl/resource.EagerVisitorList.Visit(0xc420405860, 0x1, 0x1, 0xc420131c80, 0x1, 0xc420131c80)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:211 +0x100
k8s.io/kubernetes/pkg/kubectl/resource.(*EagerVisitorList).Visit(0xc4202ec040, 0xc420131c80, 0x7f1c947194b0, 0x0)
	<autogenerated>:115 +0x69
k8s.io/kubernetes/pkg/kubectl/resource.FlattenListVisitor.Visit(0x2190d00, 0xc4202ec040, 0xc420209380, 0xc420209440, 0xc4202ec101, 0xc420209440)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:417 +0xa3
k8s.io/kubernetes/pkg/kubectl/resource.(*FlattenListVisitor).Visit(0xc4202ec060, 0xc420209440, 0x18, 0x18)
	<autogenerated>:130 +0x69
k8s.io/kubernetes/pkg/kubectl/resource.DecoratedVisitor.Visit(0x2190d80, 0xc4202ec060, 0xc4202ec100, 0x3, 0x4, 0xc4202ec1e0, 0x1, 0xc4202ec1e0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:325 +0xd8
k8s.io/kubernetes/pkg/kubectl/resource.(*DecoratedVisitor).Visit(0xc420131bc0, 0xc4202ec1e0, 0x7f1c947194b0, 0x15da60c)
	<autogenerated>:153 +0x73
k8s.io/kubernetes/pkg/kubectl/resource.ContinueOnErrorVisitor.Visit(0x2190c80, 0xc420131bc0, 0xc4208118c0, 0x7f1c947194b0, 0x0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/visitor.go:352 +0xf1
k8s.io/kubernetes/pkg/kubectl/resource.(*ContinueOnErrorVisitor).Visit(0xc420405880, 0xc4208118c0, 0x40f3f8, 0xb0)
	<autogenerated>:144 +0x60
k8s.io/kubernetes/pkg/kubectl/resource.(*Result).Visit(0xc4201feee0, 0xc4208118c0, 0x0, 0xc420131c50)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/resource/result.go:95 +0x62
k8s.io/kubernetes/pkg/kubectl/cmd.RunApply(0x21b9da0, 0xc42003eae0, 0xc420198240, 0x218fe40, 0xc42000c018, 0x218fe40, 0xc42000c020, 0xc420166930, 0x1773ee8, 0x4)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/apply.go:357 +0x70c
k8s.io/kubernetes/pkg/kubectl/cmd.NewCmdApply.func1(0xc420198240, 0xc4201aa980, 0x0, 0x2)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/apply.go:113 +0x188
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute(0xc420198240, 0xc4201aa820, 0x2, 0x2, 0xc420198240, 0xc4201aa820)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:603 +0x22b
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0xc420262b40, 0x8000104, 0x0, 0xffffffffffffffff)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:689 +0x339
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute(0xc420262b40, 0xc42003eae0, 0x218fe00)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:648 +0x2b
k8s.io/kubernetes/cmd/kubectl/app.Run(0x0, 0x0)
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubectl/app/kubectl.go:39 +0xd5
main.main()
	/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubectl/kubectl.go:26 +0x22
",closed,False,2018-06-04 14:55:31,2019-02-23 01:16:39
kubectl,neopaf,https://github.com/kubernetes/kubectl/issues/498,https://api.github.com/repos/kubernetes/kubectl/issues/498,Misleading printout of forward-ports,"**Kubernetes version** (use `kubectl version`):

```
rualpe-ws:pbrf-k8s paf$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:55:54Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.6"", GitCommit:""9f8ebd171479bec0ada837d7ee641dec2f8c6dd1"", GitTreeState:""clean"", BuildDate:""2018-03-21T15:13:31Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
rualpe-ws:pbrf-k8s paf$ 
```

**Environment**:

```
rualpe-ws:pbrf-k8s paf$ sw_vers
ProductName:	Mac OS X
ProductVersion:	10.13.4
BuildVersion:	17E202
rualpe-ws:pbrf-k8s paf$ 
```

**What happened**:

```
rualpe-ws:pbrf-k8s paf$ kubectl --namespace monitoring port-forward service/grafana 3000:80
Forwarding from 127.0.0.1:3000 -> 3000
Forwarding from [::1]:3000 -> 3000
```

**What you expected to happen**:

Since port 3000 is forwarded to port 80, I expected printout to show ""-> 80"".

**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",open,False,2018-06-05 09:15:26,2019-01-24 22:45:39
kubectl,whs-dot-hk,https://github.com/kubernetes/kubectl/issues/499,https://api.github.com/repos/kubernetes/kubectl/issues/499,Autocomplete for kubectl pod name,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
No
**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
autocomplete
---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
Feature request
<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->
Autocomplete of the very long pod name
For example, after I deploy a pod
I want to type less to get the description of the pod
So, I would like tab to work
To autocomplete the pod name for me

**Kubernetes version** (use `kubectl version`):
1.10.3

**Environment**:
- **Cloud provider or hardware configuration**: minikube
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:


**What you expected to happen**:


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",closed,False,2018-06-06 06:59:47,2018-06-09 02:01:28
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/500,https://api.github.com/repos/kubernetes/kubectl/issues/500,"""get ingress"" still shows port 80 even though it is HTTPS only.","**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT
**Kubernetes version** (use `kubectl version`): v1.10.2
**Environment**:
- **Cloud provider or hardware configuration**: GKE

I have a HTTPS only ingress that shows port 80 in kubectl get (kubectl v1.10.2):

```
# kubectl get ing
NAME       HOSTS     ADDRESS          PORTS     AGE
helloweb   *         35.227.248.205   80, 443   4m
```

I think port 80 actually isn't open:

![image](https://user-images.githubusercontent.com/159209/40799994-f488d8f6-64c3-11e8-9da5-8ff2ee86b31c.png)


describe output:

```
Name:             helloweb
Namespace:        default
Address:          35.227.248.205
Default backend:  helloweb-backend:443 (10.32.4.7:8443)
TLS:
  yourdomain-tls terminates
Rules:
  Host  Path  Backends
  ----  ----  --------
  *     *     helloweb-backend:443 (10.32.4.7:8443)
Annotations:
  ingress.kubernetes.io/backends:                    {""k8s-be-30921--906590fce6179ef9"":""Unknown""}
  ingress.kubernetes.io/https-forwarding-rule:       k8s-fws-default-helloweb--906590fce6179ef9
  ingress.kubernetes.io/https-target-proxy:          k8s-tps-default-helloweb--906590fce6179ef9
  ingress.kubernetes.io/ssl-cert:                    k8s-ssl-09cce9ee44983641-6d1404f893135fac--906590fce6179ef9
  ingress.kubernetes.io/url-map:                     k8s-um-default-helloweb--906590fce6179ef9
  kubectl.kubernetes.io/last-applied-configuration:  {""apiVersion"":""extensions/v1beta1"",""kind"":""Ingress"",""metadata"":{""annotations"":{""kubernetes.io/ingress.allow-http"":""false""},""labels"":{""app"":""hello""},""name"":""helloweb"",""namespace"":""default""},""spec"":{""backend"":{""serviceName"":""helloweb-backend"",""servicePort"":443},""tls"":[{""secretName"":""yourdomain-tls""}]}}

  kubernetes.io/ingress.allow-http:  false
Events:
  Type    Reason   Age              From                     Message
  ----    ------   ----             ----                     -------
  Normal  ADD      3m               loadbalancer-controller  default/helloweb
  Normal  CREATE   1m               loadbalancer-controller  ip: 35.227.248.205
  Normal  Service  1m (x3 over 1m)  loadbalancer-controller  default backend set to helloweb-backend:30921
```

My YAML:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: helloweb
  labels:
    app: hello
  annotations:
    kubernetes.io/ingress.allow-http: ""false"" # disable HTTP
spec:
  tls:
    - secretName: yourdomain-tls
  backend:
    serviceName: helloweb-backend
    servicePort: 443
```

This looks like a `kubectl` bug that it just assumes port 80 is open for all ingresses?",open,False,2018-06-06 17:18:25,2019-01-23 21:12:26
kubectl,neoel,https://github.com/kubernetes/kubectl/issues/501,https://api.github.com/repos/kubernetes/kubectl/issues/501,Allow [kubectl config set-cluster] to specify certificate-authority data in command.,"Currently kubectl only allows a certificate to be specified from the filesystem, it's possible to embed the certificate in the configuration file. 
I'd like to be able to pass the certificate base64 encoded directly with `kubectl config set-cluster`.
This will allow me to generate a configuration command without having to write the cert to the filesystem.

@k8s-mirror-cli-feature-requests ",open,False,2018-06-20 09:29:36,2019-03-15 05:34:42
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/502,https://api.github.com/repos/kubernetes/kubectl/issues/502,"--output=go-template returning weird (""<no value>"") string, breaking integration","kubectl v1.10.4

The following command prints `<no value>` to the screen:

```
kubectl config view --minify --flatten \
  -o=go-template='{{(index (index .clusters 0).cluster ""certificate-authority-data"")}}'
```

when the ""clusters"" field in the kubeconfig doesn't have the `""certificate-authority-data""` field, for example:

```
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://localhost:6443
  name: docker-for-desktop-cluster
```

`-o=go-template` should NOT cause a weird string value like `""<no value>""` to be printed to stdout. Instead this should print nothing and exit.

This is breaking integrations like `kubectl [...] | base64 --decode`, which works when a value is present, but `<no value>` is not an expected output from the specified template.",open,False,2018-06-22 19:04:46,2018-12-25 10:38:47
kubectl,chrissound,https://github.com/kubernetes/kubectl/issues/503,https://api.github.com/repos/kubernetes/kubectl/issues/503,kubectl set image does not work on initContainers,"kubectl version

Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.6"", GitCommit:""9f8ebd171479bec0ada837d7ee641dec2f8c6dd1"", GitTreeState:""clean"", BuildDate:""2018-03-21T15:21:50Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.6"", GitCommit:""9f8ebd171479bec0ada837d7ee641dec2f8c6dd1"", GitTreeState:""clean"", BuildDate:""2018-03-21T15:13:31Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}


```
kubectl set image deployment/abcxyz abcxyz=""123""
Error from server (NotFound): deployments.extensions ""abcxyz"" not found
```

When a container by that name does exist - but it is defined in 'initContainers'.",closed,False,2018-06-27 19:59:55,2019-01-17 13:18:05
kubectl,max8899,https://github.com/kubernetes/kubectl/pull/504,https://api.github.com/repos/kubernetes/kubectl/issues/504,fix some typos,taht -> that,closed,True,2018-06-28 05:39:28,2018-07-02 16:56:17
kubectl,gkarthiks,https://github.com/kubernetes/kubectl/issues/505,https://api.github.com/repos/kubernetes/kubectl/issues/505,kubectl go-template 'eq' not working with integers,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT

I am running the below kubectl command with go-template as output format with some conditional checks. I am evaluating the value of a variable to 1, but it fails and throws me below error. 


### Error Output

> error: error executing template ""{{if .status.initContainerStatuses}}{{range .status.initContainerStatuses}}{{if .state.terminated}}{{\""Found a terminated container\\n\""}}{{.state.terminated.exitCode}}{{\""\\n\""}}{{if eq .state.terminated.exitCode 1}}{{\""Now if is working \\n\""}}{{end}}{{end}}{{end}}{{end}}"": template: output:1:178: executing ""output"" at <eq .state.terminated...>: error calling eq: incompatible types for comparison

My kubectl command is like below

> kubectl get pod <pod_name> -n <namespace> -o go-template='{{if .status.initContainerStatuses}}{{range .status.initContainerStatuses}}{{if .state.terminated}}{{""Found a terminated container\n""}}{{.state.terminated.exitCode}}{{""\n""}}{{if eq .state.terminated.exitCode 1}}{{""Now if is working \n""}}{{end}}{{end}}{{end}}{{end}}'



**Kubernetes version** (use `kubectl version`):
Client Version: v1.10.0
Server Version: v1.9.1+a0ce1bc657

**Environment**:
- **Cloud provider or hardware configuration**: OpenShift
- **OS** (e.g. from /etc/os-release): Mac
- **Kernel** (e.g. `uname -a`): Darwin
- **Install tools**: brew

**What happened**:
Got the comparison error for ` eq .state.terminated.exitCode 1`

**What you expected to happen**:
Executes without error.

**How to reproduce it** (as minimally and precisely as possible):
execute the above command for a init container terminated pod or comapare any int values in go-template.

**Anything else we need to know**:

",closed,False,2018-07-02 23:01:35,2019-01-07 20:09:26
kubectl,mlda065,https://github.com/kubernetes/kubectl/issues/506,https://api.github.com/repos/kubernetes/kubectl/issues/506,`get nodes -w` works differently to all other `get x -w`,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?**: No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

* `get nodes`
---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

Bug report

**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.0"", GitCommit:""91e7b4fd31fcd3d5f436da26c980becec37ceefe"", GitTreeState:""clean"", BuildDate:""2018-06-27T20:17:28Z"", GoVersion:""go1.10.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:05:37Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:

I've tried two environments, with the same behaviour happening on both

* Azure Kubernetes Service
  * ""basic networking""
  * no RBAC
  * disabled ""HTTP Application Routing""
  * Australian region
  * v1.10.3
* Openstack, deployed with rancher
   * Rancher v2.0.4
   * Mirantis Openstack
   * Ubuntu 16.04
   * v1.10.3
   * I don't know the other details of this deployment. I'm just a user of Rancher and Openstack. Other people in my company manage them. I can go ask around for hardware etc if required, but since the same behaviour happens on Azure, I think it's not important.

**What happened**:

`kubectl get nodes -w` continually prints duplicates of previous lines. Even though the nodes are not changing state, every second or so a new line is printed. 

This is inconsistent. It is different behaviour to `kubectl get pods -w`, `kubectl get services -w`, `kubectl get deployments -w` etc.

```
$ k get nodes -w
NAME                       STATUS    ROLES     AGE       VERSION
aks-agentpool-23999449-0   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-1   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-2   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-3   Ready     agent     7m        v1.10.3
aks-agentpool-23999449-1   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-2   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-3   Ready     agent     7m        v1.10.3
aks-agentpool-23999449-0   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-1   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-2   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-3   Ready     agent     7m        v1.10.3
aks-agentpool-23999449-0   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-1   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-2   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-3   Ready     agent     7m        v1.10.3
aks-agentpool-23999449-0   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-1   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-2   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-3   Ready     agent     8m        v1.10.3
aks-agentpool-23999449-0   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-1   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-2   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-3   Ready     agent     8m        v1.10.3
```

**What you expected to happen**:

A list of nodes is printed. Nothing else is printed, except for nodes changing state or new nodes being added

```
$ k get nodes -w
NAME                       STATUS    ROLES     AGE       VERSION
aks-agentpool-23999449-0   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-1   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-2   Ready     agent     22h       v1.10.3
aks-agentpool-23999449-3   Ready     agent     7m        v1.10.3
```

and **nothing else** (unless I'm adding new nodes, or a node fails, gets upgraded etc)


**How to reproduce it** (as minimally and precisely as possible):

`kubectl get nodes -w`

**Anything else we need to know**:

I want to run `kubectl get nodes -w` and then add a new node. I expect to see the list of existing nodes, and then see the new node being privisioned. It's hard to see what's happening to the new node (if anything) because my stdout is flooded with redundant info about the old nodes which haven't changed at all.",closed,False,2018-07-05 01:47:08,2018-07-27 01:50:16
kubectl,carlosonunez,https://github.com/kubernetes/kubectl/issues/507,https://api.github.com/repos/kubernetes/kubectl/issues/507,`kubectl config` seems to hash file names when --client-certificate is given a path,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`): 1.10.2


**Environment**:
- **Cloud provider or hardware configuration**: AWS `m3.medium`
- **OS** (e.g. from /etc/os-release): Ubuntu 18.04
- **Kernel** (e.g. `uname -a`): `Linux ad33c053e9d5 4.4.0-128-generic #154-Ubuntu SMP Fri May 25 14:15:18 UTC 2018 x86_64 Linux`
- **Install tools**: 
- **Others**:


**What happened**:

Trying to run `set-credentials` for a given cluster fails when I give `--client-certificate` a path that is not in the current working directory.

The resultant error looked like this:

```
error: error reading client-certificate data from /data/6a9444ae3eb1.pem: open /data/6a9444ae3eb1.pem: no such file or directory
```

when `--client-certificate` was set to `/data/ca.pem`.

I'm able to read the file correctly, and this does not occur when I set `--client-certificate` to `ca.pem`. However, the documentation suggests that I should be able to provide a path.

This does not happen with `set-cluster`.

**What you expected to happen**:

The command below should have returned 0:

```bash
kubectl config set-cluster carlosnunez.me-develop         --certificate-authority=""/data/ca.pem""         --embed-certs=true         --server=https://34.235.65.81,:6443    --kubeconfig=hostname.kubeconfig &&   kubectl config set-credentials system:node:hostname       --client-certificate=""/data/hostname,pem""             --client-key=""/data/hostname-key.pem""      --embed-certs=true         --kubeconfig=hostname.kubeconfig &&  kubectl config set-context default         --cluster=carlosnunez.me-develop                --user=system:node:hostname             --kubeconfig=hostname.kubeconfig &&  kubectl config use-context default --kubeconfig=hostname.kubeconfig &&  mv hostname.kubeconfig /data
```

**How to reproduce it** (as minimally and precisely as possible):

1. Create worker and controller certificates per [these instructions](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md)
2. Run the command above.


**Anything else we need to know**:

I ran the above command in a Docker container. This happened while bootstrapping the cluster, so no cluster was available during the time at which this bug occurred.

",closed,False,2018-07-06 07:46:07,2018-07-06 08:46:02
kubectl,mlda065,https://github.com/kubernetes/kubectl/issues/508,https://api.github.com/repos/kubernetes/kubectl/issues/508,add non-plural option kubectl config get-context (no trailing s),"tldr: Add `kubectl config get-context` as a synonym for `kubectl config get-contexts`

<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?**: No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): 

* `get-context`

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

Feature Request

**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.0"", GitCommit:""91e7b4fd31fcd3d5f436da26c980becec37ceefe"", GitTreeState:""clean"", BuildDate:""2018-06-27T20:17:28Z"", GoVersion:""go1.10.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.6"", GitCommit:""9f8ebd171479bec0ada837d7ee641dec2f8c6dd1"", GitTreeState:""clean"", BuildDate:""2018-03-21T15:13:31Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

* If you want info about pods, you say 
   - `kubectl get pods` or
   - `kubectl get pod`
* If you want info about services, you say 
   - `kubectl get services` or
   - `kubectl get service`
* If you want info about deployments, you say 
   - `kubectl get deployments` or
   - `kubectl get deployment`
* If you want info about contexts, you say `kubectl config get-contexts`, but you *cannot* say `kubectl config get-context`.

May I suggest making `get-contexts` plural-agnostic, like everything else?

I'd also like to suggest changing the syntax entirely from `kubectl config get-context` to `kubectl get contexts`, but that's probably a more audacious change.",open,False,2018-07-09 06:21:35,2019-03-25 11:09:25
kubectl,aleofreddi,https://github.com/kubernetes/kubectl/issues/509,https://api.github.com/repos/kubernetes/kubectl/issues/509,kubectl convert -o name fails when using stdin,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
`name` `stdin` `error: list types are not supported by name printing: *v1.List`

---

**Is this a BUG REPORT or FEATURE REQUEST?** BUG

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.0"", GitCommit:""91e7b4fd31fcd3d5f436da26c980becec37ceefe"", GitTreeState:""clean"", BuildDate:""2018-06-27T22:29:25Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""darwin/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** macOSX
- **Kernel** `Darwin Andreas-MacBook-Pro.local 17.6.0 Darwin Kernel Version 17.6.0: Tue May  8 15:22:16 PDT 2018; root:xnu-4570.61.1~1/RELEASE_X86_64 x86_64`
- **Install tools**: `brew`
- **Others**:


**What happened**:

```
$ curl -s https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/deployment.yaml > /tmp/deploy.yaml; kubectl convert -o name -f /tmp/deploy.yaml 
deployment.apps/nginx-deployment
$ curl -s https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/deployment.yaml | kubectl convert -o name -f -
error: list types are not supported by name printing: *v1.List
```

**What you expected to happen**:

```
$ curl -s https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/deployment.yaml > /tmp/deploy.yaml; kubectl convert -o name -f /tmp/deploy.yaml 
deployment.apps/nginx-deployment
$ curl -s https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/deployment.yaml | kubectl convert -o name -f -
deployment.apps/nginx-deployment
```

**How to reproduce it** (as minimally and precisely as possible):

`-o name` with any stdin input manifest will do

**Anything else we need to know**:

Version 1.9.2 is working as expected.",open,False,2018-07-09 14:55:47,2019-03-25 11:09:24
kubectl,dj13may91,https://github.com/kubernetes/kubectl/issues/510,https://api.github.com/repos/kubernetes/kubectl/issues/510,Kubectl top nodes/pods background url,"I wanted to know how kubectl top nodes or kubectl top pods fetches the information as in what api does it hit. i couldn't find it in the kubernetes api docs : https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/
neither there is any function in the library : https://github.com/kubernetes-client/python/blob/master/kubernetes/README.md
",closed,False,2018-07-17 06:03:13,2018-07-19 09:55:48
kubectl,mengqiy,https://github.com/kubernetes/kubectl/issues/511,https://api.github.com/repos/kubernetes/kubectl/issues/511,kubectl skew tests failing on GKE,"skew-cluster-stable1-kubectl-latest-gke and skew-cluster-stable1-kubectl-latest-gke-serial start to fail last night.
https://k8s-testgrid.appspot.com/sig-cli-master#skew-cluster-stable1-kubectl-latest-gke
and 
https://k8s-testgrid.appspot.com/sig-cli-master#skew-cluster-stable1-kubectl-latest-gke-serial

It seems we are having issue bringing up a GKE cluster.

/kind bug
/sig cli
",closed,False,2018-07-18 16:47:23,2018-07-24 22:18:25
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/512,https://api.github.com/repos/kubernetes/kubectl/issues/512,"--user not showing in help for ""create {cluster}rolebinding""","**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

kubectl 1.11.0

`--user` flag not showing in help for: 
- `kubectl create clusterrolebinding --help`
- `kubectl create rolebinding --help`

commands.

Example:

```
$ kubectl create clusterrolebinding -h
Create a ClusterRoleBinding for a particular ClusterRole.

Examples:
  # Create a ClusterRoleBinding for user1, user2, and group1 using the cluster-admin ClusterRole
  kubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 --user=user2 --group=group1

Options:
      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in
the template. Only applies to golang and jsonpath output formats.
      --clusterrole='': ClusterRole this ClusterRoleBinding should reference
      --dry-run=false: If true, only print the object that would be sent, without sending it.
      --generator='clusterrolebinding.rbac.authorization.k8s.io/v1alpha1': The name of the API generator to use.
      --group=[]: Groups to bind to the role
  -o, --output='': Output format. One of:
json|yaml|wide|name|custom-columns=...|custom-columns-file=...|go-template=...|go-template-file=...|jsonpath=...|jsonpath-file=...
See custom columns [http://kubernetes.io/docs/user-guide/kubectl-overview/#custom-columns], golang template
[http://golang.org/pkg/text/template/#pkg-overview] and jsonpath template
[http://kubernetes.io/docs/user-guide/jsonpath].
      --save-config=false: If true, the configuration of current object will be saved in its annotation. Otherwise, the
annotation will be unchanged. This flag is useful when you want to perform kubectl apply on this object in the future.
      --serviceaccount=[]: Service accounts to bind to the role, in the format <namespace>:<name>
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The
template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].
      --validate=true: If true, use a schema to validate the input before sending it

Usage:
  kubectl create clusterrolebinding NAME --clusterrole=NAME [--user=username] [--group=groupname]
[--serviceaccount=namespace:serviceaccountname] [--dry-run] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```",open,False,2018-07-19 01:26:56,2019-01-24 21:49:34
kubectl,dj13may91,https://github.com/kubernetes/kubectl/issues/513,https://api.github.com/repos/kubernetes/kubectl/issues/513,How can i get  API KEY of my kubernetes cluster to call authorised apis ?,"https://github.com/kubernetes-client/java/blob/master/kubernetes/README.md

ApiClient defaultClient = Configuration.getDefaultApiClient();
        
        // Configure API key authorization: BearerToken
        ApiKeyAuth BearerToken = (ApiKeyAuth) defaultClient.getAuthentication(""BearerToken"");
        BearerToken.setApiKey**(""YOUR API KEY"")**;
        // Uncomment the following line to set a prefix for the API key, e.g. ""Token"" (defaults to null)
        //BearerToken.setApiKeyPrefix(""Token"");

        ApisApi apiInstance = new ApisApi();
        try {
            V1APIGroupList result = apiInstance.getAPIVersions();
            System.out.println(result);
        } catch (ApiException e) {
            System.err.println(""Exception when calling ApisApi#getAPIVersions"");
            e.printStackTrace();
        }

while running :  /api/v1/namespaces/{namespace}/pods/{name}/exec
i get not authorised.

Where can i get this ""YOUR API KEY""? 
found below but is not working : 
 kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d '\t'

It gives some token but i am not able to proceed with this. Error: 

io.kubernetes.client.ApiException: Bad Request
	at io.kubernetes.client.ApiClient.handleResponse(ApiClient.java:882)
	at io.kubernetes.client.ApiClient.execute(ApiClient.java:798)
	at io.kubernetes.client.apis.CoreV1Api.connectGetNamespacedPodExecWithHttpInfo(CoreV1Api.java:1258)....",closed,False,2018-07-19 09:50:50,2018-09-26 00:07:26
kubectl,cbluth,https://github.com/kubernetes/kubectl/issues/514,https://api.github.com/repos/kubernetes/kubectl/issues/514,Create ConfigMap from http source,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): `""from-file"" ""http""`

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
`FEATURE REQUEST`
<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.4"", GitCommit:""5ca598b4ba5abb89bb773071ce452e33fb66339d"", GitTreeState:""clean"", BuildDate:""2018-06-06T08:13:03Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.2"", GitCommit:""81753b10df112992bf51bbc2c2f85208aad78335"", GitTreeState:""clean"", BuildDate:""2018-04-27T09:10:24Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```


**Environment**:
- **Cloud provider or hardware configuration**: 
bare-metal, 3 masters, 6 worker nodes, kubespray
- **OS** (e.g. from /etc/os-release):
```
root@master1:~# cat /etc/os-release 
NAME=""Ubuntu""
VERSION=""16.04.4 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.4 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
root@master1:~# 
```
- **Kernel** (e.g. `uname -a`):
`Linux master1 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux`

- **Install tools**:
- **Others**:


**What happened**:
```
cbluth@dl7490:~/x$ kubectl -n platform create configmap nets-config --from-file=https://raw.githubusercontent.com/pires/kubernetes-nats-cluster/master/nats.conf
error: error reading https://raw.githubusercontent.com/pires/kubernetes-nats-cluster/master/nats.conf: no such file or directory
cbluth@dl7490:~/x$
```

**What you expected to happen**: I thought this would work


**How to reproduce it** (as minimally and precisely as possible):


**Anything else we need to know**:

",open,False,2018-07-25 12:34:48,2019-01-07 09:31:38
kubectl,Pehesi97,https://github.com/kubernetes/kubectl/issues/515,https://api.github.com/repos/kubernetes/kubectl/issues/515,--recursive option with wildcards or regexes,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): recursive resource management kubernetes

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): Feature request

<!--
If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

I think it would be nice if we could specify a wildcard or regex when using `kubectl [command] -f --recursive`, like: `kubectl apply -f . -R --regex *-resources.yml`, so kubectl wouldn't try to parse other .yml files on file tree.

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:17:39Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:05:37Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**: Local environment, Macbook Pro 2015
- **OS** (e.g. from /etc/os-release): MacOS 10.13.1 High Sierra
- **Kernel** (e.g. `uname -a`): 17.2.0 Darwin Kernel Version 17.2.0: Fri Sep 29 18:27:05 PDT 2017; root:xnu-4570.20.62~3/RELEASE_X86_64 x86_64

**What happened**:
When I tried to apply some configs using the `--recursive` option, kubectl tried to parse some other .yml files on my file tree.

**What you expected to happen**:
I expected an option that allowed me to specify what files I should ignore or lookup for.

**How to reproduce it** (as minimally and precisely as possible):
`kubectl apply -f . -R` in any directory that has kubernetes resource manifests and other .yml or .json files.

**Anything else we need to know**:",closed,False,2018-07-26 20:31:39,2018-07-27 02:09:29
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/516,https://api.github.com/repos/kubernetes/kubectl/issues/516,[WIP] Kubectl main out of core repo with vendored dependencies,"* To build `kubectl` binary (top-level directory): `go build kubectl.go`
* Pulls only one file (`cmd/kubectl/kubectl.go`, `kubectl` main) out of the core repo.
* Doesn't move any other `kubectl` code.
* Vendors all remaining `kubectl` dependencies.
* Allows decomposition of `kubectl`, including some `kubectl` subcommands in different repos.
* Allows for multiple ownership of pieces of `kubectl`.
* Allows more frequent releases, and different release cadences.
* Simplifies testing.
",closed,True,2018-07-31 22:47:19,2018-09-20 10:11:29
kubectl,seans3,https://github.com/kubernetes/kubectl/issues/517,https://api.github.com/repos/kubernetes/kubectl/issues/517,additionalPrinterColumns for CRD's doesn't work in k8s 1.11 for columns with array data,"**Kubernetes version** (use `kubectl version`): 1.11 (server-side printing)

**Environment**:
- **Cloud provider or hardware configuration**: GKE 

**Bug**
Reproduce using the following steps:

1) Create  CRD:

crd.yaml
```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: foos.example.istio.io
spec:
  group: example.istio.io
  names:
    kind: Foo
    listKind: FooList
    plural: foos
    singular: foo
  scope: Namespaced
  version: v1alpha1
  additionalPrinterColumns:
  - JSONPath: .spec.servers[*].hosts
    name: hosts
    type: string

```
```
$ kubectl apply -f crd.yaml
```
2) Create an instance of the CRD:

crd-instance.yaml
```
apiVersion: example.istio.io/v1alpha1
kind: Foo
metadata:
  name: foo0
spec:
  servers:
  - hosts:
    - foo.example.com
    - bar.example.com
  - hosts:
    - baz.example.com
```

```
$ kubectl apply -f crd-instance.yaml
```

3) Print instance of CRD:

```
$ kubectl get foos foo0
NAME      HOSTS
foo0      [foo.example.com bar.example.com]
```

### EXPECTED

Notice that only the first array of hosts is printed (missing second array `[baz.example.com]`). We would expect that both arrays would have been printed. If we specifically request the JSONPath, then it prints correctly.

```
$ kubectl get foos foo0 -o jsonpath='{.spec.servers[*].hosts}'
[foo.example.com bar.example.com] [baz.example.com]
```",open,False,2018-08-03 20:20:32,2019-03-25 11:09:23
kubectl,JaroVojtek,https://github.com/kubernetes/kubectl/issues/518,https://api.github.com/repos/kubernetes/kubectl/issues/518,kubectl not working properly : unable to upgrade connection: Unauthorized,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`): client/server: v1.11.0


**Environment**:
- **Cloud provider or hardware configuration**:  On Premise
- **OS** (e.g. from /etc/os-release):
NAME=""CentOS Linux""
VERSION=""7 (Core)""
- **Kernel** (e.g. `uname -a`):
3.10.0-862.9.1.el7.x86_64
- **Install tools**:
- **Others**:


**What happened**:

I have generated my own certificates for kubernetes cluster and stored them in /etc/kubernetes/pki/ folder before kubernetes cluster initialization

Kubernetes cluster initialization went without problems and all kube-system pods are up and running.

However i am not able to perform some commands with kubeclt like 

```
kubectl logs weave-net-qfk8c -n kube-system -c ""weave""
error: You must be logged in to the server (the server has asked for the client to provide credentials ( pods/log weave-net-qfk8c))

[root@qa053 kubernetes]# kubectl -n kube-system port-forward tiller-deploy-759cb9df9-rltzf 44134
error: error upgrading connection: unable to upgrade connection: Unauthorized

```

I have checked /etc/kubernetes/manifests/kube-apiserver.yaml file and I can see proper link to certificates
```
--client-ca-file=/etc/kubernetes/pki/ca.crt

--kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
--kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
```

**What you expected to happen**:
wokring normally as certificates are avilable in those paths

**How to reproduce it** (as minimally and precisely as possible):

**Anything else we need to know**:

",closed,False,2018-08-10 13:13:50,2019-01-23 21:13:08
kubectl,psreed,https://github.com/kubernetes/kubectl/issues/519,https://api.github.com/repos/kubernetes/kubectl/issues/519,[Windows] kubectl run not accepting valid JSON as --override on Windows (same JSON works on Mac) ,"/kind bug

**What happened**:
- ""kubectl run"" with --overrides is rejected with ""error: Invalid JSON Patch"" on WINDOWS
- exact same command verified works fine on MacOS

**What you expected to happen**:
Patch applied to my kubectl run. See proper expected output from Mac OS run below.

**How to reproduce it (as minimally and precisely as possible)**:
```
kubectl run pfa --image=myimage --overrides='{""apiVersion"":""apps/v1beta1"",""spec"":{""template"":{""spec"":{""imagePullSecrets"":[{""name"":""myregkey""}]}}}}' --dry-run -o yaml
```
**Anything else we need to know?**:
Output on Windows:
```
error: Invalid JSON Patch
```

Output on Windows (with -v6 added to the end of the command):
```
I0810 12:11:06.986876  135292 round_trippers.go:405] GET https://192.168.99.100:8443/apis/apps/v1beta1?timeout=32s 200 OK in 11 milliseconds
I0810 12:11:06.987852  135292 loader.go:359] Config loaded from file C:\Users\paul/.kube/config
F0810 12:11:06.992733  135292 helpers.go:119] error: Invalid JSON Patch
```

Output on MacOS:
```apiVersion: apps/v1beta1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    run: pfa
  name: pfa
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      run: pfa
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: pfa
    spec:
      containers:
      - image: myimage
        imagePullPolicy: Always
        name: pfa
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: myregkey
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status: {}
```


**Environment**:
- Kubernetes version (use `kubectl version`):
Windows Version:
```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.2"", GitCommit:""bb9ffb1654d4a729bb4cec18ff088eacc153c239"", GitTreeState:""clean"", BuildDate:""2018-08-07T23:17:28Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""windows/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:44:10Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

Mac OS Version:
```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.2"", GitCommit:""bb9ffb1654d4a729bb4cec18ff088eacc153c239"", GitTreeState:""clean"", BuildDate:""2018-08-08T16:31:10Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.0"", GitCommit:""fc32d2f3698e36b93322a3465f63a14e9f0eaead"", GitTreeState:""clean"", BuildDate:""2018-03-26T16:44:10Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Cloud provider or hardware configuration:
using minikube 0.28.2 with --vm-driver=virtualbox in both Windows and Mac OS cases.

- OS (e.g. from /etc/os-release):
Windows 10 & MacOS
- Kernel (e.g. `uname -a`):
Darwin <my_machine_name> 17.7.0 Darwin Kernel Version 17.7.0: Thu Jun 21 22:53:14 PDT 2018; root:xnu-4570.71.2~1/RELEASE_X86_64 x86_64

- Install tools:
minikube and kubernetes-cli installed using brew on MacOS, and using chocolatey on Windows.
- Others:
",open,False,2018-08-10 16:42:25,2019-01-08 01:44:48
kubectl,adampl,https://github.com/kubernetes/kubectl/issues/520,https://api.github.com/repos/kubernetes/kubectl/issues/520,kubectl apply doesn't update nodeSelector,"**BUG REPORT**

**Kubernetes version**:
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.2"", GitCommit:""bb9ffb1654d4a729bb4cec18ff088eacc153c239"", GitTreeState:""clean"", BuildDate:""2018-08-07T23:17:28Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.1"", GitCommit:""b1b29978270dc22fecc592ac55d903350454310a"", GitTreeState:""clean"", BuildDate:""2018-07-17T18:43:26Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Hardware configuration**: bare metal x86_64
- **OS**: Ubuntu 16.04.5 LTS
- **Kernel**: 4.4.0-131-generic
- **Install tools**: Rancher 2.0

**What happened**:
When I apply a modified YAML file for an existing Deployment, all changes are received by the cluster except `nodeSelector`.

**What you expected to happen**:
`nodeSelector` should be updated.

**How to reproduce it**:
1. Write a deployment file with some node selector in the pod spec template
1. Apply the deployment: `kubectl apply -f path/to/file.yaml` (should create the deployment)
1. Modify the nodeSelector (eg. add new entry) and another parameter (eg. Replicas)
1. Again apply the deployment: `kubectl apply -f path/to/file.yaml`
1. Observe that other changes are reflected, but nodeSelector stays unchanged
",open,False,2018-08-11 17:53:20,2019-01-24 18:13:33
kubectl,donbowman,https://github.com/kubernetes/kubectl/issues/521,https://api.github.com/repos/kubernetes/kubectl/issues/521,Kubectl exec output is not binary clean,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): NO

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): binary

---

**Is this a BUG REPORT or FEATURE REQUEST?** BUG REPORT
<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.1"", GitCommit:""b1b29978270dc22fecc592ac55d903350454310a"", GitTreeState:""clean"", BuildDate:""2018-07-17T18:53:20Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.2"", GitCommit:""bb9ffb1654d4a729bb4cec18ff088eacc153c239"", GitTreeState:""clean"", BuildDate:""2018-08-07T23:08:19Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: kube-spawn
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

```
NAME=""Ubuntu""
VERSION=""18.04.1 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.1 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```

**What happened**:

```
$ kubectl exec -it POD sum /bin/bash
50572  1088

$ kubectl exec -it POD cat /bin/bash > /tmp/bash; sum /tmp/bash
34079  1092
```

**What you expected to happen**:

I expect that the output path of kubectl exec is binary clean.

**How to reproduce it** (as minimally and precisely as possible):

Run the kubectl exec commands above.

**Anything else we need to know**:

",closed,False,2018-08-14 12:49:19,2018-09-26 17:13:29
kubectl,dnltsk,https://github.com/kubernetes/kubectl/issues/522,https://api.github.com/repos/kubernetes/kubectl/issues/522,kubectl get service doesn't work without -w,"`kubectl get service my-service -w` works fine and outputs

`my-service   LoadBalancer   10.0.xxx.xxx   104.46.xxx.xxx   3030:31053/TCP   37m`

The same command without `-w` or `--watch` outputs the following exception message

```
No resources found.
Error from server (NotAcceptable): unknown (get services my-service)
```

A similar exception message is also outputted when firing other commands like `kubectl get pods` or `kubectl get nodes`

The Kubernetes cluster is running on AKS. I'm using OSX High Sierra.

```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.2"", GitCommit:""bb9ffb1654d4a729bb4cec18ff088eacc153c239"", GitTreeState:""clean"", BuildDate:""2018-08-07T23:17:28Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.7"", GitCommit:""8e1552342355496b62754e61ad5f802a0f3f1fa7"", GitTreeState:""clean"", BuildDate:""2017-09-28T23:56:03Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

How can I solve this?",closed,False,2018-08-17 00:44:14,2018-09-26 00:13:53
kubectl,juanvallejo,https://github.com/kubernetes/kubectl/issues/523,https://api.github.com/repos/kubernetes/kubectl/issues/523,The `patch` command should not require a KUBECONFIG when using --local,"The `kubectl patch` command currently fails when there is no kubeconfig present, no `--config` flag is provided, and `--local` is given:

```bash
# ensure no kubeconfig exists
$ rm ~/.kube/config
# run patch without hitting the server
$ kubectl patch /path/to/obj.yaml --local --type=json -o json
error: Missing or incomplete configuration info.  Please login or point to an existing, complete config file:

  1. Via the command-line flag --config
  2. Via the KUBECONFIG environment variable
  3. In your home directory as ~/.kube/config

To view or setup config directly use the 'config' command.
```

Since we are not hitting the server, no kubeconfig should be needed.
This error [occurs here](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/cmd/patch.go#L156) when attempting to retrieve a namespace in the command's Complete method.

We should be looking up the namespace much later in the command, and only if not running as `--local`.
We could consider adding a `NamespaceFunc` field to the command's options struct that is called when needed in the Run method.

cc @deads2k @soltysh 
/sig cli",closed,False,2018-08-20 14:37:33,2018-08-24 13:47:29
kubectl,izelnakri,https://github.com/kubernetes/kubectl/issues/524,https://api.github.com/repos/kubernetes/kubectl/issues/524,Add ANSI colors to kubectl describe and other outputs,"**FEATURE REQUEST**:

Currently kubectl outputs are pretty boring without a color:

![kubetest_ _-bash_ _181x31](https://user-images.githubusercontent.com/1190931/44599951-29bfa480-a7d8-11e8-9e0e-1aa6186bd3ec.jpg)

Adding color to them will make them easier to read. Each key could be colored based on its indentation level. Each key/value pair in labels,annotations,taints etc could be colored differently as well.


",open,False,2018-08-24 18:00:33,2019-02-16 00:10:25
kubectl,johanot,https://github.com/kubernetes/kubectl/issues/525,https://api.github.com/repos/kubernetes/kubectl/issues/525,"Add --cascade or --force flag to kubectl delete namespace, for non-empty namespaces","Today, if you drain a node, kubectl will prompt you for confirmation in the form of `--ignore-daemonsets` and `--delete-local-data` if applicable. This is fine and a nice guard for doing something dangerous.

Along the same lines, I would like some sort of `--force` or `--cascade` flag for deleting namespaces, in case the namespace contains objects. This would imply `kubectl get -n <namespace> all` before the actual delete is executed.

/kind feature
/sig cli

**What happened**:
My namespace and all objects within it was deleted without confirmation or questioning.

**What you expected to happen**:
A warning or bail out asking for a confirmation or a ""I-know-what-I'm-doing-flag` before proceeding.

**How to reproduce it** (as minimally and precisely as possible):
`kubectl delete ns <any-namespace>`
",open,False,2018-08-27 08:29:04,2019-01-18 19:19:31
kubectl,imran21,https://github.com/kubernetes/kubectl/issues/526,https://api.github.com/repos/kubernetes/kubectl/issues/526,unexpected error during validation: unable to resolve Kubernetes cluster API URL dns: lookup api.clust1.kube.devopsimran.com on 172.31.0.2:53: server misbehaving,"In Route53 the A record is not created. 
When I run
 ./kops validate cluster
Its getting error as below

 <unexpected error during validation: unable to resolve Kubernetes cluster API URL dns: lookup api.clust1.kube.devopsimran.com on 172.31.0.2:53: server misbehaving>
my nodes in EC2 is all creating fine and successfully but i cant to anything on ( ./kops validate cluster) and (kubectl)

Could anyone help me to solve this.",closed,False,2018-08-29 08:21:02,2018-08-29 10:06:06
kubectl,schollii,https://github.com/kubernetes/kubectl/issues/527,https://api.github.com/repos/kubernetes/kubectl/issues/527,"Need a real ""get-all"" command","**FEATURE REQUEST**:

Add a get-all command that really shows *all* objects, with some flags to filter on certain types (namespaced vs non-namespaced, etc). 

Similarly, the dashboard should have a mode to show all resources (again, dashboard does not show networkpolicy objects, not even in pod descriptions to say ""this pod is selected by this networkpolicy object"". But dashboard improvement should be implemented separately, as the most important and urgent one is the kubectl new command. 

**Kubernetes version** (use `kubectl version`):

1.11.2

**What happened**:

`kubectl get all` only shows a small subset of kubernetes objects in a cluster, and there does not seem to be a command to get all objects (secrets, network policies, etc). This caused me hours of wasted time because I was trying to replicate a deployment object from another cluster into a new cluster, and didn't see that there was a networkpolicy needed for the service to expose that deployment. 

**What you expected to happen**:

That `kubect get all` would show all, not just a small number of types of resources. 

**How to reproduce it** (as minimally and precisely as possible):

Create a networkpolicy then do `kubectl get all`, it doesn't show up. However, it does show up with `kubectl get netpol` and `kubectl api-resources -o name   | xargs -t -n 1 kubectl get --ignore-not-found --show-kind | grep SOMETHING_IN_NAME`
",open,False,2018-08-29 13:23:40,2019-03-07 04:13:45
kubectl,schollii,https://github.com/kubernetes/kubectl/issues/528,https://api.github.com/repos/kubernetes/kubectl/issues/528,`kubectl describe pod/xxx` should show what resources selected the pod,"**FEATURE REQUEST**:

Add a flag or command that shows which objects have selected a specified pod, or for each pod, what objects selected it. For example, if I create a network policy with a selector, then each pod that matches this selector should show this in its description, something like this: 

Selected-by: 
   networkpolicy/name-of-policy
   service/name-of-service

Similarly, the dashboard should show that information. But dashboard improvement should be implemented separately, as the most important and urgent one is the kubectl extension. 

**Kubernetes version** (use `kubectl version`):

1.11.2

**What happened**:

`kubectl describe pod/name` does not show that there are networkpolicy that selected it, this caused hours of wasted time because I was trying to replicate a deployment object from another cluster into a new cluster, and didn't see that there was a networkpolicy needed for the service to expose that deployment's pods. 

",open,False,2018-08-29 13:32:41,2019-03-25 11:09:23
kubectl,cblecker,https://github.com/kubernetes/kubectl/issues/529,https://api.github.com/repos/kubernetes/kubectl/issues/529,kubernetes/kubectl: Switch to lgtm/approve for merge,"Hi @pwittrock / @soltysh,
Are you okay if we swapped this repo to use the lgtm/approve plugins as the merge driver for tide, rather than github reviews? This would help with a consistent experience across the org. Right now, the lgtm and approval plugins are enabled, but the labels mean nothing to tide -- it's only looking for a github review from someone with write access to merge.

https://github.com/kubernetes/test-infra/blob/fb63779200943f9dc4888206f07a6a4511365025/prow/config.yaml#L347-L355

Thoughts?

cc: @kubernetes/sig-cli-maintainers @kubernetes/sig-testing ",closed,False,2018-08-30 23:21:22,2018-09-04 14:28:49
kubectl,thinkerbot,https://github.com/kubernetes/kubectl/issues/530,https://api.github.com/repos/kubernetes/kubectl/issues/530,JsonPath errors printed on stdout,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): jsonpath stderr stdout

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): bug report

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.2"", GitCommit:""bb9ffb1654d4a729bb4cec18ff088eacc153c239"", GitTreeState:""clean"", BuildDate:""2018-08-08T16:31:10Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:05:37Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Docker for Mac
- **OS** (e.g. from /etc/os-release): Mac 
- **Kernel** (e.g. `uname -a`):  `Darwin ... 17.7.0 Darwin Kernel Version 17.7.0: Thu Jun 21 22:53:14 PDT 2018; root:xnu-4570.71.2~1/RELEASE_X86_64 x86_64`
- **Install tools**: brew
- **Others**:


**What happened**:

A `kubectl get` with a bad jsonpath causes error output on stdout.

**What you expected to happen**:

Error output should go to stderr.

**How to reproduce it** (as minimally and precisely as possible):

Run the following in which I'm redirecting stderr to /dev/null.  What you see is the output on stdout; all this output should be on stderr and stdout be empty.

```bash
kubectl get services -o jsonpath='${bad}' 2>/dev/null
# Error executing template: unrecognized identifier bad. Printing more information for debugging the template:
# 	template was:
# 		${bad}
# 	object given to jsonpath engine was:
# 		map[string]interface {}...
```

Notably half the error output DOES goes to stderr.  

```bash
kubectl get services -o jsonpath='${bad}' >/dev/null
# error: error executing jsonpath ""${bad}"": unrecognized identifier bad
```

**Anything else we need to know**:

",closed,False,2018-08-31 13:38:59,2018-10-19 11:57:33
kubectl,KeZhang,https://github.com/kubernetes/kubectl/issues/531,https://api.github.com/repos/kubernetes/kubectl/issues/531,"If the cluster time is not same with the client time, the ""Age"" is incorrect","There is some NTP issue in my cluster machine, it delay **6 hours** for the stand time. When I use the kubectl to get the info of resources, I found the **""Age""** always show **>6 Hours**.
When I login into the k8s cluster master node machine, its time is delay just delay **6 hours** compare to my local desktop time

There is a little confused when I just create a new resource, that make me think I didn't create correctly.


**Kubernetes version** (use `kubectl version`):

`
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.8"", GitCommit:""bc6162cc70b4a39a7f39391564e0dd0be60b39e9"", GitTreeState:""clean"", BuildDate:""2017-10-05T06:54:19Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.2"", GitCommit:""bb9ffb1654d4a729bb4cec18ff088eacc153c239"",GitTreeState:""clean"", BuildDate:""2018-08-07T23:08:19Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
`
 

**What happened**:

""Age"" is time may incorrect
  
**What you expected to happen**:

""Age"" is time may incorrect

**How to reproduce it** (as minimally and precisely as possible):

Make the kubectl client and cluster system time different.

**Anything else we need to know**:

",closed,False,2018-09-04 03:49:55,2018-09-06 07:47:57
kubectl,terrynie,https://github.com/kubernetes/kubectl/issues/532,https://api.github.com/repos/kubernetes/kubectl/issues/532,Bug -- kubectl exec con't kill the process sometimes,"Kubernetes version: 

Client Version: version.Info{Major:""1"", Minor:""5"", GitVersion:""v1.5.1"", GitCommit:""82450d03cb057bab0950214ef122b67c83fb11df"", GitTreeState:""clean"", BuildDate:""2016-12-14T00:57:05Z"", GoVersion:""go1.7.4"", Compiler:""gc"", Platform:""darwin/amd64""}

Amazon ec2
Mac OSX 10.14 Beta (18A326h)
Homebrew

**What happened**:
![1111536895239_ pic_hd](https://user-images.githubusercontent.com/5274043/45528358-76643180-b812-11e8-8226-298d2b4b5af1.jpg)

when the command `kubectl  exec` terminated, the port is still in use sometimes.

**What you expected to happen**:
It should kill the process and release the port.

",closed,False,2018-09-14 03:40:42,2018-09-14 06:57:44
kubectl,arno-joule,https://github.com/kubernetes/kubectl/issues/533,https://api.github.com/repos/kubernetes/kubectl/issues/533,Request Header asking wrong media type: returns 406,"**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-10T11:44:36Z"", GoVersion:""go1.11"", Compiler:""gc"", Platform:""darwin/amd64""}

Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.11"", GitCommit:""1df6a8381669a6c753f79cb31ca2e3d57ee7c8a3"", GitTreeState:""clean"", BuildDate:""2018-04-05T17:16:46Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**: Rancher v2.0.8 / Running on linux VM in Azure
- **OS** (e.g. from /etc/os-release): 
- **Kernel** (e.g. `uname -a`): 
- **Install tools**: 
- **Others**:


**What happened**: when running `kubectl get pods -n istio-system` I get 
```
No resources found.
Error from server (NotAcceptable): unknown (get pods)
```

**What you expected to happen**: when running `kubectl get pods -n istio-system` I expect
```
NAME                                        READY     STATUS      RESTARTS   AGE
istio-citadel-686f89db46-wbf7w              1/1       Running     0          1h
istio-cleanup-secrets-2rjzc                 0/1       Completed   0          1h
istio-egressgateway-6b74548d99-xbnss        1/1       Running     0          1h
istio-galley-85b9b48569-dcqdp               1/1       Running     0          1h
istio-ingressgateway-5b8f967748-6s5w5       1/1       Running     0          1h
istio-pilot-68c89487bf-hrqrp                2/2       Running     0          1h
istio-policy-8f49956fc-ndzd8                2/2       Running     0          1h
istio-sidecar-injector-7466999545-hl6td     1/1       Running     0          59m
istio-statsd-prom-bridge-58f8596c67-64gfh   1/1       Running     0          1h
istio-telemetry-6b99f6ff66-x6glp            2/2       Running     0          1h
istio-tracing-75d76fb9f-jnlf8               1/1       Running     0          1h
prometheus-884dbbcd5-fhvct                  1/1       Running     0          1h
```

**How to reproduce it** (as minimally and precisely as possible):
1. With an up-to-date rancher v2.0.8 using nginx for ingress
2. With an local install of kubectl v 1.11.3
3. Retrieve the kubeconfig file and put it in ~/.kube
4. Execute `kubectl get pods -n istio-system`

**Anything else we need to know**:
* I've been looking at similar error either in kubectl or Rancher but none turned out
* The issue seems to be bound with the specific server, yet the it's a request that seems to be causing the issue
* I investigated the error further using -v8 I get a 406 error with the following Response Body: 
```
{
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""only the following media types are accepted: application/json, application/yaml, application/vnd.kubernetes.protobuf"",
  ""reason"": ""NotAcceptable"",
  ""code"": 406
}
```
The request header accept media type is for some reason `Accept: application/json;as=Table;v=v1beta1;g=meta.k8s.io, application/json` whereas all other requests are `Accept: application/json, */*`

",closed,False,2018-09-19 18:47:34,2018-09-26 00:22:35
kubectl,adubkov,https://github.com/kubernetes/kubectl/issues/534,https://api.github.com/repos/kubernetes/kubectl/issues/534,"Long run of ""kubectl run"" does not refresh IAM token","**BUG REPORT**

**Kubernetes version** : v1.11.1

**Environment**:
- **Cloud provider or hardware configuration**: Kubernetes cluster on EKS, kubectl on Mac

**What happened**:

When I'm running container with `--stdin=true` on EKS cluster for 20-30min, container app complete  successful, but `kubectl` return an error:
```
$ time kubectl run --rm -i sleep --restart=Never --image=ubuntu:precise -- sleep 999
If you don't see a command prompt, try pressing enter.
pod ""sleep"" deleted
error: You must be logged in to the server (the server has asked for the client to provide credentials (get pods))

real	16m41.928s
user	0m0.332s
sys	0m0.125s
```

It seems that `kubectl run` does internally call `get pods` and it doesn't refresh IAM token.

**What you expected to happen**:
Check creds exprication ttl and refresh them as needed.

**How to reproduce it** (as minimally and precisely as possible):

```
$ time kubectl run --rm -i sleep --restart=Never --image=ubuntu:precise -- sleep 999
```

",open,False,2018-09-19 19:57:08,2019-01-30 17:31:17
kubectl,J-Siu,https://github.com/kubernetes/kubectl/issues/535,https://api.github.com/repos/kubernetes/kubectl/issues/535,Selector does not work for service.,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-09T18:02:47Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-09T17:53:03Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
- **Cloud provider or hardware configuration**: VirtualBox
- **OS** (e.g. from /etc/os-release): Ubuntu 18.04.1 LTS
- **Kernel** (e.g. `uname -a`): Linux u64s01 4.15.0-34-generic #37-Ubuntu SMP Mon Aug 27 15:21:48 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**: apt

```sh
apt-get update && apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl
```

- **Others**:


**What happened**:

Selector does not work for service. 

**What you expected to happen**:

Selector should work for service.

**How to reproduce it** (as minimally and precisely as possible):

1. Deploy ghost:

```
kubectl run ghost --image=ghost --port 2368 --expose=true
```

2. Verify service:

```
$ kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE       SELECTOR
ghost        NodePort    10.100.72.171   <none>        2368:30538/TCP   1d        run=ghost
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP          1d        <none>
```

3. Try with selector:

```
$ kubectl get svc -o wide -l run=ghost
No resources found.

$ kubectl delete svc -l run=ghost
No resources found
```

**Anything else we need to know**:

It work if I select k8s-app
```
$ kubectl get svc -o wide --all-namespaces
NAMESPACE     NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE       SELECTOR
default       ghost           NodePort    10.100.72.171   <none>        2368:30538/TCP   1d        run=ghost
default       kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP          1d        <none>
kube-system   kube-dns        ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP    1d        k8s-app=kube-dns
kube-system   tiller-deploy   ClusterIP   10.104.224.90   <none>        44134/TCP        9h        app=helm,name=tiller

$ kubectl get svc -o wide --all-namespaces -l k8s-app=kube-dns
NAMESPACE     NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE       SELECTOR
kube-system   kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP   1d        k8s-app=kube-dns

$ kubectl get svc -o wide --all-namespaces -l run=ghost
No resources found.
```",closed,False,2018-09-23 08:17:41,2018-10-06 11:30:08
kubectl,Tibingeo,https://github.com/kubernetes/kubectl/issues/536,https://api.github.com/repos/kubernetes/kubectl/issues/536,kubectl command sleeps after 5 minutes,"---

**BUG REPORT**


**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.2"", GitCommit:""81753b10df112992bf51bbc2c2f85208aad78335"", GitTreeState:""clean"", BuildDate:""2018-04-27T09:22:21Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.5"", GitCommit:""cce11c6a185279d037023e02ac5249e14daa22bf"", GitTreeState:""clean"", BuildDate:""2017-12-07T16:05:18Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```



**What happened**:
I am running bash script into a pod which  have a sleep command which will sleep for 10 minutes, after that 10 minutes kubectl command should be exited but it is not happening.

I tried logging into pod and checked that bash script is complete already.

**What you expected to happen**:
In ideal scenario once the bash script is complete even kubectl command should exit with proper exit code. 

**How to reproduce it** (as minimally and precisely as possible):
`/tmp/script.sh` should be in pod which should contain bash command `sleep 10m`
Run below command
`kubectl -s localhost:8086 exec pod-name -- /bin/bash -c /tmp/script.sh`

After 10 minutes of sleep  bash script will be exited with exit code 0, but `kubectl` is not getting exited.

Same scenario I have tried with `sleep 4m` in the `/tmp/script.sh` after 4 minutes of sleep both script command will be  exited with exit code 0.


To add more context it will work as expected if my `/tmp/script.sh` is 
```
for i in {1..400}
do 
    echo $i
done
```
That means if some process is running in pod then `kubectl` will exit on script completion but if pod had no process to run for more than 5 minutes than if issue is happening.",open,False,2018-09-24 15:22:37,2019-01-23 21:09:54
kubectl,raravena80,https://github.com/kubernetes/kubectl/issues/537,https://api.github.com/repos/kubernetes/kubectl/issues/537,Command default config rules for kubectl (perhaps in ~/.kube/config),"
---

/kind feature
/sig cli

**Kubernetes version** (use `kubectl version`):
All

**Environment**:
- **Cloud provider or hardware configuration**: N/A
- **OS** (e.g. from /etc/os-release): N/A
- **Kernel** (e.g. `uname -a`): N/A
- **Install tools**: kubectl
- **Others**:


**What happened**:

We would like to have rules for kubectl that allow you to specify default configs for certain commands

For example:

`kubectl get pods`

The default for `--show-all` is `true`

**What you expected to happen**:

But what if want to run the same but with the default `--show-all=false` without specifying it like this:

`kubectl get pods --show-all=false`. I just want to run it like this

`kubectl get pods`

**How to reproduce it** (as minimally and precisely as possible):

`kubectl get pods --show-all=false`

**Anything else we need to know**:

It would be nice to have a generic structure in the config for all the default config options.

Some background [here](https://stackoverflow.com/questions/52488362/hide-completed-and-other-finished-pods-by-default)",open,False,2018-09-25 01:11:39,2019-03-25 16:56:07
kubectl,mxey,https://github.com/kubernetes/kubectl/issues/538,https://api.github.com/repos/kubernetes/kubectl/issues/538,port-forward should support named ports,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

FEATURE REQUEST

**Kubernetes version** (use `kubectl version`): v1.11.3


**Environment**:
- **Cloud provider or hardware configuration**: Libvirt
- **OS** (e.g. from /etc/os-release): Client on macOS 10.14.0, server CentOS 7
- **Kernel** (e.g. `uname -a`): 3.10.0-862.9.1.el7.x86_64
- **Install tools**: kubeadm
- **Others**:


**What happened**:

```
$ kubectl port-forward http-server http
error: Error parsing local port 'http': strconv.ParseUint: parsing ""http"": invalid syntax
```

**What you expected to happen**:

```
$ kubectl port-forward http-server http
Forwarding from 127.0.0.1:8080 -> 8080
Forwarding from [::1]:8080 -> 8080
```

**How to reproduce it** (as minimally and precisely as possible):

pod.yaml:
```
apiVersion: v1
kind: Pod
metadata:
  name: http-server
spec:
  containers:
  - image: gcr.io/google_containers/defaultbackend:1.4
    imagePullPolicy: IfNotPresent
    name: default-http-backend
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
```

```
kubectl create -f pod.yaml
kubectl port-forward  http-server web
```



",closed,False,2018-09-25 13:24:20,2018-10-10 23:46:36
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/539,https://api.github.com/repos/kubernetes/kubectl/issues/539,[Plugin] [1.12] plugin binaries are overshadowed by themselves if they appear in PATH twice,"**Kubernetes version** (use `kubectl version`): 1.12.0-beta.2

**What happened**:

I have added a directory that contains my plugins to my $PATH twice, like:

```
export PATH=""$HOME/.krew/bin:$PATH:$HOME/.krew/bin""
```

then I ran `kubectl plugin list` only to find out the plugins are overshadowed by themselves, e.g.

```
[...]
/Users/ahmetb/.krew/bin/kubectl-open-svc
  - warning: /Users/ahmetb/.krew/bin/kubectl-open-svc is overshadowed by a similarly named plugin: /Users/ahmetb/.krew/bin/kubectl-open-svc
```
(not the paths are exact)

Also `kubectl open-svc` doesn't work because of this now.

**What you expected to happen**:

Deduplication of exact path names.

**How to reproduce it** (as minimally and precisely as possible):

1. Create an executable `kubectl-foo` in a dir
2. add that dir to $PATH twice
3. run `kubectl plugin list`.
4. Observe plugins are overshadowed by exactly themselves.

/cc @soltysh
^ apologies if this is not the correct person to cc.
/cc @lbb

",closed,False,2018-09-25 18:34:16,2018-09-27 19:01:04
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/540,https://api.github.com/repos/kubernetes/kubectl/issues/540,[1.12] plugin commands with dashes cannot be invoked with their original names,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT
**Kubernetes version** (use `kubectl version`): 1.12.0-beta.2

**What happened**:

There are already plugins named with dashes (`-`) in the plugin name, such as:
- `open-svc`
- `ca-cert`
- `rm-standalone-pods`

in the [krew package manager](https://github.com/GoogleContainerTools/krew-index/tree/4bcf63c95a3f42a37fbaca1d735f2efb981d4d65/plugins). There are also many plugins **outside** krew, such as:

- [decode-secret](https://github.com/brosandilabs/kubectl-decode-secret)
- [rbac-lookup](https://github.com/reactiveops/rbac-lookup)
- ...

kubectl 1.12 splits the names of these plugins into separate subcommands, such as:

| previously called | now called as |
|--|--|
| `kubectl plugin open-svc` | `kubectl open svc [...]` |
| `kubectl plugin rm-standalone-pod` | `kubectl rm standalone pod [...]` |

**What you expected to happen**:

This new behavior is quite unexpected for me. This makes it:

1. impossible to write plugins with dashes in the name

     For example, if I write a plugin named `port-forward-better`, my users have to invoke it as `kubectl port forward better` (not desirable) as opposed to `kubectl port-forward-better`.

1. Undesired categorization of plugins that share the same prefix:

     For example, if I'm writing a plugin named `rm-standalone-pods` installing a plugin with another `rm-` prefix (such as `rm-completed-jobs`). They will be (someday) grouped under `kubectl rm` despite they're different plugins.

**I Expected:** if a plugin named `kubectl-delete-all` exists, I should be able to call it `kubectl delete-all`. Being able to call it as `kubectl delete all` _may_ still be ok as the artifact of the feature, but it should not prevent `kubectl delete-all` invocation.


I am sorry I wasn't there to review the design but I think design won't work for the broader plugin ecosystem.

/cc @soltysh 
/cc @lbb",closed,False,2018-09-25 18:46:35,2018-09-26 02:38:32
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/541,https://api.github.com/repos/kubernetes/kubectl/issues/541,Adds initial required SECURITY_CONTACTS file,"* Initial SECURITY_CONTACTS for the kubectl repo.
* Uses required template.
* These are the same contacts as every other kubernetes repo for now.",closed,True,2018-09-26 18:32:07,2018-09-28 20:16:15
kubectl,DazWilkin,https://github.com/kubernetes/kubectl/issues/542,https://api.github.com/repos/kubernetes/kubectl/issues/542,`kubectl get events --watch` appears borked with non-default output formats,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

`is:issue ""kubectl get events""`

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`): v1.10.7-gke.2


**Environment**:
- **Cloud provider or hardware configuration**: Kubernetes Engine
- **OS** (e.g. from /etc/os-release): COS
- **Kernel** (e.g. `uname -a`): 4.14.56+
- **Install tools**:
- **Others**:


**What happened**:

```
kubectl get events --all-namespaces --watch --output=jsonpath=""{.message}""
kubectl get events --all-namespaces --watch --output=json
kubectl get events --all-namespaces --watch --output=yaml
kubectl get events --all-namespaces --watch --output=custom-columns=MESSAGE:.message
```
(Appears to) correctly stream a series of recent events, then blocks awaiting subsequent events. On the occurrence of the next event, the command terminates.

**What you expected to happen**:

I expect the command to behave consistently with+without the output specifier (`output=wide` works). 

```
kubectl get events --all-namespaces --watch
kubectl get events --all-namespaces --watch --output=wide
```
I had originally been using a `range` command in the JSON output but realized this probably is erroneous (non-sensical) when the events are being watched:
```
kubectl get events --output=jsonpath=""{range .items[*]}{.message}\n{end}"" --watch
```

**How to reproduce it** (as minimally and precisely as possible):
See examples above.

**Anything else we need to know**:
A reason for wanting to do this is -- as is shown -- to be able more conveniently filter the output results. In this case, I'm playing around with the node-problem-detector and I'm only interested in seeing the `message` value in order to determine that the utility is working.
",open,False,2018-09-28 00:10:05,2019-02-25 13:21:42
kubectl,tech509201941,https://github.com/kubernetes/kubectl/issues/543,https://api.github.com/repos/kubernetes/kubectl/issues/543,Can't access cluster via kubeconfig: json access token error,"I try to connect from a docker ubuntu image to my gcloud kubernetes cluster. For whatever reason kubectl get pods results in ""error executing access token command"":

`kubectl --kubeconfig kubernetes/access/config get pods
`
```
Unable to connect to the server: error executing access token command
""/google/google-cloud-sdk/bin/gcloud config config-helper --format=json"":
```

`err=fork/exec /google/google-cloud-sdk/bin/gcloud: no such file or directory output= stderr=`

I generated my kube config via:

`gcloud container clusters get-credentials cluster_name --region=europe-north1`

It looks likes this:

```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0..
    server: https://IP
  name: gke_project_name_europe-north1_proto-cluster-ha-1
contexts:
- context:
    cluster: gke_project_name_europe-north1_proto-cluster-ha-1
    user: gke_project_name_europe-north1_proto-cluster-ha-1
  name: gke_project_name_europe-north1_proto-cluster-ha-1
current-context: gke_project_name_europe-north1_proto-cluster-ha-1
kind: Config
preferences: {}
users:
- name: cluster-admin
  user:
    token: bearer_token
- name: gke_project_name_europe-north1_proto-cluster-ha-1
  user:
    auth-provider:
      config:
        access-token: ya29...
        cmd-args: config config-helper --format=json
        cmd-path: /google/google-cloud-sdk/bin/gcloud
        expiry: 2018-10-05T13:58:25Z
        expiry-key: '{.credential.token_expiry}' # Is this correct?
        token-key: '{.credential.access_token}' # Is this correct?
      name: gcp
- name: kubernetes-admin
  user: {}

```
Has anyone an idea on how to fix this json access token error?
Thanks a lot :)",open,False,2018-10-06 14:22:57,2019-01-07 18:49:05
kubectl,tech509201941,https://github.com/kubernetes/kubectl/issues/544,https://api.github.com/repos/kubernetes/kubectl/issues/544,Can't access successfully created context,"I want to create a kubectl context within my docker container.
This fails, although kubectl responds that ""Context ""kubernetes-admin"" created."":

```
$ kubectl config --kubeconfig=SenecaMeshBase/kubernetes/access/config set-context kubernetes-admin
Context ""kubernetes-admin"" created.
$ kubectl config get-contexts
CURRENT   NAME   CLUSTER   AUTHINFO   NAMESPACE
$ kubectl config use-context kubernetes-admin
error: no context exists with the name: ""kubernetes-admin"".
ERROR: Job failed: error executing remote command: command terminated with non-zero exit code: Error executing in Docker Container: 1
```

How can this be the case?
Thanks a lot :)",closed,False,2018-10-06 14:25:34,2019-03-28 01:46:35
kubectl,hebbarsushma,https://github.com/kubernetes/kubectl/issues/545,https://api.github.com/repos/kubernetes/kubectl/issues/545,"kubectl get pod -n default  -l "" ""  output behaviour ","
 FEATURE REQUEST
*Kubernetes version** (use `kubectl version`):all

**What happened**:
>kubectl get pod -n default -l  "" ""    
Above command lists all the pod present in default namespace


**What you expected to happen**:
Since -l "" "" does not match with any of the pod  label  command  should produce  output as
>No resources found . 

Can anyone explain this behaviour.





",closed,False,2018-10-15 07:40:45,2018-10-15 16:06:33
kubectl,mingfang,https://github.com/kubernetes/kubectl/issues/546,https://api.github.com/repos/kubernetes/kubectl/issues/546,kubectl explain crd --recursive goes into an infinite loop,"The reason is because the visitor at path spec.validation.openAPIV3Schema.allOf will reference an object type that references itself, causing the visitor to go into an infinite loop.

The visitor needs some of kind of loop detection.",closed,False,2018-10-20 05:32:00,2018-10-31 00:41:23
kubectl,mingfang,https://github.com/kubernetes/kubectl/issues/547,https://api.github.com/repos/kubernetes/kubectl/issues/547,Move the CRDFinder to the cli-runtime repo,The CRDFinder and perhaps others in the util package are generic and very useful to be part of the cli-runtime https://github.com/kubernetes/cli-runtime repo,closed,False,2018-10-20 15:26:23,2018-10-20 16:07:40
kubectl,Ark-kun,https://github.com/kubernetes/kubectl/issues/548,https://api.github.com/repos/kubernetes/kubectl/issues/548,kubectl get --all-namespaces does not see namespaced pod in some cases,"BUG REPORT

**Kubernetes version** (use `kubectl version`):
```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-09T18:02:47Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.7-gke.6"", GitCommit:""9b635efce81582e1da13b35a7aa539c0ccb32987"", GitTreeState:""clean"", BuildDate:""2018-08-16T21:33:47Z"", GoVersion:""go1.9.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: GKE
- **OS** (e.g. from /etc/os-release): Debian

**What happened**:
After we moved pods to a non-default namespaces, some kubectl invocations can no longer find the pods.

$ kubectl get pod --all-namespaces | grep pod-7957d899d9-yyyy
kubeflow      pod-7957d899d9-7gzn4                                  1/1       Running     0          14m

$ kubectl get pod --all-namespaces pod-7957d899d9-yyyy
error: a resource cannot be retrieved by name across all namespaces

**What you expected to happen**:
I expect kubectl get <kind> <name> result to be consistent with kubectl get <kind> 


**How to reproduce it** (as minimally and precisely as possible):
Create some pods in non-dewfault namespace and try to use kubectl get on their names.

",open,False,2018-10-23 03:22:05,2019-03-24 19:54:20
kubectl,mml,https://github.com/kubernetes/kubectl/issues/549,https://api.github.com/repos/kubernetes/kubectl/issues/549,'kubectl get cs' is misleading when the apiserver is unreachable,"```console
% kubectl --server=127.0.0.1:8080 get cs
the server doesn't have a resource type ""cs""
% kubectl --server=127.0.0.1:8080 get componentstatuses
The connection to the server 127.0.0.1:8080 was refused - did you specify the right host or port?
```

The main problem is that the server is unreachable, which we should surface in both cases.",closed,False,2018-10-24 20:39:57,2018-10-31 22:41:00
kubectl,seans3,https://github.com/kubernetes/kubectl/issues/550,https://api.github.com/repos/kubernetes/kubectl/issues/550,kubectl expose crashes when there are no selectors (apps group only),"**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""13+"", GitVersion:""v1.13.0-alpha.2.185+f698f0fe179f8c"", GitCommit:""f698f0fe179f8cfbd95fe0d248bf994afd49a04f"", GitTreeState:""clean"", BuildDate:""2018-10-29T23:49:32Z"", GoVersion:""go1.11.1"", Compiler:""gc"", Platform:""linux/amd64""}
```
This bug can be reproduced back to `kubectl version 1.9.7`

**Environment**:
Google Kubernetes Engine: GKE -- not used. `kubectl` never gets to the point of talking to the server.

**How to Reproduce**

`kubectl expose -f test/fixtures/pkg/kubectl/cmd/convert/appsdeployment.yaml --port 80`

The bug occurs for workloads in the **apps** group (NOTE: **extensions/v1beta1** works).

The **apps/v1** deployment yaml looks like (NOTE: no selectors, only pod template labels):

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
```

The output is:

```
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x1302b31]

goroutine 1 [running]:
k8s.io/kubernetes/pkg/kubectl/polymorphichelpers.mapBasedSelectorForObject(0x19d0440, 0xc000258900, 0x17d6301, 0x8, 0xc00021c858, 0x1)
	/usr/local/google/home/seans/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/polymorphichelpers/mapbasedselectorforobject.go:69 +0x1b1
k8s.io/kubernetes/pkg/kubectl/cmd/expose.(*ExposeServiceOptions).RunExpose.func1(0xc00028fce0, 0x0, 0x0, 0x0, 0x0)
	/usr/local/google/home/seans/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/pkg/kubectl/cmd/expose/expose.go:255 +0x1630
k8s.io/kubernetes/vendor/k8s.io/cli-runtime/pkg/genericclioptions/resource.DecoratedVisitor.Visit.func1(0xc00028fce0, 0x0, 0x0, 0xc0008913a8, 0x86fb1e)
	/usr/local/google/home/seans/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/vendor/k8s.io/cli-runtime/pkg/genericclioptions/resource/visitor.go:315 +0xdc
...
```

This bug is caused by a nil pointer dereference for missing selector in the deployment or replicaset spec. In **extensions/v1beta1** defaulting, the pod template labels are copied into the selectors. This defaulting does not happen for the **apps** group (e.g. **apps/v1**).",closed,False,2018-10-30 00:05:43,2018-10-31 21:48:17
kubectl,raravena80,https://github.com/kubernetes/kubectl/issues/551,https://api.github.com/repos/kubernetes/kubectl/issues/551,"rsync behavior to copy files from pod to pod, perhaps for `kubectl cp`","<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

/sig cli
/area kubectl
/kind feature

Right now looks like `kubectl cp` only allows copying local/remote and remote/local. This works for most cases by what if somebody wants to sync files from pod to pod

It would be nice to have something like this:

`kubectl cp <namespace1>/<pod1>:/tmp/foo.txt <namespace2>/<pod1>:/tmp/foo.txt`

This would probably also require a change in the `kube-apiserver`.

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

```$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.1"", GitCommit:""4ed3216f3ec431b140b1d899130a69fc671678f4"", GitTreeState:""clean"", BuildDate:""2018-10-05T16:46:06Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.1"", GitCommit:""4ed3216f3ec431b140b1d899130a69fc671678f4"", GitTreeState:""clean"", BuildDate:""2018-10-05T16:36:14Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Any
- **OS** (e.g. from /etc/os-release): Any
- **Kernel** (e.g. `uname -a`): Any
- **Install tools**: kubectl
- **Others**: N/A


**What happened**:

`kubectl cp` supports local/remote or remote/local

**What you expected to happen**:

Support remote/remote

**How to reproduce it** (as minimally and precisely as possible):

Run any `kubectl cp` command.

**Anything else we need to know**:

Some background [info](https://stackoverflow.com/questions/53070742/how-to-copy-files-from-pod-in-one-namespace-to-pod-in-another-namespace)
",open,False,2018-10-30 19:44:30,2019-03-28 12:10:06
kubectl,troyswanson,https://github.com/kubernetes/kubectl/issues/552,https://api.github.com/repos/kubernetes/kubectl/issues/552,kubectl version: needs better error message when server not available,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

BUG REPORT


**Kubernetes version**:

v1.12.2


**Environment**:
- **Cloud provider or hardware configuration**: MacBook Pro 2017
- **OS**: MacOS High Sierra
- **Kernel**: Darwin Kernel Version 17.7.0
- **Install tools**: Homebrew
- **Others**: N/A


**What happened**:

When issuing the `kubectl version` command, in addition to the version information, an error was produced, as well as an output of some HTML.

```
~ $ kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.2"", GitCommit:""17c77c7898218073f14c8d573582e8d2313dc740"", GitTreeState:""clean"", BuildDate:""2018-10-30T21:39:38Z"", GoVersion:""go1.11.1"", Compiler:""gc"", Platform:""darwin/amd64""}
error: got '<!DOCTYPE html>
<html lang=""en"" dir=""ltr"">
<meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8"">
<meta name=""robots"" content=""noindex"">
<title>Login - Adminer</title>
<link rel=""stylesheet"" type=""text/css"" href=""version?file=default.css&amp;version=4.6.3"">
<script src='version?file=functions.js&amp;version=4.6.3' nonce=""OTM5NDUyNzJhMzgzMmU1MjQ0MDc1MTJhMTIzOWZmYWE=""></script>
<link rel=""shortcut icon"" type=""image/x-icon"" href=""version?file=favicon.ico&amp;version=4.6.3"">
<link rel=""apple-touch-icon"" href=""version?file=favicon.ico&amp;version=4.6.3"">

<body class=""ltr nojs"">
<script nonce=""OTM5NDUyNzJhMzgzMmU1MjQ0MDc1MTJhMTIzOWZmYWE="">
mixin(document.body, {onkeydown: bodyKeydown, onclick: bodyClick, onload: partial(verifyVersion, '4.6.3', 'version?', '944539:431296')});
document.body.className = document.body.className.replace(/ nojs/, ' js');
var offlineMessage = 'You are offline.';
var thousandsSeparator = ',';
</script>

<div id=""help"" class=""jush-sql jsonly hidden""></div>
<script nonce=""OTM5NDUyNzJhMzgzMmU1MjQ0MDc1MTJhMTIzOWZmYWE="">mixin(qs('#help'), {onmouseover: function () { helpOpen = 1; }, onmouseout: helpMouseout});</script>

<div id=""content"">
<h2>Login</h2>
<div id='ajaxstatus' class='jsonly hidden'></div>
<form action='' method='post'>
<div></div>
<table cellspacing='0'>
<tr><th>System<td><select name='auth[driver]'><option value=""server"" selected>MySQL<option value=""sqlite"">SQLite 3<option value=""sqlite2"">SQLite 2<option value=""pgsql"">PostgreSQL<option value=""oracle"">Oracle (beta)<option value=""mssql"">MS SQL (beta)<option value=""firebird"">Firebird (alpha)<option value=""simpledb"">SimpleDB<option value=""mongo"">MongoDB<option value=""elastic"">Elasticsearch (beta)</select>
<tr><th>Server<td><input name=""auth[server]"" value=""db"" title=""hostname[:port]"" placeholder=""localhost"" autocapitalize=""off"">
<tr><th>Username<td><input name=""auth[username]"" id=""username"" value="""" autocapitalize=""off""><script nonce=""OTM5NDUyNzJhMzgzMmU1MjQ0MDc1MTJhMTIzOWZmYWE="">focus(qs('#username'));</script>
<tr><th>Password<td><input type=""password"" name=""auth[password]"">
<tr><th>Database<td><input name=""auth[db]"" value="""" autocapitalize=""off"">
</table>
<p><input type='submit' value='Login'>
<label><input type='checkbox' name='auth[permanent]' value='1'>Permanent login</label>
</form>
</div>

<form action='' method='post'>
<div id='lang'>Language: <select name='lang'><option value=""en"" selected>English<option value=""ar"">العربية<option value=""bg"">Български<option value=""bn"">বাংলা<option value=""bs"">Bosanski<option value=""ca"">Català<option value=""cs"">Čeština<option value=""da"">Dansk<option value=""de"">Deutsch<option value=""el"">Ελληνικά<option value=""es"">Español<option value=""et"">Eesti<option value=""fa"">فارسی<option value=""fi"">Suomi<option value=""fr"">Français<option value=""gl"">Galego<option value=""he"">עברית<option value=""hu"">Magyar<option value=""id"">Bahasa Indonesia<option value=""it"">Italiano<option value=""ja"">日本語<option value=""ko"">한국어<option value=""lt"">Lietuvių<option value=""ms"">Bahasa Melayu<option value=""nl"">Nederlands<option value=""no"">Norsk<option value=""pl"">Polski<option value=""pt"">Português<option value=""pt-br"">Português (Brazil)<option value=""ro"">Limba Română<option value=""ru"">Русский<option value=""sk"">Slovenčina<option value=""sl"">Slovenski<option value=""sr"">Српски<option value=""ta"">த‌மிழ்<option value=""th"">ภาษาไทย<option value=""tr"">Türkçe<option value=""uk"">Українська<option value=""vi"">Tiếng Việt<option value=""zh"">简体中文<option value=""zh-tw"">繁體中文</select><script nonce=""OTM5NDUyNzJhMzgzMmU1MjQ0MDc1MTJhMTIzOWZmYWE="">qsl('select').onchange = function () { this.form.submit(); };</script> <input type='submit' value='Use' class='hidden'>
<input type='hidden' name='token' value='135968:716411'>
</div>
</form>
<div id=""menu"">
<h1>
<a href='https://www.adminer.org/' target=""_blank"" rel=""noreferrer noopener"" id='h1'>Adminer</a> <span class=""version"">4.6.3</span>
<a href=""https://www.adminer.org/#download"" target=""_blank"" rel=""noreferrer noopener"" id=""version""></a>
</h1>
</div>
<script nonce=""OTM5NDUyNzJhMzgzMmU1MjQ0MDc1MTJhMTIzOWZmYWE="">setupSubmitHighlight(document);</script>
': invalid character '<' looking for beginning of value
```

**What you expected to happen**:

Only version information to be printed.


**How to reproduce it**:

1. `brew install kubernetes-cli`
2. `kubectl version`


**Anything else we need to know**:

Nothing that I can think of, but feel free to ask questions and I'll be happy to help!",closed,False,2018-11-01 14:35:26,2019-01-19 11:35:48
kubectl,joberget,https://github.com/kubernetes/kubectl/issues/553,https://api.github.com/repos/kubernetes/kubectl/issues/553,Deb-package for Bionic Beaver 18.04,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
**FEATURE REQUEST**

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
1.12.2

**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): Ubuntu 18.04.1 LTS (Bionic Beaver)
- **Kernel** (e.g. `uname -a`): 4.15.0-38-generic #41-Ubuntu SMP Wed Oct 10 10:59:38 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**: kubectl
- **Others**: None


**What happened**:
Would love to have kubectl packaged as deb-package for bionic beaver!

",closed,False,2018-11-02 10:59:33,2019-04-01 12:57:43
kubectl,wknapik,https://github.com/kubernetes/kubectl/issues/554,https://api.github.com/repos/kubernetes/kubectl/issues/554,Support for waiting on multiple deployments to complete and also support for timeouts,"**Is this a request for help?**
No.

**What keywords did you search in Kubernetes issues before filing this one?**
rollout status

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
FEATURE REQUEST

**Kubernetes version** (use `kubectl version`):
N/A

**Environment**:
- **Cloud provider or hardware configuration**:
N/A
- **OS** (e.g. from /etc/os-release):
N/A
- **Kernel** (e.g. `uname -a`):
N/A
- **Install tools**:
N/A
- **Others**:
N/A

**What happened**:
```
% kubectl rollout status -w deployment/{foo,bar}
error: rollout status is only supported on individual resources and resource collections - 2 resources were found
%
```
```
% kubectl rollout status --help|grep -ci timeout
0
%
```
 
**What you expected to happen**:
I expected kubectl to wait until both deployments were complete and I also expected to be able to set a timeout.

**How to reproduce it** (as minimally and precisely as possible):
`kubectl rollout status -w deployment/{foo,bar}`
`kubectl rollout status --help|grep -ci timeout`

**Anything else we need to know**:
No.
",open,False,2018-11-02 18:32:53,2019-01-09 15:04:10
kubectl,TheKangaroo,https://github.com/kubernetes/kubectl/issues/555,https://api.github.com/repos/kubernetes/kubectl/issues/555,kubectl --prune does not delete last managed item(s) in namespace,"**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.0"", GitCommit:""91e7b4fd31fcd3d5f436da26c980becec37ceefe"", GitTreeState:""clean"", BuildDate:""2018-06-27T20:08:34Z"", GoVersion:""go1.10.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-09T17:53:03Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: baremetal/vmware cluster
- **OS** (e.g. from /etc/os-release): CoreOS 1855.4.0
- **Kernel** (e.g. `uname -a`): 4.14.67-coreos
- **Install tools**: vanilla kubernetes
- **Others**:


**What happened**:
kubctl apply --prune won't delete a resource (or multiple resources) if there is no other resource with the selected label left in this namespace. 

**What you expected to happen**:
prune should delete *all* resources with the specified label if they are not in the set of applied resources.

**How to reproduce it** (as minimally and precisely as possible):
Apply a pod to a namespace and two pods to another namespace.
```
$ cat all.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: default-http-backend
    testlable: managed
  name: default-http-1
  namespace: first
spec:
  containers:
  - image: gcr.io/google_containers/defaultbackend:1.4
    imagePullPolicy: IfNotPresent
    name: default-http-backend
    ports:
    - containerPort: 8080
      protocol: TCP
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: default-http-backend
    testlable: managed
  name: default-http-1
  namespace: second
spec:
  containers:
  - image: gcr.io/google_containers/defaultbackend:1.4
    imagePullPolicy: IfNotPresent
    name: default-http-backend
    ports:
    - containerPort: 8080
      protocol: TCP
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: default-http-backend
    testlable: managed
  name: default-http-2
  namespace: second
spec:
  containers:
  - image: gcr.io/google_containers/defaultbackend:1.4
    imagePullPolicy: IfNotPresent
    name: default-http-backend
    ports:
    - containerPort: 8080
      protocol: TCP


$ kubectl apply --prune -l 'testlable=managed' --cascade=true -f all.yaml
pod/default-http-1 created
pod/default-http-2 created
pod/default-http-1 created
```
If you delete one pod definition in the second namespace prune apply works as expected:
```
$ cat /only-two.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: default-http-backend
    testlable: managed
  name: default-http-1
  namespace: first
spec:
  containers:
  - image: gcr.io/google_containers/defaultbackend:1.4
    imagePullPolicy: IfNotPresent
    name: default-http-backend
    ports:
    - containerPort: 8080
      protocol: TCP
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: default-http-backend
    testlable: managed
  name: default-http-1
  namespace: second
spec:
  containers:
  - image: gcr.io/google_containers/defaultbackend:1.4
    imagePullPolicy: IfNotPresent
    name: default-http-backend
    ports:
    - containerPort: 8080
      protocol: TCP

$ kubectl apply --prune -l 'testlable=managed' --cascade=true -f all.yaml
pod/default-http-1 unchanged
pod/default-http-1 unchanged
pod/default-http-2 pruned
```
But if I want to delete both resources from the second namespace, nothing happens:
```
$ cat only-one.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: default-http-backend
    testlable: managed
  name: default-http-1
  namespace: first
spec:
  containers:
  - image: gcr.io/google_containers/defaultbackend:1.4
    imagePullPolicy: IfNotPresent
    name: default-http-backend
    ports:
    - containerPort: 8080
      protocol: TCP

$ kubectl apply --prune -l 'testlable=managed' --cascade=true -f all.yaml
pod/default-http-1 unchanged
```

**Anything else we need to know**:
This happens regardless if I delete one or multiple resources from the second namespace. Prune won't touch the second namespace if there are no manged resources left in there.
",closed,False,2018-11-05 15:34:16,2019-04-04 17:12:21
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/556,https://api.github.com/repos/kubernetes/kubectl/issues/556,Create Better Documentation for Kubectl and Tooling,"Proof of concept for new kubectl documentation:

https://pwittrock-kubectl.firebaseapp.com/pages/generators.html

",open,False,2018-11-07 17:36:22,2019-03-28 15:50:38
kubectl,wknapik,https://github.com/kubernetes/kubectl/issues/557,https://api.github.com/repos/kubernetes/kubectl/issues/557,`kubectl get ... --namespace non-existent` should not exit with 0,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
No.

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
The relevant ones.
---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.9"", GitCommit:""e6ab4ee54f71e7a403cc733534bab86fd959ecb7"", GitTreeState:""clean"", BuildDate:""2018-10-16T12:14:47Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.11"", GitCommit:""1bfeeb6f212135a22dc787b73e1980e5bccef13d"", GitTreeState:""clean"", BuildDate:""2018-09-28T21:35:22Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release): Arch Linux
- **Kernel** (e.g. `uname -a`): Linux something 4.18.16-arch1-1-ARCH #1 SMP PREEMPT Sat Oct 20 22:06:45 UTC 2018 x86_64 GNU/Linux
- **Install tools**:
- **Others**:


**What happened**:
```
$ kubectl get pods --namespace non-existent
No resources found.
$ echo $?
0
$
```

**What you expected to happen**:
I expected kubectl to return anything other than zero when information is requested about resources in a non-existent namespace. The current behavior hides errors and generates bugs in scripts/confusion in interactive use. Fail fast.

**How to reproduce it** (as minimally and precisely as possible):
```
kubectl get pods --namespace non-existent
```

**Anything else we need to know**:
No.
",open,False,2018-11-07 23:18:24,2019-02-11 14:03:05
kubectl,seans3,https://github.com/kubernetes/kubectl/issues/558,https://api.github.com/repos/kubernetes/kubectl/issues/558,kubectl port-forward flaky e2e tests,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

**BUG REPORT**

**Test Grid Test**
https://gubernator.k8s.io/build/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-new-master-gci-kubectl-skew/11783#sig-cli-kubectl-port-forwarding-k8sio-with-a-server-listening-on-localhost-should-support-forwarding-over-websockets

**Kubernetes version** (use `kubectl version`):
Client: 1.11
Server: 1.11

**What happened**:

Test Grid test fails intermittently.

**FLAKE 1**

 It looks like a **RACE CONDITION**. The failure looks like:

```
Nov  6 14:40:52.585: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Creating the pod
STEP: Sending the expected data to the local port
STEP: Reading data from the local port
STEP: Verifying logs
Nov  6 14:41:09.010: INFO: Missing ""^Accepted client connection$"" from log: 
Nov  6 14:41:09.190: INFO: Pod log:
Accepted client connection
Received expected client data
```

This appears to be a race condition because it looks like its checking for `""^Accepted client connection$""`, then that missing statement happens 180 ms later.

**FLAKE 2**

There are multiple flakes with the following error:

`Failed to parse kubectl port-forward output: Forwarding from [::1]:34011 -> 80`

Log context is:

```
STEP: Creating the target pod
STEP: Running 'kubectl port-forward'
Nov  5 11:13:59.004: INFO: starting port-forward command and streaming output
Nov  5 11:13:59.004: INFO: Asynchronously running '../../../../kubernetes_skew/cluster/kubectl.sh ../../../../kubernetes_skew/cluster/kubectl.sh --server=https://35.202.150.113 --kubeconfig=/workspace/.kube/config port-forward --namespace=e2e-tests-port-forwarding-2xcwq pfpod :80'
Nov  5 11:13:59.005: INFO: reading from `kubectl port-forward` command's stdout
Nov  5 11:13:59.244: INFO: Failed to parse kubectl port-forward output: Forwarding from [::1]:34011 -> 80

Nov  5 11:13:59.260: INFO: Pod log:

[AfterEach] [sig-cli] Kubectl Port forwarding
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142
STEP: Collecting events from namespace ""e2e-tests-port-forwarding-2xcwq
```

A link to the failure is:  https://gubernator.k8s.io/build/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-new-master-gci-kubectl-skew/11756#sig-cli-kubectl-port-forwarding-k8sio-with-a-server-listening-on-0000-k8sio-that-expects-a-client-request-should-support-a-client-that-connects-sends-no-data-and-disconnects

The following PR seems very relevant to this flake: https://github.com/kubernetes/kubernetes/pull/46517

**What you expected to happen**:

Test Grid test to pass deterministically

**What else**

`kubectl port-forward` e2e test is at: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/kubectl/portforward.go

/kind flake
/sig cli
/area kubectl
/priority P2
/assign goblain
/assign m1kola
",closed,False,2018-11-08 18:46:30,2018-12-31 17:41:36
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/559,https://api.github.com/repos/kubernetes/kubectl/issues/559,Kubectl Book,"Initial revision of kubectl book for managing Kubernetes Applcations.

Rendered content viewable here:
- https://pwittrock-kubectl.firebaseapp.com

Tracking Bugs for how things *should* work here:
- https://github.com/kubernetes/kubectl/projects/7",closed,True,2018-11-12 04:39:40,2018-11-12 19:57:07
kubectl,xichengliudui,https://github.com/kubernetes/kubectl/pull/560,https://api.github.com/repos/kubernetes/kubectl/issues/560,To add and update the function name,To add and update the function name,closed,True,2018-11-12 07:08:43,2019-02-22 07:39:15
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/561,https://api.github.com/repos/kubernetes/kubectl/issues/561,Remove node_modules from GitBook,,closed,True,2018-11-12 20:07:46,2018-11-12 20:22:10
kubectl,sudeeshjohn,https://github.com/kubernetes/kubectl/issues/562,https://api.github.com/repos/kubernetes/kubectl/issues/562,"kubelet --version shows info log ""proto: duplicate proto type registered: pluginregistration....""","**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

```
root@ubuntu18:~/test-infra/tests/k8s-conformance# kubelet --version
2018/11/15 08:15:21 proto: duplicate proto type registered: pluginregistration.PluginInfo
2018/11/15 08:15:21 proto: duplicate proto type registered: pluginregistration.RegistrationStatus
2018/11/15 08:15:21 proto: duplicate proto type registered: pluginregistration.RegistrationStatusResponse
2018/11/15 08:15:21 proto: duplicate proto type registered: pluginregistration.InfoRequest
Kubernetes v1.14.0-alpha.0.324+843a67b215dcfa
root@ubuntu18:~/test-infra/tests/k8s-conformance#
```


**Environment**:
- **Cloud provider or hardware configuration**: KVM Guest

- **OS** (e.g. from /etc/os-release):

```
root@ubuntu18:~/test-infra/tests/k8s-conformance# cat /etc/os-release
NAME=""Ubuntu""
VERSION=""18.04.1 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.1 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
root@ubuntu18:~/test-infra/tests/k8s-conformance#
```
- **Kernel** (e.g. `uname -a`):

```
root@ubuntu18:~/test-infra/tests/k8s-conformance# uname -a
Linux ubuntu18.04-icp-node-4 4.15.0-36-generic #39-Ubuntu SMP Mon Sep 24 16:19:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
root@ubuntu18:~/test-infra/tests/k8s-conformance#
```

**What happened**:
kubeadm init --kubernetes-version ci-cross/latest is failing as the kubelet --version is unable to parse

```

++ /root/test-infra/tests/k8s-conformance/k8s-e2e/tmp/v1.14.0-alpha.0.324+843a67b215dcfa/kubeadm init --kubernetes-version ci-cross/latest
Running kubeadm init...
[init] Using Kubernetes version: v1.14.0-alpha.0.324+843a67b215dcfa
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.03.1-ce. Latest validated version: 18.06
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Swap]: running with swap on is not supported. Please disable swap
	[ERROR KubeletVersion]: couldn't get kubelet version: Unable to parse output from Kubelet: ""2018/11/15 08:06:25 proto: duplicate proto type registered: pluginregistration.PluginInfo\n2018/11/15 08:06:25 proto: duplicate proto type registered: pluginregistration.RegistrationStatus\n2018/11/15 08:06:25 proto: duplicate proto type registered: pluginregistration.RegistrationStatusResponse\n2018/11/15 08:06:25 proto: duplicate proto type registered: pluginregistration.InfoRequest\nKubernetes v1.14.0-alpha.0.324+843a67b215dcfa""

```



**What you expected to happen**:

kubelet --version should have given only 

```Kubernetes v1.14.0-alpha.0.324+843a67b215dcfa```


**How to reproduce it** (as minimally and precisely as possible):

get the latest kubelet and run kubelet --version

```

- curl -sSL https://dl.k8s.io/ci-cross/v1.14.0-alpha.0.324+843a67b215dcfa/bin/linux/amd64/kubelet -o kubelet 
- ./kubelet --version

```

",closed,False,2018-11-15 08:23:22,2018-11-18 07:30:08
kubectl,xichengliudui,https://github.com/kubernetes/kubectl/pull/563,https://api.github.com/repos/kubernetes/kubectl/issues/563,Update notes,Update the function name in the log,closed,True,2018-11-16 02:42:09,2018-12-04 08:32:19
kubectl,nightfury1204,https://github.com/kubernetes/kubectl/issues/564,https://api.github.com/repos/kubernetes/kubectl/issues/564,kubectl edit or apply can not update .status when status sub resource is enabled,"
**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT
<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.0"", GitCommit:""0ed33881dc4355495f623c6f22e7dd0b7632b7c0"", GitTreeState:""clean"", BuildDate:""2018-09-27T17:05:32Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-09T17:53:03Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}


**Environment**:
-  minikube

**What happened**:
I have a crd that has status subresource is enabled. When i edit the status of the crd using `kubectl edit`, the changes doesn't apply. 

**What you expected to happen**:
`kubectl edit` should apply the changes in status field.

**How to reproduce it** (as minimally and precisely as possible):
```
$ cat customresourcedefination.yaml 
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: foos.try.com
spec:
  group: try.com
  version: v1alpha1
  scope: Namespaced
  subresources:
    status: {}
  names:
    plural: foos
    singular: foo
    kind: Foo

$ kubectl apply -f customresourcedefination.yaml
```
```
$ cat foo.yaml 
apiVersion: try.com/v1alpha1
kind: Foo
metadata:
  name: my-foo
status:
  hello: world

$ kubectl apply -f foo.yaml
```

```
# edit the status
$ kubectl edit foo/my-foo
```


**Anything else we need to know**:
If  status subresource is disabled for crd, then `kubectl edit` works fine.
",open,False,2018-11-21 06:59:28,2019-03-07 08:15:07
kubectl,AVKudryashov,https://github.com/kubernetes/kubectl/issues/565,https://api.github.com/repos/kubernetes/kubectl/issues/565,kubectl x509,"## Is this a BUG REPORT or FEATURE REQUEST?

BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->


## Versions

**kubeadm version** (use `kubeadm version`): 1.12.2

**Environment**:
- **Kubernetes version** (use `kubectl version`): 1.12.2
 **Cloud provider or hardware configuration**: Local
- **OS** (e.g. from /etc/os-release): CentOS-7 (7.5.1804 (Core))
- **Kernel** (e.g. `uname -a`): 3.10.0-862.el7.x86_64
- **Others**:


## What happened?
New installation of kubernetes. After creating a single master cluster with kubeadm im trying to install a pod network add-on
```
[kub@cen-n-1 ~]$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
Unable to connect to the server: x509: certificate signed by unknown authority
```
## What you expected to happen?
Install a pod network add-on
## How to reproduce it (as minimally and precisely as possible)?
install the latest kubernetes via kubeadm

## Anything else we need to know?
",closed,False,2018-11-26 12:17:19,2018-11-27 04:47:25
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/566,https://api.github.com/repos/kubernetes/kubectl/issues/566,"Unclear what ""kubectl config view --merge"" does","I'm on kubectl v1.12 and can't tell what the `--merge` flag in `kubectl config view` does. Here's the help:

    --merge=true: Merge the full hierarchy of kubeconfig files

I have two contexts with non-overlapping cluster/user entries, so I export them to separate files, and can merge them without `--merge`. Moreover, using `--merge` provides the same output.

## Repro steps

```
# export A
kubectl config use-context cluster-a
kubectl config view --flatten > a

# export B
kubectl config use-context cluster-b
kubectl config view --flatten > b

# merge without --merge
KUBECONFIG=a:b kubectl config view --flatten > m1

# merge with --merge
KUBECONFIG=a:b kubectl config view --flatten --merge > m2

# diff them
diff m1 m2
```

The diff above returns empty (it's the same result with/without --merge). So what does `--merge` actually do?",closed,False,2018-11-30 18:09:48,2019-01-13 18:25:22
kubectl,willbeason,https://github.com/kubernetes/kubectl/issues/567,https://api.github.com/repos/kubernetes/kubectl/issues/567,kubectl --dry-run does not catch invalid CRD metadata.name,"**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No.

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): dry-run, metadata.name, CRD

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

**Kubernetes version** (use `kubectl version`):

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.0"", GitCommit:""ddf47ac13c1a9483ea035a79cd7c10005ff21a6d"", GitTreeState:""clean"", BuildDate:""2018-12-03T21:04:45Z"", GoVersion:""go1.11.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.7-gke.11"", GitCommit:""dc4f6dda6a08aae2108d7a7fdc2a44fa23900f4c"", GitTreeState:""clean"", BuildDate:""2018-11-10T20:22:02Z"", GoVersion:""go1.9.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: linux/amd64
- **OS** (e.g. from /etc/os-release): 
- **Kernel** (e.g. `uname -a`): 4.18.10-1rodete2-amd64 #1 SMP Debian 4.18.10-1rodete2 (2018-10-09) x86_64 GNU/Linux
- **Install tools**:
- **Others**:

**What happened**: kubectl create --dry-run on a CRD with an invalid metadata.name succeeds, but kubectl create fails with an invalid name error.

**What you expected to happen**: kubectl create --dry-run should return that metadata.name is invalid.

**How to reproduce it** (as minimally and precisely as possible):
1. Create a CRD in a file called `file.yaml` with metadata.name: Sync
2. `kubectl create -f file.yaml --dry-run` [no error]
3. `kubectl create -f file.yaml` [returns error since metadata.name is not a valid DNS1123 label]
",closed,False,2018-12-04 21:16:55,2019-03-05 00:11:53
kubectl,tomfotherby,https://github.com/kubernetes/kubectl/issues/568,https://api.github.com/repos/kubernetes/kubectl/issues/568,kubectl describe hpa not working with v1.13.0,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** no.

**What keywords did you search in Kubernetes issues before filing this one?** ""hpa""

---

**Is this a BUG REPORT or FEATURE REQUEST?** : Bug report

**Kubernetes version**

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.0"", GitCommit:""ddf47ac13c1a9483ea035a79cd7c10005ff21a6d"", GitTreeState:""clean"", BuildDate:""2018-12-03T21:04:45Z"", GoVersion:""go1.11.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10+"", GitVersion:""v1.10.11-eks"", GitCommit:""6bf27214b7e3e1e47dce27dcbd73ee1b27adadd0"", GitTreeState:""clean"", BuildDate:""2018-12-04T13:33:10Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

I installed using instructions from https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl - i.e. in my Ubuntu Desktop:
```
sudo apt-get update && sudo apt-get install -y apt-transport-https
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo ""deb https://apt.kubernetes.io/ kubernetes-xenial main"" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubectl
```

**Environment**:
- **Cloud provider or hardware configuration**: AWS EKS

**What happened**:

When using kubectl v1.13.0 I cannot `describe hpa`:

```
$ kubectl describe hpa my-hpa
Error from server (NotFound): the server could not find the requested resource
```
I tried `kubectl describe HorizontalPodAutoscaler talentdesk` as well, same issue.

**What you expected to happen**:

Before I upgraded kubectl to v1.13 , I had v1.10 and it worked ok.

```
$ kubectl describe hpa my-hpa
Name:                                                  my-hpa
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           kubectl.kubernetes.io/last-applied-configuration={""apiVersion"":""autoscaling/v2beta1"",""kind"":""HorizontalPodAutoscaler"",""metadata"":{""annotations"":{},""name"":""my-hpa"",""namespace"":""default""},""spec"":{""...
...
```

**Anything else we need to know**:

My HPA manifests use `apiVersion: autoscaling/v2beta1`:

```
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
...
```

I noticed [the hpa documentation](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) mentions `v2beta2` but I'm using `v2beta1` - I tried to upgrade my manifest but kubernetes v1.10 doesn't support `v2beta2` - this is the error I get: 

```
$ kubectl apply -f my-hpa.yaml
error: unable to recognize ""my-hpa.yaml"": no matches for kind ""HorizontalPodAutoscaler"" in version ""autoscaling/v2beta2""
```

I'm stuck on kubernetes v1.10 for now because AWS EKS doesn't yet allow upgrades.",closed,False,2018-12-06 19:42:42,2019-03-28 02:54:51
kubectl,omeid,https://github.com/kubernetes/kubectl/issues/569,https://api.github.com/repos/kubernetes/kubectl/issues/569,Feature Request: Support $HOME/.kube/config.d/*,"Greetings!

kubectl already allows defining multiple clusters and users in `$HOME/.kube/config` however editing this file by hand or even by tools is a bit cumbersome.

If `kubectl` supported loading multiple config files from `$HOME/.kube/config.d/` it would make dealing with different cluster configurations much easier.

For example,

1. kubespray already generates a config file, but it is still not easy to use it if you already have a config file setup (kubernetes-sigs/kubespray/pull/1647).

2. aws eks cli already mutates the config file, but dealing with multiple clusters or updating this information requires way more mental overhead than it should require.


I would like to hear some feedback on this and whatever a pull request would be considered.


Many Thanks!",open,False,2018-12-17 05:32:34,2019-02-06 19:14:58
kubectl,pwittrock,https://github.com/kubernetes/kubectl/issues/570,https://api.github.com/repos/kubernetes/kubectl/issues/570,kustomize follow ups,"- Make resource builder call a library that has options instead of the `builder.NewCmdBuild`
- Throw an error if the directory contains kustomization files **by checking the GVK**
- Check if the target is a kuztomization file by GVK, not filename",open,False,2018-12-18 22:43:53,2019-01-11 01:09:45
kubectl,TheKangaroo,https://github.com/kubernetes/kubectl/issues/571,https://api.github.com/repos/kubernetes/kubectl/issues/571,kubectl create secret docker-registry omits auth key in dockerconfigjson,"**BUG REPORT**

**Kubernetes version** (use `kubectl version`): 1.13.x


**Environment**:
- **OS**: kubectl on MacOS/hyperkube container


**What happened**:

I try to create a docker pull secret like: 
```
kubectl create secret docker-registry test --docker-server=registry.example.com  --docker-username=username --docker-password=password --docker-email=test@example.com --dry-run -oyaml
```
This creates an object like:
```
apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJyZWdpc3RyeS5leGFtcGxlLmNvbSI6eyJVc2VybmFtZSI6InVzZXJuYW1lIiwiUGFzc3dvcmQiOiJwYXNzd29yZCIsIkVtYWlsIjoidGVzdEBleGFtcGxlLmNvbSJ9fX0=
kind: Secret
metadata:
  creationTimestamp: null
  name: test
type: kubernetes.io/dockerconfigjson
```
And the base64-decoded string looks like this:
```
# echo 'eyJhdXRocyI6eyJyZWdpc3RyeS5leGFtcGxlLmNvbSI6eyJVc2VybmFtZSI6InVzZXJuYW1lIiwiUGFzc3dvcmQiOiJwYXNzd29yZCIsIkVtYWlsIjoidGVzdEBleGFtcGxlLmNvbSJ9fX0=' |base64 -D
{""auths"":{""registry.example.com"":{""Username"":""username"",""Password"":""password"",""Email"":""test@example.com""}}}
```
Mind the missing auth key.

**What you expected to happen**:
When I run the same command with the kubelet 1.12.0 everything works as expected:
```
kubectl create secret docker-registry test --docker-server=registry.example.com  --docker-username=username --docker-password=password --docker-email=test@example.com --dry-run -oyaml
```
This creates an object like:
```
apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJyZWdpc3RyeS5leGFtcGxlLmNvbSI6eyJ1c2VybmFtZSI6InVzZXJuYW1lIiwicGFzc3dvcmQiOiJwYXNzd29yZCIsImVtYWlsIjoidGVzdEBleGFtcGxlLmNvbSIsImF1dGgiOiJkWE5sY201aGJXVTZjR0Z6YzNkdmNtUT0ifX19
kind: Secret
metadata:
  creationTimestamp: null
  name: test
type: kubernetes.io/dockerconfigjson
```
And the base64-decoded string looks like this:
```
# echo 'eyJhdXRocyI6eyJyZWdpc3RyeS5leGFtcGxlLmNvbSI6eyJ1c2VybmFtZSI6InVzZXJuYW1lIiwicGFzc3dvcmQiOiJwYXNzd29yZCIsImVtYWlsIjoidGVzdEBleGFtcGxlLmNvbSIsImF1dGgiOiJkWE5sY201aGJXVTZjR0Z6YzNkdmNtUT0ifX19' |base64 -D
{""auths"":{""registry.example.com"":{""username"":""username"",""password"":""password"",""email"":""test@example.com"",""auth"":""dXNlcm5hbWU6cGFzc3dvcmQ=""}}}
```
This one creates the auth key.

**How to reproduce it** (as minimally and precisely as possible):
Run the command above with kubectl v1.12.x and 1.13.x 

**Anything else we need to know**:
I can reproduce it on my Mac and in the hyperkube docker container.
I ran it against an 1.11.5 cluster, ~~but try to test agains an newer cluster soon.~~
Same result against a 1.13.1 cluster.",closed,False,2018-12-21 12:02:03,2018-12-26 20:08:40
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/572,https://api.github.com/repos/kubernetes/kubectl/issues/572,better apply --prune,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
feature request

While `kubectl apply` is widely used in different workflows, users need `kubectl apply --prune` to sync the resources on cluster with configuration saved in files, directories or repos. 

Currently `kubectl apply --prune` is still in Alpha. It prunes objects by matching labels. Users must be really careful when they use this command so that they don't delete objects unintentionally. There are a list of issues with current pruning behavior: https://github.com/kubernetes/kubernetes/issues/69879, https://github.com/kubernetes/kubernetes/issues/68400, https://github.com/kubernetes/kubernetes/issues/66430, https://github.com/kubernetes/kubernetes/issues/40635, https://github.com/kubernetes/kubectl/issues/555

We need better `kubectl apply --prune` so that it will meet the users' requirement of syncing resources and get rid of current issues.",open,False,2019-01-04 22:17:29,2019-02-26 07:03:18
kubectl,SurferJeffAtGoogle,https://github.com/kubernetes/kubectl/issues/573,https://api.github.com/repos/kubernetes/kubectl/issues/573,Inscrutable error message.,"**Kubernetes version** (use `kubectl version`):

> kubectl version
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.7"", GitCommit:""0c38c362511b20a098d7cd855f1314dad92c2780"", GitTreeState:""clean"", BuildDate:""2018-08-20T10:09:03Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""windows/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10+"", GitVersion:""v1.10.9-gke.5"", GitCommit:""d776b4deeb3655fa4b8f4e8e7e4651d00c5f4a98"", GitTreeState:""clean"", BuildDate:""2018-11-08T20:33:00Z"", GoVersion:""go1.9.3b4"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:

Linux and Windows.  I tried on both platforms.

**What happened**:
```
rennie@cloudshell:~/csharp-docs-samples/appengine/flexible/SocialAuth (dotnet-docs-samples-tests)$ kubectl apply -f social-auth-deployment.yaml
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
The Deployment ""social-auth"" is invalid: spec.selector: Invalid value: v1.LabelSelector{MatchLabels:map[string]string{""run"":""social-auth"", ""app"":""social-auth""}, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable
```

**What you expected to happen**:

A useful error message.  I got rid of the error message by deleting my existing deployments.  Nothing in the error message indicates that the problem is with an existing deployment.
",open,False,2019-01-09 22:17:45,2019-01-09 22:17:45
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/574,https://api.github.com/repos/kubernetes/kubectl/issues/574,"""plugins list"" command raises errors for non-existing dirs in $PATH","It appears like `kubectl plugins list` explicitly prints errors for dirs that don't exist but added to $PATH env variable.

**Kubernetes version** (use `kubectl version`): v1.13.1

**Environment**:
- **Kernel** (e.g. `uname -a`): macOS 10.14.2.

**What happened**:

I have this env var:

```
echo $PATH
/Users/ahmetb/.krew/bin:/Users/ahmetb/workspace/dotfiles/bin:/Users/ahmetb/.homebrew/opt/python/libexec/bin:/Users/ahmetb/.homebrew/opt/ruby/bin:/Users/ahmetb/.homebrew/opt/gnu-sed/libexec/gnubin:/Users/ahmetb/.homebrew/opt/openssl/bin:/Users/ahmetb/.homebrew/opt/gettext/bin:/Users/ahmetb/.homebrew/opt/ncurses/bin:/Users/ahmetb/.homebrew/opt/grep/libexec/gnubin:/Users/ahmetb/.homebrew/opt/gnu-sed/libexec/gnubin:/Users/ahmetb/.homebrew/opt/gnu-tar/libexec/gnubin:/Users/ahmetb/.homebrew/opt/gnu-indent/libexec/gnubin:/Users/ahmetb/.homebrew/opt/gnu-getopt/libexec/gnubin:/Users/ahmetb/.homebrew/opt/coreutils/libexec/gnubin:/Users/ahmetb/.homebrew/bin:/Users/ahmetb/.homebrew/sbin:/usr/local/git/current/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin:~/.dotnet/tools:/Users/ahmetb/gotools/bin:/Users/ahmetb/.homebrew/opt/fzf/bin
```

```
$ kubectl plugin list

error: unable to read directory ""/Users/ahmetb/.krew/bin"" in your PATH: open /Users/ahmetb/.krew/bin: no such file or directory
error: unable to read directory ""/Users/ahmetb/.homebrew/opt/gnu-indent/libexec/gnubin"" in your PATH: open /Users/ahmetb/.homebrew/opt/gnu-indent/libexec/gnubin: no such file or directory
error: unable to read directory ""/Users/ahmetb/.homebrew/opt/gnu-getopt/libexec/gnubin"" in your PATH: open /Users/ahmetb/.homebrew/opt/gnu-getopt/libexec/gnubin: no such file or directory
error: unable to read directory ""~/.dotnet/tools"" in your PATH: open ~/.dotnet/tools: no such file or directory
error: unable to find any kubectl plugins in your PATH
```

**What you expected to happen**:

Don't print `error: unable to read directory` for dirs that don't exist. This should be a -v=3 level bug.
",closed,False,2019-01-10 06:03:43,2019-02-01 12:45:13
kubectl,ahmetb,https://github.com/kubernetes/kubectl/issues/575,https://api.github.com/repos/kubernetes/kubectl/issues/575,Regression: [Plugin] [1.12] plugin binaries are overshadowed by themselves if they appear in PATH twice,"Issue https://github.com/kubernetes/kubectl/issues/539 has started to repeat itself in kubectl 1.13.1:

I get this error 
```
$ kubectl plugin list
/Users/ahmetb/.krew/bin/kubectl-krew
  - warning: /Users/ahmetb/.krew/bin/kubectl-krew is overshadowed by a similarly named plugin: /Users/ahmetb/.krew/bin/kubectl-krew
```

when I duplicate dirs in my $PATH by doing `export PATH=""$PATH:$PATH""`.

/kind bug
/priority P1
/cc @juanvallejo ",closed,False,2019-01-10 17:46:37,2019-01-22 16:46:02
kubectl,babilen5,https://github.com/kubernetes/kubectl/issues/576,https://api.github.com/repos/kubernetes/kubectl/issues/576,Please add fish tab completion,"# Fish Completion - Feature Request

Please implement tab completion for the fish shell by building on work done in https://github.com/spf13/cobra/pull/754 or consider shipping automatically generated manpages beforehand.

Unfortunately an earlier feature request (#131) was closed even though the issue had not been addressed. Previous attempts to add completion failed due to the inability to generate them programmatically, but with the changes to cobra referenced above fixing this should be relatively straightforward.

Fish users will appreciate your efforts. Thanks!

## Kubernetes version

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.2"", GitCommit:""cff46ab41ff0bb44d8584413b598ad8360ec1def"", GitTreeState:""clean"", BuildDate:""2019-01-10T23:35:51Z"", GoVersion:""go1.11.4"", Compiler:""gc"", Platform:""linux/amd64""}
```

## Related Issues

- https://github.com/kubernetes/kubectl/issues/131 (closed for no apparent reason)
- https://github.com/spf13/cobra/pull/754 (fish completion support in cobra)
- https://github.com/kubernetes/kubernetes/pull/58484 (previous PR with static completion code)",open,False,2019-01-19 00:38:18,2019-01-19 00:38:18
kubectl,groner,https://github.com/kubernetes/kubectl/issues/577,https://api.github.com/repos/kubernetes/kubectl/issues/577,kubectl set env does not apply prefix when removing environment variables,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): 
kubectl set env prefix remove (I also searched in kubernetes/kubernetes)

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
kubectl 1.13.0

**Environment**:
- **Cloud provider or hardware configuration**: local test env
- **OS** (e.g. from /etc/os-release): ubuntu 18.04.1
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:
Using `kubectl set env TARGET --prefix=TEST_ SOMEVAR-` does not apply the prefix when removing environment variables.

**What you expected to happen**:
The prefix is applied when removing environment variables, not just when setting them.

**How to reproduce it** (as minimally and precisely as possible):
```
# Create a deployment 
:; kubectl run --image ubuntu:bionic envtest \
    --env=TEST_SOMEVAR=foo \
    --env=SOMEVAR=bar \
     -- sleep inf
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
deployment.apps/envtest created

# Try to remove TEST_SOMEVAR using the --prefix option.
:; kubectl set env deploy envtest --prefix=TEST_ SOMEVAR-
deployment.extensions/envtest env updated

# Observe that the prefix was not used, it removed SOMEVAR instead.
:; kubectl set env deploy envtest --list
# Deployment envtest, container envtest
TEST_SOMEVAR=foo

# Clean up
:; kubectl delete deploy envtest 
deployment.extensions ""envtest"" deleted
```

**Anything else we need to know**:
It looks like the prefix is being applied to only the updated environment variables here:
https://github.com/kubernetes/kubernetes/blob/408d4c0cb865899fbda6ebfafc27ee9d8fcadc64/pkg/kubectl/cmd/set/set_env.go#L331-L335
",open,False,2019-01-25 16:41:04,2019-02-18 17:15:35
kubectl,kuengnee,https://github.com/kubernetes/kubectl/issues/578,https://api.github.com/repos/kubernetes/kubectl/issues/578,Unable to to access k8s in aws eks,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):

Yes but unable to find the relevance troubleshooting guide

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):

EKS AWS
---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

Not sure if it is a bug

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

Nope

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

v1.13.2""

**Environment**:
- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release): macOS Mojave 10.14.2 (18C54)
- **Kernel** (e.g. `uname -a`): Darwin Kernel Version 18.2.0: Mon Nov 12 20:24:46 PST 2018; root:xnu-4903.231.4~2/RELEASE_X86_64 x86_64
- **Install tools**: kubectl aws-iam-authenticator brew
- **Others**:


**What happened**:

When configuration is done using aws eks update-kubeconfig --name=clustername follow by
""kubectl get all""

It show the below message
error: the server doesn't have a resource type ""all""

**What you expected to happen**:

list of pods and services

**How to reproduce it** (as minimally and precisely as possible):
Install aws-iam-authenticator using brew
Install kubectl using brew
aws eks update-kubeconfig --name=clustername
kubectl get all


**Anything else we need to know**:

",open,False,2019-01-28 09:46:10,2019-02-05 02:54:52
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/579,https://api.github.com/repos/kubernetes/kubectl/issues/579,Update Kubectl GitBook,"Batch updates to kubectl book.

- Expand resource layout config
- Update kustomization docs
- Update npm deps",closed,True,2019-01-28 18:47:34,2019-01-28 19:29:24
kubectl,mdeknowis,https://github.com/kubernetes/kubectl/issues/580,https://api.github.com/repos/kubernetes/kubectl/issues/580,Missing access right leads to endless error output,"
<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):

```txt
kubectl version
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.0"", GitCommit:""ddf47ac13c1a9483ea035a79cd7c10005ff21a6d"", GitTreeState:""clean"", BuildDate:""2018-12-03T21:04:45Z"", GoVersion:""go1.11.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.6+IKS"", GitCommit:""002d263ed027db260968616b951fb46f2bab9bb1"", GitTreeState:""clean"", BuildDate:""2019-01-09T08:07:22Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
```


**Environment**:
- **Cloud provider or hardware configuration**: IBM Kubernetes

**What happened**:

`kubectl rollout status deployment` is not failing but logging thousend lines of the same error:

```txt
check rollout status of deployment
E0130 09:40:22.308805     176 reflector.go:134] k8s.io/client-go/tools/watch/informerwatcher.go:110: Failed to list *unstructured.Unstructured: deployments.extensions ""any-deployment"" is forbidden: User ""system:serviceaccount:dev:test-kubectl"" cannot list deployments.extensions in the namespace ""dev""
E0130 09:40:23.312517     176 reflector.go:134] k8s.io/client-go/tools/watch/informerwatcher.go:110: Failed to list *unstructured.Unstructured: deployments.extensions ""any-deployment"" is forbidden: User ""system:serviceaccount:dev:test-kubectl"" cannot list deployments.extensions in the namespace ""dev""
E0130 09:40:24.315702     176 reflector.go:134] k8s.io/client-go/tools/watch/informerwatcher.go:110: Failed to list *unstructured.Unstructured: deployments.extensions ""any-deployment"" is forbidden: User ""system:serviceaccount:dev:test-kubectl"" cannot list deployments.extensions in the namespace ""dev""
...
```

**What you expected to happen**:

It just failing similar to this:

```txt
Error from server (Forbidden): deployments.extensions ""any-deployment"" is forbidden: User ""system:serviceaccount:ado-dev:test-kubectl"" cannot get deployments.extensions in the namespace ""dev""
```

**How to reproduce it** (as minimally and precisely as possible):

Create `test-kubectl.yml`

```yaml
# Required because of inter namespace communication
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
#  namespace: ""dev""
  name: test-kubectl
rules:
- apiGroups: ["""", ""extensions""]
  resources: [""deployments""]
  verbs: [""get""]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: test-kubectl
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: test-kubectl
subjects:
- kind: ServiceAccount
  name: test-kubectl
  namespace: dev
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-kubectl
imagePullSecrets:
- name: bluemix-default-secret
```

Install it

```txt
kubectl -n=dev create -f test-kubectl.yml
```

execute kubectl in a pod that is assigned to this service account:
```txt
kubectl rollout status deployment any-deployment -n=dev
```



",open,False,2019-01-30 10:01:23,2019-04-01 18:23:27
kubectl,jamielennox,https://github.com/kubernetes/kubectl/issues/581,https://api.github.com/repos/kubernetes/kubectl/issues/581,go-template should accept stdin,"I just found out about --output=go-template and want to use it within a script to read some secrets and set them into local environment variables. I'm essentially using a kube secret as a place to communicate values between multiple processes. I was expecting to basically be able to do: 

```
kubectl get secret --namespace ""$NAMESPACE"" mysecret -o go-template=- << EOF > ./test-output
export DB_USER={{ .data.dbuser | b64decode }}
...
EOF
source ./test-output
```

What i end up with in my `./test-output` is `-`. I haven't gone through the code yet, but I haven't found any obvious way to use a go-template from stdin. 

I can accomplish this in other ways, but this would be good to have as well.


`Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.2"", GitCommit:""cff46ab41ff0bb44d8584413b598ad8360ec1def"", GitTreeState:""clean"", BuildDate:""2019-01-13T23:15:13Z"", GoVersion:""go1.11.4"", Compiler:""gc"", Platform:""darwin/amd64""}`",open,False,2019-02-03 07:53:03,2019-02-03 08:19:37
kubectl,duglin,https://github.com/kubernetes/kubectl/issues/582,https://api.github.com/repos/kubernetes/kubectl/issues/582,kubectl get all has lots of blank lines to stderr,"`kubectl get all` will dump a list of known resources to the screen, to stdout. However, it will also print a bunch of blank lines to stderr for no apparent reason :-)
See this:
```
$ kubectl get all > out 2> err
$ wc out err
   97   424 10355 out
   14     0    14 err
  111   424 10369 total
[master]root@docker:~/knative/helloworld$ cat err














$
```
In this case it printed 14 blank lines. This is problematic because when I try to script things I now can't just grab stdout (when there are no errors) I now need to do something with stderr even though there are no errors. If we really want blank lines in the output between the resources then send it to stdout instead. ",open,False,2019-02-03 14:56:30,2019-03-05 17:59:33
kubectl,ncouture,https://github.com/kubernetes/kubectl/issues/583,https://api.github.com/repos/kubernetes/kubectl/issues/583,Support bash-like parameter expansion,"For instance, it would be nice to be able to execute:
```
kubectl apply -f nginx-{dep,svc}.yaml
```

or

```
kubectl apply -f nginx-*.yaml
```

instead of having to execute:
```
kubectl apply -f nginx-dep.yaml
kubectl apply -f nginx-svc.yaml
```",open,False,2019-02-03 19:41:52,2019-03-05 02:18:36
kubectl,mvehar,https://github.com/kubernetes/kubectl/issues/584,https://api.github.com/repos/kubernetes/kubectl/issues/584,"[BUG] kubectl -n <NAMESPACE> delete pod,svc,....,ns --all > deletes all namespaces","**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
namespace, delete namespace, delete ns
---

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-09T18:02:47Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.4"", GitCommit:""f49fa022dbe63faafd0da106ef7e05a29721d3f1"", GitTreeState:""clean"", BuildDate:""2018-12-14T06:59:37Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""}
```


**Environment**:
- **Cloud provider or hardware configuration**: Hetzner/VM ubuntu servers
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04 LTS
- **Kernel** (e.g. `uname -a`): Linux w2-maria 4.4.0-141-generic #167-Ubuntu SMP Wed Dec 5 10:40:15 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux


**What happened**:
All namespaces were deleted

**What you expected to happen**:
Kubectl had ""-n"" flag, operation should be limited to selected namespace

**How to reproduce it** (as minimally and precisely as possible):
**!Warning! - this can/will delete all amespaces including kube-system etc.** 
`kubectl -n <NAMESPACE> delete pod,svc,....,ns --all
`
**Anything else we need to know**:
Rancher 2.1.5 cluster.
",open,False,2019-02-04 09:12:33,2019-02-04 09:13:26
kubectl,weibeld,https://github.com/kubernetes/kubectl/issues/585,https://api.github.com/repos/kubernetes/kubectl/issues/585,Completion for kubectl plugins,"**FEATURE REQUEST**

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`): 1.13

**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): macOS 10.14
- **Kernel** (e.g. `uname -a`): `Darwin Kernel Version 18.2.0: Mon Nov 12 20:24:46 PST 2018; root:xnu-4903.231.4~2/RELEASE_X86_64`


**What happened**:

Having installed a kubectl [plugin](https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/) `kubectl-test`:

~~~bash
$ kubectl t[tab][tab]
taint top
~~~

You can't complete plugin commands as you can complete built-in commands. 

**What you expected to happen**:

~~~bash
$ kubectl t[tab][tab]
taint test top
~~~

~~~bash
$ kubectl te[tab]  # Completes to 'kubectl test'
~~~

**Anything else we need to know**:

Completion for plugin names could be implemented in the kubectl completion script. But completion for arguments of plugins are plugin-dependent and can't be implemented in the kubectl completion script. In this case, plugin creators could provide completion scripts for their plugins which can then be invoked by the kubectl completion script when it detects that the user is using a plugin command.

",open,False,2019-02-06 19:09:53,2019-03-08 10:13:27
kubectl,DizzyThermal,https://github.com/kubernetes/kubectl/issues/586,https://api.github.com/repos/kubernetes/kubectl/issues/586,kubectl cluster-info dump: error: a versioned object must be passed to a printer,"**Background**:
I am performing prerequisite steps on my OpenShift Origin (OKD) v3.11 cluster to prepare to deploy [Ambassador](https://www.getambassador.io). The documentation I'm following [to deploy is here](https://www.getambassador.io/user-guide/getting-started/), specifically the part where it asks me to check if RBAC is enabled by running:
```
$ kubectl cluster-info dump --namespace kube-system | grep authorization-mode
```

this results in the error:
```
error: a versioned object must be passed to a printer
```

I understand that the answer to my actual question - whether RBAC is enabled in my OKD cluster - is likely **Yes**, but I still would expect the dump to return content, not this error.

I've searched keywords, such as: **kubectl cluster-info dump versioned object printer**, but I haven't come across a discussion about this and wasn't sure if this was a bug or something I'm failing to do.

**Kubernetes version** (use `kubectl version`): 
```
Client Version: version.Info{Major:""1"", Minor:""11+"", GitVersion:""v1.11.0+d4cacc0"", GitCommit:""d4cacc0"", GitTreeState:""clean"", BuildDate:""2018-10-15T09:45:30Z"", GoVersion:""go1.10.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11+"", GitVersion:""v1.11.0+d4cacc0"", GitCommit:""d4cacc0"", GitTreeState:""clean"", BuildDate:""2019-02-01T22:34:55Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: OpenShift Origin (OKD) v3.11
- **OS** (e.g. from /etc/os-release): RHEL Server 7.6 (Maipo)
- **Kernel** (e.g. `uname -a`): 3.10.0-957.1.3.el7.x86_64

--
Are there additional options or flags that need to be specified to make `kubectl cluster-info dump`  work?

Thanks in advance",open,False,2019-02-06 20:38:12,2019-02-28 14:29:43
kubectl,AndrisPM,https://github.com/kubernetes/kubectl/issues/587,https://api.github.com/repos/kubernetes/kubectl/issues/587,Error: forwarding ports: error upgrading connection: error dialing backend: - Azure Kubernetes Service,"Hi Team,
We have upgraded our Kubernates Service cluster on Azure to latest version 1.12.4. After that we suddenly  recognize that pods and nodes cannot communicate between anymore by private ip :

    kubectl get pods -o wide -n kube-system -l component=kube-proxy
    NAME               READY     STATUS    RESTARTS   AGE       IP           NODE
    kube-proxy-bfhbw   1/1       Running   2          16h       10.0.4.4     aks-agentpool-16086733-1
    kube-proxy-d7fj9   1/1       Running   2          16h       10.0.4.35    aks-agentpool-16086733-0
    kube-proxy-j24th   1/1       Running   2          16h       10.0.4.97    aks-agentpool-16086733-3
    kube-proxy-x7ffx   1/1       Running   2          16h       10.0.4.128   aks-agentpool-16086733-4
    
As you see the node aks-agentpool-16086733-0 has private IP 10.0.4.35 . When we try to check logs on pods which are on this node we got such error:
Get https://aks-agentpool-16086733-0:10250/containerLogs/emw-sit/nginx-sit-deploy-864b7d7588-bw966/nginx-sit?tailLines=5000&timestamps=true: dial tcp 10.0.4.35:10250: i/o timeout

We got the Tiller ( Helm) on this node as well, and if try to connect to tiller we got such error from Client PC:

    shmits-imac:~ andris.shmits01$ helm version
    Client: &version.Version{SemVer:""v2.12.3"", GitCommit:""eecf22f77df5f65c823aacd2dbd30ae6c65f186e"", GitTreeState:""clean""}
    Error: forwarding ports: error upgrading connection: error dialing backend: dial tcp 10.0.4.35:10250: i/o timeout

Does anybody have any idea why the pods and nodes lost connectivity by private IP ? 

",open,False,2019-02-07 14:50:13,2019-03-25 16:45:50
kubectl,webdeb,https://github.com/kubernetes/kubectl/issues/588,https://api.github.com/repos/kubernetes/kubectl/issues/588,Allow kubeconfig to be Base64 encoded string,"**FEATURE REQUEST**

kubectl --kubeconfig=$K8S_CONFIG_BASE64 [do something]

Would be nice, if you could just pass the whole kubeconfig as base64 string to kubectl command.
The main reason for this is to use kubectl in CI/CD pipelines, where people are storing their kubeconfig in some sort of settings/variables (like gitlabs secrets). Also if you use some kind of kubectl container to run kubectl.

AFAIK the mainly used workaround is to decode that string, create a tmp/file, and then executing the command.",open,False,2019-02-11 01:05:57,2019-04-05 00:07:47
kubectl,josefkorbel,https://github.com/kubernetes/kubectl/issues/589,https://api.github.com/repos/kubernetes/kubectl/issues/589,Password with special charts being stripped down while depoying pod,"This is not duplicate to #46 

I was following [this][1] tutorial for deploying Django App to Kubernetes Cluster. I've created cloudsql credentials and exported them as in the tutorial

```
export DATABASE_USER=<your-database-user>
export DATABASE_PASSWORD=<your-database-password>
```
However my password was generated by LastPass and contains special characters, which are striped out in Kubernetes Pod thus making the password incorrect.

This is my password (altered, just showing the special chars)
`5bb4&sL!EB%e`

So i've tried various ways of exporting this string, echoing it out always show correct password, however in Kubernetes Dashboard the password is always incorrect (Also altered in DevTools, but some chars are just stripped out)

[![enter image description here][2]][2]

Things I've tried

```
export DATABASE_PASSWORD=$'5bb4&sL\!EB\%e'
export DATABASE_PASSWORD='5bb4&sL!EB%e'
```

Echoing is always good but kubernetes is always stripping it.

Deploying with `skaffold deploy`

  [1]: https://cloud.google.com/python/django/kubernetes-engine
  [2]: https://i.stack.imgur.com/yR5o9.png",closed,False,2019-02-12 10:45:30,2019-02-12 11:28:44
kubectl,webdeb,https://github.com/kubernetes/kubectl/pull/590,https://api.github.com/repos/kubernetes/kubectl/issues/590,Feature: allow to pass kubeconfig as a base64 string,"This PR makes it possible to provide --kubeconfig=""base64:$KUBE_CONFIG_BASE64"" as proposed in the issue #588 

---

Hello Guys, this is my first code in go ever, please be cool, if something is missing, docs, tests, code, whatever.. no big deal, I'll fix it.

thx",closed,True,2019-02-15 00:04:56,2019-02-16 20:03:25
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/591,https://api.github.com/repos/kubernetes/kubectl/issues/591,Refine Kubectl GitBook,"Flesh out the GitBook more.

Hosted at: https://pwittrock-kubectl.firebaseapp.com/",closed,True,2019-02-26 02:09:03,2019-03-12 20:42:40
kubectl,rifelpet,https://github.com/kubernetes/kubectl/issues/592,https://api.github.com/repos/kubernetes/kubectl/issues/592,"""describe ingress"" does not list service endpoints for ingresses not in kube-system","**Kubernetes version** (use `kubectl version`): 
```
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.1"", GitCommit:""eec55b9ba98609a46fee712359c7b5b365bdd920"", GitTreeState:""clean"", BuildDate:""2018-12-13T19:44:19Z"", GoVersion:""go1.11.2"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.7"", GitCommit:""65ecaf0671341311ce6aea0edab46ee69f65d59e"", GitTreeState:""clean"", BuildDate:""2019-01-24T19:22:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release): N/A
- **Kernel** (e.g. `uname -a`): N/A
- **Install tools**: N/A
- **Others**:


**What happened**:
`kubectl describe ingress` on any ingress not in the kube-system namespace shows `<none>` endpoints for services that do indeed have endpoints.

```
Name:             test
Namespace:        test
Default backend:  default-http-backend:80 (<none>)
Rules:
  Host                       Path  Backends
  ----                       ----  --------
  example.com
                             /*   example:80 (<none>)
Annotations:
```

Ingresses that are in the kube-system namespace correctly have their endpoints listed:

```
Name:             test
Namespace:        kube-system
Address:
Default backend:  example:80 (X.X.X.X:80,Y.Y.Y.Y:80)
Rules:
  Host  Path  Backends
  ----  ----  --------
  *     *     example:80 (X.X.X.X:80,Y.Y.Y.Y:80)
Annotations:
```

**What you expected to happen**:

endpoints are listed for ingresses outside of the kube-system namespace. Something like:

```
Name:             test
Namespace:        test
Default backend:  default-http-backend:80 (X.X.X.X:80,Y.Y.Y.Y:80)
Rules:
  Host                       Path  Backends
  ----                       ----  --------
  example.com
                             /*   example:80 (X.X.X.X:80,Y.Y.Y.Y:80)
Annotations:
```

**How to reproduce it** (as minimally and precisely as possible):

1. Create ingresses in both the `kube-system` namespace and a different namespace. Both should point to valid services with ""ready"" endpoints.
2. Run `kubectl describe ingress` on both ingresses. The `kube-system` ingress will list the service's endpoints but the other namespace's ingress will not.

**Anything else we need to know**:

Running `kubectl -v 9 describe ingress` on the ingress in the other namespace shows some 404s for getting the service and its endpoints but incorrectly using the kube-system namespace rather than the same namespace in which the ingress is located. 

```
I0227 12:05:46.669052   43775 round_trippers.go:419] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.13.1 (darwin/amd64) kubernetes/eec55b9"" -H ""Authorization: Basic ******"" 'https://$CLUSTER_NAME/api/v1/namespaces/kube-system/endpoints/example'
I0227 12:05:46.682031   43775 round_trippers.go:438] GET https://$CLUSTER_NAME/api/v1/namespaces/kube-system/endpoints/example 404 Not Found in 12 milliseconds
I0227 12:05:46.682071   43775 round_trippers.go:444] Response Headers:
I0227 12:05:46.682084   43775 round_trippers.go:447]     Content-Type: application/json
I0227 12:05:46.682092   43775 round_trippers.go:447]     Date: Wed, 27 Feb 2019 20:05:46 GMT
I0227 12:05:46.682099   43775 round_trippers.go:447]     Content-Length: 196
I0227 12:05:46.682178   43775 request.go:942] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""endpoints \""example\"" not found"",""reason"":""NotFound"",""details"":{""name"":""example"",""kind"":""endpoints""},""code"":404}
I0227 12:05:46.682351   43775 round_trippers.go:419] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl/v1.13.1 (darwin/amd64) kubernetes/eec55b9"" -H ""Authorization: Basic ************"" 'https://$CLUSTER_NAME/api/v1/namespaces/kube-system/services/example'
I0227 12:05:46.695058   43775 round_trippers.go:438] GET https://$CLUSTER_NAME/api/v1/namespaces/kube-system/services/example 404 Not Found in 12 milliseconds
I0227 12:05:46.695089   43775 round_trippers.go:444] Response Headers:
I0227 12:05:46.695099   43775 round_trippers.go:447]     Content-Type: application/json
I0227 12:05:46.695105   43775 round_trippers.go:447]     Date: Wed, 27 Feb 2019 20:05:46 GMT
I0227 12:05:46.695110   43775 round_trippers.go:447]     Content-Length: 194
I0227 12:05:46.695176   43775 request.go:942] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""services \""example\"" not found"",""reason"":""NotFound"",""details"":{""name"":""example"",""kind"":""services""},""code"":404}
```  ",open,False,2019-02-27 20:28:31,2019-02-28 17:23:55
kubectl,briansan,https://github.com/kubernetes/kubectl/issues/593,https://api.github.com/repos/kubernetes/kubectl/issues/593,output on create/update/delete to contain namespace,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?**: No

**What keywords did you search in Kubernetes issues before filing this one?**: namespace

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): feature request

When a resource is modified in someway, the typical output from kubectl looks like the following:
```deployment.apps/nginx created```
It would be nice if the output would also contain the namespace like:
```deployment.apps/nginx created in default```

",open,False,2019-02-27 23:49:00,2019-02-27 23:49:00
kubectl,tarvitz,https://github.com/kubernetes/kubectl/issues/594,https://api.github.com/repos/kubernetes/kubectl/issues/594,kubectl get --raw /apis/* works incorrect on windows and git minigw xterm like terminal,"---

**Is this a BUG REPORT**:

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.4"", GitCommit:""c27b913fddd1a6c480c229191a087698aa92f0b1"", GitTreeState:""clean"", BuildDate:""2019-02-28T13:37:52Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""windows/amd64""}
Server Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.0"", GitCommit:""ddf47ac13c1a9483ea035a79cd7c10005ff21a6d"", GitTreeState:""clean"", BuildDate:""2018-12-03T20:56:12Z"", GoVersion:""go1.11.2"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: From scratch / server independent due to linux version of kubectl works properly.
- **OS** (e.g. from /etc/os-release): 
```NAME=""Ubuntu""
VERSION=""18.04.1 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.1 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- **Kernel** (e.g. `uname -a`):
```
Linux k8s.w40k.net 4.15.0-32-generic #35-Ubuntu SMP Fri Aug 10 17:58:07 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
```
- **Install tools**:
- **Others**: Running kubectl on windows 10 x64 in git's minigw xterm like terminal

```
$ git --version
git version 2.21.0.windows.1
```

**What happened**:
```
kubectl get --raw /apis/apps/v1
```
does not work, neither works any other apis .

**What you expected to happen**:
If it works properly

**How to reproduce it** (as minimally and precisely as possible):
```
#: type
kubectl get --raw /apis/apps/v1 -v 10

#: recieve
I0303 02:58:14.114842   18760 loader.go:359] Config loaded from file C:\Users\Tarvitz/.kube/config
I0303 02:58:14.153842   18760 round_trippers.go:419] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl.exe/v1.13.4 (windows/amd64) kubernetes/c27b913"" 'https://178.79.148.185:6443/Program%20Files/Git/apis/apps/v1'
I0303 02:58:14.387840   18760 round_trippers.go:438] GET https://178.79.148.185:6443/Program%20Files/Git/apis/apps/v1 404 Not Found in 233 milliseconds
I0303 02:58:14.387840   18760 round_trippers.go:444] Response Headers:
I0303 02:58:14.387840   18760 round_trippers.go:447]     Content-Type: application/json
I0303 02:58:14.387840   18760 round_trippers.go:447]     Content-Length: 563
I0303 02:58:14.387840   18760 round_trippers.go:447]     Date: Sat, 02 Mar 2019 23:58:15 GMT
I0303 02:58:14.387840   18760 request.go:942] Response Body: {
  ""paths"": [
    ""/apis"",
    ""/apis/"",
    ""/apis/apiextensions.k8s.io"",
    ""/apis/apiextensions.k8s.io/v1beta1"",
    ""/healthz"",
    ""/healthz/etcd"",
    ""/healthz/log"",
    ""/healthz/ping"",
    ""/healthz/poststarthook/generic-apiserver-start-informers"",
    ""/healthz/poststarthook/start-apiextensions-controllers"",
    ""/healthz/poststarthook/start-apiextensions-informers"",
    ""/metrics"",
    ""/openapi/v2"",
    ""/swagger-2.0.0.json"",
    ""/swagger-2.0.0.pb-v1"",
    ""/swagger-2.0.0.pb-v1.gz"",
    ""/swagger.json"",
    ""/swaggerapi"",
    ""/version""
  ]
}
I0303 02:58:14.387840   18760 request.go:1144] body was not decodable (unable to check for Status): Object 'Kind' is missing in '{
  ""paths"": [
    ""/apis"",
    ""/apis/"",
    ""/apis/apiextensions.k8s.io"",
    ""/apis/apiextensions.k8s.io/v1beta1"",
    ""/healthz"",
    ""/healthz/etcd"",
    ""/healthz/log"",
    ""/healthz/ping"",
    ""/healthz/poststarthook/generic-apiserver-start-informers"",
    ""/healthz/poststarthook/start-apiextensions-controllers"",
    ""/healthz/poststarthook/start-apiextensions-informers"",
    ""/metrics"",
    ""/openapi/v2"",
    ""/swagger-2.0.0.json"",
    ""/swagger-2.0.0.pb-v1"",
    ""/swagger-2.0.0.pb-v1.gz"",
    ""/swagger.json"",
    ""/swaggerapi"",
    ""/version""
  ]
}'
I0303 02:58:14.387840   18760 helpers.go:198] server response object: [{
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""the server could not find the requested resource"",
  ""reason"": ""NotFound"",
  ""details"": {
    ""causes"": [
      {
        ""reason"": ""UnexpectedServerResponse"",
        ""message"": ""unknown""
      }
    ]
  },
  ""code"": 404
}]
F0303 02:58:14.387840   18760 helpers.go:116] Error from server (NotFound): the server could not find the requested resource
```

**Anything else we need to know**:
URI for sending request by kubectl is wrong apparently:
```I0303 02:58:14.153842   18760 round_trippers.go:419] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: kubectl.exe/v1.13.4 (windows/amd64) kubernetes/c27b913"" 'https://178.79.148.185:6443/Program%20Files/Git/apis/apps/v1'```
Tried with version of kubectl got by link: https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-using-curl

Probably it's related to git's minigw terminal may be it's not, however URI request sending to server is obviously not good. kubectl running in cmd and powershell works fine.
",open,False,2019-03-03 00:08:16,2019-03-03 00:09:42
kubectl,richstokes,https://github.com/kubernetes/kubectl/issues/595,https://api.github.com/repos/kubernetes/kubectl/issues/595,"""error: unexpected EOF"" when following logs","**Kubernetes version** (use `kubectl version`):

> Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.3"", GitCommit:""721bfa751924da8d1680787490c54b9179b1fed0"", GitTreeState:""clean"", BuildDate:""2019-02-04T04:48:03Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""darwin/amd64""}
> Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.6"", GitCommit:""b1d75deca493a24a2f87eb1efde1a569e52fc8d9"", GitTreeState:""clean"", BuildDate:""2018-12-16T04:30:10Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""linux/amd64""}

**Environment**:
AWS EC2 via kops

**What happened**:
After approx 10 mins, following a log bails out with ""error: unexpected EOF"". The pod is not generating output during this time, so feels like the log follow gives up if its not receiving data often enough.

**What you expected to happen**:
Logs should be followed indefinitely, even if the pod has quiet periods.

**How to reproduce it** (as minimally and precisely as possible):
`kubectl logs <podname> -f`
",open,False,2019-03-03 17:48:49,2019-03-03 17:48:49
kubectl,tehho,https://github.com/kubernetes/kubectl/issues/596,https://api.github.com/repos/kubernetes/kubectl/issues/596,AAD with MFA Error while retrieving OAuth token: Unknown Error,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.):
Sort of.

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.):
AAD, OAuth token, Failed to acquire a token

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
Bug

**Kubernetes version** (use `kubectl version`):
Client: 1.13.3
Server: 1.12.6

**Environment**:
- **Cloud provider or hardware configuration**: AKS
- **OS** (e.g. from /etc/os-release): Windows and Mac
- **Kernel** (e.g. `uname -a`): Windows 10 1809 and Darwin Kernel Version 18.2.0: Thu Dec 20 20:46:53 PST 2018; root:xnu-4903.241.1~1/RELEASE_X86_64 x86_64
- **Install tools**: Choco for windows, brew install
- **Others**: Love the tool and kubernetes

**What happened**:
We are unable to login to kubernetes with MFA enabled. 
Our office is permitted without MFA so that work but an outside site gets error:

""To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code CUKXHW3AG to authenticate.
E0304 10:00:31.778179    3212 azure.go:127] Failed to acquire a token: acquiring a new fresh token: waiting for device code authentication to complete: autorest/adal/devicetoken: Error while retrieving OAuth token: Unknown Error
To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code C83X9S8QB to authenticate.
E0304 10:01:03.795739    3212 round_trippers.go:174] CancelRequest not implemented by *azure.azureRoundTripper""

**What you expected to happen**:
Work... No but a better error message why and yes work with MFA. 

**How to reproduce it** (as minimally and precisely as possible):
AKS with AAD integration MFA on the user logging in. 

**Anything else we need to know**:
AAD app is setup using https://docs.microsoft.com/en-us/azure/aks/aad-integration. 
",open,False,2019-03-04 09:17:43,2019-03-04 09:22:15
kubectl,philips,https://github.com/kubernetes/kubectl/issues/597,https://api.github.com/repos/kubernetes/kubectl/issues/597,SECURITY_CONTACTS: please include developers of kubectl,"The current SECURITY_CONTACTS document in kubectl simply includes the Kubernetes Product Security Team. Please replace this list with kubectl developers who should be contacted by the Kubernetes Product Security Team in case of a security disclosure.

For example you might choose a subset of the approvers from OWNERS",open,False,2019-03-04 16:16:43,2019-03-04 16:17:09
kubectl,waynesi,https://github.com/kubernetes/kubectl/issues/598,https://api.github.com/repos/kubernetes/kubectl/issues/598,"""kubectl rollout history --revision=n"" produces wrong/inconsistent output with ""-o yaml""?","<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): kubectl rollout

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): Bug Report

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.4"", GitCommit:""c27b913fddd1a6c480c229191a087698aa92f0b1"", GitTreeState:""clean"", BuildDate:""201
9-02-28T13:37:52Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.1"", GitCommit:""eec55b9ba98609a46fee712359c7b5b365bdd920"", GitTreeState:""clean"", BuildDate:""201
8-12-13T10:31:33Z"", GoVersion:""go1.11.2"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: GCP n1-highcpu-2 (2 vCPUs, 1.8 GB memory)
- **OS** (e.g. from /etc/os-release):  Ubuntu 16.04.6 LTS (Xenial Xerus)
- **Kernel** (e.g. `uname -a`):  `` Linux node0 4.15.0-1027-gcp #28~16.04.1-Ubuntu SMP Fri Jan 18 10:10:51 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux``
- **Install tools**: kubeadm

```
kubeadm version: &version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.4"", GitCommit:""c27b913fddd1a6c480c229191a087698aa92f0b1"", GitTreeState:""clean"", BuildDate:""2
019-02-28T13:35:32Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/amd64""}
```

- **Others**:


**What happened**:
I was learning the basic of rolling update on a DaemonSet. The template is very simple ...
```
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: ds-one
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
        system: DaemonSetOne
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
```

After I used ``kubectl set image ds ds-one nginx=nginx:1.12.1-alpine`` to flip the image between nginx:1.9.1 and 1.12.1-alpine back and forth a few times (and deleted the pods to get them updated), I run ``kubectl rollout history daemonset ds-one`` to check the rollout history ...

```
daemonset.extensions/ds-one 
REVISION  CHANGE-CAUSE
3         <none>
4         <none>
```

Then I use ``kubectl rollout history daemonset ds-one --revision=3`` and ``... --revision=4`` to check the details of each revision.
```
daemonset.extensions/ds-one with revision #3
Pod Template:
  Labels:       app=nginx
        system=DaemonSetOne
  Containers:
   nginx:
    Image:      nginx:1.9.1
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
```

```
daemonset.extensions/ds-one with revision #4
Pod Template:
  Labels:       app=nginx
        system=DaemonSetOne
  Containers:
   nginx:
    Image:      nginx:1.12.1-alpine
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
```

However, when I repeated the same  two commands with extra ""-o yaml"", I now got the exact same results which say ``- image: nginx:1.12.1-alpine`` (the latest revision) regardless which revision I specified in the command.


**What you expected to happen**:
The help says ``-o`` only applies to the output format while ``--revision`` shows the details. So using ``-o`` together with ``--revision`` should not change the details produced by ``--revision`` option I reckon?






",open,False,2019-03-04 22:05:27,2019-03-04 22:05:27
kubectl,sassanh,https://github.com/kubernetes/kubectl/issues/599,https://api.github.com/repos/kubernetes/kubectl/issues/599,"Unresolved issue mistakenly closed, please reopen","This is not a bug report nor question, it's a meta issue to reopen another issue.

https://github.com/kubernetes/kubectl/issues/151 is about:

1. Bad documentation (which is solved now)
2. Kubernetes not having a reasonably simple API to query ""ALL"" resources in a namespace (which is not resolved yet.)

As many commented there that command doesn't work for me too with the same error they reported.

As that issue is not resolved yet and adding comments to a closed issue doesn't bring any attention I create this issue to ask you kindly reopen that issue (and do not close it nor any other issues until they're solved).",closed,False,2019-03-05 17:45:48,2019-03-05 17:51:49
kubectl,wyardley,https://github.com/kubernetes/kubectl/issues/600,https://api.github.com/repos/kubernetes/kubectl/issues/600,Kubectl should not validate whether there's an active gcloud config,"**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): Google search for exact error message. Turned up #23496 (already closed), however I think this is a slightly different use case. Also turned up some ones related to spurious errors.

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.0"", GitCommit:""ddf47ac13c1a9483ea035a79cd7c10005ff21a6d"", GitTreeState:""clean"", BuildDate:""2018-12-03T21:04:45Z"", GoVersion:""go1.11.2"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: GKE
- **OS** (e.g. from /etc/os-release): Debian 9
- **Kernel** (e.g. `uname -a`): `Linux b4b75670c524 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 GNU/Linux`
- **Install tools**:
- **Others**:


**What happened**:
(slightly simplified to avoid using variables in shell scripts)

`gcloud -q --configuration ""some-project"" --project ""some-project"" container clusters get-credentials ""somecluster"" --zone ""us-central1-a""`
`gcloud config -q configurations describe ""some-project"" > /dev/null 2>&1 || gcloud config -q configurations create ""some-project"" --no-activate`
`gcloud --configuration=""some-project"" auth activate-service-account --key-file ""/some/file.json""`
[this generates a valid Kube config in `~/.kube/config`]
`kubectl --context ""gke_some-project_us-central1-a_somecluster"" --cluster ""gke_some-project_us-central1-a_somecluster"" delete -f kubernetes.yml`

**What you expected to happen**:
`kubectl` would run the requested operation (in the example above, `delete`). IMHO, I should not need to have an _active_ `gcloud` configuration to run `kubectl`. In the past, use of these two tools was more or less completely decoupled; the behavior in 1.13 seems a little too ""magic"" to me.

**How to reproduce it** (as minimally and precisely as possible):
Run any `kubectl` command against a GKE cluster without an active `gcloud` config, without `CLOUDSDK_CONFIG` or `CLOUDSDK_ACTIVE_CONFIG_NAME` set

**Anything else we need to know**:
See also https://stackoverflow.com/questions/52704015/unable-to-access-kubernetes-cluster-using-the-go-client-when-cloudsdk-config-is",open,False,2019-03-05 21:35:03,2019-03-06 00:09:19
kubectl,ajmanlove,https://github.com/kubernetes/kubectl/issues/601,https://api.github.com/repos/kubernetes/kubectl/issues/601,kubectl get logs does not return all expected log entries,"**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):

BUG REPORT (possibly)
Seeing this on two clients (aws linux and darwin) and a single shared server, debian.

**Kubernetes version** (use `kubectl version`):
My Local mac
```
Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:17:39Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.6"", GitCommit:""9f8ebd171479bec0ada837d7ee641dec2f8c6dd1"", GitTreeState:""clean"", BuildDate:""2018-03-21T15:13:31Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

AWS Ec2 box:
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.4"", GitCommit:""9befc2b8928a9426501d3bf62f72849d5cbcd5a3"", GitTreeState:""clean"", BuildDate:""2017-11-20T05:28:34Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.6"", GitCommit:""9f8ebd171479bec0ada837d7ee641dec2f8c6dd1"", GitTreeState:""clean"", BuildDate:""2018-03-21T15:13:31Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
Server AWS Hosted, kops deployment, k8s-1.9-debian-jessie-amd64-hvm-ebs-2018-03-11 (ami-4bfe6f33)
- **Others**:


**What happened**:
**What you expected to happen**:
Was noticing our logging ETL was dropping records. Narrowed this down to the step in the process that gathers logs from k8s by way of kubectl. This uses a command such as this, once a minute:

```kubectl --namespace NS logs POD_ID --since-time 2019-03-07T14:35:34.315Z```

In debugging, I generated large volumes of known log records and asserted their passage through the etl. It appears that `kubectl get logs` is returning only some of the logs I expect. For example, generating an expected 30,000 log messages, the command `kubectl --namespace NS logs POD_ID` returns only the last 4719 messages, a total of 3044213 bytes.

Use of `--limit-bytes` flag does not appear to change this behavior. Is it possible that `get logs` may be enforcing some hard coded byte limit or perhaps some other misconfiguration?

**How to reproduce it** (as minimally and precisely as possible):
Generate a large volume of log entries and either ask for all logs since container start or with a timestamp far enough back in time to have the resulting log payload large enough. 

Any insight is appreciated.

",closed,False,2019-03-07 21:21:02,2019-03-07 21:52:10
kubectl,joelsmith,https://github.com/kubernetes/kubectl/pull/602,https://api.github.com/repos/kubernetes/kubectl/issues/602,Update embargo doc link in SECURITY_CONTACTS and change PST to PSC,See https://github.com/kubernetes/security/issues/8 for more information,closed,True,2019-03-08 18:08:38,2019-03-20 02:46:26
kubectl,jon-walton,https://github.com/kubernetes/kubectl/issues/603,https://api.github.com/repos/kubernetes/kubectl/issues/603,"Windows 10 WSL - ""cannot allocate memory"" when calling a plugin","<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): windows; allocate

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): bug report

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`): ```Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.3"", GitCommit:""721bfa751924da8d1680787490c54b9179b1fed0"", GitTreeState:""clean"", BuildDate:""2019-02-01T20:08:12Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/amd64""}```


**Environment**:
- **Cloud provider or hardware configuration**: 
- **OS** (e.g. from /etc/os-release): Windows 10.0.17763 / Ubuntu 18.04.2 LTS
- **Kernel** (e.g. `uname -a`): ```Linux DESKTOP-3I1L0FQ 4.4.0-17763-Microsoft #253-Microsoft Mon Dec 31 17:49:00 PST 2018 x86_64 x86_64 x86_64 GNU/Linux```
- **Install tools**:
- **Others**:


**What happened**:

kubectl often returns ```cannot allocate memory``` instead of executing a plugin. Calling the plugin directly always works.

**What you expected to happen**:

calling the plugin via kubectl to work as if called directly

**How to reproduce it** (as minimally and precisely as possible):

Use the example plugin from https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#example-plugin
```text
$ k plugin list
The following kubectl-compatible plugins are available:

/home/jon/.kube/plugins/kubectl-foo
```

kubectl often returns ```cannot allocate memory``` instead of executing the plugin.
```text
$ k foo
cannot allocate memory
$ k foo version
1.0.0
$ k foo version
cannot allocate memory
$ k foo version
cannot allocate memory
$ k foo
cannot allocate memory
$ k foo
cannot allocate memory
$ k foo
cannot allocate memory
$ k foo
cannot allocate memory
$ k foo
I am a plugin named kubectl-foo
$ k foo
cannot allocate memory
```

executing the plugin directly always works
```text
$ kubectl-foo version
1.0.0
$ kubectl-foo
I am a plugin named kubectl-foo
$ kubectl-foo version
1.0.0
$ kubectl-foo version
1.0.0
$ kubectl-foo version
1.0.0
$ kubectl-foo version
1.0.0
$ kubectl-foo version
1.0.0
```

**Anything else we need to know**:

This also happens with windows compatible plugins when called directly from Windows 10, rather then WSL; however it's easier for me to provide details using linux tooling.

Here's a `strace -o kubectl -ff -y kubectl foo version` 
[kubectl.zip](https://github.com/kubernetes/kubectl/files/2948451/kubectl.zip)

",open,False,2019-03-09 14:10:36,2019-04-05 17:33:05
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/604,https://api.github.com/repos/kubernetes/kubectl/issues/604,add gitbook-cli into kubectl book package.json,"add gitbook-cli into kubectl book package.json

Necessary for hosting on netlify",closed,True,2019-03-11 21:42:38,2019-03-12 16:08:40
kubectl,HariSekhon,https://github.com/kubernetes/kubectl/issues/605,https://api.github.com/repos/kubernetes/kubectl/issues/605,'kubectl desc' shortcut for 'kubectl describe',"**Kubernetes version** (use `kubectl version`): 1.8

Feature Request to have `kubectl describe` to support the shorted form `kubectl desc`, in a similar way that you can `kubectl get po` instead of `kubectl get pods`.


",closed,False,2019-03-12 11:21:14,2019-03-12 12:25:12
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/606,https://api.github.com/repos/kubernetes/kubectl/issues/606,a test pr for showing kubectl book,,closed,True,2019-03-12 18:24:36,2019-03-12 20:30:07
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/pull/607,https://api.github.com/repos/kubernetes/kubectl/issues/607,add google analytics to kubectl book,,closed,True,2019-03-12 20:55:53,2019-03-26 14:38:20
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/608,https://api.github.com/repos/kubernetes/kubectl/issues/608,Kubectl Book: Add Favicon and clean wording,"- Add a favicon.ico for the kubectl logo
- Minor wording cleanup on kubectl introduction",closed,True,2019-03-13 00:39:11,2019-03-13 15:09:36
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/609,https://api.github.com/repos/kubernetes/kubectl/issues/609,GitBook: Refine App Composition and Deployment guidelines,- Also make them more clearly Alpha,closed,True,2019-03-13 15:46:33,2019-03-13 20:56:34
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/610,https://api.github.com/repos/kubernetes/kubectl/issues/610,Refine App Composition and Deployment guidelines,- Also make them more clearly Alpha / Beta,closed,True,2019-03-14 17:16:14,2019-03-14 17:48:29
kubectl,soltysh,https://github.com/kubernetes/kubectl/pull/611,https://api.github.com/repos/kubernetes/kubectl/issues/611,Various fixes,/assign @pwittrock ,closed,True,2019-03-14 22:03:11,2019-03-26 09:40:14
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/612,https://api.github.com/repos/kubernetes/kubectl/issues/612,kubectl book - add `newName` function,- update the customizing pod template with `newName`,closed,True,2019-03-14 22:19:49,2019-03-14 23:52:57
kubectl,rgembalik,https://github.com/kubernetes/kubectl/issues/613,https://api.github.com/repos/kubernetes/kubectl/issues/613,kubectl powershell completion,"**FEATURE REQUEST**
It would be great to see autocompletion support for PowerShell. Is it possible for Windows releases?",open,False,2019-03-14 22:25:57,2019-03-14 22:25:57
kubectl,RPing,https://github.com/kubernetes/kubectl/issues/614,https://api.github.com/repos/kubernetes/kubectl/issues/614,autocompletion on CustomResourceDefinition,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): CustomResourceDefinition

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): FEATURE REQUEST

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.7"", GitCommit:""65ecaf0671341311ce6aea0edab46ee69f65d59e"", GitTreeState:""clean"", BuildDate:""2019-01-24T19:32:00Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11+"", GitVersion:""v1.11.7-gke.12"", GitCommit:""06f08e60069231bd21bdf673cf0595aac80b99f6"", GitTreeState:""clean"", BuildDate:""2019-02-25T20:37:10Z"", GoVersion:""go1.10.8b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Install tools**: gcloud
- **Others**: bash


**What happened**:
some CustomResourceDefinition I created by istio, e.g. VirtualService
```
$ kubectl get [TAB]
certificatesigningrequest  deployment                 networkpolicy              podtemplate                statefulset
clusterrolebinding         endpoints                  node                       replicaset                 status
componentstatus            event                      persistentvolume           replicationcontroller      storageclass
configmap                  horizontalpodautoscaler    persistentvolumeclaim      rolebinding
controllerrevision         ingress                    pod                        secret
cronjob                    job                        poddisruptionbudget        service
daemonset                  namespace                  podsecuritypolicy          serviceaccount
```

**What you expected to happen**:
autocompletion on CustomResourceDefinition

**How to reproduce it** (as minimally and precisely as possible):
`kubectl get [TAB]`

**Anything else we need to know**:
by `kubectl api-resources`, I'm sure the CustomResourceDefinition I created exists:
```
virtualservices                                   networking.istio.io                true         VirtualService
```
and `kubectl get virtualservices --all-namespaces` confirmed that.",closed,False,2019-03-16 11:31:19,2019-03-16 16:49:56
kubectl,ticapix,https://github.com/kubernetes/kubectl/issues/615,https://api.github.com/repos/kubernetes/kubectl/issues/615,kubectl get deploy -o yaml | kubectl apply -f - fails,"**Is this a BUG REPORT**:

**Kubernetes version** (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.4"", GitCommit:""c27b913fddd1a6c480c229191a087698aa92f0b1"", GitTreeState:""clean"", BuildDate:""2019-02-28T13:37:52Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.8"", GitCommit:""4e209c9383fa00631d124c8adcc011d617339b3c"", GitTreeState:""clean"", BuildDate:""2019-02-28T18:40:05Z"", GoVersion:""go1.10.8"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **OVH Managed kubernetes**:

**What happened**:
I've deployed gravitee.io with helm and I've tried to activate linkerd. While doing so, I figured out that the following command doesn't work

`kubectl get -n default deploy gravitee-ui -o yaml | kubectl apply -f -`

```
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
warning: error calculating patch from openapi spec: map: map[] does not contain declared merge key: name
```

Full output [error.log](https://github.com/kubernetes/kubectl/files/2975055/error.log)

Output of the `kubectl get` being feed to `kubectl apply` [config.txt](https://github.com/kubernetes/kubectl/files/2975054/config.txt)


**What you expected to happen**:

I'm expecting that the yaml generated by kubectl can be applied back without modification to kubectl

**How to reproduce it** (as minimally and precisely as possible):

- Install gravitee.io using this helm chart https://github.com/gravitee-io/gravitee-kubernetes/tree/master/gravitee
- trying the above command

**Anything else we need to know**:

I've added a `name` field to some place where I knew there could be one, but I'm still having the same issue, 
",closed,False,2019-03-17 11:29:00,2019-03-24 12:50:45
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/616,https://api.github.com/repos/kubernetes/kubectl/issues/616,GitBook: Update structure titles,,closed,True,2019-03-18 19:51:33,2019-03-18 20:12:21
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/617,https://api.github.com/repos/kubernetes/kubectl/issues/617,kubectl book: Fix exec examples caused by bad refactoring,"- refactoring replaced '$ ' with 'deploy'
- should have replaced '$ ' with ''",closed,True,2019-03-19 16:39:52,2019-03-19 16:56:27
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/618,https://api.github.com/repos/kubernetes/kubectl/issues/618,Address Doc Feedback,,closed,True,2019-03-20 20:33:05,2019-03-22 17:48:50
kubectl,seans3,https://github.com/kubernetes/kubectl/pull/619,https://api.github.com/repos/kubernetes/kubectl/issues/619,Adds kubectl logo to repository,"* Adds kubectl logo images in three sizes (full, medium, and small)
* The full size is 3182x3200
* The medium size is 300x300
* The small size is 200x200
* Includes the medium logo into the README.md",closed,True,2019-03-21 23:48:38,2019-03-21 23:58:19
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/620,https://api.github.com/repos/kubernetes/kubectl/issues/620,Kubectl Book Final Edits,,closed,True,2019-03-22 23:32:52,2019-03-24 19:06:32
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/621,https://api.github.com/repos/kubernetes/kubectl/issues/621,Fix examples,,closed,True,2019-03-25 16:28:19,2019-03-25 16:28:49
kubectl,Liujingfang1,https://github.com/kubernetes/kubectl/issues/622,https://api.github.com/repos/kubernetes/kubectl/issues/622,request a repo in kubernetes-sigs,"Request to open a new repo under kubernetes-sigs.
We'll use this repo to experiment Apply and Prune commands
and corresponding libraries.

Here are ideas we plan to work on POC for:
- https://github.com/kubernetes/enhancements/pull/886
- https://github.com/kubernetes/enhancements/pull/810

The name of this repo could be kapply.",closed,False,2019-03-25 17:23:38,2019-03-28 16:08:18
kubectl,dankohn,https://github.com/kubernetes/kubectl/issues/623,https://api.github.com/repos/kubernetes/kubectl/issues/623,Please upload svg of kubectl logo,"https://github.com/kubernetes/kubectl/pull/619 added the kubectl logo in PNG format in 3 sizes.

Could you please upload as an svg, which scales to any resolution and is suitable for print. Alternatively, you can email the source file (AI, PSD, etc.) to dan at linuxfoundation.org and I'll convert it and upload.

I'll also get stickers made for https://store.cncf.io/collections/kubernetes

Cc @seans3 @pwittrock ",open,False,2019-03-25 21:50:00,2019-03-25 21:50:00
kubectl,jonas,https://github.com/kubernetes/kubectl/pull/624,https://api.github.com/repos/kubernetes/kubectl/issues/624,Remove cp examples from the port-forward page,"The examples at the end of the port_forward_to_pods page are derived
from the copying_container_files page when it was created.",closed,True,2019-03-26 03:32:20,2019-03-29 16:48:59
kubectl,superbrothers,https://github.com/kubernetes/kubectl/issues/625,https://api.github.com/repos/kubernetes/kubectl/issues/625,kubectl docs: Need Open Graph tags,"kubectl docs does not have Open Graph (og) tags. I think it is better to have them. (kubernetes.io has og tags.)

- http://ogp.me/
",open,False,2019-03-26 08:30:13,2019-03-26 08:30:13
kubectl,BenHall,https://github.com/kubernetes/kubectl/pull/626,https://api.github.com/repos/kubernetes/kubectl/issues/626,Dockerfile for GitBook,A Dockerfile for making it easier to run the GitBook locally when making changes,open,True,2019-03-26 10:28:19,2019-03-26 10:28:44
kubectl,thomas-riccardi,https://github.com/kubernetes/kubectl/issues/627,https://api.github.com/repos/kubernetes/kubectl/issues/627,"Typo in docs/book: ""Conainer""","<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): typo, conainer

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

There are typos in https://kubectl.docs.kubernetes.io/ : ""conainer"" instead of ""container"" : https://github.com/kubernetes/kubectl/search?q=Conainer

",closed,False,2019-03-26 13:56:05,2019-03-28 15:52:45
kubectl,dhain,https://github.com/kubernetes/kubectl/issues/628,https://api.github.com/repos/kubernetes/kubectl/issues/628,"kubectl proxy, API watch request blocks until an event happens","<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): no

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): proxy, watch, resourceVersion

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

If this is a FEATURE REQUEST, please:
  - Describe *in detail* the feature/behavior/change you'd like to see.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.0"", GitCommit:""641856db18352033a0d96dbc99153fa3b27298e5"", GitTreeState:""clean"", BuildDate:""2019-03-25T15:53:57Z"", GoVersion:""go1.12.1"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.11"", GitCommit:""637c7e288581ee40ab4ca210618a89a555b6e7e9"", GitTreeState:""clean"", BuildDate:""2018-11-26T14:25:46Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: bare metal
- **OS** (e.g. from /etc/os-release): coreos
- **Kernel** (e.g. `uname -a`): 4.19.25-coreos
- **Install tools**: Typhoon
- **Others**:


**What happened**:
API requests with `watch=true` that don't immediately return any events (eg. with the most recent `resourceVersion`) via `kubectl proxy` completely block without returning HTTP headers until an event comes in.

**What you expected to happen**:
HTTP headers (ie. with `Transfer-Encoding: chunked`) should be returned immediately. This is a problem for my application, because the HTTP library I'm using won't return a streaming response object until it gets the headers from the server.

**How to reproduce it** (as minimally and precisely as possible):
```
$ kubectl proxy &
Starting to serve on 127.0.0.1:8001
$ RESOURCE_VERSION=$(curl -s -XGET http://localhost:8001/api/v1/pods | awk -F\"" '/resourceVersion/ { print $4 }' | head -n1)
$ curl --no-buffer --http1.1 -i -XGET ""http://localhost:8001/api/v1/pods?resourceVersion=${RESOURCE_VERSION}&watch=true""
```
Above just hangs without showing any output. When I request against the API directly without going through the proxy, it works correctly:
```
$ curl --no-buffer --http1.1 --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt -H ""Authorization: bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)"" --no-buffer -i -XGET ""https://kubernetes.default.svc/api/v1/pods?resourceVersion=${RESOURCE_VERSION}&watch=true""
HTTP/1.1 200 OK
Content-Type: application/json
Date: Wed, 27 Mar 2019 15:52:49 GMT
Transfer-Encoding: chunked

```

**Anything else we need to know**:
This appears to be new behavior in 1.14.0, as this application worked with prior versions of kubectl.
",open,False,2019-03-27 15:56:12,2019-03-27 15:59:54
kubectl,Rio,https://github.com/kubernetes/kubectl/issues/629,https://api.github.com/repos/kubernetes/kubectl/issues/629,Typo in docs and command reference error in `cluster information`,"Found a typo in `Field Merge Semantics` in the new docs.
`Last Applied Resource Condfig` should be `Last Applied Resource Config`.

Also a non-existent command is referenced in `Cluster Information`: `kubectl resource-types`.
The example does use the correct command `kubectl api-resources`.",closed,False,2019-03-28 09:38:53,2019-03-28 15:48:47
kubectl,Rio,https://github.com/kubernetes/kubectl/pull/630,https://api.github.com/repos/kubernetes/kubectl/issues/630,fix(docs): Fix typo and command reference,closes #629 ,closed,True,2019-03-28 09:40:36,2019-03-28 16:04:08
kubectl,Rio,https://github.com/kubernetes/kubectl/issues/631,https://api.github.com/repos/kubernetes/kubectl/issues/631,Email address listed in docs contribution guide is bouncing,The contributions guide linked to in the introduction of the docs (https://github.com/kubernetes/kubectl/blob/master/docs/book/CONTRIBUTING.md) lists an email address which appears to be bouncing.,open,False,2019-03-28 10:05:00,2019-03-28 10:05:00
kubectl,Rio,https://github.com/kubernetes/kubectl/issues/632,https://api.github.com/repos/kubernetes/kubectl/issues/632,Configmap/Secret generation documentation is missing an option for generating from environment files,"In the section `Secrets and ConfigMaps` there are explanations about how to generate configmaps using the `literals` and `files` options. One is missing however which is the `env` option that generates the same type of configmaps as the `literals` option but sources the key/value pairs from an environment file.

Also it's not very clear that the options used on `configMapGenerator` are also used in `secretGenerator`.",open,False,2019-03-28 10:41:21,2019-03-28 10:41:21
kubectl,chrisz100,https://github.com/kubernetes/kubectl/pull/633,https://api.github.com/repos/kubernetes/kubectl/issues/633,Fixed a typo in field merge semantics,,closed,True,2019-03-28 10:53:35,2019-03-28 15:51:09
kubectl,Rio,https://github.com/kubernetes/kubectl/pull/634,https://api.github.com/repos/kubernetes/kubectl/issues/634,feat(docs): add section on environment style configmap/secret generation,"This adds an example of generating configmaps/secrets using the `env` option.
It also adds a bit of clarification that the same `literals`, `files` and `env` options can be used for secrets.

closes #632 ",open,True,2019-03-28 10:56:16,2019-03-28 10:57:03
kubectl,chrisz100,https://github.com/kubernetes/kubectl/pull/635,https://api.github.com/repos/kubernetes/kubectl/issues/635,replaced Conainer with Container,fixes #627 ,closed,True,2019-03-28 11:43:25,2019-03-28 15:52:45
kubectl,mikeraimondi,https://github.com/kubernetes/kubectl/pull/636,https://api.github.com/repos/kubernetes/kubectl/issues/636,fix typo,,closed,True,2019-04-01 15:20:52,2019-04-02 15:08:37
kubectl,pplu,https://github.com/kubernetes/kubectl/issues/637,https://api.github.com/repos/kubernetes/kubectl/issues/637,kubectl --help exit code 0,"**Kubernetes version** (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.0"", GitCommit:""641856db18352033a0d96dbc99153fa3b27298e5"", GitTreeState:""clean"", BuildDate:""2019-03-25T15:53:57Z"", GoVersion:""go1.12.1"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Not needed
- **OS** (e.g. from /etc/os-release): Linux

**What happened**:
```
pplu@beezelbot:~/$ kubectl
kubectl controls the Kubernetes cluster manager.
[...]
pplu@beezelbot:~/$ echo $?
0

pplu@beezelbot:~/$ kubectl get pods --help
Display one or many resources
[...]
pplu@beezelbot:~/$ echo $?
0
```

**What you expected to happen**:

I would expect a non-zero exit code from the command when help is displayed.

",open,False,2019-04-02 12:34:14,2019-04-02 12:34:14
kubectl,pwittrock,https://github.com/kubernetes/kubectl/pull/638,https://api.github.com/repos/kubernetes/kubectl/issues/638,Book: update labeling recommendations,,closed,True,2019-04-04 14:44:31,2019-04-04 14:49:53
kubectl,dellintosh,https://github.com/kubernetes/kubectl/issues/639,https://api.github.com/repos/kubernetes/kubectl/issues/639,kubectl set env -o yaml removes namespace,"<!-- Thanks for filing an issue! Before hitting the button, please answer these questions. -->

**Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No

**What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): `set env`, `namespace`

---

**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): **BUG REPORT**

<!--
If this is a BUG REPORT, please:
  - Fill in as much of the template below as you can.  If you leave out
    information, we can't help you as well.

In both cases, be ready for followup questions, and please respond in a timely
manner.  If we can't reproduce a bug or think a feature already exists, we
might close your issue.  If we're wrong, PLEASE feel free to reopen it and
explain why.
-->

**Kubernetes version** (use `kubectl version`): 
```console
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.4"", GitCommit:""c27b913fddd1a6c480c229191a087698aa92f0b1"", GitTreeState:""clean"", BuildDate:""2019-03-01T23:36:43Z"", GoVersion:""go1.12"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.7"", GitCommit:""65ecaf0671341311ce6aea0edab46ee69f65d59e"", GitTreeState:""clean"", BuildDate:""2019-01-24T19:22:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Example is run locally (`--local` flag)
- **OS** (e.g. from /etc/os-release): MacOS 10.13.6
- **Kernel** (e.g. `uname -a`): Darwin Kernel Version 17.7.0
- **Install tools**: 
- **Others**:

**What happened**: When running the following command against the provided deployment yaml, the output removes the `namespace` field from the yaml.

**What you expected to happen**: I expect the environment variable to be injected (or updated) but the rest of the deployment definition to remain the same, in this case the `namespace` (if set) value should not be removed.

**How to reproduce it** (as minimally and precisely as possible): 

Assuming that you have a local sample file `nginx-sample.yaml`: 

`nginx-sample.yaml`:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: nginx-test
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: 512Mi
            cpu: 200m
          limits:
            memory: 1Gi
            cpu: 400m
```

run the following command:

```console
$ kubectl set env -o yaml --local -f nginx-sample.yaml -e FOO=BAR
```
Notice that the output yaml has removed the `nginx-test` namespace:
```diff
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx-deployment
- namespace: nginx-test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - env:
+        - name: FOO
+          value: bar
        image: nginx:1.7.9
        name: nginx
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 400m
            memory: 1Gi
          requests:
            cpu: 200m
            memory: 512Mi
```

**Anything else we need to know**: I have also attempted to add the namespace as an inline argument:
```console
$ kubectl set env --namespace=nginx-test -o yaml --local -f nginx-sample.yaml -e FOO=BAR
$ kubectl set env -n nginx-test -o yaml --local -f nginx-sample.yaml -e FOO=BAR
```
with the same results as above.
",open,False,2019-04-04 20:30:59,2019-04-04 21:06:34
