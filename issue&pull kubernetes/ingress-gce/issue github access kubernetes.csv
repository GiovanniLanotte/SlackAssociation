name repository,creator user,url_html issue,url_api issue,title,body,state,pull request,data open,updated at
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/1,https://api.github.com/repos/kubernetes/ingress-gce/issues/1,Handle forbiddenError for XPN clusters by raising event,,closed,True,2017-10-06 16:58:19,2018-04-18 19:33:16
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/2,https://api.github.com/repos/kubernetes/ingress-gce/issues/2,Update repo to be GCE specific,"Please review commit-by-commit.

FYI: viewing the history of a moved file requires using `git log --follow [file]`. From what I've googled, Github does not use `follow` when showing commit history.

Todo in separate PR:
 - Migrate to dep instead of godep, remove unnecessary vendored dependencies at the same time.
 - Move main.go into a `cmds/glbc` dir.",closed,True,2017-10-06 18:06:39,2018-04-18 19:33:15
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/3,https://api.github.com/repos/kubernetes/ingress-gce/issues/3,Fix issue when setting instance group named ports,"Found an issue regarding the recent change to setting named ports. The wrong list was used when being appended to the instance group's existing list of ports. If you run the added test case, the unit test shows that ports get duplicated. ",closed,True,2017-10-06 23:05:09,2018-04-18 19:33:18
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/4,https://api.github.com/repos/kubernetes/ingress-gce/issues/4,Migrate to dep,"Ran:
```
dep init
dep ensure
dep prune
```

Dep does not do a great job at pruning, yet. Some dep users are depending on .gitignore to remove unneeded files, especially `_test.go`
```text
https://github.com/golang/dep/issues/944
https://github.com/golang/dep/issues/1113
https://github.com/golang/dep/issues/120#issuecomment-306518546
```



Resulting dep status:
```
➜  dep status
PROJECT                                           CONSTRAINT          VERSION             REVISION  LATEST   PKGS USED
cloud.google.com/go                               *                                       3b1ae45            2   
github.com/PuerkitoBio/purell                     *                   v1.0.0              8a29053   0bcb03f  1   
github.com/PuerkitoBio/urlesc                     *                                       5bd2802            1   
github.com/beorn7/perks                           *                                       3ac7bf7            1   
github.com/davecgh/go-spew                        *                                       782f496            1   
github.com/dgrijalva/jwt-go                       *                                       01aeca5            1   
github.com/docker/distribution                    *                                       edc3ab2            2   
github.com/emicklei/go-restful                    *                                       ff4f55a            2   
github.com/emicklei/go-restful-swagger12          *                   1.0.1               dcef7f5   dcef7f5  1   
github.com/ghodss/yaml                            *                                       73d445a            1   
github.com/go-openapi/jsonpointer                 *                                       46af16f            1   
github.com/go-openapi/jsonreference               *                                       13c6e35            1   
github.com/go-openapi/spec                        *                                       6aced65            1   
github.com/go-openapi/swag                        *                                       1d0bd11            1   
github.com/gogo/protobuf                          *                                       c0656ed            2   
github.com/golang/glog                            *                                       44145f0            1   
github.com/golang/groupcache                      *                                       02826c3            1   
github.com/golang/protobuf                        *                                       4bd1920            5   
github.com/google/btree                           *                                       7d79101            1   
github.com/google/gofuzz                          *                                       44d8105            1   
github.com/googleapis/gnostic                     *                                       0c51083            3   
github.com/gorilla/websocket                      *                                       6eb6ad4            1   
github.com/gregjones/httpcache                    *                                       787624d            2   
github.com/hashicorp/golang-lru                   *                                       a0d98a5            2   
github.com/howeyc/gopass                          *                   branch master       bf9dde6   bf9dde6  1   
github.com/imdario/mergo                          *                                       6633656            1   
github.com/json-iterator/go                       *                   1.0.0               36b1496   6ed2715  1   
github.com/juju/ratelimit                         *                   branch master       5b9ff86   5b9ff86  1   
github.com/mailru/easyjson                        *                                       d5b7844            3   
github.com/matttproud/golang_protobuf_extensions  *                                       fc2b8d3            1   
github.com/opencontainers/go-digest               *                                       a6d0ee4            1   
github.com/pborman/uuid                           *                                       ca53cad            1   
github.com/petar/GoLLRB                           *                   branch master       53be0d3   53be0d3  1   
github.com/peterbourgon/diskv                     *                   v2.0.1              5f041e8   5f041e8  1   
github.com/prometheus/client_golang               *                                       e7e9030            2   
github.com/prometheus/client_model                *                                       fa8ad6f            1   
github.com/prometheus/common                      *                                       13ba4dd            3   
github.com/prometheus/procfs                      *                                       65c1f6f            2   
github.com/spf13/pflag                            *                                       9ff6c69            1   
golang.org/x/crypto                               *                                       81e9090            4   
golang.org/x/net                                  *                                       1c05540            6   
golang.org/x/oauth2                               *                                       a6bd8ce            5   
golang.org/x/sys                                  *                                       7ddbeae            2   
golang.org/x/text                                 *                                       b19bf47            16  
google.golang.org/api                             *                                       98825bb            8   
google.golang.org/appengine                       *                   v1.0.0              150dc57   150dc57  10  
gopkg.in/gcfg.v1                                  *                   v1.2.0              27e4946   27e4946  4   
gopkg.in/inf.v0                                   *                   v0.9.0              3887ee9   3887ee9  1   
gopkg.in/warnings.v0                              *                   v0.1.1              8a33156   8a33156  1   
gopkg.in/yaml.v2                                  *                                       53feefa            1   
k8s.io/api                                        branch release-1.8  branch release-1.8  fe29995   fe29995  24  
k8s.io/apiextensions-apiserver                    *                                       e15e93d            1   
k8s.io/apimachinery                               branch release-1.8  branch release-1.8  9d38e20   9d38e20  50  
k8s.io/apiserver                                  *                   branch release-1.8  f9b476d   c1e53d7  11  
k8s.io/client-go                                  *                   branch master       82aa063   d92e849  79  
k8s.io/kube-openapi                               *                                       868f2f2            1   
k8s.io/kubernetes                                 ^1.8.0              v1.8.0              0b9efae   0b9efae  33  
k8s.io/utils                                      *                                       9fdc871            1   
```

Note: I'm purposely preventing github auto-linking so we're not spamming the golang/dep repo. 
",closed,True,2017-10-06 23:31:18,2017-10-09 18:08:44
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/5,https://api.github.com/repos/kubernetes/ingress-gce/issues/5,Move main.go to cmd/glbc,"The makefile should be thrown away and remade at some point, but it's a non-goal for this PR. ",closed,True,2017-10-07 00:41:18,2017-10-09 18:08:42
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/6,https://api.github.com/repos/kubernetes/ingress-gce/issues/6,Minor cleanup to instance group management,"Changes
- Fixed log line which was previously printing a slice of memory addresses instead of port numbers.
https://github.com/kubernetes/ingress-gce/blob/ef0e824a0250c97298b65b87fbf573898d456642/instances/instances.go#L119
- Renamed ""AddInstanceGroup"" to ""EnsureInstanceGroupsAndPorts"" to actually reflect the actions of the func
- Split ""EnsureInstanceGroups"" and created ""EnsureInstanceGroupAndPorts"" for the actions on a single instance group.
- Removed unused return variable of nodeports from ""EnsureInstanceGroupsAndPorts""",closed,True,2017-10-07 01:42:43,2018-04-18 19:33:19
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/7,https://api.github.com/repos/kubernetes/ingress-gce/issues/7,Stop ignoring test files and non-go files in vendor,,closed,True,2017-10-09 17:59:43,2017-10-09 18:08:39
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/8,https://api.github.com/repos/kubernetes/ingress-gce/issues/8,Release 0.9.7,"Test Cases
 - [x] Basic ingress creation and deletion
 - [x] Existing ingress & add and removing 2 nodes
 - [x] Apply static IP annotation to ingress
 - [x] Ingress creation/deletion with existing ILB service.
    - @nikhiljindal Found an issue with interop between service controller and MC but it's not a release blocker.
 - [x] Existing ingress and add/remove paths
 - [x] Ingress with TLS secret creation & deletion
 - [x] Ingress with HTTPS backend
 - [x] Multi-zone cluster with multiple concurrent ingresses

Test Clusters
- [x] 1.8
- [x] 1.7 (basic ingress test should be sufficient)
- [ ] 1.6 (basic ingress test should be sufficient)

Areas to check
- Named ports of instance groups
- Instances in instance groups
- URL Map matches ingress rules
- Backend health
- Firewall rule update
- GLBC Log for errors


Temporary image for testing
*nicksardo/glbc:0.9.7*",closed,True,2017-10-09 18:25:13,2018-04-18 19:33:19
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/9,https://api.github.com/repos/kubernetes/ingress-gce/issues/9,Get value of string pointer for log message,,closed,True,2017-10-09 23:36:32,2017-10-09 23:43:16
ingress-gce,G-Harmon,https://github.com/kubernetes/ingress-gce/pull/10,https://api.github.com/repos/kubernetes/ingress-gce/issues/10,Fix the glbc build by removing 'godeps' from command.,We no longer should invoke 'godeps' after the recent shuffling of repos (ingress -> ingress-gce/ingress-nginx split) and Makefiles.,closed,True,2017-10-09 23:50:12,2017-10-09 23:54:26
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/11,https://api.github.com/repos/kubernetes/ingress-gce/issues/11,PRE-NEG changes,Some simple refactor that required by the following NEG changes. ,closed,True,2017-10-10 00:50:27,2017-10-11 17:52:50
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/12,https://api.github.com/repos/kubernetes/ingress-gce/issues/12,Update travis,,closed,True,2017-10-10 20:14:39,2017-12-23 21:23:45
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/13,https://api.github.com/repos/kubernetes/ingress-gce/issues/13,Invalid do not merge,,closed,True,2017-10-10 20:16:03,2018-06-10 18:06:13
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/14,https://api.github.com/repos/kubernetes/ingress-gce/issues/14,add test into makefile,,closed,True,2017-10-10 20:19:40,2017-10-11 17:30:06
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/15,https://api.github.com/repos/kubernetes/ingress-gce/issues/15,Update build,"Look at the commits for review. The Makefile and rules.mk are from kube-dns and support a clean build environment with multi architectures.

The standard k8s projects have been following is to put library code in pkg/",closed,True,2017-10-11 07:29:40,2017-12-23 21:23:35
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/16,https://api.github.com/repos/kubernetes/ingress-gce/issues/16,Add e2e testing,"_From @bprashanth on November 10, 2016 18:48_

https://github.com/kubernetes/contrib/issues/1441#issuecomment-256981778

> e2e testing: If we could figure out a way to setup an e2e builder that runs https://github.com/kubernetes/kubernetes/blob/master/test/e2e/ingress.go#L58 for every commit just like the cadvisor repo https://github.com/google/cadvisor, that would be great. I'm sure our test-infra people would be more than willing to help with this problem (file an issue like kubernetes/test-infra#939, but more descriptive maybe :)

@porridge fyi

_Copied from original issue: kubernetes/ingress-nginx#5_",closed,False,2017-10-11 17:32:47,2018-02-07 17:47:34
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/17,https://api.github.com/repos/kubernetes/ingress-gce/issues/17,Point gce ingress health checks at the node for onlylocal services,"_From @bprashanth on November 18, 2016 22:35_

We now have a new beta annotation on Services, `external-traffic` (http://kubernetes.io/docs/user-guide/load-balancer/#loss-of-client-source-ip-for-external-traffic). With this annotation set to `onlyLocal` `NodePort` Services only proxy to local endpoints. If there are no local endpoints, iptables is configured to drop packets. Currently sticking an `onlyLocal` Service behind an Ingress works, but does so in a suboptimal way.

The issue is, currently, the best way to configure lb health checks is to set high failure threshold so we detect nodes with bad networking, but not flake on bad endpoints. With this approach, if all endpoints evacuate a node, it'll take eg: 10 health checks*10 seconds per health check = 1.5 minutes to mark that node unhealthy, but the node will start DROPing packets for the NodePort immediately. If we pointed the lb health check at the `healthcheck-nodeport` (a nodePort that's managed by kube-proxy), it would fail in < 10s even with the high thresholds described above. 

@thockin 

_Copied from original issue: kubernetes/ingress-nginx#19_",closed,False,2017-10-11 17:33:50,2018-03-13 05:40:33
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/18,https://api.github.com/repos/kubernetes/ingress-gce/issues/18,Add example using grpc and http2 ,"_From @aledbf on December 1, 2016 22:39_



_Copied from original issue: kubernetes/ingress-nginx#39_",open,False,2017-10-11 17:34:01,2019-02-21 19:30:48
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/19,https://api.github.com/repos/kubernetes/ingress-gce/issues/19,Ingress creates wrong firewall rule after `default-http-backend` service was clobbered.,"_From @MrHohn on December 8, 2016 18:46_

From kubernetes/kubernetes#36546.

During development, found ingress firewall test failed instantly on my cluster. Turned out due to some reasons, the `default-http-backend` service was deleted and recreated, and it was then allocated a different nodePort. Because ingress controller records this nodePort at the beginning but never refresh([ref1](https://github.com/kubernetes/ingress/blob/master/controllers/gce/main.go#L208) and [ref2](https://github.com/kubernetes/ingress/blob/master/controllers/gce/controller/cluster_manager.go#L274)), it always create firewall rule with the stale nodePort after.

@bprashanth 

_Copied from original issue: kubernetes/ingress-nginx#48_",closed,False,2017-10-11 17:34:20,2018-05-04 21:52:12
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/20,https://api.github.com/repos/kubernetes/ingress-gce/issues/20,GLBC ingress: only handle annotated ingress,"_From @tamalsaha on December 14, 2016 21:49_

Hi,
We are running a HAProxy based ingress in our clusters. But for a few service, we would like to run GLBC ingress. I did not see any way to tell ingress controllers, which Ingress resource they can handle. Can Ingress controllers can only handle ingress that has a specific annotation applied to it (similar to how schedulers do it):

`""ingress.alpha.kubernetes.io/controller"": glbc`

Here is a way I could see being implemented. Glbc controller adds a new flag --ingress-controller.

If --ingress-controller flag value is empty, then glbc should handle any Ingress that has no annotation ""ingress.alpha.kubernetes.io/controller"" or annotation set to """" string.

If --ingress-controller flag is not empty, then only handle Ingress that has
""ingress.alpha.kubernetes.io/controller"" : """".

Thanks.

_Copied from original issue: kubernetes/ingress-nginx#59_",closed,False,2017-10-11 17:34:37,2018-03-12 21:32:35
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/21,https://api.github.com/repos/kubernetes/ingress-gce/issues/21,e2e test leaves garbage around,"_From @porridge on December 22, 2016 9:54_

`e2e-down.sh` explicitly removes some containers, and at least one exits after some time, but there are plenty others which stay around apparently indefinitely.

I'm not sure whether this is a known deficiency of hypercube or we're just using it improperly.

```
porridge@beczulka:~/Desktop/coding/go/src/k8s.io/ingress$ docker ps 
CONTAINER ID        IMAGE                                                    COMMAND                  CREATED             STATUS              PORTS               NAMES
ac39a0a5730e        gcr.io/google_containers/kube-dnsmasq-amd64:1.4          ""/usr/sbin/dnsmasq --""   9 minutes ago       Up 9 minutes                            k8s_dnsmasq.bee611d9_kube-dns-v20-6caok_kube-system_8ff29f9e-c82a-11e6-9097-24770389c384_107a8b1a
4b26865f46da        gcr.io/google_containers/exechealthz-amd64:1.2           ""/exechealthz '--cmd=""   13 minutes ago      Up 13 minutes                           k8s_healthz.3613f95_kube-dns-v20-6caok_kube-system_8ff29f9e-c82a-11e6-9097-24770389c384_5ee00e74
224be6a6f7bc        gcr.io/google_containers/pause-amd64:3.0                 ""/pause""                 13 minutes ago      Up 13 minutes                           k8s_POD.a6b39ba7_kube-dns-v20-6caok_kube-system_8ff29f9e-c82a-11e6-9097-24770389c384_3e36dd7a
e85eab303994        gcr.io/google_containers/pause-amd64:3.0                 ""/pause""                 13 minutes ago      Up 13 minutes                           k8s_POD.2225036b_kubernetes-dashboard-v1.4.0-96im4_kube-system_8ff1e7e5-c82a-11e6-9097-24770389c384_ff54a5b2
dd4bc152a110        gcr.io/google_containers/hyperkube-amd64:v1.4.5          ""/hyperkube apiserver""   6 days ago          Up 6 days                               k8s_apiserver.213c742_k8s-master-0.0.0.0_kube-system_501bec47043160feec61f2839ec6a4c5_29321e20
cfbb7e04222e        gcr.io/google_containers/hyperkube-amd64:v1.4.5          ""/setup-files.sh IP:1""   6 days ago          Up 6 days                               k8s_setup.2cde3c3c_k8s-master-0.0.0.0_kube-system_501bec47043160feec61f2839ec6a4c5_baf0cb65
219ad2ef7f1c        gcr.io/google_containers/hyperkube-amd64:v1.4.5          ""/copy-addons.sh mult""   6 days ago          Up 6 days                               k8s_kube-addon-manager-data.270e200b_kube-addon-manager-0.0.0.0_kube-system_c3c035106a9df5bd5f54b3e87143ddbf_636a1308
a2fac3257ece        gcr.io/google_containers/kube-addon-manager-amd64:v5.1   ""/opt/kube-addons.sh""    6 days ago          Up 6 days                               k8s_kube-addon-manager.ed858faf_kube-addon-manager-0.0.0.0_kube-system_c3c035106a9df5bd5f54b3e87143ddbf_2ccd5348
189bb0cf3d59        gcr.io/google_containers/pause-amd64:3.0                 ""/pause""                 6 days ago          Up 6 days                               k8s_POD.d8dbe16c_k8s-master-0.0.0.0_kube-system_501bec47043160feec61f2839ec6a4c5_45247fc2
d971a4ae6c4a        gcr.io/google_containers/pause-amd64:3.0                 ""/pause""                 6 days ago          Up 6 days                               k8s_POD.d8dbe16c_kube-addon-manager-0.0.0.0_kube-system_c3c035106a9df5bd5f54b3e87143ddbf_286fe6e0
porridge@beczulka:~/Desktop/coding/go/src/k8s.io/ingress$ 
```

_Copied from original issue: kubernetes/ingress-nginx#80_",closed,False,2017-10-11 17:35:00,2018-03-12 21:32:37
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/22,https://api.github.com/repos/kubernetes/ingress-gce/issues/22,GCE cloud: add a kube-specific header to GCE API calls,"_From @thockin on January 3, 2017 22:16_

https://github.com/kubernetes/kubernetes/issues/39391

_Copied from original issue: kubernetes/ingress-nginx#104_",closed,False,2017-10-11 17:35:13,2018-03-12 21:32:37
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/23,https://api.github.com/repos/kubernetes/ingress-gce/issues/23,GCE: improve default backend handling,"_From @bprashanth on January 10, 2017 10:43_

Today the lb pool manages the default backend. This makes for some confusing situations. 
1. Don't get the default backend in main, instead pipe the --default-backend arg through the cluster-manager down into 3 pools: firewall, backend, lb
2. Create the default backend in Sync() of backend pool, when there are nodePorts, so users that don't care about ingress don't get a default backend
3. Delete the default backend in GC() of the backend pool, when there are 0 nodePorts
4. When the lb pool wants to know the default backend, it re-gets the service passed through --default-backend, and reconstructs its name through the namer. If the backend pool hasn't created it, throw an error event.
5. Do the same thing from the firewall pool


_Copied from original issue: kubernetes/ingress-nginx#120_",closed,False,2017-10-11 17:35:33,2018-05-04 21:50:02
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/24,https://api.github.com/repos/kubernetes/ingress-gce/issues/24,Specify IP addresses the Ingress controller is listening on,"_From @cluk33 on January 17, 2017 13:47_

We are running k8s on bare metal. It would be great to specify the IP addresses the nginx ingress controller is listening on.

This would enable us
1) to route traffic for specific IPV4 IPs to k8s
2) also route (http) traffic for IPV6 addresses to k8s.

AFAIK 1) can be achieved by using a service in front of the ingress ctrl with external (IPV4) addresses. But currently we do not see any possibility to achieve 2).

Might be related to #131 .

Thanks a lot!

_Copied from original issue: kubernetes/ingress-nginx#137_",closed,False,2017-10-11 17:35:42,2018-02-07 17:48:52
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/25,https://api.github.com/repos/kubernetes/ingress-gce/issues/25,High glbc CPU usage,"_From @jmn on January 27, 2017 1:53_

I noticed in Stackdriver monitoring that glbc is using about 30% CPU constantly in my cluster. @thockin said on Slack that this might be related to misconfigured GCE ingress. However I looked through all my Ingresses and they are all class nginx.  Anyone got a clue or a suggestion as to troubleshoot further?

One thing which I noticed is in my Ingress ""kube-lego"", which is configured automatically by kube-lego, there is a part ""status"" which looks like this:

```
status:
  loadBalancer:
    ingress:
    - ip: 130.211....
    - ip: 146.148....

```
The second IP adress adress is my nginx loadbalancer, however: the first IP adress is unknown to me, I currently do not know where that is from.


_Copied from original issue: kubernetes/ingress-nginx#183_",closed,False,2017-10-11 17:35:56,2018-02-07 17:53:46
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/26,https://api.github.com/repos/kubernetes/ingress-gce/issues/26,GCE: respect static-ip assignment via update ,"_From @bprashanth on January 31, 2017 23:4_

Currently if you create an ingress then take its static-ip and assign the annotation, the controller is not smart enough to *not* delete the IP. To make this work: 
1. Gate Static IP cleanup on a call to `getEffectiveIP()`: https://github.com/kubernetes/ingress/blob/master/controllers/gce/loadbalancers/loadbalancers.go#L828 (just like we do here https://github.com/kubernetes/ingress/blob/master/controllers/gce/loadbalancers/loadbalancers.go#L548)
2. Pipe the runtimeInfo into GC, instead of just the name (https://github.com/kubernetes/ingress/blob/master/controllers/gce/controller/controller.go#L324, https://github.com/kubernetes/ingress/blob/master/controllers/gce/controller/controller.go#L299)
3. Store the updated runtimeInfo in the L7 struct (), before calling delete()

This way the call to getEffectiveIP will observe the annotation value right before deleting the IP

_Copied from original issue: kubernetes/ingress-nginx#196_",closed,False,2017-10-11 17:36:24,2018-03-12 22:33:33
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/27,https://api.github.com/repos/kubernetes/ingress-gce/issues/27,[GLBC] Expose GCE backend parameters in Ingress object API,"_From @itamaro on February 7, 2017 10:9_

When using GCE Ingress controller, the GCE Ingress controller (GLBC) provisions GCE backends with a bunch of default parameters.

It would be great if it was possible to tweak the parameters that are currently ""untweakable"" from the Ingress object API (AKA from my YAML's).

**Specific use case:** GCE backends are provisioned with a default timeout of 30 seconds, which is not sufficient for some long requests. I'd like to be able to control the timeout per-backend.

_Copied from original issue: kubernetes/ingress-nginx#243_",closed,False,2017-10-11 17:36:37,2017-10-23 19:34:19
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/28,https://api.github.com/repos/kubernetes/ingress-gce/issues/28,[GLBC] Expose GCE backend parameters in Ingress object API,"_From @itamaro on February 7, 2017 10:9_

When using GCE Ingress controller, the GCE Ingress controller (GLBC) provisions GCE backends with a bunch of default parameters.

It would be great if it was possible to tweak the parameters that are currently ""untweakable"" from the Ingress object API (AKA from my YAML's).

**Specific use case:** GCE backends are provisioned with a default timeout of 30 seconds, which is not sufficient for some long requests. I'd like to be able to control the timeout per-backend.

_Copied from original issue: kubernetes/ingress-nginx#243_",closed,False,2017-10-11 17:38:04,2018-11-30 05:26:54
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/29,https://api.github.com/repos/kubernetes/ingress-gce/issues/29,"Disabled HttpLoadBalancing, unable to create Ingress with glbc:0.9.1","_From @tonglil on February 14, 2017 1:42_

# Update

I was able to create the Ingress after this comment: https://github.com/kubernetes/ingress/issues/267#issuecomment-281452897

Does this mean in order to run your own GCE Ingress, you have to always set this file? No information is provided about this in the docs.

# Original Issue

1. I disabled the cluster's addon via:
```
gcloud container clusters update tony-test --update-addons HttpLoadBalancing=DISABLED
```

2. Then `kubectl apply -f rc.yaml` this: https://github.com/kubernetes/ingress/blob/master/controllers/gce/rc.yaml

3. Then I apply the following config:
```yaml
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: echo-app
  name: echo-app
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: echo-app
    spec:
      containers:
      - image: gcr.io/google_containers/echoserver:1.4
        name: echo-app
        ports:
        - containerPort: 8080
          protocol: TCP
        readinessProbe:
          failureThreshold: 10
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 1
        resources: {}
        terminationMessagePath: /dev/termination-log
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: echo-app-tls
  annotations:
    kubernetes.io/ingress.allow-http: ""false""
spec:
  backend:
    serviceName: echo-app
    servicePort: 88
---
apiVersion: v1
kind: Service
metadata:
  name: echo-app
spec:
  type: NodePort
  selector:
    app: echo-app
  ports:
    - name: http
      port: 88
      protocol: TCP
      targetPort: 8080
```

4. What I get when `kubectl describe ingress echo-app-tls`:
```
Name:                   echo-app-tls
Namespace:              default
Address:
Default backend:        echo-app:88 (10.254.33.15:8080)
Rules:
  Host  Path    Backends
  ----  ----    --------
  *     *       echo-app:88 (10.254.33.15:8080)
Annotations:
  backends:     {""k8s-be-30659--d785be79bbf6d463"":""UNHEALTHY""}
  url-map:      k8s-um-default-echo-app-tls--d785be79bbf6d463
Events:
  FirstSeen     LastSeen        Count   From                            SubObjectPath   Type            Reason  Message
  ---------     --------        -----   ----                            -------------   --------        ------  -------
  2m            2m              1       {loadbalancer-controller }                      Normal          ADD     default/echo-app-tls
  1m            <invalid>       16      {loadbalancer-controller }                      Warning         GCE     instance not found
  1m            <invalid>       16      {loadbalancer-controller }                      Normal          Service default backend set to echo-app:30659
```

I can let it wait for >1 hour and it is the same.

_Copied from original issue: kubernetes/ingress-nginx#267_",closed,False,2017-10-11 17:43:44,2017-10-17 21:19:57
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/30,https://api.github.com/repos/kubernetes/ingress-gce/issues/30,Some links in 'ingress' repo GCE release notes are 404,"_From @whereisaaron on February 20, 2017 21:59_

With each GCE release in this 'ingress' repo, including 'GCE: 0.9.1', the release notes have links to the old 'contrib' repo that are (now) 404:

https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md
https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/README.md#troubleshooting
https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/README.md#wishlist

![screenshot 2017-02-20 at 4 51 47 pm](https://cloud.githubusercontent.com/assets/1109585/23143014/08fbd220-f78d-11e6-8c99-74d14b683eda.png)


_Copied from original issue: kubernetes/ingress-nginx#310_",closed,False,2017-10-11 17:44:13,2018-04-08 19:06:54
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/31,https://api.github.com/repos/kubernetes/ingress-gce/issues/31,[GLBC] GCE resources of non-snapshotted ingresses are not deleted,"_From @nicksardo on March 13, 2017 20:34_

For the GLBC to know about load balancer GCE resources, it must first have snapshotted an ingress object that created those resources. This happens when an ingress object is deleted while the GLBC is offline or starting up. 

The following are test logs from gce-gci-latest-upgrade-etcd:

Successful Test #334 : [glbc.log](https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-gci-latest-upgrade-etcd/334/artifacts/bootstrap-e2e-master/glbc.log)
```shell
I0313 19:04:50.100770       5 utils.go:166] Syncing e2e-tests-ingress-upgrade-pjrvv/static-ip
... # GLBC has listed existing ingress objects and knows about the `static-ip` ingress
E0313 19:04:55.101688       5 utils.go:168] Requeuing e2e-tests-ingress-upgrade-pjrvv/static-ip, err Waiting for stores to sync
... # Ingress is being requeued because other stores have not finished syncing
I0313 19:04:55.107613       5 utils.go:166] Syncing e2e-tests-ingress-upgrade-pjrvv/static-ip
I0313 19:04:55.107669       5 controller.go:295] Syncing e2e-tests-ingress-upgrade-pjrvv/static-ip
... # Ingresses are listed from apiserver
I0313 19:04:55.594861       5 loadbalancers.go:165] Creating loadbalancers [0xc42067a2d0]
# Following the above line, the ingress obj will be added to the L7.snapshotter (cache)
I0313 19:04:55.755621       5 controller.go:128] Delete notification received for Ingress e2e-tests-ingress-upgrade-pjrvv/static-ip
# Ingress watcher notices the ingress object was deleted
```

Failed Test #332 : [glbc.log](https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-gci-latest-upgrade-etcd/332/artifacts/bootstrap-e2e-master/glbc.log)
```shell
I0313 17:11:53.401594       5 utils.go:166] Syncing e2e-tests-ingress-upgrade-9shgs/static-ip
... 
I0313 17:11:54.570150       5 controller.go:128] Delete notification received for Ingress e2e-tests-ingress-upgrade-9shgs/static-ip
... # Watcher notices the ingress object has been deleted; however, we have not yet snapshotted the ingress object
E0313 17:11:58.401711       5 utils.go:168] Requeuing e2e-tests-ingress-upgrade-9shgs/static-ip, err Waiting for stores to sync
...
I0313 17:11:58.401773       5 controller.go:295] Syncing e2e-tests-ingress-upgrade-9shgs/static-ip
... # Ingresses are listed from apiserver but because the ingress was deleted, the list is empty
I0313 17:11:58.401852       5 loadbalancers.go:165] Creating loadbalancers []
# Ingress object is not created 
```

The L7Pool `GC` func deletes resources of ingresses stored in the l7.snapshotter that are not mentioned by name in the arg slice. Because the test ingress was never stored in the snapshot cache, the GCE resources are never deleted.

The failed test log also contains multiple blocks of the following:
```
I0313 17:12:27.161699       5 backends.go:336] GCing backend for port 32397
I0313 17:12:27.161767       5 backends.go:233] Deleting backend k8s-be-32397--179702b6ab620fd3
...
E0313 17:12:27.390153       5 utils.go:168] Requeuing e2e-tests-ingress-upgrade-9shgs/static-ip, err Error during sync <nil>, error during GC googleapi: Error 400: The backend_service resource 'k8s-be-32397--179702b6ab620fd3' is already being used by 'k8s-um-e2e-tests-ingress-upgrade-9shgs-static-ip--179702b6ab620', resourceInUseByAnotherResource
```
The GLBC knows about the extraneous backends because the BackendPool uses the `CloudListingPool`. This implementation calls a `List` [func](https://github.com/kubernetes/ingress/blob/master/controllers/gce/backends/backends.go#L255) to reflect the current state of GCE.



_Copied from original issue: kubernetes/ingress-nginx#431_",closed,False,2017-10-11 17:44:24,2019-03-11 23:41:54
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/32,https://api.github.com/repos/kubernetes/ingress-gce/issues/32,[GLBC] Changing front-end configuration does not remove unnecessary target proxies/ssl-certs,"_From @tonglil on March 20, 2017 17:59_

Porting this issue from contrib over: https://github.com/kubernetes/contrib/issues/1517.

There is no enforcement of the annotation `kubernetes.io/ingress.allow-http: ""false""` when it is set to false, after previously being set to true or unset.

See during the edge hop:
- Checks annotation: https://github.com/kubernetes/ingress/blob/37bdb3952e090e50f44207ad0e54b1f2a4ef1055/controllers/gce/loadbalancers/loadbalancers.go#L598-L602
- Creates: https://github.com/kubernetes/ingress/blob/37bdb3952e090e50f44207ad0e54b1f2a4ef1055/controllers/gce/loadbalancers/loadbalancers.go#L619-L627

No deletion/cleanup enforcement happens if it is set to false.

_Copied from original issue: kubernetes/ingress-nginx#468_",open,False,2017-10-11 17:44:36,2019-03-06 11:58:10
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/33,https://api.github.com/repos/kubernetes/ingress-gce/issues/33,Use GCE load balancer controller with backend buckets,"_From @omerzach on February 28, 2017 1:20_

We're happily using the GCE load balancer controller in production to route traffic to a few different services. We'd like to have some paths point at backend buckets in Google Cloud Storage instead of backend services running in Kubernetes.

Right now if we manually create this backend bucket and then configure the load balancer to point certain paths at it the UrlMap is updates appropriately but almost immediately reverted to its previous setting, presumably because the controller sees it doesn't match the YAML we initially configured the Ingress with.

I have two questions:
1. Is there any immediate workaround where we could continue using the Kubernetes controller but manually modify the UrlMap to work with a backend bucket?
2. Would a pull request to add support for backend buckets in the load balancer controller be something the community is interested in? We're willing to invest engineering effort into this, though none of our team has experience with Go or the Kubernetes codebase so we might need a bit of guidance.

(For some context, we'd like to do something like this: https://cloud.google.com/compute/docs/load-balancing/http/using-http-lb-with-cloud-storage)

_Copied from original issue: kubernetes/ingress-nginx#353_",open,False,2017-10-11 17:45:17,2019-04-01 07:56:07
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/34,https://api.github.com/repos/kubernetes/ingress-gce/issues/34,Document how to avoid 502s,"_From @esseti on September 20, 2017 9:22_

Hello, 
i've a problem with the ingress and the fact that the 502 page pops up when there are ""several"" request. I've a JMeter spinning 10 threads for 20 times, and I get more than 50 times the 502 over 2000 calls in total (less than 0,5%).

reading the [readme](https://github.com/kubernetes/ingress/tree/master/controllers/gce#troubleshooting) it says 
it says that this error is probably due to

> The loadbalancer is probably bootstrapping itself.

but the loadbalancer is already there, so does it means that all the pods serving that url are busy? is there a way to avoid the 502 waiting for a pod to be free?

if not, is there a way to personalize the 502 page? because I expose APIs in JSON format, and I would like to show a JSON error rather than a html page.

_Copied from original issue: kubernetes/ingress-nginx#1396_",open,False,2017-10-11 17:49:16,2019-03-13 22:07:30
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/35,https://api.github.com/repos/kubernetes/ingress-gce/issues/35,GCE: ingress only shows the first backend's healthiness in `backends` annotation,"_From @MrHohn on September 20, 2017 1:43_

From https://github.com/kubernetes/features/issues/27#issuecomment-330589051.

We attach `backends` annotation to ingress object after LB creation:
```
  ...
  backends:		{""k8s-be-30910--7b4223ab4c1af15d"":""UNHEALTHY""}
```

And from the implementation:
https://github.com/kubernetes/ingress/blob/937cde666e533e4f70087207910d6135c672340a/controllers/gce/backends/backends.go#L437-L452

Using only the first backend's healthiness to represent the healthiness for all backends seems incorrect.

cc @freehan 

_Copied from original issue: kubernetes/ingress-nginx#1395_",open,False,2017-10-11 17:49:30,2019-02-20 06:54:05
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/36,https://api.github.com/repos/kubernetes/ingress-gce/issues/36,"GCE: WebSocket: connection is broken CloseEvent {isTrusted: true, wasClean: false, code: 1006, reason: """", type: ""close"", …}","_From @unfii on August 29, 2017 13:28_

Last ingress GCE.

in ingress logs.

```
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath   Type            Reason  Message
  ---------     --------        -----   ----                    -------------   --------        ------  -------
  58m           58m             1       loadbalancer-controller                 Normal          ADD     default/idecisiongames
  58m           58m             1       loadbalancer-controller                 Normal          ADD     default/idecisiongames
  57m           57m             1       loadbalancer-controller                 Warning         GCE     googleapi: Error 409: The resource 'projects/dm-apps-beta/global/sslCertificates/k8s-ssl-default-idecisiongames
--be0f1d8aa9717d3c' already exists, alreadyExists
  57m           57m             1       loadbalancer-controller                 Normal          CREATE  ip: 130.211.32.230
  57m           57m             1       loadbalancer-controller                 Normal          CREATE  ip: 130.211.32.230
  57m           4m              17      loadbalancer-controller                 Warning         GCE     No node tags supplied and also failed to parse the given lists of hosts for tags. Abort creating firewall rule.
```

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
  labels:
    app: idg-server
  annotations:
    kubernetes.io/ingress.class: ""gce""
    kubernetes.io/ingress.global-static-ip-name: ""test""
spec:
  tls:
  - secretName: test
  backend:
    serviceName: idg-server
    servicePort: 3000
```

And also default deployment from 
https://github.com/kubernetes/ingress/tree/master/examples/deployment/gce

In console we see.

`WebSocket: connection is broken CloseEvent {isTrusted: true, wasClean: false, code: 1006, reason: """", type: ""close"", …}`

How to fix it?

_Copied from original issue: kubernetes/ingress-nginx#1262_",closed,False,2017-10-11 17:49:47,2018-03-13 05:40:32
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/37,https://api.github.com/repos/kubernetes/ingress-gce/issues/37,Why does GCE ingress defer promoting static IP,"_From @tonglil on March 8, 2017 17:40_

Something I have thought about for a while but haven't been able to reason about: https://github.com/kubernetes/ingress/blob/master/controllers/gce/loadbalancers/loadbalancers.go#L603-L608.

This seems like both :80 and :443 needs to be set before an IP can be promoted to static. Is there a reason why? I've git blamed and the past doesn't show a good reason for it either: https://github.com/kubernetes/contrib/pull/504/commits/a2f82d5f48fe470db0a259888b29582699c3fb9f#diff-0ca32fb01d30b7ffd4f37d77adcce2e5R483.

I think the ip should either: always be promoted to static, or by a separate annotation to do so when the ingress creates an IP.

It's a bad UX currently when the user has to run another `gcloud` command/go into the console to promote the address when they only want HTTPS (via `kubernetes.io/ingress.allow-http: ""false""`).

Thoughts?

_Copied from original issue: kubernetes/ingress-nginx#405_",closed,False,2017-10-11 17:50:11,2018-04-23 16:11:12
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/38,https://api.github.com/repos/kubernetes/ingress-gce/issues/38,Does/will the GCE ingress controller support whitelist-source-range?,"_From @nickform on April 6, 2017 12:11_

Can anyone comment on when we will be able to use the 'whitelist-source-range' annotation with the GCE Ingress Controller? From Googling plus experimentation the answer is not ""it works right now"" but perhaps we've made a mistake? If not, is this something which might happen in future? Thanks!

_Copied from original issue: kubernetes/ingress-nginx#566_",closed,False,2017-10-11 17:50:23,2019-02-21 06:45:24
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/39,https://api.github.com/repos/kubernetes/ingress-gce/issues/39,GCE health check does not pick up changes to pod readinessProbe,"_From @ConradIrwin on April 11, 2017 4:22_

Importing this issue from https://github.com/kubernetes/kubernetes/issues/43773 as requested.

**Kubernetes version** (use `kubectl version`): 1.4.9


**Environment**:
- **Cloud provider or hardware configuration**: GKE
- **OS** (e.g. from /etc/os-release): 
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:


**What happened**:

I created an ingress with type GCE

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: admin-proxy
  labels:
    name: admin-proxy
  annotations:
    kubernetes.io/ingress.allow-http: ""false""
spec:
  tls:
    - secretName: ingress-admin-proxy
  backend:
    serviceName: admin-proxy
    servicePort: 80
```

This set up the health-check on the google cloud backend endpoint to call ""/"" as [documented](https://github.com/kubernetes/ingress/blob/master/controllers/gce/README.md#health-checks).

Unfortunately my service doesn't return 200 on `""/""` and so I added a readinessCheck to the pod as suggested by the documentation.

**What you expected to happen**:

I expected the health check to be automatically updated.

Instead I had to delete the ingress and re-create it in order for the health-check to update.

**How to reproduce it** (as minimally and precisely as possible):

1. Create a deployment with no readiness probe.
2. Create an ingress pointing to the pods created by that deployment.
3. Add a readiness probe to the deployment.

**Anything else we need to know**:



_Copied from original issue: kubernetes/ingress-nginx#582_",closed,False,2017-10-11 17:50:34,2018-10-31 22:00:55
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/40,https://api.github.com/repos/kubernetes/ingress-gce/issues/40,[GLBC] Garbage collection runs too frequently - after every sync,"_From @nicksardo on April 28, 2017 22:5_

GC runs at the end of every sync for ever ingress. If there are many ingress objects, this results in many GCP calls. It's even more troublesome in the case of federated clusters.

We should perform a loadbalancer-specific GC on delete notification, but only run cluster-wide GC on a less frequent basis, such as resyncPeriod (set to 10min).
cc @madhusudancs @csbell 

_Copied from original issue: kubernetes/ingress-nginx#674_",open,False,2017-10-11 17:50:53,2019-02-20 06:54:13
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/41,https://api.github.com/repos/kubernetes/ingress-gce/issues/41,[GLBC] Surface event when front-end not created,"_From @pijusn on May 4, 2017 11:2_

I recently ran following Shell script with given Kubernetes definitions and after some time, GLBC was created but it did not get any front-end created.

```
set -ex

read -p ""Press any key to start..."" -sn1

gcloud config set project echo-test-166405
gcloud container clusters create echo-cluster --machine-type=""g1-small"" --num-nodes=1

echo ""Cluster created""

openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls.key -out /tmp/tls.crt -subj ""/C=LT""
kubectl create secret tls echoserver-tls --key /tmp/tls.key --cert /tmp/tls.crt

# Note: At this point I just waited a minute to make sure it's all OK.
echo ""Self-generated certificate created""
read -p ""Press any key to continue... "" -sn1

kubectl apply -f echoserver/00-namespace.yaml
kubectl apply -f echoserver/deployment.yaml
kubectl apply -f echoserver/service.yaml
kubectl apply -f echoserver/ingress-tls.yaml
```

Here are all YAML files merged into one:
```
apiVersion: v1
kind: Namespace
metadata:
  name: echoserver
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: echoserver
  namespace: echoserver
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: echoserver
    spec:
      containers:
      - image: gcr.io/google_containers/echoserver:1.0
        imagePullPolicy: Always
        name: echoserver
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: echoserver
  namespace: echoserver
spec:
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
  type: NodePort
  selector:
    app: echoserver
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: echoserver
  namespace: echoserver
  annotations:
    kubernetes.io/tls-acme: ""true""
    kubernetes.io/ingress.class: ""gce""
    kubernetes.io/ingress.allow-http: ""false""
spec:
  tls:
  - hosts:
    - echo.pijusn.eu
    secretName: echoserver-tls
  rules:
  - host: echo.pijusn.eu
    http:
      paths:
      - backend:
          serviceName: echoserver
          servicePort: 80
```

I expected HTTPS front-end to be created. It was a fresh project, fresh cluster so no quotes were kicking in.

After I removed `kubernetes.io/ingress.allow-http: ""false""` it did create an HTTP front-end but still did not create HTTPS one.

This seems like an issue. Also, if you have ideas where to look for an error message or something (why it failed to create it) - please share.

_Copied from original issue: kubernetes/ingress-nginx#686_",closed,False,2017-10-11 17:51:03,2018-02-12 16:44:02
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/42,https://api.github.com/repos/kubernetes/ingress-gce/issues/42,Ingress Healthcheck Configuration,"_From @freehan on May 15, 2017 21:25_

On GCE, ingress controller sets up default healthcheck for backends. The healthcheck will point to the nodeport of backend services on every node. Currently, there is no way to describe detail configuration of healthcheck in ingress. On the other side, each application may want to handle healthcheck differently. To bypass this limitation, on Ingress creation, ingress controller will scan all backend pods and pick the first ReadinessProbe it encounters and configure healthcheck accordingly. However, healthcheck will not be updated if ReadinessProbe was updated. (Refer: https://github.com/kubernetes/ingress/issues/582)

I see 3 options going forward with healthcheck

1) Expand the Ingress or Service spec to include more configuration for healthcheck. It should include the capabilities provided by major cloud providers, GCP, AWS...

2) Keep using readiness probe for healthcheck configuration, 
            a) Keep today's behavior and communicate clearly regarding the expectation. However, this still breaks the abstraction and declarative nature of k8s. 
            b) Let ingress controller watch the backend pods for any updates for ReadinessProbe. This seems expensive and complicated. 
 
3) Only setup default healthcheck for ingresses. Ingress controller will only ensure the healthcheck exist periodically, but do not care about its detail configuration. User can configure it directly thru the cloud provider.  


I am in favor of option 3). There are always more bells and whistles on different cloud providers. The higher layer we go, the more features we can utilize. For L7 LB, there is no clean simple way to describe every intention. So is the case for health check. To ensure a smooth experience, k8s still sets up the basics. For advance use cases, user will have to configure it thru the cloud provider. 

Thoughts? @kubernetes/sig-network-misc 

_Copied from original issue: kubernetes/ingress-nginx#720_",open,False,2017-10-11 17:52:34,2019-03-27 02:58:00
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/43,https://api.github.com/repos/kubernetes/ingress-gce/issues/43,[GLBC] LB garbage collection orphans named ports in instance groups,"_From @nicksardo on May 8, 2017 18:54_

The GLBC does not remove the named port from an instance group when a backend is being deleted.

If users are frequently created/deleting services and ingresses, instance groups will become polluted with old node ports. Eventually, users will hit a max limit. 
```
Exceeded limit 'MAX_DISTINCT_NAMED_PORTS' on resource 'k8s-ig--aaaaaaaaaaaa'
```

Temporary workaround
```shell
region=us-central1
cluster_id=$(kubectl get configmaps ingress-uid -o jsonpath='{.data.uid}' --namespace=kube-system)
ports=$(gcloud compute backend-services list --global --format='value(port,port)' |  xargs printf 'port%s:%s,')
for zone in b c f; do
  gcloud compute instance-groups unmanaged set-named-ports k8s-ig--$cluster_id --zone=$region-$zone --named-ports=$ports
done
```
Modify the region and list of zone suffix in the script.

_Copied from original issue: kubernetes/ingress-nginx#695_",closed,False,2017-10-11 17:52:49,2018-08-21 18:28:54
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/44,https://api.github.com/repos/kubernetes/ingress-gce/issues/44,GLBC: Each ingress sync updates resources for all ingresses,"_From @nicksardo on July 11, 2017 23:58_

For every ingress, the controller runs resource synchronization for all ingresses, thus resulting in unnecessary work.

Furthermore, if a GCP error occurs when handling one of the ingress resources, an event will be raised for every ingress resource.

_Copied from original issue: kubernetes/ingress-nginx#950_",closed,False,2017-10-11 17:53:17,2018-02-12 22:08:29
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/45,https://api.github.com/repos/kubernetes/ingress-gce/issues/45,controllers/gce/README.md doc review,"_From @ensonic on July 12, 2017 9:54_

https://github.com/kubernetes/ingress/blob/master/controllers/gce/README.md#the-ingress

```
Though glbc doesn't support HTTPS yet, security configs would also be global.
```
You probably want to say that it does not support https when communicating with the backends. There is a chapter on TLS termination below.

---
https://github.com/kubernetes/ingress/blob/master/controllers/gce/README.md#creation
```
kubectl create -f rc.yaml
replicationcontroller ""glbc"" created
```
This seems to be outdated:
```
kubectl create -f https://raw.githubusercontent.com/kubernetes/ingress/master/controllers/gce/rc.yaml
service ""default-http-backend"" created
replicationcontroller ""l7-lb-controller"" created
```
This also means that the log commadns are outdated and should be updated to e.g. `kubectl logs --follow l7-lb-controller-fw4ps l7-lb-controller`

```
Go to your GCE console and confirm that the following resources have been created through the HTTPLoadbalancing panel
```
There is no `HTTPLoadbalancing panel`, but there is this page:
https://pantheon.corp.google.com/networking/loadbalancing/loadBalancers/list

---
https://github.com/kubernetes/ingress/blob/master/controllers/gce/README.md#updates
```
Say you don't want a default backend ...
```
If you omit the default backend you seem to get some implicit default backend, which is always unhealthy - since it returns 404. Having a default that has a readyness check would be nice so that GLBC would actually use it.

---
https://github.com/kubernetes/ingress/blob/master/controllers/gce/README.md#paths

some yaml is shown as plaintext.

---
There is probably more, lets fix it iteratively.

_Copied from original issue: kubernetes/ingress-nginx#951_",closed,False,2017-10-11 17:53:28,2018-05-04 22:56:29
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/46,https://api.github.com/repos/kubernetes/ingress-gce/issues/46,multiple TLS certs are not correctly handled by GCE (no SNI support),"_From @ensonic on July 12, 2017 10:9_

I have setup and ingress for 3 microservices under 3 subdomains, each having their own cert.
When I startup the ingree I see this in the l7-lb-controller log:
W0712 10:01:30.733403       1 tls.go:58] Ignoring 2 certs and taking the first for ingress default/tls-termination

IMHO that cannot work and indeed I get a single cert applied to all 3 subdomains and as expected e.g browsers complain about the mismatch. I would expect the Host header to be used to select the appropriate cert.

This is how the config looks like for 2 hosts, example.com is just used for illustration
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: tls-termination
  annotations:
    kubernetes.io/ingress.class: ""gce""
    kubernetes.io/ingress.allow-http: ""false""
spec:
  tls:
  - hosts:
    - api.example.com
    secretName: api-tls
  - hosts:
    - www.example.com
    secretName: www-tls
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: api
          servicePort: 80
  - host: www.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: www
          servicePort: 80

```

Using a single cert covering all subdomains is maybe doable, but would not be nice, since the services should not need to know about each other.

_Copied from original issue: kubernetes/ingress-nginx#952_",closed,False,2017-10-11 17:53:47,2018-05-04 22:51:58
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/47,https://api.github.com/repos/kubernetes/ingress-gce/issues/47,GLBC: Ingress can't be properly created: Insufficient Permission,"_From @bbzg on July 16, 2017 11:36_

I recently upgraded to kubernetes 1.7 with RBAC on GKE, and I am seeing this problem:

```
  FirstSeen	LastSeen	Count	From			SubObjectPath	Type		Reason		Message
  ---------	--------	-----	----			-------------	--------	------		-------
  6h		6m		75	loadbalancer-controller			Warning		GCE :Quota	googleapi: Error 403: Insufficient Permission, insufficientPermissions
```

I have double-checked my quotas, and they are all green.

I have also tried granting the Node service account `Project > Editor` permissions, and I have added the Node service account to the `cluster-admin` ClusterRole, just in case it had anything to do with that (which it should not, right?).

GKE Cluster logs (slightly redacted):

```
{
 insertId:  ""x""   
 jsonPayload: {
  apiVersion:  ""v1""    
  involvedObject: {
   apiVersion:  ""extensions""     
   kind:  ""Ingress""     
   name:  ""ingress-testing""     
   namespace:  ""default""     
   resourceVersion:  ""425826""     
   uid:  ""x""     
  }
  kind:  ""Event""    
  message:  ""googleapi: Error 403: Insufficient Permission, insufficientPermissions""    
  metadata: {
   creationTimestamp:  ""2017-07-15T12:54:37Z""     
   name:  ""ingress-testing.x""     
   namespace:  ""default""     
   resourceVersion:  ""53520""     
   selfLink:  ""/api/v1/namespaces/default/events/ingress-testing.14d1822c5ed30595""     
   uid:  ""x""     
  }
  reason:  ""GCE :Quota""    
  source: {
   component:  ""loadbalancer-controller""     
  }
  type:  ""Warning""    
 }
 logName:  ""projects/x/logs/events""   
 receiveTimestamp:  ""2017-07-15T19:11:59.117152623Z""   
 resource: {
  labels: {
   cluster_name:  ""app-cluster""     
   location:  """"     
   project_id:  ""x""     
  }
  type:  ""gke_cluster""    
 }
 severity:  ""WARNING""   
 timestamp:  ""2017-07-15T19:11:54Z""   
}
```

I have tried figuring out what the cause might be, but have not found anything that was applicable.

What can I do to get Ingress working again in my cluster?

Thanks!

_Copied from original issue: kubernetes/ingress-nginx#975_",closed,False,2017-10-11 17:54:02,2017-10-11 21:53:28
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/48,https://api.github.com/repos/kubernetes/ingress-gce/issues/48,K8s-NEG Integration,"End-to-end K8s-NEG integration
",closed,True,2017-10-11 18:58:29,2017-10-27 02:22:09
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/49,https://api.github.com/repos/kubernetes/ingress-gce/issues/49,GLBC: Ingress can't be properly created: Insufficient Permission,"_From @bbzg on July 16, 2017 11:36_

I recently upgraded to kubernetes 1.7 with RBAC on GKE, and I am seeing this problem:

```
  FirstSeen	LastSeen	Count	From			SubObjectPath	Type		Reason		Message
  ---------	--------	-----	----			-------------	--------	------		-------
  6h		6m		75	loadbalancer-controller			Warning		GCE :Quota	googleapi: Error 403: Insufficient Permission, insufficientPermissions
```

I have double-checked my quotas, and they are all green.

I have also tried granting the Node service account `Project > Editor` permissions, and I have added the Node service account to the `cluster-admin` ClusterRole, just in case it had anything to do with that (which it should not, right?).

GKE Cluster logs (slightly redacted):

```
{
 insertId:  ""x""   
 jsonPayload: {
  apiVersion:  ""v1""    
  involvedObject: {
   apiVersion:  ""extensions""     
   kind:  ""Ingress""     
   name:  ""ingress-testing""     
   namespace:  ""default""     
   resourceVersion:  ""425826""     
   uid:  ""x""     
  }
  kind:  ""Event""    
  message:  ""googleapi: Error 403: Insufficient Permission, insufficientPermissions""    
  metadata: {
   creationTimestamp:  ""2017-07-15T12:54:37Z""     
   name:  ""ingress-testing.x""     
   namespace:  ""default""     
   resourceVersion:  ""53520""     
   selfLink:  ""/api/v1/namespaces/default/events/ingress-testing.14d1822c5ed30595""     
   uid:  ""x""     
  }
  reason:  ""GCE :Quota""    
  source: {
   component:  ""loadbalancer-controller""     
  }
  type:  ""Warning""    
 }
 logName:  ""projects/x/logs/events""   
 receiveTimestamp:  ""2017-07-15T19:11:59.117152623Z""   
 resource: {
  labels: {
   cluster_name:  ""app-cluster""     
   location:  """"     
   project_id:  ""x""     
  }
  type:  ""gke_cluster""    
 }
 severity:  ""WARNING""   
 timestamp:  ""2017-07-15T19:11:54Z""   
}
```

I have tried figuring out what the cause might be, but have not found anything that was applicable.

What can I do to get Ingress working again in my cluster?

Thanks!

_Copied from original issue: kubernetes/ingress-nginx#975_",closed,False,2017-10-11 21:52:37,2017-10-11 21:53:19
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/50,https://api.github.com/repos/kubernetes/ingress-gce/issues/50,Zonelister logic prevents instance removal from instance group & instance group GC,"_From @nikhiljindal on July 25, 2017 1:38_

Looking at the code (https://github.com/kubernetes/ingress/blob/a58b80017170eecbe8b2d6573b66192cafe0d32a/controllers/gce/controller/controller.go#L177), it looks like we are not doing anything when nodes are added and deleted. We should be updating the instance group when that happens.

Am I missing something?

_Copied from original issue: kubernetes/ingress-nginx#1012_",open,False,2017-10-11 21:55:46,2019-02-20 06:54:33
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/51,https://api.github.com/repos/kubernetes/ingress-gce/issues/51,Feature request: ssl-redirect on gce controller,"_From @smeruelo on August 18, 2017 15:1_

Is there any plan to support `ingress.kubernetes.io/ssl-redirect` annotation on gce controller?

If not, what's the recommended workaround for this? Examining headers is not always possible, since backend application is often out of our control.

_Copied from original issue: kubernetes/ingress-nginx#1180_",open,False,2017-10-11 21:56:09,2019-04-01 13:27:27
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/issues/52,https://api.github.com/repos/kubernetes/ingress-gce/issues/52,Document ingress.gcp.kubernetes.io/pre-shared-cert annotation,Update GLBC documentation with this annotation + usage,closed,False,2017-10-11 21:57:43,2018-03-09 22:32:30
ingress-gce,G-Harmon,https://github.com/kubernetes/ingress-gce/pull/53,https://api.github.com/repos/kubernetes/ingress-gce/issues/53,Rename local var to reflect what it is.,"This variable name looked wrong, so I'm fixing it.",closed,True,2017-10-13 19:04:35,2017-10-16 20:30:38
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/54,https://api.github.com/repos/kubernetes/ingress-gce/issues/54,revendor k8s cloud provider and its dependencies,"/assign nicksardo
",closed,True,2017-10-16 20:12:22,2017-10-16 20:31:16
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/55,https://api.github.com/repos/kubernetes/ingress-gce/issues/55,Measure and Adjust Resource Request Limit ,"Many changes such as https://github.com/kubernetes/ingress-gce/pull/48 are going into ingress-gce controller. Need to measure and adjust resource request.

",open,False,2017-10-18 17:40:15,2018-02-07 17:56:07
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/pull/56,https://api.github.com/repos/kubernetes/ingress-gce/issues/56,Updating backends/fakes to return 404 in the same way as all other fakes,"Updating `pkg/backends/fakes.go` to return 404 when object does not exist.
Verified that all other packages (`pkg/firewalls`, `pkg/healthchecks` and `pkg/instances`) return 404s in the same way.

Updated `pkg/healthchecks/fakes.go` to use the same util method as all other packages.

cc @nicksardo @bowei ",closed,True,2017-10-21 20:33:59,2017-10-23 17:23:23
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/pull/57,https://api.github.com/repos/kubernetes/ingress-gce/issues/57,Updating loadbalancer/fakes.go to return 404,"Same as https://github.com/kubernetes/ingress-gce/pull/56.
Missed `pkg/loadbalancers/fakes.go` in that PR.",closed,True,2017-10-25 20:42:54,2017-10-25 23:32:23
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/pull/58,https://api.github.com/repos/kubernetes/ingress-gce/issues/58,Extracting out annotations to a separate package to allow reuse,"Extracting out annotations related code from `pkg/controller/utils.go` into a separate `pkg/annotations` package. This will allow reusing annotation keys rather than duplicating them everywhere.

cc @bowei @nicksardo @csbell @G-Harmon ",closed,True,2017-10-25 21:33:02,2017-10-25 23:31:40
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/pull/59,https://api.github.com/repos/kubernetes/ingress-gce/issues/59,Extracting tlsloader into a separate package to enable reuse,"TlsLoader is the interface used to load TLSCerts for an ingress.

cc @bowei @nicksardo @csbell @G-Harmon @madhusudancs ",closed,True,2017-10-27 18:49:54,2017-10-27 23:31:31
ingress-gce,tsloughter,https://github.com/kubernetes/ingress-gce/issues/60,https://api.github.com/repos/kubernetes/ingress-gce/issues/60,Is there nginx-controller like session affinity support,"I've been trying to find a solid answer on how to use (if it is even possible) session affinity with a GCE ingress resource.

My understanding is that nginx supports this by going around the service's vip and directly to the pods, updating the nginx configuration as it needs to when the pods change -- since while service's support clientip affinity this would be based on the nginx ip and not the client making the http request so the service must be bypassed.

Can the GCE ingress do similar or is there an alternative way to get affinity?",closed,False,2017-10-27 21:12:03,2018-07-05 02:50:45
ingress-gce,Obdolbacca,https://github.com/kubernetes/ingress-gce/issues/61,https://api.github.com/repos/kubernetes/ingress-gce/issues/61,TLS certificate validations causes tls creation to fail,"Due to lack output couldn't track what's going wrong.
The chain like provided in attachment causes tls backend creation to be silently skipped
[example.pem.gz](https://github.com/kubernetes/ingress-gce/files/1427715/example.pem.gz)
It's ok with self-signed certificate, but constantly fails with this chain, or any chain issued by our PKI.
If we try to manually add forwarding rules and https-proxy, it's ok, also.",closed,False,2017-10-30 15:20:20,2017-11-20 15:00:01
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/62,https://api.github.com/repos/kubernetes/ingress-gce/issues/62,move neg annotation to annotations package,,closed,True,2017-10-30 17:39:12,2017-10-31 17:39:20
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/63,https://api.github.com/repos/kubernetes/ingress-gce/issues/63,NEG: skip endpoint updates with no subset change,,closed,True,2017-10-30 20:53:04,2017-10-31 17:41:32
ingress-gce,leo-baltus,https://github.com/kubernetes/ingress-gce/issues/64,https://api.github.com/repos/kubernetes/ingress-gce/issues/64,support for session affinity,"The GCE loadbalancer has support for session affinity, either by client-ip or a generated cookie. As far as i know there is no way to specify this in an ingress manifest. 

It would be very useful to have this functionality.",closed,False,2017-10-31 13:43:37,2017-11-15 19:23:14
ingress-gce,leo-baltus,https://github.com/kubernetes/ingress-gce/issues/65,https://api.github.com/repos/kubernetes/ingress-gce/issues/65,support for proper health checks,"The GCE loadbalancer perfoms health checks to see if backend are fit to take traffic. These health checks seem to be derived from the readinessProbe defined in the pod the service is pointing to where the service is taken from the ingress.

However, a pod can expose multiple ports where each port can be exposed by a service, but there can be only one readinessProbe on a pod. Also the semantics of each exposed service doe not have to match the semantics of the readinessProbe. 

It would make more sense if the LB-healthcheck would be defined either in the ingress or in the service.
",closed,False,2017-10-31 13:55:07,2017-11-15 19:21:42
ingress-gce,spiffxp,https://github.com/kubernetes/ingress-gce/pull/66,https://api.github.com/repos/kubernetes/ingress-gce/issues/66,Rename OWNERS assignees: to approvers:,"They are effectively the same, assignees is deprecated

ref: kubernetes/test-infra#3851",closed,True,2017-10-31 23:31:24,2017-11-01 15:25:22
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/67,https://api.github.com/repos/kubernetes/ingress-gce/issues/67,DO NOT MERGE,,closed,True,2017-11-01 23:38:12,2018-06-10 18:10:03
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/68,https://api.github.com/repos/kubernetes/ingress-gce/issues/68,Split namer into its own file,This does not actual code changes,closed,True,2017-11-02 19:32:26,2017-11-02 19:36:13
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/pull/69,https://api.github.com/repos/kubernetes/ingress-gce/issues/69,Removing non code test's dependency on testing package,"Removing loadbalancer package's dependency on testing.
Without this change, any binary that depends on loadbalancer package also starts exposing test flags.

cc @nicksardo @bowei ",closed,True,2017-11-02 21:27:18,2017-11-02 21:37:29
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/70,https://api.github.com/repos/kubernetes/ingress-gce/issues/70,Centralize more of the naming of GCE resources,"- Moves Namer out to its own source file.
- Improve some of the logging in the unit test.
- Lots of minor naming consistencies (look at the commits)",closed,True,2017-11-04 01:04:17,2017-11-06 23:55:07
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/issues/71,https://api.github.com/repos/kubernetes/ingress-gce/issues/71,Add tests for multi cluster ingresses,"Tasks:
- [x] Add an e2e test to ensure that instance groups annotation is added to an ingress with the multicluster ingress class annotation.
- [ ] Ensure that the instance groups annotation is added for each zone in a multi zonal cluster.

cc @nicksardo @G-Harmon",open,False,2017-11-06 20:37:50,2019-01-10 22:48:18
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/72,https://api.github.com/repos/kubernetes/ingress-gce/issues/72,Move GetNamedPort to Namer,- Split off unit tests for namer into namer_test.go,closed,True,2017-11-07 00:43:11,2017-12-23 21:24:46
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/73,https://api.github.com/repos/kubernetes/ingress-gce/issues/73,Add unit test for functions in namer,,closed,True,2017-11-07 01:53:47,2017-11-07 05:59:08
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/74,https://api.github.com/repos/kubernetes/ingress-gce/issues/74,Add ability to change prefix in the Namer,"- This allows the multicluster code to share use of the Namer.
- Add more unit testing
- Coverage is now ~90% of namer code.",closed,True,2017-11-07 18:38:36,2017-12-05 22:23:10
ingress-gce,stevenbarragan,https://github.com/kubernetes/ingress-gce/issues/75,https://api.github.com/repos/kubernetes/ingress-gce/issues/75,Invalid value for field 'namedPorts[*].port': '0',"I'm trying to create a new ingress controller but I'm getting this error:
```
googleapi: Error 400: Invalid value for field 'namedPorts[12].port': '0'. Must be greater than or equal to 1, invalid
```
Then I checked the other ingresses, they still work but I'm getting the same exact error. The new ingress does not work at all.

I found this [answer](https://stackoverflow.com/a/45974827/13253220) but I have not port0 in my `ports`. I notice I have exactly 12 named ports in my instance group, and I'm guessing the array `namedPorts` is a zero-based so accessing the 12 element might be causing the issue.

I'm not exactly sure what triggered it, but I updated to 1.8.2 recently.

This is my ingress
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.gcp.kubernetes.io/pre-shared-cert: certificate
    ingress.kubernetes.io/force-ssl-redirect: ""true""
    kubernetes.io/ingress.allow-http: ""false""
    kubernetes.io/ingress.global-static-ip-name: static-ip-name
  generation: 1
  labels:
    app: core
    chart: core-0.1.0
    heritage: Tiller
    release: core
  name: core
  namespace: develop
spec:
  backend:
    serviceName: core
    servicePort: 80
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          serviceName: core
          servicePort: 80
status:
  loadBalancer: {}
```

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.6"", GitCommit:""4bc5e7f9a6c25dc4c03d4d656f2cefd21540e28c"", GitTreeState:""clean"", BuildDate:""2017-09-14T06:55:55Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.2-gke.0"", GitCommit:""52ea03646e64b35a5b092ab32bb529400c296aa6"", GitTreeState:""clean"", BuildDate:""2017-10-24T23:31:18Z"", GoVersion:""go1.8.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

Any ideas?

it may be related to https://github.com/kubernetes/ingress-gce/issues/43",closed,False,2017-11-09 23:49:37,2018-09-10 20:28:07
ingress-gce,mikepc,https://github.com/kubernetes/ingress-gce/issues/76,https://api.github.com/repos/kubernetes/ingress-gce/issues/76,ingress path confusing,"      paths:
      - path: /posts/*
        backend:
          serviceName: post-service
          servicePort: 8585

Requests to /posts are sent to the default backend (404!), while sub-path requests are forwarded accordingly.

There needs to be sufficient documentation around this so that we know how to configure the ingress forwarding. I've tried adding 2 separate paths one for /posts, and /posts/* and that didn't work either.

How can I send all requests for /posts to my post-service backend?",closed,False,2017-11-12 20:26:40,2018-05-25 20:57:27
ingress-gce,JordanP,https://github.com/kubernetes/ingress-gce/issues/77,https://api.github.com/repos/kubernetes/ingress-gce/issues/77,examples/websocket/server.go: concurrent write to socket,"In the `handleWSConn` function, the goroutine calls `c.WriteMessage` and the other for loop also calls `c.WriteMessage`. But according to Gorilla WS doc, `WriteMessage` is not goroutine safe. We need to use a mutex to protect concurrent writes.

I saw a couple of goroutine panics because of this while doing some light stress tests.",closed,False,2017-11-30 14:54:27,2017-12-11 18:36:47
ingress-gce,G-Harmon,https://github.com/kubernetes/ingress-gce/pull/78,https://api.github.com/repos/kubernetes/ingress-gce/issues/78,Add ListGlobalForwardingRules to the LoadBalancers interface.,"I have an upcoming commit in the k8s-multicluster-ingress repo where I will use this.

cc @bowei @nicksardo @nikhiljindal ",closed,True,2017-12-05 15:33:18,2017-12-06 17:54:35
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/79,https://api.github.com/repos/kubernetes/ingress-gce/issues/79,"WIP: Adds pkg/cache, a type-safe way to store GCP objects","This commit adds the data structure, follow on commits will use the data
structure to implement both a cache of the cloud state as well as use it
to implement the synchronization routines. See the package docstring in
cache.go for more details.

The cache code is type-safe and generated from descriptions contains in
the pkg/cache/meta. The contents of `gen.go` are derived from `$ go run
gen/main.go > gen.go`

As the code is not expected to change often (only when types are
added/removed), the generated code is committed to the repo and the code
generation is expected to be run by hand. If there are significant
divergences from these assumptions, then full integration into the build
process will be needed.",closed,True,2017-12-05 22:43:40,2018-06-19 18:21:56
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/80,https://api.github.com/repos/kubernetes/ingress-gce/issues/80,Update websocket example,"Changes:
- Fixes occasional panics due to concurrent writing
- Adds html page for connecting to the websocket
- Adds copyright header

Fixes #77 ",closed,True,2017-12-07 20:21:16,2017-12-11 18:36:47
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/81,https://api.github.com/repos/kubernetes/ingress-gce/issues/81,Fix some trivial invalid go variable names in glbc,,closed,True,2017-12-08 07:19:32,2018-06-10 18:04:31
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/82,https://api.github.com/repos/kubernetes/ingress-gce/issues/82,"Add build, coverage and report badges",As what we had for [kubernetes/dns](https://github.com/kubernetes/dns). The coverage result should be available at the next build.,closed,True,2017-12-09 01:30:13,2018-01-06 01:42:02
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/83,https://api.github.com/repos/kubernetes/ingress-gce/issues/83,Patch trivial fixes from goreport page.,"Mostly fixing misspells, exported comments and redundant if blocks. No functional changes.",closed,True,2017-12-09 02:11:38,2018-01-23 02:06:35
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/pull/84,https://api.github.com/repos/kubernetes/ingress-gce/issues/84,Fixing typos in gce-tls example readme,"The example ingress name is `test` and not `gce-test`.
Also need to run the `curl` command without `-k` to get the SSL self signed cert error.",closed,True,2017-12-11 19:49:29,2017-12-11 20:21:55
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/85,https://api.github.com/repos/kubernetes/ingress-gce/issues/85,Minor fixes to example JS,"Fixes:
- Slashes in URL were being escaped when hydrating template.
- Use correct func for printing error message.",closed,True,2017-12-12 21:26:13,2017-12-12 21:39:51
ingress-gce,spiffxp,https://github.com/kubernetes/ingress-gce/pull/86,https://api.github.com/repos/kubernetes/ingress-gce/issues/86,Add code-of-conduct.md,"Refer to kubernetes/community as authoritative source for code of conduct

ref: kubernetes/community#1527",closed,True,2017-12-20 18:32:37,2017-12-21 23:39:06
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/issues/87,https://api.github.com/repos/kubernetes/ingress-gce/issues/87,Support multiple addresses (including IPv6),"The ingress spec supports specifying an ip address with the `kubernetes.io/ingress.global-static-ip-name` annotation, but the ingress-gce controller assumes that it is an ipv4 IP address.

GCLB supports specifying both an ipv4 and ipv6 IPs as per: https://cloud.google.com/compute/docs/load-balancing/http/cross-region-example.

Are there plans to support ipv6?
I tried to find an existing issue, but didnt and hence am filling this. Feel free to close as duplicate if there is an existing issue.

cc @bowei @nicksardo @csbell ",open,False,2017-12-21 19:41:01,2019-04-04 17:13:36
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/88,https://api.github.com/repos/kubernetes/ingress-gce/issues/88,Add e2e make rule,"Adds a new rule to the Makefile which pushes an image of glbc for e2e testing. This target basically does the same thing as a regular build and push but we need it for two reasons:

1. Modify the CONTAINER_PREFIX so that it is clear that image is for e2e testing.
2. In test-infra/kubetest, we need to build an image of glbc for testing and having a target called e2e so that we kubetest can just say ""make e2e"" makes it clearer what we are doing.",closed,True,2017-12-22 17:14:53,2018-01-02 18:38:24
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/89,https://api.github.com/repos/kubernetes/ingress-gce/issues/89,Delete unreferenced constants,"deleteType,invalidPort,nameLenLimit",closed,True,2017-12-22 21:57:44,2017-12-23 21:23:06
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/90,https://api.github.com/repos/kubernetes/ingress-gce/issues/90,Move taskQueue to utils.PeriodicTaskQueue,"- Adds unit test for TaskQueue
- Fixes minor violation of abstractions in controller_test.go",closed,True,2017-12-23 00:46:45,2017-12-28 01:16:51
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/91,https://api.github.com/repos/kubernetes/ingress-gce/issues/91,Translator,"Move translator into its own package
Clean up cross dependencies; this is a pure refactor
",closed,True,2017-12-23 08:59:40,2017-12-28 00:14:40
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/92,https://api.github.com/repos/kubernetes/ingress-gce/issues/92,Minor cleanup,"Move K8sAnnotationPrefix into annotations
Move AppProtocol and associated constants into annotations package ￼…
Remove stuttering IngAnnotations -> Ingress
Remove stuttering SvcAnnotations -> Service
Split off gceurlmap into its own file",closed,True,2017-12-23 21:30:06,2017-12-28 01:28:12
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/93,https://api.github.com/repos/kubernetes/ingress-gce/issues/93,Ingress E2E setup is breaking ,"The PR at https://github.com/kubernetes/test-infra/pull/6072 setup e2e testing for this repo. 

However the e2e tests now break with the same error (ex. #90, #91, #92). This is most likely due to an error in the setup. 

Any PR's affected should not be blocked by this and should be okay for submission.",closed,False,2017-12-27 15:22:30,2018-01-03 01:01:19
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/issues/94,https://api.github.com/repos/kubernetes/ingress-gce/issues/94,Test ingress creation after hitting error creating GCP resource due to quota,https://github.com/kubernetes/kubernetes/issues/28784,closed,False,2017-12-27 17:26:29,2018-09-23 20:37:19
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/95,https://api.github.com/repos/kubernetes/ingress-gce/issues/95,E2e test setup,Testing that presubmit e2e tests work properly. Will not be submitted so no need for review,closed,True,2017-12-27 20:52:10,2018-01-09 00:37:05
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/96,https://api.github.com/repos/kubernetes/ingress-gce/issues/96,Update vendor,,closed,True,2017-12-27 22:17:39,2017-12-28 01:17:24
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/97,https://api.github.com/repos/kubernetes/ingress-gce/issues/97,Cleanup,,closed,True,2017-12-28 01:45:31,2017-12-28 19:00:34
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/98,https://api.github.com/repos/kubernetes/ingress-gce/issues/98,Fix gce,"New code requires direct import of the auth provider otherwise the auth provider plugin will no longer be registered.
",closed,True,2017-12-28 23:40:06,2018-06-10 18:09:10
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/99,https://api.github.com/repos/kubernetes/ingress-gce/issues/99,Fix push-e2e make rule,"Fixes push-e2e make rule to push the containers to a registry it is allowed to push to. 

Specifically, kubetest does not have permissions to push to google_containers.",closed,True,2018-01-05 20:23:04,2018-01-08 23:28:38
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/100,https://api.github.com/repos/kubernetes/ingress-gce/issues/100,Deprecate --use-real-cloud and --verbose flags,- Includes some minor logging cleanups,closed,True,2018-01-05 23:54:29,2018-01-06 00:18:25
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/101,https://api.github.com/repos/kubernetes/ingress-gce/issues/101,Cleanup,,closed,True,2018-01-06 01:50:48,2018-01-08 19:25:57
ingress-gce,hdave,https://github.com/kubernetes/ingress-gce/issues/102,https://api.github.com/repos/kubernetes/ingress-gce/issues/102,Large file upload fails after 30 seconds,"I am using GLBC on a Google Cloud cluster - Kubernetes v1.8.  In this instance, the ingress is for the Nexus repository manager and is defined as follows: 

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: my-nexus-https
  annotations:
    kubernetes.io/ingress.class: ""gce""
    kubernetes.io/ingress.allow-http: ""false""
  labels:
    app: ops-nginx
spec:
  tls:
  - hosts:
    - mycorp.com
    secretName: testssl
  backend:
    serviceName: my-nexus
    servicePort: 8081
```

If I then attempt to upload a file to Nexus via curl, it runs for about 30 seconds then fails as follows:

```
curl -i -k -T bigfile https://mycorp.com/repository/testbigfiles/

* GnuTLS recv error (-110): The TLS connection was non-properly terminated.
* Closing connection 0
curl: (56) GnuTLS recv error (-110): The TLS connection was non-properly terminated.
```

Logs in Nexus indicate client just dropped:

```
Caused by: org.eclipse.jetty.io.EofException: Early EOF
	at org.eclipse.jetty.server.HttpInput$3.noContent(HttpInput.java:791)
	at org.eclipse.jetty.server.HttpInput.read(HttpInput.java:157)
	at org.sonatype.nexus.common.hash.MultiHashingInputStream.read(MultiHashingInputStream.java:66)
	at com.google.common.io.CountingInputStream.read(CountingInputStream.java:63)
	at java.security.DigestInputStream.read(DigestInputStream.java:161)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at com.google.common.io.ByteStreams.copy(ByteStreams.java:106)
	at org.sonatype.nexus.blobstore.file.internal.SimpleFileOperations.create(SimpleFileOperations.java:60)
	at org.sonatype.nexus.blobstore.file.FileBlobStore.lambda$0(FileBlobStore.java:287)
	at org.sonatype.nexus.blobstore.file.FileBlobStore.tryCreate(FileBlobStore.java:350)
```

I did not expect this behavior.  This does not happen if I use the `Type=LoadBalancer` L4 load balancer.  Thinking it might have been the L7's health check, I examined it in the cluster dashboard, but it is reporting healthy.

```
      ""ingress.kubernetes.io/backends"": ""{\""k8s-be-31391--74104f0a9bad5b66\"":\""HEALTHY\""}"",
```

Any help is appreciated.
",closed,False,2018-01-10 22:05:49,2018-01-11 22:41:37
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/103,https://api.github.com/repos/kubernetes/ingress-gce/issues/103,"Modify VERSION to use ""latest"" in push-e2e target","Using latest as the tag for our e2e testing image will allow the e2e jobs in Prow to simply pull using the tag ""latest"".  This is not ideal but it fixes our jobs for now. ",closed,True,2018-01-11 19:09:18,2018-01-11 19:15:06
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/104,https://api.github.com/repos/kubernetes/ingress-gce/issues/104,Remove the duplicate health check example and restructure example folder,"All examples here are apparently gce specific, hence remove the additional gce folder.

/assign @nicksardo 
who seems to be the last person that managed them.",closed,True,2018-01-13 00:20:59,2018-01-13 00:34:29
ingress-gce,sdouche,https://github.com/kubernetes/ingress-gce/issues/105,https://api.github.com/repos/kubernetes/ingress-gce/issues/105,Wrong health check with end-to-end https scheme,"Hi,
I'm trying to set https end-to-end (GCP LB to K8S Pod). On the Pod I have gunicorn that doesn't support HTTP and HTTPS in the same time, but only one. So I must use HTTPS everywhere.

 I set this in the deployment object:
```
        readinessProbe:
          httpGet:
            scheme: HTTPS
            path: /heartbeat/
            port: 443
          initialDelaySeconds: 5
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 10
```

I add this to the service object:
```
  annotations:
    service.alpha.kubernetes.io/app-protocols: '{""my-https-port"":""HTTPS""}'
``` 

Unfortunately, the health check is set to check `/` on HTTP, and the backend service is on HTTP.

Note: The same configuration with HTTP works well (the LB is up and running).

Did I miss something?

Regards.",closed,False,2018-01-16 09:35:06,2018-02-05 19:49:17
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/106,https://api.github.com/repos/kubernetes/ingress-gce/issues/106,Sync one ingress,"A bunch of patches:
* A lot of clean up and movement
* Sync one ingress (as opposed to all) in Checkpoint
* Call GC explicitly when an Ingress is deleted
* Shutdown() firewall when there are no more Ingresses",closed,True,2018-01-16 18:35:57,2018-01-30 21:25:49
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/107,https://api.github.com/repos/kubernetes/ingress-gce/issues/107,Add unit test for the generated GCE config reader func,"@nicksardo might have the most context.
/assign @nicksardo 

Ref https://docs.google.com/document/d/1E2ufxZGujFCYKYOsvBjb4VMNjBy8CM-MMycoT_UVwIs/edit#heading=h.t6chylryqex6.",closed,True,2018-01-17 17:47:47,2018-01-24 16:28:48
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/108,https://api.github.com/repos/kubernetes/ingress-gce/issues/108,Remove .travis.yml,"We don't need Travis anymore since Prow runs our unit tests per PR now.

Note: The Travis CI build will obviously fail here.",closed,True,2018-01-17 23:03:56,2018-01-18 21:36:52
ingress-gce,Fandekasp,https://github.com/kubernetes/ingress-gce/issues/109,https://api.github.com/repos/kubernetes/ingress-gce/issues/109,rewrite,"Hi, I've been struggling to set a rewrite rule for my ingress (GCP Kubernetes with a Deployment, Service NodePort and Ingress). What I'm trying to get is:
* api.example.com/  →  backend/api/
* www.example.com/ → backend/www/

So that my server can server requests whether they start with /api/ or /www/, without a care about the subdomains.

I'm creating this issue because I have a question regarding your comment in https://github.com/kubernetes/ingress-gce#paths
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginxtest-ingress
spec:
  rules:
  - http:
      paths:
      - path: /hostname
        backend:
          serviceName: nginxtest
          servicePort: 80
```

```
Note what just happened, the endpoint exposes /hostname, and the loadbalancer forwarded the entire matching url to the endpoint. This means if you had '/foo' in the Ingress and tried accessing /hostname, your endpoint would've received /foo/hostname and not known how to route it.
```
I wish that was true... for me, loadbalancerip/foo would be the only queryable url, while everything else returns 404 Not Found errors. The Ingress only forward a request if it matches the path, that's it.

Anyways, if you have a suggestion regarding my problem, I'd really appreciate. Couldn't figure out how to get it working with annotations [rewrite-url or app-root](https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md)
",open,False,2018-01-18 07:08:53,2019-03-27 19:46:29
ingress-gce,ahmetb,https://github.com/kubernetes/ingress-gce/issues/110,https://api.github.com/repos/kubernetes/ingress-gce/issues/110,Ingress without default backend explicitly configured doesn't work at all,"I'm trying to create a fanout ingress on GKE (v1.7):

- `/*` &rarr; `cats` Service
- `/dogs/*` &rarr; `dogs` Service

When I deploy these resources, after LB is configured fine (I can hit the gke `default backend: 404`), I can neither get `http://IP/` or `http://IP/dogs/` to work. I always get either `default backend: 404` or HTTP 502 from Google’s LB.

Is it because I don't have a `spec.backend` that returns 200 OK on `GET /`? From the documentation, I expect this to work.

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: cats-and-dogs
spec:
  rules:
  - http:
      paths:
      - path: /*
        backend:
          serviceName: cats
          servicePort: 8080
      - path: /dogs/*
        backend:
          serviceName: dogs
          servicePort: 8080
```

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: cats
  labels:
    app: cats
spec:
  template:
    metadata:
      labels:
        app: cats
    spec:
      containers:
      - name: web
        image: gcr.io/google-samples/hello-app:1.0
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  labels:
    run: cats
  name: cats
spec:
  type: NodePort
  selector:
    run: cats
  ports:
  - port: 8080
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: dogs
  labels:
    app: dogs
spec:
  template:
    metadata:
      labels:
        app: dogs
    spec:
      containers:
      - name: web
        image: gcr.io/google-samples/hello-app:2.0
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  labels:
    run: dogs
  name: dogs
spec:
  type: NodePort
  selector:
    run: dogs
  ports:
  - port: 8080
```

/cc @nicksardo ",closed,False,2018-01-18 18:00:13,2018-01-18 19:26:41
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/111,https://api.github.com/repos/kubernetes/ingress-gce/issues/111,Add unit test to verify pre-shared cert retention,"There was a bug when transitioning from a pre-shared certificate to a secret-based certificate. The controller deletes the pre-shared certificate when it really should ignore it (quote https://github.com/kubernetes/ingress-nginx/pull/639). This PR adds an unit test for that.

Ref https://docs.google.com/document/d/1E2ufxZGujFCYKYOsvBjb4VMNjBy8CM-MMycoT_UVwIs/edit#heading=h.djafujeypqcp.

/assign @nicksardo @rramkumar1 ",closed,True,2018-01-19 19:49:23,2018-01-24 16:27:47
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/112,https://api.github.com/repos/kubernetes/ingress-gce/issues/112,Emit event on TLS errors,"From https://github.com/kubernetes/kubernetes/pull/58643, found that no event is emitted when glbc fails to retrieve TLS cert. It might be helpful to make this visible to users.

/assign @nicksardo ",closed,True,2018-01-23 00:38:15,2018-07-25 22:56:41
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/113,https://api.github.com/repos/kubernetes/ingress-gce/issues/113,Update vendor,,closed,True,2018-01-23 23:50:18,2018-06-10 18:06:22
ingress-gce,zrosenbauer,https://github.com/kubernetes/ingress-gce/pull/114,https://api.github.com/repos/kubernetes/ingress-gce/issues/114,Update README.md to point to example,,closed,True,2018-01-24 00:30:15,2018-01-24 17:01:32
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/115,https://api.github.com/repos/kubernetes/ingress-gce/issues/115,Remove some vars from make push-e2e target,"We now pass these vars through inside of the e2e job config so they are no longer needed in the Makefile.

/assign @bowei ",closed,True,2018-01-24 00:36:01,2018-01-24 16:24:13
ingress-gce,znorris,https://github.com/kubernetes/ingress-gce/issues/116,https://api.github.com/repos/kubernetes/ingress-gce/issues/116,Experiencing downtime when updating hosts backend in ingress controller,"## Issue
Why would I experience downtime when I update more than one backend service at a time, but not when I updated a single backend? (This may not be the correct question or issue summary but at the moment I'm not clear on why this is happening. It could have to do with the old backend being completely dereferenced from my ingress config.)

## Reproduce
I've created a repo that includes pretty much everything one would need to reproduce this issue. However, I think the example below illustrates it well enough that you won't need the repo.
https://github.com/znorris/gce_ingress_troubleshoot

## Example
For instance, I had a single app & service handling requests for several hosts.
```
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  spec:
    rules:
    - host: a.host
      http:
        paths:
        - backend:
            serviceName: app-z
            servicePort: 80
    - host: b.host
      http:
        paths:
        - backend:
            serviceName: app-z
            servicePort: 80
    - host: c.host
      http:
        paths:
        - backend:
            serviceName: app-z
            servicePort: 80
```
```
NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
app-z         NodePort    10.0.0.1        <none>        80:30001/TCP     1d
```

I then added a new app/deployment and service for each of the three hosts.
```
NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
app-a         NodePort    10.0.0.1        <none>        80:30002/TCP     1d
app-b         NodePort    10.0.0.1        <none>        80:30003/TCP     1d
app-c         NodePort    10.0.0.1        <none>        80:30004/TCP     1d
app-z         NodePort    10.0.0.1        <none>        80:30001/TCP     1d
```

I verified that the apps were responding to HCs and that the NodePorts were working.
Once that was complete I updated a single hosts backend in the ingress (`a.host`).
```
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  spec:
    rules:
    - host: a.host
      http:
        paths:
        - backend:
            serviceName: app-a
            servicePort: 80
    - host: b.host
      http:
        paths:
        - backend:
            serviceName: app-z
            servicePort: 80
    - host: c.host
      http:
        paths:
        - backend:
            serviceName: app-z
            servicePort: 80
```


I waited for the load balancer to update, and for the new service/app to respond to this traffic. I also verified in the cloud console that the health check associated with this new backend was passing. This is how I would expect everything to work and it had zero downtime.

I then decided that I had 5 more hosts to update on the ingress controller, and that it would be faster to update the remaining hosts all at once.
```
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  spec:
    rules:
    - host: a.host
      http:
        paths:
        - backend:
            serviceName: app-a
            servicePort: 80
    - host: b.host
      http:
        paths:
        - backend:
            serviceName: app-b
            servicePort: 80
    - host: c.host
      http:
        paths:
        - backend:
            serviceName: app-c
            servicePort: 80
```

Once that was done I began to see failing requests (HTTP 502 from load balancer) for roughly 5 minutes for those hosts I had changed in bulk. After the 5 minutes requests were OK. During that time the load balancer was logging these 502's:
```
jsonPayload:{
  @type:  ""type.googleapis.com/google.cloud.loadbalancing.type.LoadBalancerLogEntry""   
  statusDetails:  ""failed_to_connect_to_backend""   
 }
 ```

I then checked that the appropriate service/app was responding to requests for all hosts in the ingress controller. Everything looked good. I then went into cloud console and verified that the `app-z` backend was no longer present and that its HCs had been cleaned up as well. They were, so I then removed the old service and deployment/app.
```
NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
app-a         NodePort    10.0.0.1        <none>        80:30002/TCP     1d
app-b         NodePort    10.0.0.1        <none>        80:30003/TCP     1d
app-c         NodePort    10.0.0.1        <none>        80:30004/TCP     1d
```
I'm now in my desired state and everything is working as expected.",closed,False,2018-01-26 22:32:59,2018-12-07 13:42:04
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/117,https://api.github.com/repos/kubernetes/ingress-gce/issues/117,Code review comments,This was somehow missing from the branch that was merged,closed,True,2018-01-29 22:32:20,2018-01-29 22:34:23
ingress-gce,mrwonko,https://github.com/kubernetes/ingress-gce/pull/118,https://api.github.com/repos/kubernetes/ingress-gce/issues/118,Fix link in Beta Limitations,There was a stray space preventing a link from working properly.,closed,True,2018-01-30 08:40:27,2018-02-06 05:15:55
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/119,https://api.github.com/repos/kubernetes/ingress-gce/issues/119,Always set -logtostderr (this matches the original behavior),"We will remove this when a release can be cut to support logging to
stderr.",closed,True,2018-01-30 21:12:14,2018-06-10 18:08:00
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/120,https://api.github.com/repos/kubernetes/ingress-gce/issues/120,fix typo,,closed,True,2018-01-30 22:16:30,2018-06-10 18:07:47
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/121,https://api.github.com/repos/kubernetes/ingress-gce/issues/121,Introduce flag endpoint,"This endpoint currently prints the version and current verbosity level. You can also change the verbosity during runtime with a simple PATCH.  
```
curl -X PUT http://localhost:8080/flag?v=10
```

More information can be added later.",closed,True,2018-02-02 19:58:12,2018-02-16 04:55:47
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/122,https://api.github.com/repos/kubernetes/ingress-gce/issues/122,Firewall Sync: Allow entire nodeport range,"## Changes
- Firewall rule whitelists entire nodeport range instead of pin-holding nodeports used by ingresses. This is not a concern because the source IP ranges are only used by Google for health checkers and proxies.
- Nodeport range is definable via flag.
- Begin enforcing target tags are up-to-date. They were previously only updated at firewall change.
- Use the `gce.LoadBalancerSourceRanges()` static func for specifying the GCE source ranges. This is definable via flag.
",closed,True,2018-02-02 20:02:52,2018-02-03 07:45:14
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/123,https://api.github.com/repos/kubernetes/ingress-gce/issues/123,Sync ingress-specific backends and minor logging changes,"
## Changes
- Backend service pool only syncs the list of service ports used for the respective ingress.
- Consolidate all ingress references to a single object which was deep copied.
- Changing log level of a frequently printed line from 4->5.
- Minor modifications to event handlers

#### Example with two ingresses in separate namespaces:
Before
```
# While syncing any ingress
I0205 23:03:18.031082       1 backends.go:216] Sync: backends [{30466 HTTP testing/my-echo-svc {1 0 my-http-port}} {32349 HTTP default/my-echo-svc {1 0 my-http-port}}]
```

After
```
# While syncing default/my-echo-svc
I0205 22:59:43.566574       1 backends.go:224] Sync: backends [{32349 HTTP default/my-echo-svc {1 0 my-http-port} 80 false}]
# While syncing testing/my-echo-svc
I0205 22:59:45.215794       1 backends.go:224] Sync: backends [{30466 HTTP testing/my-echo-svc {1 0 my-http-port} 80 false}]
```",closed,True,2018-02-03 07:40:27,2018-03-05 06:37:23
ingress-gce,AndyBarron,https://github.com/kubernetes/ingress-gce/issues/124,https://api.github.com/repos/kubernetes/ingress-gce/issues/124,Support for rewrite-target annotation,"https://github.com/kubernetes/ingress-gce/blob/master/docs/annotations.md#url-related

This has been a pain point for us deploying microservices on GKE. We need `static-ip-name`, so we can't switch to the Nginx ingress.
",closed,False,2018-02-04 08:28:03,2018-02-07 17:32:32
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/125,https://api.github.com/repos/kubernetes/ingress-gce/issues/125,sync node on node status change,helps to avoid 502s when node gets shutdown. ,closed,True,2018-02-05 22:30:55,2018-02-06 00:45:56
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/126,https://api.github.com/repos/kubernetes/ingress-gce/issues/126,Add some documentation for how to run e2e tests locally.,"Really rough draft for some documentation that helps explain how to run some e2e tests locally.

/assign @nicksardo ",closed,True,2018-02-05 23:01:53,2018-02-06 00:50:44
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/issues/127,https://api.github.com/repos/kubernetes/ingress-gce/issues/127,Default backend service is created when no ingress needs it,"Current logic creates a backend-service when ingresses exist. However, this is not needed if every ingress provides its own default service.",closed,False,2018-02-06 01:11:46,2018-05-10 22:35:49
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/128,https://api.github.com/repos/kubernetes/ingress-gce/issues/128,Rename Port to NodePort,"In order to prevent confusion with the `SvcPort` and `SvcTargetPort`, I suggest we be more specific by renaming `Port` to `NodePort`",closed,True,2018-02-06 07:41:12,2018-02-06 17:19:34
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/129,https://api.github.com/repos/kubernetes/ingress-gce/issues/129,Periodic resync,Periodic resync behavior was inadvertently removed in previous PR: https://github.com/kubernetes/ingress-gce/pull/123/files#diff-3c862eb54a8e0e161b534e0c67e5379eL143,closed,True,2018-02-07 22:48:56,2018-02-08 20:07:22
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/issues/130,https://api.github.com/repos/kubernetes/ingress-gce/issues/130,Ingress name has random trailing hash in events,"Created one ingress named `pre-shared-cert` under namespace `default`, got below events:
```
$ kubectl get event
LAST SEEN   FIRST SEEN   COUNT     NAME                               KIND      SUBOBJECT   TYPE      REASON    SOURCE                    MESSAGE
11m         11m          1         pre-shared-cert.15118864b017425f   Ingress               Normal    ADD       loadbalancer-controller   default/pre-shared-cert
11m         11m          8         pre-shared-cert.15118869c0878688   Ingress               Warning   GCE       loadbalancer-controller   googleapi: Error 404: The resource 'projects/k8s-ingress-e2e-scale-backup/global/sslCertificates/test-pre-shared-cert' was not found, notFound
11m         11m          7         pre-shared-cert.15118869c087f98f   Ingress               Warning   Service   loadbalancer-controller   Could not find nodeport for backend {ServiceName:echoheaders-https ServicePort:{Type:0 IntVal:80 StrVal:}}: service default/echoheaders-https not found in store
5m          11m          18        pre-shared-cert.15118869c0884157   Ingress               Normal    Service   loadbalancer-controller   no user specified default backend, using system default
11m         11m          1         pre-shared-cert.1511886a2dafc276   Ingress               Normal    CREATE    loadbalancer-controller   ip: X.X.X.X
```",closed,False,2018-02-09 03:04:18,2018-02-09 03:10:25
ingress-gce,anuraaga,https://github.com/kubernetes/ingress-gce/issues/131,https://api.github.com/repos/kubernetes/ingress-gce/issues/131,SSL certificate name non-unique when namespace + ingress name too long,"I recently ran into an issue where due to a combination of somewhat long namespace + ingress name, a GCE load balancer was created for each of two ingresses with very similar name

```
k8s-um-abcdefghijklmn-abcde-prod-abcdefghijklmn-admin-server-p0
k8s-um-abcdefghijklmn-abcde-prod-abcdefghijklmn-admin-server-q0
```

and the SSL certificate configured for both had the same name

`k8s-ssl-abcdefghijklmn-abcde-prod-abcdefghijklmn-admin-server-0`

While it seems GCP allows this, other ingress-aware components, in my case cert-manager, can get confused and the issue I saw was the SSL certificate being bounced between the two load balancers. As cert-manager presumably has no idea what the GCP load balancer name / SSL cert name is for an ingress, I guess the issue is in the ingress controller. Either it should correctly operate ingresses of different names which happen to have the same name of load balancer / SSL cert, or ingresses should be rejected if they would result in too long a name to be unique.",closed,False,2018-02-09 10:01:41,2018-04-27 21:46:58
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/issues/132,https://api.github.com/repos/kubernetes/ingress-gce/issues/132,Fix the coverage badge,"Coverage badge currently shows up as `unknown` in README as it isn't setup yet.

/assign",closed,False,2018-02-09 18:40:24,2018-07-16 20:15:37
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/133,https://api.github.com/repos/kubernetes/ingress-gce/issues/133,Emit event on RuntimeInfo error,"Follow up of https://github.com/kubernetes/ingress-gce/pull/112. Emit error event separately for now.

/assign @nicksardo ",closed,True,2018-02-09 19:32:41,2018-02-09 19:45:19
ingress-gce,mooperd,https://github.com/kubernetes/ingress-gce/issues/134,https://api.github.com/repos/kubernetes/ingress-gce/issues/134,Issue closed without comment,"Hi, What happens with this issue? Did it get fixed? What can we expect with the fix?

https://github.com/kubernetes/ingress-gce/issues/44#issuecomment-356700216",closed,False,2018-02-12 11:47:41,2018-02-12 22:08:39
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/135,https://api.github.com/repos/kubernetes/ingress-gce/issues/135,Verbose flag: set v=3,"V=3 for 0.9.8 is more or less equivalent to v=4 for 0.9.7. This change will allow us to change the glbc.manifest to use `--verbose` and be backwards compatible with the 0.9.7 image.


cc @rramkumar1 ",closed,True,2018-02-12 17:50:24,2018-02-12 19:26:22
ingress-gce,danehans,https://github.com/kubernetes/ingress-gce/issues/136,https://api.github.com/repos/kubernetes/ingress-gce/issues/136,Orphaned Resources After Cluster Delete,"After running the following commands, I see orphaned resources that were created by the gce ingress controller:

```
$ kubectl delete ing $ING -n $ING_NAMESPACE
$ gcloud container clusters delete $CLUSTER_NAME
```
List of orphaned resources:
```
$ gcloud compute instance-groups list
NAME                      LOCATION    SCOPE  NETWORK  MANAGED  INSTANCES
k8s-ig--6c90c323237f439e  us-west1-a  zone   default  No       0
k8s-ig--71cb3990c3cf10a6  us-west1-a  zone   default  No       0

$ gcloud compute backend-services list
NAME                            BACKENDS                                            PROTOCOL
k8s-be-30286--71cb3990c3cf10a6  us-west1-a/instanceGroups/k8s-ig--71cb3990c3cf10a6  HTTP
k8s-be-31225--6c90c323237f439e  us-west1-a/instanceGroups/k8s-ig--6c90c323237f439e  HTTP
k8s-be-31253--6c90c323237f439e  us-west1-a/instanceGroups/k8s-ig--6c90c323237f439e  HTTP
k8s-be-31345--71cb3990c3cf10a6  us-west1-a/instanceGroups/k8s-ig--71cb3990c3cf10a6  HTTP
k8s-be-31823--6c90c323237f439e  us-west1-a/instanceGroups/k8s-ig--6c90c323237f439e  HTTP
k8s-be-32421--71cb3990c3cf10a6  us-west1-a/instanceGroups/k8s-ig--71cb3990c3cf10a6  HTTP

$ gcloud compute url-maps list
NAME                                             DEFAULT_SERVICE
k8s-um-fortio-fortio-ingress--6c90c323237f439e   backendServices/k8s-be-31253--6c90c323237f439e
k8s-um-fortio-fortio-ingress--71cb3990c3cf10a6   backendServices/k8s-be-30286--71cb3990c3cf10a6
k8s-um-fortio-fortio-ingress2--6c90c323237f439e  backendServices/k8s-be-31823--6c90c323237f439e

$ gcloud compute target-http-proxies list
NAME                                             URL_MAP
k8s-tp-fortio-fortio-ingress--71cb3990c3cf10a6   k8s-um-fortio-fortio-ingress--71cb3990c3cf10a6
k8s-tp-fortio-fortio-ingress2--6c90c323237f439e  k8s-um-fortio-fortio-ingress2--6c90c323237f439e

$ gcloud compute forwarding-rules list
NAME                                             REGION    IP_ADDRESS      IP_PROTOCOL  TARGET
a38f1bb8811e511e89c5b42010a8a013                 us-west1  35.197.65.23    TCP          us-west1/targetPools/a38f1bb8811e511e89c5b42010a8a013
a5f61b2bd126f11e89c5b42010a8a013                 us-west1  35.203.177.233  TCP          us-west1/targetPools/a5f61b2bd126f11e89c5b42010a8a013
ac78780fb11e111e8858442010a8a020                 us-west1  35.230.48.211   TCP          us-west1/targetPools/ac78780fb11e111e8858442010a8a020
k8s-fw-fortio-fortio-ingress--71cb3990c3cf10a6             35.227.193.162  TCP          k8s-tp-fortio-fortio-ingress--71cb3990c3cf10a6
k8s-fw-fortio-fortio-ingress2--6c90c323237f439e            35.190.69.114   TCP          k8s-tp-fortio-fortio-ingress2--6c90c323237f439e
```

I would expect the gce ingress controller to remove these resources when the ingress or cluster is deleted.",open,False,2018-02-15 18:37:41,2018-11-30 17:24:56
ingress-gce,mattdodge,https://github.com/kubernetes/ingress-gce/issues/137,https://api.github.com/repos/kubernetes/ingress-gce/issues/137,Multiple Healthcheck Requests from GCP L7,"TL;DR: For the healthchecks created by the L7 load balancer I am seeing 6 requests, all at once, from each node in my cluster. Would expect to see 1 request per node.

I have an ingress set up to use the GCE ingress controller. It targets multiple services but each behaves the same so I will just use one example. The service's pod is configured with an http readiness probe configured to do a health check every 10 seconds.
```
readinessProbe:
  httpGet:
    path: /healthcheck
    port: 5000
  timeoutSeconds: 5
  periodSeconds: 10
```

The GCP Health Check (https://console.cloud.google.com/compute/healthChecks) gets created (although with an interval of 70 seconds, not sure how to make that match the ingress config).

Every 10 seconds I see the kubelet of the node the pod is running on make a health check request to the pod. This is working as expected.

However, every 70 seconds I am seeing **six (6)** health check requests come from every other node in the cluster. So a cluster with 3 nodes is sending 18 health check requests every 70 seconds.

I have attached a log showing the requests come in if that's helpful. [[health_check.log](https://github.com/kubernetes/ingress-gce/files/1733855/health_check.log)] You can see at the 16:30:57 mark I add 3 more nodes to the cluster resulting in 18 more requests coming through (nodes `10.138.0.[5,6,7]`). Also note, the ingress period for this log is actually 25 seconds and GCP health check period is 35 seconds, I was tweaking values during testing.

Main question: how can I get the health checks from GCP to only send one health check every interval? Or maybe, what is the reason for sending 6?
Secondary question: can I configure the GCP health check interval from the ingress configuration?",closed,False,2018-02-17 16:48:36,2018-02-20 18:38:23
ingress-gce,caarlos0,https://github.com/kubernetes/ingress-gce/issues/138,https://api.github.com/repos/kubernetes/ingress-gce/issues/138,"""internal"" ingresses","Hey!

Sorry if this is not the right place to ask or if I'm missing something, but, I wonder how can I create ingress with ""internal"" IPs (visible only inside the VPC)?

As far as I looked into, it only creates ingress with addresses like 35.x.x.x. I tried internal load balancers as well, which work, but there is 5 LB limit and I need **a lot** more than that.

Thanks!",closed,False,2018-02-23 19:46:05,2018-02-27 17:52:34
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/139,https://api.github.com/repos/kubernetes/ingress-gce/issues/139,fix event message for attach/detach NEs,,closed,True,2018-02-27 00:21:14,2018-02-27 17:50:13
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/140,https://api.github.com/repos/kubernetes/ingress-gce/issues/140,Add instructions and a tool for people who want to try out a new version of the ingress controller before it is released.,"See title

/assign @nicksardo ",closed,True,2018-03-02 21:48:51,2018-03-14 21:19:39
ingress-gce,muzuro,https://github.com/kubernetes/ingress-gce/issues/141,https://api.github.com/repos/kubernetes/ingress-gce/issues/141,"REST request fails with ""The server encountered a temporary error and could not complete your request. Please try again in 30 seconds""","I am deploying application at google kubernetes engine. Applicaion has 2 services. There is also `Ingress` wich i am trying to use to expose one service and `ingress` also used for https support. I have 1 `NodePort` service ""gateway"" and `ClusterIp` service ""internal"". ""Internal"" should be accessed from gateway. Here is services config:

```
	apiVersion: extensions/v1beta1
	kind: Ingress
	metadata:
	  name: x-ingress
	  annotations:
		kubernetes.io/ingress.global-static-ip-name: x-x-ip
		kubernetes.io/tls-acme: ""true""
	  labels:
		app: gateway
	spec:
	  tls:
		- secretName: secret
		  hosts:
		  - x.x.com
	  backend:
		serviceName: gateway
		servicePort: 80
	---
	apiVersion: certmanager.k8s.io/v1alpha1
	kind: ClusterIssuer
	metadata:
	  name: x-x
	spec:
	  acme:
		server: https://acme-v01.api.letsencrypt.org/directory
		email: x@x.com
		privateKeySecretRef:
		  name: x-x
		http01: {}
	---
	apiVersion: v1
	kind: Service
	metadata:
	  annotations:
		service.alpha.kubernetes.io/tolerate-unready-endpoints: ""true""
	  name: gateway
	  labels:
		app: gateway
	spec:
	  type: NodePort
	  ports:
	  - port: 80
		name: gateway
		targetPort: 8080
	  selector:
		app: gateway
	---
	apiVersion: v1
	kind: Service
	metadata:
	  annotations:
		service.alpha.kubernetes.io/tolerate-unready-endpoints: ""true""
	  name: internal
	  labels:
		app: internal
	spec:
	  ports:
	  - port: 8082
		name: internal
		targetPort: 8082
	  selector:
		app: internal
```

Gateway serve static content and REST resources. Static content is served ok, so i see html and images and scripts. But when i try to call REST endpoint i got 502 error with text:  `The server encountered a temporary error and could not complete your request. Please try again in 30 seconds.` Gateway forward REST request to ""internal"" service and return response from internal. Gateway access internal service with url `http://internal:8082/some/rest`. I got errors when i call any request wich should be forwarded to ""internal"".
Actualy i have same scheme without `Ingress` and it works. ""Gateway"" is `LoadBalancer` service and ""internal"" is `NodePort`. I need `Ingress` for https.

What i have tried(without success):
I noticed i don't have any forwading rules related to `8082` port, only `80` and `443`( i have used `gcloud compute forwarding-rules list` and `gcloud compute forwarding-rules describe` commands). I have added them with console cloud:
![image](https://user-images.githubusercontent.com/8950484/36937702-76415dfa-1f28-11e8-89c4-af1f4f163d22.png)
I have also found information about health check - so i have added ""/"" endpoints in each service with 200 response.

Here is output of `kubectl describe svc`
  
```
	Name:                     gateway
	Namespace:                default
	Labels:                   app=gateway
	Annotations:              service.alpha.kubernetes.io/tolerate-unready-endpoints=true
	Selector:                 app=gateway
	Type:                     NodePort
	IP:                       *
	Port:                     gateway  80/TCP
	TargetPort:               8080/TCP
	NodePort:                 gateway  31168/TCP
	Endpoints:                *:8080
	Session Affinity:         None
	External Traffic Policy:  Cluster
	Events:                   <none>
    ---
	Name:              internal
	Namespace:         default
	Labels:            app=internal
	Annotations:       service.alpha.kubernetes.io/tolerate-unready-endpoints=true
	Selector:          app=internal
	Type:              ClusterIP
	IP:                *
	Port:              internal  8082/TCP
	TargetPort:        8082/TCP
	Endpoints:         *:8082
	Session Affinity:  None
	Events:            <none>
```",closed,False,2018-03-03 18:20:52,2018-07-03 15:20:00
ingress-gce,prameshj,https://github.com/kubernetes/ingress-gce/pull/142,https://api.github.com/repos/kubernetes/ingress-gce/issues/142,Support for multiple tls certificates,"Currently, the api accepts multiple tls secrets, but only the first one is used. This change supports multiple tls secrets as well as multiple preshared certs specified as a comma-separated annotation string.",closed,True,2018-03-06 20:29:39,2018-04-06 20:45:12
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/143,https://api.github.com/repos/kubernetes/ingress-gce/issues/143,Consider Emitting Events for GC errors,"GC should record event when error occurs. Some GC problem needs user attention to fix. For instance, if user reference resources created by ingress controller from other manual configured resources. These resources could not be GCed. ingress controller will end up GC it forever and logging on the master while user gets zero signal to about the problem. 

",open,False,2018-03-07 00:21:19,2018-10-31 19:05:38
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/pull/144,https://api.github.com/repos/kubernetes/ingress-gce/issues/144,Adding information about using GCP SSL certs for frontend HTTPS,"Updating the documentation so that it clearly states the 2 options that users have to store their certs: as k8s secrets or as GCP SSL certs.

cc @csbell @G-Harmon @nicksardo @bowei ",closed,True,2018-03-07 09:46:11,2018-03-07 17:03:34
ingress-gce,paOol,https://github.com/kubernetes/ingress-gce/issues/145,https://api.github.com/repos/kubernetes/ingress-gce/issues/145,sitemap endpoint?,"when i `docker-compose up` and run my app locally, I am able to go to 
`/sitemap.xml`.
However, when I push the changes to GCE and visit the same endpoint, I try to go to 

`/sitemap.xml` and instead get sent to  `api/somethingelse/sitemap.xml`.

how do I get the correct endpoint to load?

This is my ingress file.

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: example-ingress
  namespace: default
  annotations:
    kubernetes.io/tls-acme: ""true""
    kubernetes.io/ingress.class: ""gce""
    kubernetes.io/ingress.global-static-ip-name: example-ip
spec:
  tls:
  - hosts:
    - example.com
    secretName: example-tls
  backend:
    serviceName: example
    servicePort: 80
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: example
          servicePort: 80
```",closed,False,2018-03-08 06:04:43,2018-03-15 19:38:23
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/146,https://api.github.com/repos/kubernetes/ingress-gce/issues/146,Ingress HTTP/2 Support,"- Adds HTTP2 annotation to Service
- Creates alpha HealthCheck and BackendService resources when the protocol is HTTP/2, by using a composite BackendService type
- Tests for HealthCheck/BackendServices using HTTP/2",closed,True,2018-03-08 18:52:00,2018-04-09 21:25:36
ingress-gce,G-Harmon,https://github.com/kubernetes/ingress-gce/pull/147,https://api.github.com/repos/kubernetes/ingress-gce/issues/147,update: s_k/ingress-k/ingress-gce_ in annotations.md,cc @bowei @nikhiljindal ,closed,True,2018-03-10 01:14:11,2018-03-13 15:21:10
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/148,https://api.github.com/repos/kubernetes/ingress-gce/issues/148,Initial implementation for ingress rate limiting,"This PR adds support for rate limiting certain GCE API call's that are made by the controller. Currently, the cloud provider layer does not rate limit any standard controller calls (such as BackendService Get). As a result, in some cases when API calls are frequent, we are blowing through project quota and getting rate limited on the GCE side. This could also cause the controller to slow down. By allowing for customization of what calls can be rate limited, we can prevent blowing through project quota and we can intelligently rate limit so that we do not experience slowdown.  

Will add unit tests once the overall approach is LGTM'd.

/assign @nicksardo @bowei ",closed,True,2018-03-13 03:47:05,2018-03-16 18:18:34
ingress-gce,rangapv,https://github.com/kubernetes/ingress-gce/issues/149,https://api.github.com/repos/kubernetes/ingress-gce/issues/149,ingress-gce does not work...very little documentation ..instructions lead to 404 pages,"Hi,
I am trying to get external load balancer on GCE.
My cluster is kubernetes 1.9.3 (kubeadm)...

I am trying to install the ingress-gce from github...the documentation is very confusing and also there is no right pointers...

I started from here...

https://github.com/kubernetes/ingress-gce

Then I just downloaded the rc.yaml and the app.yaml did a kubectl then it would not start the loadbalancer pod bocs the liveliness probe was failing then I changed the rc.yaml where the liveliness probe was doing a healtcheck on the loadbalncer pod on port 8081 to 8080..now the pod seems to be running...

Now how will the backend loadbalancer in the GCP get created ? how will the fowarding rules get created ..?

I also tried the compiling of the binary(very cumbersome process ..no documentation) though I manged it.
 Since the rc.yaml is pulling the image do I still need to compile the binary ? or is it just an option ?

What are the scopes that is required for the instances so that it goes and creates the LoadBalancer and its rules ..?

This gives me 404....

https://github.com/kubernetes/ingress-gce/blob/master/examples/deployment/gce/README.md


Can you brief me about correct steps...

REgards
Ranga",closed,False,2018-03-14 08:16:08,2018-05-04 22:56:11
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/150,https://api.github.com/repos/kubernetes/ingress-gce/issues/150,Cleanup some unused files,"This PR removes:

1. tests/manifests - Not being used
2. rc.yaml, ingress-app.yaml - We have sufficient examples in examples/ so these do not need to exist anymore.

/assign @nicksardo ",closed,True,2018-03-14 20:52:22,2018-07-12 16:22:38
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/151,https://api.github.com/repos/kubernetes/ingress-gce/issues/151,Fix copyright in deploy/glbc/script.sh,/assign @nicksardo ,closed,True,2018-03-14 21:22:52,2018-03-14 21:24:39
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/152,https://api.github.com/repos/kubernetes/ingress-gce/issues/152,ingress-gce major refactor and performance fixes,"Using this issue to track launch of ingress-gce 1.0.0

The following fixes are planned for the 1.0.0 release:

1. Large refactor to fix scalability of controller. 
2. Implementation of rate limiting for GCE calls.",closed,False,2018-03-16 22:55:04,2018-03-16 23:17:03
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/153,https://api.github.com/repos/kubernetes/ingress-gce/issues/153,Fix formatting error in docs/README.md,/assign @nicksardo ,closed,True,2018-03-19 16:38:48,2018-03-19 16:57:39
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/154,https://api.github.com/repos/kubernetes/ingress-gce/issues/154,Duplicate patch from PR #148 into main repo.,"PR #148 made a raw patch in vendor/k8s.io/kubernetes/pkg/cloudprovider/. Using this issue to track that a corresponding patch is made in the main kubernetes repo.

/assign ",closed,False,2018-03-19 17:49:49,2018-03-26 23:02:50
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/issues/155,https://api.github.com/repos/kubernetes/ingress-gce/issues/155,`dep ensure` is broken,"In the middle of updating vendor, found `dep ensure` is already broken without any modification to codes:
```
k8s.io/ingress-gce$ dep ensure -v
Warning: the following project(s) have [[constraint]] stanzas in Gopkg.toml:
                                                                                  
  ✗  github.com/docker/distribution            
  ✗  github.com/opencontainers/go-digest                 
  ✗  github.com/spf13/pflag                                                        
                                                
However, these projects are not direct dependencies of the current project:
they are not imported in any .go files, nor are they in the 'required' list in        
Gopkg.toml. Dep only applies [[constraint]] rules to direct dependencies, so         
these rules will have no effect.                                                               
                                                                           
Either import/require packages from these projects so that they become direct
dependencies, or convert each [[constraint]] to an [[override]] to enforce rules
on these projects, if they happen to be transitive dependencies,        
                                     
Root project is ""k8s.io/ingress-gce""
 23 transitively valid internal packages
 41 external packages imported from 8 projects
(0)   ✓ select (root)
(1)     ? attempt github.com/golang/glog with 1 pkgs; at least 1 versions to try
(1)         try github.com/golang/glog@23def4e6c14b4da8ac2ed8007337bc5eb5007998
(1)     ✓ select github.com/golang/glog@23def4e6c14b4da8ac2ed8007337bc5eb5007998 w/1 pkgs
(2)     ? attempt github.com/gorilla/websocket with 1 pkgs; at least 1 versions to try
(2)         try github.com/gorilla/websocket@v1.2.0
(2)     ✓ select github.com/gorilla/websocket@v1.2.0 w/1 pkgs
(3)     ? attempt github.com/prometheus/client_golang with 1 pkgs; at least 1 versions to try
(3)         try github.com/prometheus/client_golang@v0.8.0
(3)     ✓ select github.com/prometheus/client_golang@v0.8.0 w/2 pkgs
(4)     ? attempt github.com/beorn7/perks with 1 pkgs; at least 1 versions to try
(4)         try github.com/beorn7/perks@master
(4)     ✓ select github.com/beorn7/perks@master w/1 pkgs
(5)     ? attempt github.com/prometheus/client_model with 1 pkgs; at least 1 versions to try
(5)         try github.com/prometheus/client_model@master
(5)     ✓ select github.com/prometheus/client_model@master w/1 pkgs
(6)     ? attempt github.com/golang/protobuf with 1 pkgs; at least 1 versions to try
(6)         try github.com/golang/protobuf@master
(6)     ✓ select github.com/golang/protobuf@master w/1 pkgs
(7)     ? attempt github.com/prometheus/common with 2 pkgs; at least 1 versions to try
(7)         try github.com/prometheus/common@master
(7)     ✓ select github.com/prometheus/common@master w/3 pkgs
(8)     ? attempt github.com/matttproud/golang_protobuf_extensions with 1 pkgs; at least 1 versions to try
(8)         try github.com/matttproud/golang_protobuf_extensions@v1.0.0
(8)     ✓ select github.com/matttproud/golang_protobuf_extensions@v1.0.0 w/1 pkgs
(9)     ? attempt k8s.io/kubernetes with 6 pkgs; at least 1 versions to try
(9)         try k8s.io/kubernetes@7652c252d4ce2aea1563853bef0438769c5781fd
(9)     ✓ select k8s.io/kubernetes@7652c252d4ce2aea1563853bef0438769c5781fd w/41 pkgs
(10)  ? revisit github.com/prometheus/client_golang to add 1 pkgs
(10)    ✓ include 1 more pkgs from github.com/prometheus/client_golang@v0.8.0
(10)  ? attempt cloud.google.com/go with 1 pkgs; at least 1 versions to try
(11)      try cloud.google.com/go@v0.17.0
(11)  ✓ select cloud.google.com/go@v0.17.0 w/1 pkgs
(11)  ? attempt github.com/davecgh/go-spew with 1 pkgs; at least 1 versions to try
(12)      try github.com/davecgh/go-spew@v1.1.0
(12)  ✓ select github.com/davecgh/go-spew@v1.1.0 w/1 pkgs
(12)  ? attempt github.com/dgrijalva/jwt-go with 1 pkgs; at least 1 versions to try
(13)      try github.com/dgrijalva/jwt-go@v3.1.0
(13)  ✓ select github.com/dgrijalva/jwt-go@v3.1.0 w/1 pkgs
(13)  ? attempt github.com/docker/distribution with 1 pkgs; at least 1 versions to try
(14)      try github.com/docker/distribution@edc3ab29cdff8694dd6feb85cfeb4b5f1b38ed9c
(14)  ✓ select github.com/docker/distribution@edc3ab29cdff8694dd6feb85cfeb4b5f1b38ed9c w/2 pkgs
(14)  ? attempt golang.org/x/oauth2 with 2 pkgs; at least 1 versions to try
(15)      try golang.org/x/oauth2@master
(15)  ✓ select golang.org/x/oauth2@master w/5 pkgs
(15)  ? attempt golang.org/x/net with 2 pkgs; at least 1 versions to try
(16)      try golang.org/x/net@master
(16)  ✓ select golang.org/x/net@master w/2 pkgs
(16)  ? attempt github.com/prometheus/procfs with 1 pkgs; at least 1 versions to try
(17)      try github.com/prometheus/procfs@master
(17)  ✓ select github.com/prometheus/procfs@master w/2 pkgs
(17)  ? attempt github.com/golang/groupcache with 1 pkgs; at least 1 versions to try
(18)      try github.com/golang/groupcache@master
(18)  ✓ select github.com/golang/groupcache@master w/1 pkgs
(18)  ? attempt google.golang.org/api with 2 pkgs; at least 1 versions to try
(19)      try google.golang.org/api@373a4c220f5c90e5b7ff7101779c5be385d171be
(19)  ✓ select google.golang.org/api@373a4c220f5c90e5b7ff7101779c5be385d171be w/5 pkgs
(20)    ← no more versions of k8s.io/apiextensions-apiserver to try; begin backtrack
(19)  ← backtrack: no more versions of google.golang.org/api to try
(18)  ← backtrack: no more versions of github.com/golang/groupcache to try
(17)  ← backtrack: no more versions of github.com/prometheus/procfs to try
(16)  ← backtrack: no more versions of golang.org/x/net to try
(15)  ← backtrack: no more versions of golang.org/x/oauth2 to try
(14)  ← backtrack: no more versions of github.com/docker/distribution to try
(13)  ← backtrack: no more versions of github.com/dgrijalva/jwt-go to try
(12)  ← backtrack: no more versions of github.com/davecgh/go-spew to try
(11)  ← backtrack: no more versions of cloud.google.com/go to try
(10)  ← backtrack: popped 1 pkgs from github.com/prometheus/client_golang
(9)     ← backtrack: no more versions of k8s.io/kubernetes to try
(8)     ← backtrack: no more versions of github.com/matttproud/golang_protobuf_extensions to try
(7)     ← backtrack: no more versions of github.com/prometheus/common to try
(6)     ← backtrack: no more versions of github.com/golang/protobuf to try
(5)     ← backtrack: no more versions of github.com/prometheus/client_model to try
(4)     ← backtrack: no more versions of github.com/beorn7/perks to try
(3)     ← backtrack: no more versions of github.com/prometheus/client_golang to try
(2)     ← backtrack: no more versions of github.com/gorilla/websocket to try
(1)     ← backtrack: no more versions of github.com/golang/glog to try
  ✗ solving failed

Solver wall times by segment:
     b-source-exists: 8.563369086s
         b-list-pkgs: 4.526788677s
              b-gmal: 1.783593559s
  b-deduce-proj-root: 1.168543572s
         select-atom: 140.675344ms
            unselect:  132.55547ms
             satisfy: 120.609012ms
         select-root:   2.532885ms
            new-atom:   1.225076ms
           backtrack:    251.339µs
               other:     60.042µs
            add-atom:      3.987µs

  TOTAL: 16.440208049s

Solving failure: exit status 128
```

/assign",closed,False,2018-03-19 18:22:02,2018-03-21 17:14:03
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/156,https://api.github.com/repos/kubernetes/ingress-gce/issues/156,Ingress Controller Clobbers Backend Service,"Symptom: 
- Only 1 ingress is present. But a total of 3 LBs visibile in the GCP Pantheon UI in project. (2 Manual, 1 managed by ingress )
- Ingress controller constantly deleted and recreated a BackendService. Wiping out the existing configs
- The backend service is referenced by the existing ingress.
- Ingress controller tried to delete a TLS cert, but it was referenced by another LB set up manually. 

The setup was lost. The symptom seemed to stop after TLS cert is removed. (Not sure)

I suspect some combination of ingress config changes, failed GC and customer setup (enable IAP?) caused conflicts between GC and Checkpoint. Hence leading to delete/create backend-service.

",closed,False,2018-03-19 18:39:09,2018-09-15 19:21:50
ingress-gce,maxiplay,https://github.com/kubernetes/ingress-gce/issues/157,https://api.github.com/repos/kubernetes/ingress-gce/issues/157,"Seems ingress don't transfer the ""Transfer-encoding"" Header from backend","I tried to use Google cloud cdn with my container engine configuration. When backend respond with GZIP and Transfer-encoding : chunked it  ingress don't transfer the ""Transfer-encoding"" Header from backend so I can't have a cache hit.

When the content is not Gzipped i have the ""Content-Length"" header and a cache hit success.

I linked the issue to stackoverflo question, https://stackoverflow.com/questions/49297244/impossible-to-use-google-cloud-cdn-cache-with-ingress-controller-transfer-encod?noredirect=1#comment85706992_49297244

apiVersion: extensions/v1beta1",closed,False,2018-03-19 22:20:23,2018-04-27 21:37:23
ingress-gce,jonyhy96,https://github.com/kubernetes/ingress-gce/pull/158,https://api.github.com/repos/kubernetes/ingress-gce/issues/158,fix some grammar mistakes,"fix some grammar mistakes.
```release-note
None
```",closed,True,2018-03-20 01:21:54,2018-03-20 06:19:42
ingress-gce,jonyhy96,https://github.com/kubernetes/ingress-gce/pull/159,https://api.github.com/repos/kubernetes/ingress-gce/issues/159,fix some grammar mistakes,"fix some gramar mistakes.
```release-note
None
```",closed,True,2018-03-20 06:20:40,2018-03-20 06:21:21
ingress-gce,jonyhy96,https://github.com/kubernetes/ingress-gce/pull/160,https://api.github.com/repos/kubernetes/ingress-gce/issues/160,fix some grammar mistakes,"fix some gramar mistakes.
```release-note
None
```",closed,True,2018-03-20 06:24:34,2018-03-25 04:45:20
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/161,https://api.github.com/repos/kubernetes/ingress-gce/issues/161,Add Ingress HTTP2 feature gate,Feature gate for https://github.com/kubernetes/ingress-gce/pull/146,closed,True,2018-03-20 18:27:06,2019-02-06 19:54:05
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/issues/162,https://api.github.com/repos/kubernetes/ingress-gce/issues/162,Missing ListUrlMaps from LoadBalancers interface,"I see that ListUrlMaps is missing from LoadBalancers interface at: https://github.com/kubernetes/ingress-gce/blob/master/pkg/loadbalancers/interfaces.go#L37.

I need it for kubemci as part of work for https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress/issues/145.

Any objections to me sending a PR to add it?

cc @nicksardo @bowei ",closed,False,2018-03-21 21:32:30,2018-03-22 23:41:07
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/163,https://api.github.com/repos/kubernetes/ingress-gce/issues/163,Add ServiceExtension CRD lifecycle management and empty spec definition,"Initial PR for ServiceExtension implementation. This hasn't touched any GLBC workflow. Just added CRD create/update logic and an empty spec definition & code-gen stuffs. New code is gated behind a flag.

NOTE: Code-gen scripts are derived from https://github.com/kubernetes/sample-controller. Longer-term we might want https://github.com/kubernetes-sigs/kubebuilder, but would like to get this in first.

P.S. `dep ensure -update` seems to pull a lot more things into vendor, not sure if I did it right. Dep version is v0.4.1.

@bowei @nicksardo @rramkumar1 @G-Harmon @nikhiljindal ",closed,True,2018-03-21 21:35:39,2018-04-11 21:49:46
ingress-gce,gmflau,https://github.com/kubernetes/ingress-gce/issues/164,https://api.github.com/repos/kubernetes/ingress-gce/issues/164,502 Server Error,"I managed to use External Load Balancer to allow traffic to go from public Internet to my application over HTTP (port 8888) [http://\<External-IP-from-LB\>:8888].  However, I failed to access the same application through NodePort and Ingress Controller [http://\<External-IP-from-Ingress\>/opsc].  It returned a page saying:

Error: Server Error
The server encountered a temporary error and could not complete your request.
Please try again in 30 seconds.

Here are the yamls to set up my environment:

```
apiVersion: v1
kind: Service
metadata:
  name: opscenter-nodeport
  labels:
    app: opscenter
spec:
  type: NodePort
  ports:
    - port: 8888
      name: opsc-gui-port
  selector:
    app: opscenter
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
  - http:
      paths:
      - path: /opsc
        backend:
          serviceName: opscenter-nodeport
          servicePort: 8888
```

Any idea to share what could go wrong?  Any idea to troubleshoot the problem?",closed,False,2018-03-22 04:30:28,2018-05-04 21:59:31
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/pull/165,https://api.github.com/repos/kubernetes/ingress-gce/issues/165,Adding ListUrlMaps to LoadBalancers interface,"Fixes https://github.com/kubernetes/ingress-gce/issues/162

cc @nicksardo @G-Harmon ",closed,True,2018-03-22 23:38:52,2018-03-22 23:41:07
ingress-gce,sunlintong,https://github.com/kubernetes/ingress-gce/pull/166,https://api.github.com/repos/kubernetes/ingress-gce/issues/166, reduce link jump,I think it's more convenient.,closed,True,2018-03-23 03:42:19,2018-03-23 19:37:09
ingress-gce,jonyhy96,https://github.com/kubernetes/ingress-gce/pull/167,https://api.github.com/repos/kubernetes/ingress-gce/issues/167,change rule to rules,"change rule to rules
```release-note
None
```",closed,True,2018-03-25 04:52:47,2018-03-28 06:11:20
ingress-gce,AdamDang,https://github.com/kubernetes/ingress-gce/pull/168,https://api.github.com/repos/kubernetes/ingress-gce/issues/168,Update testing.md,"Delete a duplicated ""that"" in line 44.",closed,True,2018-03-25 12:33:46,2018-04-01 06:50:30
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/pull/169,https://api.github.com/repos/kubernetes/ingress-gce/issues/169,Use given name rather than regenerating it in UrlMap fake,"This is what we do for all other resources like forwarding rule, target proxies, etc.
UrlMap is the only resource for which we use namer again.
Fixing this inconsistency.

Found while writing a test for https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress/issues/145 for kubemci. We pass a nil namer while instantiating FakeLoadBalancers in our tests.

cc @bowei @nicksardo @G-Harmon ",closed,True,2018-03-26 22:23:29,2018-03-26 22:55:00
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/170,https://api.github.com/repos/kubernetes/ingress-gce/issues/170,Checkpoint() takes a single LB rather than a list of LBs,,closed,True,2018-03-27 05:57:55,2018-06-10 18:06:52
ingress-gce,jeremymclain,https://github.com/kubernetes/ingress-gce/pull/171,https://api.github.com/repos/kubernetes/ingress-gce/issues/171,fixed backside https example link,,closed,True,2018-03-27 23:37:02,2018-04-16 18:05:49
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/172,https://api.github.com/repos/kubernetes/ingress-gce/issues/172,Bake ServiceExtension into GLBC workflow,"PR depends on https://github.com/kubernetes/ingress-gce/pull/163 (thinking of not distracting folks). Please only look at the last commit if you'd like to take a glance before that is merged.

This makes ServiceExtension create/update/delete trigger ingress sync. Also, the corresponding ServiceExtension will be save in `backends.ServicePort` if a service references it. This doesn't have actual impact on LB configuration yet.

Logic is gated behind a flag.

/hold
",closed,True,2018-03-28 03:59:15,2018-03-30 21:54:48
ingress-gce,sunlintong,https://github.com/kubernetes/ingress-gce/pull/173,https://api.github.com/repos/kubernetes/ingress-gce/issues/173,correct spell error,,closed,True,2018-03-28 06:11:12,2018-03-28 16:36:44
ingress-gce,jonyhy96,https://github.com/kubernetes/ingress-gce/pull/174,https://api.github.com/repos/kubernetes/ingress-gce/issues/174,fix comment mistake,"fix comment mistake
```release-note
None
```",closed,True,2018-03-28 06:13:48,2018-03-28 16:36:56
ingress-gce,sunlintong,https://github.com/kubernetes/ingress-gce/pull/175,https://api.github.com/repos/kubernetes/ingress-gce/issues/175,behaviour --> behavior,"Maybe it's not an error, but it makes me not so comfortable.  :P",closed,True,2018-03-28 06:23:00,2018-03-28 16:38:42
ingress-gce,sunlintong,https://github.com/kubernetes/ingress-gce/pull/176,https://api.github.com/repos/kubernetes/ingress-gce/issues/176,Update testing.md,spell mistake,closed,True,2018-03-28 06:29:40,2018-03-28 16:38:54
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/177,https://api.github.com/repos/kubernetes/ingress-gce/issues/177,Minor spelling and capitalization changes," - Fixes capitalizations of initialisms
 - Fixes spelling mistakes
 - References `kubemci` instead of federation.
 - Deletes some documentation that only make sense in the ingress-nginx repo.",closed,True,2018-03-28 16:19:18,2018-03-28 16:36:19
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/pull/178,https://api.github.com/repos/kubernetes/ingress-gce/issues/178,Updating FakeLoadBalancers.Delete to return NotFound when appropriate,"Ref https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress/issues/155

Fakes should return NotFound as the real code does.
This will help us catch issues like https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress/issues/155 in our unit tests.

cc @G-Harmon @nicksardo @bowei @MrHohn ",closed,True,2018-03-28 19:59:01,2018-03-28 21:35:22
ingress-gce,sht5,https://github.com/kubernetes/ingress-gce/issues/179,https://api.github.com/repos/kubernetes/ingress-gce/issues/179,original http request origin and host headers are overriden,"when my client makes an http request to the Ingress domain  i see that the request origin and host headers are sent correctly but on my backend i can see they are changed by Ingress to some internal ip. This is crucial for setting up CORS for my app ( checks domain and alters response headers accordingly).
",closed,False,2018-03-29 06:46:23,2019-02-18 21:33:09
ingress-gce,atotto,https://github.com/kubernetes/ingress-gce/pull/180,https://api.github.com/repos/kubernetes/ingress-gce/issues/180,Fix path default-http-backend.yaml,"I get the following error:

```
sed: can't read default-http-backend.yaml: No such file or directory
```

Thanks.",closed,True,2018-03-29 09:09:07,2018-03-30 00:09:05
ingress-gce,tvainika,https://github.com/kubernetes/ingress-gce/issues/181,https://api.github.com/repos/kubernetes/ingress-gce/issues/181,Unclear documentation of spec.rules.http.paths,"Documentation of `spec.rules.http.paths` is unclear regarding, regexp versus wildcard, and multiple matches priority.

According [Kubernetes documentation](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#httpingresspath-v1beta1-extensions), `http.paths` should be regular expression. However when testing it on my GCE, it clearly seems to be wildcard match like all examples when looking at GCE load balancer [documentation](https://cloud.google.com/compute/docs/load-balancing/http/url-map).

Second, I did not find anywhere explanation what happens if there is multiple matching path configurations. Which one will be selected? The one that is listed first or last in my `Ingress.yaml`, or something else?",open,False,2018-03-29 10:43:25,2019-02-04 07:16:36
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/issues/182,https://api.github.com/repos/kubernetes/ingress-gce/issues/182,ingress controller should only manage instance groups for multicluster ingress,"Forked from https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress/issues/131

ingress-gce controller is configuring the whole load balancer including creating url map, target proxy, etc for multicluster ingresses.
This is a regression. It should only manage instance groups.

Thanks to @MrHohn for finding the problem.
@nicksardo is working on a fix and I am going to send a PR to update the ingress-gce tests to catch this early.

Filing this issue to track

cc @csbell @G-Harmon @bowei ",closed,False,2018-03-29 19:19:29,2018-03-31 22:45:56
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/183,https://api.github.com/repos/kubernetes/ingress-gce/issues/183,Fix sync of multi-cluster ingress,"Fixes #182
Also fixes an pre-existing issue of keeping around backend services if MCI and normal ingresses shared backend services. If the normal ingress was deleted, the backend service wouldn't be GC'd until MCI was deleted.

The logic for syncing MCI vs GCE-ingress was not easily distinguishable. I've re-ordered the sync logic and broke down `Checkpoint` into something I think is more clear. 
Checkpoint is broken up into three separate funcs and are called in the following order:
1. `EnsureInstanceGroupsAndPorts()`
    - MCI updates annotations and early returns.
2. `EnsureLoadBalancer()`
3. `EnsureFirewall()`

cc @nikhiljindal @csbell @bowei  @MrHohn  @rramkumar1 ",closed,True,2018-03-29 19:24:53,2018-03-31 00:36:27
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/issues/184,https://api.github.com/repos/kubernetes/ingress-gce/issues/184,Condense backend pool with default backend pool,Maintain one backend pool instead of having two.,closed,False,2018-03-30 19:47:06,2018-05-10 20:51:09
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/185,https://api.github.com/repos/kubernetes/ingress-gce/issues/185,Test Failing: [sig-network] Loadbalancing: L7 GCE [Slow] [Feature:Ingress] multicluster ingress should get instance group annotation,"https://k8s-testgrid.appspot.com/sig-network-gce#ingress-gce-e2e&width=5

probably due to this: https://github.com/kubernetes/ingress-gce/pull/183

",closed,False,2018-04-02 19:58:45,2018-04-03 04:38:08
ingress-gce,nikhiljindal,https://github.com/kubernetes/ingress-gce/issues/186,https://api.github.com/repos/kubernetes/ingress-gce/issues/186,Controller doesnt reenque on instance group GC failure,"Found while debugging kubemci failures.

Steps to repro:
* Create an MCI ingress using kubemci
* Delete the MCI using kubemci CLI

Expected: All GCP resources are deleted

Actual: Instance group is not deleted

Cause:
In kubemci we first delete the ingress and then delete the GCP resources (including backend service).
When ingress is deleted from cluster, ingress-gce controller tries to delete the corresponding instance group, but it fails since the backend service is still referring it.

Proposed fix:
* For now, I will update kubemci to delete GCP resources first so that the backend service is deleted by the time we delete the ingress.
* ingress-gce controller should be fixed to retry the deletion.

cc @bowei @nicksardo @MrHohn  @freehan ",closed,False,2018-04-03 04:47:18,2018-04-27 21:30:34
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/187,https://api.github.com/repos/kubernetes/ingress-gce/issues/187,Cherry-pick checkpoint changes to 1.0,"Fixes MCI sync.

",closed,True,2018-04-03 17:57:42,2018-04-03 17:59:30
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/188,https://api.github.com/repos/kubernetes/ingress-gce/issues/188,Add some documentation on post-release TODO's,"This is mainly for keeping track of things we need to do when a new release is cut. 

/assign @nikhiljindal  ",closed,True,2018-04-04 16:06:08,2018-04-04 23:56:19
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/189,https://api.github.com/repos/kubernetes/ingress-gce/issues/189,Satisfy golang 1.10 vetting,,closed,True,2018-04-05 16:55:04,2018-04-05 18:06:24
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/190,https://api.github.com/repos/kubernetes/ingress-gce/issues/190,Update glbc manifest to v1.0.1,,closed,True,2018-04-05 17:27:33,2018-04-05 17:54:20
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/191,https://api.github.com/repos/kubernetes/ingress-gce/issues/191,Small changes to deploy/glbc/README,/assign @nicksardo ,closed,True,2018-04-05 17:47:55,2018-04-05 17:54:53
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/192,https://api.github.com/repos/kubernetes/ingress-gce/issues/192,Bump golang build image to 1.10,,closed,True,2018-04-05 18:09:43,2018-04-05 18:25:58
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/193,https://api.github.com/repos/kubernetes/ingress-gce/issues/193,Update vendor/ to support multiple TLS certificates interface,This updates all of the vendor to support multple TLS certificates.,closed,True,2018-04-06 07:53:23,2018-06-10 18:04:52
ingress-gce,mahuihuang,https://github.com/kubernetes/ingress-gce/pull/194,https://api.github.com/repos/kubernetes/ingress-gce/issues/194,Fix grammar mistake,Fix grammar mistake,closed,True,2018-04-06 08:50:30,2018-05-09 19:09:46
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/195,https://api.github.com/repos/kubernetes/ingress-gce/issues/195,Support for multiple TLS certificates,Ref: #142,closed,True,2018-04-06 18:32:55,2018-05-21 17:39:12
ingress-gce,m1kola,https://github.com/kubernetes/ingress-gce/pull/196,https://api.github.com/repos/kubernetes/ingress-gce/issues/196,Fixes a typo in BETA_LIMITATIONS.md,,closed,True,2018-04-07 20:23:36,2018-04-16 18:05:22
ingress-gce,jody-frankowski,https://github.com/kubernetes/ingress-gce/issues/197,https://api.github.com/repos/kubernetes/ingress-gce/issues/197,Failing to create an Ingress shouldn't stop the controller from creating the others,"(Not sure if that is the right place to fill this issue :smiley:)
It looks like the controller doesn't try to create other Ingress object when it fails for one.

See [this issue](https://github.com/jenkins-x/jx/issues/599) for the reproduction steps.
In a nutshell: Ingresses with non-NodePort services break proper Ingresses with NodePort services.

Also (not specified in the other issue) here are the ingresses that break ingress-gce (note the `ClusterIP` type):
```
kubectl get svc
NAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE
heapster                        ClusterIP   10.15.241.123   <none>        8082/TCP    4d
[...]
```

Two things bug me:
* ingress-gce seems to break all together when it fails for just one Ingress
* ingress-gce seems to report errors related to other ingresses for a given one",closed,False,2018-04-09 16:53:02,2018-05-04 22:03:21
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/198,https://api.github.com/repos/kubernetes/ingress-gce/issues/198,Ensure .go/cache in build,"Patch from https://github.com/kubernetes/contrib/pull/2877. Local test (`make test`) is failing without this:
```
Checking go vet: FAIL                                                                         
go: disabling cache (/.cache/go-build) due to initialization failure: mkdir /.cache: permission denied
```",closed,True,2018-04-10 01:28:32,2018-04-10 18:53:36
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/199,https://api.github.com/repos/kubernetes/ingress-gce/issues/199,Ensure only needed nodeports on instance group on ingress sync,"I might be missing something, but it seems `ensureInstanceGroupAndPorts()` is already doing what we need --- ensuring given ports are included in instance group, so the change is tiny?

@rramkumar1 @nicksardo ",closed,True,2018-04-10 18:40:53,2018-04-10 20:33:43
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/200,https://api.github.com/repos/kubernetes/ingress-gce/issues/200,Change naming of SSL certs,"Changes
 - Rename SSL certs from `k8s-ssl-{nameHash}--{clusterId}-{secretHash}` to `k8s-ssl-{nameHash}-{secretHash}--{clusterId}` to be consistent with other resources.
 - Get rid of `sslCertPrefix` for simplification.",closed,True,2018-04-10 18:50:56,2018-04-11 16:11:33
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/issues/201,https://api.github.com/repos/kubernetes/ingress-gce/issues/201,Fail earlier instead of using defaults," - Ingresses targeting services that are not typed NodePort or LoadBalancer should fail and not create any additional resources. 
 - translator.ToURLMap should not need to fetch backend services. They were asserted earlier.",closed,False,2018-04-10 20:53:22,2018-09-25 01:04:54
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/202,https://api.github.com/repos/kubernetes/ingress-gce/issues/202,Vendor in Cluster Registry code,/assign @nicksardo ,closed,True,2018-04-11 15:32:25,2018-04-11 23:03:42
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/203,https://api.github.com/repos/kubernetes/ingress-gce/issues/203,Introduce MultiClusterContext as part of ControllerContext,"This PR introduces a new MultiClusterContext which is needed to operate in MCI mode. For now, this context will contain a Cluster Registry client and informer.

",closed,True,2018-04-11 15:58:28,2018-04-12 17:55:14
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/204,https://api.github.com/repos/kubernetes/ingress-gce/issues/204,Support Leader Election for GLBC for HA masters,"GLBC should be leader-elected, (the same way we are doing for scheduler, controller-manager, cluster-autoscaler, etc.)
As an example, this is what scheduler is doing:
https://github.com/kubernetes/kubernetes/blob/master/cmd/kube-scheduler/app/server.go#L414
",closed,False,2018-04-11 17:34:18,2018-06-11 15:56:29
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/205,https://api.github.com/repos/kubernetes/ingress-gce/issues/205,Start changelog file,,closed,True,2018-04-11 17:50:18,2018-04-11 18:00:53
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/206,https://api.github.com/repos/kubernetes/ingress-gce/issues/206,Remove duplicate nodeport translation,,closed,True,2018-04-11 17:54:17,2018-04-11 18:00:23
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/207,https://api.github.com/repos/kubernetes/ingress-gce/issues/207,Refactor translator.ToURLMap to not re-fetch backend services,"/assign @nicksardo @MrHohn 

Will update tests once approach is LGTM'd.",closed,True,2018-04-11 19:10:01,2018-04-12 15:54:15
ingress-gce,youhonglian,https://github.com/kubernetes/ingress-gce/pull/208,https://api.github.com/repos/kubernetes/ingress-gce/issues/208,typo fix ,line 441,closed,True,2018-04-12 15:40:46,2018-04-16 18:04:15
ingress-gce,youhonglian,https://github.com/kubernetes/ingress-gce/pull/209,https://api.github.com/repos/kubernetes/ingress-gce/issues/209,update README.md,line 21,closed,True,2018-04-12 16:00:54,2018-04-16 18:04:06
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/210,https://api.github.com/repos/kubernetes/ingress-gce/issues/210,Update post-release-steps.md,/assign @nicksardo ,closed,True,2018-04-12 16:11:20,2018-04-12 16:13:24
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/211,https://api.github.com/repos/kubernetes/ingress-gce/issues/211,Split sync up ,"Splitting up sync so we no longer have the defer statement.

Fixing controller tests so we know when the sync func fails.",closed,True,2018-04-12 20:57:17,2018-04-12 22:42:40
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/212,https://api.github.com/repos/kubernetes/ingress-gce/issues/212,Add multi-cluster flag,,closed,True,2018-04-12 20:58:13,2018-04-12 21:03:32
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/213,https://api.github.com/repos/kubernetes/ingress-gce/issues/213,Fix multiple secrets with same certificate,"Sometimes we hit timeouts in the e2e test: https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/ci-ingress-gce-e2e/1719 due to a certificate never getting deleted by the controller.
```log
...
I0412 10:38:57.917] Apr 12 10:38:57.917: INFO: No backend services found
I0412 10:38:59.195] Apr 12 10:38:59.194: INFO: Monitoring glbc's cleanup of gce resources:
I0412 10:38:59.195] k8s-ssl-350024a07559f009-83697e1b6f4cfc12--b4d5aa05f1df8c87 (ssl-certificate)
I0412 10:38:59.195] 
I0412 10:39:07.635] Apr 12 10:39:07.634: INFO: No backend services found
I0412 10:39:08.924] Apr 12 10:39:08.924: INFO: Monitoring glbc's cleanup of gce resources:
I0412 10:39:08.925] k8s-ssl-350024a07559f009-83697e1b6f4cfc12--b4d5aa05f1df8c87 (ssl-certificate)
... for a long time
```


glbc.log
```log
I0412 10:34:39.828468       1 l7.go:611] validating https for e2e-tests-ingress-zqvkk-echomap--b4d5aa05f1df8c87
I0412 10:34:40.122296       1 l7.go:285] Populating ssl cert k8s-ssl-350024a07559f009-83697e1b6f4cfc12--b4d5aa05f1df8c87 for l7 e2e-tests-ingress-zqvkk-echomap--b4d5aa05f1df8c87
I0412 10:34:40.122337       1 l7.go:344] Creating new sslCertificate k8s-ssl-350024a07559f009-62bfca2bfa77e63f--b4d5aa05f1df8c87 for e2e-tests-ingress-zqvkk-echomap--b4d5aa05f1df8c87
I0412 10:34:41.970998       1 l7.go:344] Creating new sslCertificate k8s-ssl-350024a07559f009-62bfca2bfa77e63f--b4d5aa05f1df8c87 for e2e-tests-ingress-zqvkk-echomap--b4d5aa05f1df8c87
E0412 10:34:42.331197       1 l7.go:351] Failed to create new sslCertificate k8s-ssl-350024a07559f009-62bfca2bfa77e63f--b4d5aa05f1df8c87 for e2e-tests-ingress-zqvkk-echomap--b4d5aa05f1df8c87 - googleapi: Error 409: The resource 'projects/k8s-ingress-boskos-17/global/sslCertificates/k8s-ssl-350024a07559f009-62bfca2bfa77e63f--b4d5aa05f1df8c87' already exists, alreadyExists
E0412 10:34:42.331293       1 taskqueue.go:85] Requeuing ""e2e-tests-ingress-zqvkk/echomap"" due to error: Cert creation failures - k8s-ssl-350024a07559f009-62bfca2bfa77e63f--b4d5aa05f1df8c87 Error:googleapi: Error 409: The resource 'projects/k8s-ingress-boskos-17/global/sslCertificates/k8s-ssl-350024a07559f009-62bfca2bfa77e63f--b4d5aa05f1df8c87' already exists, alreadyExists (ingresses)
```
At which point, the error causes the controller to forget to cleanup the old certificate, thus an orphan.

I don't yet know why the TLS list has the same certificate multiple times. It may be the test jig causing this; however, the controller should handle this situation.",closed,True,2018-04-12 22:53:18,2018-04-13 17:57:58
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/214,https://api.github.com/repos/kubernetes/ingress-gce/issues/214,Code to setup removal of ServicePort logic out of translator ,A followup PR will modify the controller to use this code rather than the code in pkg/translator.,closed,True,2018-04-12 23:00:48,2018-04-14 00:09:58
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/issues/215,https://api.github.com/repos/kubernetes/ingress-gce/issues/215,Replace link parsing and generation with cloud library,Todo: Replace code which generates or parses full/partial URLs with gce-cloud functions: https://github.com/kubernetes/kubernetes/pull/62516,closed,False,2018-04-13 09:16:32,2018-06-26 19:11:27
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/216,https://api.github.com/repos/kubernetes/ingress-gce/issues/216,Cherrypick: Fix multiple secrets with same certificate,Cherrypick fix https://github.com/kubernetes/ingress-gce/pull/213 for release 1.1,closed,True,2018-04-13 17:34:44,2018-04-16 20:59:21
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/217,https://api.github.com/repos/kubernetes/ingress-gce/issues/217,Bootstrap multi-cluster controller,All credit goes to @rramkumar1.,closed,True,2018-04-13 21:34:42,2018-04-13 22:13:40
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/218,https://api.github.com/repos/kubernetes/ingress-gce/issues/218,Set glog levels in loadbalancer pool & fix markdown,,closed,True,2018-04-16 17:37:45,2018-04-18 19:33:02
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/issues/219,https://api.github.com/repos/kubernetes/ingress-gce/issues/219,Scale test failed because ingress wasn't deleted,"Test run: https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/ci-ingress-gce-e2e-scale/258
GLBC log: https://storage.googleapis.com/kubernetes-jenkins/logs/ci-ingress-gce-e2e-scale/258/artifacts/e2e-258-68260c-master/glbc.log
Build log: https://storage.googleapis.com/kubernetes-jenkins/logs/ci-ingress-gce-e2e-scale/258/build-log.txt
Test code: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/network/scale/ingress.go

Seems that `ing-scale-45` was not deleted. 

### Timeline
build.log:
```log
I0415 11:31:50.938] Apr 15 11:31:50.919: INFO: Cleaning up ingresses...
```
glbc.log
```log
I0415 11:31:50.920575       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-1 deleted, enqueueing
I0415 11:31:50.925053       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-4 deleted, enqueueing
I0415 11:31:50.937525       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-3 deleted, enqueueing
I0415 11:31:50.937558       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-2 deleted, enqueueing
I0415 11:31:50.951442       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-0 deleted, enqueueing
I0415 11:31:50.951470       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-5 deleted, enqueueing
I0415 11:31:50.966010       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-6 deleted, enqueueing
...
I0415 11:31:53.171198       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-60 deleted, enqueueing
I0415 11:31:53.221113       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-98 deleted, enqueueing
I0415 11:31:53.270698       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-92 deleted, enqueueing
I0415 11:31:53.321062       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-51 deleted, enqueueing
I0415 11:31:53.371148       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-99 deleted, enqueueing
```
ing-scale-45 was not in the above list of event handler logs.

Instead, ing-scale-45 continues to be periodically enqueued.  However, it didn't actually get synced until 12:20. 
```log
I0415 11:39:42.262865       1 controller.go:147] Periodic enqueueing of e2e-tests-ingress-scale-c4p8n/ing-scale-45
...
I0415 11:49:42.263056       1 controller.go:147] Periodic enqueueing of e2e-tests-ingress-scale-c4p8n/ing-scale-45
...
I0415 11:59:42.263368       1 controller.go:147] Periodic enqueueing of e2e-tests-ingress-scale-c4p8n/ing-scale-45
...
I0415 12:09:42.263533       1 controller.go:147] Periodic enqueueing of e2e-tests-ingress-scale-c4p8n/ing-scale-45
...
I0415 12:19:42.263882       1 controller.go:147] Periodic enqueueing of e2e-tests-ingress-scale-c4p8n/ing-scale-45
...

I0415 12:52:29.439280       1 controller.go:138] Ingress e2e-tests-ingress-scale-c4p8n/ing-scale-45 deleted, enqueueing
I0415 12:52:29.439327       1 controller.go:240] Syncing e2e-tests-ingress-scale-c4p8n/ing-scale-45
I0415 12:52:29.439823       1 controller.go:260] Ingress ""e2e-tests-ingress-scale-c4p8n/ing-scale-45"" no longer exists, triggering GC
I0415 12:52:29.439856       1 l7s.go:182] GCing loadbalancer e2e-tests-ingress-scale-c4p8n-ing-scale-45--12b3b819e0bddc89
I0415 12:52:29.440530       1 l7s.go:133] Deleting lb e2e-tests-ingress-scale-c4p8n-ing-scale-45--12b3b819e0bddc89
I0415 12:52:29.440571       1 l7.go:845] Deleting global forwarding rule k8s-fw-e2e-tests-ingress-scale-c4p8n-ing-scale-45--12b3b819e0b0
I0415 12:52:33.142799       1 l7.go:852] Deleting global forwarding rule k8s-fws-e2e-tests-ingress-scale-c4p8n-ing-scale-45--12b3b819e00
I0415 12:52:37.390834       1 l7.go:859] Deleting static IP k8s-fw-e2e-tests-ingress-scale-c4p8n-ing-scale-45--12b3b819e0b0(35.190.76.229)
I0415 12:52:41.381293       1 l7.go:866] Deleting target https proxy k8s-tps-e2e-tests-ingress-scale-c4p8n-ing-scale-45--12b3b819e00
I0415 12:52:44.706582       1 l7.go:876] Deleting sslcert k8s-ssl-77dd24d67214d362-72e064981ed4f559--12b3b819e0bddc89
I0415 12:52:47.222874       1 l7.go:889] Deleting target http proxy k8s-tp-e2e-tests-ingress-scale-c4p8n-ing-scale-45--12b3b819e0b0
I0415 12:52:50.163649       1 l7.go:896] Deleting url map k8s-um-e2e-tests-ingress-scale-c4p8n-ing-scale-45--12b3b819e0b0
I0415 12:52:52.905790       1 backends.go:510] Deleting backend service k8s-be-30181--12b3b819e0bddc89
I0415 12:52:56.330013       1 healthchecks.go:194] Deleting health check k8s-be-30181--12b3b819e0bddc89
```


This correlates to the namespace deletion in kube-apiserver.
```
I0415 12:52:29.407968       1 wrap.go:42] DELETE /apis/extensions/v1beta1/namespaces/e2e-tests-ingress-scale-c4p8n/ingresses: (3.367637ms) 200 [[kube-controller-manager/v1.11.0 (linux/amd64) kubernetes/ee4d90a/system:serviceaccount:kube-system:namespace-controller] [::1]:46634]
```

Kube-apiserver has no record of the specific ingress being deleted at 11:31. Interestingly, build.log has no record of the ingress deletion failing.

cc @MrHohn @rramkumar1 ",closed,False,2018-04-16 20:28:14,2018-04-23 21:17:55
ingress-gce,AdamDang,https://github.com/kubernetes/ingress-gce/pull/220,https://api.github.com/repos/kubernetes/ingress-gce/issues/220,Typo fix in admin.md,"It's better to use same format for lb and LB
line 29: lb->LB",closed,True,2018-04-17 03:58:10,2018-05-01 13:38:54
ingress-gce,buzzedword,https://github.com/kubernetes/ingress-gce/pull/221,https://api.github.com/repos/kubernetes/ingress-gce/issues/221,Update annotations.md,"This documentation mistakingly implies the prefix for `pre-shared-cert` is `ingress.kubernetes.io`, when it is actually `ingress.gcp.kubernetes.io`. Using the former will not create an HTTPS mapping in the GCLB, and does not emit any errors indicating there is an issue with the annotation.",closed,True,2018-04-17 06:04:38,2018-04-17 20:45:05
ingress-gce,AdamDang,https://github.com/kubernetes/ingress-gce/pull/222,https://api.github.com/repos/kubernetes/ingress-gce/issues/222,Correct the returned message: does not exists->does not exist,Correct the returned message,closed,True,2018-04-17 06:24:38,2018-05-09 19:09:14
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/223,https://api.github.com/repos/kubernetes/ingress-gce/issues/223,Integrate ClusterServiceMapper into translator. ,Extracts ServicePort logic out of translator using the new ClusterServiceMapper implementation. Also adds tests for untested code paths.,closed,True,2018-04-17 20:58:07,2018-04-18 17:18:25
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/224,https://api.github.com/repos/kubernetes/ingress-gce/issues/224,Update changelog for 1.1.1,,closed,True,2018-04-17 21:04:50,2018-04-17 21:10:38
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/225,https://api.github.com/repos/kubernetes/ingress-gce/issues/225,Remove dead code for e2e testing. We do all e2e testing through k/k now,/assign @nicksardo ,closed,True,2018-04-18 03:17:37,2018-04-18 06:15:22
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/226,https://api.github.com/repos/kubernetes/ingress-gce/issues/226,Add support for logging latest commit hash of GLBC build being used,"This will make it easier for us during debugging e2e test failures. Specifically, we now do not need to cross-reference with the latest e2e image push job run to find out the commit used in the test.

/assign @nicksardo ",closed,True,2018-04-18 04:07:36,2018-04-18 17:17:19
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/227,https://api.github.com/repos/kubernetes/ingress-gce/issues/227,THIS IS A TEST,,closed,True,2018-04-18 15:45:27,2018-04-18 15:49:26
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/228,https://api.github.com/repos/kubernetes/ingress-gce/issues/228,Bump glbc.yaml to 1.1.1,,closed,True,2018-04-18 17:01:57,2018-04-18 17:03:48
ingress-gce,jdyer09,https://github.com/kubernetes/ingress-gce/issues/229,https://api.github.com/repos/kubernetes/ingress-gce/issues/229,Documentation: Would like to see example yaml snippet for GCP certificate resource for SSL/TLS,"I'm assuming it would look like such, but it is somewhat unclear:
```apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: no-rules-map
  annotations:
    ingress.gcp.kubernetes.io/pre-shared-cert: ""example.com-cert""
spec:
  backend:
    serviceName: s1
    servicePort: 443
```",closed,False,2018-04-18 17:08:08,2018-10-31 19:01:21
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/230,https://api.github.com/repos/kubernetes/ingress-gce/issues/230,Add support for logging latest commit hash of GLBC build being used,"#226 was merged to HEAD. Cherry picking that change to mci-dev

/assign @nicksardo ",closed,True,2018-04-18 18:52:15,2018-04-18 18:56:58
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/231,https://api.github.com/repos/kubernetes/ingress-gce/issues/231,Add support for logging latest commit hash of GLBC build being used,#226 was merged to HEAD. Cherry picking that change to release-1.1,closed,True,2018-04-18 18:52:29,2018-04-18 19:08:49
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/232,https://api.github.com/repos/kubernetes/ingress-gce/issues/232,Add support for logging latest commit hash of GLBC build being used,"#226 was merged to HEAD. Cherry picking that change to release-1.0

/assign @nicksardo ",closed,True,2018-04-18 18:52:56,2018-04-18 19:08:55
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/233,https://api.github.com/repos/kubernetes/ingress-gce/issues/233,First pass at an interface to manage informers,"This interface will allow for easy management of service and ingress informers for target clusters.

/assign @nicksardo ",closed,True,2018-04-19 15:31:23,2018-04-19 23:53:18
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/234,https://api.github.com/repos/kubernetes/ingress-gce/issues/234,Add ability for MCI controller to enqueue ingresses,"Tested this locally and it worked. Stood up a cluster registry with a couple cluster inside of it and then deleted one of the clusters. Logs showed that all ingresses in the cluster registry host were enqueued.

/assign @nicksardo ",closed,True,2018-04-19 17:22:10,2018-04-19 23:42:27
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/235,https://api.github.com/repos/kubernetes/ingress-gce/issues/235,Small addition to ClusterServiceMapper interface,"This PR adds a new method to the interface. Specifically, now the list of expected services can be overwritten with a call to SetExpectedServices. 

/assign @nicksardo ",closed,True,2018-04-19 18:03:00,2018-04-19 23:50:19
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/236,https://api.github.com/repos/kubernetes/ingress-gce/issues/236,Add code for building cluster client and other resources.,"This PR does a lot of things:

1. Add a helper for creating a kubernetes client given a Cluster spec. This code is really hacky but it will  suffice for now. 
2. Add a helper for doing some work when the ""add"" and ""delete"" event handlers fire for a cluster. Specifically, this creates the client, informer managers, and service mappers for each cluster. 

Note: I will probably add testing for all of this code in a separate PR.

/assign @nicksardo ",closed,True,2018-04-20 00:10:22,2018-04-20 18:05:00
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/237,https://api.github.com/repos/kubernetes/ingress-gce/issues/237,Add an interface to manage target resources,"/assign @nicksardo 

Next PR will hopefully tie everything together in the sync loop.",closed,True,2018-04-20 18:48:01,2018-04-20 22:26:57
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/238,https://api.github.com/repos/kubernetes/ingress-gce/issues/238,Final changes to make MCI controller work ,"Final changes to make the MCI controller work end-to-end. 

A cleanup PR will follow this one to improve naming and add relevant tests.

/assign @nicksardo ",closed,True,2018-04-23 22:11:21,2018-04-23 22:24:46
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/239,https://api.github.com/repos/kubernetes/ingress-gce/issues/239,BackendService Naming,"- Use NEG naming schema for NEG Backends associated with NEG-enabled ServicePorts. Rely on garbage collection to remove Backends with the incorrect naming schema.
- Instead of md5 hash, use a sha256 hash of concatenated values for NEG name suffix
- Use BackendService name as the key in BackendPool.snapshotter",closed,True,2018-04-25 18:58:40,2019-02-06 19:54:05
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/240,https://api.github.com/repos/kubernetes/ingress-gce/issues/240,Rename serviceextension -> backendconfig,"This PR is purely for renaming . I think `BackendConfig` is the name we were looking at?

Ref https://github.com/kubernetes/ingress-gce/pull/163.",closed,True,2018-04-26 01:01:10,2018-04-30 22:23:09
ingress-gce,sonu27,https://github.com/kubernetes/ingress-gce/issues/241,https://api.github.com/repos/kubernetes/ingress-gce/issues/241,Failing to pick up health check from readiness probe,"When I create a GCE ingress, Google Load Balancer does not set the health check from the readiness probe. According to the docs ([Ingress GCE health checks][1]) it should pick it up.

> Expose an arbitrary URL as a readiness probe on the pods backing the Service.

Any ideas why?

Deployment:

    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: frontend-prod
      labels:
        app: frontend-prod
    spec:
      selector:
        matchLabels:
          app: frontend-prod
      replicas: 3
      strategy:
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 1
        type: RollingUpdate
      template:
        metadata:
          labels:
            app: frontend-prod
        spec:
          imagePullSecrets:
            - name: regcred
          containers:
          - image: app:latest
            readinessProbe:
              httpGet:
                path: /healthcheck
                port: 3000
              initialDelaySeconds: 15
              periodSeconds: 5
            name: frontend-prod-app
          - env:
            - name: PASSWORD_PROTECT
              value: ""1""
            image: nginx:latest
            readinessProbe:
              httpGet:
                path: /health
                port: 80
              initialDelaySeconds: 5
              periodSeconds: 5
            name: frontend-prod-nginx

---

Service:

    apiVersion: v1
    kind: Service
    metadata:
      name: frontend-prod
      labels:
        app: frontend-prod
    spec:
      type: NodePort
      ports:
      - port: 80
        targetPort: 80
        protocol: TCP
        name: http
      selector:
        app: frontend-prod

---

Ingress:

    apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      name: frontend-prod-ingress
      annotations:
        kubernetes.io/ingress.global-static-ip-name: frontend-prod-ip
    spec:
      tls:
        - secretName: testsecret
      backend:
        serviceName: frontend-prod
        servicePort: 80


  [1]: https://github.com/kubernetes/ingress-gce#health-checks",closed,False,2018-04-26 15:15:06,2019-02-17 22:10:46
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/242,https://api.github.com/repos/kubernetes/ingress-gce/issues/242,Condense backendPool and defaultBackendPool ,"This PR attempts to condense management of all backends into one backend pool. Users of the condensed backend pool can now access the default backend's NodePort through a method in the BackendPool interface. 

/assign @nicksardo 

Should fix #184 & #127",closed,True,2018-04-27 15:54:51,2018-05-10 20:51:10
ingress-gce,AdamDang,https://github.com/kubernetes/ingress-gce/pull/243,https://api.github.com/repos/kubernetes/ingress-gce/issues/243,Update gce.md,"secrete->secret
""secrete"" and ""secret"" are very different meaning.",closed,True,2018-04-27 16:11:53,2018-04-29 03:50:30
ingress-gce,AdamDang,https://github.com/kubernetes/ingress-gce/pull/244,https://api.github.com/repos/kubernetes/ingress-gce/issues/244,Typo fix: secrete->secret,secrete->secret,closed,True,2018-04-29 03:52:46,2018-05-09 19:08:55
ingress-gce,djensen47,https://github.com/kubernetes/ingress-gce/issues/245,https://api.github.com/repos/kubernetes/ingress-gce/issues/245,Long time-to-first-byte problem,"I've been experiencing long wait times for time-to-first-byte (TTFB) using the ingress-gce on GKE.

I compared going through the ingress-gce versus connecting directly to a pod. Going directly to the pod via a portforward, TTFB times are in the 300ms range.

Via the ingress I have noticed:
- TTFB times between 1 - 5s
- Happens a lot on GET OPTION calls but not always
- Randomly occurs on other calls
- These are all fetch calls (ajax-style) and in a single browser reload, it happens only once

We have two rules in our configuration, three when the echoserver is up, and also tls.

I also tried this against the ""echo server"" and I see long >300ms TTFB on GET /favicon.ico

My best guess at reproduction is to:
- Set up a cluster
- Deploy the ""echo server"" gcr.io/google_containers/echoserver:1.4
- Deploy another webserver that the ingress can communicate with
- Create an ingress that has two backends and tls
- Open Chrome with developer tools open to the Network tab
- Hit the echo server
- Try this several times
- Notice that favicon.ico will very between acceptable TTFB times of <100ms to >300ms possibly even as high as 1s.",closed,False,2018-05-01 23:00:23,2018-05-31 00:11:34
ingress-gce,ppawiggers,https://github.com/kubernetes/ingress-gce/issues/246,https://api.github.com/repos/kubernetes/ingress-gce/issues/246,Support SSL policies,"GCLB [provides 3 SSL policies](https://cloud.google.com/compute/docs/load-balancing/ssl-policies?hl=en_US): COMPATIBLE, MODERN and RESTRICTED, which disable specific ciphers. By default, an ingress resource on GCLB selects the COMPATIBLE SSL policy, but I'd like that to be configurable (so TLS v1.0 can be disabled for my resource).",open,False,2018-05-02 09:18:43,2019-01-17 23:36:57
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/247,https://api.github.com/repos/kubernetes/ingress-gce/issues/247,Add util functions for backendConfig annotation,/assign @nicksardo @bowei ,closed,True,2018-05-03 17:20:17,2018-05-04 19:08:59
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/248,https://api.github.com/repos/kubernetes/ingress-gce/issues/248,Re-vendor K8s to ~1.11.0-alpha.2,,closed,True,2018-05-03 19:04:55,2018-05-03 19:12:55
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/issues/249,https://api.github.com/repos/kubernetes/ingress-gce/issues/249,Rewrite ingress-gce general documentation,TODO: Rewrite README.md emphasizing on the features this controller does support and provide links to the appropriate example directories. ,closed,False,2018-05-04 22:55:47,2018-10-31 21:48:07
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/issues/250,https://api.github.com/repos/kubernetes/ingress-gce/issues/250,Need e2e test to ensure an ingress with non-nodeport services won't break the others,"Ref https://github.com/kubernetes/ingress-gce/issues/197.

We can simply iterate on the existing testcase: 
`should update ingress while sync failures occur on other ingresses` (https://github.com/kubernetes/kubernetes/blob/fc60d36a8e74bad7d8bce550489dcf25e113c5d2/test/e2e/network/ingress.go#L149).",closed,False,2018-05-04 23:04:04,2018-10-02 00:19:10
ingress-gce,AdamDang,https://github.com/kubernetes/ingress-gce/pull/251,https://api.github.com/repos/kubernetes/ingress-gce/issues/251,Typo fix:kubernets->kubernetes/existance->existence,"Line 52: kubernets->kubernetes
Line 389: existance->existence",closed,True,2018-05-06 15:42:52,2018-05-09 19:08:44
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/252,https://api.github.com/repos/kubernetes/ingress-gce/issues/252,Add utils for retrieving backendconfigs for service (and reversely),"/assign @rramkumar1 @nicksardo 
cc @bowei ",closed,True,2018-05-07 21:45:04,2018-05-11 21:04:50
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/253,https://api.github.com/repos/kubernetes/ingress-gce/issues/253,Add tests for gceurlmap and extends GetDefaultBackendName slightly,"For future use cases, we will need to perform non-destructive reads on the gceurlmap. This PR adds that capability and also adds tests for all functionality. 

/assign @nicksardo ",closed,True,2018-05-08 16:07:44,2018-05-08 17:36:57
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/254,https://api.github.com/repos/kubernetes/ingress-gce/issues/254,Refactor gceurlmap to a struct representation,"Will fix tests once approach looks good.

/assign @nicksardo ",closed,True,2018-05-08 18:18:12,2018-05-09 21:36:19
ingress-gce,dtomcej,https://github.com/kubernetes/ingress-gce/pull/255,https://api.github.com/repos/kubernetes/ingress-gce/issues/255,Add traefik to annotations list,"### Reason for PR

Update Documentation

### What does this PR Do

Updates ingress/annotation information about the Traefik ingress controller functionality, and adds links to the project website",closed,True,2018-05-09 11:39:20,2018-06-05 14:21:24
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/256,https://api.github.com/repos/kubernetes/ingress-gce/issues/256,Minor cleanup to docs and examples,,closed,True,2018-05-09 18:32:02,2018-05-09 19:08:23
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/257,https://api.github.com/repos/kubernetes/ingress-gce/issues/257, Merge the logic of ToUrlMap() and IngressToNodePorts() ,"This PR attempts to merge the logic of two function in the translator. Right now, ToUrlMap() and IngressToNodePorts() both iterate over an Ingress spec in very similar ways. In order to reduce complexity of having two similar code paths being executed at different times, we can merge these two functions together to ensure easier testing and maintainability. 

This also gives us the opportunity to store all config in one place (GCEURLMap). Now, this struct uses a ServicePort to denote a backend rather than the backend's name. (credit to @nicksardo  for this idea)

If this approach looks reasonable, I will add tests for it (there are no tests for any of this currently). 

This PR is a prerequisite for condensing backend pool's (#242).

/assign @nicksardo ",closed,True,2018-05-09 23:09:37,2018-05-17 01:27:55
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/258,https://api.github.com/repos/kubernetes/ingress-gce/issues/258,Use leader election to prevent multiple controllers running,"Prior to starting the ingress or neg controllers, the binary will now wait to acquire a resource lock.",closed,True,2018-05-09 23:57:58,2018-06-06 17:57:19
ingress-gce,kimroen,https://github.com/kubernetes/ingress-gce/issues/259,https://api.github.com/repos/kubernetes/ingress-gce/issues/259,GCE: Converting ephemeral IP to static doesn't update related TargetHTTPSProxy,"This could be user error or issues with documentation, but I was encouraged to make an issue after first commenting on the help document I was following, so here goes:

I was going through [this tutorial on getting an Ingress set up](https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer), and then later ran into some issues getting TSL-termination working. When visiting the domain using `https://`, the browser didn't get a response from the server.

After some head-scratching, I realized the problem I was having was that the TargetHTTPSProxy and TargetHTTProxy had different IP-addresses, so HTTPS-traffic never reached my URL map.

Step 5 of the document linked above explains how to convert the ephemeral IP you get when creating the Ingress to a static one. I had followed this advice, but only the TargetHTTPProxy was using this static IP, and the TargetHTTPSProxy was still using an ephemeral IP that had changed.

Some speculation on a few different things the problem could be:

1. The TargetHTTPSProxy didn't use the static IP automatically as I was expecting
2. The `kubernetes.io/ingress.global-static-ip-name` annotation on the Ingress needs to be set when converting the IP
3. HTTP and HTTPS were already using different IP-addresses before I converted and I just didn't notice I was only converting one of them

Hopefully that helps!",open,False,2018-05-10 10:55:21,2018-10-31 20:08:31
ingress-gce,sonu27,https://github.com/kubernetes/ingress-gce/pull/260,https://api.github.com/repos/kubernetes/ingress-gce/issues/260,Update healthcheck docs regarding containerPort,Fixes issue #241,closed,True,2018-05-10 13:44:55,2018-05-11 15:59:34
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/261,https://api.github.com/repos/kubernetes/ingress-gce/issues/261,"Clean up unit tests for load balancer pool, backend pool and controller","The unit tests for these three components are quite messy and should get a thorough clean up.

/assign",closed,False,2018-05-10 19:03:26,2018-08-10 15:45:11
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/262,https://api.github.com/repos/kubernetes/ingress-gce/issues/262,BackendService naming for NEG backend services & healthchecks,"Continuation of https://github.com/kubernetes/ingress-gce/pull/239

More changes:
 - Neg suffix hash uses the UID truncated to 8 chars.
 - Neg suffix hash input fields are separated by a delimiter
 - ServicePort now has an extension function to return the correct name. 
 - `namer.Backend` is renamed to `namer.IGBackend` to prevent confusion between IG/NEG.

TODO in another PR:
Rewrite unit tests to not care what kind of serviceport their given. ",closed,True,2018-05-10 22:21:49,2018-05-11 22:49:23
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/263,https://api.github.com/repos/kubernetes/ingress-gce/issues/263,Ingress e2e tests should ensure that default backend passes health checks.,/assign,closed,False,2018-05-10 22:36:26,2018-06-11 16:04:41
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/264,https://api.github.com/repos/kubernetes/ingress-gce/issues/264,Condense health checkers into one health checker for all backends.,"This PR should wait on #262

TODOs before merge:

1. Manually test case of ingress using system default backend w/ other backends and verify health checks.
2. Verify that users can still modify health check settings for all backends and that the controller will not reconcile. 

/assign @nicksardo ",closed,True,2018-05-11 17:38:25,2018-05-14 16:55:04
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/265,https://api.github.com/repos/kubernetes/ingress-gce/issues/265,Introduce configuration for IAP & CDN into BackendConfig spec,"This is just to get a tentative spec checked in so I can start working on wiring up the actual implementation for the features. 

The spec is still subject to change.",closed,True,2018-05-11 19:03:48,2018-05-11 22:01:46
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/266,https://api.github.com/repos/kubernetes/ingress-gce/issues/266,Small aesthetic fixes to code base.,"

/assign @nicksardo ",closed,True,2018-05-11 21:24:45,2018-05-11 21:57:29
ingress-gce,ChristianAlexander,https://github.com/kubernetes/ingress-gce/pull/267,https://api.github.com/repos/kubernetes/ingress-gce/issues/267,Fix typo in faq,,closed,True,2018-05-13 14:51:12,2018-05-25 22:40:36
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/268,https://api.github.com/repos/kubernetes/ingress-gce/issues/268,Add event handlers for BackendConfig,"Add BackendConfig event handlers. Verified that this works with a simple manual test.

/assign @nicksardo 
FYI: @MrHohn ",closed,True,2018-05-14 18:29:14,2018-05-15 20:38:19
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/269,https://api.github.com/repos/kubernetes/ingress-gce/issues/269,Explore options for removing code which adds legacy GCE health check settings ,/assign ,closed,False,2018-05-14 19:13:29,2018-08-14 05:23:04
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/270,https://api.github.com/repos/kubernetes/ingress-gce/issues/270,Pass service/port tuple separate from ServicePort,"Changes:
- Users can now configure the default backend service **port** by flag - defaults to ""http"".
- Controller checks that the default backend service exists and has the specified port name.
- Now only the ServicePortID is passed to the LBC - the nodeport is no longer cached.
- TranslateIngress may now return an error. This will occur if the default backend service does not exist at sync-time.",closed,True,2018-05-15 16:10:36,2018-05-15 18:06:00
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/271,https://api.github.com/repos/kubernetes/ingress-gce/issues/271,BackendConfig v1alpha1->v1beta1,"Rename and code gen stuff.
/assign @rramkumar1 @nicksardo ",closed,True,2018-05-15 21:13:23,2018-05-15 21:26:41
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/272,https://api.github.com/repos/kubernetes/ingress-gce/issues/272,Add simple validation for BackendConfig using OpenAPI schema generation ,"1. Update codegen scripts to include generation of OpenAPI schema for BackendConfig

2. Use the generated schema to construct a validation spec which is used to initialize the CRD. If everything works right, when a CR is pushed, the validation spec will kick in and make sure the spec is valid based on the types we have defined for the BackendConfig (pkg/apis/backendconfig/v1beta1/types.go).

3. Refactor the crd code to be more generic by introducing a CRDMeta type which stores all information needed to build a CRD.

FYI: One of the commits is a vendor update and one is a patch to generated code.

/assign @MrHohn ",closed,True,2018-05-16 04:55:41,2018-05-17 21:47:45
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/273,https://api.github.com/repos/kubernetes/ingress-gce/issues/273,Fix BackendConfigKey to use beta,Sorry I missed this in https://github.com/kubernetes/ingress-gce/pull/271.,closed,True,2018-05-16 22:03:05,2018-05-17 18:44:22
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/274,https://api.github.com/repos/kubernetes/ingress-gce/issues/274,Remove NEG FeatureGate,Make NEG controller run by default. ,closed,False,2018-05-17 19:03:46,2018-08-01 23:16:33
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/275,https://api.github.com/repos/kubernetes/ingress-gce/issues/275,TranslateIngress changes,"**Changes**
 - Stop gracefully falling back on system default backend if specified backend is not found - no longer need EventRecorder in translater
 - Return list of errors from TranslateIngress
 - ensureIngress will error if TranslateIngress hits any errors. `ToSvcPorts` iterates all ingresses (for now) and will continue to collect all known service ports
 - Deleted a lot of tests in controller_test.go. Some of them should be re-created in different packages (tests that check GCE resource config), and new tests should be made for the controller that assert controller logic such as garbage collection, k8s object lifecycle edge cases.
 - Re-wrote tests for controller and created tests for TranslateIngress.
",closed,True,2018-05-18 18:13:02,2018-05-18 21:02:08
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/276,https://api.github.com/repos/kubernetes/ingress-gce/issues/276,fake backendservices save alpha objects by default,"- Have FakeBackendServices save Alpha BackendServices by default. GA methods convert the object to alpha version. This is to avoid converting an Alpha object to GA, and potentially losing data present on fields that are only in Alpha but not GA.
- Use `alphaErrFunc` to throw error if not alpha whitelisted (currently only used for HTTP/2)",closed,True,2018-05-18 22:24:42,2018-05-21 18:40:28
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/277,https://api.github.com/repos/kubernetes/ingress-gce/issues/277,Update gce provider in vendor,"Update kubernetes/kubernetes to https://github.com/kubernetes/kubernetes/commit/77a08ee2d774d1785670677d4d4b7d1a1739c12c.

Also updated the signature for `SetSslCertificateForTargetHttpsProxy()` (https://github.com/kubernetes/kubernetes/commit/f9d1f7eb75c3696c75e7e42848de34d79b3253b8).

/assign @nicksardo ",closed,True,2018-05-19 00:33:14,2018-05-21 00:11:45
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/278,https://api.github.com/repos/kubernetes/ingress-gce/issues/278,Update documentation to include multiple-TLS support.,/assign @nicksardo ,closed,True,2018-05-20 21:30:39,2018-05-22 16:05:59
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/279,https://api.github.com/repos/kubernetes/ingress-gce/issues/279,Ensure Load Balancer using IG links instead of IG compute object,"Right now, BackendPool.Ensure takes in a list of compute.InstanceGroup. However, the backend pool does not use any field of the IG other than its link. Therefore, it's cleaner to just pass in a list of IG links rather than the full object.

/assign @nicksardo ",closed,True,2018-05-21 15:21:29,2018-05-21 15:40:12
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/280,https://api.github.com/repos/kubernetes/ingress-gce/issues/280,Refactor pkg/backends to use new BackendService composite type ,"This PR introduces a new mechanism for operating on the compute BackendService API's. Previously, we maintained an internal ""BackendService"" type which contained the two actual compute types (alpha and GA) we needed. This resulted in numerous ""if"" statements and duplicated code just for one or the other compute type. 

With this PR, we now have a composite ""BackendService"" which encapsulates all the fields in the compute alpha type. Now, any field mutations can go directly to this composite type and helper methods will deal with converting the composite into the appropriate type before sending the API call off the GCE.

As a longer term TODO, all the code in compute.go and compute_test.go can be generated .",closed,True,2018-05-21 16:13:49,2018-06-04 23:06:13
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/281,https://api.github.com/repos/kubernetes/ingress-gce/issues/281,Define security policy API in BackendConfig,@rramkumar1 @bowei ,closed,True,2018-05-22 21:26:09,2018-05-25 21:40:14
ingress-gce,manishrajkarnikar,https://github.com/kubernetes/ingress-gce/issues/282,https://api.github.com/repos/kubernetes/ingress-gce/issues/282,Does not work if workers are in different subnet. ,"I have workers spanning different subnet for security reasons.  I see following error during the creation of unmanaged instance group. As a result, it cannot add any instance in the group


``` 
Requeuing ""default/zoneprinter"" due to error: [googleapi: Error 400: Resource 'projects/xxxx/zones/us-central1-b/instances/k8s-ig--xxxxxx' is expected to be in the subnetwork 'projects/xxxx/regions/us-central1/subnetworks/app-hosting-us-central1' but is in the subnetwork 'projects/xxxx/regions/us-central1/subnetworks/platform-us-central1'., wrongSubnetwork ....] (ingresses)

 ```",closed,False,2018-05-23 13:30:15,2018-11-17 16:39:02
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/283,https://api.github.com/repos/kubernetes/ingress-gce/issues/283,Make sure structs in OpenAPI spec are serialized with 'type: object',"This will ensure that if a user pushes a spec like below, that an error will be returned to them:

```
spec:
  iap:
```

/assign @MrHohn ",closed,True,2018-05-23 18:21:59,2018-05-23 18:42:24
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/284,https://api.github.com/repos/kubernetes/ingress-gce/issues/284,Add annotation for exposing NEGs,"This PR adds a new annotation for exposing standalone NEGs on Services without requiring an Ingress.

Additionally:
- Refactors NEG naming to use ServicePort Ports instead of TargetPorts, because it is possible to have multiple ServicePorts point to the same TargetPort. Note that ServicePort Ports are always int32, while TargetPorts may be an int or a string.
- Pass ServicePort into via a map mapping Port:TargetPort within `/pkg/neg` (PortNameMap type)
- feature flag around exposed NEGs

To do in later PRs:
- merge the expose NEG and ingress NEG annotations

",closed,True,2018-05-23 18:32:03,2018-06-20 17:51:17
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/285,https://api.github.com/repos/kubernetes/ingress-gce/issues/285,Bake backend config into ServicePort,"Thinking it might be great to separate this out of the actual backend service feature implementation. @rramkumar1 Feel free to close this if you have code changes ready, I remembered you were doing something similar...
@bowei ",closed,True,2018-05-24 08:03:41,2018-05-24 21:32:30
ingress-gce,jessfraz,https://github.com/kubernetes/ingress-gce/issues/286,https://api.github.com/repos/kubernetes/ingress-gce/issues/286,Create a SECURITY_CONTACTS file.,"As per the email sent to kubernetes-dev[1], please create a SECURITY_CONTACTS
file.

The template for the file can be found in the kubernetes-template repository[2].
A description for the file is in the steering-committee docs[3], you might need
to search that page for ""Security Contacts"".

Please feel free to ping me on the PR when you make it, otherwise I will see when
you close this issue. :)

Thanks so much, let me know if you have any questions.

(This issue was generated from a tool, apologies for any weirdness.)

[1] https://groups.google.com/forum/#!topic/kubernetes-dev/codeiIoQ6QE
[2] https://github.com/kubernetes/kubernetes-template-project/blob/master/SECURITY_CONTACTS
[3] https://github.com/kubernetes/community/blob/master/committee-steering/governance/sig-governance-template-short.md
",closed,False,2018-05-24 14:43:14,2018-05-31 16:42:11
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/287,https://api.github.com/repos/kubernetes/ingress-gce/issues/287,Add SECURITY_CONTACTS file,Fixes https://github.com/kubernetes/ingress-gce/issues/286,closed,True,2018-05-29 22:31:17,2018-05-31 16:42:11
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/288,https://api.github.com/repos/kubernetes/ingress-gce/issues/288,Fix bug with ensuring BackendService settings + health checks,"This PR fixes the logical OR flow in ensureBackendService to ensure that no short-circuiting occurs. Because of this short circuit, when a BackendService was being ensured only the protocol would be ensured, not the health check or description. However, on subsequent syncs, health check and description would eventually get ensured. Regardless, it makes the code clearer to fix this.

Finally, this PR also ensures that we only get a legacy health check if the BackendService is pointing to one. Previously, we would always get it. This addresses nicksardo@'s comment in #269

/assign @nicksardo

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/288)
<!-- Reviewable:end -->
",closed,True,2018-05-30 16:35:17,2018-05-31 18:40:05
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/289,https://api.github.com/repos/kubernetes/ingress-gce/issues/289,Add custom validation for BackendConfig + hook validation into Translator,"This PR adds custom validation for the BackendConfig, particularly IAP. It also hooks this validation into the existing logic in the translator.

This is the first of a couple upcoming PR's that will provide the plumbing for IAP + CDN support.

/assign @MrHohn @nicksardo

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/289)
<!-- Reviewable:end -->
",closed,True,2018-05-30 22:21:08,2018-05-31 15:54:59
ingress-gce,ahmetb,https://github.com/kubernetes/ingress-gce/issues/290,https://api.github.com/repos/kubernetes/ingress-gce/issues/290,https-only GKE ingress is still showing port 80,"I have a HTTPS only ingress that shows port 80 in kubectl get (kubectl v1.10.2):

```
# kubectl get ing
NAME       HOSTS     ADDRESS          PORTS     AGE
helloweb   *         35.227.248.205   80, 443   4m
```

I think port 80 actually isn't open:

![image](https://user-images.githubusercontent.com/159209/40799994-f488d8f6-64c3-11e8-9da5-8ff2ee86b31c.png)


describe output:

```
Name:             helloweb
Namespace:        default
Address:          35.227.248.205
Default backend:  helloweb-backend:443 (10.32.4.7:8443)
TLS:
  yourdomain-tls terminates
Rules:
  Host  Path  Backends
  ----  ----  --------
  *     *     helloweb-backend:443 (10.32.4.7:8443)
Annotations:
  ingress.kubernetes.io/backends:                    {""k8s-be-30921--906590fce6179ef9"":""Unknown""}
  ingress.kubernetes.io/https-forwarding-rule:       k8s-fws-default-helloweb--906590fce6179ef9
  ingress.kubernetes.io/https-target-proxy:          k8s-tps-default-helloweb--906590fce6179ef9
  ingress.kubernetes.io/ssl-cert:                    k8s-ssl-09cce9ee44983641-6d1404f893135fac--906590fce6179ef9
  ingress.kubernetes.io/url-map:                     k8s-um-default-helloweb--906590fce6179ef9
  kubectl.kubernetes.io/last-applied-configuration:  {""apiVersion"":""extensions/v1beta1"",""kind"":""Ingress"",""metadata"":{""annotations"":{""kubernetes.io/ingress.allow-http"":""false""},""labels"":{""app"":""hello""},""name"":""helloweb"",""namespace"":""default""},""spec"":{""backend"":{""serviceName"":""helloweb-backend"",""servicePort"":443},""tls"":[{""secretName"":""yourdomain-tls""}]}}

  kubernetes.io/ingress.allow-http:  false
Events:
  Type    Reason   Age              From                     Message
  ----    ------   ----             ----                     -------
  Normal  ADD      3m               loadbalancer-controller  default/helloweb
  Normal  CREATE   1m               loadbalancer-controller  ip: 35.227.248.205
  Normal  Service  1m (x3 over 1m)  loadbalancer-controller  default backend set to helloweb-backend:30921
```

My YAML:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: helloweb
  labels:
    app: hello
  annotations:
    kubernetes.io/ingress.allow-http: ""false"" # disable HTTP
spec:
  tls:
    - secretName: yourdomain-tls
  backend:
    serviceName: helloweb-backend
    servicePort: 443
```

any ideas if this is a kubectl bug?",closed,False,2018-05-31 18:16:14,2019-03-31 01:16:50
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/291,https://api.github.com/repos/kubernetes/ingress-gce/issues/291,Add support for security policy,"<s>The first commit is for updating vendor. I will soon split that into another PR.</s> Updated vendor in https://github.com/kubernetes/ingress-gce/pull/293.
/assign @rramkumar1 @bowei ",closed,True,2018-05-31 21:01:52,2018-06-19 22:41:16
ingress-gce,octplane,https://github.com/kubernetes/ingress-gce/issues/292,https://api.github.com/repos/kubernetes/ingress-gce/issues/292,Align node filtering with service controller,"Hello,

Would there be any interest in aligning the current node selection currently done when building the Instances Group:

https://github.com/kubernetes/ingress-gce/blob/master/pkg/controller/utils.go#L202-L215

- code ignores `Unschedulable` nodes,

to the one in k8s:

https://github.com/kubernetes/kubernetes/blob/97fdf8ac70dfdaf07074dee6b6cb345be71e96ec/pkg/controller/service/service_controller.go#L585-L619

code ignores:
- `Unschedulable` nodes
- Nodes with the label `node-role.kubernetes.io/master` 
- Nodes with the label `alpha.service-controller.kubernetes.io/exclude-balancer`

More generally, it might be nice to allow to blacklist labels via configuration.",closed,False,2018-06-01 12:49:27,2018-08-10 15:44:44
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/293,https://api.github.com/repos/kubernetes/ingress-gce/issues/293,Update vendor for gce provider,"Update gce provider to include https://github.com/kubernetes/kubernetes/pull/64528.

Hey @nicksardo could you take a look? Thanks!
/assign @nicksardo ",closed,True,2018-06-01 17:18:41,2018-06-01 17:28:17
ingress-gce,svenbovens,https://github.com/kubernetes/ingress-gce/issues/294,https://api.github.com/repos/kubernetes/ingress-gce/issues/294,Error 400: The SSL certificate could not be parsed.,"On all our ingresses on GKE we see the following error:
```
Type     Reason   Age                From                     Message
----     ------   ----               ----                     -------
Warning  GCE      9m (x40 over 10h)  loadbalancer-controller  googleapi: Error 400: The SSL certificate could not be parsed.
```
It is not clear from the error what certificate is being talked about. (it's also shown on ingresses without tls termination). 

Next to that, creating an ingress no longer seems to automatically open up the firewall anymore for the related node ports, so I can imagine this is related to the above error. 

Since we have no access on the master nor find any more info with regard to the above, we have no idea on how we can recover from this or where to look for more info.",closed,False,2018-06-01 20:06:14,2018-06-02 10:24:25
ingress-gce,lbernail,https://github.com/kubernetes/ingress-gce/issues/295,https://api.github.com/repos/kubernetes/ingress-gce/issues/295,Exclude Master and ExcludeBalancer nodes from Instance Groups,"The list of nodes added to instance groups is not consistent with the service controller.

In the service controller, when calling `ensureLoadBalancer` the node list provided to the cloud controller is built without Master and ExcludeBalancer nodes (with the feature gate ServiceNodeExclusion). Labels filtered out are `node-role.kubernetes.io/master` and `alpha.service-controller.kubernetes.io/exclude-balancer`. See function `getNodeConditionPredicate`:

https://github.com/kubernetes/kubernetes/blob/97fdf8ac70dfdaf07074dee6b6cb345be71e96ec/pkg/controller/service/service_controller.go#L585

This function then filters out nodes that are not ready.

In GCE, the load-balancer controller then creates instance groups based on these nodes.

However the logic is different in the ingress-gce controller: the `getReadyNodeNames ` function lists Ready nodes and filters out Unschedulable nodes: https://github.com/kubernetes/ingress-gce/blob/56422181dafba96a5ac41eeb2d0ac68f2926c1e3/pkg/controller/utils.go#L202

This is an issue for us because we want to use both Internal Load Balancers and GCLB ingress and the list of nodes do not match (we run Kubernetes on GCE and we have kubelets on masters). In addition, our masters are behind an ILB and the ingress controller can't add them to instance groups because they are already part of load-balanced one.

If this makes sense, I'm happy to provide a PR.",closed,False,2018-06-01 20:33:21,2018-06-05 18:18:45
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/296,https://api.github.com/repos/kubernetes/ingress-gce/issues/296,name NEGs with ServicePort instead of TargetPort,Refactor that makes https://github.com/kubernetes/ingress-gce/pull/284 easier,closed,True,2018-06-01 23:42:04,2018-06-15 21:56:05
ingress-gce,AdamDang,https://github.com/kubernetes/ingress-gce/pull/297,https://api.github.com/repos/kubernetes/ingress-gce/issues/297,Typo fix in gce-ingress-controller.yaml,permissable->permissible,closed,True,2018-06-02 09:39:55,2018-06-06 07:14:45
ingress-gce,svenbovens,https://github.com/kubernetes/ingress-gce/issues/298,https://api.github.com/repos/kubernetes/ingress-gce/issues/298,Invalid certificate leads to firewall rule no longer being updated on GKE,"Related to #294. We had an invalid certificate pointed to by an ingress leading to an event ""Error 400: The SSL certificate could not be parsed."" shown on all ingresses.

When creating a new ingress in this state everything would be done properly except adapting the firewall rule to add the new port. This lead to failing health checks for the backend and the ingress not being usable.

In the logs of that firewall rule, the events with subtype _compute.firewalls.update_ would no longer be seen.

Two things can be done here: 

- if there's an invalid certificate pointed to by an (unrelated) ingress, the firewall rule should still be adapted for new ingresses.
- if the update of the firewall rule fails for some reason, this should be shown somewhere on the ingress.

We discovered the issue on gke 1.8. The issue persisted after an update to 1.9.7-gke.1.

When the invalid certificate was fixed, the firewall rules are being updated again.",closed,False,2018-06-02 10:51:23,2018-06-07 16:20:56
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/299,https://api.github.com/repos/kubernetes/ingress-gce/issues/299,Slight refactor of controller context to include both NEG & BackendConfig switches,"This ensures that all of our switches are maintained in the controller context. It also makes some parts of the code easier to read.

/assign @nicksardo

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/299)
<!-- Reviewable:end -->
",closed,True,2018-06-04 16:41:42,2018-06-04 17:04:30
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/300,https://api.github.com/repos/kubernetes/ingress-gce/issues/300,Add IngressValidator and supporting utilities,"This adds a reusability Ingress spec testing utility. The vanilla
features of Ingress are tested with IngressValidator. Additional addon
features are supported by extending the validator with the Feature
modules in the features/ package.",closed,True,2018-06-04 20:05:54,2018-06-10 18:06:30
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/301,https://api.github.com/repos/kubernetes/ingress-gce/issues/301,IAP + CDN ,"This is the last PR for IAP + CDN integration.

TODO's in separate PR's:
    1. Need to write an e2e test using new test framework.
    2. Need to add more unit tests to verify behavior.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/301)
<!-- Reviewable:end -->
",closed,True,2018-06-04 23:41:31,2018-07-17 23:06:21
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/302,https://api.github.com/repos/kubernetes/ingress-gce/issues/302,Split l7.go into resource-specific files (no logic changes),,closed,True,2018-06-05 16:48:19,2018-06-05 19:40:25
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/303,https://api.github.com/repos/kubernetes/ingress-gce/issues/303,Use generated mocks to implement unit tests for pkg/backends ,"This PR integrates the gce generated mocks into pkg/backends. 

This PR vendors k/pkg/cloudprovider/providers/gce/cloud/mock and there are two raw patches to the vendored code which will need to be upstreamed at some point:
   1. Add a new file called gce_fake.go which exposes a generic fake GCECloud object.
   2. Add new hooks functions to existing mock.go file

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/303)
<!-- Reviewable:end -->
",closed,True,2018-06-05 19:42:58,2018-06-05 21:53:07
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/304,https://api.github.com/repos/kubernetes/ingress-gce/issues/304,Use cloud ResourceID for URL parsing and generation,"Removes the plethora of helper functions which all perform the same basic action:
 - `retrieveObjectName`
 - `comparableGroupPath`
 - `getResourceNameFromLink`
 - `CompareLinks`
 - `BackendServiceComparablePath`
 - `retrieveName`

Replacing them with a few utility functions which depend on on the [cloud package](https://godoc.org/k8s.io/kubernetes/pkg/cloudprovider/providers/gce/cloud).",closed,True,2018-06-05 20:13:24,2018-06-05 22:17:08
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/305,https://api.github.com/repos/kubernetes/ingress-gce/issues/305,Store feature names in backend service description,/assign @rramkumar1 ,closed,True,2018-06-05 20:56:50,2018-06-16 19:13:07
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/306,https://api.github.com/repos/kubernetes/ingress-gce/issues/306,Add skeleton for the e2e tests,,closed,True,2018-06-05 22:01:29,2018-06-05 22:20:00
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/307,https://api.github.com/repos/kubernetes/ingress-gce/issues/307,"Revert ""Use cloud ResourceID for URL parsing and generation""",Reverts kubernetes/ingress-gce#304,closed,True,2018-06-05 22:17:28,2018-06-05 22:24:52
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/308,https://api.github.com/repos/kubernetes/ingress-gce/issues/308,"Use cloud ResourceID for URL parsing, generation, and comparison","Removes the plethora of helper functions which all perform the same basic action:
 - `retrieveObjectName`
 - `comparableGroupPath`
 - `getResourceNameFromLink`
 - `CompareLinks`
 - `BackendServiceComparablePath`
 - `BackendServiceRelativeResourcePath`
 - `retrieveName`

Replacing them with a few utility functions which depend on on the [cloud package](https://godoc.org/k8s.io/kubernetes/pkg/cloudprovider/providers/gce/cloud).",closed,True,2018-06-05 22:25:03,2018-06-19 20:13:48
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/309,https://api.github.com/repos/kubernetes/ingress-gce/issues/309,Add version and gitcommit to the e2e test,,closed,True,2018-06-05 22:29:35,2018-06-10 18:05:53
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/310,https://api.github.com/repos/kubernetes/ingress-gce/issues/310, Add beta backend service support to composite type,/assign @rramkumar1 ,closed,True,2018-06-05 22:43:14,2018-06-06 16:23:28
ingress-gce,jmhodges,https://github.com/kubernetes/ingress-gce/issues/311,https://api.github.com/repos/kubernetes/ingress-gce/issues/311,tls secrets not updating due to invalid resource.name,"This is with v1.10.2-gke.3 (the GKE default now, I believe)

Updating a tls Secret used in a GCLB Ingress is failing because the `resource.name` field generated by the Ingress (or something) is invalid. Error from a `kubectl describe ingress apps`:

```
Warning Sync 5m (x941 over 4d) loadbalancer-controller Cert creation failures - k8s-ssl-69d4fb7e3d37d4e1-3275ae2d33a9a727-- Error:googleapi: Error 400: Invalid value for field 'resource.name': 'k8s-ssl-69d4fb7e3d37d4e1-3275ae2d33a9a727--'. Must be a match of regex '(?:a-z?)', invalid
```

(The important bit is `Invalid value for field 'resource.name': 'k8s-ssl-69d4fb7e3d37d4e1-3275ae2d33a9a727--'. Must be a match of regex '(?:a-z?)'`)

The TLS certs used by the GCLB Ingress should be updated to what is inside the Secret but, instead, the old (soon to expire) cert is the one being served.

Not sure how to reproduce other than trying to update a cert, I guess? I'm not sure how that resource.name field gets constructed. I use Let's Encrypt created tickets and they refresh often.

I've got a production certificate expiring in 10 days and I'm not sure how to fix this.",closed,False,2018-06-06 00:34:20,2018-06-20 08:39:01
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/312,https://api.github.com/repos/kubernetes/ingress-gce/issues/312,Simple web server for testing ingress-gce features,,closed,True,2018-06-06 00:47:25,2018-06-06 17:55:58
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/313,https://api.github.com/repos/kubernetes/ingress-gce/issues/313,Moved BackendService composite type to its own package,"/assign @nicksardo 
fyi @MrHohn

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/313)
<!-- Reviewable:end -->
",closed,True,2018-06-06 16:18:24,2018-06-06 16:48:56
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/314,https://api.github.com/repos/kubernetes/ingress-gce/issues/314,Add server version to echo,,closed,True,2018-06-06 18:21:40,2018-06-06 18:37:08
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/315,https://api.github.com/repos/kubernetes/ingress-gce/issues/315,Support caching in echoserver,"Using the request query string, we can turn caching on and off.

Verified it works as expected:

Request: curl localhost:8080/?cache=true
```
HTTP/1.1 200 OK
Cache-Control: max-age=86400,public
Content-Type: application/json
Date: Wed, 06 Jun 2018 18:38:31 GMT
Content-Length: 247
```

Request: curl localhost:8080/?cache=[false,blah,empty]
```
HTTP/1.1 200 OK
Content-Type: application/json
Date: Wed, 06 Jun 2018 18:38:31 GMT
Content-Length: 247
```

/assign @nicksardo ",closed,True,2018-06-06 18:40:48,2018-06-06 19:35:20
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/316,https://api.github.com/repos/kubernetes/ingress-gce/issues/316,Move cloud to ControllerContext,,closed,True,2018-06-06 20:03:58,2018-06-06 21:18:07
ingress-gce,matti,https://github.com/kubernetes/ingress-gce/issues/317,https://api.github.com/repos/kubernetes/ingress-gce/issues/317,Ingress health check not following ReadinessProbe,"with:

```
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
```

the google health check is still `/`

Also confirmed by @Gogoro:

"" When I create an ingress pointing to a service, which again points to a pod it just keeps hitting / instead of the path I defined in readinessProbe and livenessProbe. I can see in the logs that the pod itself checks itself easily, but the healthcheck goes ham on /.""

In this discussion issue: https://github.com/kubernetes/ingress-gce/issues/42",closed,False,2018-06-07 11:03:24,2018-10-25 09:45:57
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/318,https://api.github.com/repos/kubernetes/ingress-gce/issues/318,Break out some helper functions for better testing + reuse,"The motivation behind this change is to be able to reuse these new utility functions in our new testing framework. Also, breaking these functions out make them easier to test.

/assign @nicksardo 
/assign @MrHohn

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/318)
<!-- Reviewable:end -->
",closed,True,2018-06-07 17:32:00,2018-06-07 18:01:28
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/319,https://api.github.com/repos/kubernetes/ingress-gce/issues/319,IAP + CDN e2e testing implementation,"Note that this PR also contains some small changes to the validator:
    1.  In order to prevent the http client in the validator from following redirects, I implemented the 
         CheckRedirect hook. 
    2.  Move the pathToDefaultBackend variable up higher so that other files can use it.

Ref: #301
<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/319)
<!-- Reviewable:end -->
",closed,True,2018-06-07 20:57:48,2018-06-19 22:41:50
ingress-gce,gce-2017,https://github.com/kubernetes/ingress-gce/issues/320,https://api.github.com/repos/kubernetes/ingress-gce/issues/320,Option to launch GCP TCP Proxy LB from Kubernetes,"It would be very useful to have a way to launch also GCP TCP Proxy LB directly Kubernetes. Currently there are only options to launch GCP Network LBs (Load Balancer and GCP HTTP(S) LBs (ingress).

There is currently way is to do it manually configuring Nginx ingress controller, but it will be a more user friendly option to be able to create it directly from k8s.",open,False,2018-06-08 10:17:03,2018-12-15 09:12:47
ingress-gce,rgfunk,https://github.com/kubernetes/ingress-gce/issues/321,https://api.github.com/repos/kubernetes/ingress-gce/issues/321,Ingress tries to create SSL certificate from secret with illegal name,"With revision acbecb5671a06fe5582e3d5b80d1aa6d48347be8 the naming for SSL certificates created from secrets was changed to include the ingress-uid as a suffix (in namer.go). However, clusters with ingress resources created before kubernetes version 1.3 have an empty ingress-uid, as described here:

https://github.com/kubernetes/ingress-gce/blob/master/BETA_LIMITATIONS.md#changing-the-cluster-uid

This leads to an illegal name for the ssl certificate (ending in ""--""). Setting the uid is not an option as this would require deleting all ingress resources, which is currently not possible. It would be great if the naming supported a fallback in case the ingress-uid is not set.

The creation of all other ingress resources works fine without having the ingress-uid set.",closed,False,2018-06-08 14:19:12,2018-06-12 00:21:16
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/322,https://api.github.com/repos/kubernetes/ingress-gce/issues/322,Fix ingress translation logic to not GC backend if non-fatal error occurred,"/assign @nicksardo

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/322)
<!-- Reviewable:end -->
",closed,True,2018-06-08 18:00:04,2018-06-11 17:35:32
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/323,https://api.github.com/repos/kubernetes/ingress-gce/issues/323,Add error types for errors to improve testing and readability,"/assign @MrHohn

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/323)
<!-- Reviewable:end -->
",closed,True,2018-06-08 20:55:52,2018-06-11 16:58:07
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/324,https://api.github.com/repos/kubernetes/ingress-gce/issues/324,Fix build to only build the executable target,"The build was building everything in the repo recursively, causing
an unstable built for the e2e-test.",closed,True,2018-06-08 22:44:57,2018-06-10 18:07:13
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/325,https://api.github.com/repos/kubernetes/ingress-gce/issues/325,Many small cleanups to get basic_test.go working,"basic_test.go should contain everything need to write succinct e2e tests

- Add Sandboxes for isolation
- SIGINT handling
- fixtures for testing
- basic_test.go
- GCP helpers to validate object graph",closed,True,2018-06-10 05:42:37,2018-06-10 18:03:31
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/326,https://api.github.com/repos/kubernetes/ingress-gce/issues/326,Fix typos in copyright year,,closed,True,2018-06-10 18:02:42,2018-06-10 18:06:40
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/327,https://api.github.com/repos/kubernetes/ingress-gce/issues/327,Make Ingress builder reusable,"- Get Ingress definition from builder with method Build() rather than
  direct access to the object.
- Fix line breaks to make invocations more readable.",closed,True,2018-06-10 18:58:29,2018-06-10 19:04:59
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/328,https://api.github.com/repos/kubernetes/ingress-gce/issues/328,Add Dockerfile for fuzzer,"/assign @bowei

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/328)
<!-- Reviewable:end -->
",closed,True,2018-06-11 18:43:40,2018-06-11 19:00:26
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/329,https://api.github.com/repos/kubernetes/ingress-gce/issues/329,Fixes,"    Add Dockerfile for the fuzzer
    Fixed invalid reference to personal project
",closed,True,2018-06-11 18:59:22,2018-06-11 19:40:55
ingress-gce,prameshj,https://github.com/kubernetes/ingress-gce/pull/330,https://api.github.com/repos/kubernetes/ingress-gce/issues/330,Handle empty cluster name in sslcert namer,Fixes #321 ,closed,True,2018-06-11 19:42:40,2018-06-12 00:21:16
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/331,https://api.github.com/repos/kubernetes/ingress-gce/issues/331,Add fixtures and helpers in e2e framework for BackendConfig,"/assign @bowei

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/331)
<!-- Reviewable:end -->
",closed,True,2018-06-11 22:14:50,2018-06-14 04:01:00
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/332,https://api.github.com/repos/kubernetes/ingress-gce/issues/332,NEG Metrics,"promethues dump:
```
# HELP neg_controller_neg_sync_count Number of execution for a NEG syncer.
# TYPE neg_controller_neg_sync_count counter
neg_controller_neg_sync_count{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach""} 3
# HELP neg_controller_neg_sync_duration_seconds Sync latency of a NEG syncer
# TYPE neg_controller_neg_sync_duration_seconds histogram
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""0.005""} 0
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""0.01""} 0
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""0.025""} 0
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""0.05""} 0
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""0.1""} 0
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""0.25""} 0
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""0.5""} 2
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""1""} 2
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""2.5""} 3
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""5""} 3
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""10""} 3
neg_controller_neg_sync_duration_seconds_bucket{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach"",le=""+Inf""} 3
neg_controller_neg_sync_duration_seconds_sum{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach""} 3.252480561
neg_controller_neg_sync_duration_seconds_count{key=""k8s1-84a1883c1680fcf8-default-hostname-9376-d05afea4"",result=""success"",type=""attach""} 3
# HELP neg_controller_sync_timestamp The timestamp of the last execution of NEG controller sync loop.
# TYPE neg_controller_sync_timestamp gauge
neg_controller_sync_timestamp 1.5288422052249308e+18

```",closed,True,2018-06-11 23:29:13,2018-06-12 23:07:40
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/333,https://api.github.com/repos/kubernetes/ingress-gce/issues/333,Update Dockerfile for the e2e test,,closed,True,2018-06-12 19:07:42,2018-06-13 00:45:39
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/334,https://api.github.com/repos/kubernetes/ingress-gce/issues/334,"Add quotes to echo, allow CONTAINER_BINARIES override",,closed,True,2018-06-12 19:35:47,2018-06-14 02:23:55
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/335,https://api.github.com/repos/kubernetes/ingress-gce/issues/335,Allow LoadBalancer services,,closed,True,2018-06-13 05:57:31,2018-06-14 02:23:57
ingress-gce,mirake,https://github.com/kubernetes/ingress-gce/pull/336,https://api.github.com/repos/kubernetes/ingress-gce/issues/336,Typo fix: existance -> existence,,closed,True,2018-06-13 10:51:05,2018-06-13 20:22:30
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/337,https://api.github.com/repos/kubernetes/ingress-gce/issues/337,Update README to include code coverage,"/assign @nicksardo

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/337)
<!-- Reviewable:end -->
",closed,True,2018-06-14 21:19:40,2018-06-14 21:27:35
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/338,https://api.github.com/repos/kubernetes/ingress-gce/issues/338,Add logging to the GLBCFromVIP for debugging,,closed,True,2018-06-15 06:27:32,2018-06-15 19:18:19
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/339,https://api.github.com/repos/kubernetes/ingress-gce/issues/339,Testing improvements,"This PR makes test improvements and additions in numerous packages.

/assign @nicksardo

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/339)
<!-- Reviewable:end -->
",closed,True,2018-06-15 15:49:54,2018-06-15 19:18:10
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/340,https://api.github.com/repos/kubernetes/ingress-gce/issues/340,Use generated mocks for load balancer unit tests [WIP],"

<!-- Reviewable:start -->
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/340)
<!-- Reviewable:end -->
",closed,True,2018-06-15 20:26:43,2018-08-06 14:57:03
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/341,https://api.github.com/repos/kubernetes/ingress-gce/issues/341,use flag instead of gate for NEG,https://github.com/kubernetes/ingress-gce/issues/274,closed,True,2018-06-15 21:58:43,2018-06-18 20:21:33
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/342,https://api.github.com/repos/kubernetes/ingress-gce/issues/342,Minor fix for retrieving backendService resource,"Should be `backendService` instead of `urlMap`?
@rramkumar1 ",closed,True,2018-06-15 23:58:09,2018-06-16 00:02:18
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/343,https://api.github.com/repos/kubernetes/ingress-gce/issues/343,"Retry on getting PROJECT, dump out project resources","Retry on metadata curl with explicit error out if it cannot be
contacted.

Capture more information about the project resources so
it is easier to do post-mortem debugging.",closed,True,2018-06-16 18:59:35,2018-06-17 00:19:37
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/344,https://api.github.com/repos/kubernetes/ingress-gce/issues/344,Delete ingress and wait for resource deletion,"This avoids leaking resources during cluster teardown in the
normal case.",closed,True,2018-06-16 21:13:35,2018-06-17 00:19:50
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/345,https://api.github.com/repos/kubernetes/ingress-gce/issues/345,Modify IAP + CDN support to not touch settings if section in spec is missing,"/assign @MrHohn

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/345)
<!-- Reviewable:end -->
",closed,True,2018-06-18 21:55:13,2018-06-18 22:35:03
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/346,https://api.github.com/repos/kubernetes/ingress-gce/issues/346,Adds readme for e2e-tests,,closed,True,2018-06-19 06:58:19,2018-06-19 07:12:47
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/347,https://api.github.com/repos/kubernetes/ingress-gce/issues/347,"On removal of backend config name from service annotaion, ensure no existing settings are affected","/assign @nicksardo 

This ensures that when someone dereferences a BackendConfig from the annotation, that we don't touch their settings,

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/347)
<!-- Reviewable:end -->
",closed,True,2018-06-19 15:43:54,2018-06-19 16:09:03
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/348,https://api.github.com/repos/kubernetes/ingress-gce/issues/348,Make sure we get a BackendService after updating it to populate object fingerprint [WIP],"Turns out that we had an update call without a corresponding get to update the fingerprint. 

/assign @nicksardo

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/348)
<!-- Reviewable:end -->
",closed,True,2018-06-19 18:05:16,2018-06-20 00:25:33
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/349,https://api.github.com/repos/kubernetes/ingress-gce/issues/349,Add Liveness Probe for NEG controller,,closed,True,2018-06-19 21:33:33,2018-06-20 17:51:23
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/350,https://api.github.com/repos/kubernetes/ingress-gce/issues/350,merge Ingress NEG annotation and Expose NEG annotation,"Follow-up to https://github.com/kubernetes/ingress-gce/pull/284 .

- Merges the Expose NEG and Ingress NEG annotations into a single key. Example: `{""ingress"": true,""exposed_ports"":{""3000"":{},""4000"":{}}}`
- Use kubeClient to actually update the Kubernetes Service in addition to updating it in serviceLister.
- Additional cleanup of documentation.",closed,True,2018-06-20 17:44:31,2018-06-22 23:29:23
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/351,https://api.github.com/repos/kubernetes/ingress-gce/issues/351,Minor fix to backend config errors,/assign @rramkumar1 ,closed,True,2018-06-20 19:30:41,2018-06-20 20:45:26
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/352,https://api.github.com/repos/kubernetes/ingress-gce/issues/352,Trigger ingress sync on system default backend update,"Given that setting backend config with system default backend is valid, update on system default backend should trigger ingress sync as well.

/assign @rramkumar1 ",closed,True,2018-06-20 21:23:48,2018-06-20 22:07:07
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/353,https://api.github.com/repos/kubernetes/ingress-gce/issues/353,Add backendconfig client to e2e framework,/assign @rramkumar1 ,closed,True,2018-06-20 22:59:24,2018-06-20 23:48:01
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/354,https://api.github.com/repos/kubernetes/ingress-gce/issues/354,Fix hasAlphaResource and hasBetaResource,"Seems like resourceType was mistakenly hardcoded.
/assign @rramkumar1 ",closed,True,2018-06-20 23:11:01,2018-06-20 23:23:49
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/355,https://api.github.com/repos/kubernetes/ingress-gce/issues/355,Add host to echo dump,,closed,True,2018-06-20 23:32:20,2018-06-21 01:03:44
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/356,https://api.github.com/repos/kubernetes/ingress-gce/issues/356,URLMap sync,"Changes
 - `UpdateUrlMap()` mostly turned into static func `toComputeURLMap()`. It's sync logic moved to `ensureComputeURLMap()`.
 - `L7s.Sync()` calls `ensureComputeURLMap()` to assert the URLMap's existence and correctness instead of making a separate, later call to `UpdateUrlMap()` 
 - `utils.GCEURLMap` used a map for hostname->path rules which made the generated URLMap's order non-deterministic (presumably this caused unnecessary API calls). It now uses a slice and maintains uniqueness.

Bulk of this PR is in tests or related datafiles.",closed,True,2018-06-21 01:03:05,2018-07-18 17:55:18
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/357,https://api.github.com/repos/kubernetes/ingress-gce/issues/357,Implement fuzzer for feature security policy,"Implement mostly an empty fuzzer for security policy. There isn't much we can check from the response. The actual e2e test and validation logic will be in a separate PR.

/assign @rramkumar1 ",closed,True,2018-06-21 17:07:25,2018-06-21 21:11:18
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/358,https://api.github.com/repos/kubernetes/ingress-gce/issues/358,nit fixes,,closed,True,2018-06-21 21:28:05,2018-06-21 21:33:27
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/359,https://api.github.com/repos/kubernetes/ingress-gce/issues/359,Make WaitForIngressDeletion() a util func and add a knob for omitting default backend,"Thinking a knob for omitting default backend service might be useful given it can be shared among LBs.

/assign @rramkumar1 @bowei ",closed,True,2018-06-21 22:15:40,2018-06-22 23:35:06
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/360,https://api.github.com/repos/kubernetes/ingress-gce/issues/360,Implement e2e test for security policy,"This e2e test mostly checks if security policy is properly set on the relevant backend service resource upon ingress creation/update.

/assign @rramkumar1 @bowei ",closed,True,2018-06-22 23:34:57,2018-06-27 21:36:17
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/361,https://api.github.com/repos/kubernetes/ingress-gce/issues/361,Fix missing gcloud command in e2e script,,closed,True,2018-06-23 07:25:42,2018-06-23 16:23:20
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/362,https://api.github.com/repos/kubernetes/ingress-gce/issues/362,"Revendor GCE go client, cloud provider and fixes to make it work",,closed,True,2018-06-25 18:31:44,2018-06-25 18:39:42
ingress-gce,tianyicaii,https://github.com/kubernetes/ingress-gce/issues/363,https://api.github.com/repos/kubernetes/ingress-gce/issues/363,"""./deploy/glbc/script.sh --clean"" does not reset the file ""./deploy/glbc/yaml/default-http-backend.yaml""","The second time to run the script (after --clean) would cause the file ""./deploy/glbc/yaml/default-http-backend.yaml"" in invalid syntax with extra empty ""nodePort"" field.

```
--- a/deploy/glbc/yaml/default-http-backend.yaml
+++ b/deploy/glbc/yaml/default-http-backend.yaml
@@ -62,6 +62,8 @@ spec:
     targetPort: 8080
     protocol: TCP
     name: http
+    nodePort:  31594
+    nodePort: 

```",closed,False,2018-06-25 18:48:16,2018-06-26 20:29:13
ingress-gce,anfernee,https://github.com/kubernetes/ingress-gce/pull/364,https://api.github.com/repos/kubernetes/ingress-gce/issues/364,Fix order-dependency in test cases,,closed,True,2018-06-25 23:28:13,2018-06-25 23:43:53
ingress-gce,anfernee,https://github.com/kubernetes/ingress-gce/pull/365,https://api.github.com/repos/kubernetes/ingress-gce/issues/365,Fix the issue where Shutdown doesn't shutdown taskqueue,"Originally, PeriodicTaskQueue has 2 loops:
- wait.Until managed by stopCh
- for loop in worker() managed by workqueue

Shutdown only exits the second loop. Failing to close stopCh before
calling Shutdown will cause panic because of double close workDone
channel.",closed,True,2018-06-25 23:28:37,2018-10-30 16:05:14
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/366,https://api.github.com/repos/kubernetes/ingress-gce/issues/366,Swtich to use beta HealthCheck for NEG,cc @agau4779 ,closed,True,2018-06-26 00:20:46,2018-06-27 18:41:49
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/367,https://api.github.com/repos/kubernetes/ingress-gce/issues/367,Add simple e2e test for CDN & IAP ,"I still need to add a test to verify the state transitions

/assign @bowei

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/367)
<!-- Reviewable:end -->
",closed,True,2018-06-26 18:53:03,2018-06-27 21:38:37
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/368,https://api.github.com/repos/kubernetes/ingress-gce/issues/368,Update deploy script to edit copy of default backend service yaml,"Fixes #363

/assign @MrHohn

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/368)
<!-- Reviewable:end -->
",closed,True,2018-06-26 19:03:17,2018-06-26 20:29:14
ingress-gce,GGotardo,https://github.com/kubernetes/ingress-gce/issues/369,https://api.github.com/repos/kubernetes/ingress-gce/issues/369,Option to share LB between Ingresses,"I wan't to organize my cluster in multiples namespaces (**app1**, **app2**) and work with Ingress to access each of them. Something like:

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: app1-ing
  namespace: app1
  annotations:
    kubernetes.io/ingress.global-static-ip-name: ip-ingress-backend
spec:
  rules:
  - host: app1-service1.example.com
    http:
      paths:
      - backend:
          serviceName: nginx-1
          servicePort: 80
        path: /service1
 - host: app1-service2.example.com
    http:
      paths:
      - backend:
          serviceName: nginx-2
          servicePort: 80
        path: /service2
```

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: app2-ing
  namespace: app2
  annotations:
    kubernetes.io/ingress.global-static-ip-name: ip-ingress-backend
spec:
  rules:
  - host: app2-service1.example.com
    http:
      paths:
      - backend:
          serviceName: nginx-1
          servicePort: 80
        path: /service1
   - host: app2-service2.example.com
    http:
      paths:
      - backend:
          serviceName: nginx-2
          servicePort: 80
        path: /service2
```

But when I try to do so, the following error is showed while creating the second Ingress:
```bash
googleapi: Error 400: Invalid value for field 'resource.IPAddress': 'xxxx'. Specified IP address is in-use and would result in a conflict., invalid
```

It tries to create another LB, but it should share the same one, just creating new backends/frontends.",open,False,2018-06-26 20:25:12,2019-03-12 16:45:02
ingress-gce,lbernail,https://github.com/kubernetes/ingress-gce/pull/370,https://api.github.com/repos/kubernetes/ingress-gce/issues/370,Align node filtering with kubernetes service controller,"Addresses #292 

Use the same node selection logic as the kubernetes service controller:
- filter out master nodes
- filter out non-ready node
- filter out nodes with label `alpha.service-controller.kubernetes.io/exclude-balancer`

In the kubernetes service controller, the service node exclusion is a feature gate but there is no feature gate on this controller, so I removed the feature gate test (it seems unlikely that this label would be applied to a node that doesn't require exclusion). If you think it's not a good call I can revisit this.

I have a custom build available on dockerhub for testing: `lbernail/glbc:v0.2`",closed,True,2018-06-27 12:56:41,2018-08-07 17:00:06
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/371,https://api.github.com/repos/kubernetes/ingress-gce/issues/371,Fix WaitForGCLBDeletion() callers,"Sorry, the signature of `WaitForGCLBDeletion()` was changed by https://github.com/kubernetes/ingress-gce/pull/360 but new callers were added in https://github.com/kubernetes/ingress-gce/pull/367 so codes can't built at the moment.

/assign @rramkumar1 ",closed,True,2018-06-27 23:11:30,2018-06-27 23:14:44
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/372,https://api.github.com/repos/kubernetes/ingress-gce/issues/372,Add a negative test case for referencing not exist BackendConfig,"This test creates a set of Ingress & Service that references a not exist BackendConfig and waits for a warning event being emitted. 

/assign @rramkumar1 @bowei ",closed,True,2018-06-28 00:18:59,2018-06-28 17:49:35
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/373,https://api.github.com/repos/kubernetes/ingress-gce/issues/373,Add option to specify number of consecutive runs for e2e framework [WIP],"This will allow us to implement some upgrade testing. Specifically, we can continuously run the e2e tests while the GLBC is getting updated without destroying the sandboxes.

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/kubernetes/ingress-gce/373)
<!-- Reviewable:end -->
",closed,True,2018-06-28 00:43:27,2018-06-28 16:01:55
ingress-gce,AdamDang,https://github.com/kubernetes/ingress-gce/pull/374,https://api.github.com/repos/kubernetes/ingress-gce/issues/374,Typo in message: SyncNetworkEndpiontGroupFailed->SyncNetworkEndpointGroupFailed,Line 127: SyncNetworkEndpiontGroupFailed->SyncNetworkEndpointGroupFailed,closed,True,2018-06-28 06:07:06,2018-07-16 20:42:54
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/375,https://api.github.com/repos/kubernetes/ingress-gce/issues/375,Add plumbing to support upgrade testing. ,Adds the ability to run a test inside a sandbox continuously. This allows us to test functionality while the ingress is being updated.,closed,True,2018-06-28 18:01:41,2018-07-03 20:42:27
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/376,https://api.github.com/repos/kubernetes/ingress-gce/issues/376,Fix run.sh to properly print exit code of test run,/assign @bowei ,closed,True,2018-06-28 18:40:17,2018-06-28 20:16:50
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/377,https://api.github.com/repos/kubernetes/ingress-gce/issues/377,PortNameMap should also compare values,cc @agau4779 ,closed,True,2018-06-28 23:15:44,2018-06-29 17:22:20
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/378,https://api.github.com/repos/kubernetes/ingress-gce/issues/378,cherrypick #377 into release-1.2,#377,closed,True,2018-06-29 17:30:12,2018-06-29 17:45:41
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/379,https://api.github.com/repos/kubernetes/ingress-gce/issues/379,Add test filtering support in e2e runner,/assign @bowei ,closed,True,2018-06-29 18:17:57,2018-06-29 18:49:36
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/380,https://api.github.com/repos/kubernetes/ingress-gce/issues/380,Implement ingress sync timestamp,/assign @rramkumar1 @bowei ,closed,True,2018-06-29 18:19:15,2018-06-29 18:56:02
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/381,https://api.github.com/repos/kubernetes/ingress-gce/issues/381,Some fixes for 1.2,cc @agau4779 ,closed,True,2018-06-29 21:20:48,2018-07-03 17:58:02
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/382,https://api.github.com/repos/kubernetes/ingress-gce/issues/382,promote http2 to beta,"Verified that the beta resources are available on GCP compute, and that the k8s e2e http2 test works with these changes.",closed,True,2018-06-29 22:05:02,2018-07-18 18:48:45
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/383,https://api.github.com/repos/kubernetes/ingress-gce/issues/383,Add more negative test cases for backend config,/assign @rramkumar1 @bowei ,closed,True,2018-06-29 22:11:41,2018-07-09 16:22:17
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/384,https://api.github.com/repos/kubernetes/ingress-gce/issues/384,Patch NEG version into features.go and add more docs for features package,"`func (sp ServicePort) Version() meta.Version ` is deprecated, new feature version logic should be added to features.go instead. 

Still working on fixing the unit test.
/assign @freehan @agau4779 
",closed,True,2018-06-29 22:39:50,2018-06-29 23:42:45
ingress-gce,tomoe,https://github.com/kubernetes/ingress-gce/issues/385,https://api.github.com/repos/kubernetes/ingress-gce/issues/385,Unneeded health check created for kube-system:default-http-backend service ,"When I create an Ingress that has default backends, the controller still creates a GCE healthcheck pointing to kube-system:default-http-backend service (default 404 server) even thought there's no chance of hitting the pod. 

Usually, that doesn't do any harm, but possibly duplicated healthcheck probes may overload the l7-default-backend pod and causes the pod to restart with error messages because the resource setting for the container is very small (cpu: 10m, memory: 20M). 

Now I think: 
- it would be nice if HC gets created if it really is needed
- (not an issue of the controller, but) l7-default-backend resource setting may need to be increased, autoscalable, and/or configurable.  
",closed,False,2018-07-03 08:17:52,2018-07-09 22:56:20
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/386,https://api.github.com/repos/kubernetes/ingress-gce/issues/386,Increase timeout on waiting for GLBC resource deletion,/assign @bowei ,closed,True,2018-07-03 17:56:27,2018-07-03 18:21:49
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/387,https://api.github.com/repos/kubernetes/ingress-gce/issues/387,Modify security policy e2e test to create unique GCP resources.,"If this test runs in multiple clusters, then we will see failures since the security policy created will not be unique to that clusters. Adding the sandbox namespace to the end of the name ensures we don't see these errors.

/assign @bowei 
/cc @MrHohn ",closed,True,2018-07-03 20:33:37,2018-07-03 21:56:22
ingress-gce,munnerz,https://github.com/kubernetes/ingress-gce/pull/388,https://api.github.com/repos/kubernetes/ingress-gce/issues/388,Fire warning event instead of hard failing if TLS certificate is not present,"This is a follow up/in relation to my comment made here: https://github.com/kubernetes/ingress-gce/pull/112#issuecomment-394784727

This switches ingress-gce back to the previous behaviour of 'soft failing' if the specified TLS secret exists. Without this change, we've had many users complain that auto-TLS using cert-manager (and also kube-lego) is broken.

Please see the discussion in #112 and https://github.com/jetstack/cert-manager/issues/606 for more detail.

The one notable difference, is instead of simply logging a warning using `glog` (which on GKE is not visible to end users), we now fire a Warning event, but importantly **still continue afterwards**. This is essential for something like cert-manager, as without it users would need to create some kind of self-signed placeholder certificate.

For reference, ingress-nginx will actually generate its own self signed certificate, and if the specified Secret cannot be found, it will serve using this. This would also be acceptable for ingress-gce, however is a more considerable change.

I really hope we can get this patch accepted in some form, as users are currently unable to automatically obtain TLS certificates from Let's Encrypt without extra workarounds (i.e. manually specifying Certificate resources).

Most importantly however, #112 was a *breaking change* that was not communicated, and as a result has caused issues downstream for end-users.

Thanks for taking a look!",closed,True,2018-07-04 11:41:27,2018-09-12 17:21:05
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/389,https://api.github.com/repos/kubernetes/ingress-gce/issues/389,Cherrypick #381and #384 into release-1.2,,closed,True,2018-07-06 20:56:29,2018-07-09 16:28:39
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/390,https://api.github.com/repos/kubernetes/ingress-gce/issues/390,Cherrypick #383: Unmask get backend config errors,/assign @rramkumar1 ,closed,True,2018-07-09 18:56:11,2018-07-09 18:59:37
ingress-gce,mgub,https://github.com/kubernetes/ingress-gce/pull/391,https://api.github.com/repos/kubernetes/ingress-gce/issues/391,Update README.md,The link to SSL certificate concepts was broken. I updated the link.,closed,True,2018-07-10 03:36:44,2018-08-14 23:28:01
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/392,https://api.github.com/repos/kubernetes/ingress-gce/issues/392,Add plumbing for upgrade testing,"Based on the design we discussed offline (used a ConfigMap to manage the stability statuses of Ingresses in test sandboxes).

Open to thoughts on how to structure the type which manages the ConfigMap (StatusManager)

/assign @bowei ",closed,True,2018-07-10 16:06:27,2018-11-14 23:30:41
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/393,https://api.github.com/repos/kubernetes/ingress-gce/issues/393,Implement security policy validator for real,"This PR teaches security policy validator to actually check for response's status code.
/assign @rramkumar1 @bowei ",closed,True,2018-07-11 06:05:40,2018-07-11 22:32:20
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/394,https://api.github.com/repos/kubernetes/ingress-gce/issues/394,Documentation fixes,"Patches up some holes created by #150

/assign @nicksardo ",closed,True,2018-07-12 16:17:48,2018-07-12 16:21:30
ingress-gce,tomoe,https://github.com/kubernetes/ingress-gce/issues/395,https://api.github.com/repos/kubernetes/ingress-gce/issues/395,Way to check what version of GLBC is running on a GKE cluster,"I cannot seem to find a way to check what version of GLBC is running on a GKE cluster. 
There should be a document and/or an API that enables users to find out that version information. ",closed,False,2018-07-13 02:59:50,2018-08-14 21:39:04
ingress-gce,chaima-ennar,https://github.com/kubernetes/ingress-gce/issues/396,https://api.github.com/repos/kubernetes/ingress-gce/issues/396,"unhealthy backend services, with 400, 412, 409 errors ","I am trying to deploy an application composed of two services front and back, I am using ingress and ""gce"" as a controller, Every time I modify the docker image of the service ( I add modifications in the code and nothing beoyond that), the service turns to be unhealthy. and I got these errors , 

> loadbalancer-controller  received errors when updating backend service: googleapi: Error 400: The resource 'projects/fl../global/backendServices/k8s-be-32112--17c72.....ece101' is not ready, resourceNotReady
 

> loadbalancer-controller  googleapi: Error 412: Invalid fingerprint., conditionNotMet

> error 404 services already exists, 

here is my ingress ressource 


```
piVersion: extensions/v1beta1
kind: Ingress
metadata:
  labels:
    app: app
    part: ingress
  name: my-irool-ingress
  annotations:
    kubernetes.io/ingress.class: ""gce""
    kubernetes.io/ingress.global-static-ip-name: my-ip
spec:
 backend:
    serviceName: client-svc
    servicePort: 80
 rules:
  - http:
        paths:
        - path: /back
          backend:
            serviceName: back-svc
            servicePort: 9000
  - http:
        paths:
        - path: /back/*
          backend:
            serviceName: back-svc
            servicePort: 9000
```

```

apiVersion: v1
kind: Service
metadata:
  labels:
    app: app
    part: back
  name: back-svc
  namespace: default
spec:
  type: NodePort
  ports:
  - port: 9000
    nodePort: 30049
    protocol: TCP
  selector:
    app: app
    part: back

```
And I the result is ? 
ingress.kubernetes.io/backends:       {""k8s-be-30049--17c7.....e101"":""UNHEALTHY"",""k8s-be-30651--17c7......e101"":""HEALTHY""}

So could you please to find the issue here ? ",closed,False,2018-07-13 09:35:13,2018-10-30 15:54:13
ingress-gce,ashi009,https://github.com/kubernetes/ingress-gce/issues/397,https://api.github.com/repos/kubernetes/ingress-gce/issues/397,Feature Request: Name GCP resources better,"It's annoying to see a backend being referred as `k8s-be-31073--d4ff5a71b6e25bd6`, or an instance group being named as `k8s-ig--d4ff5a71b6e25bd6`.  All these names are totally meaningless on the GKE console.

Is it possible to give them some more meaningful names?
",open,False,2018-07-13 10:29:52,2018-11-30 17:22:36
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/398,https://api.github.com/repos/kubernetes/ingress-gce/issues/398,Remove error return value from controller initialization,"/assign @nicksardo 

The functions which create the LB controller and the NEG controller return an error for no apparent reason",closed,True,2018-07-13 22:03:03,2018-07-16 23:35:29
ingress-gce,evanlucas,https://github.com/kubernetes/ingress-gce/issues/399,https://api.github.com/repos/kubernetes/ingress-gce/issues/399,Add annotation for specifying backend-service timeout,"The backend-services created in gke when using this seem to reset the timeout back to 30 under certain circumstances (which I haven't yet figured out). Any chance an annotation could be provided that would allow this to be set? For context, we are using these for websockets, so the default of 30 seconds is not really desirable. I'd be happy to make the changes if this is something yall would be interested in allowing, although, I'm not sure what the preferred annotation name would be.

Thanks!",closed,False,2018-07-14 17:48:59,2018-07-14 22:56:55
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/400,https://api.github.com/repos/kubernetes/ingress-gce/issues/400,BackendService Sync bug in 1.2,"Somehow, the backend pool is not getting the correct service ports to sync. 

The logs in question is below. See the highlighted log lines.


I0715 22:08:29.085212       1 controller.go:122] Ingress default/weather-frontend added, enqueuing
I0715 22:08:29.087433       1 controller.go:268] Syncing default/weather-frontend
I0715 22:08:29.088726       1 event.go:218] Event(v1.ObjectReference{Kind:""Ingress"", Namespace:""default"", Name:""weather-frontend"", UID:""9a9766fb-887b-11e8-8139-42010a8000e2"", APIVersion:""extensions"", ResourceVersion:""338003"", FieldPath:""""}): type: 'Normal' reason: 'ADD' default/weather-frontend
I0715 22:08:29.089596       1 controller.go:322] Updating NEG visibility annotation ""{\""network_endpoint_groups\"":{\""80\"":\""k8s1-d5fe052f-default-weather-api-80-81f38065\""},\""zones\"":[\""us-central1-b\""]}"" on service default/weather-api.
I0715 22:08:29.101494       1 manager.go:83] EnsureSyncer default/weather-api: removing map[] ports, adding map[80:80] ports
I0715 22:08:29.101536       1 syncer.go:80] New syncer for service default/weather-api port 80 NEG ""k8s1-d5fe052f-default-weather-api-80-81f38065""
I0715 22:08:29.101548       1 syncer.go:113] Starting NEG syncer for service port default/weather-api-80/80
I0715 22:08:29.101588       1 controller.go:246] Syncing service ""default/weather-frontend""
I0715 22:08:29.101932       1 syncer.go:196] Sync NEG ""k8s1-d5fe052f-default-weather-api-80-81f38065"" for default/weather-api-80/80.
I0715 22:08:29.105143       1 controller.go:322] Updating NEG visibility annotation ""{\""network_endpoint_groups\"":{\""80\"":\""k8s1-d5fe052f-default-weather-frontend-80-89bd2927\""},\""zones\"":[\""us-central1-b\""]}"" on service default/weather-frontend.
I0715 22:08:29.111893       1 manager.go:83] EnsureSyncer default/weather-frontend: removing map[] ports, adding map[80:80] ports
I0715 22:08:29.111954       1 syncer.go:80] New syncer for service default/weather-frontend port 80 NEG ""k8s1-d5fe052f-default-weather-frontend-80-89bd2927""
I0715 22:08:29.111968       1 syncer.go:113] Starting NEG syncer for service port default/weather-frontend-80/80
I0715 22:08:29.111997       1 controller.go:246] Syncing service ""default/weather-api""
I0715 22:08:29.115084       1 syncer.go:196] Sync NEG ""k8s1-d5fe052f-default-weather-frontend-80-89bd2927"" for default/weather-frontend-80/80.
I0715 22:08:29.115884       1 manager.go:83] EnsureSyncer default/weather-api: removing map[] ports, adding map[] ports
I0715 22:08:29.116322       1 controller.go:246] Syncing service ""default/weather-frontend""
I0715 22:08:29.118509       1 manager.go:83] EnsureSyncer default/weather-frontend: removing map[] ports, adding map[] ports
I0715 22:08:29.185277       1 instances.go:94] Creating instance group us-central1-b/k8s-ig--d5fe052f3ca01df6.
I0715 22:08:31.014116       1 instances.go:137] Instance group us-central1-b/k8s-ig--d5fe052f3ca01df6 does not have ports [30013], adding them now.
I0715 22:08:31.665468       1 instances.go:230] Adding nodes [gke-test-default-pool-643e9c7e-qdsz gke-test-default-pool-643e9c7e-rk8l gke-test-default-pool-643e9c7e-t0tt] to k8s-ig--d5fe052f3ca01df6 in zone us-central1-b
**I0715 22:08:32.384980       1 backends.go:249] Sync: backends [{{kube-system/default-http-backend {1 0 http}} 30013 80 HTTP 8080 false <nil>} {{default/weather-frontend {0 80 }} 0 80 HTTP 80 true <nil>}]**
I0715 22:08:32.580493       1 healthchecks.go:149] Creating health check for port 30013 with protocol HTTP
I0715 22:08:34.476449       1 backends.go:294] Creating backend service for port 30013 named k8s-be-30013--d5fe052f3ca01df6
I0715 22:08:34.486190       1 composite.go:56] Creating ga backend service k8s-be-30013--d5fe052f3ca01df6
I0715 22:08:37.384209       1 backends.go:487] Backend service ""k8s-be-30013--d5fe052f3ca01df6"" has instance groups [], want [projects/mixia-cube/zones/us-central1-b/instanceGroups/k8s-ig--d5fe052f3ca01df6]
I0715 22:08:37.384994       1 composite.go:82] Updating ga backend service k8s-be-30013--d5fe052f3ca01df6
I0715 22:08:40.277462       1 healthchecks.go:142] Creating beta health check with protocol HTTP
I0715 22:08:41.922219       1 backends.go:294] Creating backend service for port 0 named k8s1-d5fe052f-default-weather-frontend-80-89bd2927
I0715 22:08:41.922320       1 composite.go:49] Creating beta backend service k8s1-d5fe052f-default-weather-frontend-80-89bd2927
I0715 22:08:45.082228       1 l7s.go:116] Syncing load balancer default/weather-frontend
I0715 22:08:45.087210       1 l7s.go:67] Creating l7 default-weather-frontend--d5fe052f3ca01df6
I0715 22:08:45.242218       1 url_maps.go:51] Creating url map k8s-um-default-weather-frontend--d5fe052f3ca01df6 for backend k8s-be-30013--d5fe052f3ca01df6
I0715 22:08:47.476854       1 target_proxies.go:39] Creating new http proxy for urlmap k8s-um-default-weather-frontend--d5fe052f3ca01df6
I0715 22:08:50.093687       1 forwarding_rules.go:72] Creating forwarding rule for proxy ""https://www.googleapis.com/compute/v1/projects/mixia-cube/global/targetHttpProxies/k8s-tp-default-weather-frontend--d5fe052f3ca01df6"" and ip :80-80
I0715 22:08:56.799437       1 firewalls.go:89] Creating firewall rule ""k8s-fw-l7--d5fe052f3ca01df6""
E0715 22:09:05.382941       1 taskqueue.go:85] Requeuing ""default/weather-frontend"" due to error: googleapi: Error 404: The resource 'projects/mixia-cube/global/backendServices/k8s1-d5fe052f-default-weather-api-80-81f38065' was not found, notFound (ingresses)
I0715 22:09:05.382999       1 controller.go:268] Syncing default/weather-frontend
**I0715 22:09:05.385375       1 event.go:218] Event(v1.ObjectReference{Kind:""Ingress"", Namespace:""default"", Name:""weather-frontend"", UID:""9a9766fb-887b-11e8-8139-42010a8000e2"", APIVersion:
""extensions"", ResourceVersion:""338003"", FieldPath:""""}): type: 'Warning' reason: 'Sync' Error during sync: googleapi: Error 404: The resource 'projects/mixia-cube/global/backendServices/k
8s1-d5fe052f-default-weather-api-80-81f38065' was not found, notFound**
**I0715 22:09:05.631611       1 backends.go:249] Sync: backends [{{kube-system/default-http-backend {1 0 http}} 30013 80 HTTP 8080 false <nil>} {{default/weather-api {0 80 }} 0 80 HTTP 80 
true <nil>}]**
I0715 22:09:05.806434       1 healthchecks.go:130] Health check k8s-be-30013--d5fe052f3ca01df6 already exists and has the expected path /healthz
I0715 22:09:05.985606       1 healthchecks.go:142] Creating beta health check with protocol HTTP
I0715 22:09:07.897490       1 backends.go:294] Creating backend service for port 0 named k8s1-d5fe052f-default-weather-api-80-81f38065
I0715 22:09:07.897596       1 composite.go:49] Creating beta backend service k8s1-d5fe052f-default-weather-api-80-81f38065
I0715 22:09:10.375592       1 l7s.go:116] Syncing load balancer default/weather-frontend
I0715 22:09:10.476402       1 url_maps.go:46] Url map k8s-um-default-weather-frontend--d5fe052f3ca01df6 already exists
I0715 22:09:17.191068       1 url_maps.go:167] Updating URLMap: ""k8s-um-default-weather-frontend--d5fe052f3ca01df6""
I0715 22:09:18.896482       1 controller.go:419] Updating loadbalancer default/weather-frontend with IP 130.211.15.1
I0715 22:09:18.902523       1 event.go:218] Event(v1.ObjectReference{Kind:""Ingress"", Namespace:""default"", Name:""weather-frontend"", UID:""9a9766fb-887b-11e8-8139-42010a8000e2"", APIVersion:
""extensions"", ResourceVersion:""338003"", FieldPath:""""}): type: 'Normal' reason: 'CREATE' ip: 130.211.15.1
I0715 22:09:18.905848       1 controller.go:246] Syncing service ""default/weather-api""
I0715 22:09:18.907233       1 controller.go:144] Ingress default/weather-frontend changed, enqueuing
```",closed,False,2018-07-15 22:35:21,2018-07-27 18:02:55
ingress-gce,nicksardo,https://github.com/kubernetes/ingress-gce/pull/401,https://api.github.com/repos/kubernetes/ingress-gce/issues/401,Pace operation polling,Example ingress creation: https://pastebin.com/raw/HApszAgu,closed,True,2018-07-16 18:42:24,2018-07-17 16:11:08
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/402,https://api.github.com/repos/kubernetes/ingress-gce/issues/402,uniq function should compare more than NodePort difference,cc: @agau4779 ,closed,True,2018-07-17 00:01:30,2018-07-17 23:35:57
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/403,https://api.github.com/repos/kubernetes/ingress-gce/issues/403,Extract firewall management into separate controller,"This PR attempts to move the management of the L7 firewall rule into a separate controller. 

Summary of changes:

1. Add controller.go to pkg/firewalls. This new controller also creates and interacts with the firewall pool, rather than the ClusterManager. Currently, the controller watches ingresses and services and updates the firewall rule as necessary. In the future, we can extend this to support the firewall rules for expose NEG.

2. Remove all firewall related stuff from ClusterManager and pkg/controller/controller.go

3. Move IsHealthy from ClusterManager to the controller the health check is meant for. It really has notplace in ClusterManager.

This leads well into a followup PR which can remove the ClusterManager entirely. Open to discussion on some of the design decisions in the PR. /assign @nicksardo ",closed,True,2018-07-17 16:30:30,2018-07-24 18:24:58
ingress-gce,buckhx,https://github.com/kubernetes/ingress-gce/issues/404,https://api.github.com/repos/kubernetes/ingress-gce/issues/404,External CNAME records do not route properly via hostnames,"Using CNAME records to route to services via hostnames does not work. With the following setup, dev.test.io will 404. Changing the records to both be A records in the external DNS pointing to the same static IP routes properly with the same Ingress definition.

Example Zones:
```
test.io            A              <static-ip-of-glb>
dev.test.io     CNAME    test.io 
```

Example Ingress:
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: gateway
  namespace: dev 
  annotations:
    kubernetes.io/ingress.class: gce 
spec:
  tls:
  - hosts:
    - dev.test.io
    - test.io
  rules:
  - host: test.io
    http:
      paths:
      - path: /
        backend:
          serviceName: web 
          servicePort: 80
  - host: dev.test.io
    http:
      paths:
      - path: /
        backend:
          serviceName: web-dev
          servicePort: 80
```",closed,False,2018-07-18 14:51:34,2018-07-18 15:35:25
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/405,https://api.github.com/repos/kubernetes/ingress-gce/issues/405,Introduce ControllerContextConfig and move some command-line tunable stuff there," This PR is one of many that aim to eliminate the need for ClusterManager. In this PR, I introduce ControllerContextConfig which simply encapsulates the values for some command line flags, most notably the default backend service port ID. This eliminates the need for ClusterManager to keep track of it.

/assign @nicksardo ",closed,True,2018-07-18 15:45:46,2018-07-18 16:00:43
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/406,https://api.github.com/repos/kubernetes/ingress-gce/issues/406,Remove EnsureLB from ClusterManager,"Yet another PR to slowly remove ClusterManager from existence. This PR inlines the EnsureLB call into the controller itself.

/assign @nicksardo ",closed,True,2018-07-18 16:32:51,2018-07-18 16:39:02
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/407,https://api.github.com/repos/kubernetes/ingress-gce/issues/407,cherrypick #402 to release-1.2 branch,,closed,True,2018-07-18 18:08:22,2018-07-18 18:46:19
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/408,https://api.github.com/repos/kubernetes/ingress-gce/issues/408,Raw patch to cloud provider to fix operations issue,"Raw patch on cloud provider.

Ref: https://github.com/kubernetes/kubernetes/pull/66400",closed,True,2018-07-19 20:56:24,2018-07-19 22:10:58
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/409,https://api.github.com/repos/kubernetes/ingress-gce/issues/409,Cherry pick on release-1.2 for #408,,closed,True,2018-07-19 20:57:22,2018-07-19 22:13:22
ingress-gce,chrishiestand,https://github.com/kubernetes/ingress-gce/issues/410,https://api.github.com/repos/kubernetes/ingress-gce/issues/410,ingress controller gave me 2 IP addresses instead of 1 when I added TLS,"I was simply running the ingress on WAN port 80 and everything was working fine. Then at the same time I added a tls cert and this annotation to an ingress manifest:

```yaml
  kubernetes.io/ingress.allow-http: ""false""
```
```yaml
    tls:
    - hosts:
      - mysite.com
      - www.mysite.com
      secretName: tls-mysite-com-prod
```

And now the kubernetes ingress object reports that it is listening on both ports, but in fact the original IP address only listens to port 80. When I log into the GCP console, I can see that my port 443 frontend has a different IP than the port 80 frontend.

The k8s cluster is GKE v1.10.5-gke.2.

I do not think this is supposed to happen. Email me if you want specifics (username at gmail), I don't want to publicly post the project details today.",closed,False,2018-07-20 07:17:01,2018-12-21 20:59:31
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/411,https://api.github.com/repos/kubernetes/ingress-gce/issues/411,Fix issue with yaml in glbc deploy/,/assign @MrHohn ,closed,True,2018-07-24 16:06:05,2018-07-24 16:46:24
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/412,https://api.github.com/repos/kubernetes/ingress-gce/issues/412,Bump timeout in tests to match reality,/assign @rramkumar1 ,closed,True,2018-07-24 17:02:44,2018-07-24 17:08:10
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/413,https://api.github.com/repos/kubernetes/ingress-gce/issues/413,Rename context's hcLock to lock,"Makes it easier for other stuff to use the lock :)

/assign @MrHohn ",closed,True,2018-07-25 18:30:19,2018-07-25 19:08:50
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/414,https://api.github.com/repos/kubernetes/ingress-gce/issues/414,unit test: add locking when read from shared map,"/assign @rramkumar1 
cc @bowei ",closed,True,2018-07-25 22:33:28,2018-07-25 22:37:51
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/415,https://api.github.com/repos/kubernetes/ingress-gce/issues/415,Expose newIndexer in pkg/context,/assign @MrHohn ,closed,True,2018-07-25 23:38:05,2018-07-25 23:50:05
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/416,https://api.github.com/repos/kubernetes/ingress-gce/issues/416,Add doc link for backend config,"Add a reference link for now.
@rramkumar1 @bowei ",closed,True,2018-07-30 17:34:29,2018-07-30 17:46:18
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/417,https://api.github.com/repos/kubernetes/ingress-gce/issues/417,Introduce cloud.google.com/app-protocols to eventually replace existing annotation,/assign @bowei ,closed,True,2018-07-31 23:36:30,2018-08-13 21:47:10
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/418,https://api.github.com/repos/kubernetes/ingress-gce/issues/418,Remove all code related to legacy health checks,"Fixes #269
",closed,True,2018-08-01 15:10:10,2018-08-14 05:23:04
ingress-gce,Kronuz,https://github.com/kubernetes/ingress-gce/issues/419,https://api.github.com/repos/kubernetes/ingress-gce/issues/419,Multiple pre-shared certificates not working,"In the documentation (https://github.com/kubernetes/ingress-gce/blob/fea549960931d72f6bd9505796d75ea8a085f92b/README.md#gcp-ssl-cert) it says multiple pre-shared certs can be specified as follows:

```yaml
  annotations:
      ingress.gcp.kubernetes.io/pre-shared-cert: ""my-certificate-1, my-certificate-2, my-certificate-3""
```

However, when I try that, I get the following error and certificates are not loaded:
```
Warning  GCE      4s                  loadbalancer-controller  googleapi: Error 400: Invalid value 'my-certificate-1, my-certificate-2, my-certificate-3'. Values must match the following regular expression: '[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?', invalidParameter
```

Version is:
```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.7"", GitCommit:""dd5e1a2978fd0b97d9b78e1564398aeea7e7fe92"", GitTreeState:""clean"", BuildDate:""2018-04-19T00:05:56Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.7-gke.3"", GitCommit:""9b5b719c5f295c99de68ffb5b63101b0e0175376"", GitTreeState:""clean"", BuildDate:""2018-05-31T18:32:23Z"", GoVersion:""go1.9.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```",closed,False,2018-08-02 19:29:16,2018-08-14 05:28:17
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/420,https://api.github.com/repos/kubernetes/ingress-gce/issues/420,Move joiner methods into context,"Realized that the need for an extra type to encapsulate the join methods was kind of unnecessary. Moved those methods into ControllerContext. 

/assign @MrHohn ",closed,True,2018-08-06 15:21:59,2018-08-06 16:53:57
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/421,https://api.github.com/repos/kubernetes/ingress-gce/issues/421,Remove EnsureInstanceGroupsAndPorts wrapper func,"The EnsureInstanceGroupsAndPorts wrapper func seems quite unnecessary given that the cluster manager itself can call the interface method directly.

/assign @MrHohn ",closed,True,2018-08-06 16:26:57,2018-08-06 16:56:25
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/422,https://api.github.com/repos/kubernetes/ingress-gce/issues/422,Refactor to remove ClusterManager completely ,"This PR removes the ClusterManager completely from existence. All resource pools that were managed in the ClusterManager are now directly managed by the LB controller. Furthermore, all wrapper functions (EnsureInstanceGroupsAndPorts) are implemented by the LB controller itself.

Note: Still need to figure out unit test failure.

/assign @bowei ",closed,True,2018-08-07 02:48:05,2018-08-07 22:16:52
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/423,https://api.github.com/repos/kubernetes/ingress-gce/issues/423,export TrimFieldsEvenly,,closed,True,2018-08-07 16:52:50,2018-08-15 17:59:08
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/424,https://api.github.com/repos/kubernetes/ingress-gce/issues/424,Extract BackendPool interface into three 3 separate interfaces,"This PR extracts the BackendPool interface into 3 separate interfaces:

    1. BackendPool - Simple CRUD for backends
    2. Syncer - Sync and GC backends
    3. Linker - Link backends to their associated groups. 

This allows us to break the current BackendPool implementation up into more manageable chunks and each chunk is much more testable. Furthermore, some implementations, namely BackendPool and Syncer, can be easily reused. 

/assign @bowei ",closed,True,2018-08-07 22:22:05,2018-08-13 23:29:31
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/425,https://api.github.com/repos/kubernetes/ingress-gce/issues/425,Push dependency on the GCECloud up out of the neg controller,,closed,True,2018-08-07 22:22:22,2018-08-08 18:13:10
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/issues/426,https://api.github.com/repos/kubernetes/ingress-gce/issues/426,ingress-gce-image-push job is failing,"From https://k8s-testgrid.appspot.com/sig-network-ingress-gce-e2e#ingress-gce-image-push, job is constantly failing since today. Probably caused by a recent test infra change (https://github.com/kubernetes/test-infra/pull/8945). Will take another look.

/assign",closed,False,2018-08-07 23:33:44,2018-08-07 23:54:58
ingress-gce,dims,https://github.com/kubernetes/ingress-gce/issues/427,https://api.github.com/repos/kubernetes/ingress-gce/issues/427,Deprecate echoserver and add new container image based on cmd/echo ,please see discussion in https://github.com/kubernetes/kubernetes/pull/67035,open,False,2018-08-08 12:53:34,2019-02-21 00:19:12
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/428,https://api.github.com/repos/kubernetes/ingress-gce/issues/428,Introduce a new interface to encapsulate Ingress sync and controller implementation of the sync,"This PR introduces one high level sync interface and an interface for controllers to implement the sync. 
The benefit of this PR is that it enforces a simple contract for controllers to follow and makes creating controller mocks much easier. For example, the existing controller in pkg/controller/controller.go would be modified to implement the Controller interface. 

Note that this PR works at a layer above #424. A controller would potentially implement SyncBackends using the new interfaces defined in pkg/backends.

Also note that this abstraction will make it easier to parallelize syncs although the current implementation would need to be rewired a bit.

The second commit attempts to use the new interfaces in pkg/controller the way the code is currently structured.

/assign @bowei ",closed,True,2018-08-08 19:30:33,2018-08-23 20:10:41
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/429,https://api.github.com/repos/kubernetes/ingress-gce/issues/429,move NewIndexer to utils,,closed,True,2018-08-10 20:55:07,2018-08-10 21:57:13
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/430,https://api.github.com/repos/kubernetes/ingress-gce/issues/430,Remove unused named ports from instance group's ,Fixes #43 ,closed,True,2018-08-13 17:28:53,2018-08-21 18:28:55
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/431,https://api.github.com/repos/kubernetes/ingress-gce/issues/431,Fix bug in backend syncer where backend service was being created without health check,"Introduced by #424. Looks like we can't create a BackendService without a health check. Fixed the BackendPool.Create implementation to now take in a health check link.

/assign @bowei ",closed,True,2018-08-14 05:16:44,2018-08-14 06:35:45
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/432,https://api.github.com/repos/kubernetes/ingress-gce/issues/432,Add a version mapping for both GCE and GKE clusters,"Fixes #395 

/assign @bowei ",closed,True,2018-08-14 15:22:45,2018-08-15 14:34:06
ingress-gce,lbernail,https://github.com/kubernetes/ingress-gce/issues/433,https://api.github.com/repos/kubernetes/ingress-gce/issues/433,"When using NEG services only, the controller still creates instance groups","Hello

We have been testing the NEG feature for a little while and I noticed that even when we only have services using NEGs, the controller still creates the instance groups (they are not attached to the backend-services which only point to NEGs)

Is this expected?

Laurent",closed,False,2018-08-14 15:40:47,2018-08-22 00:00:37
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/434,https://api.github.com/repos/kubernetes/ingress-gce/issues/434,Fix null-pointer exception in url map ensure logic,"The previous code could potentially have a NPE if l.um = nil.  Modified the code to use the Name field of the L7 struct instead, which we know will always be non-nil.

Example error log:

```I0814 11:19:06.891594       1 l7s.go:66] Creating l7 e2e-tests-ingress-ckx44-hostname--bd7d411a662def7f
E0814 11:19:07.199597       1 runtime.go:66] Observed a panic: ""invalid memory address or nil pointer dereference"" (runtime error: invalid memory address or nil pointer dereference)
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:72
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:65
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:51
/usr/local/go/src/runtime/asm_amd64.s:573
/usr/local/go/src/runtime/panic.go:502
/usr/local/go/src/runtime/panic.go:63
/usr/local/go/src/runtime/signal_unix.go:388
/go/src/k8s.io/ingress-gce/pkg/loadbalancers/url_maps.go:66
/go/src/k8s.io/ingress-gce/pkg/loadbalancers/l7.go:122
/go/src/k8s.io/ingress-gce/pkg/loadbalancers/l7s.go:88
/go/src/k8s.io/ingress-gce/pkg/controller/controller.go:370
/go/src/k8s.io/ingress-gce/pkg/controller/controller.go:297
/go/src/k8s.io/ingress-gce/pkg/controller/controller.go:115
/go/src/k8s.io/ingress-gce/pkg/utils/taskqueue.go:90
/go/src/k8s.io/ingress-gce/pkg/utils/taskqueue.go:58
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88
/go/src/k8s.io/ingress-gce/pkg/utils/taskqueue.go:58
/usr/local/go/src/runtime/asm_amd64.s:2361
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x1693311]
```

/assign @MrHohn ",closed,True,2018-08-14 18:40:17,2018-08-14 18:45:58
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/435,https://api.github.com/repos/kubernetes/ingress-gce/issues/435,BackendPool Update() should set backend service version,"Makes it easier to not make a mistake of forgetting to set version when calling Update(). All logic is now encapsulated in Update() implementation itself.

/assign @bowei ",closed,True,2018-08-15 01:37:47,2018-08-15 01:48:55
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/436,https://api.github.com/repos/kubernetes/ingress-gce/issues/436,Fix error handling in controller sync(),"In the current logic, it appears that errors during garbage collection at the end of a sync were being suppressed. Now, any errors during GC should be logged properly.

/assign @MrHohn ",closed,True,2018-08-15 15:35:30,2018-08-15 17:04:36
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/437,https://api.github.com/repos/kubernetes/ingress-gce/issues/437,Add myself to OWNERS so I can do a release.,/assign @MrHohn ,closed,True,2018-08-15 18:24:46,2018-08-15 18:27:38
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/issues/438,https://api.github.com/repos/kubernetes/ingress-gce/issues/438,ci-ingress-gce-e2e-scale is failing,"Ingress scale job (https://k8s-testgrid.appspot.com/sig-network-ingress-gce-e2e#ingress-gce-e2e-scale) has been failing since 08-13. Most likely due to resource leakage. Sample failure:
```
[sig-network] Loadbalancing: L7 Scalability GCE [Slow] [Serial] [Feature:IngressScale] Creating and updating ingresses should happen promptly with small/medium/large amount of ingresses 2h41m
go run hack/e2e.go -v --test --test_args='--ginkgo.focus=\[sig\-network\]\sLoadbalancing\:\sL7\sScalability\sGCE\s\[Slow\]\s\[Serial\]\s\[Feature\:IngressScale\]\sCreating\sand\supdating\singresses\sshould\shappen\spromptly\swith\ssmall\/medium\/large\samount\sof\singresses$'

/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingress_scale.go:57
Aug 16 02:13:01.499: Unexpected error while running ingress scale test: [Ingress failed to acquire an IP address within 1h20m0s Ingress failed to acquire an IP address within 1h20m0s Ingress failed to acquire an IP address within 1h20m0s Ingress failed to acquire an IP address within 1h20m0s Ingress failed to acquire an IP address within 1h20m0s Ingress failed to acquire an IP address within 1h20m0s]
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingress_scale.go:59
```

/assign",closed,False,2018-08-16 17:26:27,2018-09-19 23:49:07
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/439,https://api.github.com/repos/kubernetes/ingress-gce/issues/439,Cherry-pick necessary commits for v1.3.0 release.,/assign @bowei ,closed,True,2018-08-16 21:02:16,2018-08-16 22:44:32
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/440,https://api.github.com/repos/kubernetes/ingress-gce/issues/440,do not create instance group with NEG backends,ref: https://github.com/kubernetes/ingress-gce/issues/433,closed,True,2018-08-16 22:02:08,2018-08-21 22:51:34
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/441,https://api.github.com/repos/kubernetes/ingress-gce/issues/441,Fix main controller health check,"Ignore a 404 error since we are purposefully trying to get a backend that does not exist.

/assign @MrHohn ",closed,True,2018-08-16 23:26:17,2018-08-16 23:37:01
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/442,https://api.github.com/repos/kubernetes/ingress-gce/issues/442,Remove 'Description' and 'Required' from OpenAPI schema root layer,"This is for fixing an incompatibility issue with https://github.com/kubernetes/features/issues/571.

/assign @rramkumar1 
cc @Liujingfang1

<s>Note: The `Description` entry is manually removed, as adding `omitempty` doesn't seem to stop generating it.</s> Fixed by removing struct comment.",closed,True,2018-08-20 18:37:45,2018-08-20 20:19:30
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/443,https://api.github.com/repos/kubernetes/ingress-gce/issues/443,Move NegStatus and PortNameMap to pkg/neg/types,/assign @freehan ,closed,True,2018-08-20 23:32:22,2018-08-21 18:38:40
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/444,https://api.github.com/repos/kubernetes/ingress-gce/issues/444,Cherrypick #442 into release-1.3,Cherrypicking https://github.com/kubernetes/ingress-gce/pull/442 so it can make the next patch release.,closed,True,2018-08-21 20:28:34,2018-08-21 21:12:47
ingress-gce,jainishshah17,https://github.com/kubernetes/ingress-gce/issues/445,https://api.github.com/repos/kubernetes/ingress-gce/issues/445,Allow configuration-snippet Annotation.,"I have need to do URL rewrite to use my application. I want to do following:

```
      rewrite ^/(v2)/token /artifactory/api/docker/null/v2/token;
      rewrite ^/(v2)/([^\/]*)/(.*) /artifactory/api/docker/$2/$1/$3;
```

It will be great to have support for `configuration-snippet`.",closed,False,2018-08-22 00:13:53,2018-10-30 15:58:38
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/446,https://api.github.com/repos/kubernetes/ingress-gce/issues/446,Add some utility functions to support finalizers,"Adding some functions needed to support finalizers. This code can easily be reused by other modules.

Much of this code was adapted from https://github.com/kubernetes/kubernetes/pull/54569

/assign @bowei @MrHohn ",closed,True,2018-08-22 03:40:54,2018-08-22 22:35:26
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/447,https://api.github.com/repos/kubernetes/ingress-gce/issues/447,Export getPatchBytes in pkg/util,/assign @MrHohn ,closed,True,2018-08-22 22:56:42,2018-08-22 23:05:26
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/448,https://api.github.com/repos/kubernetes/ingress-gce/issues/448,Slight refactor of Controller interface to eliminate Ingress type specifically,"This PR tries to eliminate the Ingress type from being referenced in the Controller and Syncer interfaces in pkg/sync. This ensures that implementations of the Controller interface can use any Ingress implementation.

/assign @bowei ",closed,True,2018-08-24 00:59:50,2018-08-24 01:12:56
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/449,https://api.github.com/repos/kubernetes/ingress-gce/issues/449,Move joinErrs() to utils and export it,/assign @MrHohn ,closed,True,2018-08-24 23:30:10,2018-08-24 23:39:20
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/450,https://api.github.com/repos/kubernetes/ingress-gce/issues/450,deflake TestGetNodePortsUsedByIngress unit test,,closed,True,2018-08-27 19:11:25,2018-08-27 19:15:47
ingress-gce,prameshj,https://github.com/kubernetes/ingress-gce/pull/451,https://api.github.com/repos/kubernetes/ingress-gce/issues/451,Do not truncate tls certs based on target proxy limit,Hardcoding limit means more changes in ingress when targetproxy limit is increased.,closed,True,2018-08-27 22:39:30,2018-09-04 18:03:24
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/452,https://api.github.com/repos/kubernetes/ingress-gce/issues/452,fix and refactor on NEG annotation handling,"Refactor NEG annotation handling
Fix a problem where NEG annotation mal format errors are eaten. 

cc: @agau4779 ",closed,True,2018-08-28 22:22:41,2018-08-29 01:09:05
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/453,https://api.github.com/repos/kubernetes/ingress-gce/issues/453,Add JSONMerge patch utilities and move some files around,"This PR adds a JSON Merge patch function in addition to the previous strategic merge patch function.

Also, move some files around.

/assign @MrHohn ",closed,True,2018-08-29 16:54:25,2018-08-29 17:08:29
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/454,https://api.github.com/repos/kubernetes/ingress-gce/issues/454,Cherrypick #452 into release-1.3,,closed,True,2018-08-29 18:18:56,2018-08-29 18:23:47
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/455,https://api.github.com/repos/kubernetes/ingress-gce/issues/455,Remove JSONMergePatch util,/assign @MrHohn ,closed,True,2018-08-29 20:13:36,2018-08-29 20:37:58
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/456,https://api.github.com/repos/kubernetes/ingress-gce/issues/456,NEG controller bug fix,,closed,True,2018-08-30 20:40:59,2018-08-30 20:45:55
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/457,https://api.github.com/repos/kubernetes/ingress-gce/issues/457,Cherrypick #456 into release-1.3,,closed,True,2018-08-30 20:44:00,2018-08-30 20:48:32
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/458,https://api.github.com/repos/kubernetes/ingress-gce/issues/458,Refactor some uses of snapshotter.Add() to use bool rather than real object,"This will be the first of a series of refactors to the core usage of the Snapshotter.

I also have a bigger refactor in the works for pkg/loadbalancers so I will apply this same change there in a followup.

/assign @bowei 
",closed,True,2018-08-31 17:43:28,2018-08-31 20:25:32
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/459,https://api.github.com/repos/kubernetes/ingress-gce/issues/459,Harden NEG GC,"Modified 2 things:
- make NEG GC more frequent
- delay initial NEG GC to allow services to be fully processed. ",closed,True,2018-08-31 18:11:37,2018-09-13 17:45:51
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/460,https://api.github.com/repos/kubernetes/ingress-gce/issues/460,Refactor LoadBalancerPool to use cloud listing snapshotter,"Currently the L7 pool uses an in memory snapshotter. This is problematic because if the controller crashes, the in memory state is lost and load balancer resources could potentially become orphaned. 

This PR refactors the L7 pool to use the cloud listing snapshotter (similar to pkg/backends). By using the cloud lister, we can fetch all LB names from GCP and perform our GC against those names.

/assign @bowei ",closed,True,2018-08-31 20:20:44,2019-01-04 18:44:12
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/461,https://api.github.com/repos/kubernetes/ingress-gce/issues/461,Refactor Ingress Filtering and Ingress Backend Traversal,,closed,True,2018-08-31 21:51:35,2018-08-31 23:22:54
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/462,https://api.github.com/repos/kubernetes/ingress-gce/issues/462,Cherrypick #461 into release 1.3,,closed,True,2018-08-31 23:24:39,2018-08-31 23:28:49
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/463,https://api.github.com/repos/kubernetes/ingress-gce/issues/463,Support short names in CRD Meta. Allows for abbreviating CRD's in kubectl,"For example, ""kubectl get backendconfig"" could be ""kubectl get bcfg""

/assign @MrHohn ",closed,True,2018-09-04 17:11:41,2018-09-04 17:16:27
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/464,https://api.github.com/repos/kubernetes/ingress-gce/issues/464,"Revert ""Refactor some uses of snapshotter.Add() to use bool rather than real object""","Reverts kubernetes/ingress-gce#458

This is causing e2e tests to fail because it depends on #460. Will submit this later.

/assign @MrHohn ",closed,True,2018-09-04 18:26:48,2018-09-04 18:31:09
ingress-gce,prameshj,https://github.com/kubernetes/ingress-gce/issues/465,https://api.github.com/repos/kubernetes/ingress-gce/issues/465,"Disabling TLS on an active ingress needs to delete resources(target proxy, certs)",Currently these will not be garbage collected. Unused certs will be deleted only when there is a tls config update. ,open,False,2018-09-04 20:50:39,2019-01-29 19:17:11
ingress-gce,rweindl,https://github.com/kubernetes/ingress-gce/issues/466,https://api.github.com/repos/kubernetes/ingress-gce/issues/466,Issue with multiple domains and SSL certificates when using ingress-gce,"I am exposing a micro service running on GCP's kubernetes engine via Google's Ingress load balancer. According to [[Here]](https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-multi-ssl) the load balancer supports multiple SSL certificates for different domain names via SNI lookup.

However, only the first specified SSL certificate is returned and therefore I receive a `Your connection is not private` warning for a domain-b. I would love to make an ingress controller for several domains each with their own certificate.

My investigation shows that always the first specified SSL certificate is returned as the server certificate while performing the TLS handshake. (In the example below always certificate `domain-a-net` would be returned.

I verified that assumption as proposed in [Here](https://stackoverflow.com/questions/52142078/multiple-domains-and-ssl-certificates-with-gcps-kubernetes-ingress?noredirect=1#comment91337491_52142078) by calling the different domains with `curl -k -v <domain_name>`. Changing the order of the tls hosts will return a different certificate.

I tested it with two different `ingress.yaml` formats:

**Version 1:**
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: some-name
spec:  
  tls:
  - hosts: 
    - www.domain-a.net
    - domain-a.net
    secretName: domain-a-net
  - hosts: 
    - www.domain-b.org
    - domain-b.org
    secretName: domain-b-org
  backend:
    serviceName: some-name
    servicePort: 443
```

**Version 2:**
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress
spec:
  rules:
  - host: domain-a.net
    http:
      paths:
        - path: /*
          backend:
            serviceName: some-name
            servicePort: 443    
  - host: www.domain-a.net
    http:
      paths:
        - path: /*
          backend:
            serviceName: some-name
            servicePort: 443
  - host: www.domain-b.org
    http:
      paths:
        - path: /*
          backend:
            serviceName: some-name
            servicePort: 443
  - host: domain-b.org
    http:
      paths:
        - path: /*
          backend:
            serviceName: some-name
            servicePort: 443
  tls:
  - hosts:
    - www.domain-a.net
    - domain-a.net
    secretName: domain-a-net  
  - hosts:
    - www.domain-b.net
    - domain-b.net
    secretName: domain-b-net 
```

Thanks for taking a look into that issue. ",closed,False,2018-09-05 22:46:26,2018-11-05 16:05:02
ingress-gce,mirake,https://github.com/kubernetes/ingress-gce/pull/467,https://api.github.com/repos/kubernetes/ingress-gce/issues/467,Fix typo,,closed,True,2018-09-08 14:46:02,2018-09-08 14:48:11
ingress-gce,mirake,https://github.com/kubernetes/ingress-gce/pull/468,https://api.github.com/repos/kubernetes/ingress-gce/issues/468,Fix typo: permissable -> permissible,Signed-off-by: ruicao <ruicao@alauda.io>,closed,True,2018-09-08 14:50:47,2018-09-14 01:46:00
ingress-gce,rllin-fathom,https://github.com/kubernetes/ingress-gce/issues/469,https://api.github.com/repos/kubernetes/ingress-gce/issues/469,GKE ingress with https load balancer and IAP/security policy enabled,"I have an application that uses GKE Ingress for a load balancer.  Recently GKE started supporting declaring IAP support via `BackendConfig`.  I followed the documentation at [1] and [2].  However, now, GKE seems to hang while creating my Ingress.

Below is the yaml for my service, ingress and backendconfig.

`kubectl -n randall-test-1 get svc,ing,backendconfig -o yaml`

```
apiVersion: v1
items:
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      beta.cloud.google.com/backend-config: '{""default"": ""airflow-backend-config""}'
      service.alpha.kubernetes.io/app-protocols: '{""web"":""HTTPS""}'
    creationTimestamp: 2018-09-10T19:23:13Z
    name: airflow
    namespace: randall-test-1
    resourceVersion: ""2155724""
    selfLink: /api/v1/namespaces/randall-test-1/services/airflow
    uid: X-X-X-X-X
  spec:
    clusterIP: X.X.X.X
    externalTrafficPolicy: Cluster
    ports:
    - name: web
      nodePort: 30099
      port: 8080
      protocol: TCP
      targetPort: web
    selector:
      app: airflow
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubernetes.io/ingress.allow-http: ""false""
    creationTimestamp: 2018-09-10T19:23:13Z
    generation: 1
    name: airflow
    namespace: randall-test-1
    resourceVersion: ""2155721""
    selfLink: /apis/extensions/v1beta1/namespaces/randall-test-1/ingresses/airflow
    uid: X-X-X-X-X
  spec:
    backend:
      serviceName: airflow
      servicePort: 8080
    tls:
    - secretName: tls
  status:
    loadBalancer: {}
- apiVersion: cloud.google.com/v1beta1
  kind: BackendConfig
  metadata:
    clusterName: """"
    creationTimestamp: 2018-09-10T19:23:13Z
    generation: 1
    name: airflow-backend-config
    namespace: randall-test-1
    resourceVersion: ""2155728""
    selfLink: /apis/cloud.google.com/v1beta1/namespaces/randall-test-1/backendconfigs/airflow-backend-config
    uid: X-X-X-X-X
  spec:
    iap:
      enabled: true
      oauthclientCredentials:
        secretName: oauth2
kind: List
metadata:
  resourceVersion: """"
  selfLink: """"
```

The hang gives me no insight.

```cluster@master0:~/kube-config$ kubectl -n randall-test-1 describe ing
Name:             airflow
Namespace:        randall-test-1
Address:
Default backend:  airflow:8080 (X.X.X.X:8080)
TLS:
  tls terminates
Rules:
  Host  Path  Backends
  ----  ----  --------
  *     *     airflow:8080 (X.X.X.X:8080)
Annotations:
Events:
  Type    Reason  Age   From                     Message
  ----    ------  ----  ----                     -------
  Normal  ADD     6m    loadbalancer-controller  randall-test-1/airflow
```

However, in GKE console, I just get `Creating ingress` as a status for > 20 mins with no resolution.  I also check my `Load Balancers` in console and see nothing.

Any ideas what is happening or what else I can check?

I also tried to do this with just `securityPolicy` which is supposed to link the Load Balancer with a Cloud Armor policy.   This also doesn't work with a similar hang.

[1] https://cloud.google.com/iap/docs/enabling-kubernetes-howto
[2] https://cloud.google.com/kubernetes-engine/docs/concepts/backendconfig",closed,False,2018-09-10 20:22:55,2018-09-25 15:51:15
ingress-gce,prmmbr,https://github.com/kubernetes/ingress-gce/issues/470,https://api.github.com/repos/kubernetes/ingress-gce/issues/470,"GCE ingress stucks on ""Creating ingress"" status, existing ingresses don't update","GCE Ingress stucks on ""Creating ingress"" status.
Also changes on existing ingresses don't appear on load balancer.

Cluster version: `1.10.7-gke.1`

Ingress is creating without any custom properties:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.global-static-ip-name: dev-namespace
  creationTimestamp: 2018-09-11T08:53:30Z
  generation: 1
  name: main
  namespace: dev
  resourceVersion: ""36830410""
  selfLink: /apis/extensions/v1beta1/namespaces/dev/ingresses/main
  uid: 282ec501-b5a0-11e8-97b0-42010a8002e3
spec:
  rules:
  - host: backend1.domain
      paths:
      - backend:
          serviceName: backend1
          servicePort: 3001
  - host: backend2.domain
    http:
      paths:
      - backend:
          serviceName: backend2
          servicePort: 8080
  - host: backend3.domain
    http:
      paths:
      - backend:
          serviceName: backend3
          servicePort: 80
status:
  loadBalancer: {}
```

```
Name:             main
Namespace:        dev
Address:
Default backend:  default-http-backend:80 (10.52.2.12:8080)
Rules:
  Host                                  Path  Backends
  ----                                  ----  --------
  backend1.domain
                                           backend1:3001 (<none>)
  backend2.domain
                                           backend2:8080 (<none>)
  backend3.domain
                                           backend3:80 (<none>)
Annotations:
Events:
  Type    Reason  Age   From                     Message
  ----    ------  ----  ----                     -------
  Normal  ADD     58m   loadbalancer-controller  dev/main
  Normal  ADD     52m   loadbalancer-controller  dev/main
  Normal  ADD     46m   loadbalancer-controller  dev/main
  Normal  ADD     41m   loadbalancer-controller  dev/main
  Normal  ADD     35m   loadbalancer-controller  dev/main
  Normal  ADD     30m   loadbalancer-controller  dev/main
  Normal  ADD     24m   loadbalancer-controller  dev/main
  Normal  ADD     18m   loadbalancer-controller  dev/main
  Normal  ADD     13m   loadbalancer-controller  dev/main
  Normal  ADD     7m    loadbalancer-controller  dev/main
  Normal  ADD     59s   loadbalancer-controller  dev/main
```

I don't see any error/warn messages on cluster/balancer logs.
How can I find problem cause and fix it?
",closed,False,2018-09-11 13:02:38,2018-09-11 17:48:23
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/471,https://api.github.com/repos/kubernetes/ingress-gce/issues/471,Ingress-GCE has a nil pointer exception,"We are aware of a nil pointer issue in v1.3.2. This bug was actually fixed in #434 but did not make it into the 1.3 release branch. Since this nil pointer crashes the controller, the issue is not surfaced to users other than Ingresses not being synced. 

The current workaround is to delete the Ingress which is not being synced and recreate it. A fix will be coming in the next 1.3 patch release (v1.3.3)",closed,False,2018-09-11 16:54:24,2019-01-21 12:46:15
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/472,https://api.github.com/repos/kubernetes/ingress-gce/issues/472,Cherrypick of #434 on release 1.3,"/assign @freehan 

Fixes #471",closed,True,2018-09-12 15:44:32,2018-09-12 21:52:17
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/473,https://api.github.com/repos/kubernetes/ingress-gce/issues/473,Cherrypick #388 on release 1.3 branch,"/assign @bowei 

FYI: @munnerz ",closed,True,2018-09-12 17:26:54,2018-09-13 21:16:35
ingress-gce,addisonbair,https://github.com/kubernetes/ingress-gce/issues/474,https://api.github.com/repos/kubernetes/ingress-gce/issues/474,"Addon-manager ""Reconcile"" annotation deletes default-http-backend service and deployment","After deploying GLBC via `deploy/glbc/script.sh`, I noticed the service and deployment for the default-http-backend was getting deleted after approximately 15 seconds.

Since I don't have access to the masters (GKE) I can't be completely sure, but it appears there is a conflict between the Addon-manager running on the master and the annotations on the objects within deploy/glbc/yaml/default-http-backend.yaml. By changing the annotations to addonmanager.kubernetes.io/mode: EnsureExists from addonmanager.kubernetes.io/mode: Reconcile, the Addon-manager does not delete these objects.",closed,False,2018-09-13 22:13:28,2018-09-17 20:26:08
ingress-gce,wangxy518,https://github.com/kubernetes/ingress-gce/pull/475,https://api.github.com/repos/kubernetes/ingress-gce/issues/475,Update README.md,"the url is a 404 link, and there is no ""services-firewalls.md"" in the   HEAD/docs/user-guide path.",closed,True,2018-09-14 02:44:28,2018-11-07 21:43:10
ingress-gce,sfriedel,https://github.com/kubernetes/ingress-gce/issues/476,https://api.github.com/repos/kubernetes/ingress-gce/issues/476,GKE Ingress controller ignoring ingress.class annotation,"I'm running multiple ingress controllers on GKE, ingress-gce via the ""HTTP load balancing"" addon and ingress-nginx installed via the helm chart.

This setup was working nicely for roughly half a year but since the latest upgrade to 1.10.7-gke.1 the gce ingress controller is creating cloud load balancers for Ingress resources that it should not act on because they have an `kubernetes.io/ingress.class: nginx` annotation.

This is one if the Ingress resources that the GCE ingress controller created a load balancer for:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.kubernetes.io/backends: '{""k8s-be-30389--b3e8e09a299650c8"":""Unknown"",""k8s-be-30967--b3e8e09a299650c8"":""Unknown"",""k8s-be-31662--b3e8e09a299650c8"":""Unknown"",""k8s-be-32269--b3e8e09a299650c8"":""Unknown"",""k8s-be-32270--b3e8e09a299650c8"":""HEALTHY"",""k8s-be-32586--b3e8e09a299650c8"":""Unknown"",""k8s-be-32744--b3e8e09a299650c8"":""Unknown""}'
    ingress.kubernetes.io/forwarding-rule: k8s-fw-review-review-feature-wi-fqtki7-civist--b3e8e09a299650c0
    ingress.kubernetes.io/target-proxy: k8s-tp-review-review-feature-wi-fqtki7-civist--b3e8e09a299650c0
    ingress.kubernetes.io/url-map: k8s-um-review-review-feature-wi-fqtki7-civist--b3e8e09a299650c0
    kubernetes.io/ingress.class: nginx
  creationTimestamp: 2018-09-05T13:29:12Z
  generation: 1
  labels:
    app: review-feature-wi-fqtki7-civist
    chart: civist-0.1.0
    heritage: Tiller
    release: review-feature-wi-fqtki7
  name: review-feature-wi-fqtki7-civist
  namespace: review
  resourceVersion: ""68953312""
  selfLink: /apis/extensions/v1beta1/namespaces/review/ingresses/review-feature-wi-fqtki7-civist
  uid: *** REMOVED ***
spec:
  rules:
  - host: *** REMOVED ***
    http:
      paths:
      - backend:
          serviceName: svc1
          servicePort: http
  - host: *** REMOVED ***
    http:
      paths:
      - backend:
          serviceName: svc2
          servicePort: http
  - host: *** REMOVED ***
    http:
      paths:
      - backend:
          serviceName: svc3
          servicePort: http
  - host: *** REMOVED ***
    http:
      paths:
      - backend:
          serviceName: svc4
          servicePort: http
  - host: *** REMOVED ***
    http:
      paths:
      - backend:
          serviceName: svc5
          servicePort: http
  - host: *** REMOVED ***
    http:
      paths:
      - backend:
          serviceName: svc6
          servicePort: svc-http
status:
  loadBalancer:
    ingress:
    - ip: *** REMOVED ***
```

Any ideas why this is happening?",closed,False,2018-09-17 14:45:36,2019-03-12 16:43:30
ingress-gce,harishanchu,https://github.com/kubernetes/ingress-gce/issues/477,https://api.github.com/repos/kubernetes/ingress-gce/issues/477,Changes to ingress resource doesn't update forwarding rules most of the time in 1.10.6-gke.2,"I have upgraded my gke cluster from 1.10.5-gke.4 to 1.10.6-gke.2.

Now GLBC is not making appropriate changes to the load balancer configuration in google cloud whenever I add or make changes to ingress resources.

After ingress resource modification I see events like below.
```
  Normal  UpdateCertificate  15m   cert-manager             Successfully updated Certificate ""foo-tls""
  Normal  ADD                12m   loadbalancer-controller  dev/foo
  Normal  ADD                6m    loadbalancer-controller  dev/foo
```
But loadbalancer didn't add any new forwarding rules to it.
Some times it works when I delete and recreate the ingress resource all together.

I can confirm that I'm not out of quota on any resource.",closed,False,2018-09-17 15:43:09,2018-09-17 18:53:18
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/478,https://api.github.com/repos/kubernetes/ingress-gce/issues/478,Remove add on manager label from GLBC Yaml deployment,"Fixes #474 
/assign @MrHohn ",closed,True,2018-09-17 19:36:16,2018-09-17 20:26:08
ingress-gce,mrbobbytables,https://github.com/kubernetes/ingress-gce/pull/479,https://api.github.com/repos/kubernetes/ingress-gce/issues/479,Replace kubernetes-users mailing list link with discuss forum link,"This PR updates the link that currently point to the [Google Groups Kubernetes Mailing List](https://groups.google.com/forum/#!forum/kubernetes-users) to the new [discuss.kubernetes.io](https://discuss.kubernetes.io) forum. 

The mailing list has been archived and set to read only. For more information on this, see issue https://github.com/kubernetes/community/issues/2492

/cc @castrojo 
",closed,True,2018-09-17 22:37:05,2018-09-18 17:00:29
ingress-gce,nimeshksingh,https://github.com/kubernetes/ingress-gce/issues/480,https://api.github.com/repos/kubernetes/ingress-gce/issues/480,ingress-gce fails to create backend services when across zones with different balancing modes,"We have a gke cluster with two zones, one of which was added significantly later than the other.  The ingress controller added backend services with a balancing mode of UTILIZATION for the first zone, and when we added the second zone it set the balancing mode in the new zone to RATE.  Now when we try to add new backend services, it goes into a failure loop where it tries the RATE balancing mode, which fails in one zone, then tries the UTILIZATION balancing mode, which fails in the second zone.  This continues forever and we're blocked from adding new backends.

Image showing the initial state before trying to add a second backend service:
<img width=""690"" alt=""screen shot 2018-09-17 at 5 45 12 pm"" src=""https://user-images.githubusercontent.com/7454380/45658032-b5f47d00-baa1-11e8-9c9e-2c78be01c62d.png"">

GKE Master version: 1.10.7-gke.1",open,False,2018-09-18 00:48:44,2019-04-02 18:27:11
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/481,https://api.github.com/repos/kubernetes/ingress-gce/issues/481,Ingress-GCE does not GC LB resources when ingress class changed and no other ingress on GCE exists,"Repro steps:
1. Make sure there is no ingress exists in the cluster.
2. Create a test ingress with no ingress class annotation
3. Wait for ingress-gce controller to provision LB
4. Add a non-GCE ingress class on the test ingress
5. The LB resources will be leaked. 

Root cause:
- Ingress-gce controller does not act on non-relevant ingresses.  https://github.com/kubernetes/ingress-gce/blob/release-1.3/pkg/controller/controller.go#L127
- Ingress-gce controller only GC resources in the ingress sync loop.
https://github.com/kubernetes/ingress-gce/blob/release-1.3/pkg/controller/controller.go#L287
",closed,False,2018-09-18 19:12:09,2018-12-09 18:43:35
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/482,https://api.github.com/repos/kubernetes/ingress-gce/issues/482,add work queue to process endpoint changes,"This makes the informer processing lockless. It used to depend on SyncerManager internal locks.

cc:  @agau4779 ",closed,True,2018-09-19 17:18:55,2018-10-14 23:35:09
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/issues/483,https://api.github.com/repos/kubernetes/ingress-gce/issues/483,Tool or job to cleanup ingress related GCP resource after test failure,"Ref https://github.com/kubernetes/ingress-gce/issues/438#issuecomment-422948508. Ingress scale test occasionally will hit quota issue due to resource leaked by previous runs / test failures. Test failures can be caused by various reasons --- new change breaks controller (e.g. nil pointer), test timeout, test infrastructure problem (e.g. [run#858](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/logs/ci-ingress-gce-e2e-scale/858/) never finish), etc.

Manually deleting leftover resource every time is not tenable. We should at least have a cleanup script we can run. More preferable if an automated job can run it.

@krzyzacy Is this something we should put on https://k8s-testgrid.appspot.com/sig-testing-misc#ci-janitor? Or we should have our own janitor because this is ingress specific?

",closed,False,2018-09-19 20:44:19,2018-09-20 06:02:26
ingress-gce,awly,https://github.com/kubernetes/ingress-gce/pull/484,https://api.github.com/repos/kubernetes/ingress-gce/issues/484,Bump Alpine base image version,"Alpine fixed a RCE vulnerability in apk in 3.8.1 (https://alpinelinux.org/posts/Alpine-3.8.1-released.html)
Update the base image used for ingress-gce.",closed,True,2018-09-19 23:04:20,2018-10-02 18:36:31
ingress-gce,bluemalkin,https://github.com/kubernetes/ingress-gce/issues/485,https://api.github.com/repos/kubernetes/ingress-gce/issues/485,Firewall rule required message ignores existing rules (shared VPC),"I've created my own firewall rule with custom tags on the GKE node pools to allow the Google health checks.

Despite creating the rule and that everything works, I still get the event message:
`gcloud compute firewall-rules create k8s-fw-l7--b1a36f0a2cbc5f47 --network xxx --description ""GCE L7 firewall rule"" --allow tcp:30000-32767 --source-ranges 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 --target-tags xxx --project xxx`

It should check for any existing rule first.",closed,False,2018-09-20 01:39:15,2018-12-06 04:44:09
ingress-gce,stepyu,https://github.com/kubernetes/ingress-gce/pull/486,https://api.github.com/repos/kubernetes/ingress-gce/issues/486,fix typos,,closed,True,2018-09-20 08:59:53,2018-09-20 18:55:06
ingress-gce,anuraaga,https://github.com/kubernetes/ingress-gce/issues/487,https://api.github.com/repos/kubernetes/ingress-gce/issues/487,HTTP2 health check does not use readiness probe path,"I recently switched some services from protocol HTTPS to HTTP2 and noticed that health checks stopped being sent to the path defined in the deployment config, but are just sent to the root path.

The deployment config is identical for 

```yaml
...
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /internal/health
            port: 8080
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 15
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /internal/health
            port: 8080
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 5
...
```

If svc is HTTPS, the health check uses the path
```
Description
Kubernetes L7 health check generated with readiness probe settings.

Path
/internal/health
Protocol
HTTPS
Port
31508
```

If svc is HTTP2, the health check does not
```
Description
Default kubernetes L7 Loadbalancing health check.

Protocol
HTTP2
Port
31021
```

The issue seems to be because of comparing the string `HTTP2` to `HTTPS`, as kube API does not support the scheme `HTTP2`.",closed,False,2018-09-21 05:08:44,2018-10-08 20:49:26
ingress-gce,anuraaga,https://github.com/kubernetes/ingress-gce/pull/488,https://api.github.com/repos/kubernetes/ingress-gce/issues/488,Use HTTPS readiness probe for HTTP2 services because the Kubernetes A…,"…PI does not make a distinction between HTTPS and HTTP2 for probes.

Fixes #487 ",closed,True,2018-09-21 05:09:49,2018-10-08 20:49:26
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/489,https://api.github.com/repos/kubernetes/ingress-gce/issues/489,Restructure NEG controller,"No functional changes. Only refactoring: 

- Created sub-package for `/metrics` and `/syncer`
- Move common types and interfaces into `/types`
- Rename `syncer` to `batchSyncer`

cc: @agau4779 ",closed,True,2018-09-21 19:55:07,2018-09-24 18:58:05
ingress-gce,AhmedMItman,https://github.com/kubernetes/ingress-gce/issues/490,https://api.github.com/repos/kubernetes/ingress-gce/issues/490,can not access  nginx through ingress and problem in curl ,"i have kubernetes-cluster v1.10 over centos 7
i am trying to deploy ingress , i followed video 

and finally i found 
-> kubectl get ing
NAME      HOSTS              ADDRESS   PORTS     AGE
nginx     kub-master                               80, 443     9h
there is now address for nginx 

-> and when i execute ""  curl --ipv4 -v http://kub-master ""
* About to connect() to kub-master port 80 (#0)
*   Trying kub-master...
* Connection refused
* Failed connect to kub-master; Connection refused
* Closing connection 0
curl: (7) Failed connect to kub-master; Connection refused
 
-> kubectl cluster-info 
Kubernetes master is running at http://localhost:8080
KubeDNS is running at http://localhost:8080/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

-> systemctl restart flanneld
Job for flanneld.service failed because a timeout was exceeded. See ""systemctl status flanneld.service"" and ""journalctl -xe"" for details.

-> systemctl status flanneld
● flanneld.service - Flanneld overlay address etcd agent
   Loaded: loaded (/usr/lib/systemd/system/flanneld.service; enabled; vendor preset: disabled)
   Active: activating (start) since Fri 2018-09-21 17:22:37 EDT; 1min 10s ago
 Main PID: 18327 (flanneld)
   Memory: 22.8M
   CGroup: /system.slice/flanneld.service
           └─18327 /usr/bin/flanneld -etcd-endpoints=http://kube-master:2379 -etcd-prefix=/kube-centos/network

Sep 21 17:23:38 kub-mst.coral.io flanneld-start[18327]: E0921 17:23:38.833256   18327 network.go:102] failed to register network: operation not supported


#how to solve this issue and access port 80  ? ",closed,False,2018-09-21 21:38:37,2018-09-24 07:12:35
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/491,https://api.github.com/repos/kubernetes/ingress-gce/issues/491,Modify GroupKey type to contain group name + modify NEG linker to consider provided name,/cc @agau4779 ,closed,True,2018-09-24 17:00:04,2018-09-24 17:06:22
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/492,https://api.github.com/repos/kubernetes/ingress-gce/issues/492,Restructure syncer package,"Structural changes only. No functional change: 
- Refactor syncer package to support multiple NEG syncer implementations.
- Extract common utils from batchSyncer for reuse. 

cc: @agau4779 ",closed,True,2018-09-24 23:40:06,2018-10-14 23:54:48
ingress-gce,scaccogatto,https://github.com/kubernetes/ingress-gce/issues/493,https://api.github.com/repos/kubernetes/ingress-gce/issues/493,unable to activate gzip,"With my system:

```
| (Ingress) 
| Google Cloud Load Balancer 
|--------> Website Backend
|--------> Website Backend
|--------> Website Backend
|--------> Website Backend
```
Ingress config: https://gist.github.com/scaccogatto/8796908c22cc6437fc645f3fba587edd
Backend nginx config: https://github.com/scaccogatto/vue-nginx/blob/master/docker/nginx.conf

Following:
https://cloud.google.com/load-balancing/docs/https/setting-up-https#compression-not-working

As you can see from my configs gzip directives are here, also **I found that my Request Headers contains accept-encoding: gzip, deflate, br**.

Unfortunately when I run Chrome's audit it says that I should enable text compression even if most of online gzip checkers say that gzip is active (but I don't trust them)

Also I found that my **Response Headers does not contain**: `content-encoding: gzip` so I assume gzip is not working correctly.

Searching on the net I found this: https://stackoverflow.com/a/48084669/10412176 (that actually says the same things that's on the docs) and I am starting to think than I am changing the wrong configuration and the docs reefer to the Ingress configuration (that is not reachable as fas as I know)

I don't really know if it is a bug or not so I'm asking here. ",open,False,2018-09-25 09:33:15,2019-01-31 13:08:59
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/494,https://api.github.com/repos/kubernetes/ingress-gce/issues/494,include NEG naming scheme for NameBelongsToCluster,cc @agau4779 ,closed,True,2018-09-25 21:28:47,2018-09-25 21:34:01
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/495,https://api.github.com/repos/kubernetes/ingress-gce/issues/495,merge negBelongsToCluster into IsNEG,"follow up of https://github.com/kubernetes/ingress-gce/pull/494
cc: @agau4779 ",closed,True,2018-09-25 22:19:39,2018-09-27 23:29:18
ingress-gce,aledbf,https://github.com/kubernetes/ingress-gce/pull/496,https://api.github.com/repos/kubernetes/ingress-gce/issues/496,Update defaultbackend image to 1.5,"**What this PR does / why we need it**:

Update docker image for default backend service (render 404 page)

**Release note**:

```release-note
Update defaultbackend image to 1.5.
```

@bowei @timstclair 
",closed,True,2018-09-26 21:40:02,2018-10-02 17:22:32
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/497,https://api.github.com/repos/kubernetes/ingress-gce/issues/497,Update vendor,"Update vendor to include a raw patch that was upstreamed to k/k (https://github.com/kubernetes/kubernetes/pull/69042)

Since that upstreamed PR has not yet been tagged, Gopkg.toml had to be modified to pull k/k files from the master branch. Once that PR is tagged, we should modify back to using a specific revision.

Note that some manual changes had to be made to pkg/flags/flags.go and cmd/glbc/main.go to pass the tests.

/assign @bowei 

",closed,True,2018-09-27 17:00:52,2018-09-27 23:14:10
ingress-gce,jonpulsifer,https://github.com/kubernetes/ingress-gce/issues/498,https://api.github.com/repos/kubernetes/ingress-gce/issues/498,A new home for 404-server (defaultbackend),":wave: we recently bumped the defaultbackend to version 1.5 in https://github.com/kubernetes/ingress-nginx/pull/3125 to conceal `/metrics/` behind port 10254

During that discussion a [PR was merged](https://github.com/kubernetes/ingress-nginx/pull/3126) to not require the defaultbackend service for ingress-nginx.

I originally moved the 404-server into the old `kubernetes/ingress` repo from contrib, but now we've gone our separate ways w/ nginx and gce so I'd like to find a new home for this image.

Should we move it here? I think so? Is there a better place? I don't think it makes sense for the ingress-nginx repo to own this image any longer.",closed,False,2018-09-27 18:05:24,2018-11-14 17:33:19
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/499,https://api.github.com/repos/kubernetes/ingress-gce/issues/499,Add a Backofff Handler utils,"Extract the backoff retry logic from `batchSyncer` and write 2 util package that can be reused by different syncers, plus unit testing:
- `backoffHandler` calculates exponential backoff delays
- `backoffRetryHandler` triggers `retryFunc` after back off delay.

These utils are building blocks for the new `transactionSyncer`

cc: @agau4779 ",closed,True,2018-09-27 20:32:51,2018-11-08 21:09:13
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/500,https://api.github.com/repos/kubernetes/ingress-gce/issues/500,Update OWNERS file to reflect reality.,/assign @bowei ,closed,True,2018-09-27 23:13:17,2018-09-27 23:22:33
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/501,https://api.github.com/repos/kubernetes/ingress-gce/issues/501,merge negBelongsToCluster into IsNEG,"Follow up of #494
Repush #495

cc: @agau4779",closed,True,2018-09-27 23:35:26,2018-10-15 18:06:17
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/502,https://api.github.com/repos/kubernetes/ingress-gce/issues/502,Fix a potential nil pointer exception,"This fix a nil pointer exception, when an endpoint does not have an associated node. 

This usually happens when the endpoints object is manually created.

cc: @agau4779 ",closed,True,2018-09-27 23:43:30,2018-10-03 21:47:25
ingress-gce,jonpulsifer,https://github.com/kubernetes/ingress-gce/pull/503,https://api.github.com/repos/kubernetes/ingress-gce/issues/503, Welcome defaultbackend to the ingress-gce repo," This commit introduces the 404-server to the ingress-gce repository.

 Previously, it was held in the kubernetes/ingress-nginx repo, but the
 nginx ingress controller no longer requires it.

 Fixes https://github.com/kubernetes/ingress-gce/issues/498

Signed-off-by: Jonathan Pulsifer <jonathan.pulsifer@shopify.com>",closed,True,2018-09-28 21:14:04,2018-11-14 18:42:25
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/504,https://api.github.com/repos/kubernetes/ingress-gce/issues/504,Expose utils.hasFinalizer,/assign @MrHohn ,closed,True,2018-09-28 23:27:00,2018-09-28 23:35:47
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/505,https://api.github.com/repos/kubernetes/ingress-gce/issues/505,Allow for setting the rate limiter on the work queue,/assing @MrHohn ,closed,True,2018-10-01 17:52:50,2018-10-01 18:05:20
ingress-gce,grayluck,https://github.com/kubernetes/ingress-gce/pull/506,https://api.github.com/repos/kubernetes/ingress-gce/issues/506,Add annotation to suppress XPN firewall events,"Tested using debug output. 

```
I1002 00:52:00.732636       1 firewalls.go:129] Could not create L7 firewall on XPN cluster. Raising event for cmd: ""gcloud compute firewall-rules create k8s-fw-l7--f55c729d54843f52 --network shared-net --description \""GCE L7 firewall rule\"" --allow tcp:30000-32767 --source-ranges 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 --target-tags gke-xpn-test-5a5694fa-node --project yankaiz-xpn-host""
I1002 00:52:00.732948       1 controller.go:178] >>>>>>>>>>>>> Emitting event for FirewallXPNError <<<<<<<<<<<<<<<
I1002 00:52:00.733404       1 controller.go:149] Syncing firewall
I1002 00:52:00.734304       1 event.go:221] Event(v1.ObjectReference{Kind:""Ingress"", Namespace:""default"", Name:""ing-web"", UID:""ac329930-c5da-11e8-a4db-42010af0009b"", APIVersion:""extensions/v1beta1"", ResourceVersion:""18952"", FieldPath:""""}): type: 'Normal' reason: 'XPN' Firewall change required by network admin: `gcloud compute firewall-rules create k8s-fw-l7--f55c729d54843f52 --network shared-net --description ""GCE L7 firewall rule"" --allow tcp:30000-32767 --source-ranges 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 --target-tags gke-xpn-test-5a5694fa-node --project yankaiz-xpn-host`
I1002 00:52:00.998370       1 firewalls.go:97] Creating firewall rule ""k8s-fw-l7--f55c729d54843f52""
I1002 00:52:01.127250       1 firewalls.go:129] Could not create L7 firewall on XPN cluster. Raising event for cmd: ""gcloud compute firewall-rules create k8s-fw-l7--f55c729d54843f52 --network shared-net --description \""GCE L7 firewall rule\"" --allow tcp:30000-32767 --source-ranges 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 --target-tags gke-xpn-test-5a5694fa-node --project yankaiz-xpn-host""
I1002 00:52:01.127283       1 controller.go:178] >>>>>>>>>>>>> Emitting event for FirewallXPNError <<<<<<<<<<<<<<<
I1002 00:52:01.127865       1 event.go:221] Event(v1.ObjectReference{Kind:""Ingress"", Namespace:""default"", Name:""ing-web"", UID:""ac329930-c5da-11e8-a4db-42010af0009b"", APIVersion:""extensions/v1beta1"", ResourceVersion:""18952"", FieldPath:""""}): type: 'Normal' reason: 'XPN' Firewall change required by network admin: `gcloud compute firewall-rules create k8s-fw-l7--f55c729d54843f52 --network shared-net --description ""GCE L7 firewall rule"" --allow tcp:30000-32767 --source-ranges 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 --target-tags gke-xpn-test-5a5694fa-node --project yankaiz-xpn-host`
I1002 00:52:02.445783       1 syncer.go:62] Sync: backend {ID:default/web/8080 NodePort:31247 Port:8080 Protocol:HTTP TargetPort:8080 NEGEnabled:false BackendConfig:<nil>}
...
I1002 00:58:44.936370       1 firewalls.go:97] Creating firewall rule ""k8s-fw-l7--f55c729d54843f52""
I1002 00:58:44.957027       1 instances.go:136] Setting named ports us-central1-c on instance group k8s-ig--f55c729d54843f52/[31247]
I1002 00:58:45.123424       1 firewalls.go:129] Could not create L7 firewall on XPN cluster. Raising event for cmd: ""gcloud compute firewall-rules create k8s-fw-l7--f55c729d54843f52 --network shared-net --description \""GCE L7 firewall rule\"" --allow tcp:30000-32767 --source-ranges 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 --target-tags gke-xpn-test-5a5694fa-node --project yankaiz-xpn-host""
I1002 00:58:45.123455       1 controller.go:175] >>>>>>>>>>>> FirewallXPNError suppressed  <<<<<<<<<<<<<<<<<<<
```

/hold adding unit tests.
/assign @bowei ",closed,True,2018-10-02 01:02:34,2018-11-01 21:51:40
ingress-gce,krzykwas,https://github.com/kubernetes/ingress-gce/pull/508,https://api.github.com/repos/kubernetes/ingress-gce/issues/508,Implement support for ManagedCertificate CRD,This change adds to Ingress support for ManagedCertificate CRD to integrate with https://github.com/GoogleCloudPlatform/gke-managed-certs,closed,True,2018-10-02 16:06:09,2018-11-19 14:57:24
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/509,https://api.github.com/repos/kubernetes/ingress-gce/issues/509,Add Syncer Skeleton,"The syncer skeleton contains all the mechanism that handles syncer life cycle, including
- Start/Stop
- Sync signal handling, reporting error and retry handling
- The syncer only requires the extension struct to provide the sync function. 



cc @agau4779 ",closed,True,2018-10-02 23:26:43,2018-11-14 23:28:40
ingress-gce,vrobert78,https://github.com/kubernetes/ingress-gce/issues/510,https://api.github.com/repos/kubernetes/ingress-gce/issues/510,Any plan to support QUIC in Google LB ?,"Do you have an ETA to add the support of QUIC in the definition of an ingress ?

Currently, we have to enable it manually.

https://cloudplatform.googleblog.com/2018/06/Introducing-QUIC-support-for-HTTPS-load-balancing.html",open,False,2018-10-10 07:36:35,2018-11-06 19:21:15
ingress-gce,abstrctn,https://github.com/kubernetes/ingress-gce/issues/511,https://api.github.com/repos/kubernetes/ingress-gce/issues/511,Control backend service IAM policy from BackendConfig,"[We can now grant](https://cloud.google.com/iap/docs/managing-access) the IAP-Secured Web App User role to individual users on a single backend service.

Currently this needs to be configured manually after the GLBC creates a new backend service; and because they're attached to the with the backend service, anything that causes it to be recreated would cause the permissions to be lost. This is somewhat limiting from a portability standpoint, as the kubernetes manifests aren't sufficient to configure IAP access from scratch.

Having a way to control those permissions from within a BackendConfig seems like a useful extension that would prevent accidental loss and allow for more seamless configuration when creating a new service for the first time.

Would adding something like a `members` array to the BackendConfig spec make sense for controlling this?

```yaml
apiVersion: cloud.google.com/v1beta1
kind: BackendConfig
metadata:
  name: my-backend-config
spec:
  iap:
    enabled: true
    members:
    - user@example.com
    - server@example.gserviceaccount.com
```

One question here would be how to treat members granted access outside of Kubernetes. My preference would be to have the BackendConfig serve as the canonical access list, meaning changes made outside would get reverted, but that would complicate the change detection trigger for the controller.",open,False,2018-10-15 19:42:14,2019-02-08 01:39:16
ingress-gce,Freyert,https://github.com/kubernetes/ingress-gce/pull/512,https://api.github.com/repos/kubernetes/ingress-gce/issues/512,WIP feat: https sslpolicy support,"I'm trying to add support for specifying an `SSLPolicy` via an annotation: `ingress.gcp.kubernetes.io/ssl-policy`.

Would love a bit of direction if possible, but this seemed like a pretty easy change. There just seem to be a lot of different places in the code to make changes and I'm guaranteed to be missing something.

Also any pointers/tips on automation you all have written so I can build an image and deploy it in my cluster. I'm assuming just basic go build for binaries.",closed,True,2018-10-16 19:46:44,2018-10-19 12:06:12
ingress-gce,bpineau,https://github.com/kubernetes/ingress-gce/pull/513,https://api.github.com/repos/kubernetes/ingress-gce/issues/513,BackendConfig support for timeouts and connection draining,"As suggested in #28, BackendConfig is a natural way to expose
those settings.",closed,True,2018-10-17 17:47:45,2018-11-29 23:00:39
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/514,https://api.github.com/repos/kubernetes/ingress-gce/issues/514,Replace snapshotter in pkg/backends with a GCE cloud lister implementation that is solely used for garbage collection ,"Currently, pkg/backends uses a snapshotter which is used to keep some state on what backends that the controller managed. This snapshotter lists from the cloud periodically but this is unnecessary. We only need to list from the cloud when we are performing garbage collection.

This PR adds a new package called cloudlist which contains an implementation for a simple cloud lister that is decoupled from the existing snapshotter. The existing snapshotter implementation in pkg/backends is then replaced with the new cloud listing implementation. 

The goal is to use the new cloud lister in pkg/loadbalancers and pkg/instances as well. Once this is done, the snapshotter code in pkg/pools can be completely removed.",closed,True,2018-10-18 19:06:28,2018-12-21 17:23:20
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/515,https://api.github.com/repos/kubernetes/ingress-gce/issues/515,Tracking issue for FrontendConfig work,"For Ingress-GCE, we have the BackendConfig CRD which encapsulates features on the GCE Backend Service

Similarly, for frontend components of the load balancer (target proxy, url map, etc) we need a way to encapsulate features (i.e SSL Policy). The current method of using annotations will not scale.

Using this issue to track implementation of a ""FrontendConfig"" CRD. Ideally, we can provide the scaffolding and solicit members of the community to provide implementations for each feature.

/assign",open,False,2018-10-18 20:46:56,2019-03-25 15:37:33
ingress-gce,lornemet-eno,https://github.com/kubernetes/ingress-gce/issues/516,https://api.github.com/repos/kubernetes/ingress-gce/issues/516,Support session affinity via Ingress annotation,"ingress.kubernetes.io/affinity annotation is currently discarded.
The expected behavior would be to disable/enable session affinity based on the annotations value:
none (default) -> NONE
cookie -> GENERATED_COOKIE
??? -> CLIENT_IP

While simply turning on session affinity in the GCE ingress itself would not enable session affinity down to the POD (GCE would still send traffic to the NodePort of computes in the resourcegroup, which would not care about HTTP cookies), but would eliminate the need of manual/out-of-kubernetes configuration if additional measures are implamented (ie deployment -> daemonset, or pod antiaffinity + externalTrafficPolicy: Local).",closed,False,2018-10-19 13:28:54,2018-11-29 22:57:37
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/517,https://api.github.com/repos/kubernetes/ingress-gce/issues/517,Add pkg/common/operator & pkg/common/typed to make resource joins much cleaner.,"/assign @bowei 

PR is broken up into 3 commits for easier review.",closed,True,2018-10-19 21:19:28,2018-11-26 18:10:56
ingress-gce,ilkerc,https://github.com/kubernetes/ingress-gce/issues/518,https://api.github.com/repos/kubernetes/ingress-gce/issues/518,ERR_SPDY_PROTOCOL_ERROR when streaming text/event-stream ,"Seeing error 'net::ERR_SPDY_PROTOCOL_ERROR 200' on console and connection gets dropped.
Not sure if it's because of the content type but happens only on text/eventstream s while sending the request over the ingress. Connection closes almost exactly 30 seconds.

Curl with -v
```
* HTTP/2 stream 1 was not closed cleanly: INTERNAL_ERROR (err 2)
* Closing connection 0
* TLSv1.2 (OUT), TLS alert, Client hello (1):
curl: (92) HTTP/2 stream 1 was not closed cleanly: INTERNAL_ERROR (err 2)
```

Anyone having insights about the issue?",closed,False,2018-10-20 01:20:51,2019-03-19 02:40:55
ingress-gce,bpineau,https://github.com/kubernetes/ingress-gce/pull/519,https://api.github.com/repos/kubernetes/ingress-gce/issues/519,Fix events-based e2e tests,"controller.go changes transformed ingress spec errors from Kubernetes Events to simple logs:
https://github.com/kubernetes/ingress-gce/blob/master/pkg/controller/controller.go#L455  is returning.
So we never hit https://github.com/kubernetes/ingress-gce/blob/master/pkg/controller/controller.go#L461 .

As a consequence, some e2e tests, waiting for those events, bails out
(ie. https://github.com/kubernetes/ingress-gce/blob/master/cmd/e2e-test/backend_config_test.go#L114 ).

Notifying those errors as events -as they did previously- seems the proper course of action,
and generaly useful beyond testing.

Unrelated: end users may authenticate to GCP using OpenID Connect (at least I do);
linking in client-go's plugin/pkg/client/auth/oidc allow them to run tests on their
machines.",closed,True,2018-10-22 09:52:37,2018-10-22 22:38:40
ingress-gce,bpineau,https://github.com/kubernetes/ingress-gce/pull/520,https://api.github.com/repos/kubernetes/ingress-gce/issues/520,Update deploy/glbc yaml files for BackendConfig,"Not sure if BackendConfig support was intentionaly left out of the deployment
example (ie. after all it's still off by default, that feature is possibly not
yet considered ready for usage outside GKE?), or this example was just forgotten.

If the later, including it would be nice as those GKE users having already
defined BackendConfigs and willing to try ingress-gce git HEAD would be up
for a suprise.

Otherwise, let's just drop this PR.",closed,True,2018-10-22 21:21:03,2018-10-23 16:18:30
ingress-gce,bpineau,https://github.com/kubernetes/ingress-gce/pull/521,https://api.github.com/repos/kubernetes/ingress-gce/issues/521,e2e tests for timeout and draining timeout,"As suggested by @MrHohn 

Those are simple tests checking that the settings from BackendConfig propagates well
to the live BackendService.

Let me know if you'd rather me to write fuzz http reqs testing real timeouts expirations
(that's much more involved -esp. the connection draining part- so it'll take more time though).",closed,True,2018-10-22 21:26:42,2018-11-19 19:08:00
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/522,https://api.github.com/repos/kubernetes/ingress-gce/issues/522,[e2e test] append key value to resources instead of pointer,"`CheckResourceDeletion()` appends pointer of the key to `resources` while detecting un-deleted resources, which would likely be pointing to a wrong key as the for loop goes. This PR changes it to append value of the key instead.
/assign @rramkumar1 ",closed,True,2018-10-23 19:00:49,2018-10-23 19:04:31
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/523,https://api.github.com/repos/kubernetes/ingress-gce/issues/523,"In e2e tests, always skip checking for deletion of default backend service",/assign @MrHohn ,closed,True,2018-10-23 19:01:09,2018-10-23 19:09:31
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/pull/524,https://api.github.com/repos/kubernetes/ingress-gce/issues/524,Add an example for running specific test case to readme,"Thinking this might be helpful as it isn't quite obvious to me.
/assign @rramkumar1 ",closed,True,2018-10-24 21:25:57,2018-10-24 21:33:56
ingress-gce,nyurik,https://github.com/kubernetes/ingress-gce/issues/525,https://api.github.com/repos/kubernetes/ingress-gce/issues/525,Certificate update path is not clear,"Could someone clarify what happens during the certificate update:
* When using k8s secret, will Ingress automatically pick up the new certificate when it gets updated in-place?
* For GCP certificates, it is not possible to update a cert in-place - a new certificate must first be created under a new name, then the https proxy is updated to use it, and then the old cert can be deleted.  This means that k8s annotation would not be very practical - certs are much easier to update with Terraform, using the randomly generated name. Is there a recommended approach for this method?
Thx!",closed,False,2018-10-24 22:40:09,2019-01-11 23:16:12
ingress-gce,bpineau,https://github.com/kubernetes/ingress-gce/pull/526,https://api.github.com/repos/kubernetes/ingress-gce/issues/526,BackendConfig support for session affinity,"Allow user provided session affinity type and ttl.

Sending e2e/fuzz tests on a distinct PR.",closed,True,2018-10-25 12:35:49,2018-10-29 16:50:23
ingress-gce,bpineau,https://github.com/kubernetes/ingress-gce/pull/527,https://api.github.com/repos/kubernetes/ingress-gce/issues/527,e2e and fuzz tests for session affinity,,closed,True,2018-10-25 12:36:32,2018-11-02 22:00:09
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/528,https://api.github.com/repos/kubernetes/ingress-gce/issues/528,add util for transaction table,"util needed for transaction-syncer

cc @agau4779 ",closed,True,2018-10-25 21:40:20,2018-11-15 00:22:44
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/529,https://api.github.com/repos/kubernetes/ingress-gce/issues/529,Managed Certs integration: Fix code so that informers are not instantiated if managed certs is not enabled,"This should fix the current failures in the OSS CI:

Ref: https://gubernator.k8s.io/build/kubernetes-jenkins/logs/ci-ingress-gce-e2e/3251",closed,True,2018-10-30 22:41:25,2018-10-31 13:14:38
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/530,https://api.github.com/repos/kubernetes/ingress-gce/issues/530,Replace existing CHANGELOG.md with one generated via a script,"/assign @MrHohn 
/assign @freehan 

Once we become better with our issue tracking in terms of labels, the output of this script will be even better.",closed,True,2018-10-31 22:39:18,2018-10-31 22:49:47
ingress-gce,grayluck,https://github.com/kubernetes/ingress-gce/pull/531,https://api.github.com/repos/kubernetes/ingress-gce/issues/531,Add gke.io/suppress-firewall-xpn-error annotation to suppress XPN firewall events,"Cherrypick https://github.com/kubernetes/ingress-gce/pull/506 into release-1.4

Add annotation to suppress XPN firewall events when failed to manipulate firewall due to XPN and permission error.",closed,True,2018-11-01 19:45:52,2018-11-01 19:54:12
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/532,https://api.github.com/repos/kubernetes/ingress-gce/issues/532,Transaction Syncer,"This PR introduces transaction NEG syncer. This syncer is non blocking. It will not wait for batches of NEG operation to finish before trigger more NEG operations. 

PR prerequistes:
#528
#509
#499

TODOs:
- add a flag to switch between batch syncer and transaction syncer
- add more unit test
- make this more readable",closed,True,2018-11-02 23:03:32,2018-11-20 00:04:46
ingress-gce,bpineau,https://github.com/kubernetes/ingress-gce/pull/533,https://api.github.com/repos/kubernetes/ingress-gce/issues/533,README.md: don't assume instance groups where NEG is an option,"README.md is light on details related to recent features (NEGs,
BackendConfig, etc.). That may be material for an other PR ?

For now, let's just edit so the content remains correct (ie. not
assuming instance groups exclusively, when NEG is an option).",closed,True,2018-11-04 20:19:47,2018-11-06 00:21:00
ingress-gce,bpineau,https://github.com/kubernetes/ingress-gce/issues/534,https://api.github.com/repos/kubernetes/ingress-gce/issues/534,Is github release tab forgotten or abandoned?,"[raising the matter in case that's an oversight - otherwise please ignore and close]

The [Github ""release"" tab](https://github.com/kubernetes/ingress-gce/releases) was used to publish releases, up to v1.2.3.
That may drive people to think v1.2.3 is actually the last release? (at least it got me confused)",closed,False,2018-11-04 20:24:23,2018-11-05 20:35:13
ingress-gce,arthurk,https://github.com/kubernetes/ingress-gce/issues/535,https://api.github.com/repos/kubernetes/ingress-gce/issues/535,Update version mapping for GKE 1.11.2,Which version of ingress-gce is running on GKE 1.11.2-gke.9?,closed,False,2018-11-05 07:08:20,2018-11-05 20:35:36
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/536,https://api.github.com/repos/kubernetes/ingress-gce/issues/536,Update version mapping to latest state,Fixes #535,closed,True,2018-11-05 16:40:38,2018-11-05 18:22:57
ingress-gce,NickLavrov,https://github.com/kubernetes/ingress-gce/issues/537,https://api.github.com/repos/kubernetes/ingress-gce/issues/537,Long namespace/ingress names cause collisions with auto-created resources on GKE,"I noticed this after creating one ingress that worked fine, then creating another ingress which caused the first ingress to behave incorrectly (default backend 404, etc).

As far as I can tell, at least these resources and annotations are added to the ingress by GKE:
- ingress.kubernetes.io/forwarding-rule
- ingress.kubernetes.io/target-proxy
- ingress.kubernetes.io/url-map

If my namespace was called `xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx` (32 chars) and my ingress was named `xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`, I'd see resources with names like:
`k8s-um-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-xxxxxxxxxxxxx0`

This is fine if there's only one ingress, but since my other ingress also started with the same set of characters, there would be a collision. I've looked at the resources/annotations it creates when using shorter names, and I notice that it means to append some random characters at the end. Here, it looks like I'm hitting the 64 character limit before it gets to that random string.

Maybe removing the 64 character limit or adding documentation about how these names are generated would help with this. It took me a while to look into this annotation as the root of my issues.

NOTE: This is the file that contains the naming logic https://github.com/kubernetes/ingress-gce/blob/master/pkg/utils/namer.go",open,False,2018-11-05 23:56:43,2019-02-27 18:42:41
ingress-gce,rllin-fathom,https://github.com/kubernetes/ingress-gce/issues/538,https://api.github.com/repos/kubernetes/ingress-gce/issues/538,GKE BackendConfig permissions change `container.backendConfigs.get` does not exist,"I am getting the following error when trying to apply a yaml declaring a `BackendConfig`.  This was working as of yesterday for several months.  

```backendconfigs.cloud.google.com ""XXXXXXXX"" is forbidden: User ""USER@DOMAIN.COM"" cannot get backendconfigs.cloud.google.com in the namespace ""NAMESPACE"": Required ""container.backendConfigs.get"" permission.```

It seems like GKE changed permissions required to create BackendConfigs very recently?

however I can't find `container.backendConfigs.get` as a permission when I run `gcloud iam roles describe roles/container.admin` so it's unclear how to fix this.",closed,False,2018-11-06 00:17:21,2018-11-14 00:03:29
ingress-gce,mikeweiwei,https://github.com/kubernetes/ingress-gce/pull/539,https://api.github.com/repos/kubernetes/ingress-gce/issues/539,Fix typos,,closed,True,2018-11-08 08:35:12,2018-11-14 18:06:22
ingress-gce,mikeweiwei,https://github.com/kubernetes/ingress-gce/pull/540,https://api.github.com/repos/kubernetes/ingress-gce/issues/540,fix logging calls,,closed,True,2018-11-08 08:38:59,2018-11-14 18:08:56
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/541,https://api.github.com/repos/kubernetes/ingress-gce/issues/541,Readiness Probe does not get reflected for NEG enabled ingress,"When the following conditions apply:
- NEG is enabled on the ingress referenced service 
- Service is ClusterIP type 

Then LB health check is created with default setting. ",closed,False,2018-11-08 21:19:40,2018-12-20 01:44:37
ingress-gce,mooncak,https://github.com/kubernetes/ingress-gce/pull/542,https://api.github.com/repos/kubernetes/ingress-gce/issues/542,Cleanup the duplication,"Cleanup the duplication:

1. docs/faq/gce.md
2. pkg/utils/namer.go",closed,True,2018-11-10 16:40:35,2018-11-14 18:09:48
ingress-gce,mooncak,https://github.com/kubernetes/ingress-gce/pull/543,https://api.github.com/repos/kubernetes/ingress-gce/issues/543,"Fix typos: snpshot->snapshot, inmplements->implements","Fix typos: snpshot->snapshot, inmplements->implements",closed,True,2018-11-10 16:43:49,2018-11-14 18:07:51
ingress-gce,Megalepozy,https://github.com/kubernetes/ingress-gce/pull/544,https://api.github.com/repos/kubernetes/ingress-gce/issues/544,Small fix for README,,closed,True,2018-11-11 13:08:25,2018-11-19 17:49:09
ingress-gce,ukai,https://github.com/kubernetes/ingress-gce/issues/545,https://api.github.com/repos/kubernetes/ingress-gce/issues/545,configure maxRatePerInstance in backend,"ingress configures backend with maxRatePerInstance=1 like

backends:
- balancingMode: RATE
  capacityScaler: 1.0
  group: https://www.googleapis.com/compute/v1/projects/$project/zones/$zone/instanceGroups/$ig
  maxRatePerInstance: 1.0

This comment says maxRatePerInstance value doesn't matter
https://github.com/kubernetes/ingress-gce/blob/658331802409cf9469f49d9fb70ad27cc85a1a31/pkg/backends/ig_linker.go#L57

but actually I experienced lots of 502 errors when it received too many requests per second.

I manually raised maxRatePerInstance (e.g. to 20.0), 502 errors disappeared.
can we configure maxRatePerInstance by ingress annotation or so.

we might also want to configure timeoutSec too.

",closed,False,2018-11-12 08:15:57,2018-11-15 02:10:52
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/546,https://api.github.com/repos/kubernetes/ingress-gce/issues/546,Ingress Upgrade Testing,Continuation of https://github.com/kubernetes/ingress-gce/pull/392 .,closed,True,2018-11-14 23:14:26,2018-11-27 19:28:54
ingress-gce,pondohva,https://github.com/kubernetes/ingress-gce/pull/547,https://api.github.com/repos/kubernetes/ingress-gce/issues/547,do not leak LB when ingress class is changed,"This PR fixes #481 

",closed,True,2018-11-19 14:01:04,2018-12-09 18:43:35
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/548,https://api.github.com/repos/kubernetes/ingress-gce/issues/548,Connection Draining E2E: Fix test log message level,"While polling for the connection draining transition, the test calls t.Errorf() instead of t.Logf(). This causes the test to fail rather than perform the poll-loop again, as intended.

/assign @bowei ",closed,True,2018-11-20 17:01:01,2018-11-20 18:12:39
ingress-gce,mkozjak,https://github.com/kubernetes/ingress-gce/issues/549,https://api.github.com/repos/kubernetes/ingress-gce/issues/549,ErrImagePull: k8s.gcr.io/defaultbackend:1.5 not found,Works with gcr.io/google-containers/defaultbackend:1.4.,closed,False,2018-11-21 14:18:58,2018-11-26 19:07:24
ingress-gce,krzykwas,https://github.com/kubernetes/ingress-gce/pull/550,https://api.github.com/repos/kubernetes/ingress-gce/issues/550,Filter ManagedCertificate objects properly (catch-all selector was applied),Because of a bug in code Ingress adds all ManagedCertificate objects to the load balancer if managed-certificates annotation is not empty. Instead it should only add to the load balancer the ones found in the annotation. This PR fixes the bug.,closed,True,2018-11-21 15:30:53,2018-12-10 17:28:35
ingress-gce,krzykwas,https://github.com/kubernetes/ingress-gce/pull/551,https://api.github.com/repos/kubernetes/ingress-gce/issues/551,Fix wrong filtering of ManagedCertificate objects.,"Because of a bug Ingress adds all ManagedCertificate objects in the cluster to the load balancer if managed-certificates annotation is not empty. Instead it should only add the objects found in the annotation. This PR fixes this bug.

/assign @rramkumar1 ",closed,True,2018-11-21 15:35:08,2018-12-10 17:28:35
ingress-gce,krzykwas,https://github.com/kubernetes/ingress-gce/pull/552,https://api.github.com/repos/kubernetes/ingress-gce/issues/552,Fix wrong filtering of ManagedCertificate objects.,"Because of a bug Ingress adds all ManagedCertificate objects in the cluster to the load balancer if managed-certificates annotation is not empty. Instead it should only add the objects found in the annotation. This PR fixes this bug.

/assign @rramkumar1",closed,True,2018-11-21 15:37:55,2018-12-10 17:02:25
ingress-gce,PassKit,https://github.com/kubernetes/ingress-gce/issues/553,https://api.github.com/repos/kubernetes/ingress-gce/issues/553,Ingress controller does not support HTTP2 with mutual TLS,"When creating an Ingress controller for a service using the `service.alpha.kubernetes.io/app-protocols: '{""grpc"":""HTTP2""}'` annotation, Ingress-GCE will automatically create a HTTP2 health check to `/`.

When mutual TLS is deployed on the containers, this check will never pass, since the health check does not have access to certificates.

The following readiness probes that use [gRPC Health Probe](https://github.com/grpc-ecosystem/grpc-health-probe) are ignored by GCE.

```
readinessProbe:
  exec:
    command: [""/hc"", ""-addr=127.0.0.1:9999"", ""-tls"", ""-tls-ca-cert=/tls/ca-chain.pem"", ""-tls-client-cert=/tls/client.pem"", ""-tls-client-key=/tls/client-key.pem""]
  initialDelaySeconds: 
livenessProbe:
  exec:
    command: [""/hc"", ""-addr=127.0.0.1:9999"", ""-tls"", ""-tls-ca-cert=/tls/ca-chain.pem"", ""-tls-client-cert=/tls/client.pem"", ""-tls-client-key=/tls/client-key.pem""]
  initialDelaySeconds: 10
```
There appears to be no way to override the creation of the GCE default HTTP2 health check. ",closed,False,2018-11-21 18:55:30,2018-11-27 16:44:03
ingress-gce,jeromefroe,https://github.com/kubernetes/ingress-gce/issues/554,https://api.github.com/repos/kubernetes/ingress-gce/issues/554,"Backend health is reported as ""Unknown"" if there are no pods in the first zone of a regional cluster","Currently, the health of a backend is determined by looking at the first `HealthStatus` of the service group:

```go
// TODO: Look at more than one backend's status
// TODO: Include port, ip in the status, since it's in the health info.
hs, err := b.cloud.GetGlobalBackendServiceHealth(name, be.Backends[0].Group)
if err != nil || len(hs.HealthStatus) == 0 || hs.HealthStatus[0] == nil {
	return ""Unknown""
}
// TODO: State transition are important, not just the latest.
return hs.HealthStatus[0].HealthState
```

For regional clusters, the call to `GetGlobalBackendServiceHealth` will return a backend, and therefore a `HealthStatus`, for each zone. Consequently, if the serving pods of a service are running in a subset of the zones in a cluster, then the health of the service will be marked as `Unknown` if that subset of zones doesn't include the first zone in the cluster.

As a concrete example, I have a regional cluster running in `us-east1`, and if I have a service with two serving pods, one in `us-east1-c` and `us-east1-d` respectively, then the state of the service will be ""Unknown"" because there are no serving pods in `us-east1-b` and `hs.HealthStatus[0]` is nil as a result. This can be confusing for a user because other services which also have two serving pods will be marked as healthy arbitrarily if one of their pods is in `us-east1-b`.

The code in question includes a TODO for looking at more than one backend's status and I would be more than happy to work on a PR to do just that. Before doing so though I wanted to start a discussion on what the desired logic should be. Three potential options I see are:

1. If any `HealthState` is `""HEALTHY""`, report the backend as `""HEALTHY""` as well.
2. If a majority of the non-nil `HealthState`'s are `""HEALTHY""`, report the backend as `""HEALTHY""` as well.
3. If any non-nil `HealthState` is `""UNHEALTHY""`, report the backend as `""UNHEALTHY""`.

My personal take would be to follow approach 2 as I think it offers a good balance between options 1 and 3. Unlike option 3, it won't report the backend as unhealthy due to temporary disruptions in the serving pods, but unlike option 1 it can signal potential major disruptions before all backends are unhealthy. This is by no means an exhaustive list, and we may want to consider nil `HealthState`'s as well (I'm not familiar enough with Google Cloud API myself to understand their significance) so any feedback would be greatly appreciated. Thanks!
",closed,False,2018-11-22 15:20:36,2018-11-27 17:52:24
ingress-gce,cdaguerre,https://github.com/kubernetes/ingress-gce/issues/555,https://api.github.com/repos/kubernetes/ingress-gce/issues/555,URL map / backend service mapping is totally shuffled,"Hi, I have a single ingress pointing to multiple backend services with the following configuration:
```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: services
  annotations:
    kubernetes.io/ingress.class: gce
    kubernetes.io/ingress.global-static-ip-name: {{ .Values.ipName }}
    ingress.gcp.kubernetes.io/pre-shared-cert: {{ .Values.certificateName }}
spec:
  backend:
    serviceName: prometheus-grafana
    servicePort: 80
  rules:
  - host: grafana.{{ .Values.host }}
    http:
      paths:
      - path: /
        backend:
          serviceName: prometheus-grafana
          servicePort: 80
  - host: prometheus.{{ .Values.host }}
    http:
      paths:
      - path: /
        backend:
          serviceName: prometheus-prometheus
          servicePort: 9090
  - host: alerts.{{ .Values.host }}
    http:
      paths:
      - path: /
        backend:
          serviceName: prometheus-alertmanager
          servicePort: 9093
  - host: rabbitmq.{{ .Values.host }}
    http:
      paths:
      - path: /
        backend:
          serviceName: rabbitmq
          servicePort: 15672
  {{- if .Values.mailcatcherEnabled }}
  - host: mailcatcher.{{ .Values.host }}
    http:
      paths:
      - path: /
        backend:
          serviceName: mailcatcher
          servicePort: 80
  {{- end }}
```
These are all nodePort services with:
prometheus: 32500
alertmanager: 32501
grafana: 32502
mailcatcher: 32600
rabbitmq: 30385 

The (automatically created) GCLB mixes up the url map, see below:
<img width=""645"" alt=""capture d ecran 2018-11-22 a 17 35 28"" src=""https://user-images.githubusercontent.com/4642448/48915230-3b1f5800-ee7e-11e8-974e-0606a1464e32.png"">
disclaimer: I searched the repo but didn't find any similar issues... ;)
",closed,False,2018-11-22 16:41:24,2018-11-23 08:26:20
ingress-gce,bowei,https://github.com/kubernetes/ingress-gce/pull/556,https://api.github.com/repos/kubernetes/ingress-gce/issues/556,defaultbackend image now has the architecture in the image name,fixes #549,closed,True,2018-11-26 19:01:30,2018-11-26 19:07:24
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/557,https://api.github.com/repos/kubernetes/ingress-gce/issues/557,Ingress-GCE docs overhaul,"Using this issue to track the work being done to revamp the documentation in this repository.

/assign",open,False,2018-11-26 21:34:35,2019-03-12 16:44:47
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/558,https://api.github.com/repos/kubernetes/ingress-gce/issues/558,Initialize directories which will house our updated documentation.,"Note that this PR does not attempt to delete or modify any existing documentation. This will be done step-by-step in follow up PR's.

Relevant Issue: #557",closed,True,2018-11-26 21:36:47,2018-12-10 18:09:28
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/559,https://api.github.com/repos/kubernetes/ingress-gce/issues/559,Scaffolding for FrontendConfig ,"This PR adds the scaffolding needed to implement features for a FrontendConfig CRD. 

A follow up PR will be added to include some documentation for contributors + possibly an example implementation as well.

Relevant Issue: #515",open,True,2018-11-26 21:43:25,2019-03-08 17:32:19
ingress-gce,Starefossen,https://github.com/kubernetes/ingress-gce/issues/560,https://api.github.com/repos/kubernetes/ingress-gce/issues/560,Example service and ingress gives Unknown Host error,"I am following the instructions in the README.md.

Kubernetes/GKE version:

```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-09T18:02:47Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10+"", GitVersion:""v1.10.9-gke.5"", GitCommit:""d776b4deeb3655fa4b8f4e8e7e4651d00c5f4a98"", GitTreeState:""clean"", BuildDate:""2018-11-08T20:33:00Z"", GoVersion:""go1.9.3b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

Creating the GCE ingress controller:

```
$ kubectl create -f examples/deployment/gce-ingress-controller.yaml
```

I am verifying that the controller is in fact running:

```
$ kubectl get pods -n kube-system | grep gce
gce-ingress-5ffdd58b7c-j2qnx                        1/1       Running   0          45m
```

Next I am crating the example service and ingress:

```
$ kubectl create -f examples/multi-path/svc.yaml examples/multi-path/gce-multi-path-ingress.yaml
```

And verifying that they are in fact created:

```
$ kubectl get all
NAME                    READY     STATUS    RESTARTS   AGE
pod/echoheaders-4hhcf   1/1       Running   0          2m

NAME                                DESIRED   CURRENT   READY     AGE
replicationcontroller/echoheaders   1         1         1         2m

NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/echoheadersx   NodePort   10.23.251.126   <none>        80:30301/TCP   2m
service/echoheadersy   NodePort   10.23.245.89    <none>        80:30284/TCP   2m

NAME      HOSTS                     ADDRESS         PORTS     AGE
echomap   foo.bar.com,bar.baz.com   35.241.43.197   80        4m
```

So far so good. Now accessing the Ingress Address directly `http://35.241.43.197` gives the `echoheadersx` backend. 

However any access using one of the configured hosts via `curl` or `/etc/hosts` results in an ""Unknown Host"" error like this:

```
$ curl -H ""Host: foo.bar.com"" http://35.241.43.197/foo
<HTML>
<HEAD>
<TITLE>Unknown Host</TITLE>
</HEAD>

<BODY BGCOLOR=""white"" FGCOLOR=""black"">
<H1>Unknown Host</H1>
<HR>

<FONT FACE=""Helvetica,Arial""><B>
Description: Unable to locate the server requested ---
the server does not have a DNS entry.  Perhaps there is a misspelling
in the server name, or the server no longer exists.  Double-check the
name and try again.
</B></FONT>
<HR>
</BODY>
```

Any suggestions?",closed,False,2018-11-27 15:31:57,2019-03-12 05:24:38
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/561,https://api.github.com/repos/kubernetes/ingress-gce/issues/561,rename basic_upgrade -> upgrade,,closed,True,2018-11-27 19:40:59,2019-02-06 19:55:19
ingress-gce,one000mph,https://github.com/kubernetes/ingress-gce/issues/562,https://api.github.com/repos/kubernetes/ingress-gce/issues/562,load balancer controller out of sync with gcp and ingress annotations,"I have a use case where we need to reassign a reserved external IP from one Ingress to another. In the GCP console and CLI I am seeing that all forwarding rules, url maps, etc. are configured correctly but in `kubectl describe ing/<ingress-name>` I am getting the following error
```
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Warning  Sync    6m (x28 over 55m)  loadbalancer-controller  Error during sync: googleapi: Error 400: Invalid value for field 'resource.IPAddress': 'XX.XXX.XXX.XXX'. Invalid IP address specified., invalid
```
The IP `XX.XXX.XXX.XXX` is the old IP that was assigned to the Ingress prior to reconfiguring the frontend (forwarding rules in GCP). If I view the load balancer config in GCP Console I see a different IP `YY.YYY.YYY.YYY`. The IP `XX.XXX.XXX.XXX` is no longer reserved in GCP at all.

I have the following annotations on my kubernetes Ingress
```
    ingress.kubernetes.io/forwarding-rule: [redacted] # maps YY.YYY.YYY.YYY:80 to the correct backends
    ingress.kubernetes.io/https-forwarding-rule: [redacted] # maps YY.YYY.YYY.YYY:443 to the correct backends
    ingress.kubernetes.io/https-target-proxy: [redacted]
    ingress.kubernetes.io/ssl-cert: [redacted]
    ingress.kubernetes.io/target-proxy: [redacted]
    ingress.kubernetes.io/url-map: [redacted]
```

Why is the IP `XX.XXX.XXX.XXX` still hanging around. It seems strange that I cannot reassign an external IP to a LoadBalancer/Ingress. Any insights into this problem would be appreciated.",closed,False,2018-11-28 01:00:10,2019-03-13 17:04:31
ingress-gce,DennisVis,https://github.com/kubernetes/ingress-gce/issues/563,https://api.github.com/repos/kubernetes/ingress-gce/issues/563,BackendConfig OpenAPI spec,"I'm using Deployment Manager to deploy all k8s resources by leveraging custom types. To create these types Deployment Manager needs access to an OpenAPI spec. For the Ingress this works fine, but I can't find the one for the BackendConfig type. 

I see one is created in `pkg/apis/backendconfig/v1beta1/zz_generated.openapi.go`, is this exposed anywhere on the gke cluster?",open,False,2018-11-29 10:34:52,2019-01-09 23:38:41
ingress-gce,hlee-va,https://github.com/kubernetes/ingress-gce/issues/564,https://api.github.com/repos/kubernetes/ingress-gce/issues/564,Possible to enable OCSP stapling?,"We have a gce ingress configured as the front-end to serve CDN contents in our system. Everything works fine except when using firefox to visit a CDN content via HTTPS, it will complain that `MOZILLA_PKIX_ERROR_REQUIRED_TLS_FEATURE_MISSING` because GCE ingress doesn't have OCSP stapling enabled. 

Been googling a lots but didn't find any useful answer. Is there any known workaround?
I found this commit which seems to solve the problem, but no idea how to use it. https://github.com/kubernetes/ingress-gce/commit/f6ba3abca3d99666ea7ece1b9738babba798e166

Here's our ingress yaml: 
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: cdn-ingress
  namespace: <<<namespace>>>
spec:
  tls:
    - secretName: cdn-wildcard-cert
  backend:
    serviceName: cdn-node-port-service
    servicePort: 80
  rules:
    - host: ""*.<<our-cdn-domain>>.com""
      http:
        paths:
          - path: /*
            backend:
              serviceName: cdn-node-port-service
              servicePort: 80
```

",open,False,2018-11-29 22:39:55,2018-12-09 20:12:24
ingress-gce,pondohva,https://github.com/kubernetes/ingress-gce/pull/565,https://api.github.com/repos/kubernetes/ingress-gce/issues/565,add lbName annotation,"this PR create feature for #369 

I've created annotation ""kubernetes.io/ingress.loadbalancer-name"" to share LB between ingresses. Work still in progress, but it contains a lot of changes, so please look at this PR to control progress.

",open,True,2018-12-01 13:00:08,2019-03-27 15:53:36
ingress-gce,acasademont,https://github.com/kubernetes/ingress-gce/issues/566,https://api.github.com/repos/kubernetes/ingress-gce/issues/566,BackendConfig support for user-defined request headers,Would be great if BackendConfig had support for user-defined request headers as per https://cloud.google.com/load-balancing/docs/backend-service#user-defined-request-headers,open,False,2018-12-03 20:26:01,2019-03-12 16:44:32
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/567,https://api.github.com/repos/kubernetes/ingress-gce/issues/567,Ingress upgrade testing fixes,,closed,True,2018-12-04 22:37:32,2018-12-05 18:26:53
ingress-gce,kakarot1994,https://github.com/kubernetes/ingress-gce/issues/568,https://api.github.com/repos/kubernetes/ingress-gce/issues/568,Not exposed in ingress,"I have noticed the feature is not exposed in Ingress.Please help out.

Thanks and Regards",closed,False,2018-12-05 03:58:44,2019-01-14 17:15:04
ingress-gce,m1kola,https://github.com/kubernetes/ingress-gce/issues/569,https://api.github.com/repos/kubernetes/ingress-gce/issues/569,The networking.gke.io/suppress-firewall-xpn-error: true annotation doesn't in combination with kubernetes.io/ingress.global-static-ip-name,"I tested on `1.11.3-gke.18` and it seems like there is an issue with `networking.gke.io/suppress-firewall-xpn-error` annotation: it doesn't seem to work in combination with `kubernetes.io/ingress.global-static-ip-name`. 

If you create a static IP address using `gcloud` and then create a new `Ingress` resource using a yaml similar to the one below the `Ingress` resource will be created without both `kubernetes.io/ingress.global-static-ip-name` and `networking.gke.io/suppress-firewall-xpn-error: true` annotations (at least this is what I see in `kubectl describe ingress.extensions/app`).

Also for new `Ingress` objects controller will get a new IP address which will be different from what you specify in `kubernetes.io/ingress.global-static-ip-name`.

**Expected result:** 

1. A new `Ingress` resource reuses previously reserved IP address (specified by name in the `kubernetes.io/ingress.global-static-ip-name` annotation).
2. Controller doesn't produce XPN firewall events
3. I can see both `kubernetes.io/ingress.global-static-ip-name` and `networking.gke.io/suppress-firewall-xpn-error` annotations in `kubectl describe ingress.extensions/app`.

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.global-static-ip-name: some-static-ip-name
    networking.gke.io/suppress-firewall-xpn-error: true
  name: app
  namespace: some-namespace
spec:
  backend:
    serviceName: app
    servicePort: 80
  tls:
  - hosts:
    - some.host.com
    secretName: some-tls-secret
```

---

Related issue #485 and PR #506  

cc @grayluck @rramkumar1 

/kind bug",open,False,2018-12-06 11:05:59,2019-03-26 21:09:07
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/570,https://api.github.com/repos/kubernetes/ingress-gce/issues/570,Create task queues with a name so that usage metrics can be tracked by Prometheus,/assign @bowei ,closed,True,2018-12-06 21:18:47,2018-12-10 18:16:30
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/571,https://api.github.com/repos/kubernetes/ingress-gce/issues/571,Add masterupgrading flag,,closed,True,2018-12-07 19:25:42,2019-02-06 19:55:43
ingress-gce,pondohva,https://github.com/kubernetes/ingress-gce/pull/572,https://api.github.com/repos/kubernetes/ingress-gce/issues/572,Implement custom request headers for backend config,"Add suport for custom request headers, issue #566 ",open,True,2018-12-09 18:12:27,2019-04-05 18:03:21
ingress-gce,jaceq,https://github.com/kubernetes/ingress-gce/pull/573,https://api.github.com/repos/kubernetes/ingress-gce/issues/573,Fixed misspell (annotation name) i README.md,"Fixed a misspell in annotation name. 
Now readme is correct and matching .yml files.",closed,True,2018-12-10 09:47:55,2018-12-10 17:46:03
ingress-gce,CodeLingoBot,https://github.com/kubernetes/ingress-gce/pull/574,https://api.github.com/repos/kubernetes/ingress-gce/issues/574,Fix error format strings according to best practices from CodeReviewComments,"Use [CodeLingo](https://codelingo.io) to automatically fix error format strings following the
[Go Code Review Comments guidelines](https://github.com/golang/go/wiki/CodeReviewComments) in [CONTRIBUTING.md](https://github.com/kubernetes/ingress-gce/blob/master/CONTRIBUTING.md).

This patch was generated by running the CodeLingo Rewrite Flow over the ""[go-error-fmt](https://github.com/codelingo/codelingo/blob/master/tenets/codelingo/code-review-comments/go-error-fmt/codelingo.yaml)"" Tenet. Note: the same Tenet can be used to automate PR reviews and generate contributor docs by [Installing](https://github.com/apps/codelingo) the CodeLingo GitHub app. Learn more at [codelingo.io](https://codelingo.io).",closed,True,2018-12-11 04:57:41,2019-01-09 00:07:44
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/575,https://api.github.com/repos/kubernetes/ingress-gce/issues/575,Increase deletion timeout for e2e tests,"Hoping this will mitigate some of the errors we are seeing. We need to figure out a way to maybe increase/decrease the timeout based on the environment (e.g how many ingresses are being synced)

/assign @MrHohn ",closed,True,2018-12-11 06:06:34,2018-12-11 06:11:56
ingress-gce,pondohva,https://github.com/kubernetes/ingress-gce/pull/576,https://api.github.com/repos/kubernetes/ingress-gce/issues/576,do not use condition predicate while getting zone for node,"This is small part of fix for #50 (fixes this log messages https://github.com/kubernetes/ingress-gce/issues/50#issuecomment-335960715)

GetNodeConditionPredicate filtered nodes with conditions different from ""Ready"", and we're getting this messages when node is deleting from instance group.",closed,True,2018-12-11 16:07:05,2019-01-03 12:12:32
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/577,https://api.github.com/repos/kubernetes/ingress-gce/issues/577,Ingress e2e test cleanup,"- Start and stop ConfigMap informer based on k8s master state
- Delete Ingress and break loop upon test finish",closed,True,2018-12-11 18:48:28,2019-02-06 19:55:43
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/578,https://api.github.com/repos/kubernetes/ingress-gce/issues/578,update openapi generator. generate v1 backendconfig files,,closed,True,2018-12-11 22:40:25,2019-02-05 21:25:12
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/579,https://api.github.com/repos/kubernetes/ingress-gce/issues/579,add a flag to control neg syncer type,enable transaction syncer as default in order for CI to pick up. ,closed,True,2018-12-13 22:22:46,2018-12-13 22:34:17
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/580,https://api.github.com/repos/kubernetes/ingress-gce/issues/580,remove loop from flush(). start/stop informers depending on k8s master,,closed,True,2018-12-13 23:04:49,2019-02-06 19:55:18
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/581,https://api.github.com/repos/kubernetes/ingress-gce/issues/581,fix a bug where transaction neg syncer will miss neg retry ,,closed,True,2018-12-13 23:36:28,2018-12-13 23:46:30
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/582,https://api.github.com/repos/kubernetes/ingress-gce/issues/582,reflect readiness probe in health check for NEG enabled ClusterIP service backend,fixes: https://github.com/kubernetes/ingress-gce/issues/541,closed,True,2018-12-14 18:41:49,2018-12-20 01:44:37
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/583,https://api.github.com/repos/kubernetes/ingress-gce/issues/583,Container Native Load Balancing Umbrella Issue,"This is the umbrella issue for container native load balancing, aka network endpoint group (NEG). 

Current Known Issues: 

- [X] [readiness probe not reflected on health check when NEG is enabled](https://github.com/kubernetes/ingress-gce/issues/541)

- [ ] [NEG Programming Latency](https://cloud.google.com/kubernetes-engine/docs/how-to/container-native-load-balancing#align_rollouts)

- [ ] [Disruption caused by 0->1, 1->0 transitions](https://cloud.google.com/kubernetes-engine/docs/how-to/container-native-load-balancing#scale-to-zero_workloads_interruption)

- [ ] [May leak NEG resource when cluster is deleted](https://cloud.google.com/kubernetes-engine/docs/how-to/container-native-load-balancing#incomplete_garbage_collection)",open,False,2018-12-14 19:16:13,2019-03-14 06:32:59
ingress-gce,JBodkin-LH,https://github.com/kubernetes/ingress-gce/issues/584,https://api.github.com/repos/kubernetes/ingress-gce/issues/584,Firewall change required by network admin,"When adding either an internal load balancer (service) or external load balancer (ingress), the events mention that a firewall change needs to be made manually when using a shared vpc. Would it be possible to add the ability to configure the firewall rules in a shared vpc?

```
Events:
  Type    Reason                    Age   From                Message
  ----    ------                    ----  ----                -------
  Normal  EnsuringLoadBalancer      17m   service-controller  Ensuring load balancer
  Normal  LoadBalancerManualChange  17m   gce-cloudprovider   Firewall change required by network admin: `gcloud compute firewall-rules create a8a48ebdf020011e9bba242010a9a001 --network xxx --description ""{\""kubernetes.io/service-name\"":\""xxx\"", \""kubernetes.io/service-ip\"":\""x.x.x.x\""}"" --allow tcp:3306 --source-ranges x.x.x.x/16 --target-tags xxx --project xxx`
  Normal  EnsuredLoadBalancer       17m   service-controller  Ensured load balancer
  Normal  UpdatedLoadBalancer       6m    service-controller  Updated load balancer with new hosts
```

I gave the service account permissions in the parent project to give network and compute admin. I'm successfully able to run the above command inside a pod running in the cluster with gcloud installed.

Cluster Version: 1.11.3-gke.18",closed,False,2018-12-17 13:58:54,2019-01-09 09:02:05
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/585,https://api.github.com/repos/kubernetes/ingress-gce/issues/585,"Revert ""Remove unused named ports from instance group's""","This reverts commit fbe88aa88b961ab683af8486da6e040cbd75584c.

/assign @bowei",closed,True,2018-12-17 17:41:15,2018-12-17 17:46:53
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/586,https://api.github.com/repos/kubernetes/ingress-gce/issues/586,Cherry-pick of #585 to 1.4 branch,/assign @bowei ,closed,True,2018-12-17 17:42:31,2018-12-17 17:48:36
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/587,https://api.github.com/repos/kubernetes/ingress-gce/issues/587,"[1.4 Cherry pick] Revert ""Remove unused named ports from instance group's""","This reverts commit fbe88aa88b961ab683af8486da6e040cbd75584c.

Cherry pick onto 1.4 branch

/assign @bowei ",closed,True,2018-12-17 17:50:42,2019-01-04 23:38:48
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/588,https://api.github.com/repos/kubernetes/ingress-gce/issues/588,Upgrade Test: Ignore connection refused errors when WaitForIngress() is called.,/assign @agau4779 ,closed,True,2018-12-19 22:34:15,2018-12-21 18:11:48
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/589,https://api.github.com/repos/kubernetes/ingress-gce/issues/589,Migrate to kubebuilder for BackendConfig [WIP],,closed,True,2018-12-20 17:55:29,2019-03-13 05:02:58
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/590,https://api.github.com/repos/kubernetes/ingress-gce/issues/590,Replace all uses of Snapshotter with CloudLister,"Follow-up to https://github.com/kubernetes/ingress-gce/pull/514 .

Replace the use of /pkg/storage/pools.go (Snapshotter) with a CloudLister interface, to be called as-needed during Garbage Collection. This should reduce the number of calls we make to the GCE API, as well as make GC more reliable.

We will use this for L7 loadbalancers, InstanceGroups, and Backends.

Verified that this properly garbage collects IGs, Backends, and Loadbalancers (TargetProxies, Forwarding Rules, URL Maps) via manual testing",closed,True,2018-12-20 22:46:53,2019-01-11 19:16:54
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/591,https://api.github.com/repos/kubernetes/ingress-gce/issues/591,Instance is Not Removed from IG when node is marked as unschedulable,"**How to reproduce it (as minimally and precisely as possible)**:
```
kubectl cordon ${Node}
```

**What happened**:

Ingress controller did not remove the corresponding instance from IG and emit error messages like this:
```
E1220 14:43:16.052232  257911 instances.go:214] Failed to get zones for e2e-test-mixia-minion-group-3mzg: node not found e2e-test-mixia-minion-group-3mzg, skipping
```

**What you expected to happen**:
Unschedulable node got removed from instance group.  


**Root Cause**:
Ingress controller failed to look up the corresponding zone of the instance when the node is marked as unschedulable. This is because the util function `GetZoneForNode` filter out the nodes that are not ready OR unschedulable. This is introduced here: #370

I think the `GetZoneForNode` should not filter out any nodes. It should return the zone of the node whenever is necessary. Thoughts?

/kind bug",closed,False,2018-12-20 22:52:09,2019-01-02 22:14:39
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/592,https://api.github.com/repos/kubernetes/ingress-gce/issues/592,"[E2E] In run.sh, add option to disable the resource dump at the end of the test.","In certain cases to reduce the noise in the logs, we may want to not dump all the network resources in the project.

/assign @bowei ",closed,True,2018-12-21 05:49:40,2018-12-21 18:03:28
ingress-gce,olib963,https://github.com/kubernetes/ingress-gce/issues/593,https://api.github.com/repos/kubernetes/ingress-gce/issues/593,Feature Request - Configuration via ConfigMap,"I would like to be able to inject the ingress configuration that is usually gathered from static annotations instead through a configmap. I am happy to look into implementing this myself if this would be an acceptable addition. I was hoping in the ingress I could write:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: my-ingress
  annotations:
    kubernetes.io/ingress.class: ""gce""
spec:
  confgMapName: ingress
  ...
```

where the configmap is

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: ingress
data:
    kubernetes.io/ingress.global-static-ip-name: ""my-ip-name""
    kubernetes.io/ingress.allow-http: ""false""
```

This would allow me to reuse the same declarative Gitops repo and simply terraform a new cluster and static IP, then have terraform inject the IP name as config.",closed,False,2018-12-28 09:36:27,2019-01-11 09:46:44
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/594,https://api.github.com/repos/kubernetes/ingress-gce/issues/594,add unit test for GetZoneForNode helper func,Add a unit test for #576,closed,True,2019-01-02 22:35:56,2019-01-02 22:48:51
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/595,https://api.github.com/repos/kubernetes/ingress-gce/issues/595,Ingress controller should react to node scale down event from autoscaler,"Cluster autoscaler adds the [ToBeDeletedByClusterAutoscaler](
https://github.com/kubernetes/autoscaler/blob/e7dd38375165997b057967259b0d577ca253eebb/cluster-autoscaler/utils/deletetaint/delete.go#L34) taint on the candidate node. Then it goes on to evict pods from the nodes. After everything settles, it will delete the node.

Ingress controller should observe the taint and react by removing instance from instance group so that connection draining is triggered. This help avoid new connection to be arrive at the removing nodes which causes 502s. ",open,False,2019-01-04 22:21:45,2019-03-28 21:10:58
ingress-gce,arianitu,https://github.com/kubernetes/ingress-gce/issues/596,https://api.github.com/repos/kubernetes/ingress-gce/issues/596,"Ingress controller does not work with TCP readiness probe, defaults back to HTTP","I want to use a TCP readiness probe, but it keeps creating a HTTP health check **instead of a TCP health check.** In my deployment of my service, I have this:


```
        readinessProbe:
          tcpSocket:
            port: 80
            
          initialDelaySeconds: 5
          periodSeconds: 10
          
        livenessProbe:
          tcpSocket:
            port: 80
              
          initialDelaySeconds: 15
          periodSeconds: 20

        ports:
          - containerPort: 80

```

When I go to the GCE health check tab, it still uses HTTP.  Is it because I am using port 80 here?",closed,False,2019-01-07 20:17:26,2019-03-26 21:19:14
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/597,https://api.github.com/repos/kubernetes/ingress-gce/issues/597,Switch to use the beta+ NEG API,,closed,True,2019-01-07 22:48:06,2019-02-15 00:11:18
ingress-gce,ConradIrwin,https://github.com/kubernetes/ingress-gce/issues/598,https://api.github.com/repos/kubernetes/ingress-gce/issues/598,BackendConfig timeoutSec doesn't seem to work with GCE Ingress,"I created a backendconfig object like this:

```
  apiVersion: cloud.google.com/v1beta1
  kind: BackendConfig
  metadata:
    name: mediaproxy-backend-config
  spec:
    timeoutSec: 120
    cdn:
      enabled: true
      cachePolicy:
        includeHost: true
        includeProtocol: true
        includeQueryString: true
```

and attached it to the service like this:

```
apiVersion: v1
kind: Service
metadata:
  name: mediaproxy
  labels:
    name: mediaproxy
  annotations:
    beta.cloud.google.com/backend-config: '{""ports"": {""8080"":""mediaproxy-backend-config""}}'
spec:
  type: NodePort
  ports:
    - port: 8080
  selector:
    name: mediaproxy
```

and created an ingress like this:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: mediaproxy
  labels:
    name: mediaproxy
  annotations:
    kubernetes.io/tls-acme: ""true""
spec:
  tls:
    - secretName: ingress-mediaproxy-lego
      hosts:
      - {{ .Domains.Mediaproxy }}
  backend:
    serviceName: mediaproxy
    servicePort: 8080
```

The load balancer created in my google cloud project correctly had the CDN enabled, but the timeout was still displayed as 30 seconds in the Google Cloud Console. I have since edited the timeout using the Google Cloud Console and it seems to stick with the value of 120 that I set manually.
",closed,False,2019-01-08 01:32:29,2019-01-08 01:51:46
ingress-gce,ConradIrwin,https://github.com/kubernetes/ingress-gce/issues/599,https://api.github.com/repos/kubernetes/ingress-gce/issues/599,BackendConfig `cdn: enable: true` uses scary defaults,"I created a backend config like this:

```
apiVersion: cloud.google.com/v1beta1
kind: BackendConfig
metadata:
  name: mediaproxy-backend-config
spec:
  timeoutSec: 120
  cdn:
    enabled: true
```

Expecting it to be equivalent to the default CDN policy, which is more explicitly written like this:

```
apiVersion: cloud.google.com/v1beta1
kind: BackendConfig
metadata:
  name: mediaproxy-backend-config
spec:
  timeoutSec: 120
  cdn:
    enabled: true
    cachePolicy:
      includeHost: true
      includeProtocol: true
      includeQueryString: true
```

Unfortunately the default if `cachePolicy` is omitted is actually equivalent to:

```
apiVersion: cloud.google.com/v1beta1
kind: BackendConfig
metadata:
  name: mediaproxy-backend-config
spec:
  timeoutSec: 120
  cdn:
    enabled: true
    cachePolicy:
      includeHost: false
      includeProtocol: false
      includeQueryString: false
```

This is a very unsafe default, in our case query parameters are essential to prevent data leakage between users, and I would like to ask that the default be the same as the Google Cloud CDN default.

For now I can work around the problem by explicitly setting the policy, but it was a nasty surprise.",closed,False,2019-01-08 01:50:00,2019-01-16 19:48:15
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/600,https://api.github.com/repos/kubernetes/ingress-gce/issues/600,Delete documentation that is now covered by GCP docs,"Ref: #557 

The bulk of the deletions here are happening in examples/ and in docs/annotations.md. All of this is now moved over to the GCP docs (see README.md for the links).

Also note that the new README.md mentions that CHANGELOG.md will contain high-level release notes along with a list of all changes. The high-level release notes will come in another CL. ",closed,True,2019-01-08 21:47:26,2019-01-08 23:01:21
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/601,https://api.github.com/repos/kubernetes/ingress-gce/issues/601,Add link to OSS Ingress docs in README.md,"/assign @MrHohn 

Ref #557",closed,True,2019-01-09 01:31:55,2019-01-09 01:36:46
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/602,https://api.github.com/repos/kubernetes/ingress-gce/issues/602,Release Summary: v1.4.1,"Release Notes:

- Fix InstanceGroup named port deletion bug - #587 
- [Alpha Feature] Bug fix for GCP Managed Certs integration - #552

**Available in GKE versions 1.11.6-gke.2 and above**
",closed,False,2019-01-09 15:31:36,2019-01-09 21:17:02
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/603,https://api.github.com/repos/kubernetes/ingress-gce/issues/603,Add scaffolding for supporting additional whitebox testing,,open,True,2019-01-10 17:45:47,2019-01-15 15:48:19
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/604,https://api.github.com/repos/kubernetes/ingress-gce/issues/604,Add changelog for v1.4.1 + Add config file for github-changelog-generator,/assign @MrHohn ,closed,True,2019-01-10 20:02:12,2019-01-10 20:13:54
ingress-gce,Arconapalus,https://github.com/kubernetes/ingress-gce/issues/605,https://api.github.com/repos/kubernetes/ingress-gce/issues/605,GKE ingress stuck in creating after deploying ingress 1.11.5.gke.5,"I also am running into this issue on 1.11.5-gke.5. The creating ingress is stuck and I have deleted and recreated the ingress. Should I delete node and cluster and recreate at 1.10.7.gke.2?
https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer
I followed this guide up to the point of apply -f basic-ingress.yml and ingress is stuck creating, 
I waiting for an hour then deleted and re-deployed",closed,False,2019-01-11 23:39:52,2019-02-22 19:25:55
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/issues/606,https://api.github.com/repos/kubernetes/ingress-gce/issues/606,Multiple e2e jobs are failing,"Ref https://k8s-testgrid.appspot.com/sig-network-ingress-gce-e2e#Summary. Below test jobs are failing since 1/10/2018:
- https://k8s-testgrid.appspot.com/sig-network-ingress-gce-e2e#ingress-gce-e2e
- https://k8s-testgrid.appspot.com/sig-network-ingress-gce-e2e#ingress-gce-e2e-multi-zone
- https://k8s-testgrid.appspot.com/sig-network-ingress-gce-e2e#ingress-gce-e2e-scale
- https://k8s-testgrid.appspot.com/sig-network-ingress-gce-e2e#ingress-gce-downgrade-e2e

Likely some recent change broke the head? From the timestamp, probably https://github.com/kubernetes/ingress-gce/pull/590?

Some errors from GCLB logs (https://storage.googleapis.com/kubernetes-jenkins/logs/ci-ingress-gce-e2e/3822/artifacts/e2e-3822-58614-master/glbc.log):
```
E0114 14:31:54.681044       1 taskqueue.go:62] Requeuing ""ingress-3548/echoheaders-https"" due to error: error running post-process routine: update ingress status error: ingresses.extensions ""echoheaders-https"" not found (ingresses)
...
E0114 14:49:22.544623       1 taskqueue.go:62] Requeuing ""ingress-3942/multiple-certs"" due to error: error running post-process routine: update ingress status error: ingresses.extensions ""multiple-certs"" not found (ingresses)
...
E0114 16:44:21.380627       1 taskqueue.go:62] Requeuing ""ingress-6176/echomap"" due to error: error running post-process routine: update ingress status error: ingresses.extensions ""echomap"" not found (ingresses)
...
E0114 13:13:36.986365       1 taskqueue.go:62] Requeuing ""ingress-9079/hostname"" due to error: error running post-process routine: update ingress status error: ingresses.extensions ""hostname"" not found (ingresses)
```

cc @agau4779 @rramkumar1 ",closed,False,2019-01-14 19:06:39,2019-01-15 17:43:15
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/607,https://api.github.com/repos/kubernetes/ingress-gce/issues/607,standardize loadbalancer naming for GC,"Addresses https://github.com/kubernetes/ingress-gce/issues/606

Fixes the Garbage Collection routine in l7s. GC takes in `names` as an argument, where `names` comes from the garbage collection state, which is a list of Ingress names which are formatted something like `something-a/something-b`. We derive the loadbalancer name from the `something-a` portion, by affixing the clusterid to it.",closed,True,2019-01-14 19:56:52,2019-02-06 19:55:18
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/608,https://api.github.com/repos/kubernetes/ingress-gce/issues/608,do not use condition predicate while getting zone for node,Cherrypick #576 into 1.4 release,closed,True,2019-01-14 20:04:25,2019-01-14 20:04:39
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/609,https://api.github.com/repos/kubernetes/ingress-gce/issues/609,do not use condition predicate while getting zone for node,Cherrypick #576 into 1.4 release,closed,True,2019-01-14 20:05:09,2019-01-14 21:18:56
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/610,https://api.github.com/repos/kubernetes/ingress-gce/issues/610,"methods should use parts of l7 that they need, not whole l7 object","Follow-up to comment on https://github.com/kubernetes/ingress-gce/pull/590

Change method signatures in pkg/loadbalancers such that `Cleanup()` does not need to instantiate a new L7 object to work.",closed,True,2019-01-15 02:02:35,2019-02-06 19:55:18
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/611,https://api.github.com/repos/kubernetes/ingress-gce/issues/611,[BackendConfig] CDN default cache key policy should be true instead of false.,"Fixes #599

Currently verifying whether this works and other existing state transitions still hold. ",closed,True,2019-01-15 21:08:59,2019-01-16 18:14:58
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/612,https://api.github.com/repos/kubernetes/ingress-gce/issues/612,(1.4 Cherry Pick) [BackendConfig] CDN default cache key policy should be true instead of false,"/assign @MrHohn 

Ref: #611 ",closed,True,2019-01-16 18:18:47,2019-01-16 18:23:58
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/613,https://api.github.com/repos/kubernetes/ingress-gce/issues/613,Add Finalizers for Ingress,"The finalizer `ingress.finalizer.cloud.google.com` will be applied to the Ingress upon update if it does not have a finalizer yet. It will be removed only after the resources related to the Ingress have been deleted first. Absence of a finalizer means the Ingress itself can be safely deleted.

If a resource used for this ingress fails to be deleted because it’s ‘in use’ by another resource, then the resource will be skipped. If it cannot be deleted for another reason, the controller will attempt deletion at a later attempt.
",closed,True,2019-01-16 23:00:43,2019-01-28 18:58:59
ingress-gce,mcfedr,https://github.com/kubernetes/ingress-gce/issues/614,https://api.github.com/repos/kubernetes/ingress-gce/issues/614,Sync error when ingress name has dots,"Its valid for kubernetes resources to have dots in the names, so an ingress with a name like `some.host.com-ingress` is valid, but the gce ingress fails when you create such an ingress, with an error like

```
Error during sync: Error running load balancer syncing routine: googleapi: Error 400: Invalid value 'k8s-um-default-some.home.com-ingress--521bdf95908d2d0'. Values must match the following regular expression: '[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?', invalidParameter
```

This mismatch of valid values could be handled by the controller",open,False,2019-01-17 20:52:50,2019-01-22 07:54:39
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/615,https://api.github.com/repos/kubernetes/ingress-gce/issues/615,Add changelog for 1.4.2 release,,closed,True,2019-01-18 21:57:49,2019-02-06 19:55:18
ingress-gce,jpigree,https://github.com/kubernetes/ingress-gce/issues/616,https://api.github.com/repos/kubernetes/ingress-gce/issues/616,BackendConfig security policy not enforced ,"Hi. I created a GKE cluster in version ""1.11.5-gke.5"" (with autoscaling activated) and I use the pre-installed gce ingress controller to expose my applications on the WAN. However, I need to **firewall** them (filtering on source IP) so I use the ""security policy"" field in the BackendConfig object to enforce my cloud armor policy on the load balancer. However I have a hard time making it work consistently.

Indeed, I often end up in a state where the cloud armor policy is not enforced without any change on the BackendConfig object. And actually the only steps to make it work again are to empty the ""security policy"" field/recreate the BackendConfig object until it works.

Here is my current configuration for a simple helloworld application:
```yaml
apiVersion: cloud.google.com/v1beta1
kind: BackendConfig
metadata:
  name: internal-http
  labels:
    app.kubernetes.io/name: ""helloworld""
spec:
  securityPolicy:
    name: ""<cloud armor policy name>""
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld
  labels:
    app.kubernetes.io/name: ""helloworld""
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ""helloworld""
    spec:
      containers:
      - name: helloworld
        image: <a simple flask application answering with helloworld when receiving HTTP GET on /hello>
        ports:
          - name: http
            containerPort: 5000
---
apiVersion: v1
kind: Service
metadata:
  name: helloworld
  labels:
     app.kubernetes.io/name: ""helloworld""
  annotations:
    beta.cloud.google.com/backend-config: '{""ports"": {""http"":""internal-http""}}'
spec:
  type: ""NodePort""
  ports:
  - port: 5000
    name: http
  selector:
    app.kubernetes.io/name: ""helloworld""
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: helloworld
  labels:
    app.kubernetes.io/name: ""helloworld""
  annotations:
    kubernetes.io/ingress.class: ""gce""
    kubernetes.io/ingress.allow-http: false
    certmanager.k8s.io/cluster-issuer: letsencrypt
    kubernetes.io/tls-acme: “true”
    kubernetes.io/ingress.global-static-ip-name: ""<the static reserved ip name>""
spec:
  tls:
  - hosts:
    - <helloworld fqdn>
    secretName: <secret containing Letsencrypt certificate>
  rules:
  - host: <helloworld fqdn>
    http:
      paths:
      - path: /hello
        backend:
          serviceName: helloworld
          servicePort: 5000
```

This configuration stopped working when I recreated my cluster and reapplied my manifests. I think this is due to my recreation, because I did it with terraform which didn't emptied the ""securityPolicy"" field of the BackendConfig object before deletion. Is this the expected behaviour though? What can I do to recover when this happens?

When debugging, I saw that describing the backendconfig object does not tell the state of the load balancer. Is there another way of getting those informations?

Finally, I am a bit scared to use the BackendConfig to firewall my services right now, because it can potentially expose my services to the WAN even if my desired state explicitly tells otherwise without throwing an error.

I will gladly take advices here. Thanks for your help.


",closed,False,2019-01-23 04:24:12,2019-02-18 21:34:45
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/617,https://api.github.com/repos/kubernetes/ingress-gce/issues/617,Move stuff in deploy/ into docs/deploy [WIP],,closed,True,2019-01-23 23:23:42,2019-03-05 21:26:01
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/618,https://api.github.com/repos/kubernetes/ingress-gce/issues/618,make backoff retry handler unit test faster,"Use FakeClock to make unit test run faster. 
",closed,True,2019-01-24 20:43:23,2019-01-25 01:07:00
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/619,https://api.github.com/repos/kubernetes/ingress-gce/issues/619,Add more unit tests for transaction syncer,,closed,True,2019-01-28 20:09:19,2019-02-15 00:51:47
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/620,https://api.github.com/repos/kubernetes/ingress-gce/issues/620,Simplify upgrade_test - only loop during k8s master changes.,"Ingress Upgrade test flow is roughly: 
- create Ingress, whitebox test
- loop until master upgraded flag is set, then exit loop when the Ingress path update is triggered
- whitebox test the Ingress again
- delete all resources",closed,True,2019-01-30 18:19:49,2019-02-06 19:55:19
ingress-gce,srikanth19,https://github.com/kubernetes/ingress-gce/issues/621,https://api.github.com/repos/kubernetes/ingress-gce/issues/621,GCP - Kubernetes Ingress Backend service unhealthy,"Hi, 

I have deployed a SAS Viya programming on a pod using the below deployment file. This has SASStudio running on port 80. I am trying to expose the SAS Studio using the nodeport and Ingress through below yaml files. But in the GCP console, the backend service is displayed has unhealthy and if I try to access the Ip either it is hitting default backend or page not found. I am able to access the service using the cluster IP and node IP along with the port number.

If I use LoadBalancer service instead of Ingress, I am able to access the service using the Loadbalancer Ip.

When using the Ingress, the backend service is shown as unhealthy.

I have used the curl command with node Ip and nodeport Ip. Below is the result.

[root@k8jumphost doc]# curl -ILS ""10.142.0.19:30100""
HTTP/1.1 302 Found
Date: Fri, 01 Feb 2019 07:53:31 GMT
Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips
Location: http://10.142.0.19:30100/SASStudio
Content-Type: text/html; charset=iso-8859-1

HTTP/1.1 302
Date: Fri, 01 Feb 2019 07:53:32 GMT
Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips
Location: /SASStudio/
Transfer-Encoding: chunked

HTTP/1.1 200
Date: Fri, 01 Feb 2019 07:53:34 GMT
Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips
Content-Security-Policy: default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data:;
X-Content-Type-Options: nosniff
X-XSS-Protection: 1; mode=block
Content-Type: text/html;charset=UTF-8
Content-Language: en-US
Content-Length: 7836



[root@k8jumphost working]# cat sas-analytics.yaml
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: sas-programming-deployment
  labels:
    app: sas-programming
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: sas-programming
    spec:
      securityContext:
        fsGroup: 1001
      hostname: sas-programming
      containers:
      - name: sas-programming
        image: gcr.io/sgf-sas-on-k8/sas-viya-programming
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - mountPath: /sas/sasdata
          name: sasdata-mount
        ports:
        - containerPort: 5570
        - containerPort: 80
        env:
        - name: CASENV_CAS_VIRTUAL_HOST
          value: ""sas-programming""
        - name: CASENV_CAS_VIRTUAL_PORT
          value: ""30300""
      volumes:
      - name: sasdata-mount
        persistentVolumeClaim:
          claimName: sas-programming-pvc

[root@k8jumphost doc]# cat sas-analytics-nodeport.yaml
---

apiVersion: v1
kind: Service
metadata:
  name: sas-programming-svc-nodeport
  labels:
    app: sas-programming
spec:
  selector:
    app: sas-programming
  ports:
  - name: http
    nodePort: 30100
    port: 80
    targetPort: 80
  - name: cas
    nodePort: 30200
    port: 5570
    targetPort: 5570
  type: NodePort

[root@k8jumphost doc]# cat sas-analytics-ingress.yaml
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: sas-programming-ingress
  labels:
    app: sas-programming
spec:
  rules:
  - host: sas-programming
    http:
      paths:
      - path: /
        backend:
          serviceName: sas-programming-svc-nodeport
          servicePort: 80
  ",closed,False,2019-02-01 08:40:47,2019-02-07 22:26:49
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/622,https://api.github.com/repos/kubernetes/ingress-gce/issues/622,clean up HTTP(S) resources when changing protocols,Addresses #32 ,closed,True,2019-02-01 19:22:37,2019-02-06 21:43:23
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/623,https://api.github.com/repos/kubernetes/ingress-gce/issues/623,Configure leader election with completely separate k8s client,"Due to some scaling issues, we need to configure leader election with a separate k8s client then what is used for other API server operations (watch, events, etc). 

This puts ingress-gce in line w/ how scheduler and controller-manager implement leader election. ",closed,True,2019-02-05 01:23:10,2019-02-06 04:27:14
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/624,https://api.github.com/repos/kubernetes/ingress-gce/issues/624,Configure leader election with completely separate k8s client [release-1.4],"Cherrypick #623 onto release-1.4 branch

/assign @bowei ",closed,True,2019-02-06 17:40:09,2019-02-18 21:37:03
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/625,https://api.github.com/repos/kubernetes/ingress-gce/issues/625,"Cherry-pick #613 onto 1.4, part 2","Cherry pick https://github.com/kubernetes/ingress-gce/pull/613 , part 2. 

Part 1: https://github.com/kubernetes/ingress-gce/pull/627",closed,True,2019-02-06 18:25:00,2019-02-06 19:50:05
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/626,https://api.github.com/repos/kubernetes/ingress-gce/issues/626,do not leak LB when ingress class is changed [release-1.4],Cherry-picking #547 ,closed,True,2019-02-06 18:35:06,2019-02-18 21:39:44
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/627,https://api.github.com/repos/kubernetes/ingress-gce/issues/627,"Cherry-pick #613 onto 1.4, part 1","Cherry-picking #613, part 1",closed,True,2019-02-06 18:59:44,2019-02-06 19:49:44
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/628,https://api.github.com/repos/kubernetes/ingress-gce/issues/628,Cherry-pick #613 onto 1.4 branch,Cherry-pick 2nd commit in #613 ,closed,True,2019-02-06 20:03:41,2019-02-06 21:42:27
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/629,https://api.github.com/repos/kubernetes/ingress-gce/issues/629,clean up HTTP(S) resources when changing protocols,Addresses #32 ,closed,True,2019-02-06 21:44:56,2019-02-09 00:06:18
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/630,https://api.github.com/repos/kubernetes/ingress-gce/issues/630,Update GKE version mapping in README,/assign @MrHohn ,closed,True,2019-02-07 16:58:47,2019-02-07 17:54:24
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/631,https://api.github.com/repos/kubernetes/ingress-gce/issues/631,clean up HTTP(S) resources when changing protocols,,open,True,2019-02-09 00:04:34,2019-02-11 18:37:29
ingress-gce,rodlogic,https://github.com/kubernetes/ingress-gce/issues/632,https://api.github.com/repos/kubernetes/ingress-gce/issues/632,Defaults per cluster or project?,"Is there a way to provide a default TLS certificate or set of certificates when creating a LB that is not at the Ingress or Service level as an annotation?

I would like to provision one TLS certificate and one DNS name for the cluster, but I don't want to have to specify the TLS cert annotation on every ingress/service. If the ingress specifies a host that is not part of the cert they app owner is out of luck and should either correct the ingress or deploy it elsewhere.

 ",open,False,2019-02-09 12:00:14,2019-02-20 19:14:56
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/633,https://api.github.com/repos/kubernetes/ingress-gce/issues/633,reflect readiness probe in health check for NEG enabled ClusterIP service backend [release-1.4],"Cherrypick #582 onto release-1.4 branch

bug fix: correctly reflect readiness probe in health check for NEG enabled ClusterIP service backend",closed,True,2019-02-11 18:41:48,2019-02-18 21:38:00
ingress-gce,domcar,https://github.com/kubernetes/ingress-gce/issues/634,https://api.github.com/repos/kubernetes/ingress-gce/issues/634,How to expose ports,"Hi,

I wonder how I can open tcp ports with ingress-gce.
To be clear, I used until now nginx-controller and I used to  create a configmap, for example:
```
apiVersion: v1
data:
  ""22"": gitlab/gitlab-gitlab-shell:22
kind: ConfigMap
metadata:
  name: gitlab-nginx-ingress-tcp
  namespace: gitlab
```
With this configmap I can use port 22 on my gitlab deployment.

How can I achieve the same with gce-ingress?
If the same question was already answered or if there is a tutorial I can follow, could you please point me to it?

Thanks for the help!",closed,False,2019-02-12 13:36:25,2019-03-11 23:40:02
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/635,https://api.github.com/repos/kubernetes/ingress-gce/issues/635,"Revert ""Cherry-pick #613 onto 1.4 branch""","Reverts kubernetes/ingress-gce#628. This did not pick back cleanly. 

/assign @MrHohn ",closed,True,2019-02-12 18:40:24,2019-02-12 18:44:24
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/636,https://api.github.com/repos/kubernetes/ingress-gce/issues/636,"Revert ""Cherry-pick #613 onto 1.4, part 1""","Reverts kubernetes/ingress-gce#627. This also did not pick back cleanly.

/assign @MrHohn ",closed,True,2019-02-12 18:44:49,2019-02-12 18:59:28
ingress-gce,michallowicki,https://github.com/kubernetes/ingress-gce/pull/637,https://api.github.com/repos/kubernetes/ingress-gce/issues/637,Remove direct support for ManagedCertificate CRD,Managed certificates controller will use pre-shared-cert annotation.,closed,True,2019-02-13 13:39:44,2019-02-13 17:01:07
ingress-gce,inversion,https://github.com/kubernetes/ingress-gce/issues/638,https://api.github.com/repos/kubernetes/ingress-gce/issues/638,Switching service selector causes small amount of downtime with NEGs,"I have implemented a blue/green update strategy for my deployment by updating a version label in the deployment template. I then update the service selector to use the new version label when enough pods of the new version are ready.

I have found that strategy this causes about 5-10 seconds of downtime (HTTP 502s) when using NEGs. The service events show:

```
Detach 2 network endpoint(s) (NEG ""k8s1-my-service-neg"" in zone ""us-central1-b"")
... 2 second delay...
Attach 1 network endpoint(s) (NEG ""k8s1-my-service-neg"" in zone ""us-central1-b"")
```

There were two pods of the old version (matched by the old service selector) and there is one pod of the new version at this point.

I'm wondering if there is a way to avoid changing service selectors causing downtime?

I suspect this issue is related to
https://cloud.google.com/kubernetes-engine/docs/how-to/container-native-load-balancing#scale-to-zero_workloads_interruption (#583)",open,False,2019-02-13 14:27:37,2019-02-20 22:52:30
ingress-gce,FrankPetrilli,https://github.com/kubernetes/ingress-gce/pull/639,https://api.github.com/repos/kubernetes/ingress-gce/issues/639,Import limitations and add that pods must exist,"Import limitations from examples/health-checks/README.md and add new limitation mentioned in #241.

This caveat that all pods must exist is unclear to GCP customers and should be documented as a known limitation.",closed,True,2019-02-13 23:41:54,2019-02-26 11:07:22
ingress-gce,CodeLingoBot,https://github.com/kubernetes/ingress-gce/pull/640,https://api.github.com/repos/kubernetes/ingress-gce/issues/640,CodeLingo Setup,"Add the Effective Go and Code Review Comments Tenet Bundles from https://golang.org/doc/effective_go.html and https://github.com/golang/go/wiki/CodeReviewComments. You need to [install](https://github.com/apps/codelingo) the CodeLingo GitHub App to automate code reviews, bug fixes and contributor docs with these Tenets. Find more Tenet bundles at [www.codelingo.io/tenets](link)",closed,True,2019-02-13 23:42:15,2019-02-19 03:46:32
ingress-gce,michallowicki,https://github.com/kubernetes/ingress-gce/pull/641,https://api.github.com/repos/kubernetes/ingress-gce/issues/641,Support secret-based and pre-shared certs at the same time,"This should allow for a no-downtime migration from secret-based to pre-shared certs, as the secret-based cert will be still used until the secret is removed.",closed,True,2019-02-14 14:49:01,2019-02-22 16:07:55
ingress-gce,agau4779,https://github.com/kubernetes/ingress-gce/pull/642,https://api.github.com/repos/kubernetes/ingress-gce/issues/642,Add backendconfig v1,"This PR adds v1 version of the BackendConfig CustomResourceDefinition. It is feature gated behind a flag, `EnableBackendConfigV1`. Note that to enable BackendConfig v1, ingress requires both `EnableBackendConfigV1` and `EnableBackendConfig` to be true.

We also introduce a composite type, `backendconfig.BackendConfig`, to house the shared attributes between v1 and v1beta1. ",open,True,2019-02-15 01:15:11,2019-02-22 18:58:42
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/643,https://api.github.com/repos/kubernetes/ingress-gce/issues/643,Update CHANGELOG and version mapping for v1.4.3,/assign @MrHohn ,closed,True,2019-02-18 21:41:52,2019-02-19 18:33:34
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/644,https://api.github.com/repos/kubernetes/ingress-gce/issues/644,Move lone function in kubeapi.go into existing utils.go,,closed,True,2019-02-19 04:38:36,2019-02-19 19:35:26
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/645,https://api.github.com/repos/kubernetes/ingress-gce/issues/645,Better UX for user who have HttpLoadBalancing add-on disabled,"As per comment by @zvictor in #605, for users who have accidently disabled the HttpLoadBalancing add-on, there needs to be a better way to notify them that creating an Ingress will not result in any action (since the controller is not deployed).

Using this issue to track discussion, design, etc.",open,False,2019-02-20 16:33:50,2019-03-05 02:42:01
ingress-gce,ashi009,https://github.com/kubernetes/ingress-gce/issues/646,https://api.github.com/repos/kubernetes/ingress-gce/issues/646,Controller fail on syncing config when a service is not found,"Ingress controller fails to sync the entire config in case a backend service is missing.

We ran into last week. We deployed the ingress config for a new service along with other changes to the cluster before actually deploying the new service. None of the change was made until the new service is deployed. At the meantime ingress controller only reported a warning mentioned nothing on the gravity of the issue. We later realized this issue as some other changes are not effective, though all the checks against the ingress look sane.

GKE service console also failed to show this kind of issues as errors, it looks just like a minor warning:
![image](https://user-images.githubusercontent.com/687367/53396822-66b36080-39e0-11e9-8fd9-0e5e13d4df14.png)

IMHO, the ingress controller should do partial sync, and create some dummy backend resources that are unhealthy. (I think the major issue here is about the naming, as current implementation uses node port number to name backends.)
",open,False,2019-02-26 08:09:18,2019-03-13 04:52:16
ingress-gce,iftachsc,https://github.com/kubernetes/ingress-gce/issues/647,https://api.github.com/repos/kubernetes/ingress-gce/issues/647,If readiness probe is on port different than the service (app) port - ingress failes to sync the correct healthcheck,"I have a deployment that its healthcheck is running on different port than the app port (80).
our Ingress resource creates a HTTP loadbalancer with HTTP healthcheck on the root path (/)-  on the NodePort exposing the service port 80. 

the service has definition of the healtcheck port (in addition to the app port) and also the deployment has it, including containerPort field. the pod is healthy and apears under Endpoints.

in order to make sure this is the issue, changing the readinessProbe port to the serving port and recreating ingress yielded the correct healthcheck, reading the correct path this time (/healthz/ready)

this is a big issue for us. that actually convinces us to let go HTTP LB and go TCP LB because we dont want to manually create the HTTP LB risking connectivity loss if instance groups change underneath. 
",closed,False,2019-02-26 12:40:06,2019-03-20 16:47:15
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/648,https://api.github.com/repos/kubernetes/ingress-gce/issues/648,Shorten the name of the namespace for test sandboxes,"Shortening the name of the namespace will allow us to not hit the name truncation bug that currently exists.

This is a temporary measure until we have a concrete design on how to fix the bug. ",closed,True,2019-02-26 20:35:25,2019-02-26 21:00:45
ingress-gce,dacox,https://github.com/kubernetes/ingress-gce/issues/649,https://api.github.com/repos/kubernetes/ingress-gce/issues/649,Removing Node Pool from Cluster Breaks Ingress Conroller,"Hi everyone,

we're currently on `1.11.6-gke.11`. We've had some issues with `gce-ingress` in the past, and after a recent outage are trying to dig into the root cause.

We currently have a handful of Node Pools and use an Ingress to map traffic to two services in our cluster. This has created the Cloud Load Balancer with two Backend Services with one health check each. Each Backend Service points to the same Instance Group that is also created by the controller (am I getting this right?).

As I understand, because our Service's have `externalTrafficPolicy=Cluster (default)`, both Backend Services show all nodes in the Cluster as healthy, not just the ones that pertain to our Services.

We recently removed a Node Pool from the Cluster. To do this, we cordoned each node in the pool and then drained them. A few hours later we removed the node pool.

Immediately upon deleting the pool, we experienced a large amount of downtime. It did not seem to be resolved when the last node went offline. At one point, we saw that both Backend Services reported `0/0` as healthy.

After about half an hour, we were able to stop the fire by `kubectl delete ingress-myingress` and then re-creating it.

We now understand that we would get some 502s when the nodes were being deleted due to our `externalTrafficPolicy`. What we are having a hard time grappling with is why the outage did not recover until we re-created the ingress. 

Cheers!

",open,False,2019-02-26 23:11:41,2019-03-04 20:52:23
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/650,https://api.github.com/repos/kubernetes/ingress-gce/issues/650,Modify NameBelongToCluter to tolerate truncated cluster name suffix,ref: #537,closed,True,2019-02-26 23:38:47,2019-02-27 18:47:19
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/651,https://api.github.com/repos/kubernetes/ingress-gce/issues/651,Clean up unused feature flags for NEG and BackendConfig,"Remove feature flags for both NEG and BackendConfig. Both these features are stable and about to go GA soon so there is no reason to flag gate them. Any further improvements to both these features will have their own flag-gate, if necessary. 

Also clean up the deprecated flags --verbose and --use-real-cloud.",closed,True,2019-02-27 00:54:20,2019-03-13 06:00:51
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/652,https://api.github.com/repos/kubernetes/ingress-gce/issues/652,Cherrypick #650 into release 1.5 branch,Ref: #650,closed,True,2019-02-27 19:38:14,2019-02-27 21:26:56
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/653,https://api.github.com/repos/kubernetes/ingress-gce/issues/653,"Revert ""Shorten the name of the namespace for test sandboxes""","Reverts kubernetes/ingress-gce#648

#650 is the permanent solution.",closed,True,2019-02-27 22:08:30,2019-02-27 22:16:38
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/654,https://api.github.com/repos/kubernetes/ingress-gce/issues/654,Fix upgrade test to create an Ingress with no default backend,"Because the created Ingress previously specified a default backend, it was possible that the new path we add post-upgrade could fail but we still pass the test. This happens because the service being used for the default backend and the service being used for the new path are the same. 

",closed,True,2019-02-28 17:14:49,2019-03-01 00:52:50
ingress-gce,yuwenma,https://github.com/kubernetes/ingress-gce/pull/655,https://api.github.com/repos/kubernetes/ingress-gce/issues/655,Rebase base images from scratch and alpine to distroless,"[Kubernetes issue/70249
](https://github.com/kubernetes/kubernetes/issues/70249)

Internal Doc: go/rebase-gke-image-to-distroless

Test: Tested the Dockerfile files validation via ""make build bin"".
",closed,True,2019-02-28 20:18:52,2019-03-05 22:26:40
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/656,https://api.github.com/repos/kubernetes/ingress-gce/issues/656,Update glbc.yaml with latest minor release tag,/assign @MrHohn ,closed,True,2019-03-01 00:20:15,2019-03-01 00:29:50
ingress-gce,linydquantil,https://github.com/kubernetes/ingress-gce/issues/657,https://api.github.com/repos/kubernetes/ingress-gce/issues/657,Does it support a second jump?,"Our back-end service is consul-cluster, I deployed nginx services in front of consul cluster;
But when I'm visiting, it can jump to /ui normally. But what's displayed on the page is default server - 404
And then I use loadbalancer to open web services； It will have normal access to the back-end consul ui；
This is how I deploy consul：https://github.com/kelseyhightower/consul-on-kubernetes
and my nginx services:
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-consul-conf
data:
  nginx.conf: |
    user nginx;
    worker_processes  2;
    error_log  /var/log/nginx/error.log;
    events {
      worker_connections  10240;
    }
    http {
      server_tokens off;
      log_format  main
              'remote_addr:$remote_addr\t'
              'time_local:$time_local\t'
              'method:$request_method\t'
              'uri:$request_uri\t'
              'host:$host\t'
              'status:$status\t'
              'bytes_sent:$body_bytes_sent\t'
              'referer:$http_referer\t'
              'useragent:$http_user_agent\t'
              'forwardedfor:$http_x_forwarded_for\t'
              'request_time:$request_time';
      access_log    /var/log/nginx/access.log main;
      server {
          listen       18500;
          server_name  _;
          location / {
              root   html;
              index  index.html index.htm;
          }
      }
      include /etc/nginx/virtualhost/virtualhost.conf;
    }
  htpasswd: |
    consul_access:$apr1$2JLE03xxxxxxU6.
  virtualhost.conf: |
    upstream consul {
      server consul-0.consul.xxx.svc.cluster.local:8500;
      server consul-1.consul.xxx.svc.cluster.local:8500;
      server consul-2.consul.xxx.svc.cluster.local:8500;
    }
    server {
      listen 18500 default_server;
      server_name _;
      access_log /var/log/nginx/consul.access.log main;
      error_log /var/log/nginx/consul.error.log;
      location / {
        proxy_http_version 1.1;
        proxy_pass http://consul;
        proxy_read_timeout 300;
        proxy_connect_timeout 300;
        proxy_redirect off;
        auth_basic ""Restricted"";
        auth_basic_user_file /etc/nginx/htpasswd;
      }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-consul-ui
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-consul-ui
  template:
    metadata:
      labels:
        app: nginx-consul-ui
    spec:
      containers:
      - name: nginx
        image: nginx:1.14
        imagePullPolicy: Always
        ports:
        - containerPort: 18500
        volumeMounts:
        - mountPath: /etc/nginx # mount nginx-conf volumn to /etc/nginx
          readOnly: true
          name: nginx-conf
        - mountPath: /var/log/nginx
          name: log
        resources:
          limits:
            cpu: ""512m""
            memory: 512Mi
          requests:
            cpu: ""256m""
            memory: 256Mi
      volumes:
      - name: nginx-conf
        configMap:
          name: nginx-consul-conf # place ConfigMap `nginx-conf` on /etc/nginx
          items:
            - key: nginx.conf
              path: nginx.conf
            - key: htpasswd
              path: htpasswd
            - key: virtualhost.conf
              path: virtualhost/virtualhost.conf # dig directory
      - name: log
        emptyDir: {}
```
",open,False,2019-03-01 01:43:26,2019-03-06 05:55:28
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/658,https://api.github.com/repos/kubernetes/ingress-gce/issues/658,Update upgrade_test.go to look for 2 backend services in whitebox test,"This is a follow up to #654. Since we removed usage of a default backend in the Ingress spec, the system default backend service (404 server) will also be created.",closed,True,2019-03-01 17:40:48,2019-03-01 18:56:50
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/659,https://api.github.com/repos/kubernetes/ingress-gce/issues/659,"[Upgrade Test] After path addition to Ingress post-upgrade, do not error out if we receive 404's.","Since we add a new host to the Ingress, some amount of 404's are expected. ",closed,True,2019-03-01 21:45:41,2019-03-01 22:04:48
ingress-gce,grayluck,https://github.com/kubernetes/ingress-gce/pull/660,https://api.github.com/repos/kubernetes/ingress-gce/issues/660,Add exact error to the log when firewall operation fails,Currently the error from gce cloud is thrown away. Need this error to be in the log for better troubleshooting.,closed,True,2019-03-01 21:55:05,2019-03-01 22:03:28
ingress-gce,thockin,https://github.com/kubernetes/ingress-gce/issues/661,https://api.github.com/repos/kubernetes/ingress-gce/issues/661,GCP Description field is blank,Service LBs populate the description field with helpful information.   I see GCE ingress does not do that for things like forwarding rules.  Would be helpful in UIs.,open,False,2019-03-04 16:37:52,2019-03-27 15:33:23
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/662,https://api.github.com/repos/kubernetes/ingress-gce/issues/662,Cherrypick #576 into release 1.4 branch,"ref: #649

",closed,True,2019-03-04 18:52:33,2019-03-04 20:54:38
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/663,https://api.github.com/repos/kubernetes/ingress-gce/issues/663,[E2E Test] Add lock around random number generation for sandbox namespace,"Every so often, we see that the same namespace is generated for different tests. Since rand.Rand is apparently not safe concurrent use, this is expected. Adding a lock around random number generation should fix the issue.",closed,True,2019-03-05 18:23:01,2019-03-06 01:34:00
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/664,https://api.github.com/repos/kubernetes/ingress-gce/issues/664,"Revert ""Rebase base images from scratch and alpine to distroless""","Reverts kubernetes/ingress-gce#655.

Since this was merged, our CI has been failing [1] with the following error:

oci runtime error: container_linux.go:247: starting container process caused ""exec: \""sh\"": executable file not found in $PATH""

[1] https://k8s-testgrid.appspot.com/sig-network-ingress-gce-e2e",closed,True,2019-03-05 18:59:18,2019-03-05 19:05:01
ingress-gce,yuwenma,https://github.com/kubernetes/ingress-gce/pull/665,https://api.github.com/repos/kubernetes/ingress-gce/issues/665,Rebase from scratch images to distroless,"Only rebase the empty base image to distroless. Previous PR 655 rebased both empty and alpine and was reverted in [PR/664](https://github.com/kubernetes/ingress-gce/pull/664/files) due to the fact that the alpine images are run in kubelet with bash commands (which distroless doesn't have).  

",closed,True,2019-03-05 19:16:03,2019-03-05 22:21:11
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/666,https://api.github.com/repos/kubernetes/ingress-gce/issues/666,Replace uses of glog with klog,k/k has already done this migration so we should do the same.,closed,True,2019-03-05 21:56:54,2019-03-06 21:46:27
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/issues/667,https://api.github.com/repos/kubernetes/ingress-gce/issues/667,Move e2e tests from k/k into this repository,"Many of the Ingress-GCE e2e tests defined in k/k [1] can also be defined in this repo and will be much easier to maintain in this repo. At then end of this migration, the only tests in k/k that should remain are the conformance tests.

Using this issue to track the required changes. 

[1] https://github.com/kubernetes/kubernetes/blob/master/test/e2e/network/ingress.go",open,False,2019-03-06 19:14:20,2019-04-01 16:02:27
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/668,https://api.github.com/repos/kubernetes/ingress-gce/issues/668,Move tests for app protocols from k/k to Ingress-GCE repo,"The PR is split into two commits:

1. Extract echoheaders response body into public struct so it can be used elsewhere
2. Porting of tests

Note that the PR to remove the tests from k/k is https://github.com/kubernetes/kubernetes/pull/75143

Ref: #667",closed,True,2019-03-06 22:50:14,2019-03-29 17:16:24
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/669,https://api.github.com/repos/kubernetes/ingress-gce/issues/669,Update vendored k/k to remove glog usage,"Follow up to #666.

Remove all uses of glog inside of vendored k/k so that we are able to call klog.InitFlags() without getting an error that flags are being initialized twice.

The PR is split into two commits:

1. Vendor and associated changes to get 'make test' to pass
2. Wrap klog.InitFlags() inside of an init() so it's only called once.",closed,True,2019-03-07 02:33:11,2019-03-08 01:09:13
ingress-gce,jawlitkp,https://github.com/kubernetes/ingress-gce/issues/670,https://api.github.com/repos/kubernetes/ingress-gce/issues/670,How do I set up RPS limit,,open,False,2019-03-07 14:44:13,2019-03-17 06:34:18
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/671,https://api.github.com/repos/kubernetes/ingress-gce/issues/671,Add tests for HTTPS ,"Porting from k/k. Note that the PR to remove the tests in k/k is https://github.com/kubernetes/kubernetes/pull/75840

This PR is already pretty big so I'll add transition tests in a followup.

Ref: #667 ",closed,True,2019-03-07 20:29:46,2019-03-29 17:17:35
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/672,https://api.github.com/repos/kubernetes/ingress-gce/issues/672,WIP: add neg test,"WIP 
do not merge",closed,True,2019-03-07 22:03:32,2019-03-19 23:39:08
ingress-gce,joelsmith,https://github.com/kubernetes/ingress-gce/pull/673,https://api.github.com/repos/kubernetes/ingress-gce/issues/673,Update embargo doc link in SECURITY_CONTACTS and change PST to PSC,See https://github.com/kubernetes/security/issues/8 for more information,closed,True,2019-03-08 18:06:03,2019-03-09 00:20:27
ingress-gce,benfdking,https://github.com/kubernetes/ingress-gce/issues/674,https://api.github.com/repos/kubernetes/ingress-gce/issues/674,Confusion about the root health check,"Please forgive me if I have put this in the wrong place. 

As has been often stated in the issues, there is some confusion about the ingress health check requiring a 200 at the root. After some investigation, we noticed that the request contained a distinctive `User-Agent` header `GoogleHC/1.0`. With that in mind, we built some Go middleware based on that finding:

https://github.com/tumelohq/gke-ingress-healthcheck-middleware

It is very simple and works on the assumption that the request is:

1. indeed at the root path
3. the User-Agent is ""GoogleHC/1.0""
3. the request method is ""GET""

Would there be any way of either confirming this to ensure that this is expected and does not suddenly break? This information could also possibly be put in some documentation to avoid further confusion. 
",open,False,2019-03-08 22:50:21,2019-04-03 19:56:06
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/675,https://api.github.com/repos/kubernetes/ingress-gce/issues/675,Changing app-protocol from HTTP2 to other protocols will cause glbc to hit nil pointer ,"**Symptom:** Create an ingress with HTTP2 app protocol. For instance: https://github.com/kubernetes/kubernetes/tree/master/test/e2e/testing-manifests/ingress/http2
Manually change the app protocol on the spec of service from HTTP2 to something like HTTPS,HTTP. This will cause glbc to hit nil pointer and crash with following stacktrace in ingress-gce 1.2:

```
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:72
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:65
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:51
/usr/local/go/src/runtime/asm_amd64.s:573
/usr/local/go/src/runtime/panic.go:502
/usr/local/go/src/runtime/panic.go:63
/usr/local/go/src/runtime/signal_unix.go:388
/go/src/k8s.io/ingress-gce/pkg/healthchecks/healthchecks.go:332
/go/src/k8s.io/ingress-gce/pkg/healthchecks/healthchecks.go:244
/go/src/k8s.io/ingress-gce/pkg/healthchecks/healthchecks.go:106
/go/src/k8s.io/ingress-gce/pkg/backends/backends.go:220
/go/src/k8s.io/ingress-gce/pkg/backends/backends.go:283
/go/src/k8s.io/ingress-gce/pkg/backends/backends.go:252
/go/src/k8s.io/ingress-gce/pkg/controller/cluster_manager.go:98
/go/src/k8s.io/ingress-gce/pkg/controller/controller.go:349
/go/src/k8s.io/ingress-gce/pkg/controller/controller.go:299
/go/src/k8s.io/ingress-gce/pkg/controller/controller.go:111
/go/src/k8s.io/ingress-gce/pkg/utils/taskqueue.go:84
/go/src/k8s.io/ingress-gce/pkg/utils/taskqueue.go:54
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134
/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88
/go/src/k8s.io/ingress-gce/pkg/utils/taskqueue.go:54
/usr/local/go/src/runtime/asm_amd64.s:2361
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x1674e63]

goroutine 99 [running]:
k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)
	/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:58 +0x107
panic(0x19a12a0, 0x29aa020)
	/usr/local/go/src/runtime/panic.go:502 +0x229
k8s.io/ingress-gce/pkg/healthchecks.NewHealthCheck(...)
	/go/src/k8s.io/ingress-gce/pkg/healthchecks/healthchecks.go:332
k8s.io/ingress-gce/pkg/healthchecks.(*HealthChecks).Get(0xc42007bec0, 0xc420a8e3e0, 0x1e, 0x1cda028, 0x2, 0x0, 0x0, 0x0)
	/go/src/k8s.io/ingress-gce/pkg/healthchecks/healthchecks.go:244 +0x313
k8s.io/ingress-gce/pkg/healthchecks.(*HealthChecks).Sync(0xc42007bec0, 0xc42068b880, 0xc, 0xc420518c10, 0xb, 0x0)
	/go/src/k8s.io/ingress-gce/pkg/healthchecks/healthchecks.go:106 +0xae
k8s.io/ingress-gce/pkg/backends.(*Backends).ensureHealthCheck(0xc420303960, 0xc420518b90, 0xc, 0xc420518c10, 0xb, 0x0, 0x1bb, 0x0, 0x0, 0x7f21, ...)
	/go/src/k8s.io/ingress-gce/pkg/backends/backends.go:220 +0xc0
k8s.io/ingress-gce/pkg/backends.(*Backends).ensureBackendService(0xc420303960, 0xc420518b90, 0xc, 0xc420518c10, 0xb, 0x0, 0x1bb, 0x0, 0x0, 0x7f21, ...)
	/go/src/k8s.io/ingress-gce/pkg/backends/backends.go:283 +0x233
k8s.io/ingress-gce/pkg/backends.(*Backends).Ensure(0xc420303960, 0xc4208ab980, 0x1, 0x1, 0xc4203d1b30, 0x1, 0x1, 0x1670ad8, 0x184a100)
	/go/src/k8s.io/ingress-gce/pkg/backends/backends.go:252 +0x1fd
k8s.io/ingress-gce/pkg/controller.(*ClusterManager).EnsureLoadBalancer(0xc4208f7680, 0xc4203f9b20, 0xc4208aa380, 0x1, 0x1, 0xc4203d1b30, 0x1, 0x1, 0x1, 0x1)
	/go/src/k8s.io/ingress-gce/pkg/controller/cluster_manager.go:98 +0x1ed
k8s.io/ingress-gce/pkg/controller.(*LoadBalancerController).ensureIngress(0xc4204405a0, 0xc4208b7a20, 0xc420771c00, 0x3, 0x4, 0xc4208aa280, 0x1, 0x1, 0x0, 0x0)
	/go/src/k8s.io/ingress-gce/pkg/controller/controller.go:349 +0x4b6
k8s.io/ingress-gce/pkg/controller.(*LoadBalancerController).sync(0xc4204405a0, 0xc42039dd80, 0x14, 0xc420699b01, 0xc420472d00)
	/go/src/k8s.io/ingress-gce/pkg/controller/controller.go:299 +0x3e5
k8s.io/ingress-gce/pkg/controller.(*LoadBalancerController).(k8s.io/ingress-gce/pkg/controller.sync)-fm(0xc42039dd80, 0x14, 0xf, 0xc4208dddd8)
	/go/src/k8s.io/ingress-gce/pkg/controller/controller.go:111 +0x3e
k8s.io/ingress-gce/pkg/utils.(*PeriodicTaskQueue).worker(0xc420400240)
	/go/src/k8s.io/ingress-gce/pkg/utils/taskqueue.go:84 +0x17a
k8s.io/ingress-gce/pkg/utils.(*PeriodicTaskQueue).(k8s.io/ingress-gce/pkg/utils.worker)-fm()
	/go/src/k8s.io/ingress-gce/pkg/utils/taskqueue.go:54 +0x2a
k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil.func1(0xc42072dfa8)
	/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133 +0x54
k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc420a1ffa8, 0x3b9aca00, 0x0, 0x1, 0xc4200848a0)
	/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134 +0xbd
k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait.Until(0xc42072dfa8, 0x3b9aca00, 0xc4200848a0)
	/go/src/k8s.io/ingress-gce/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88 +0x4d
k8s.io/ingress-gce/pkg/utils.(*PeriodicTaskQueue).Run(0xc420400240, 0x3b9aca00, 0xc4200848a0)
	/go/src/k8s.io/ingress-gce/pkg/utils/taskqueue.go:54 +0x55
created by k8s.io/ingress-gce/pkg/controller.(*LoadBalancerController).Run
	/go/src/k8s.io/ingress-gce/pkg/controller/controller.go:231 +0x86
```

**Root Cause:**

1.  GCE GetHealthCheck V1 API used to return the health check type to ""HTTP"" while the healthcheck object is create beta API using ""HTTP2"". 

1. Now HTTP2 is promoted to GA on GCE. GCE GetHealthCheck V1 API returns health check type to ""HTTP2"". 

1. The GCE go client used by ingress-gce does not have the new HTTP2 fields in the V1 Healthcheck API. 

1. In case 1, this path is hit https://github.com/kubernetes/ingress-gce/blob/release-1.2/pkg/healthchecks/healthchecks.go#L328

1. With 2, this path is hit https://github.com/kubernetes/ingress-gce/blob/release-1.2/pkg/healthchecks/healthchecks.go#L332  `hc.Http2HealthCheck` is nil because the V1 healthcheck object does not contain `HTTP2HealthCheck` fields due to 3

**Impacted Versions:** 1.2, 1.3, 1.4, 1.5, HEAD
",open,False,2019-03-11 22:13:03,2019-03-14 12:27:16
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/issues/676,https://api.github.com/repos/kubernetes/ingress-gce/issues/676,Make HealthCheck syncer to follow Composite type pattern,Long term fix: https://github.com/kubernetes/ingress-gce/issues/675,open,False,2019-03-11 22:17:33,2019-03-11 22:40:27
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/677,https://api.github.com/repos/kubernetes/ingress-gce/issues/677,Update CONTRIBUTING.md with up-to-date links,Previous links pointed to old repo.,closed,True,2019-03-12 00:02:51,2019-03-12 23:02:39
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/678,https://api.github.com/repos/kubernetes/ingress-gce/issues/678,examine nil pointer before applying backend protocol configurations,"Mitigates #675
",closed,True,2019-03-12 00:27:36,2019-03-15 17:19:00
ingress-gce,spencerhance,https://github.com/kubernetes/ingress-gce/pull/679,https://api.github.com/repos/kubernetes/ingress-gce/issues/679,Port basic NEG test from k/k,"Related Issue: https://github.com/kubernetes/ingress-gce/issues/667

Adds a basic NEG test ""TestNEGBasic"" that tests the functionality of enabling/disabling NEG for ingress, and verifying that the corresponding actions/GC are correct.

Adds NEG as a component to the fuzz validator framework

Adds helper functions:
- pkg/annotations/service.go: 
  - NegAnnotation.String()
  - Service.NEGAnnotation()
  - Service.NEGStatus()
- pkgg/e2e/helpers.go
  - WaitForNEGDeletion()
  - WaitForNEGConfiguration()
- pkg/e2e/fixtures.go
  - EnsureEchoService()
  - EnsureIngress()
- pkg/fuzz/gcp.go
  - GCLB.CheckNEGDeletion()
- pkg/neg/types/types.go
  - ParseNegStatus()
- pkg/fuzz/features/neg.go
  - verifyNegBackend()
- pkg/fuzz/helpers.go
  - ServiceForPath()

",closed,True,2019-03-12 18:42:30,2019-03-26 01:04:05
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/680,https://api.github.com/repos/kubernetes/ingress-gce/issues/680,Add CHANGELOG for v1.5.0 & update version mapping,,closed,True,2019-03-12 21:56:16,2019-03-12 22:00:40
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/681,https://api.github.com/repos/kubernetes/ingress-gce/issues/681,Add BackendConfig specification for customizable health checks [WIP],"Addresses #42.

This PR by itself simply defines the schema for customizable health checks and adds some helpers. There is still a bit of plumbing that needs to be done to make this all work. ",open,True,2019-03-12 22:20:00,2019-03-21 18:15:32
ingress-gce,yuwenma,https://github.com/kubernetes/ingress-gce/pull/682,https://api.github.com/repos/kubernetes/ingress-gce/issues/682,"Revert ""Revert ""Rebase base images from scratch and alpine to distroless""""","Since Rohit has updated all the glog to klog, it's safe to move forward and revert back the distroless PR.",closed,True,2019-03-12 23:59:54,2019-03-13 00:14:40
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/683,https://api.github.com/repos/kubernetes/ingress-gce/issues/683,add an option for FeatureValidator to repeat the requests,"cc: @spencerhance

Can be useful for NEG feature validator",open,True,2019-03-13 00:54:37,2019-03-13 21:07:09
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/684,https://api.github.com/repos/kubernetes/ingress-gce/issues/684,More cleanup of unused flags,This is a followup to #651,closed,True,2019-03-13 06:02:49,2019-03-13 23:32:29
ingress-gce,toyota790,https://github.com/kubernetes/ingress-gce/issues/685,https://api.github.com/repos/kubernetes/ingress-gce/issues/685,Why GCP deploy an ingress controller on the master node rather than worker node?,"I found this GitHub [page](https://github.com/kubernetes/ingress-gce/blob/master/docs/faq/gce.md#how-do-i-deploy-an-ingress-controller) stated that 

> On GCP (either GCE or GKE), every Kubernetes cluster has an Ingress controller running on the master, no deployment necessary.

I am just curious that why GCP would deploy an ingress controller on the master node rather than worker node? I think it may impact the performance of scheduling or other things if it deploy on the master. For general cases, we would probably deploy it on the master considering the performance impact. Is there any specific reason or other consideration for it?

Thank you so much! ",closed,False,2019-03-13 16:32:30,2019-03-14 18:28:58
ingress-gce,pcnfernando,https://github.com/kubernetes/ingress-gce/issues/686,https://api.github.com/repos/kubernetes/ingress-gce/issues/686,Link for the example on deploying ingress controller is not valid,"Under the heading;  [how-do-i-deploy-an-ingress-controller](https://github.com/kubernetes/ingress-gce/blob/master/docs/faq/README.md#how-do-i-deploy-an-ingress-controller), the example pointed is an invalid link.",open,False,2019-03-13 16:50:34,2019-03-13 17:06:31
ingress-gce,michallowicki,https://github.com/kubernetes/ingress-gce/pull/687,https://api.github.com/repos/kubernetes/ingress-gce/issues/687,Fix supporting secret-based and pre-shared certs at the same time,"Secret-based certs were skipped during loadbalancer runtime info update when pre-shared certs were specified.

Changed to always include secret-based certs and added a test to document this change.

This is a followup to #641",closed,True,2019-03-14 17:19:26,2019-03-15 16:52:59
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/688,https://api.github.com/repos/kubernetes/ingress-gce/issues/688,fix a bug where ingress GC will remove wanted LB if the namespace+nam…,fix a bug where ingress GC will remove wanted LB if the namespace+name length is between 44-47 chars Introduced by this commit: https://github.com/kubernetes/ingress-gce/commit/c74db67f12f55c1f9a16d9cc63fcca2a486fb353#diff-b1f4c09e8d55ceb3fe65799b21e57d43,closed,True,2019-03-14 21:45:29,2019-03-15 00:28:58
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/689,https://api.github.com/repos/kubernetes/ingress-gce/issues/689,Add e2e test for Ingress that has long namespace+name,Corresponding e2e test for #688,closed,True,2019-03-14 23:57:57,2019-03-15 22:19:56
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/690,https://api.github.com/repos/kubernetes/ingress-gce/issues/690,Add #687 onto master,"#687 was directly on the 1.5 branch but we need this change on master as well. 

FYI @michallowicki ",closed,True,2019-03-15 17:01:43,2019-03-15 17:33:00
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/691,https://api.github.com/repos/kubernetes/ingress-gce/issues/691,Cherrypick #678 into release-1.5,Ref: #678,closed,True,2019-03-15 17:43:35,2019-03-15 17:48:54
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/692,https://api.github.com/repos/kubernetes/ingress-gce/issues/692,Cherry pick #688 into release.15,Ref: #688 ,closed,True,2019-03-15 18:01:13,2019-03-15 18:16:59
ingress-gce,pedropregueiro,https://github.com/kubernetes/ingress-gce/issues/693,https://api.github.com/repos/kubernetes/ingress-gce/issues/693,Disabling/Configuring HSTS,"Is it possible to configure the HSTS options on the GCE ingress the same way it's possible to do it with the ingress-nginx:

https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#hsts",closed,False,2019-03-18 16:24:13,2019-03-19 12:20:36
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/694,https://api.github.com/repos/kubernetes/ingress-gce/issues/694,TestBasicEdge -> TestEdge,"This ensures that a regex filter of ""Basic"" does not include this test.

/assign @MrHohn ",closed,True,2019-03-18 16:50:18,2019-03-18 16:59:18
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/695,https://api.github.com/repos/kubernetes/ingress-gce/issues/695,Completely remove EnableBackendConfig flag,Followup to #651,closed,True,2019-03-19 22:45:15,2019-03-19 22:54:22
ingress-gce,spencerhance,https://github.com/kubernetes/ingress-gce/pull/696,https://api.github.com/repos/kubernetes/ingress-gce/issues/696,e2e StatusManager init() handle existing config map,"If the status manager does not shut down correctly, the config map will still exist the next time a test starts and prevent the test from running until you delete it via kubectl. This deletes and replaces the config map if it already exists.",closed,True,2019-03-19 23:34:26,2019-03-21 22:27:04
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/697,https://api.github.com/repos/kubernetes/ingress-gce/issues/697,Update docs for contributing ,"This PR organizes our contributor docs a bit better and removes lots of stale info. Specifically, there is now more information on:

1. how to get your dev environment setup
2. how to run tests and build 
3. how to run the controller locally to test changes. 

There are also other misc. updates. 

Ref: #557",closed,True,2019-03-19 23:57:59,2019-03-21 23:37:09
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/698,https://api.github.com/repos/kubernetes/ingress-gce/issues/698,"In E2E runner script, exit with error code 2 if encountered error with bootstrapping",This makes it easier for an external framework to distinguish between an error with the bootstrapping or an error due to the test failing (test failures return error code 1),closed,True,2019-03-20 00:20:41,2019-03-22 16:54:32
ingress-gce,mcfedr,https://github.com/kubernetes/ingress-gce/issues/699,https://api.github.com/repos/kubernetes/ingress-gce/issues/699,Add option to enable proxy protocol,"The google cloud load balancer can do it, so seems it would just need an annotation to enable.

https://cloud.google.com/load-balancing/docs/tcp/setting-up-tcp#proxy-protocol
",closed,False,2019-03-20 20:16:10,2019-03-20 21:25:45
ingress-gce,MrHohn,https://github.com/kubernetes/ingress-gce/issues/700,https://api.github.com/repos/kubernetes/ingress-gce/issues/700,"jobs failing with ""unknown flag: --verbose""","Ref https://k8s-testgrid.appspot.com/sig-network-ingress-gce-e2e#ingress-gce-e2e-scale, GLBC is crashlooping with ([this error](https://storage.googleapis.com/kubernetes-jenkins/logs/ci-ingress-gce-e2e-scale/1579/artifacts/e2e-1579-886f8-master/glbc.log)):
```
unknown flag: --verbose
```

Should probably cleanup manifest somewhere to stop using the recently deleted flags.

cc @rramkumar1 ",open,False,2019-03-21 16:59:05,2019-03-28 17:11:42
ingress-gce,spencerhance,https://github.com/kubernetes/ingress-gce/pull/701,https://api.github.com/repos/kubernetes/ingress-gce/issues/701,Refactor EnsureEchoService() for e2e tests to use a deployment,"EnsureEchoService() now creates a deployment instead of a single pod (existing tests use 1 replica).  This will be useful for e2e tests that test the creation of multiple NEGs.

Also includes some fixes from comments made on https://github.com/kubernetes/ingress-gce/pull/679, mainly refactoring some common checks into checkGCLB().
",closed,True,2019-03-26 01:03:29,2019-03-26 21:40:39
ingress-gce,spencerhance,https://github.com/kubernetes/ingress-gce/pull/702,https://api.github.com/repos/kubernetes/ingress-gce/issues/702,Add ClusterIP test case to NEG e2e tests,"Also:
- Removes the Protocol and ServicePort fields from EnsureEchoService() since using non-default values only makes the service invalid.
- Fix EnsureEchoService() to also update deployment scale",closed,True,2019-03-26 17:56:31,2019-03-26 22:57:23
ingress-gce,pdecat,https://github.com/kubernetes/ingress-gce/issues/703,https://api.github.com/repos/kubernetes/ingress-gce/issues/703,Firewall rule not updated properly with NEG if service uses name in targetPort or does not name its port,"Under some conditions, the `k8s-fw-l7--<uid>` firewall rule managed by the `ingress-gce` controller does not include the target pod's port to the list of allowed ports.
When this happens, health checks do not reach the pods and all requests end up in HTTP 502 errors.

For example, with the following service configuration:

```
apiVersion: v1
kind: Service
metadata:
  name: test
  namespace: default
  annotations:
    cloud.google.com/neg: '{""ingress"": true}'
spec:
  ports:
  - nodePort: 30742
    port: 80
    protocol: TCP
    targetPort: http
  selector:
    app: test
  sessionAffinity: None
  type: NodePort
```

The pods selected by this service have one container with a corresponding port named `http` and `httpGet` readiness/liveness probes referencing that port:

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: test
  name: test
  namespace: default
spec:
  containers:
  - image: nginx:latest
    name: nginx
    ports:
    - containerPort: 80
      name: http
      protocol: TCP
    livenessProbe:
      httpGet:
        path: /healthz
        port: http
        scheme: HTTP
    readinessProbe:
      httpGet:
        path: /healthz
        port: http
        scheme: HTTP
```

And FWIW, the ingress:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
  namespace: default
  annotations:
    kubernetes.io/ingress.class: ""gce""
    kubernetes.io/ingress.allow-http: ""true""
spec:
  backend:
    serviceName: test
    servicePort: 80
```

![image](https://user-images.githubusercontent.com/318490/55040609-67fc9b00-5029-11e9-801b-277df58c2c0d.png)

I've identified two work-arounds for now:

1. adding a name to the service port:

```
@@ -11,7 +11,8 @@ metadata:
     cloud.google.com/neg: '{""ingress"": true}'
 spec:
   ports:
-  - nodePort: 30742
+  - name: http
+    nodePort: 30742
     port: 80
     protocol: TCP
     targetPort: http
```

2. using the port number in the `targetPort` instead of the port name:

```
@@ -14,7 +14,7 @@ spec:
   - nodePort: 30742
     port: 80
     protocol: TCP
-    targetPort: http
+    targetPort: 80
   selector:
     app: test
   sessionAffinity: None
```

When any of those two changes is applied separately, the corresponding port is almost instantly added to the firewall rule:
![image](https://user-images.githubusercontent.com/318490/55040719-eb1df100-5029-11e9-8d8b-e68a10479a70.png)

And health checks reach the pods and all requests end up in HTTP 200 status.

Reverting those changes ends up in the original situation: port missing in firewall rule, failed health checks and 502 errors.

Tested on GKE master version 1.11.7-gke.12 with supposedly `ingress-gce` v1.4.3 according to https://github.com/kubernetes/ingress-gce/blob/master/README.md#gke-version-mapping.
I've yet to check if the issue is still current with `ingress-gce` v1.5.0 on GKE 1.12.5-gke.10+.

Having access to the GKE managed `ingress-gce` logs would greatly help troubleshooting these kind of errors. I did not face this issue in our preproduction environment because the same port was already allowed by another service that named its port.

PS: I've learned from reading the GCE ingress controller code that NEGs do not require services to be of type `NodePort` but I'm still in the process of converting ingresses to container native load balancing by adding the `cloud.google.com/neg: '{""ingress"": true}'`. I'll convert those services back to `ClusterIP` once done.

I believe this issue should be referenced by #583.",open,False,2019-03-26 23:59:19,2019-03-27 15:00:14
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/704,https://api.github.com/repos/kubernetes/ingress-gce/issues/704,Add TODO in allow_http feature to verify that HTTPS resources are cleaned up,"Currently, the feature validator only turns off the HTTPS path check but we also need to verify that the HTTPS resources are cleaned up. It is actually a known bug that we do not do this so once we fix the bug, we will need to add the corresponding validation here.

/assign @MrHohn 
",closed,True,2019-03-29 15:45:31,2019-03-29 16:07:55
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/705,https://api.github.com/repos/kubernetes/ingress-gce/issues/705,Static IP E2E test,"Porting from k/k. Note the PR to remove the tests in k/k is https://github.com/kubernetes/kubernetes/pull/75892

Ref: #667 ",closed,True,2019-03-29 17:01:56,2019-03-29 19:30:13
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/706,https://api.github.com/repos/kubernetes/ingress-gce/issues/706,Return name of validator in error from CheckResponse(),"This is a readability thing because it makes it easier to diagnose which validator failed.

/assign @bowei ",open,True,2019-03-29 18:57:57,2019-03-29 18:58:16
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/707,https://api.github.com/repos/kubernetes/ingress-gce/issues/707,Make sure static-ip validator is not run if annotation is empty + clean up addresses after test,"Followup to #705 

/hold until I verify the test also fully passes.",closed,True,2019-03-29 19:40:19,2019-03-29 20:02:14
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/708,https://api.github.com/repos/kubernetes/ingress-gce/issues/708,Update contributor docs to reference k/k Github workflow,/assign @MrHohn ,closed,True,2019-04-01 20:45:44,2019-04-01 20:56:34
ingress-gce,vbannai,https://github.com/kubernetes/ingress-gce/pull/709,https://api.github.com/repos/kubernetes/ingress-gce/issues/709,Default 404 server with metrics,"Added a new 404 server with metrics that supports the following: 
- Rebuild it with newer Go
- Supports graceful shutdown
- Add metrics serving
  - How many requests it is serving
  - Serving latency
- Add logging
  - Respong with a 404 status code and relevant message to every request
  - Configurable sampling requests to a max # of logs/sec [0.0 to 1.0]
  - Periodically if no traffic, just to say “I am alive”

Tested the setup on a local desktop with 
- model name      : Intel(R) Xeon(R) W-2135 CPU @ 3.70GHz 
- with 12 processor core
- 64GB RAM

Prometheus version 2.8.0 
- includes yml file for setting alerts and rates

Benchmark results
- Tested with ""ab"" generating 20M packets over 2000 connections 


",open,True,2019-04-02 01:10:59,2019-04-04 20:38:38
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/710,https://api.github.com/repos/kubernetes/ingress-gce/issues/710,make NEGTransition test unflakey under load,"In NEGTransition test, for every test case, after applying the new setup, it waits for ingress to stablize by calling https://github.com/kubernetes/ingress-gce/blob/master/cmd/e2e-test/neg_test.go#L90

However, if ingress-gce controller is under load, it takes more time to configure the LB based on the new config. Then the old LB setup will stays the same. And `WaitForIngress` will not block because the old setup is already working.

This PR makes the NEG feature validator to validate the reverse case (IG backends). So that `WaitForIngress` will block when NEG is disabled and wait for the ingress-gce to adjust backend service to use IG",closed,True,2019-04-03 00:44:02,2019-04-03 17:57:47
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/711,https://api.github.com/repos/kubernetes/ingress-gce/issues/711,WIP: readiness reflector,Will add unit tests,open,True,2019-04-04 00:14:51,2019-04-04 20:53:27
ingress-gce,freehan,https://github.com/kubernetes/ingress-gce/pull/712,https://api.github.com/repos/kubernetes/ingress-gce/issues/712,bump ingress setup timeout to 25 min,Test times out at waiting for ingress to stabilize. Attempt to stabilize ingress test grid by bumping up timeout.,closed,True,2019-04-04 18:24:37,2019-04-04 19:14:51
ingress-gce,pdeva,https://github.com/kubernetes/ingress-gce/issues/713,https://api.github.com/repos/kubernetes/ingress-gce/issues/713,sync error regression in 1.5.1,"We recently upgraded our GKE cluster 1.12.6-gke.7 (and subsequently to 1.12.6-gke.10) and started noticing this error in the ingress:

```
Error during sync: error running load balancer syncing routine: resource name may not be empty
```

While the ingress is still functioning, this messages keep appearing.
it seems be some regression in 1.5.1 which was introduced in 1.12.6-gke.7",open,False,2019-04-04 18:39:00,2019-04-05 15:12:33
ingress-gce,spencerhance,https://github.com/kubernetes/ingress-gce/pull/714,https://api.github.com/repos/kubernetes/ingress-gce/issues/714,Log description for NEG transition e2e tests,Log the test case description for the NEG transition tests since they're all run within one sandbox.  Should make debugging more transparent.,closed,True,2019-04-04 21:55:48,2019-04-05 00:19:44
ingress-gce,rramkumar1,https://github.com/kubernetes/ingress-gce/pull/715,https://api.github.com/repos/kubernetes/ingress-gce/issues/715,Update CHANGELOG.md and version mapping for v1.5.1 release,/assign @MrHohn ,closed,True,2019-04-05 15:43:09,2019-04-05 16:01:21
